[
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster\nScenes\nAisha Al-Mohannadi1,2, Ayisha Firoz3, Yin Yang2 Muhammad Imran1 Ferda Ofli1\n1Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar\n2College of Science & Engineering, Hamad Bin Khalifa University, Doha, Qatar\n3Department of Computing, Qatar University, Doha, Qatar\naalmohannadi@hbku.edu.qa, af2400459@student.qu.edu.qa, yyang@hbku.edu.qa, mimran@hbku.edu.qa, fofli@hbku.edu.qa\nAbstract\nSocial media imagery provides a low-latency source of situ-\national information during natural and human-induced disas-\nters, enabling rapid damage assessment and response. While\nVisual Question Answering (VQA) has shown strong per-\nformance in general-purpose domains, its suitability for the\ncomplex and safety-critical reasoning required in disaster re-\nsponse remains unclear. We introduce DisasterVQA, a bench-\nmark dataset designed for perception and reasoning in cri-\nsis contexts. DisasterVQA consists of 1,395 real-world im-\nages and 4,405 expert-curated question\u2013answer pairs span-\nning diverse events such as floods, wildfires, and earthquakes.\nGrounded in humanitarian frameworks including FEMA ESF\nand OCHA MIRA, the dataset includes binary, multiple-\nchoice, and open-ended questions covering situational aware-\nness and operational decision-making tasks. We benchmark\nseven state-of-the-art vision\u2013language models and find per-\nformance variability across question types, disaster cate-\ngories, regions, and humanitarian tasks. Although models\nachieve high accuracy on binary questions, they struggle\nwith fine-grained quantitative reasoning, object counting, and\ncontext-sensitive interpretation, particularly for underrepre-\nsented disaster scenarios. DisasterVQA provides a challeng-\ning and practical benchmark to guide the development of\nmore robust and operationally meaningful vision\u2013language\nmodels for disaster response. The dataset is publicly available\nat https://zenodo.org/records/18267770.\nIntroduction\nNatural disasters such as floods, earthquakes, wildfires, and\nhurricanes cause widespread devastation, threatening lives,\ninfrastructure, and economies. In their immediate aftermath,\nfirst responders and humanitarian agencies require timely\nand actionable intelligence to support response and recov-\nery efforts. Rapidly understanding visual conditions on the\nground is therefore critical for effective humanitarian oper-\nations. Social media plays an increasingly important role in\nmodern disaster management by providing low-latency ac-\ncess to visual information from affected areas, provided that\ndamage scenes can be accurately interpreted and systemati-\ncally analyzed (Imran et al. 2020; Agarwal et al. 2020).\nTo this end, Vision-Language Models (VLMs), which in-\ntegrate visual and textual information, offer promising capa-\nCopyright \u00a9 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nbilities for automating the analysis of complex scenes. Exist-\ning Visual Question Answering (VQA) benchmark datasets\nhave enabled significant progress in understanding the ev-\neryday scenes (Goyal et al. 2017; Hudson and Manning\n2019; Al-Tahan et al. 2024; Yu et al. 2024). However, these\ndatasets primarily focus on common objects, people, and\ndaily activities, and do not capture the unique challenges\nposed by disaster imagery.\nSome Earth observation-oriented VQA benchmarks ad-\ndress disaster-related scenarios using overhead satellite im-\nagery, but they typically focus on narrow, task-specific ob-\njectives such as disaster prediction or building damage as-\nsessment, often framed within broader general-purpose eval-\nuation settings (Danish et al. 2025; Soni et al. 2025; Wang\net al. 2025). In contrast, there is currently no VQA dataset\nthat is (i) specifically curated for disaster scenes depicted\nin natural, ground-level imagery and (ii) explicitly grounded\nin established humanitarian disaster response frameworks.\nDisaster environments differ fundamentally from everyday\nsettings: they are chaotic, partially damaged, and visually\ncomplex, requiring models to reason about critical condi-\ntions such as accessibility, damage severity, and safety risks.\nWithout dedicated benchmarks, current VLMs cannot be\nsystematically evaluated or effectively advanced for these\nhigh-stakes humanitarian applications.\nTo tackle this challenge, we introduce DisasterVQA, the\nfirst large-scale VQA dataset centered on real-world natu-\nral disaster imagery. DisasterVQA consists of around 4,405\ncarefully annotated image-question-answer triplets, cover-\ning a diverse range of disaster events. The questions are\ndesigned across three formats: binary, multiple-choice, and\nopen-ended, to comprehensively assess VLMs\u2019 reasoning\ncapabilities under varied levels of difficulty and ambiguity.\nImportantly, the questions are grounded in established hu-\nmanitarian response frameworks (e.g., FEMA ESF (FEMA\n2025), OCHA MIRA (UN OCHA 2025), etc.) and cover\ntasks such as identifying built environment damage, deter-\nmining hazard type and severity, evaluating accessibility,\nand assessing movement restrictions and controls, making\nthe dataset highly relevant for real-world disaster response\napplications. Figure 1 illustrates sample image-question-\nanswer triplets from the dataset.\nAlongside the dataset, we provide a comprehensive\nevaluation framework to drive reproducible research in\narXiv:2601.13839v1  [cs.CV]  20 Jan 2026\nFigure 1: Sample image-question-answer triplets from the proposed DisasterVQA dataset\ndisaster-focused VQA, benchmarking seven state-of-the-art\nVLM architectures (Molmo-7B, LLama-3.2-11B, Pixtral-\n12B, Mistral-Small-3.1-24B, Qwen2.5-VL-32B, GPT-4o-\nmini, GPT-4.1-mini) across binary, multiple-choice, and\nopen-ended question types and reporting baseline perfor-\nmances in terms of accuracy, precision, recall, and F1-score.\nThe evaluation reveals that while binary questions are\nhandled with relatively high reliability especially by GPT-\n4.1-mini, multiple-choice and open-ended questions re-\nmain challenging, with performance varying across disas-\nter types and geographical regions. Open-weight models,\nthough competitive in some areas, often show greater incon-\nsistency. These results underscore the importance of region-\nand task-specific benchmarking in VQA, and highlight the\nneed for improved visual reasoning in zero-shot settings. By\nexposing these strengths and limitations, our benchmark of-\nfers actionable insights for advancing VLMs in high-stakes,\nreal-world disaster scenarios.\nRelated Work\nVQA for disaster response remains underexplored, with few\ndatasets that capture the complexity of real world crisis\nscenes. To our knowledge, the only dedicated benchmark,\nFloodNet (Rahnemoonfar et al. 2021), provides around\n3,200 UAV images and 11,000 question-answer pairs for\nflood detection, road passability, and water-body differen-\ntiation. However, FloodNet is restricted to aerial images of\nflood scenarios with mostly binary and counting questions,\nlacking ground-level complexity and is limited in event di-\nversity. In the literature, advanced prompting techniques to\nFloodNet were proposed, e.g., in (Sun, Wang, and Peng\n2024; Karimi and Rahnemoonfar 2025); however, these so-\nlutions are largely constrained by the narrow question for-\nmation and single-event focus of FloodNet.\nBeyond flood-specific VQA, RSVQA (Lobry et al. 2020)\nintroduces VQA on satellite images for land cover clas-\nsification and object counting. However, RSVQA depends\nupon aerial imagery as well as simple counting / classifica-\ntion questions, which do not support the detailed inferencing\nneeded for disaster-response reasoning. The work has been\nfurther improved in Prompt-RSVQA (Chappuis et al. 2022)\nto introduce visual context prompts into language models,\nwhere the authors provide more hints such as object labels,\nand scene metadata to help improve model performance on\nRSVQA. However, its questions remain standard environ-\nmental monitoring rather than specialized, high-stakes ques-\ntions that would be necessary for disaster response.\nA recent multimodal VQA dataset, TAMMI (Boussaid\net al. 2025), includes RGB, multispectral, and SAR imagery\nfor geospatial question answering, which could be valuable\nin disaster scenarios. However, TAMMI does not include\nquestions assessing accessibility, damage severity, or safety\nin these contexts.\nGeneric VQA benchmarks, such as VQA v2.0 (Goyal\net al. 2017), GQA (Hudson and Manning 2019), and Vi-\nsual7W (Zhu et al. 2016), mostly cover everyday scenes\nStage 4: Benchmarking\nStage 3: Question Verification\nStage 2: Question\nGeneration\nCrisisMMD\n(18k images)\nIncident1M\n(1Million images)\nMEDIC\n(15K images)\nBinary Questions\nMultiple-choice\nQuestions\nOpen-ended\nQuestions\nAdd/modify\nQuestions\nFilter Incorrect\nQuestions\nVerify\nModifications\nAdd Questions\n(if needed)\nQwen-2.5 32B\nPixtral 12B\nMolmo 7B\nMistral-3.1 24B\nLlama-3.2 11B\nGPT-4o-mini\nGPT-4.1-mini\nGPT-4o\nHuman Selectors\nStage 1: Image\nSelection\nLLM-based\nquestion\ngeneration\nDisaster Positive\nFiltering\nHard-Negative\nFiltering\nManual sampling\nGroup A\nGroup B\nSource Datasets\nFigure 2: Overview of the four stages followed for the dataset curation and benchmarking\nrather than disaster ones. The clean, object-centric images\nin these datasets often fail to reflect the occlusions, debris,\nand visual chaos of disaster environments; meanwhile, de-\nspite the large number of images therein, these datasets do\nnot address critical challenges in disaster response, such as\npoor visibility or partially damaged objects.\nIn the disaster imagery domain beyond VQA, datasets\nsuch as xBD (Gupta et al. 2019) compile massive pre-\nand post-event satellite imagery with around 850K build-\ning annotations and ordinal damage levels for segmentation\nand classification tasks. However, xBD lacks a question-\nanswer interface needed for interactive reasoning about in-\nfrastructure saftey or rescue planning. In summary, no exist-\ning benchmark provides multi-disaster real-image QA pairs,\nand carefully curated reasoning questions. DisasterVQA ad-\ndresses these needs by pairing diverse ground-level disaster\nimagery with binary, multiple-choice, and open-ended ques-\ntions specifically designed to test VLM performance in dis-\naster scenarios.\nDisasterVQA Construction\nThe primary objective of DisasterVQA is to establish a\nbenchmark that rigorously evaluates the capability of VLMs\nto function as reliable, operational assistants in crisis envi-\nronments. Unlike general-domain VQA tasks, disaster re-\nsponse requires models to interpret chaotic visual evidence\nthrough specific humanitarian lenses to identify not just the\npresence of damage, but its nature, severity, and implications\nfor response operations. Therefore, our construction process\nhas two overarching goals: technical rigor and operational\nrelevance. To achieve these goals, we employed a multi-\nstage, human-in-the-loop pipeline (as shown in Figure 2)\nthat integrates diverse source imagery with expert-verified\nannotations, as detailed in the following subsections.\nImage Selection\nWe sourced our base imagery from three established\ndatasets: CrisisMMD (Alam, Ofli, and Imran 2018), Inci-\ndents1M (Weber et al. 2022), and MEDIC (Alam et al.\n2023).1 While these repositories provide extensive cover-\nage, they vary significantly in metadata quality and visual\nrelevance. To ensure the dataset supports complex reasoning\nrather than simple classification, we implemented a targeted\nmanual curation process executed by two human annotators.\nThe selection strategy prioritized semantic density over vol-\nume, adhering to two strict criteria:\n\u2022 Disaster-positive images: Annotators selected only\nthose images exhibiting high visual complexity and\nmulti-object scenes clearly indicative of disaster events\n(e.g., floods, wildfires, earthquakes). Images with low vi-\nsual signal or minimal reasoning value were discarded to\nensure every positive sample posed a genuine interpreta-\ntive challenge.\n\u2022 Hard-negative images: To evaluate model resistance\nto false positives, we explicitly curated \u201chard-negative\u201d\nsamples, which are non-disaster images that share visual\ntextures or chaotic elements as disaster scenes (e.g., con-\nstruction sites or fireworks).\nThe final curated set comprises 1,605 disaster-positive im-\nages and 21 hard-negative images, which we use for the sub-\nsequent question-generation phase.\nQuestion Generation\nIn this phase, we utilized GPT-4o to generate three dis-\ntinct types of questions for each image: binary (yes/no),\nmultiple-choice (MCQ), and open-ended. For each question\ntype, the model was prompted to generate three unique ques-\ntions, resulting in a total of nine questions per image. Dur-\ning this process, we encountered a few challenges, including\nissues related to image sensitivity and content appropriate-\nness, which led to the exclusion of certain samples from the\ndataset. Consequently, this automated curation process re-\nfined the disaster-positive pool from 1,605 to 1,386 images,\nand the hard-negative set from 21 to 12 images. Next, we\nexamined the candidate 12,555 Q&As generated for 1,395\nimages (three Q&As for each question type per image) to\nperform the human verification.\n1Incidents1M is released under MIT License while CrisisMMD\nand MEDIC are available under CC BY-NC-SA 4.0 license.\n0\n200\n400\n600\n800\n1000\n1200\nHard-Negative\nOther Disasters\nStorm\nLandslide\nWildfire\nAccidents\nFire\nHurricane\nFlood\nEarthquake\n12\n42\n46\n52\n56\n71\n136\n265\n289\n427\n21\n141\n167\n173\n135\n289\n534\n867\n851\n1,227\nImages\nQuestions\n(a) Distribution by disaster type\n0\n200\n400\n600\n800\n1000\n1200\nAfrica\nSoutheast Asia\nMiddle East\nEurope\nSouth America\nSouth Asia\nNorth America\n6\n15\n20\n23\n70\n199\n380\n16\n46\n55\n93\n216\n499\n1,201\nImages\nQuestions\n(b) Distribution by geographic region\n0\n200\n400\n600\n800\n1000\nFirefighting\nSearch and Rescue\nDebris Clearance & Earthmoving\nEnvironmental/Terrain Context\nUtilities Status & Damage\nMovement Restrictions & Controls\nWater & Flood Characteristics\nAccess & Inaccessibility\nOther Useful Information\nPopulation & Asset Exposure\nHazard Type & Severity\nBuilt Environment Damage\nHumanitarian Category\n27\n48\n48\n66\n73\n89\n113\n410\n456\n604\n710\n722\n32\n52\n50\n68\n87\n100\n131\n511\n574\n870\n912\n1,018\nImages\nQuestions\n(c) Distribution by humanitarian category\nFigure 3: Overview of dataset composition in terms of image and question counts across (a) disaster types, (b) geographic\nregions, and (c) humanitarian categories.\nQuestion Verification\nAfter question generation, we focused on question refine-\nment and selection to guarantee question quality and appro-\npriate difficulty, as we conducted two rounds of human ques-\ntion selection:\n\u2022 Group A is a team of two annotators who reviewed the\nraw GPT-4o output for triviality and correctness. They\nalso corrected the model\u2019s incorrect answers and added\nnew questions to address identified gaps.\n\u2022 Group B, consisting of two senior researchers with over\na decade of experience in AI for disaster response, per-\nformed the final quality assurance pass. This group fo-\ncused on semantic consistency and domain specificity,\nretaining questions that posed meaningful challenges for\nhumanitarian analysis. Images yielding low-quality or ir-\nrelevant Q&A pairs were discarded at this stage.\nThe final human-verified dataset contains 4,405 ques-\ntion\u2013answer pairs, including 2,153 binary (yes/no), 1,693\nmultiple-choice, and 559 open-ended questions.\nHumanitarian Mapping\nNext, we map our dataset to established humanitarian needs\ntaxonomies. We employed GPT-4o to categorize each ver-\nified Q&A pair based on definitions from several frame-\nworks, including the FEMA Emergency Support Functions\n(ESF) (FEMA 2025) and OCHA MIRA (UN OCHA 2025).\nWe instructed the model to classify questions into two oper-\national themes: Situational Awareness and Actionable Infor-\nmation. Any valid disaster-related questions falling outside\nthese specific schemas were preserved in a broader \u201cOther\nUseful Information\u201d category.\nFinal Dataset Statistics\nThe final DisasterVQA dataset2 comprises 1,395 images\npaired with 4,405 questions, representing a diverse array of\ncrisis scenarios. As shown in Figure 3.a, the data exhibit a\nnaturally long-tailed distribution, reflecting real-world con-\ntent availability pattern on social media. Earthquakes (427\n2The DisasterVQA dataset is available at https://zenodo.org/\nrecords/18267770.\nimages, 1,227 questions), Floods (289 images, 851 ques-\ntions), and Hurricanes (265 images, 867 questions) dominate\nthe corpus, together accounting for approximately 70% of all\nsamples. This skew toward large-scale natural hazards mir-\nrors source distributions in CrisisMMD and Incidents1M,\nwhere globally visible events generate disproportionate me-\ndia coverage. At the same time, we include localized and\nobject-centric incident images, such as Fires (136 images,\n534 questions) and Accidents (71 images, 289 questions), to\nevaluate model robustness beyond highly contextual disaster\nscenes. The Hard-Negative category contains 12 adversarial\nimages paired with 21 questions and serves as a control set\nfor assessing hallucination and overconfidence.\nAs most images are from specific disaster events, we\nleverage event information to infer country-level geoloca-\ntion. The dataset spans a broad set of regions (as shown\nin Figure 3.b), with strong representation from South Asia\nand North America, reflecting population density and re-\nporting bias in social media\u2013derived data. A substantial por-\ntion of samples (682 images with 2,279 questions), primarily\nsourced from Incident1M, are labeled as N/A due to miss-\ning or ambiguous location information. From an operational\nperspective, the dataset emphasizes information needs crit-\nical to humanitarian response (ref. Figure 3.c): Built Envi-\nronment Damage (720 images, 1,018 questions) and Hazard\nType & Severity (709 images, 912 questions) are the most\nprevalent categories, followed by Population/Asset Presence\n& Exposure (604 images, 870 questions). Access & Inac-\ncessibility, central to routing and logistics, accounts for 410\nimages paired with 511 questions, while less frequent cate-\ngories such as Utilities Status & Damage, Search and Res-\ncue, and Debris Clearance & Earthmoving capture high-\nimpact, mission-critical decision-making scenarios.\nBenchmarking Experiments\nWe evaluate a suite of VLMs on DisasterVQA to assess their\noperational perception and reasoning capabilities for disas-\nter response. We benchmark seven state-of-the-art VLMs\nspanning diverse architectures, training paradigms, and\nmodel scales, including both strong open-weight and high-\nperforming proprietary models. To ensure reproducibility\nand fair comparison, all experiments are conducted with\nthe temperature parameter set to zero, yielding deterministic\noutputs. This systematic evaluation highlights the strengths\nand limitations of current VLMs in disaster contexts and es-\ntablishes reference baselines to support future research.\nVision-Language Models\nWe evaluate the following seven state-of-the-art VLMs in\nour benchmarking experiments:\n\u2022 Molmo-7B-D (Deitke et al. 2025) is a 7B-parameter\nopen-weight VLM that integrates a Qwen2-7B decoder\nwith an OpenAI CLIP vision backbone via lightweight\ncross-modal adapters. The model is trained on PixMo, a\ncorpus of one million carefully curated image-text pairs,\nand is optimized for multimodal pointing, captioning,\nand visual question answering tasks.\n\u2022 LLaMA-3.2-11B (Meta AI 2024) is an 11B-parameter\nopen-weight VLM from the LLaMA-3.2-Vision family\nof pretrained and instruction-tuned models. Built upon\nthe LLaMA-3.1 text-only backbone, it is optimized for\nvisual recognition, image reasoning, image captioning,\nand general visual question answering.\n\u2022 Pixtral-12B (Agrawal et al. 2024) is an open-weight\nVLM built on the Mistral Nemo 12B text decoder\nand a 400M-parameter custom vision encoder trained\nfrom scratch. The model supports native-resolution and\nvariable-aspect-ratio image inputs, enabling interleaved\ntext-image reasoning and multi-image processing within\na 128K-token context window. It is designed for complex\nmultimodal tasks such as document question answering,\nchart understanding, and multi-turn visual dialogue.\n\u2022 Mistral-Small-3.1-24B (Mistral AI 2025) is a 24B-\nparameter open-weight VLM built on Mistral Small 3,\naugmented with enhanced vision capabilities for robust\nimage understanding within a 128K-token context win-\ndow. Its instruction-tuned variant is optimized for agentic\nmultimodal tasks such as document question answering,\nchart reasoning, and visual instruction following, achiev-\ning strong performance on multimodal benchmarks while\nremaining efficient enough for single-GPU deployment.\n\u2022 Qwen2.5-VL-32B (Bai et al. 2025) is a 32B-parameter\nopen-weight VLM from the Qwen-2.5-VL family, de-\nsigned for advanced multimodal understanding and rea-\nsoning. It excels at interpreting text-rich and structured\nvisual content, supports long-context multimodal inputs,\nand enables agentic capabilities such as visual grounding,\ntool use, and precise object localization with structured\noutputs. In this study, we use an Activation-aware Weight\nQuantization (AWQ) 4-bit variant, reducing memory us-\nage by up to fourfold with less than a 3% accuracy drop.\n\u2022 GPT-4o-mini (OpenAI 2024) is a proprietary model\nfrom OpenAI that has multimodal capabilities across\ntext, vision, and audio within a 128K-token context win-\ndow. Despite its compact design, it achieves performance\nclose to GPT-4o on challenging vision and reasoning\nbenchmarks, making it well suited for large-scale and\nresource-constrained deployments.\n\u2022 GPT-4.1-mini\n(OpenAI\n2025)\nis\na\nproprietary,\nefficiency-oriented\nvariant\nof\nthe\nGPT-4.1\nfamily\nthat preserves the full 1M-token context window and the\nmodel\u2019s enhanced capabilities in coding and instruction\nfollowing, while substantially reducing per-query costs\nand increasing throughput relative to GPT-4o. It also\nsupports multimodal inputs, making it a scalable option\nfor high-context, multimodal reasoning tasks.\nOutput Post-processing\nWith the refined questions in place, we performed zero-\nshot inference across all models. Because model outputs are\noften inconsistently formatted, we applied an LLM-based\npost-processing step using GPT-4o to standardize responses\nacross question types. This step extracts clean, structured\noutputs: binary questions are normalized to exactly Yes or\nNo, while multiple-choice responses are converted into a\nJSON array containing the selected option(s). This normal-\nization enables reliable and automated evaluation.\nFor open-ended questions, we adopt an LLM-as-a-judge\napproach with GPT-4o to assess answer correctness in addi-\ntion to output normalization. The judge model compares the\nextracted response against the ground-truth answer, account-\ning for semantic equivalence, paraphrasing, and formatting\nvariations, and produces a structured decision (Correct or\nIncorrect) along with the normalized answer. Responses\njudged as correct are incorporated into the evaluation, while\nincorrect cases are flagged for further error analysis.\nEvaluation Metrics\nWe measure three metrics in our evaluations, one for each\nquestion type:\n\u2022 Binary accuracy: The fraction of binary questions\nwhere the model\u2019s Yes/No prediction exactly matches\nthe ground truth over the total number of predictions:\nAccbinary =\nTP + TN\nTP + TN + FP + FN.\n(1)\n\u2022 Open-ended accuracy: The percentage of open-ended\nquestions where the model\u2019s output is rated as Correct\nby the LLM-as-a-judge:\nAccopen =\n1\nNopen\nNopen\nX\ni=1\nJ\n\u0000\u02c6yi, yi),\n(2)\nwhere Nopen is the total number of open-ended questions,\nyi is the ground truth answer, \u02c6yi is the model\u2019s output on\nimage-question pair i, and J\n\u0000\u00b7, \u00b7\n\u0001\nis the LLM-as-a-judge\nfunction which returns 1 if the judge rates the answer as\nCorrect and 0 otherwise.\n\u2022 Multiple-choice F1-score: We evaluate all MCQs jointly\nas a multi-label classification task. For each question, we\ncompute the number of true positives (TP), false positives\n(FP), and false negatives (FN) across predicted answer\noptions. The overall F1-score is then given by\nF1,mcq =\n2 TP\n2 TP + FP + FN\n(3)\nwhich concisely balances the model\u2019s ability to avoid\nfalse positives against its ability to capture all true posi-\ntives across the full set.\nFigure 4: Overall performance of all the models for binary,\nopen-ended, and multiple-choice questions\nComputational Resources\nAll five open-weight models (i.e., Molmo-7B-D, Llama-\n3.2-11B, Pixtral-12B, Mistral-Small-3.1-24B, and Qwen-\n2.5-VL-32B) were benchmarked on our on-premise cluster\nequipped with NVIDIA A100 GPUs. In contrast, the two\nGPT variants (i.e., GPT-4o-mini and GPT-4.1-mini) were\naccessed via the Microsoft\u2019s Azure OpenAI Service.\nExperimental Results\nIn this section, we provide a detailed analysis of the ex-\nperimental results across multiple evaluation dimensions.\nFigure 4 summarizes model performance across the three\nquestion types. GPT-4.1-mini achieves the strongest over-\nall results, with scores of 0.91 on binary questions, 0.83 on\nopen-ended questions, and 0.85 on MCQs. Among open-\nweight models, Mistral-Small-3.1-24B and Qwen2.5-VL-\n32B emerge as close competitors, outperforming GPT-4o-\nmini on the majority of evaluations. Although Molmo-7B-D\ndemonstrates competitive performance on binary questions,\nit lags behind other models on open-ended and multiple-\nchoice questions, indicating limitations in more complex\nreasoning tasks.\nAnalysis By Humanitarian Category\nFigure 5 reports model performance across humanitarian\ncategories. We evaluate each question format separately to\nshow how performance changes from coarse detection (bi-\nnary) to harder reasoning (open-ended and multiple-choice).\nOverall, binary results are strong, but gaps grow for cate-\ngories that require fine-grained attributes (e.g., flood details)\nor counting/estimation (exposure).\nBinary Questions\nBinary accuracy is high across cate-\ngories: Environmental/Terrain Context is perfect (\u22481.00)\nand Built Environment Damage is consistently strong (\u2248\n0.93). The hardest binary category is Access & Inaccessi-\nbility (\u22480.77, range \u22480.72\u20130.84), with Movement Restric-\ntions & Controls (\u22480.80) and Water & Flood Character-\nistics (\u22480.83) also below the top tier. Overall, the best bi-\nnary model is GPT-4.1-mini (\u22480.91), closely followed by\nMistral-Small-3.1-24B (\u22480.91); the top model varies by\ncategory (e.g., Mistral peaks on Built Environment Damage,\nwhile LLaMA-3.2-11B peaks on Water & Flood Character-\nistics).\nOpen-Ended\nQuestions\nOpen-ended\naccuracy\nvaries\nmore by category. Access & Inaccessibility is strongest (avg\n\u22480.90, best \u22480.98), while Population & Asset Exposure\nis weakest (avg \u22480.58, down to \u22480.43), consistent with\ncounting and clutter challenges. Other Useful Information is\nmid-range (avg \u22480.71), and Search and Rescue shows low\naverage and high variance (avg \u22480.64, \u22480.00\u20131.00), likely\nreflecting small coverage.\nMultiple-Choice\nQuestions\nMultiple-choice\nperfor-\nmance (MCQ F1) highlights fine-grained labeling difficulty.\nHazard Type & Severity is strongest (avg F1 \u22480.90, best\n\u22480.96), while Built Environment Damage is moderate\n(avg F1 \u22480.75), showing that detecting damage is easier\nthan selecting the correct damaged assets. The weakest\ncategory is Water & Flood Characteristics (avg F1 \u22480.63),\nand Access & Inaccessibility is also challenging (avg\nF1 \u22480.72), where some models over-select options (higher\nfalse positives).\nSA vs. AT and Model Size\nGrouping categories shows\nthat AT (Actionable) is easier than SA (Situational Aware-\nness) for binary questions (SA \u22480.88 vs. AT \u22480.92),\nmainly because difficult SA categories like Access & Inac-\ncessibility and Movement Restrictions require subtle \u201cusabil-\nity\u201d judgments, while AT often has clearer visual evidence\n(responders, machinery, fire). Model size effects are modest\nfor binary but clearer for structured reasoning: smaller open\nmodels average \u22480.87 on binary versus \u22480.90 for larger\nopen models, with the biggest gaps in ambiguous SA cate-\ngories; for open-ended and MCQ, larger models are gener-\nally more stable, especially on exposure (counting) and wa-\nter (fine-grained attributes).\nAnalysis By Disaster Type\nFigure 6 illustrates the performance of the state-of-the-art\nmodels on different disaster types. We have evaluated each\nquestion type separately to observe the performance across\ndifferent metrics.\nBinary Questions\nThe accuracy is uniformly high: even\nthe weakest case (Storm) remains at 0.85 with GPT-4.1-\nmini, and every other disaster is at least 0.86. Fire and Wild-\nfire top out at 0.96\u20131.00, reflecting visually unambiguous\ndamage cues, while Storms and Floods (0.85\u20130.86) prove\nslightly tougher.\nOpen-Ended Questions\nthe accuracy shows the largest\nspread, from a low of 0.73 on Floods to a peak of 0.94 on\nLandslides (GPT-4.1-mini). Accidents and Earthquakes sit\nin the low-to-mid 0.80s, whereas Fires drop to 0.79 and Hur-\nricanes actually climb to 0.88.\nMultiple-Choice Questions\nthe F1 score falls between\nthe two: Storms again lead with 0.90, followed by Earth-\nquakes (0.86) and Floods/Accidents (0.85). The most chal-\nlenging MCQs are for Hurricanes (0.81) and Landslides\nMolmo-7B-D\nLLaMA-3.2-11B\nPixtral-12B\nMistral-Small-3.1-24B\nQwen-2.5-VL-32B\nGPT-4o-mini\nGPT-4.1-mini\nAccess & Inaccessibility (SA)\nBuilt Environment Damage (SA)\nDebris Clearance & Earthmoving (AT)\nEnvironmental/Terrain Context (SA)\nFirefighting (AT)\nHazard Type & Severity (SA)\nMovement Restrictions & Controls (SA)\nOther Useful Information\nPopulation & Asset Exposure (SA)\nSearch and Rescue (AT)\nUtilities Status & Damage (SA)\nWater & Flood Characteristics (SA)\n0.74\n0.75\n0.72\n0.82\n0.79\n0.73\n0.84\n0.92\n0.91\n0.91\n0.97\n0.94\n0.91\n0.96\n0.87\n0.80\n0.93\n0.93\n0.87\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.93\n0.80\n1.00\n1.00\n0.93\n0.93\n0.93\n0.89\n0.90\n0.85\n0.91\n0.90\n0.90\n0.93\n0.86\n0.83\n0.79\n0.80\n0.80\n0.71\n0.83\n0.92\n0.84\n0.88\n0.89\n0.88\n0.88\n0.89\n0.91\n0.89\n0.88\n0.90\n0.89\n0.91\n0.91\n0.89\n0.82\n0.93\n0.96\n0.96\n0.96\n0.96\n0.86\n0.78\n0.90\n0.90\n0.88\n0.80\n0.86\n0.79\n0.88\n0.81\n0.86\n0.86\n0.79\n0.83\nBinary (Accuracy)\nMolmo-7B-D\nLLaMA-3.2-11B\nPixtral-12B\nMistral-Small-3.1-24B\nQwen-2.5-VL-32B\nGPT-4o-mini\nGPT-4.1-mini\n0.67\n0.71\n0.72\n0.74\n0.74\n0.67\n0.78\n0.71\n0.75\n0.73\n0.76\n0.75\n0.73\n0.79\n0.74\n0.87\n0.73\n0.84\n0.81\n0.72\n0.89\n0.74\n0.83\n0.74\n0.84\n0.83\n0.80\n0.91\n0.68\n0.70\n0.78\n0.80\n0.80\n0.67\n0.89\n0.79\n0.93\n0.85\n0.92\n0.94\n0.94\n0.96\n0.83\n0.78\n0.76\n0.87\n0.83\n0.78\n0.87\n0.70\n0.77\n0.76\n0.80\n0.77\n0.76\n0.83\n0.80\n0.79\n0.79\n0.80\n0.81\n0.78\n0.82\n0.76\n0.81\n0.73\n0.79\n0.80\n0.68\n0.81\n0.73\n0.71\n0.77\n0.78\n0.78\n0.79\n0.79\n0.62\n0.59\n0.67\n0.65\n0.60\n0.63\n0.64\nMCQ (F1)\nMolmo-7B-D\nLLaMA-3.2-11B\nPixtral-12B\nMistral-Small-3.1-24B\nQwen-2.5-VL-32B\nGPT-4o-mini\nGPT-4.1-mini\n0.82\n0.82\n0.91\n0.87\n0.96\n0.91\n0.98\n0.79\n0.85\n0.84\n0.82\n0.82\n0.84\n0.89\n1.00\n1.00\n1.00\n0.80\n0.80\n1.00\n1.00\n0.88\n0.88\n1.00\n0.83\n0.83\n0.79\n0.92\n0.93\n0.80\n0.91\n0.93\n0.93\n0.93\n0.94\n0.93\n0.86\n0.93\n0.86\n0.71\n0.86\n1.00\n0.66\n0.81\n0.67\n0.74\n0.77\n0.78\n0.79\n0.43\n0.54\n0.58\n0.57\n0.66\n0.61\n0.68\n1.00\n1.00\n0.50\n0.00\n1.00\n0.50\n0.50\n0.79\n0.74\n0.74\n0.74\n0.84\n0.79\n0.84\n0.71\n0.71\n0.79\n0.71\n0.71\n0.71\n0.71\nOpen-ended (Accuracy)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFigure 5: Performance by humanitarian categories across models. ST: Situational Awareness, AT: Actionable\n(0.83), followed by Wildfires (0.84), likely due to complex\nmulti-object scenes and less-straightforward choices.\nHard-Negative\nAlthough the hard-negative subset is\nsmall, it is useful for checking false positives. Most top-\nperforming models achieve perfect or near-perfect binary\naccuracy (e.g., GPT-4.1-mini at 1.00), while smaller models\nare slightly lower (e.g., Pixtral-12B at 0.88). In the multiple-\nchoice format, performance is generally strong for the best\nmodels (near-perfect F1 for GPT-4.1-mini and Qwen2.5-\nVL), and Pixtral-12B remains competitive at F1 = 0.73.\nOverall, these results suggest that models can usually recog-\nnize non-disaster scenes, but the hard-negative set still ex-\nposes mild variability across smaller architectures.\nAnalysis By Region\nFigure 7 presents three heatmaps of model performance\nacross different geographic regions. There are several pat-\nterns that identify the strengths and weaknesses of these\nmodels, and we describe them in detail by analyzing each\nquestion type.\nMultiple-Choice Questions\nIn Figure 7 (MCQ), the Mid-\ndle East is the hardest region: scores cluster around\n0.60\u20130.72 (e.g., LLaMA-3.2-11B = 0.60, GPT-4.1-mini\n= 0.72). In contrast, Africa is the easiest, with all mod-\nels \u22650.83 and several reaching 0.96. GPT-4.1-mini is the\nstrongest overall, topping or tying every region, but its low-\nest value is in the Middle East (0.72). Open-source mod-\nels are competitive in well-performing regions (e.g., South\nAmerica and Southeast Asia), yet drop more sharply in the\nMiddle East where all models compress toward the lower\nend of the scale.\nBinary Questions\nIn Figure 7 (Binary), accuracy is high\noverall across regions, with the lowest value still at 0.75\n(LLaMA-3.2-11B in Southeast Asia). However, regional dif-\nferences remain visible: Africa shows a split between perfect\nscores for some open models (LLaMA-3.2-11B and Mistral-\nSmall at 1.00) and lower scores for others (including GPT\nmodels at 0.80). South Asia is consistently strong across\nmodels (roughly 0.88\u20130.93), while Middle East is more\nmixed (e.g., LLaMA-3.2-11B = 0.78 vs. Mistral-Small\n= 0.96). Notably, GPT-4.1-mini peaks at 1.00 in Southeast\nAsia and remains solid elsewhere (typically 0.83\u20130.95).\nOpen-Ended Questions\nIn Figure 7 (Open-ended), re-\ngional variation becomes much stronger than in binary ques-\ntions. North America is clearly the hardest region, with accu-\nracies dropping to 0.35\u20130.59 across models, suggesting that\nfree-form reasoning and extraction is sensitive to region-\nspecific visual patterns and dataset composition. In contrast,\nSouth Asia and Southeast Asia are the easiest: several mod-\nels reach perfect accuracy (1.00) in these regions, includ-\ning GPT-4.1-mini and GPT-4o-mini. Europe and the Mid-\ndle East remain strong for the top models (e.g., GPT-4.1-\nmini = 0.91 in Europe and 0.86 in the Middle East), while\nsmaller open models show larger drops, especially in the\nlowest-performing regions.\nDiscussion\nQualitative Error Analysis\nFigure 8 illustrates representative failure cases across mod-\nels, revealing several recurring error patterns. A common is-\nsue is missed visual evidence in partially occluded or low-\nsalience regions. For instance, when asked whether a vehi-\ncle is submerged in water, both GPT-4.1-mini and Qwen-\n2.5-VL-32B incorrectly answered No despite a clearly visi-\nble submerged vehicle. This suggests sensitivity to contrast,\nviewpoint, or object completeness, where partially visible\nobjects are discounted during reasoning.\nA second dominant failure mode concerns fine-grained\nquantitative estimation and counting."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper\n\n#### Main Contributions\nThe paper introduces DisasterVQA (Disaster Visual Question Answering), a benchmark dataset designed for perception and reasoning in crisis contexts, specifically focused on disaster scenarios depicted in natural, ground-level imagery. The key contributions are:\n1. **Dataset Construction**: A comprehensive 1,395 real-world images and 4,405 expert-curated question-answer pairs covering diverse events such as floods, wildfires, and earthquakes.\n2. **Grounded Frameworks**: Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, ensuring questions are relevant to real-world disaster response tasks.\n3. **Question Types**: Comprehensive coverage of three types: binary (yes/no), multiple-choice, and open-ended questions, allowing for varied assessments of VLM performance.\n\n#### Methodology\nThe dataset construction process involves:\n1. **Image Selection**: Manual curation by two human annotators to ensure the dataset supports complex reasoning rather than simple classification.\n2. **Question Generation**: Utilizing GPT-4o to generate three distinct question types (binary, multiple-choice, and open-ended) for each image.\n3. **Human Verification**: Two rounds of human verification to refine questions and select appropriate ones based on difficulty and relevance.\n\n#### Key Findings\n1. **Performance Variability Across Question Types**: Models achieve high accuracy on binary questions but struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios.\n2. **Region-Specific Challenges**: The dataset reveals significant performance differences across regions, especially in the Middle East where models perform poorly compared to other regions.\n3. **Task-Specific Strengths**: Models like GPT-4o-mini show strong performance on binary questions and open-ended tasks but struggle with multiple-choice questions.\n\n#### Conclusions\n1. **Benchmarking Value**: DisasterVQA provides a challenging and practical benchmark for evaluating vision-language models in high-stakes humanitarian applications.\n2. **Future Research Directions**: The dataset highlights the need for improved visual reasoning, especially in zero-shot settings, to address current limitations of existing VLMs.\n3. **Operational Relevance**: By grounding questions in established humanitarian response frameworks, DisasterVQA ensures its relevance and applicability to real-world disaster response applications.\n\n### Detailed Breakdown\n\n#### Introduction\nThe paper addresses the gap in datasets specifically for disaster scenarios depicted in natural imagery. It highlights how Vision-Language Models (VLMs) can be used to automate complex scene analysis in disaster response, but current benchmarks do not capture the unique challenges posed by disaster imagery.\n\n#### DisasterVQA Construction\n1. **Image Selection**: The dataset is curated from established repositories like CrisisMMD and Incidents1M, with manual curation ensuring complexity and relevance.\n2. **Question Generation**: GPT-4o generates diverse question types (binary, multiple-choice, open-ended) for each image, covering a wide range of tasks relevant to disaster response.\n3. **Human Verification**: Two rounds of human verification refine the dataset by selecting appropriate questions based on difficulty and relevance.\n\n#### Evaluation Framework\nThe evaluation framework includes:\n1. **Binary Questions**: Models are evaluated using binary accuracy metrics (TP + TN / TP + TN + FP + FN).\n2. **Open-Ended Questions**: LLMs as judges assess answers, with a structured decision output for correctness.\n3. **Multiple-Choice Questions**: F1-score is used to evaluate performance across multiple answer options.\n\n#### Results and Analysis\n1. **Binary Accuracy**: Models perform well on binary questions but struggle with open-ended tasks that require fine-grained reasoning.\n2. **Region-Specific Performance**: The dataset reveals significant regional differences, particularly in the Middle East where models underperform compared to other regions.\n3. **Task-Specific Strengths**: GPT-4o-mini shows strong performance across multiple categories (binary and open-ended), but struggles with multiple-choice questions.\n\n#### Discussion\nThe paper concludes by emphasizing the need for future research in improving visual reasoning capabilities, especially in zero-shot settings, to address current limitations of VLMs. The dataset's practicality and relevance make it a valuable resource for advancing vision-language models in high-stakes disaster response scenarios.\n\n### Conclusion\nDisasterVQA is a comprehensive benchmark dataset designed to evaluate the performance of Vision-Language Models (VLMs) in real-world crisis contexts, specifically focusing on disaster imagery. By addressing the unique challenges posed by disaster scenes and providing detailed insights into model strengths and weaknesses, DisasterVQA offers actionable recommendations for advancing VLMs in high-stakes humanitarian applications."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Causal Feature Selection Framework for Stable Soft Sensor Modeling based on\nTime-Delayed Cross Mapping\nShi-Shun Chen a, b, Xiao-Yang Li a,*, Enrico Zio b, c\na School of Reliability and Systems Engineering, Beihang University, Beijing, China\nb Energy Department, Politecnico di Milano, Milan, Italy\nc Centre de Recherche sur les Risques et les Crises (CRC), MINES Paris-PSL University, Sophia\nAntipolis, France\nEmail:\ncss1107@buaa.edu.cn (Shi-Shun Chen)\nleexy@buaa.edu.cn (Xiao-Yang Li)\nenrico.zio@polimi.it, enrico.zio@mines-paristech.fr (Enrico Zio)\nCorresponding author*: Xiao-Yang Li\nHighlights\n\uf09f\nVarying causal strength across time delay is considered for feature selection.\n\uf09f\nState space reconstruction is employed to deal with interdependent variables.\n\uf09f\nTime-delayed partial cross mapping is proposed for direct causal inference.\n\uf09f\nFeatures are selected automatically via causal strength and validation performance.\n\uf09f\nFeatures selected by direct causality can enhance model stability.\nAbstract\nSoft sensor modeling plays a crucial role in process monitoring. Causal feature selection can\nenhance the performance of soft sensor models in industrial applications. However, existing methods\nignore two critical characteristics of industrial processes. Firstly, causal relationships between variables\nalways involve time delays, whereas most causal feature selection methods investigate causal\nrelationships in a same time dimension. Secondly, variables in industrial processes are often\ninterdependent, which contradicts the decorrelation assumption of traditional causal inference methods.\nConsequently, soft sensor models based on existing causal feature selection approaches often lack\nsufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature\nselection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state\nspace reconstruction to effectively handle interdependent variables in causality analysis, and considers\nvarying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is\nintroduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for\ndirect causal inference. Then, in order to achieve automatic feature selection, an objective feature\nselection strategy is presented. The causal threshold is automatically determined based on the model\nperformance on the validation set, and the causal features are then selected. Two real-world case studies\nshow that TDCCM achieves the highest average performance, while TDPCM improves soft sensor\nstability and performance in the worst scenario. On average over the two cases, TDCCM decreases root\nmean square error (RMSE) by about 8.93% compared with the best existing feature selection method,\nand TDPCM further decreases RMSE in the worst scenario by about 7.69% relative to TDCCM. The\ncode is publicly available at https://github.com/dirge1/TDPCM.\nKeywords: Soft sensor, Causal discovery, Time delay, Convergent cross mapping, Variable selection\n1\nList of Symbols\nC\nCandidate set of causal thresholds for feature selection\nD\nNumber of candidates for causal threshold determination\nE\nEmbedding dimension\nL\nLength of the time series\nM\nNumber of auxiliary variables for soft sensor modeling\nN\nNumber of historical measurements for soft sensor modeling\nMX, MY, MZ\nTrajectory matrices of X, Y and Z\nMX,l, MY,l\nEmbedding vectors of X and Y in the reconstructed space at the time point l\n,\n\u02c6 X\nY\nY l\n\uf0ae\nM\nPredicted embedding vectors of Y at the time point l using the trajectory matrix of X\n,\n,\n\u02c6 X\nY\nY l\n\uf078\n\uf078\n\uf0ae\n\uf02d\nM\nPredicted embedding vectors of Y at the time point l \u2212 \u03be using the trajectory matrix\nof X considering time delay \u03be\nQ\nNumber of disturbing variables\nU\u03b3\nStacked vector at time delay \u03b3 for TDPCM calculation\nX, Y, Z\nTime series variables\nZ\nDisturbing variable set\n\u02c6\nX\nZ\nPredicted time series of Z using the trajectory matrix of X\nc1,c2,\u2026, cD\nCandidates for causal threshold determination\ncbest\nOptimal causal threshold for feature selection\nd\nMaximum time delay considered in TDCCM/TDPCM analysis\nf\nMapping function of the soft sensor model\nl\nGeneric notation for time point\ntmin\nFirst valid prediction time for reconstructed series\nl\nit\nTime point of the ith nearest neighbor embedding vector\nwi\nWeight assigned to the ith nearest neighbor based on Euclidean distance\nx(l), y(l)\nThe value of X and Y at the time point l\nmin:\nt\nL\ny\nTime series of Y from time point tmin to time point L\n\uf028\uf029\n\u02c6X\nY\ny\nl\n\uf0ae\nScale prediction of y(l) using the trajectory matrix of X\nmin :\n\u02c6 X\nY\nt\nL\n\uf0ae\ny\nPredicted time series of Y from time point tmin to time point L using the trajectory\nmatrix of X\n\uf028\n\uf029\n,\n\u02c6X\nY\ny\nl\n\uf078\n\uf078\n\uf0ae\n\uf02d\nScale prediction of y(l \u2212 \u03be) using the trajectory matrix of X considering time delay \u03be\nmin\n,\n:\n\u02c6 X\nY\nt\nL\n\uf078\n\uf078\n\uf0ae\n\uf02d\ny\nPredicted time series of Y from time point tmin to time point L \u2013 \u03be using the trajectory\nmatrix of X considering time delay \u03be\n\u02c6\n\u02c6\nmin\n;\n,\n:\n\u02c6\nZX\nYZX\nZX\nYZX\nX\nZ\nY\nt\nL\n\uf078\n\uf078\n\uf078\n\uf078\n\uf0ae\n\uf0ae\n\uf02d\n\uf02d\ny\nPredicted time series of Y from time point tmin to time point\n\u02c6X\nZX\nYZ\nL\n\uf078\n\uf078\n\uf02d\n\uf02d\nusing the\npredicted trajectory matrix of Z considering time delay\n\u02c6X\nYZ\n\uf078\n, the predicted trajectory\nmatrix of Z is obtained using the trajectory matrix of X considering time delay\nZX\n\uf078\n2\n\u03a3\u03b3\nCovariance matrix of the stacked vector U\u03b3\n\u03a9\u03b3\nPrecision matrix corresponding to \u03a3\u03b3\nYX\n\u039e\nSet of local maxima in the TDCCM causal strength curve from Y to X\nYX\n\u03a0\nSet of local maxima in the TDPCM causal strength curve from Y to X\n\u03b3\nTime delay considered in TDPCM for evaluating direct causal strength\n\u03b3YX\nOptimal causal time delay from Y to X determined by TDPCM\n\u03be\nGeneric notation for time delay\n\u03beYX\nOptimal causal time delay from Y to X determined by TDCCM\nY\nX\n\uf072\uf0ae\nCausal strength from Y to X computed by CCM\n,\nY\nX \uf078\n\uf072\uf0ae\nTime-delayed causal strength from Y to X at delay \u03be calculated by TDCCM\nY\nX Z\n\uf072\uf0ae\nDirect causal strength from Y to X considering Z calculated by PCM\n,\nY\nX Z \uf067\n\uf072\uf0ae\nTime-delayed direct causal strength from Y to X considering Z at delay \u03b3 calculated\nby TDPCM\n,\nY\nX\n\uf067\n\uf072\uf0ae\nZ\nTime-delayed direct causal strength from Y to X considering the set Z at delay \u03b3\ncalculated by TDPCM\n\u03c4\nEmbedding time delay\n1\nIntroduction\nMonitoring, evaluating and optimizing industrial processes are crucial tasks. Generally, key\nperformance indicators (KPIs) such as product quality, energy consumption and pollutant emissions are\nrecorded continuously to reflect the state of the industrial process and provide guidance for process\ncontrol. However, as industrial processes become increasingly complex, the cost and difficulty of direct\nKPI measurement have risen, making it challenging for online monitoring KPIs [1]. Thanks to the rapid\ndevelopment of the industrial internet of things, collecting abundant sensor data of easily measurable\nauxiliary variables have become feasible. By establishing mathematical relationships between auxiliary\nvariables and KPIs, online KPI prediction can be achieved [2]. This technique is known as soft sensing\n(i.e., soft sensor modeling), and has been extensively adopted in practical industries [3].\nThere are two main approaches for developing soft sensor models: physics-based and data-driven\n[3]. The first approach is effective when the process physical mechanism is well-understood.\nNevertheless, this prerequisite is always difficult to satisfy in actual industrial scenarios. Consequently,\ndata-driven methods have emerged as vital alternatives. Representative data-driven approaches include\nstatistical models like partial least square (PLS) regression [4, 5], and machine learning models like\nrandom forest (RF) regression [6]. Moreover, recently developed deep learning methods have also been\napplied to soft sensor modeling, such as long short-term memory (LSTM) neural network [7],\nconvolutional neural network [8] and variational autoencoder [9]. Although data-driven methods have\nsignificantly boosted the development of soft sensor models, the stability of these models continues to\nbe a major concern for their practical application.\nFeature selection is one of the most effective strategies for ensuring the stability of soft sensor\n3\nmodels, because it not only helps reveal the underlying process mechanism but also improves model\nrobustness and reduces computational complexity. Existing feature selection approaches for industrial\nsoft sensing mostly fall into three categories:\n(1) Correlation-based methods, including techniques such as the Pearson correlation coefficient\n(PCC) [10] and grey relational analysis [11]. These methods identify auxiliary variables that are highly\ncorrelated with KPIs, and then select them as model inputs. However, correlation does not always imply\ncausation [12]. Variables that significantly affect each other may exhibit weak correlations due to time\ndelay, whereas highly correlated variables may lack causal relationship. Therefore, feature selection\nmethods based on correlation may fall short in accurately predicting KPIs.\n(2) Model training-based approaches, such as RF [13, 14], nonnegative garrote [15, 16] and\nattention mechanisms [17]. These methods automatically assign weights to input features during training.\nThe weights help the model emphasize relevant features and minimize the influence of less significant\nones. Nevertheless, these methods are highly dependent on the specific model architecture used, which\ncan result in inconsistent assessments of feature importance across different models. Furthermore, these\ntechniques may still capture correlations rather than uncovering causal relationships.\n(3) Causality-based methods, including mutual information (MI) [18-21] and conditional mutual\ninformation (CMI) [22, 23], Granger causality (GC) [24, 25] and transfer entropy (TE) [26, 27], have\nbeen employed for feature selection in industrial processes. However, these methods exhibit several\ncritical limitations when applied to industrial soft sensor modeling:\n\uf09f\nFirstly, MI and CMI do not incorporate time delay into causal inference. They quantify causal\ninfluence between variables at the same time point, and therefore cannot capture causal\nrelationships that manifest with time delays. This omission is problematic because industrial\nvariables commonly exhibit delayed interactions due to control feedback loops, material\ntransport or dynamic process responses [28, 29]. As a result, MI and CMI cannot identify\ncausal features with meaningful time delays.\n\uf09f\nSecondly, although GC and TE explicitly consider time delays in causal inference, they still\nsuffer from two major limitations. On one hand, they rely on the decorrelation assumption,\nwhich requires that the influence of a causal variable on the target can be isolated by\nconditioning on other variables [30]. Real industrial processes violate this assumption because\nprocess variables are inherently interdependent [31]. Therefore, GC and TE may become\nunreliable in industrial processes. On the other hand, GC and TE cannot characterize how\ncausal strength varies across different time delays. Both methods provide a single causal\ndependence result, making it impossible to determine the specific lag at which the causal effect\nis strongest or how the causal contribution evolves with increasing delay.\nThese issues highlight a gap between causal inference and practical feature selection in industrial\nprocesses and point to the need for a method that can quantify causal influence across multiple time\ndelays and work reliably in interdependent industrial processes. To address this gap, this study proposes\na causal feature selection framework tailored for industrial soft sensor modeling. The framework is\nsupported by two time\u2010delayed cross mapping methods: time\u2010delayed convergent cross mapping\n(TDCCM) and time\u2010delayed partial cross mapping (TDPCM), designed for inferring total and direct\n4\ncausality for interdependent time series across multiple time delays. Within these two methods, state-\nspace reconstruction is employed to handle variable interdependence. By quantifying causal strength\nover lag dimensions, the framework enables the selection of informative variables and their time delays.\nMoreover, an objective feature selection strategy is developed, where the causal index threshold is\nautomatically determined using validation performance rather than empirical rules.\nThe contributions of this work are summarized as follows:\n\uf09f\nTDCCM and TDPCM are introduced for inference of total and direct causality between\ninterdependent industrial variables across multiple time delays.\n\uf09f\nA time\u2010delayed causal feature selection framework tailored for industrial soft sensor modeling\nis proposed.\n\uf09f\nAn objective feature selection strategy based on the results of time-delayed causal inference\ntechniques is presented involving causal threshold optimization.\nNotably, the integration of causal inference techniques and graph neural networks is also a notable\ntrend in the field of soft sensor model development [32-36]. This paper, however, focuses on feature\nselection using causal inference techniques, distinguishing it from the objective of graph neural networks,\nwhich is to further exploit spatial information from selected features based on causal graphs.\nThe organization of the paper is as follows. The preliminaries of convergent cross mapping and\npartial cross mapping are introduced in Section 2. Next, the time\u2010delayed causal feature selection\nframework based on TDCCM and TDPCM is developed in Section 3. After that, the effectiveness of the\nproposed method in soft sensor modeling is verified by two engineering cases in Section 4. Subsequently,\nthe effectiveness of the proposed causal inference method is demonstrated by numerical cases in Section\n5. Finally, Section 6 concludes the work.\n2\nPreliminaries\n2.1 Convergent Cross Mapping\nThe convergent cross mapping (CCM) method is grounded in the theory of state-space\nreconstruction, which stems from Takens\u2019 theorem [37]. This theorem posits that a time series can be\nembedded into a higher-dimensional space to reconstruct its dynamics. Considering two time series\n\uf028\uf029\n\uf07b\n\uf07d\n1\nL\nt\nX\nx t\n\uf03d\n\uf03d\nand\n\uf028\uf029\n\uf07b\n\uf07d\n1\nL\nt\nY\ny t\n\uf03d\n\uf03d\nwith length L, their state-space reconstruction is given by:\n\uf028\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n,\n,\n,\n,\n2\n,\n,\n1\n,\n,\n2\n,\n,\n1\nX l\nY l\nx l\nx l\nx l\nx l\nE\ny l\ny l\ny l\ny l\nE\n\uf074\n\uf074\n\uf074\n\uf074\n\uf074\n\uf074\n\uf0e9\n\uf0f9\n\uf03d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf0eb\n\uf0fb\n\uf0e9\n\uf0f9\n\uf03d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf0eb\n\uf0fb\nM\nM\n(1)\nwhere\n,\nX l\nM\nand\n,\nY l\nM\nare the embedding vectors of X and Y in the reconstructed space at the time point\nl; \u03c4 is the time delay of embedding; and E is the embedding dimension. For the time series with length L,\nl ranges from\n\uf028\n\uf029\n1\n1\nE\n\uf074\n\uf02b\n\uf02d\nto L, and totally\n\uf028\n\uf0291\nL\nE\n\uf074\n\uf02d\n\uf02d\nembedding vectors can be constructed from the\ntime series. Then, the trajectory matrix of X and Y can be represented as:\n5\n\uf028\n\uf029\nmin\nmin\nmin\nmin\n,\n,\n,\n1\n,\n1\nmin\n,\n,\n,\n,\n1\n1\nX t\nY t\nX t\nY t\nX\nY\nX L\nY L\nt\nE\n\uf074\n\uf02b\n\uf02b\n\uf0e9\n\uf0f9\n\uf0e9\n\uf0f9\n\uf0ea\n\uf0fa\n\uf0ea\n\uf0fa\n\uf0ea\n\uf0fa\n\uf0ea\n\uf0fa\n\uf03d\n\uf03d\n\uf03d\uf02b\n\uf02d\n\uf0ea\n\uf0fa\n\uf0ea\n\uf0fa\n\uf0ea\n\uf0fa\n\uf0ea\n\uf0fa\n\uf0ea\n\uf0fa\n\uf0ea\n\uf0fa\n\uf0eb\n\uf0fb\n\uf0eb\n\uf0fb\nM\nM\nM\nM\nM\nM\nM\nM\n(2)\nThe trajectory matrix represents a set of sampled points constructed from monitored time series data.\nAll the sampled points are on a manifold, which is a continuous structure in the reconstructed space that\ncaptures the time-varying states of the time series. According to the theory of CCM [30], if X is causally\ninfluenced by Y, then the manifold of X contains information that can reconstruct the dynamics of Y. At\na specific time point l, the embedding vector of Y can be predicted via a weighted approximation:\n1\n,\n,\n1\n\u02c6\nl\ni\nE\nX\nY\nY l\ni\nY t\ni\nw\n\uf02b\n\uf0ae\n\uf03d\n\uf03d\uf0e5\nM\nM\n(3)\nwhere\niw  represents a weight determined by the distance between\n,\nX l\nM\nand its ith nearest neighbor\nembedding vector with corresponding time point\nl\nit  ; and\n, l\ni\nY t\nM\nrepresents the contemporaneous\nembedding vector of Y at the time point\nl\nit . The weights are calculated by [30]:\n1\n1\n/\nE\ni\ni\nj\nj\nw\nv\nv\n\uf02b\n\uf03d\n\uf03d\n\uf0e5\n(4)\n\uf028\n\uf029\n\uf028\n\uf029\n1\n,\n,\n,\n,\nd\n,\nexp\nd\n,\nl\ni\nl\nX l\nX t\ni\nX l\nX t\nv\n\uf0e6\n\uf0f6\n\uf0e7\n\uf0f7\n\uf03d\n\uf02d\n\uf0e7\n\uf0f7\n\uf0e7\n\uf0f7\n\uf0e8\n\uf0f8\nM\nM\nM\nM\n(5)\nwhere \uf028\uf029\nd ,\uf0d7\uf0d7 is the Euclidean distance between two embedding vectors; and\n1\n, l\nX t\nM\nrepresents the nearest\nembedding vector to\n,\nX l\nM\nacross all time points.\nSubsequently, the scale prediction of y(l) can be obtained by taking the first component of the\npredicted embedding vector:\n\uf028\uf029\uf028\n\uf029\n,\n1\n\u02c6\n\u02c6\nX\nY\nX\nY\nY l\ny\nl\n\uf0ae\n\uf0ae\n\uf03dM\n(6)\nBy performing this reconstruction for all eligible time points, the predicted time series of Y is\nexpressed as:\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\uf029\nmin\nT\n:\nmin\nmin\n\u02c6\n\u02c6\n\u02c6\n\u02c6\n,\n1 ,\n,\nX\nY\nt\nL\nX\nY\nX\nY\nX\nY\ny\nt\ny\nt\ny\nL\n\uf0ae\n\uf0ae\n\uf0ae\n\uf0ae\n\uf03d\n\uf02b\n\uf0e9\n\uf0f9\n\uf0eb\n\uf0fb\ny\n(7)\nwhere\nmin :\n\u02c6 X\nY\nt\nL\n\uf0ae\ny\nrepresents the predicted time series of Y from the time point tmin to the time point L using\nthe trajectory matrix of X.\nFor clarity, we write the time series X and Y in vector form as:\n\uf028\uf029\n\uf028\uf029\n\uf028\uf029\n\uf028\uf029\n\uf028\uf029\n\uf028\uf029\nT\nT\n1:\n1:\n1 ,\n2 ,\n,\n,\n1 ,\n2 ,\n,\nL\nL\nx\nx\nx L\ny\ny\ny L\n\uf03d\n\uf03d\n\uf0e9\n\uf0f9\n\uf0e9\n\uf0f9\n\uf0eb\n\uf0fb\n\uf0eb\n\uf0fb\nx\ny\n(8)\nThen, the causal strength from Y to X is quantified by comparing the predicted and actual values\nthrough the Pearson correlation coefficient [30]:\n\uf028\n\uf029\nmin\nmin\n:\n:\n\u02c6\npcc\n,\nX\nY\nY\nX\nt\nL\nt\nL\n\uf072\n\uf0ae\n\uf0ae\n\uf03d\ny\ny\n(9)\nwhere pcc(e1, e2) denotes the PCC calculation; and\nY\nX\n\uf072\uf0ae denotes the total causal strength from Y to X\ncalculated by CCM. As the time series length L approaches infinity,\nY\nX\n\uf072\uf0ae will converge to a specific\nvalue. If the convergent value exceeds the predefined threshold c, it means that Y has a causal effect on\nX and vice versa. Similarly, the dynamic of X can be predicted based on the embedding vectors of Y to\n6\ndetect the causal influence of X on Y.\n2.2 Time-Delayed Convergent Cross Mapping\nSuppose that there is a time delay \u03be in the causal relationship from Y to X. At this point,\n,x l\nM\nis\nhighly capable of predicting \uf028\n\uf029\ny l\n\uf078\n\uf02d\ntheoretically [38], and the embedding vector of Y can be predicted\nas:\n1\n,\n,\n,\n1\n\u02c6\nl\ni\nE\nX\nY\nY l\ni\nY t\ni\nw\n\uf078\n\uf078\n\uf02b\n\uf0ae\n\uf02d\n\uf03d\n\uf03d\uf0e5\nM\nM\n(10)\nwhere the definitions of the parameters are consistent with those in Eq. (3). Owing to the time delay,\nthere are\n\uf028\n\uf0291\nL\nE\n\uf078\n\uf074\n\uf02d\n\uf02d\n\uf02d\npredictable time points for Y. The corresponding scalar prediction is obtained\nby taking the first component of the vector:\n\uf028\n\uf029\uf028\n\uf029\n,\n,\n,\n1\n\u02c6\n\u02c6\nX\nY\nX\nY\nY l\ny\nl\n\uf078\n\uf078\n\uf078\n\uf078\n\uf0ae\n\uf0ae\n\uf02d\n\uf02d\n\uf03dM\n(11)\nSubsequently, after mapping the embedding vectors of X onto those of Y for all eligible time points,\nthe predicted time series of Y is expressed as:\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\nmin\nT\n,\n:\n,\nmin\n,\nmin\n,\n\u02c6\n\u02c6\n\u02c6\n\u02c6\n,\n1 ,\n,\nX\nY\nt\nL\nX\nY\nX\nY\nX\nY\ny\nt\ny\nt\ny\nL\n\uf078\n\uf078\n\uf078\n\uf078\n\uf078\n\uf078\n\uf0ae\n\uf02d\n\uf0ae\n\uf0ae\n\uf0ae\n\uf0e9\n\uf0f9\n\uf03d\n\uf02b\n\uf02d\n\uf0eb\n\uf0fb\ny\n(12)\nThen, by measuring the correlation between\nmin\n,\n:\n\u02c6 X\nY\nt\nL\n\uf078\n\uf078\n\uf0ae\n\uf02d\ny\nand\nmin:\nt\nL \uf078\n\uf02d\ny\n, the causal strength of Y on X\nconsidering a time delay \u03be can be quantified. This is defined as time-delayed CCM (TDCCM) [38], which\ncan be expressed as:\n\uf028\n\uf029\nmin\nmin\n,\n,\n:\n:\n\u02c6\npcc\n,\nX\nY\nY\nX\nt\nL\nt\nL\n\uf078\n\uf078\n\uf078\n\uf078\n\uf072\n\uf0ae\n\uf0ae\n\uf02d\n\uf02d\n\uf03d\ny\ny\n(13)\nwhere\n,\nY\nX \uf078\n\uf072\uf0ae\ndenotes the total causal strength from Y to X at time delay \u03be calculated by TDCCM. When\n\u03be = 0, TDCCM corresponds to CCM, and Eq. (13) is the same as Eq. (9).\n2.3 Partial Cross Mapping\nWhen the system has only two variables, their causal relationship is clearly direct. However, in a\ncomplex system with numerous variables, there can be two types of causation: direct and indirect, as\nillustrated in Fig. 1.\nFig. 1  Schematics of direct and indirect causal links. (a) Y directly causes X; (b) Y indirectly causes X.\nTo distinguish direct causations from indirect ones, Leng et al. [39] presented partial cross mapping\n(PCM) based on the framework of CCM. Considering a potential disturbing time series\n\uf028\uf029\n\uf07b\n\uf07d\n1\nL\nt\nZ\nz t\n\uf03d\n\uf03d\n,\nPCM removes the influence of Z when investigating the causal effect of Y on X. First, the optimal time\ndelay of the causal influence from Z to X is determined by examining all possible time delays between\nZ\nX\nY\nZ\nX\nY\na\nb\nDirect causal link\nIndirect causal link\n7\nvariables via Eq. (13) and selecting the time delay that exhibits the highest causal strength [39], which\ncan be expressed as:\n\uf028\n\uf029\n,\n0\narg max\nZX\nZ\nX \uf078\n\uf078\n\uf078\n\uf072\n\uf0ae\n\uf0b3\n\uf03d\n(14)\n, and the corresponding predicted time series\nmin\n,\n:\n\u02c6\nZX\nZX\nX\nZ\nt\nL\n\uf078\n\uf078\n\uf0ae\n\uf02d\nz\ncan be obtained via Eq. (12), denoted as \u02c6\nX\nZ .\nThen, the optimal time delay of the influence from Y to \u02c6\nX\nZ  is obtained similarly by:\n\uf028\n\uf029\n\u02c6\n\u02c6\n,\n0\narg max\nX\nX\nYZ\nY\nZ\n\uf078\n\uf078\n\uf078\n\uf072\uf0ae\n\uf0b3\n\uf03d\n(15)\n, and the corresponding predicted time series\n\u02c6\n\u02c6\nmin\n;\n,\n:\n\u02c6\nZX\nYZX\nZX\nYZX\nX\nZ\nY\nt\nL\n\uf078\n\uf078\n\uf078\n\uf078\n\uf0ae\n\uf0ae\n\uf02d\n\uf02d\ny\ncan be derived using Eq. (12). If time series\nY exhibits a high similarity with\n\u02c6\n\u02c6\nmin\n;\n,\n:\n\u02c6\nZX\nYZX\nZX\nYZX\nX\nZ\nY\nt\nL\n\uf078\n\uf078\n\uf078\n\uf078\n\uf0ae\n\uf0ae\n\uf02d\n\uf02d\ny\n, it suggests a causal influence along Y\u2192Z\u2192X. Following\nthis, Eq. (9) can be extended as:\n\uf028\n\uf029\n\u02c6\n\u02c6\n\u02c6\n\u02c6\nmin\nmin\nmin\n;\n,\n,\n:\n:\n:\n\u02c6\n\u02c6\nppcc\n,\nZX\nYZ\nYX\nX\nZX\nZX\nZX\nYZ\nYZ\nYZ\nX\nX\nX\nX\nZ\nY\nX\nY\nt\nL\nt\nL\nt\nL\nY\nX Z\n\uf078\n\uf078\n\uf078\n\uf078\n\uf078\n\uf078\n\uf078\n\uf078\n\uf078\n\uf072\n\uf0ae\n\uf0ae\n\uf0ae\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf0ae\n\uf03d\ny\ny\ny\n(16)\nwhere\n\uf028\n\uf029\n1\n2\n3\nppcc\n,\ne e e\ndenotes the partial PCC calculation quantifying the correlation between two\nvectors\n1e  and\n2e  when considering the vector\n3e  ;\nYX\n\uf078\ndenotes the optimal time delay of the causal\ninfluence from Y to X; and\nY\nX Z\n\uf072\uf0ae\ndenotes the direct causal strength from Y to X considering Z calculated\nby PCM. If the value\nY\nX Z\n\uf072\uf0ae\nexceeds the predefined threshold c, it means that Y has a direct causal effect\non X considering Z.\n2.4 Problem Formulation\nConsider a KPI that needs to be predicted, denoted as X. The monitored sensor data includes M\nauxiliary variables Y1, Y2, \u2026 YM, each of which records historical measurements over N time points.\nSpecifically, for the KPI at time l, denoted as \uf028\uf029\nx l , it can be predicted by:\n\uf028\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n1\n1\n1\n1\n2\n...\n\u02c6\n...\n...\n...\n...\n1\n2\n...\nM\nM\nM\ny l\ny l\ny l\nN\nx l\nf\ny\nl\ny\nl\ny\nl\nN\n\uf0e6\n\uf0f6\n\uf02d\n\uf02d\n\uf02d\n\uf0e9\n\uf0f9\n\uf0e7\n\uf0f7\n\uf0ea\n\uf0fa\n\uf03d\n\uf0e7\n\uf0f7\n\uf0ea\n\uf0fa\n\uf0e7\n\uf0f7\n\uf0ea\n\uf0fa\n\uf02d\n\uf02d\n\uf02d\n\uf0eb\n\uf0fb\n\uf0e8\n\uf0f8\n(17)\nwhere\n\uf028\uf029\niy\nj  represents the measurement of the ith auxiliary variable at time point j; and \uf028\uf029\n\u02c6x j  represents\nthe prediction of the KPI at time point j. The aim of soft sensor modeling is to determine the mapping\nrelationship f between the auxiliary variables and the KPI. This paper focuses on the problem of feature\nselection as illustrated in Fig. 2, aiming to select features from the M\u00d7N feature set, which involves\nmeasurements of different variables at various time points, to improve the performance and stability of\nthe soft sensor model.\nNotably, some existing studies used historical measurements of the KPI as input features during the\noperation of industrial processes [40, 41]. However, in this study, we consider the most challenging\nscenario, where the KPI is entirely unobservable during operations, and is only observable in the\nlaboratory for soft sensor model establishment. This is also the most common situation in practice.\n8\nFig. 2  Schematic diagram of the feature selection results.\n3\nMethodology\n3.1 Motivation\nThis study aims to perform feature selection on auxiliary variable measurements recorded at\nmultiple historical time points for soft sensor modeling. Quantifying the causal strength of each auxiliary\nvariable at various time delays with respect to the current value of the KPI can provide valuable guidance\nfor feature selection. However, as discussed in the introduction, existing causality-based feature selection\nmethods investigate causal relationships in a same time dimension. Besides, variables in industrial\nprocesses are often interdependent, which contradicts the decorrelation assumption of traditional causal\ninference methods. To address these issues, we propose causal feature selection framework in this study\nbased on time-delayed cross mapping.\nFor existing techniques, although TDCCM quantifies causal strengths across different time\ndimensions, it fails to distinguish direct causal relationships between variables. Consequently, TDCCM\nmay introduce redundant features or ignore significant features, thereby impairing the stability of the soft\nsensor model. While PCM offers a way to quantify direct causality, it focuses only on the maximum\ncausal strength across time delays, rather than the causal strength at specific lags, which limits its\neffectiveness for guiding feature selection in the time dimension. Therefore, in this section, we first\npropose TDPCM to infer direct causal relationships at different time delays within the state space\nreconstruction framework. Then, to achieve automatic causal feature selection, an objective selection\nstrategy is proposed involving causal threshold optimization based on the model performance on the\nvalidation set. An outline of the proposed method is illustrated in Fig. 3."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary\n\n#### Main Contributions\nThis paper proposes a novel causal feature selection framework for soft sensor modeling, addressing critical challenges faced by existing methods. The main contributions include:\n1. **Time-Delayed Cross Mapping (TDCCM) and Time-Delayed Partial Cross Mapping (TDPCM)**: These techniques are introduced to infer total causality between interdependent variables across multiple time delays.\n2. **Tailored Framework for Industrial Soft Sensor Modeling**: A causal feature selection framework specifically designed for industrial soft sensor modeling is proposed, leveraging TDCCM and TDPCM methods.\n3. **Objective Feature Selection Strategy**: An objective feature selection strategy based on the results of time-delayed causal inference techniques is presented, involving causal threshold optimization using validation performance.\n\n#### Methodology\nThe methodology employed in this study includes:\n1. **State Space Reconstruction**: Employed to handle interdependent variables and effectively deal with temporal dependencies.\n2. **Time-Delayed Convergent Cross Mapping (TDCCM)**: Introduced for total causality inference, providing a comprehensive view of causal relationships across multiple time delays.\n3. **Time-Delayed Partial Cross Mapping (TDPCM)**: Developed for direct causality inference, enabling the selection of informative features and their specific lags.\n\n#### Key Findings\n1. **Improved Accuracy**: The proposed framework achieved higher average performance compared to existing feature selection methods in two real-world case studies.\n2. **Enhanced Stability**: In the worst-case scenario, TDPCM further reduced Root Mean Square Error (RMSE) by approximately 7.69% relative to Time-Delayed Convergent Cross Mapping (TDCCM).\n3. **Effective Feature Selection**: Features selected via direct causality enhanced model stability and performance.\n\n#### Conclusions\nThe proposed causal feature selection framework based on time-delayed cross mapping significantly improves the accuracy, stability, and interpretability of soft sensor models in industrial applications. The method effectively addresses the challenges posed by interdependent variables and varying causal strengths across different time delays, thereby providing a robust solution for feature selection in soft sensor modeling.\n\n### Detailed Breakdown\n\n#### Introduction\nThe paper highlights the importance of soft sensor modeling in process monitoring and control, particularly in scenarios where direct measurements are costly or impractical. Existing methods often fall short due to limitations such as ignoring temporal dependencies and interdependence among variables. The study introduces a novel framework that leverages time-delayed cross mapping techniques for more accurate and stable feature selection.\n\n#### Methodology\n1. **State Space Reconstruction**: This technique is used to reconstruct the dynamics of interdependent variables, facilitating causal analysis across multiple time delays.\n2. **TDCCM (Time-Delayed Convergent Cross Mapping)**: Introduced to quantify total causality between variables at different lags. It provides a comprehensive view by considering all possible time delays.\n3. **TDPCM (Time-Delayed Partial Cross Mapping)**: Developed specifically for direct causality inference, enabling the selection of features and their specific causal strengths.\n\n#### Key Findings\n1. **Improved Performance**: The proposed framework demonstrated superior performance compared to existing methods in two real-world case studies, achieving higher average accuracy.\n2. **Enhanced Stability**: In challenging scenarios, TDPCM further improved model stability by reducing RMSE by approximately 7.69% relative to TDCCM.\n\n#### Conclusions\nThe study concludes that the proposed causal feature selection framework based on time-delayed cross mapping offers a robust solution for industrial soft sensor modeling. By addressing critical challenges such as temporal dependencies and interdependence among variables, the framework ensures more accurate and stable predictions, enhancing the practical applicability of soft sensor models in industrial settings.\n\n### Summary\nThis paper introduces a novel causal feature selection framework that leverages time-delayed cross mapping techniques to address key limitations faced by existing methods. The proposed framework is specifically tailored for industrial soft sensor modeling, demonstrating superior performance and stability compared to traditional approaches."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "BACH-V: Bridging Abstract and Concrete Human-Values\nin Large Language Models\nJunyu Zhang2,1, Yipeng Kang1, Jiong Guo2, Jiayu Zhan3,1,B, Junqi Wang1,B\n1State Key Laboratory of General Artificial Intellligence, BIGAI\n2Shandong University, 3Peking University\nCorrespondence: jiayu.zhan@pku.edu.cn, wangjunqi@bigai.ai\nAbstract\nDo large language models (LLMs) genuinely\nunderstand abstract concepts, or merely manip-\nulate them as statistical patterns? We introduce\nan abstraction-grounding framework that de-\ncomposes conceptual understanding into three\ncapacities: interpretation of abstract concepts\n(Abstract-Abstract, A-A), grounding of abstrac-\ntions in concrete events (Abstract-Concrete, A-\nC), and application of abstract principles to reg-\nulate concrete decisions (Concrete-Concrete,\nC-C). Using human values as a testbed\u2014given\ntheir semantic richness and centrality to align-\nment\u2014we employ probing (detecting value\ntraces in internal activations) and steering\n(modifying representations to shift behavior).\nAcross six open-source LLMs and ten value\ndimensions, probing shows that diagnostic\nprobes trained solely on abstract value descrip-\ntions reliably detect the same values in con-\ncrete event narratives and decision reasoning,\ndemonstrating cross-level transfer. Steering re-\nveals an asymmetry: intervening on value rep-\nresentations causally shifts concrete judgments\nand decisions (A-C, C-C), yet leaves abstract\ninterpretations unchanged (A-A), suggesting\nthat encoded abstract values function as sta-\nble anchors rather than malleable activations.\nThese findings indicate LLMs maintain struc-\ntured value representations that bridge abstrac-\ntion and action, providing a mechanistic and op-\nerational foundation for building value-driven\nautonomous AI systems with more transparent,\ngeneralizable alignment and control.\n1\nIntroduction\nThe capacity to reason with abstract concepts (e.g.,\njustice, curiosity, individualism) lies at the heart of\nhuman cognition (Reinboth and Farka\u0161, 2022). Un-\nlike concrete objects that can be directly perceived,\nabstract concepts derive meaning through relation-\nships with other concepts and their grounding in di-\nverse experiential instances. This capacity enables\nAbstract Concepts\nDo you Understand the Concept Individualism?\nAnswer: Individualism can drive personal ...\nToken-\nwise\nActivations\n...\n...\n...\n...\n...\n...\nInternal\nExternal\nPattern Recognition:\nTrain Abstract Probes Using\nAbstract Concept Activations\nBehavior Manipulation:\nAbstract\nIndividualism\nProbe\nUse Abstract Probe's Weight\nSteering Concrete Decisions\nConcrete Decisions\nWhat Would You Do in the situation and Why?\nAnswer:\nI would go HIKING WITH MY FRIENDS\n... shows reliability and respect for others\u2019 time ...\nand that relationships are important ...\nToken-\nwise\nActivations\n...\nAnswer:\nI would ATTEND THE WORKSHOP\nto advance my professional skills ... a unique\nopportunity for personal growth ... for success ...\nToken-\nwise\nActivations\n...\n+\n-\nConcrete - Abstract\nGrounding\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nFigure 1:\nConceptual overview of abstract-concrete\ngrounding. Value probes trained on abstract concept descrip-\ntions (top) capture internal representations that generalize to\nsteering concrete decisions (bottom), enabling behavioral con-\ntrol via recognized abstract patterns.\nhumans to transfer knowledge across domains,\nmake value-laden decisions in novel situations, and\ncommunicate complex intentions efficiently. As\nlarge language models (LLMs) (Vaswani et al.,\n2017) increasingly serve as autonomous agents in\nopen-ended environments, a fundamental question\nemerges: Do these models merely manipulate ab-\nstract terms as statistical patterns, or do they de-\n1\narXiv:2601.14007v1  [cs.CL]  20 Jan 2026\nvelop genuine conceptual understanding that can\nguide coherent, principled behavior?(Regneri et al.,\n2024; Xiong et al., 2024; Gendron et al., 2024).\nThis question is critical for both AI alignment\nand autonomous agency (Askell et al., 2021; Ngo\net al., 2022). Existing alignment techniques, such\nas reinforcement learning from human feedback\n(RLHF) (Ouyang et al., 2022) and Constitutional\nAI (Bai et al., 2022), primarily operate at the be-\nhavioral level, shaping model outputs to match hu-\nman preferences. However, such approaches do not\nguarantee that models internalize the underlying ab-\nstract principles. As a result, aligned behavior may\nbe brittle, failing to generalize under distributional\nshift or in novel ethical dilemmas (Ji et al., 2025).\nMore broadly, agents operating in open-ended en-\nvironments cannot rely on exhaustive behavioral\ntraining; they must generalize from abstract prin-\nciples to guide action (Qi et al., 2024). Without\nrobust abstract concept representations, such gen-\neralization remains fundamentally limited.\nIn this work, we introduce an abstraction-\ngrounding framework to operationalize and eval-\nuate conceptual understanding in LLMs. Under-\nstanding are characterized as three hierarchical ca-\npacities. Abstract-to-Abstract interaction (A-A)\nassesses whether a model can interpret and relate\nabstract concepts at semantic level. Abstract-to-\nConcrete interaction (A-C) evaluates whether ab-\nstract concepts can be grounded in concrete in-\nstances. Concrete-to-Concrete interaction (C-C)\ntests whether internalized abstract principles can\nconsistently regulate behavior across diverse sit-\nuations. Together, these three capacities form a\nhierarchy from conceptual interpretation (A-A),\nto grounding (A-C), to action(C-C) (Figure 1),\nenabling a principled distinction between genuine\nunderstanding and superficial pattern matching.\nWe apply our framework to address whether\nLLMs possess genuine understanding of human val-\nues. (Schwartz, 2012) Values provide a particularly\nideal testbed for abstract concept understanding:\nthey function as meta-level organizing principles\nin decision-making, bridge descriptive knowledge\nand prescriptive action, and involve inherent trade-\noffs that require contextual reasoning. Moreover,\nvalue alignment lies at the core of AI safety, mak-\ning value understanding a critical capability for\ntrustworthy autonomous systems. (Gabriel, 2020)\nUsing the abstraction-grounding framework, we\nmake four contributions: (1) A concept prob-\ning toolkit that assesses LLMs\u2019 internal repre-\nsentations, demonstrating that distinct value con-\ncepts produce distinguishable neural signatures\n(establishing existence); (2) A behavior steering\npipeline that modifies internal representations to\nproduce predictable, controllable behavioral shifts\n(establishing causality); (3) A systematic evalua-\ntion system that tests interpretation, instantiation,\nand application capacities, revealing that LLMs ex-\nhibit genuine value understanding enabling coher-\nent, preference-consistent decisions in novel con-\ntexts without explicit guidance (establishing trans-\nfer); and thus (4) an operational foundation for\nvalue-driven autonomous AI that shapes LLMs\u2019\nreasoning and decision-making via learned value\nrepresentations.\n2\nThe Abstract-Concrete Grounding\nNatural language simultaneously supports abstract\nconceptualization (e.g., equality, freedom) and con-\ncrete situated description (e.g., dog, walk). For\nhumans, abstract concepts are not merely defini-\ntional symbols: they connect to families of experi-\nences, inferential expectations, and action policies.\nFor LLMs, however, the same surface competence\ncan arise from pattern completion without robust\ninternal grounding.\nThis asymmetry reveals a critical insight: under-\nstanding is not monolithic. A model may encode\ncoherent relationships among abstract concepts yet\nfail to ground them in concrete instances; it may\nrecognize concrete instances yet fail to deploy the\nconcept to regulate action. These are qualitatively\ndistinct capacities, and conflating them obscures\nwhere understanding succeeds or fails.\nWe formalize this insight through three interac-\ntion regimes, ranging from pure interpretation to\nsituated action:\nAbstract\u2013Abstract (A-A)\naddresses whether\nthe model maintains coherent semantic representa-\ntions of abstract concepts in isolation. This requires\nprobing whether distinct abstract concepts produce\ndistinguishable internal signatures. For example,\nwhen processing definitions such as \u201cCuriosity is a\nstrong desire to know or learn something,\u201d a model\nwith genuine AA capacity should encode \u201ccurios-\nity\u201d distinctly from \u201cobedience\u201d or \u201cdiscipline\u201d.\nMoreover, it should represent \u201ccuriosity\u201d as seman-\ntically closer to \u201copenness\u201d than to \u201cconformity\u201d.\nAbstract\u2013Concrete (A-C)\naddresses the bridge\nbetween abstraction and instantiation.\nThis re-\n2\nActive Steering\nPassive Probing\nAbstract\nConcept\nUnderstanding\nCorpora\n+\nLayer i\nHooked\nActivation\nAttention\nUnEmbed\nEmbed\nMLP\nProbe Training\n\u031c\u03f7\n( Fitted )\nToken-wise Score\n( Observed Y )\n0\n6\n+\nProbe\nTesting\nToken-wise Score\n( Predicted )\n0\n6\nAveraged\nScore\n+\nLayer i\nHooked\nActivation\nAttention\nUnEmbed\nEmbed\nMLP\n+\nValue Probes\nAverage Scores\nProbing\nCorpora I\nCorpora J\nCorpora K\nProbe I\nProbe J\nProbe K\nTest Corpora\nProbe J\nTrained Probe:\n+\nLayer i\nHooked\nActivation\nAttention\nUnEmbed\nEmbed\nMLP\n+\nCorpora\nOriginal\nSteering\nSteered\nTest Result:\n+\nLayer i\nHooked\nActivation\nAttention\nUnEmbed\nEmbed\nMLP\n+\nCorpora\nB.1\nB.2\nA.1\nA.2\nA.3\n\u03e6 = \ud445\ud452\u03d9\u03e2 \u031c\u03f7 \u22c5\u03e5 + \u03e9\n\u031c\u03f7\n\u03e5\n\u03e5\n\u03e5\n\u03e5\n\u03e5 + \u030e \u22c5\u031c\u03f7\n\u03e5 + \u030e \u22c5\u031c\u03f7\nChoice Boundary\nProbe J\nProbe J\nThis[0] _sentence[0] _suggests[0]\n_that[0] _courage[6]  _enables[2]\n_individuals[1]  _to[0]   _stay[2]\n_focused[3]  ...\nScores Annotated by GPT-4o:\nScores Predicted by Probes:\n... _our[0.1]  _perception[0.2]  _of[0.0]\n_courage[5.9] _changes[1.8] ... _that[0.1]\n_bravery[5.1] _can[1.5] ... _dramatic[4.3]\n,[3.8]  _heroic[4.8]  _acts[1.1] ...\nUnknown\nValue J Relevant\nBehavior\nChoice Probability:\n\u00d7\nOutput Error\nValue J Irrelevant\nBehavior\nSteered Result:\nB.3\nFigure 2: A 3D-visualization of passive probing [A.1 - A.3] and active steering pipelines for value probes [B.1 - B.3].\nquires examining whether abstract value represen-\ntations can be grounded to concrete event descrip-\ntions. For example, a model with genuine AC ca-\npacity should recognize the presence of \u201ccuriosity\u201d\nin a narrative such as \u201cThe children discovered\na hidden diary in the attic and spent hours deci-\nphering its faded entries\u201d\u2014even though the word\n\u201ccuriosity\u201d never appears.\nConcrete\u2013Concrete (C-C)\naddresses whether\nabstract values can function as decision constraints.\nThis requires testing whether holding an abstract\nprinciple in mind can regulate choices among con-\ncrete alternatives. For example, a model with gen-\nuine CC capacity, when primed with a curiosity\nvalue and faced with options like \u201csearch for hid-\nden treasure\u201d versus \u201creturn home safely,\u201d should\nexhibit systematic preference shifts consistent with\nthe primed value.\nTogether, these three regimes form a diagnos-\ntic hierarchy: AA establishes conceptual existence,\nAC establishes experiential grounding, and CC es-\ntablishes behavioral efficacy. A model may succeed\nat one level while failing at another, and this de-\ncomposition enables precise diagnosis of where un-\nderstanding breaks down. Section 4 operationalizes\neach regime through corresponding experimental\nparadigms.\n3\nMethod\nOur framework executes the three-level abstraction-\ngrounding interactions through two complementary\napproaches: Passive Probing, which tests whether\nvalue concepts leave identifiable traces in LLM\ninternal states (establishing existence), and Ac-\ntive Steering, which tests whether modifying these\nrepresentations produces predictable behavioural\nchanges (establishing causal efficacy).\nBoth methods target the standard transformer\narchitecture. Each transformer block (a layer) con-\nsists of a multi-head self-attention (MHA) module\nand a multi-layer perceptron (MLP) module, resid-\nually connected. We extract activations from the\nMLP output at each layer, as this position captures\npost-attention integrated representations.\n3.1\nValue Concept Dataset\nWe employed GPT-4o to generate and annotate\na value concept dataset comprising 10 value di-\nmensions synthesized from hierarchical value tax-\nonomies: patriotism (Pat.), equality (Equ.), in-\ntegrity (Int.), cooperation (Coo.), individualism\n(Ind.), discipline (Dis.), curiosity (Cur.), courage\n(Cou.), satiety (Sat.)\nand rest (Res.), see Sec-\ntion B.1,B.2 for generation and Section B.3 for\nscoring.\nDataset Generation.\nFor each value dimension,\nwe follow a two-step procedure. In step 1, GPT-4o\ngenerated 800 sentences: 400 strictly relevant to\n3\nthe target value, and the remaining 400 irrelevant\n(see Appendix B.1 for examples). In step 2, we\nprompted GPT-4o to elaborate its understanding\nof each step-1 sentence, yielding 800 interpreta-\ntions (description within 80 words) that capture\nabstract-level value semantics (see Appendix B.2\nfor examples). These step-2 interpretations consti-\ntute our value concept dataset.\nDataset Annotation.\nEach token in the value\nconcept dataset was scored by GPT-4o on a 7-point\nscale from completely irrelevant (0) to highly rele-\nvant (6). We allocated 90% for probe training and\n10% for probe testing.\n3.2\nPassive Probing\nTheoretical Foundation.\nWe build on the su-\nperposition hypothesis: internal representations in\nLLMs are approximately orthogonal superpositions\nof feature vectors, where each direction encodes\na semantic meaning and each magnitude encodes\nactivation strength. Formally, an activation \u20d7x de-\ncomposes as \u20d7x = P\u20d7vi where |\u27e8\u20d7vi,\u20d7vj\u27e9| \u226a\u27e8\u20d7vi,\u20d7vi\u27e9\nfor most j \u0338= i. Crucially, the direction-meaning\ncorrespondence remains stable across tokens, en-\nabling targeted probing for specific concepts.\nTraining.\nFor each value, we used 720 sentences\n(800 \u00d7 90%) to train the value-specific probes.\nGiven a token sequence of length T, {\u20d7xl(t)}T\nt=1\ndenote the MLP output activations at layer l, and\n{y(t)}T\nt=1 denote the supervision signals (token-\nwise relevance scores) from GPT-4o. Then we train\na linear value probe P(\u20d7x) = f(\u27e8\u20d7wp, \u20d7x\u27e9+b) (where\nf is taken as ReLU), by minimizing the objective\n\u2126(\u20d7wp, b) := E\u2225y(t) \u2212P(\u20d7xl(t))\u22252\n2 + \u03bb\u2225\u20d7wp\u22251. For\neach value and each layer, we applied above train-\ning and obtained a value probe parametrized by \u20d7wp\nand b (Figure 2-A.1), yielding a complete set of\nlayer-wise value probes.\nTesting.\nWe assess probe validity on held-out test\nsentences through correlational analysis (Figure 2-\nA.2). For an input sequence, we extract token-wise\nactivations {\u20d7xl(t)}T\nt=1 at layer l, computed pre-\ndicted score \u02c6y(t) = P(\u20d7xl(t)), and then aggregate\nall token scores by averaging to produce a single\nscalar quantifying the representational strength of\nthe target abstract concept (see Figure 2-A.2). We\nrepeated the testing for each layer.\nTo validate probe specificity, we construct a pre-\ndiction matrix by applying each value probe (rows\nin Figure 2-A.3, exemplified as probe I, J, K) to\neach value corpus (columns in Figure 2-A.3, ex-\nemplified as corpora I, J, K). A valid probe should\nyield significantly higher scores on matched cor-\npora (diagonal entries, e.g., Probe J on Corpora J)\nthan on mismatched copora (off-diagonal entries,\ne.g., Probe J on Corpora J and K).\n3.3\nActive Steering\nTheoretical Foundation.\nThe value probe direc-\ntion \u20d7wp serves dual functions: it not only detects\nthe presence of a value concept (probing) but also\nprovides a lever for modifying it (steering). This du-\nality arises from the geometry of the feature space:\nthe same direction that reads out a concept can be\nused to write it in. We further build on the aggre-\ngation phenomenon: transformers compress con-\ntext into specific tokens that dominate downstream\nattention. These aggregation points compress ab-\nstract information along feature directions, making\nthem particularly responsive to activation interven-\ntion.\nSteering Operation.\nGiven the probe direction\n\u20d7wp, a steering strength \u03b1 and a normalization factor\nkp := k0/|\u20d7wp|, we modify the activation at layer\nl by: \u20d7xl(t) 7\u2192\u20d7xl(t) + \u03b1kp \u20d7wp as illustrated in Fig-\nure 2 (B.1-B.2). In order to hit the key processing\ncircuit in transformer dynamics, steering operation\nis applied to a range of token-streams in our tests.\nSteering Intervention.\nTo quantify steering ef-\nfects, we prompt the LLM model repeatedly with\nidentical choice scenarios and compute the proba-\nbility distribution over response options (Figure 2-\nB.3). Shifts in this distribution under different steer-\ning strengths \u03b1, reveal the causal influence of value\nrepresentations on decision-making behavior.\n4\nExperiments\nWe evaluate the three-level abstraction-grounding\ninteractions (Section 2) using passive probing and\nactive steering (Section 3) across 10 value con-\ncepts. This yields a 3 (A-A vs.\nA-C vs.\nC-\nC) \u00d72 (probing vs. steering) task matrix (Fig-\nure 4). Experiments were conducted on six open-\nsource LLMs: Qwen3-4B, Qwen3-8B, Llama3-3B,\nLlama3-8B, Mistral-7B, and Gemma2-9B.\n4.1\nDiagnostic Value Probe Selection\nFor each LLM and each value dimension, we ob-\ntained one value probe per layer (Section 3.2),\nyielding N probes for an N-layer model. We se-\nlected the best-performing probe as the diagnostic\n4\nprobe (used in all subsequent experiments) based\non Pearson correlation between probe predictions\n{\u02c6y(t)}T\nt=1 and ground-truth labels {y(t)}T\nt=1 on\nthe validation set sourced from the value concept\ndataset (see Section 3.1).\n0\n5\n10\n15\n20\n25\n30\n35\nLayer Probe Index\n0.56\n0.60\n0.64\n0.68\n0.72\n0.76\n0.80\nPearson Correlation\nQwen3-8B\nPat.\nEqu.\nInt.\nCoo.\nInd.\nDis.\nCur.\nCou.\nSat.\nRes.\nFigure 3: Correlation between Probe Predictions and Ground\nTruth Values Across Different Layers for LLMs.\nAs shown in Figure 3, probing performance ex-\nhibits a consistent pattern across LLMs: correlation\nrises in shallow layers, peaks in middle layers, and\ndeclines or fluctuates in deeper layers. This sug-\ngests that intermediate layers are most effective\nat encoding abstract value representations. Mean-\nwhile, the value representation signals last in vari-\nous intermediate layers, indicating a complicated\nmulti-layer information processing. We therefore\nused the peak-layer diagnostic probe for each value\ndimension in all subsequent experiments.\n4.2\nTask Settings\nFor each interaction regime, the key is to define (1)\nwhat contents should be probed and steered, and\n(2) how to evaluate the resulting effects.\nA-A: Interpretation of Abstract Values\nThis\nregime tests whether LLMs maintain coherent inter-\nnal representations of abstract value concepts\u2014the\ncapacity for interpretation (i.e., detecting seman-\ntics of abstract concepts in isolation).\nCorpora. The validation set of the value con-\ncept dataset (Section 3.1), comprising 80 Concept\nDescriptions (~4,000 tokens) of the value at an\nabstract conceptual level, is employed.\nPassive Probing. We test whether the diagnostic\nvalue probes (Section 4.1) detect abstract value\nsemantics. For each Concept Description, we apply\nthe probe for token-wise scores, then aggregate to\na mean score that quantifies the LLM\u2019s interpretive\nactivation (Figure 4, A-A, Passive Probing).\nActive Steering. We test whether intervening on\nabstract representations causally alters interpreta-\ntion. As the LLM reads each corpus, its activations\nare steered along the probe direction; then we col-\nlect the judge on relevance of the corpus and a\ngiven value (Figure 4,A-A, Active Steering).\nExpected Results. If the LLM possesses gen-\nuine value representations, probing should re-\nveal that value-specific corpora yield higher ac-\ntivation under matched probes than mismatched\nCorpora of Abstract Concept:\nCorpora of Abstract Concept:\nCorpora of Concrete Event:\nCorpora of Concrete Event:\nCorpora of Concrete Choice:\nCorpora of Concrete Choice:\nA unique perspective that allows people to view\nthe world though the lens of continuous wonder\nand inquiry.\nA unique perspective ... of continuous wonder\nand inquiry.\nCuriosity\nValue\nProbes\nAverage Scores\nProbing\nCuriosity\nValue\nProbes\nAverage Scores\nProbing\nCuriosity\nValue\nProbes\nAverage Scores\nProbing\nThe children used the diary's treasure entries as\na prop in these pirate games, ... believing it was\nreal.\nThe children used the diary's ... believing it was\nreal.\nThe children used the diary's ... believing it was\nreal.  Treasure Hunt  or  Piano Class \uff1f\nThis highlights the importance of creativity and ...\nAbstract - Abstract\nInterpretation: detected semantics\nrelated to abstract concepts.\nAbstract - Concrete\nUnderstanding: link abstract concepts\nto relevant concrete events.\nConcrete - Concrete\nApplication: use abstract rules to guide\ndecisions in concrete tasks.\nAnalyze the Event for decision making.\nIs the Concept\nrelated to\nCuriosity?\nIs the Event\nrelated to\nCuriosity?\nEvent\nWhat do you do?\nA or B ?\nUnknown...\nB, ...\nA, ...\nAnswer Options:\n\u00d7\nOutput Error.\nUnknown...\nNo, ...\nYes, ...\nAnswer Options:\n\u00d7\nOutput Error.\nUnknown...\nNo, ...\nYes, ...\nAnswer Options:\n\u00d7\nOutput Error.\nSteer 0\nCuriosity +++\nCuriosity ---\nOriginal\nChoice\nSteered +++\nChoice\nSteered ---\nChoice\nOriginal\nChoice\nSteered +\nChoice\nSteered -\nChoice\nSteer 0\nCuriosity +\nCuriosity -\nOriginal\nChoice\nSteered +\nChoice\nSteered -\nChoice\nSteer 0\nCuriosity +\nCuriosity -\nPassive\nProbing\nActive\nSteering\nPatriotism\nSatiety\nSatiety\nSatiety\nPatriotism\nPatriotism\nFigure 4: Experimental operationalization of the abstract-concrete grounding framework. A-A tests interpretation of\nabstract value concepts. A-C tests grounding of abstract values in concrete event descriptions.C-C tests application of abstract\nvalues in concrete decision-making. Each regime is evaluated via both passive probing (detecting value activation) and active\nsteering (intervening on value representations).\n5\nprobes.\nFor steering, modulating the repre-\nsentation via steering strength \u03b1 (section 3.3)\nshould shift value-relevance judgments:\nwe\nexpect a positive relationship between \u03b1 and\nEcorpusP(relevant|corpus, value, \u03b1) for each value.\nA-C:\nValue Grounding in Concrete Events\nThis regime tests whether LLMs can bridge abstrac-\ntion and instantiation, the capacity for understand-\ning (linking abstract concepts to concrete events).\nCorpora. We constructed scenario-based nar-\nratives (80-170 paragraphs per value) describing\nconcrete events. Each Event Description comprises\na scene or an event (situational context) in 1st or\n3rd person view. Sources include GPT-4o genera-\ntion and public datasets (Wikipedia (Foundation),\nQuora (Atef, 2023), CNN News (Cherguelaine,\n2023)) filtered via value-relevance.\nPassive Probing. We test whether abstract value\nprobes can detect the value implications latent in\nconcrete event descriptions. For each Event De-\nscription, we apply the diagnostic probe, compute\ntoken-wise scores, and aggregate to a mean score\nquantifying the LLM\u2019s grounding capacity (Fig-\nure 4, A-C, Passive Probing).\nActive Steering. We test whether intervening on\nabstract value representations causally alters the\nLLM\u2019s interpretation of concrete events. We steer\nthe activations along the probe direction, then mea-\nsure the LLM\u2019s judgment on whether the event re-\nlates to a given value P(relevant|event, value, \u03b1)\n(Figure 4, A-C, Active Steering).\nWe restrict\nour analysis to non-polarized events: those with\nlogitP(relevant|event, value, 0) not too large.\nExpected Results. If value representations trans-\nfer across abstraction levels, both Probing and\nSteering results will be similar to that described\nin the expected A-A results.\nC-C: Value-Guided Decision-Making\nThis\nregime tests whether internalized abstract values\nregulate behavior, the capacity for application (us-\ning abstract principles in concrete decisions).\nCorpora. We constructed a similar narrative\ndataset as in A-C. Each event bears two related\nbut exclusive action options. Sources include GPT-\n4o generation and a public dataset (Huggingface:\nstress-testing-model-spec, (Zhang et al., 2025)) fil-\ntered via value relevance.\nPassive Probing. We test whether abstract value\nprobes detect value-laden reasoning in decision pro-\ncesses. For each Chain-of-thought Description, we\napply the diagnostic probe, compute token-wise\nscores, and aggregate to a mean score quantify-\ning how strongly the decision rationale reflects the\ntarget value (Figure 4, C-C, Passive Probing).\nActive Steering. We test whether intervening\non abstract value representations causally shifts\nconcrete decisions. Similarly to A-C, after the\nsteered prefilling, we measure the LLM\u2019s choice\non two options P(relevant option| event, value, \u03b1)\n(Figure 4, A-C, Active Steering).\nExpected Results. If value representations gen-\nuinely regulate decision-making, for probing, deci-\nsion rationales generated under a given value ori-\nentation should yield higher activation under the\nmatched probe than under mismatched probes. For\nsteering, a result similar to that of A-C is expected\nunder a similar subset.\n4.3\nResults\nProbing Effects\nAcross all three interaction\nregimes (A-A, A-C, C-C), probing results align\nwith our predictions: value-specific corpora consis-\ntently yield higher activation under matched probes\nthan under mismatched probes.\nTo visualize this specificity, we constructed\ncross-validation matrices where each row repre-\nsents a value probe and each column represents\na value-relevant corpus. Diagonal cells indicate\nmatched probe\u2013corpus pairs; off-diagonal cells in-\ndicate mismatched pairs. Color-coded matrices in\nFigure 5 illustrate this using Qwen3-8B. Across all\nthree interaction tests, diagonal cells exhibit sig-\nnificantly higher probing scores than off-diagonal\ncells, demonstrating that (1) our trained probes are\nvalue-specific rather than capturing generic seman-\ntic features, (2) LLMs maintain distinguishable in-\nternal representations for different value concepts,\nand (3) these representations persist across abstrac-\ntion levels\u2014from pure conceptual interpretation\n(A-A), through concrete event grounding (A-C),\nto value-guided decision reasoning (C-C).\nAll tested LLMs exhibit the same pattern. Color-\ncoded box-plot in Figure 5 summarizes results\nacross models by computing the activation dif-\nference between diagonal and off-diagonal cells;\nall models show significantly positive differences\nacross all three regimes. Full cross-validation ma-\ntrices for each model are provided in the Appendix\n(Figure 9 for A-A, Figure 10 for A-C, Figure 11\nfor C-C).\nExternal Validation. To further validate probe\neffectiveness, we employed three advanced LLMs\n(GPT-5.2, Gemini-3-Pro, and Claude-Sonnet-4.5)\n6\nPat.\nEqu.\nInt.\nCoo.\nInd.\nDis.\nCur.\nCou.\nSat.\nRes.\n0.36\n0.23\n0.25\n0.24\n0.15\n0.15\n0.06\n0.14\n0.10\n0.06\n0.06\n0.36\nPat.\n0.25\n0.34\n0.30\n0.32\n0.19\n0.19\n0.07\n0.13\n0.11\n0.08\n0.07\n0.34\nEqu.\n0.20\n0.20\n0.38\n0.28\n0.29\n0.23\n0.11\n0.15\n0.08\n0.05\n0.05\n0.38\nInt.\n0.16\n0.22\n0.25\n0.41\n0.16\n0.21\n0.14\n0.07\n0.10\n0.07\n0.07\n0.41\nCoo.\n0.19\n0.18\n0.27\n0.28\n0.33\n0.21\n0.08\n0.14\n0.08\n0.09\n0.08\n0.33\nInd.\n0.10\n0.10\n0.17\n0.23\n0.17\n0.29\n0.10\n0.10\n0.09\n0.12\n0.09\n0.29\nDis.\n0.08\n0.10\n0.12\n0.18\n0.09\n0.17\n0.22\n0.13\n0.04\n0.09\n0.04\n0.22\nCur.\n0.13\n0.12\n0.17\n0.17\n0.20\n0.22\n0.10\n0.25\n0.04\n0.06\n0.04\n0.25\nCou.\n0.09\n0.08\n0.08\n0.10\n0.06\n0.18\n0.09\n0.10\n0.31\n0.14\n0.06\n0.31\nSat.\n0.09\n0.07\n0.14\n0.19\n0.11\n0.20\n0.08\n0.08\n0.13\n0.24\n0.07\n0.24\nRes.\n0.25\n0.16\n0.14\n0.16\n0.11\n0.12\n0.06\n0.13\n0.07\n0.07\n0.06\n0.25\nPat.\n0.17\n0.22\n0.13\n0.17\n0.11\n0.14\n0.08\n0.11\n0.09\n0.09\n0.08\n0.22\nEqu.\n0.16\n0.14\n0.22\n0.16\n0.11\n0.15\n0.08\n0.11\n0.07\n0.07\n0.07\n0.22\nInt.\n0.17\n0.14\n0.12\n0.24\n0.09\n0.14\n0.08\n0.09\n0.08\n0.07\n0.07\n0.24\nCoo.\n0.13\n0.10\n0.11\n0.13\n0.20\n0.10\n0.06\n0.08\n0.07\n0.07\n0.06\n0.20\nInd.\n0.10\n0.09\n0.10\n0.13\n0.09\n0.18\n0.07\n0.07\n0.07\n0.08\n0.07\n0.18\nDis.\n0.07\n0.07\n0.06\n0.07\n0.05\n0.08\n0.12\n0.06\n0.05\n0.05\n0.05\n0.12\nCur.\n0.15\n0.10\n0.10\n0.13\n0.10\n0.11\n0.06\n0.20\n0.05\n0.06\n0.05\n0.20\nCou.\n0.09\n0.07\n0.07\n0.09\n0.07\n0.14\n0.09\n0.09\n0.22\n0.12\n0.07\n0.22\nSat.\n0.09\n0.06\n0.06\n0.09\n0.08\n0.08\n0.08\n0.06\n0.06\n0.16\n0.06\n0.16\nRes.\n0.33\n0.25\n0.24\n0.17\n0.19\n0.14\n0.08\n0.13\n0.05\n0.06\n0.05\n0.33\nPat.\n0.23\n0.31\n0.23\n0.20\n0.19\n0.15\n0.06\n0.12\n0.05\n0.05\n0.05\n0.31\nEqu.\n0.24\n0.25\n0.37\n0.22\n0.24\n0.20\n0.11\n0.15\n0.06\n0.06\n0.06\n0.37\nInt.\n0.24\n0.27\n0.24\n0.33\n0.15\n0.15\n0.09\n0.08\n0.05\n0.06\n0.05\n0.33\nCoo.\n0.17\n0.20\n0.24\n0.18\n0.33\n0.17\n0.11\n0.13\n0.06\n0.07\n0.06\n0.33\nInd.\n0.13\n0.11\n0.19\n0.12\n0.20\n0.26\n0.12\n0.11\n0.05\n0.10\n0.05\n0.26\nDis.\n0.10\n0.10\n0.13\n0.09\n0.08\n0.12\n0.20\n0.09\n0.03\n0.04\n0.03\n0.20\nCur.\n0.17\n0.18\n0.17\n0.13\n0.15\n0.15\n0.11\n0.26\n0.04\n0.04\n0.04\n0.26\nCou.\n0.15\n0.10\n0.17\n0.11\n0.08\n0.16\n0.15\n0.10\n0.32\n0.12\n0.08\n0.32\nSat.\n0.08\n0.08\n0.11\n0.10\n0.11\n0.13\n0.11\n0.08\n0.07\n0.18\n0.07\n0.18\nRes.\nQwen3-8B A-A Probing\nAverage Scores\nQwen3-8B A-C Probing\nAverage Scores\nQwen3-8B C-C Probing\nAverage Scores\nProbes\nA-A Corpora\nA-C Corpora\nC-C Corpora\nQwen3-4B\nQwen3-8B\nLlama3-3B\nLlama3-8B\nMistral-7B\nGemma2-9B\n0.0\n0.2\n0.4\nDiagonal Dominance\n0.23\n0.23\n0.24\n0.21\n0.29\n0.30\nA-A Average Scores Differences\nQwen3-4B\nQwen3-8B\nLlama3-3B\nLlama3-8B\nMistral-7B\nGemma2-9B\n0.0\n0.1\n0.2\n0.3\n0.4\nDiagonal Dominance\n0.23\n0.32\n0.21\n0.20\n0.23\n0.25\nA-C Average Scores Differences\nQwen3-4B\nQwen3-8B\nLlama3-3B\nLlama3-8B\nMistral-7B\nGemma2-9B\n0.0\n0.1\n0.2\n0.3\n0.4\nDiagonal Dominance\n0.22\n0.30\n0.22\n0.25\n0.22\n0.22\nC-C Average Scores Differences\nFigure 5: Probing Results in A-A, A-C, and C-C Tests. Top: averaged probing scores for various probes (y-axis) on\ncorpora with various value labels (x-axis), for Qwen3-8B only. Bottom: column-wise relative diagonal dominance in max-sense\n(P\nj (Mii \u2212maxj,i\u0338=j Mij) /Mii), on the corresponding top-row heatmap, for all models.\nto directly rate value relevance of the A-C corpora\nat the corpus level. We then correlate these external\nratings with probe mean scores on the same corpora.\nResults reveal high consistency (see Figure 12 in\nAppendix), validating the effectiveness of our prob-\ning methodology: the value representations iden-\ntified by our probes correspond systematically to\nvalue judgments made by leading LLMs.\nSteering Effects\nSteering results reveal an in-\nteresting dissociation: A-C and C-C tests show\nclear steering effects consistent with our predic-\ntions, while A-A shows no significant effect.\nTo visualize steering effects, we plot the distri-\nbution of corpora across different judgment prob-\nability levels (Y-axis) at each steering strength \u03b1\n(X-axis). Figure 6 (top three panels) displays this\nfor each interaction regime, with bar width indicat-\ning corpus count at each probability level and the\nlined circle denoting the mean probability across\nall corpora.\nFor A-C and C-C, at neutral steering (\u03b1 = 0),\ncorpora are relatively uniformly distributed across\nprobability levels (we simply deleted the corpora\nthat are too polarized), with mean value-relevance\njudgment around \u223c50%. As \u03b1 decreases (sup-\npressing the value representation), the distribu-\ntion P(relevance | corpus, value, \u03b1) shifts down-\nward\u2014fewer corpora are judged as value-relevant,\nand mean probability declines accordingly (solid\nline shown in Figure 5 Top-mid and Top-right).\nAs \u03b1 increases (amplifying the value representa-\ntion), the distribution shifts upward\u2014more corpora\nare judged as value-relevant, and mean probability\nrises. This demonstrates that value representations\ncausally influence both concrete event interpreta-\ntion (A-C) and decision-making (C-C).\nFor A-A, however, mean judgment probabil-\nity remains \u223c50% regardless of steering strength\n(solid line). Analysis of the underlying distribu-\ntions reveals why: abstract concept corpora ex-\nhibit highly polarized representations\u2014passages\nare judged as either strongly relevant or strongly\nirrelevant, shown in Figure 5 Top-left, with few\nintermediate cases. This polarized structure resists\nsteering intervention. This null result is theoreti-\ncally informative. Abstract value concepts, once\nencoded, appear to function as stable representa-\ntional anchors rather than malleable activations,\nor as another hypothesis, they lie in the \u201csafe re-\ngions\u201d in the semantic representation space where\nthe steering shifts have hardly any effect to be ob-\nserved.\nFigure 6 (bottom panel) additionally displays\n7\n-1.0\n-0.5\n0\n0.5\n1.0\nSteering strength\n0%\n25%\n50%\n75%\n100%\nA-A Answers Distribution\n-1.0\n-0.5\n0\n0.5\n1.0\nSteering strength\nA-C Answers Distribution\n-1.0\n-0.5\n0\n0.5\n1.0\nSteering strength\nC-C Answers Distribution\n-1.0\n-0.5\n0\n0.5\n1.0\n0%\n20%\n40%\n60%\n80%\n100%\nA-A Corpora Steering\nRelevant\nIrrelevant\n-1.0\n-0.5\n0\n0.5\n1.0\nA-C Corpora Steering\nRelevant\nIrrelevant\n-1.0\n-0.5\n0\n0.5\n1.0\nC-C Corpora Steering\nRelevant\nIrrelevant\nUnknown\nError\nFigure 6: Steering Results. Results on Qwen3-8B for the equality (Equ) task under A-A, A-C, and C-C. Top: Answer\ndistributions (option A / B for C-C, relevant / irrelevant otherwise) across steering strengths. Bottom: Changes in corpus\npopulation distribution along the \u201crelevant / option A\u201d dimension. Steering mainly shifts intermediate populations, and is\nineffective in highly polarized settings (e.g., A-A).\nmean probabilities for alternative response options\nas complementary information, with dark-colored\nareas corresponding to the value-relevant proba-\nbilities shown in the top panels. Figure 6 uses\nQwen3-8B as an exemplar; all other tested models\nexhibit the same pattern (see Appendix Figure 13\nfor individual model results).\n5\nRelated Work\nExploring the Black Box of LLMs.\nExplain-\nable AI (xAI) has grown alongside black-box deep\nlearning models (Towell and Shavlik, 1993; Lou\net al., 2013; Ribeiro et al., 2018; Zhang et al., 2018).\nLLM interpretability generally falls into three cate-\ngories. Passive methods analyze model states to\nexplain behavior, such as examining BERT token\ninteractions (Chen et al., 2024), Claude\u2019s attention\nheads (Olsson et al., 2022), or transcoder-based fea-\nture circuits (Dunefsky et al., 2024). Active meth-\nods intervene in outputs without explaining internal\nbehavior, often steering preferences via vector syn-\nthesis (Panickssery et al., 2023; Li et al., 2023).\nHybrid methods combine explanation and inter-\nvention; notably, sparse autoencoders (SAE) iden-\ntify monosemantic features to effectively explain\nand steer models (Bricken et al., 2023; Templeton,\n2024). While cross-layer transcoders can explain\nsignificant portions of model states (Ameisen et al.,\n2025), verifying whether increasingly complex rep-\nresentations truly reflect conceptual handling re-\nmains challenging.\nHuman Values in Language.\nPsychology mod-\nels human values as low-dimensional substruc-\ntures (Rokeach, 1973; Schwartz, 2012), a field\nrecently entered by LLMs via the lexical hypoth-\nesis (John et al., 1988). Recent frameworks in-\nclude ValueKaleidoscope for context-specific tasks\n(Sorensen et al., 2023), ValueBench for psycho-\nmetric evaluation (Ren et al., 2024), ValueLex for\ncomparing human-LLM systems (Biedma et al.,\n2024), and ValueCompass for evaluating alignment\n(Yao et al., 2025). Additionally, (Kang et al., 2024)\nuse pretrained SAE features to build causal graphs\non value dimensions. While these studies typi-\ncally treat LLMs as subjects responding to question-\nnaires, this work searches for and traces internal\nvalue activation signals across concrete situations."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models\"\n\n#### Main Contributions:\n1. **Introduction of an Abstraction-Grounding Framework**: The authors introduce a three-level framework to decompose conceptual understanding into interpretation, grounding, and application capacities.\n2. **Development of Value Probing Toolkits**: They develop value probing toolkits that assess LLMs' internal representations by detecting value traces in activations and steering representations to shift behavior.\n3. **Systematic Evaluation of Interpretation, Grounding, and Application Capacities**: The framework enables a systematic evaluation of interpretation, grounding, and application capacities across various value dimensions.\n4. **Demonstration of Cross-Level Transfer**: Probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, indicating cross-level transfer.\n\n#### Methodology:\n1. **Dataset Generation**: The authors generated a dataset comprising 10 value dimensions using GPT-4o, synthesizing them from hierarchical value taxonomies.\n2. **Passive Probing**: This method tests whether value concepts leave identifiable traces in LLM internal states (establishing existence). It uses linear probes to detect abstract value semantics and evaluate their activation strength across different layers.\n3. **Active Steering**: This method tests whether modifying representations produces predictable behavioral changes, establishing causal efficacy. It involves steering activations along the probe direction and measuring LLM judgments on event relevance.\n\n#### Key Findings:\n1. **Value Representation Existence**: Probing results align with predictions: value-specific corpora consistently yield higher activation under matched probes than mismatched abstract concept corpora.\n2. **Causal Effect of Value Representations**: Steering reveals that intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A). This suggests encoded abstract values function as stable anchors rather than malleable activations.\n\n#### Conclusions:\n1. **LMs Maintain Structured Value Representations**: The findings indicate large language models maintain structured value representations that bridge abstraction and action.\n2. **Mechanistic Foundation for Autonomous AI Systems**: These results provide a mechanistic foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.\n\n#### Related Work:\n1. **Exploring the Black Box of LLMs**: The paper builds upon existing work in explainable AI (xAI) and hybrid methods.\n2. **Human Values in Language**: It leverages recent frameworks for human-LLM comparison and value evaluation studies, including ValueKaleidoscope, ValueBench, ValueLex, and ValueCompass.\n\n### Summary Conclusion:\nThe paper introduces a comprehensive framework to understand how large language models (LLMs) interpret abstract values. Through probing and steering methods, it demonstrates that LLMs maintain structured value representations across different levels of abstraction. This work provides foundational insights for developing more transparent and controllable autonomous AI systems by understanding the mechanisms underlying LLMs' value comprehension."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "CO-INITIALIZATION OF CONTROL FILTER AND SECONDARY PATH VIA\nMETA-LEARNING FOR ACTIVE NOISE CONTROL\nZiyi Yang1, Li Rao2, Zhengding Luo1*, Dongyuan Shi3, Qirui Huang1, and Woon-Seng Gan1\n1Smart Nation TRANS Lab, School of Electrical and Electronic Engineering,\nNanyang Technological University, Singapore\n2Key Laboratory of Modern Acoustics, Institute of Acoustics, Nanjing University, Nanjing, China\n3Northwestern Polytechnical University, Xi\u2019an, China\nABSTRACT\nActive noise control (ANC) must adapt quickly when the\nacoustic environment changes, yet early performance is\nlargely dictated by initialization.\nWe address this with a\nModel-Agnostic Meta-Learning (MAML) co-initialization\nthat jointly sets the control filter and the secondary-path\nmodel for FxLMS-based ANC while keeping the runtime\nalgorithm unchanged. The initializer is pre-trained on a small\nset of measured paths using short two-phase inner loops that\nmimic identification followed by residual-noise reduction,\nand is applied by simply setting the learned initial coeffi-\ncients. In an online secondary path modeling FxLMS testbed,\nit yields lower early-stage error, shorter time-to-target, re-\nduced auxiliary-noise energy, and faster recovery after path\nchanges than a baseline without re-initialization. The method\nprovides a simple fast start for feedforward ANC under envi-\nronment changes, requiring a small set of paths to pre-train.\nIndex Terms\u2014 Active Noise Control, Model-Agnostic\nMeta-Learning, Secondary Path Modeling;\n1. INTRODUCTION\nActive noise control (ANC) is widely used to reduce low-\nfrequency noise by driving a secondary source to cancel the\ndisturbance [1, 2]. While recent data-driven and machine-\nlearning approaches have been explored for ANC [3, 4, 5],\nthe filtered-x LMS (FxLMS) framework remains the standard\nmethod behind many practical ANC systems across open win-\ndows, wearables, and openspace [6, 7, 8, 9]. In FxLMS, the\nfiltered reference is generated using an estimated secondary-\npath response. If it departs from the true plant, convergence\nslows and stability margins shrink [10]. At the same time, a\ncontroller that starts from zero produces little effective cancel-\nlation until sufficient gradient information accumulates [11].\nAs a result, start-up behavior depends on both the secondary-\npath assumption and the control-filter initialization.\nWhen the acoustic environment changes, a common ap-\nproach is adding online secondary-path modeling (OSPM),\n*Corresponding author: Zhengding Luo. The code will be available at\nhttps://github.com/yzyzieee/ICASSP26 co-init meta-learning\nwhich injects auxiliary noise to identify the secondary path\nduring ANC operation [12]. In practice, only low power of\nauxiliary noise can be used, so the identification SNR is low\nand the first gradient steps are noisy. As a result, start-up\nis slow and recovery after path changes is sluggish. Power-\nscheduling variants help but do not remove this limitation.\n[13, 14].\nFast-recovery strategies for abrupt system changes have\nbeen explored in adaptive filtering, echo cancellation, and\nANC[15, 16, 17]. However, in ANC these efforts primar-\nily focus on update mechanisms rather than on initialization\nof the control filter. Typical deployments start from zeros\nor nominal values, or carry over previous coefficients, while\nmost research effort optimizes the FxLMS/OSPM mechanism\n(step-size or structure) rather than the starting point that gov-\nerns the first moments after a change [10, 18, 19]. Recently,\nmeta-learning has emerged as a way to learn initializations\nthat adapt in a few gradient steps from little data [20, 21], and\nfirst reports in ANC show faster early convergence when only\nthe control filter is initialized [22].\nThis paper addresses the problem of initialization for\nfeedforward ANC under acoustic environment changes. We\nintroduce a co-initialization for FxLMS-based algorithms that\njointly initializes the control filter and the secondary-path\nmodel based on Model-Agnostic Meta-Learning (MAML).\nThe initializer is pre-trained on a small set of measured paths\nwith short two-phase inner loops (identification followed\nby residual-noise reduction) and is deployed by setting the\nlearned initial coefficients while keeping the adaptive updates\nunchanged. Under an OSPM\u2013FxLMS evaluation setup with\nmeasured in-ear headphone paths [23], the pre-trained initial-\nizer adapts faster to environment changes: it reaches the target\nerror sooner, maintains lower early-stage residuals, requires\nless auxiliary-noise energy, and recovers more quickly after\nenvironment switches than a baseline without reinitialization\nbut with identical updates. A data study further reveals that\nthe diversity of training paths enhances the initializer, with\nsecondary-path diversity exerting the stronger effect.\nThe\nmain contributions of this work are:\narXiv:2601.13849v1  [eess.AS]  20 Jan 2026\n\u2022 Few-shot meta-learned co-initialization for FxLMS-\nbased ANC: from a small set of measured paths, we jointly\nlearn the initial controller and secondary-path model; at\ndeployment, we simply set these learned initial values and\nkeep the runtime adaptive updates unchanged.\n\u2022 Fast start and robust adaptation under environment\nchanges: the initializer reaches the target error sooner,\nmaintains lower early residuals, reduces auxiliary-noise\nenergy, and recovers faster after path switches, with gener-\nalization to similar unseen acoustic conditions.\n2. ONLINE SECONDARY PATH MODELING (OSPM)\nWe adopt the cross-updated OSPM scheme of [12] and add\na lightweight error-jump detector for re-initialization. The\noverall signal flow are shown in Figure 1.\nLet x(n) be the reference, d(n) the disturbance, and\nv(n) a zero-mean white auxiliary noise. Its scaled output is\nvm(n). All FIRs are causal column vectors. With xw(n) =\n[x(n), . . . , x(n\u2212Lw +1)]\u22a4, xs(n) = [x(n), . . . , x(n\u2212Ls +\n1)]\u22a4, and u(n) = [vm(n), . . . , vm(n \u2212Ls + 1)]\u22a4, denote the\ntrue secondary path s \u2208RLs, its estimate \u02c6s, the control filter\nw\u2208RLw, and the auxiliary canceller h\u2208RLs (driven by a de-\nlayed stack xh(n)). The control output is u(n) = w\u22a4xw(n)\nand its last Ls samples form uw(n). The microphone residual\nand the auxiliary-cleaned error are\ne(n) = d(n) + s\u22a4uw(n) + s\u22a4u(n),\n(1)\nand the cleaned error after removing the modeled auxiliary\ncomponent is\ne\u2032(n) = e(n) \u2212\u02c6s\u22a4u(n).\n(2)\nThe three adaptive filters are updated in one pass as\n\u02c6s(n+1) = \u02c6s(n) + \u00b5s u(n)\n\u0002\ne\u2032(n) \u2212h\u22a4(n)xh(n)\n\u0003\n,\n(3)\nh(n+1) = h(n) + \u00b5h xh(n)\n\u0002\ne\u2032(n) \u2212h\u22a4(n)xh(n)\n\u0003\n,\n(4)\nw(n+1) = w(n) + \u00b5w \u02dcx(n) e\u2032(n),\n(5)\nwhere \u02dcx(n) = [ \u02dcx(n), . . . , \u02dcx(n\u2212Lw+1) ]\u22a4and \u02dcx(n) =\n\u02c6s\u22a4xs(n). which ensures the control filter is always driven by\nthe auxiliary\u2013cancelled error while the secondary\u2013path esti-\nmator uses that same error further cleaned by h. To scale the\nauxiliary noise level, we adopt the amplitude schedule[13]\nwith 0<\u03b1<1:\nPx(n) = \u03b1Px(n\u22121) + (1\u2212\u03b1) x2(n),\n(6)\nPe\u2032(n) = \u03b1Pe\u2032(n\u22121) + (1\u2212\u03b1) e\u20322(n),\n(7)\nthe instantaneous auxiliary signal is\nvm(n) = caux v(n)\np\nmin{Px(n), Pe\u2032(n)} + \u03b5,\n(8)\nwhere v(n)\u223cN(0, 1) and \u03b5>0 prevents vanishing injection.\nFinally, we attach an error\u2013jump detector that refreshes\nthe initialization instantaneously upon a suspected acous-\ntic environment change.\nWe monitor the canceller norm\nJh(n) = \u2225h(n)\u22252 with a short look-back M and thresh-\nold \u03b3h; if \u2206Jh(n) = Jh(n) \u2212Jh(n \u2212M) > \u03b3h, we in-\nstantly reset h \u21900 and (\u02c6s, w) \u2190(\u03a8, \u03a6), which are the\nmeta-initializations defined in the section 3. After that, the\nLMS/FxLMS updates (3)\u2013(5) resume unchanged.\nFig. 1. Modified cross\u2013updated online secondary\u2013path mod-\neling with auxiliary noise (with error jump detector). Dashed\narrows indicate re\u2013initialization and scheduling actions.\n3. CO-INITIALIZATION FOR FEEDFORWARD ANC\nBASED ON META LEARNING\nIn the ANC situation, each acoustic condition is treated as a\ntask with discrete-time reference, disturbance, and the task\nsecondary-path impulse response s\u2208RLs (available only dur-\ning meta-training).As summarized by the pseudocode in Al-\ngorithm 1, 2 meta-parameters are learned: the control filter\ninitialization \u03a6 \u2208RLw and the secondary-path initialization\n\u03a8 \u2208RLs.The inner loop comprises Phase A (secondary-path\nupdate) and Phase B (control filter update), followed by vali-\ndation on a separate segment and an across-task meta update.\nInner-loop adaptation runs on short segments drawn from the\ninput tracks xc, dc, followed by validation on a separate seg-\nment and an across-task meta update. Scalars \u00b5w, \u00b5s denote\ninner-loop step sizes, \u03b1w, \u03b1s denote meta learning rates, and\n\u03bbw, \u03bbs \u2208(0, 1] are forgetting factors for validation losses.\n3.1. Inner Loop \u2014 Training (per sampled task)\nA contiguous segment {x(n), d(n)} is sampled. Initialization\nsets w\u2190\u03a6 and \u02c6s\u2190\u03a8.\n1) Phase A: Secondary path update (TA steps). Let Ls\ndenote the secondary-path filter length and let v(n) be the\nauxiliary-noise used to excite the secondary path during train-\ning. Define the identification output as yid(n) = s\u22a4u(n) with\nu(n) = [ v(n), v(n\u22121), . . . , v(n\u2212Ls+1) ]\u22a4. At each step,\ncompute the error\nes(n) = yid(n) \u2212\u02c6s\u22a4u(n),\n(9)\nand apply the LMS recursion\n\u02c6s(t+1) = \u02c6s(t) + \u00b5s es(t) u(t),\nt = 0, . . . , TA \u22121. (10)\n2) Phase B: Control filter update (TB steps). Define the\nfiltered reference inline as \u02dcx(n) = \u02c6s\u22a4xs(n) with xs(n) =\n[ x(n), x(n\u22121), . . . , x(n\u2212Ls+1) ]\u22a4, and let the Lw-tap con-\ntrol filter regressor be x\u2032(m) = [ \u02dcx(m), \u02dcx(m\u22121), . . . , \u02dcx(m\u2212\nLw+1) ]\u22a4. The residual and the FxLMS recursion are\ne(m) = d(m) \u2212w\u22a4x\u2032(m),\n(11)\nw(t+1) = w(t)+\u00b5w e(t) x\u2032(t),\nt = 0, . . . , TB\u22121. (12)\nPhase A and Phase B use disjoint time indices within the\ninner segment.\n3.2. Inner Loop \u2014 Testing\nA new validation batch {x\u2020(k), d\u2020(k)} from the same task is\ndrawn, while \u02c6s and w are frozen. The validation errors are\ne\u2020\ns(k) = s\u22a4u\u2020(k) \u2212\u02c6s\u22a4u\u2020(k),\nk = n \u2212ts,\n(13)\ne\u2020(k) = d\u2020(k) \u2212w\u22a4x\u2032\u2020(k),\nk = n \u2212tw.\n(14)\nThe meta-gradients are vector accumulations ( \u2206\u03a8 \u2208RLs,\n\u2206\u03a6 \u2208RLw ) over the most-recent validation samples with\nforgetting factors \u03bbs, \u03bbw \u2208(0, 1]:\n\u2206\u03a8 =\nNs\u22121\nX\nts=0\n\u03bb ts\ns e\u2020\ns(n \u2212ts) u\u2020(n \u2212ts),\n(15)\n\u2206\u03a6 =\nNw\u22121\nX\ntw=0\n\u03bb tw\nw e\u2020(n \u2212tw) x\u2032\u2020(n \u2212tw).\n(16)\n3.3. Across-task MAML update\nGiven the task\u2013level meta-gradients (15)\u2013(16), the meta-\nparameters are updated by a simultaneous gradient step\n\u03a6(i+1) = \u03a6(i) + \u03b1w \u2206\u03a6,\n\u03a8(i+1) = \u03a8(i) + \u03b1s \u2206\u03a8,\n(17)\nwith learning rates \u03b1w, \u03b1s > 0. After the update, the accu-\nmulators are reinitialized to zero, \u2206\u03a6 \u21900 and \u2206\u03a8 \u21900,\nbefore drawing the next task. Iterating this procedure for i =\n0, . . . , K\u22121 across tasks yields a co-initialization (\u03a6, \u03a8) that\nsupports few-step secondary-path identification and residual-\nnoise reduction under new conditions.\n4. SIMULATION\n4.1. Path Dataset and Diversity Evaluation\nAll simulations use measured in-ear primary and secondary\npaths from the RWTH Aachen IKS PANDAR database\n[23].\nIn total, 46 primary\u2013secondary path pairs are avail-\nable.Measurements used Bose QC20 in-ear headphones, and\nsubject-specific responses were captured in an acoustic booth.\nThe set covers 23 listeners with 3 wearing conditions (normal,\nslightly loose, loose). Signals are resampled to Fs=16 kHz\nfor runtime, and the noise is band-limited to [200, 2000] Hz.\nBecause the proposed meta-learning approach adopts\nfew-shot training based on a handful of samples per task, the\ncomposition of the training set can influence generalization.\nWe therefore quantify the distribution of paths with a scalar\ndiversity score based on the average pairwise log-spectral dis-\ntance (LSD). Concretely, LSD is computed on a log-spaced\ngrid [24] fm \u2208[100, 2000] Hz (M=64):\nLSD(i, j) =\n1\n\u221a\nM\n\u2113i \u2212\u2113j\n2,\n(18)\nwhere \u2113i =\n\u0002\n20 log10 |Hi(f1)|, . . . , 20 log10 |Hi(fM)|\n\u0003\u22a4is\nthe log-magnitude vector of path i.\nAlgorithm 1 Co\u2013initialization of control filter and secondary\npath (SP) for ANC via MAML\n1: Input: reference track xc, disturbance track dc, task sec-\nondary path s (for training)\n2: Initialize: meta control filter \u03a6, meta SP model \u03a8; inner\nsteps TA, TB, test length N; step sizes \u00b5w, \u00b5s; forgetting\nfactor \u03bb; meta rates \u03b1w, \u03b1s; total epochs K\n3: for epoch i = 1 to K do\n\u25b7across tasks\n4:\n/* Inner Loop - Training */\n5:\nSample a segment {x(n), d(n)} from (xc, dc)\n6:\nw \u2190\u03a6, \u02c6s \u2190\u03a8\n\u25b7inner initialization\n7:\n/* Phase A: Secondary Path Update */\n8:\nu(n) is samples of the reference to excite the SP,\n9:\nyid(n) = s\u22a4u(n),\nes(t) = yid(n) \u2212\u02c6s\u22a4u(n)\n10:\n\u02c6s \u2190\u02c6s + \u00b5s es(n) u(n)\n11:\n/* Phase B: Control filter Update */\n12:\nx\u2032(m) is samples of the filtered reference from \u02c6s,\n13:\ne(m) = d(m) \u2212w\u22a4x\u2032(m)\n14:\nw \u2190w + \u00b5w e(m) x\u2032(m)\n15:\n/* Inner Loop - Testing */\n16:\n\u2206\u03a6 \u21900, \u2206\u03a8 \u21900\n17:\nfor ks = 0 to Ns \u22121 do\n18:\ne\u2020\ns(ks) \u2190s\u22a4u\u2020(ks) \u2212\u02c6s\u22a4u\u2020(ks)\n19:\n\u2206\u03a8 += \u03bb ks\ns e\u2020\ns(ks) u\u2020(ks)\n20:\nend for\n21:\nfor kw = 0 to Nw \u22121 do\n22:\ne\u2020(kw) \u2190d\u2020(kw) \u2212w\u22a4x\u2032\u2020(kw)\n23:\n\u2206\u03a6 += \u03bb kw\nw e\u2020(kw) x\u2032\u2020(kw)\n24:\nend for\n25:\n/* MAML update */\n26:\n\u03a6 \u2190\u03a6 + \u03b1w \u2206\u03a6,\n\u03a8 \u2190\u03a8 + \u03b1s \u2206\u03a8\n27: end for\nTo quantify how spread each subset is, we use the log-\nspectral distance (LSD) in (18) and define a dispersion\nD(G) =\n2\n|G|(|G| \u22121)\nX\ni<j, i,j\u2208G\nLSD(i, j).\n(19)\nwhere G \u2286{1, . . . , N} is an index set of paths.\nFig-\nure 3 shows magnitude responses for all 46 pairs: thin gray\ncurves denote all paths, while highlights mark the two train-\ning sets\u2014A (diverse) and B (compact), each consisting of\nthree primary\u2013secondary pairs. The within-set dispersion D\nis 6.17/6.75 dB (primary/secondary) for A and 0.79/0.91 dB\nfor B.\n4.2. Evaluation of online modeling FxLMS under path\nchange\nIn this section, we evaluate the proposed MAML co-initialization\napproach with the OSPM\u2013FxLMS in section 2, and testbed\nwith 2 sudden switches of both the primary and secondary\npaths. Paths are taken from the dataset mentioned in sub-\nsection 4.1, which has 46 conditions. Each condition spec-\nifies a subject and wearing fit and provides a measured pri-\nmary\u2013secondary path pair. Phase 1 runs on (P1, S1) from\nFig. 2. Online modeling FxLMS with auxiliary-noise power (paths switch at t=60 s & t=120 s).\nFig. 3. Magnitude responses of the 46 measured paths from\nthe PANDAR database.\n(a) Primary paths; (b) Secondary\npaths.\nperson 22, slightly loose fit; at t=60 s the path switches to\n(P2, S2) from person 13, normal fit; at t=120 s it switches to\n(P3, S3) from person 5, normal fit.\nTwo variants of the cross-updated scheme in section 2 are\ncompared: (i) Original OSPM baseline (zero start and carry-\nover of w and \u02c6s across each switch), and (ii) Proposed MAML\nco-init (reset at each switch). All other settings are identi-\ncal, including auxiliary noise scheduling, band-limited input\nnoise [200, 2000],Hz, and random seeds.\nFigure 2 plots the sliding MSE (dB) and the injected auxiliary-\nnoise power.\nIn each phase, the MAML initializer shows\nfaster convergence and a lower early error.\nAfter both\nswitches (60 and 120 s), it reconverges more quickly and\nuses less auxiliary-noise energy than the baseline.\n4.3. Effect of training-set diversity on initialization per-\nformance\nTo study how training-set diversity affects the MAML co-\ninitialization , 4 training sets (A\u2013D) was selcted, each has 3\nprimary-secondary path pairs with different dispersions. For\neach set, the dispersion D was computed separately on pri-\nmary and secondary paths, and the residual error 5 s conver-\ngence was averaged over 5 unseen path pairs from dataset.\nThe 4 sets and their dispersion are summarized in Table 1.\nAs shown in Table 1, more path diversity yields a better\ninitializer. Among the two types of diversity, the secondary-\npath diversity matters most. Sets with large Dsec consistently\nperform better, whereas increasing only the primary-path di-\nTable 1. Effect of training-set diversity on MAML initializa-\ntion (mean noise reduction over 5 unseen conditions).\nTraining set\nDpri (dB)\nDsec (dB)\nReduction (dB)\nA\n6.17\n6.75\n26.1\nB\n0.91\n0.79\n23.2\nC\n5.37\n0.67\n23.7\nD\n1.55\n4.50\n25.8\nversity Dpri brings much smaller gains (set C has high Dpri\nbut low Dsec and improves little). This aligns with OSPM-\nFxLMS, where the filtered-x regressor and online modeling\nboth depend on the secondary path. With a fixed data bud-\nget, prioritize collecting varied secondary paths when build-\ning few-shot training sets.\n5. CONCLUSION\nWe propose a meta-learned co-initialization for feedforward\nFxLMS-based ANC, which enhances performance under\nchanging acoustic environments by pre-training on a small\nset of acoustic paths. The control filter and the secondary-\npath model were jointly initialized through pre-training on\nshort segments, and deployment was carried out by set-\nting the learned initial values while the adaptive loop was\nleft unchanged.\nEffectiveness under path changes was as-\nsessed on an OSPM based FxLMS testbed with measured\nin-ear headphone conditions.\nRelative to a baseline with-\nout re-initialization and with identical updates, lower early-\nstage residual error, shorter time to target, faster recovery\nafter path switches, and reduced probe-noise energy were\nachieved. A path-distribution study indicated that the ben-\nefits of pre-training increased with training-path diversity,\nwith secondary-path diversity exerting the strongest effect.\nThe proposed approach complements classical FxLMS-based\nANC by improving initialization, without modifying the run-\ntime adaptive update rules.\n6. ACKNOWLEDGEMENT\nThis work was supported by the Ministry of Education, Sin-\ngapore, through Academic Research Fund Tier 2 under Grant\nMOE-T2EP50122-0018.\n7. REFERENCES\n[1] Philip A Nelson, \u201cActive control of acoustic fields and\nthe reproduction of sound,\u201d Journal of Sound and Vi-\nbration, vol. 177, no. 4, pp. 447\u2013477, 1994.\n[2] Sen M Kuo and Dennis R Morgan, \u201cActive noise control\nsystem algorithms and dsp implmentations,\u201d 1996.\n[3] Zhengding Luo, Dongyuan Shi, Woon-Seng Gan, and\nQirui Huang, \u201cDelayless generative fixed-filter active\nnoise control based on deep learning and bayesian fil-\nter,\u201d\nIEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 32, pp. 1048\u20131060, 2023.\n[4] Boxiang Wang, Dongyuan Shi, Zhengding Luo, Xiaoyi\nShen, Junwei Ji, and Woon-Seng Gan,\n\u201cTransferable\nselective virtual sensing active noise control technique\nbased on metric learning,\u201d in ICASSP 2025-2025 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2025, pp. 1\u20135.\n[5] Hao Zhang and DeLiang Wang,\n\u201cDeep anc: A deep\nlearning approach to active noise control,\u201d Neural Net-\nworks, vol. 141, pp. 1\u201310, 2021.\n[6] Jihui Zhang, Thushara D. Abhayapala, Wen Zhang,\nPrasanga N. Samarasinghe, and Shouda Jiang, \u201cActive\nnoise control over space: A wave domain approach,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, vol. 26, no. 4, pp. 774\u2013786, 2018.\n[7] Bhan Lam, Dongyuan Shi, Woon-Seng Gan, Stephen J\nElliott, and Masaharu Nishimura,\n\u201cActive control of\nbroadband sound through the open aperture of a full-\nsized domestic window,\u201d Scientific reports, vol. 10, no.\n1, pp. 10021, 2020.\n[8] Ziyi Yang, Shuping Wang, Jiancheng Tao, and Xiaojun\nQiu, \u201cActive control of sound transmission through a\nfloor-level slit,\u201d The Journal of the Acoustical Society of\nAmerica, vol. 154, no. 5, pp. 2746\u20132756, 2023.\n[9] Sen M Kuo, Sohini Mitra, and Woon-Seng Gan, \u201cActive\nnoise control system for headphone applications,\u201d IEEE\nTransactions on Control Systems Technology, vol. 14,\nno. 2, pp. 331\u2013335, 2006.\n[10] Yoshinobu Kajikawa, Woon-Seng Gan, and Sen M.\nKuo, \u201cRecent advances on active noise control: open\nissues and innovative applications,\u201d APSIPA Transac-\ntions on Signal and Information Processing, vol. 1, pp.\ne3, 2012.\n[11] Stephen Elliott,\nSignal processing for active control,\nElsevier, 2000.\n[12] Ming Zhang, Hui Lan, and Wee Ser, \u201cOn comparison\nof online secondary path modeling methods with aux-\niliary noise,\u201d IEEE transactions on speech and audio\nprocessing, vol. 13, no. 4, pp. 618\u2013628, 2005.\n[13] Ming Zhang, Hui Lan, and Wee Ser, \u201cA robust online\nsecondary path modeling method with auxiliary noise\npower scheduling strategy and norm constraint manipu-\nlation,\u201d IEEE Transactions on Speech and Audio Pro-\ncessing, vol. 11, no. 1, pp. 45\u201353, 2003.\n[14] Meiling Hu, Jinpei Xue, and Jing Lu, \u201cOnline multi-\nchannel secondary path modeling in active noise control\nwithout auxiliary noise,\u201d The Journal of the Acousti-\ncal Society of America, vol. 146, no. 4, pp. 2590\u20132595,\n2019.\n[15] Paulo AC Lopes and Jos\u00b4e AB Gerald, \u201cA close to opti-\nmal adaptive filter for sudden system changes,\u201d IEEE\nSignal Processing Letters, vol. 24, no. 11, pp. 1734\u2013\n1738, 2017.\n[16] Feiran Yang, Gerald Enzner, and Jun Yang, \u201cFrequency-\ndomain adaptive kalman filter with fast recovery of\nabrupt echo-path changes,\u201d IEEE Signal Processing Let-\nters, vol. 24, no. 12, pp. 1778\u20131782, 2017.\n[17] Rodrigo P Monteiro, Gabriel A Lima, Jos\u00b4e PG Oliveira,\nDaniel SC Cunha, and Carmelo JA Bastos-Filho, \u201cAc-\ncelerating the convergence of adaptive filters for active\nnoise control using particle swarm optimization,\u201d\nin\n2017 IEEE Latin American Conference on Computa-\ntional Intelligence (LA-CCI). IEEE, 2017, pp. 1\u20136.\n[18] Huiyuan\nSun,\nThushara\nD.\nAbhayapala,\nand\nPrasanga N. Samarasinghe,\n\u201cA realistic multiple\ncircular array system for active noise control over 3d\nspace,\u201d IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 28, pp. 3041\u20133052, 2020.\n[19] Thomas Schumacher, Hauke Kr\u00a8uger, Marco Jeub, Pe-\nter Vary, and Christophe Beaugeant, \u201cActive noise con-\ntrol in headsets: A new approach for broadband feed-\nback anc,\u201d\nin 2011 IEEE International conference\non acoustics, speech and signal processing (ICASSP).\nIEEE, 2011, pp. 417\u2013420.\n[20] Chelsea Finn,\nPieter Abbeel,\nand Sergey Levine,\n\u201cModel-agnostic meta-learning for fast adaptation of\ndeep networks,\u201d in International conference on machine\nlearning. PMLR, 2017, pp. 1126\u20131135.\n[21] Timothy Hospedales, Antreas Antoniou, Paul Micaelli,\nand Amos Storkey, \u201cMeta-learning in neural networks:\nA survey,\u201d IEEE transactions on pattern analysis and\nmachine intelligence, vol. 44, no. 9, pp. 5149\u20135169,\n2021.\n[22] Dongyuan Shi, Woon-Seng Gan, Bhan Lam, and Ken-\nneth Ooi,\n\u201cFast adaptive active noise control based\non modified model-agnostic meta-learning algorithm,\u201d\nIEEE Signal Processing Letters, vol. 28, pp. 593\u2013597,\n2021.\n[23] Stefan Liebich, Johannes Fabry, Peter Jax, and Peter\nVary,\nAcoustic path database for ANC in-ear head-\nphone development, Universit\u00a8atsbibliothek der RWTH\nAachen, 2019.\n[24] Nuggehally S Jayant and Peter Noll, Digital coding of\nwaveforms: principles and applications to speech and\nvideo, vol. 2, Prentice-Hall Englewood Cliffs, NJ, 1984."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary\n\n**Title:** CO-INITIALIZATION OF CONTROL FILTER AND SECONDARY PATH VIA META-LEARNING FOR ACTIVE NOISE CONTROL\n\n**Authors:** Ziyi Yang, Li Rao, Zhengding Luo, Dongyuan Shi, Qirui Huang, Woon-Seng Gan\n\n**Institution:** Smart Nation TRANS Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Key Laboratory of Modern Acoustics, Institute of Acoustics, Nanjing University, Nanjing, China; Northwestern Polytechnical University, Xi\u2019an, China\n\n**Abstract:**\nActive noise control (ANC) systems must adapt quickly to changing acoustic environments. Early performance is largely dictated by initialization. This paper addresses this issue with a Model-Agnostic Meta-Learning (MAML)-based co-initialization approach that jointly sets the control filter and secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer, pre-trained on a small set of measured paths using short two-phase inner loops mimicking identification followed by residual-noise reduction, is applied simply by setting learned initial coefficients. In an online secondary path modeling (OSPM)-based FxLMS testbed, this approach yields lower early-stage error, shorter time-to-target, reduced auxiliary noise energy, and faster recovery after path changes compared to a baseline without re-initialization.\n\n**Main Contributions:**\n1. **Few-shot meta-learned co-initialization for FxLMS-based ANC:** From a small set of measured paths, the initial controller and secondary-path model are jointly learned; at deployment, these learned initial values are simply set while keeping adaptive updates unchanged.\n2. **Fast start and robust adaptation under environment changes:** The initializer reaches target errors sooner, maintains lower early residuals, reduces auxiliary noise energy, and recovers faster after path switches compared to a baseline without re-initialization but with identical updates.\n\n**Methodology:**\nThe paper introduces an online secondary-path modeling (OSPM) scheme based on the cross-updated OSPM approach of [12], augmented by a lightweight error-jump detector for re-initialization. The inner loop comprises phases A and B, where Phase A updates the secondary path using identification outputs and Phase B updates the control filter using filtered reference regressors. Meta-learning is applied to update meta-parameters \u03a6 (control filter initialization) and \u03a8 (secondary-path initialization). Training segments are drawn from input tracks, followed by validation on separate segments with across-task meta-updates.\n\n**Key Findings:**\n1. The proposed co-initialization approach yields significant improvements in early error reduction, shorter time-to-target, reduced auxiliary noise energy, and faster recovery after path changes compared to a baseline without re-initialization.\n2. Training set diversity positively impacts initialization performance; secondary-path diversity has the strongest effect.\n\n**Conclusions:**\nThe paper proposes an effective meta-learned co-initialization for FxLMS-based ANC that enhances system performance under changing acoustic environments by pre-training on a small set of acoustic paths. The approach provides fast start and robust adaptation, complementing classical FxLMS-based ANC without modifying runtime adaptive update rules.\n\n**References:**\nThe references list includes seminal works in active noise control (ANC), including recent advances, innovative applications, and foundational research in the field."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "2026-1-21\nWhich Reasoning Trajectories Teach Students\nto Reason Better? A Simple Metric of\nInformative Alignment\nYuming Yang1,2, Mingyoung Lai3, Wanxu Zhao1, Xiaoran Fan1, Zhiheng Xi1, Mingqi Wu1, Chiyue\nHuang4, Jun Zhao1, Haijun Lv2, Jian Tong2, Yunhua Zhou2, Yicheng Zou2,\u2020, Qipeng Guo2, Tao Gui1,\nQi Zhang1,\u2020 and Xuanjing Huang1\n1Fudan University, 2Shanghai AI Laboratory, 3University of Toronto, 4University of Sydney\nLong chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher\nto student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers\ndo not necessarily yield better students, highlighting the importance of data\u2013student suitability in distillation.\nExisting methods assess suitability primarily through student likelihood, favoring trajectories that closely\nalign with the model\u2019s current behavior but overlooking more informative ones. Addressing this, we propose\nRank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the\nsuitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically\ncombine low absolute probability with relatively high-ranked tokens under the student model, balancing\nlearning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory\u2019s\naverage token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret.\nAcross five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with\npost-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate\nits practical utility in both trajectory selection and teacher selection.\n1. Introduction\nRecent advances in reasoning-oriented large language models (LLMs) are largely driven by their ability to\ngenerate long chain-of-thought (CoT) trajectories [12, 48]. Beyond enabling complex inference at test time,\nsuch trajectories also provide powerful supervision signals for training student models [28, 35] or cold-starting\nreinforcement learning [39] through supervised fine-tuning (SFT).\nYet, stronger reasoning teachers do not necessarily yield better students [11, 22]. Our extensive experiments\nshow that the post-training effectiveness of reasoning trajectories varies substantially across student models,\nindicating that the suitability between data and student is critical for effective learning. Existing data engineering\nmethods assess data suitability primarily through the student\u2019s probability assignments [18, 47], favoring\nhigh-likelihood trajectories that align closely with the model\u2019s current behavior. Such trajectories, however,\noften provide limited new learning signals. In contrast, more informative trajectories are typically less familiar\nto the student and thus overlooked by these methods. This leads to a fundamental Informative Alignment\nchallenge: how to identify reasoning data that are both well aligned with the student and sufficiently\ninformative?\nTo address this challenge, we propose a simple yet effective metric, Rank-Surprisal Ratio (RSR), which\nquantifies the suitability of a reasoning trajectory for a given student by jointly capturing alignment and\ninformativeness. Motivated by our preliminary analysis, we argue that the dilemma between providing\nnew signals and aligning with student\u2019s existing behavior can be resolved by trajectories exhibiting absolute\nunfamiliarity and relative familiarity. Concretely, effective trajectories should deviate from the student\u2019s own\ngenerations, receiving low absolute probability under the student model, while still containing behavioral\n\u2020 Corresponding authors. Inquiries may be sent to: yumingyang23@m.fudan.edu.cn, zouyicheng@pjlab.org.cn, qz@fudan.edu.cn\n* Code is available at https://github.com/UmeanNever/RankSurprisalRatio.\narXiv:2601.14249v1  [cs.CL]  20 Jan 2026\nWhich Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment\npatterns within the student\u2019s prior experience, such that their tokens rank relatively high in the model\u2019s\nprediction distribution over the vocabulary (Figure 1).\n\u2026According to this formula, the mean is 32.  ___\n_Therefore\n_So\n_Alternatively\n_Wait _Admittedly\n\u2026\n_Actually\nHigh Ranking\nInformativeness\nAlignment\n_We\nSuitable Trajectories\nStudent\nModel\nProbability\nLow Probability\nTeacher\nTrajectory\nFigure 1: Illustration of the intuition behind RSR.\nSuitable reasoning trajectories should balance infor-\nmativeness and alignment by having low absolute\nprobability but relatively high-ranked tokens under\nthe student model.\nBased on this insight and consistent numerical pat-\nterns observed in simulation studies, we define our\nsuitability metric, Rank-Surprisal Ratio, as the ratio be-\ntween a trajectory\u2019s average token-wise rank1 and its\naverage negative log-likelihood (surprisal). RSR can\nbe computed with a single forward pass, requires no\nadditional verifier or test data, and is straightforward\nto interpret. Lower RSR indicates better informative\nalignment, identifying trajectories that are both infor-\nmative and well aligned with the student.\nWe validate the effectiveness of Rank-Surprisal Ra-\ntio through correlation analyses on 5 student LLMs\nusing math reasoning trajectories generated by 11 rep-\nresentative teacher models. Across all students, the\nRSR of trajectories exhibits a strong correlation with\npost-training performance, achieving an average Spear-\nman correlation of 0.86 and consistently outperform-\ning alternative metrics. Furthermore, to explore its\npractical value in data engineering, we apply RSR to\ntrajectory selection and teacher selection. Our experi-\nments show that RSR not only selects more effective training trajectories for each problem from candidates\ngenerated by diverse teachers, but also identifies more suitable teacher models using only a small amount of\ndata, consistently outperforming existing selection methods across all five students in both settings.\nOur main contributions are three-fold:\n\u2022 We present a systematic distillation study across a wide range of teacher and student models, showing\nthat the effectiveness of reasoning trajectories differs across students and highlighting the importance of\ndata\u2013student suitability.\n\u2022 We propose Rank-Surprisal Ratio, a simple metric that quantifies the suitability of a reasoning trajectory\nby jointly capturing alignment and informativeness, achieving a strong correlation with post-training\nperformance.\n\u2022 We demonstrate the practical utility of Rank-Surprisal Ratio in two data engineering scenarios, trajectory\nselection and teacher selection, where it serves as an effective criterion and outperforms existing methods.\n2. The Need for Student-Specific Data\nTo understand which types of reasoning trajectories most effectively improve student models after SFT, we\nconduct a comprehensive large-scale study involving five widely adopted student models and eleven diverse\nreasoning-oriented teacher models, yielding 55 teacher\u2013student pairings. We perform SFT experiments for\neach of these pairs.\n2.1. Experimental Settings\nOur teacher-student pairing study involves two major steps: (1) For each teacher model, we prompt it to\ngenerate a long CoT response for each math problem in our 5000-problem set (see \u00a7 A.1), forming a trajectory\ndataset specific to that teacher. (2) For each teacher\u2013student pair, we fine-tune the student model on the\ncorresponding teacher dataset and evaluate its reasoning performance. All students are pretrained base models.\nTo reduce variance induced by stochastic trajectory sampling, we perform three independent generation runs\nfor each teacher and conduct SFT separately on each resulting dataset for every teacher\u2013student pair. Reported\nresults are averaged over these three runs. More implementation details are provided in the appendix.\n1Higher-ranked tokens have lower rank values.\n2\nWhich Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment\nTeacher Models\nParams\nStudent Models (Base)\nTeacher\nPerformance\nQwen-3-14B\nLLaMA-3.1-8B\nQwen-2.5-7B\nQwen-3-4B\nQwen-2.5-3B\nDeepseek-R1\n671B\n77.1\n28.1\n47.3\n55.8\n29.6\n91.1\nQwen-3-235B-Thinking\n235B\n71.8\n22.0\n45.0\n53.4\n26.4\n91.2\nGPT-OSS-120B\n120B\n66.7\n15.2\n40.7\n47.9\n22.9\n88.3\nNemotron-Super\n49B\n72.2\n23.7\n48.3\n56.4\n33.0\n82.3\nQwQ-32B\n32B\n77.4\n27.1\n52.0\n61.2\n33.0\n85.2\nQwen-3-30B-Thinking\n30B\n77.2\n26.7\n50.0\n58.8\n31.2\n92.3\nMagistral-Small\n24B\n68.8\n22.8\n47.6\n52.2\n30.6\n71.0\nGPT-OSS-20B\n20B\n69.5\n17.9\n42.7\n48.4\n24.4\n83.4\nPhi-4-Reasoning-Plus\n14B\n54.1\n14.5\n35.2\n40.2\n18.2\n72.7\nQwen-3-8B\n8B\n74.6\n26.5\n52.0\n61.2\n34.2\n82.5\nQwen-3-4B-Thinking\n4B\n76.8\n28.2\n51.8\n61.9\n33.3\n87.3\nTable 1: Distillation results showing post-training reasoning performance of student models trained on trajecto-\nries from different teacher models, evaluated by average Acc@4 on AIME\u201925, AIME\u201924, AMC\u201923, and MATH500.\nDarker and lighter shading indicate the best and second-best results, respectively. Student performance varies\nsignificantly across teacher\u2013student pairs, highlighting the importance of data\u2013student suitability.\nTeachers\nWe use 11 reasoning LLMs spanning 4B to 671B parameters across multiple model families,\nincluding GPT-OSS [3], DeepSeek [12], Qwen [39], LLaMA-Nemotron [4], and Phi [1].\nBenchmarks\nWe evaluate the reasoning performance of the fine-tuned student models on four standard\nmath benchmarks: AIME\u201925, AIME\u201924, AMC\u201923, and MATH500 [14] using the Acc@4 metric, and report results\naveraged across all benchmarks.\n2.2. Results\nTable 1 presents the results of our teacher\u2013student pairing distillation study, revealing that:\nStronger teachers do not necessarily produce better students.\nTeacher capability, whether measured by\nparameter scale or reasoning performance, does not reliably predict student improvement. For example, the\n671B and 235B models often underperform smaller teachers such as QwQ-32B on multiple students. Similarly,\nteachers with strong reasoning performance do not consistently yield the best outcomes for all student models.\nData\u2013student suitability is critical for eliciting reasoning improvements.\nThe effectiveness of teacher\ntrajectories is highly student-specific and depends critically on their suitability for the student model. Pairing\nstrong teachers (Deepseek-R1) with much weaker students (Qwen-2.5-3B) often fails to yield strong per-\nformance, while weaker teachers (Qwen-3-4B-Thinking) may likewise be ineffective at improving stronger\nstudents (Qwen-3-14B). Moreover, teachers from distant model families (GPT-OSS) often lead to inferior results,\nsuggesting that unfamiliar reasoning patterns are harder for students to absorb. Overall, we find no simple\nteacher\u2013student pairing rule based on surface attributes such as parameter scale or model family, indicating\nthat reasoning data suitability is a nuanced property requiring deeper investigation.\n3. Measuring Data-Student Suitability\nIn this section, we explore metrics for measuring data\u2013student suitability, with the goal of jointly capturing\ninformativeness and alignment. We begin by introducing two fundamental token-level measures: surprisal and\nrank (\u00a7 3.1), and analyzing the limitations of existing probability-based metrics (\u00a7 3.2). We then abstract our\ninsights on suitable reasoning data and conduct simulation studies to identify quantitative patterns (\u00a7 3.3).\nFinally, we propose our trajectory-level metric (\u00a7 3.4).\n3.1. Surprisal and Rank\nWe introduce two different methods to quantify the amount of information carried by a token with respect to a\nstudent model. They serve as building blocks of our metric.\nSurprisal (Negative Log-Likelihood)\nA common measure is based on the probability of generating the\n3\nWhich Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment\nTeacher Models\nStudent\nPerformance \u2191\nProbability-based Metrics \u2193\nRank-Surprisal Metrics \u2193\nAvg-Surprisal\nAvg-Surplocal\nAvg-RSRtoken\nAvg-RSRfilter\ntoken\nRSR (Ours)\nQwen-3-8B\n52.0\n0.65\n1.16\n2.01\u00d7107\n3.15\n2.89\nQwen-3-30B-Thinking\n50.0\n0.77\n1.27\n2.25\u00d7107\n3.47\n2.95\nNemotron-Super\n48.3\n0.60\n1.04\n5.51\u00d7107\n3.95\n3.08\nDeepseek-R1\n47.8\n0.83\n1.35\n2.98\u00d7107\n3.31\n3.00\nMagistral-Small\n47.6\n0.55\n1.03\n3.58\u00d7107\n4.38\n3.09\nGPT-OSS-20B\n42.7\n1.36\n1.78\n5.15\u00d7106\n11.10\n3.83\nTable 2: Comparison of the student\u2019s post-training performance and data\u2013student suitability metrics across\ntrajectories from different teacher models, evaluated on Qwen-2.5-7B. Darker shading indicates higher perfor-\nmance or better suitability. Metrics whose trends align with performance (e.g., RSR) provide more reliable\nsuitability estimates. Complete metric scores are provided in \u00a7 C.2.\ncurrent token \ud835\udc61\ud835\udc58given its preceding context c\ud835\udc58= (\ud835\udc611, . . . , \ud835\udc61\ud835\udc58\u22121) under the student model \ud835\udf03. For numerical\nstability, probabilities are typically transformed into the log space, and the negative log-likelihood is used as a\nmeasure of informativeness, also known as Surprisal [13]\nSurprisal(\ud835\udc61\ud835\udc58) = \u2212log \ud835\udc5d\ud835\udf03\n(\ufe00\n\ud835\udc61\ud835\udc58| c\ud835\udc58\n)\ufe00\n(1)\nRank\nAnother method considers the rank of the current token within the model\u2019s prediction distribution\nover the vocabulary \ud835\udcb1. Formally, given the conditional distribution \ud835\udc5d\ud835\udf03(\u00b7 | c\ud835\udc58), the rank of token \ud835\udc61\ud835\udc58is defined as\nthe number of tokens with strictly higher probability [32].\nRank(\ud835\udc61\ud835\udc58) = 1 +\n\u2211\ufe01\n\ud835\udc61\u2032\u2208\ud835\udcb1\nI[\ud835\udc5d\ud835\udf03(\ud835\udc61\u2032 | c\ud835\udc58) > \ud835\udc5d\ud835\udf03(\ud835\udc61\ud835\udc58| c\ud835\udc58)]\n(2)\nUnlike surprisal, rank captures relative familiarity of the token by comparing target token against alternative\ncandidates, revealing signals overlooked by probability-based measures. For instance, a token may receive low\nabsolute probability while still being among the top-ranked candidates.\n3.2. Limitations of Probability-Based Metrics\nExisting work primarily relies on surprisal or log-probability to assess data suitability. For example, Zhang et\nal. [47] selects trajectories based on the average log-probability of response tokens under the student model.\nSince surprisal is the negation of log-probability, we implement this metric as the average surprisal, denoted as\nAvg-Surprisal. More recently, Just et al. [18] computes token-level log-probability based on a local context\nclocal\n\ud835\udc58\n(several preceding sentences), which we implement as average local surprisal (Avg-Surplocal). Under these\nmetrics, trajectories with lower surprisal are considered more suitable.\nHowever, as shown in Table 2, lower surprisal (i.e., higher probability) does not necessarily lead to better post-\ntraining reasoning performance. Both Avg-Surprisal and Avg-Surplocal assign lower surprisal to trajectories from\nNemotron-Super and Magistral-Small, yet training on such trajectories fails to improve reasoning performance.\nSimilar patterns are observed across all student models, indicating that probability-based metrics tend to favor\ndata that are familiar but insufficiently informative.\nAt the other extreme, trajectories with very high surprisal (e.g., GPT-OSS-20B) also perform poorly. In\ncontrast, trajectories with moderate surprisal values (e.g., Qwen-3-8B) achieve better results. This observation\nmotivates us to investigate the mechanism underlying the surprisal trade-off.\n3.3. Insight and Simulation\nThe above analysis suggests that effective teacher trajectories should balance the informativeness of the data\nwith their alignment to the student\u2019s current behavior: they should be neither overly similar to the student\u2019s\nown generations nor excessively deviant from its predicted distribution.\n4\nWhich Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment\n0\n5\n10\n15\n20\n25\nRank\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nProbability\nZA\nZB\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nRank / Surprisal\nRSRtoken\nFigure 2: Simulation of the student model\u2019s token-level\nprediction distribution \ud835\udc4d. Tokens with low probabil-\nity but relatively high rank\u2014characteristic of the \ud835\udc4d\ud835\udc35\nmode\u2014yield smaller token-level rank\u2013surprisal ratios.\nTrajectories\nProb.\nSurprisal\nRank\nRSRtoken\n\ud835\udc4b\ud835\udc34(from \ud835\udc4d\ud835\udc34)\n0.41\n1.38\n2.49\n1.69\n\ud835\udc4b\ud835\udc35(from \ud835\udc4d\ud835\udc35)\n0.10\n2.73\n4.31\n1.30\n\ud835\udc4b\ud835\udc36(from \ud835\udc4d\ud835\udc36)\n0.08\n4.73\n11.57\n2.23\n\ud835\udc4b\ud835\udc37(from \ud835\udc4d)\n0.35\n1.67\n2.93\n1.62\nTable 3: Simulation results for different types of tra-\njectories, reporting average token probability, sur-\nprisal, rank, and RSRtoken. RSRtoken shows promise\nas a reliable metric for identifying suitable reason-\ning trajectories (\u00a7 3.4).\nAt first glance, this balance may appear to pose a dilemma. However, we argue that it can be resolved\nby viewing informativeness and alignment through the lens of absolute unfamiliarity and relative familiarity.\nInformativeness does not require trajectories to be entirely unfamiliar; rather, it suffices that they deviate from\nthe dominant patterns and thus have low absolute probability of being generated by the student. Conversely,\nalignment does not require exact agreement with the student\u2019s outputs, but instead that the corresponding\ntokens have relatively higher likelihoods than other candidates in the vocabulary.\nBuilding on this insight, we propose that effective reasoning trajectories should deviate from the student\u2019s own\ngenerations while still exhibiting behavioral patterns within the student\u2019s prior experience. Therefore, tokens in\nsuch trajectories are assigned low absolute probability (high surprisal) by the student model while still ranking\nrelatively high (low rank values) in its prediction distribution.\nTo validate these quantitative patterns and identify features that characterize effective learning trajectories,\nwe conduct a simulation study.\nSimulation Setting\nWe simulate the student model\u2019s token-level prediction distribution for reasoning\ntrajectories and examine the numerical patterns exhibited by teacher trajectories that we consider effective.\nSpecifically, we model the student\u2019s prediction distribution as a bimodal distribution over the vocabulary \ud835\udcb1. The\nfirst mode, denoted as \ud835\udc4d\ud835\udc34, represents tokens that follow the student\u2019s dominant patterns, which arise from\nstudent\u2019s massive general training data. The second mode \ud835\udc4d\ud835\udc35, represents tokens whose patterns resemble\nstrong reasoning trajectories, reflecting the student\u2019s prior exposure to relevant data. We instantiate both \ud835\udc4d\ud835\udc34\nand \ud835\udc4d\ud835\udc35as Zipf distributions [26, 52] over \ud835\udcb1. Then the student\u2019s overall token-level prediction distribution \ud835\udc4dis\nconstructed as a mixture of \ud835\udc4d\ud835\udc34and \ud835\udc4d\ud835\udc35:\n\ud835\udc4d= \ud835\udf0b\ud835\udc4d\ud835\udc34+ (1 \u2212\ud835\udf0b) \ud835\udc4d\ud835\udc35,\n\ud835\udc4d\ud835\udc34, \ud835\udc4d\ud835\udc35\u223cZipf(\ud835\udefc),\n(3)\nwhere \ud835\udf0b=\n\ud835\udc40\ud835\udc34\n\ud835\udc40\ud835\udc34+\ud835\udc40\ud835\udc35and \ud835\udc40\ud835\udc34> \ud835\udc40\ud835\udc35.\nBased on \ud835\udc4d, we simulate four types of reasoning trajectories and examine their token-level surprisal and\nrank: (i) \ud835\udc4b\ud835\udc34, sampled from \ud835\udc4d\ud835\udc34, representing trajectories that closely follow the student\u2019s dominant patterns;\n(ii) \ud835\udc4b\ud835\udc35, sampled from \ud835\udc4d\ud835\udc35, representing trajectories that deviate from the dominant mode while aligning with\ncertain minor patterns within the student; (iii) \ud835\udc4b\ud835\udc36, sampled from a distribution distinct from both \ud835\udc4d\ud835\udc34and \ud835\udc4d\ud835\udc35,\nrepresenting misaligned trajectories; and (iv) \ud835\udc4b\ud835\udc37, sampled from \ud835\udc4d, representing trajectories that reflect the\nstudent\u2019s overall predictive behavior. More simulation details are provided in \u00a7 A.6.\nSimulation Results\nFigure 2 presents the simulated bimodal distribution \ud835\udc4d, where effective reasoning\ntrajectories are expected to align with the distribution of \ud835\udc4d\ud835\udc35, and thus resemble \ud835\udc4b\ud835\udc35. As shown in Table 3,\n\ud835\udc4b\ud835\udc35exhibits much higher average surprisal (lower absolute probability) than \ud835\udc4b\ud835\udc34and \ud835\udc4b\ud835\udc37, while maintaining\nrelatively low rank values (higher-ranked), consistent with the quantitative patterns implied by our analysis.\nCrucially, the results suggest a promising direction for measuring data suitability, which we formalize in \u00a7 3.4.\n5\nWhich Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment\n3.4. Proposed Metric: Rank-Surprisal Ratio\nToken-Level Metric\nMotivated by the complementary properties of surprisal and rank, we explore metrics\nthat combine these two signals to jointly capture informativeness and alignment. One promising choice is the\ntoken-level ratio of rank to surprisal, which captures the relative relationship between the two signals and is\ndenoted as RSRtoken:\nRSRtoken(\ud835\udc61\ud835\udc58) =\nRank(\ud835\udc61\ud835\udc58)\nSurprisal(\ud835\udc61\ud835\udc58).\n(4)\nTable 3 shows that trajectories of the preferred type \ud835\udc4b\ud835\udc35achieve the lowest average RSRtoken (1.30), whereas\nthe misaligned \ud835\udc4b\ud835\udc36achieve the highest, suggesting that this ratio may be a reliable indicator for identifying\neffective reasoning trajectories that balance informativeness and alignment.\nFrom Token-Level to Trajectory-Level\nWhile RSRtoken shows encouraging behavior in simulation, directly\napplying this token-level ratio to assess the overall suitability of a real trajectory presents non-trivial challenges.\nIn particular, naively averaging RSRtoken over all response tokens often leads to large and unstable values\n(Table 2). This instability stems from tokens that receive extremely high probabilities under certain contexts,\nyielding near-zero surprisal and unbounded ratios that dominate the trajectory-level average.\nA natural solution is to exclude tokens with very low surprisal when computing the average. Let \ud835\udcaf\ud835\udc3b(x)\ndenote the set of response tokens whose surprisal lies in the top \ud835\udc3b% within a trajectory x. We define a filtered\naverage as\nAvg-RSRfilter\ntoken(x) =\n\u2211\ufe00\n\ud835\udc61\ud835\udc58\u2208\ud835\udcaf\ud835\udc3b(x) RSRtoken(\ud835\udc61\ud835\udc58)\n|\ud835\udcaf\ud835\udc3b(x)|\n(5)\nEmpirically, we find that using the top 30% highest-surprisal tokens yields stronger correlation with post-\ntraining performance (Table 2). This suggests that tokens with higher surprisal have a greater impact on\nstudent learning and should be emphasized when computing the average.\nAccordingly, instead of hard filtering, we adopt a surprisal-weighted average of the token-level ratios. A\nsimple derivation shows that this weighted average is equivalent to a trajectory-level ratio between the sum of\ntoken ranks and the sum of token surprisals. For brevity, we denote \ud835\udc5f\ud835\udc58= Rank(\ud835\udc61\ud835\udc58) and \ud835\udc60\ud835\udc58= Surprisal(\ud835\udc61\ud835\udc58):\n\u2211\ufe00\n\ud835\udc58\ud835\udc60\ud835\udc58RSRtoken(\ud835\udc61\ud835\udc58)\n\u2211\ufe00\n\ud835\udc58\ud835\udc60\ud835\udc58\n=\n\u2211\ufe00\n\ud835\udc58\ud835\udc60\ud835\udc58\n\ud835\udc5f\ud835\udc58\n\ud835\udc60\ud835\udc58\n\u2211\ufe00\n\ud835\udc58\ud835\udc60\ud835\udc58\n=\n\u2211\ufe00\n\ud835\udc58Rank(\ud835\udc61\ud835\udc58)\n\u2211\ufe00\n\ud835\udc58Surprisal(\ud835\udc61\ud835\udc58)\n(6)\nThe resulting metric yields a concise form that can be interpreted as the ratio of average rank to average\nsurprisal at the trajectory level.\nIn practice, we further observe that extremely unfamiliar tokens can attain very large rank values due to\nthe large vocabulary size, which also leads to numerical instability. Since tokens with excessively large ranks\nare effectively indistinguishable from the student\u2019s perspective, we clip rank values at a threshold \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65. Thus,\nwe define our final trajectory-level metric, Rank-Surprisal Ratio (RSR), as\nRSR(x) =\n\u2211\ufe00\n\ud835\udc58min\n(\ufe00\nRank(\ud835\udc61\ud835\udc58), \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65\n)\ufe00\n\u2211\ufe00\n\ud835\udc58Surprisal(\ud835\udc61\ud835\udc58)\n(7)\nInterpretation\nOur metric admits a simple interpretation. The numerator, Rank, captures relative familiarity:\nlower rank values indicate that tokens in this trajectory are preferred among alternative candidates by the student\nand align with the model\u2019s existing behavior. The denominator, Surprisal, captures absolute unfamiliarity:\nhigher surprisal indicates deviation from dominant patterns and provides informative learning signals. A\nlower RSR therefore identifies trajectories that better balance alignment and informativeness, corresponding to\neffective reasoning supervision.\n4. Correlation Analysis\nPreliminary results in Table 2 have shown that Rank-Surprisal Ratio aligns well with post-training reasoning\nperformance. To provide a more rigorous evaluation and further demonstrate the effectiveness of RSR in\nmeasuring data-student suitability, we conduct comprehensive correlation analyses.\n6\nWhich Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment\nMetrics\n(Absolute) Spearman Correlation with Post-Training Performance\nQwen-3-14B\nLLaMA-3.1-8B\nQwen-2.5-7B\nQwen-3-4B\nQwen-2.5-3B\nAverage\nStudent-\nAgnostic\nTeacher Params\n0.04\n0.34\n0.2\n0.02\n0.26\n0.01\nAvg-Token Length\n0.49\n0.68\n0.45\n0.57\n0.47\n0.53\nTeacher Performance\n0.49\n0.34\n0.13\n0.23\n0.03\n0.23\nVerified Accuracy\n0.54\n0.43\n0.25\n0.35\n0.10\n0.33\nLLM-judged Quality\n0.61\n0.52\n0.46\n0.61\n0.40\n0.52\nRule-based Quality\n0.55\n0.56\n0.75\n0.65\n0.75\n0.65\nStudent-\nSpecific\nAvg-Surprisal\n0.24\n0.42\n0.55\n0.55\n0.70\n0.49\nAvg-Surplocal\n0.31\n0.40\n0.54\n0.59\n0.72\n0.51\nAvg-Rank\n0.41\n0.64\n0.68\n0.61\n0.62\n0.59\nInfluence Score\n0.52\n0.19\n0.32\n0.47\n0.59\n0.11\nG-Norm\n0.44\n0.54\n0.51\n0.57\n0.70\n0.55\nGRACE\n0.25\n0.58\n0.66\n0.75\n0.69\n0.59\nRank-Surprisal Ratio\n0.85\n0.85\n0.92\n0.82\n0.85\n0.86\nTable 4: Spearman correlation between different metrics and post-training reasoning performance across\nstudent models, reporting absolute values. \"Student-Agnostic\" metrics are computed independently of the\nspecific student model. Our metric, Rank-Surprisal Ratio, achieves the highest correlation across all students.\n4.1. Main Analysis\nFor each of the five student models, we compute average trajectory-level suitability (or quality) scores for\nreasoning datasets generated by eleven teacher models (\u00a7 2.1) using different metrics. We then measure the\ncorrelation between these dataset-level scores and the student\u2019s average post-training reasoning performance on\nthe corresponding teacher data. We primarily report Spearman\u2019s correlation coefficient, as Pearson correlation\nexhibits similar trends. We use a clipping threshold \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65= 100 for our RSR metric in all subsequent experiments.\nAdditional analysis details are provided in \u00a7 A.7, and complete metric scores are reported in \u00a7 C.2.\nCompared Metrics\nWe compare RSR against a diverse set of metrics for evaluating reasoning trajectories.\nThese include previously discussed teacher-side indicators (e.g., teacher model performance), basic statistics\nsuch as token length, as well as probability-based metrics (e.g., average surprisal and local surprisal; [18])\nand rank-based metrics. We also consider commonly used trajectory quality measures, such as rule-based\nquality scores derived from word frequency [43], LLM-judged quality scores, and answer accuracy on verifiable\nquestions. In addition, we include recent student-specific data suitability metrics, including gradient-based\nscores (G-Norm and GRACE; [29]) and influence scores [16].\nResults\nTable 4 shows that Rank-Surprisal Ratio consistently exhibits strong correlation with post-training\nreasoning performance across all student models, achieving an average Spearman correlation of 0.86 and\noutperforming all alternative metrics. These results indicate the effectiveness and practical value of RSR. In\ncontrast, surprisal-based and rank-based metrics alone yield substantially weaker correlations (at most 0.59),\nhighlighting the importance of capturing both informativeness and alignment through the rank-surprisal ratio.\n4.2. Ablation Study\nVariants\nAvg. Corr.\n\u2206\nRank-Surprisal Ratio (\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65= 100)\n0.856\nNo rank clipping\n0.700\n\u22120.156\nNo weighted avg. (Avg-RSRtoken)\n0.391\n\u22120.465\nRank clipping: \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65= 50\n0.696\n\u22120.160\nRank clipping: \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65= 500\n0.822\n\u22120.034\nFiltered average (Avg-RSRfilter\ntoken)\n0.793\n\u22120.064\nReduced sample size (200)\n0.864\n0.007\nTable 5: Ablation study for Rank-Surprisal Ratio. \u2206\ndenotes the change in average correlation.\nThe derivation of Rank-Surprisal Ratio involves sev-\neral design components as well as a hyperparameter\n\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65. We conduct an ablation study to examine how\nthese choices affect correlation performance.\nAs shown in Table 5, removing either rank clip-\nping or the surprisal-weighted averaging substan-\ntially degrades the correlation, validating the ne-\ncessity of both components in our metric. In addi-\ntion, the \u201cReduced sample size\u201d setting estimates\nthe dataset-level RSR using only 200 trajectories per\nteacher instead of the full 5,000. The comparable\ncorrelations observed under reduced sample size\n7\nWhich Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment\nSelection Methods\nQwen-3-14B\nL3.1-8B\nQ2.5-7B\nQ3-4B\nQ2.5-3B\nAIME24\nAIME25\nAMC23\nMATH500\nAvg.\nAvg.\nAvg.\nAvg.\nAvg.\nRandom\n59.2\n46.7\n86.2\n88.6\n70.2\n22.1\n45.7\n53.9\n27.9\nToken Length\ud835\udc5a\ud835\udc4e\ud835\udc65\n61.7\n51.7\n87.5\n84.8\n71.4\n27.3\n45.4\n51.3\n27.1\nRule-based Quality\ud835\udc5a\ud835\udc4e\ud835\udc65\n58.3\n47.5\n91.3\n92.0\n72.3\n25.8\n51.6\n58.0\n31.2\nLLM-judged Quality\ud835\udc5a\ud835\udc4e\ud835\udc65\n60.0\n49.2\n90.6\n93.6\n73.4\n25.6\n51.8\n59.1\n32.8\nSurprisal\ud835\udc5a\ud835\udc56\ud835\udc5b\n62.5\n50.0\n88.1\n92.8\n73.4\n23.5\n46.4\n53.3\n28.9\nG-Norm\ud835\udc5a\ud835\udc56\ud835\udc5b\n59.2\n50.0\n89.4\n92.4\n72.7\n26.1\n49.5\n59.1\n30.9\nRank-Surprisal Ratio\ud835\udc5a\ud835\udc56\ud835\udc5b\n67.5\n59.2\n93.1\n94.6\n78.6\n28.5\n53.2\n61.4\n34.8\nTable 6: Trajectory selection results showing post-training reasoning performance of student models trained on\ndatasets selected by different methods. \ud835\udc5a\ud835\udc4e\ud835\udc65and \ud835\udc5a\ud835\udc56\ud835\udc5bindicate maximizing or minimizing the corresponding\nmetric. Model names are abbreviated as Q for Qwen and L for LLaMA. Additional results, including GPQA\nevaluation, are provided in \u00a7 C.4.\nand alternative hyperparameter settings (e.g., \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65= 500) indicate that RSR is robust to both data scarcity\nand reasonable variations in \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65. More ablation results are provided in \u00a7 C.3.\n5. Practical Applications\nGiven the reliable data-student suitability estimation provided by Rank-Surprisal Ratio and its strong correlation\nwith post-training performance, we further examine its practical value as a data selection criterion in two\nrepresentative scenarios.\n5.1. Trajectory Selection\nExperimental Setting\nThe trajectory selection task aims to identify the most effective reasoning trajectory\nfrom a set of candidates for a given problem or prompt. We adopt a 33-to-1 setting, where each candidate\npool contains 33 trajectories generated by 11 teacher models (3 per teacher; see \u00a7 2.1), and one trajectory is\nselected according to the selection criterion. This procedure is repeated for all 5,000 training problems and all\nstudent models, yielding student-specific 5,000-trajectory teacher datasets for each selection method. We then\nfine-tune student models on the constructed datasets and evaluate their post-training reasoning performance\non standard math benchmarks, consistent with previous experiments. We compare RSR against a random\nselection baseline and multiple previously discussed metrics. For metric-based methods, candidate trajectories\nare scored and selected by either maximizing or minimizing the score.\nResults\nAs shown in Table 6, datasets selected by Rank-Surprisal Ratio consistently achieve the best post-\ntraining reasoning performance among all selection methods across student models, demonstrating the effec-\ntiveness of RSR in identifying suitable trajectories. Moreover, the performance achieved by RSR is comparable\nto, and for four students even surpasses, the best performance of any single teacher for each student model\n(Table 1), which serves as a strong upper bound obtained via brute-force search over teacher datasets. These\nresults underscore the practical value of RSR for selecting effective trajectories prior to training.\nAdditional trajectory selection results, including complete scores, further analysis, GPQA evaluation, and\nexperiments under a reduced-teacher setting, are provided in \u00a7 C.4.\n5.2. Teacher Selection\nExperimental Setting\nThe teacher selection task aims to identify the most suitable teacher model for\ngenerating reasoning trajectories to train a given student. We consider a low-resource setting in which\ngenerating full training data for every teacher model is infeasible. Instead, we sample a small set of 200\ntrajectories from each candidate teacher, score them using different metrics, and select the teacher model based\non the average trajectory-level score. We use 6 diverse teacher models (Deepseek-R1, Qwen-3-235B-Thinking,\nNemotron-Super, Qwen-3-30B-Thinking, Magistral-Small, and GPT-OSS-20B) as the candidate pool, avoiding\nconsistently well-performing teachers to ensure a non-trivial selection task.\n8\nWhich Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment\nSelection Methods\nQwen-3-14B\nLLaMA-3.1-8B\nQwen-2.5-7B\nQwen-3-4B\nQwen-2.5-3B\nAvg.\nTop-1\nTop-2\nTop-1\nTop-2\nTop-1\nTop-2\nTop-1\nTop-2\nTop-1\nTop-2\nTeacher Params\ud835\udc5a\ud835\udc4e\ud835\udc65\n77.1\n71.8\n28.1\n22.0\n47.8\n45.0\n55.8\n53.4\n29.6\n26.4\n45.7\nToken Length\ud835\udc5a\ud835\udc4e\ud835\udc65\n71.8\n77.1\n22.0\n28.1\n45.0\n47.8\n53.4\n55.8\n26.4\n29.6\n45.7\nRule-based Quality\ud835\udc5a\ud835\udc4e\ud835\udc65\n72.2\n77.1\n23.7\n28.1\n48.2\n47.8\n56.4\n55.8\n33.0\n29.6\n47.2\nLLM-judged Quality\ud835\udc5a\ud835\udc4e\ud835\udc65\n71.8\n77.2\n22.0\n26.7\n45.0\n50.0\n53.4\n58.8\n26.4\n31.2\n46.3\nSurprisal\ud835\udc5a\ud835\udc56\ud835\udc5b\n68.8\n72.2\n22.8\n23.7\n47.6\n48.2\n52.2\n56.4\n30.6\n33.0\n45.6\nGRACE\ud835\udc5a\ud835\udc56\ud835\udc5b\n72.2\n68.8\n22.8\n28.1\n47.6\n48.3\n52.2\n58.8\n30.6\n26.4\n45.6\nRank-Surprisal Ratio\ud835\udc5a\ud835\udc56\ud835\udc5b\n77.2\n77.1\n26.7\n28.1\n50.0\n47.8\n58.8\n55.8\n31.2\n30.6\n48.3\nOracle\n77.2\n77.1\n28.1\n26.7\n50.0\n48.2\n58.8\n56.4\n33.0\n31.2\n48.7\nTable 7: Teacher selection results showing post-training reasoning performance of student models trained on\ndata from teachers selected by different methods. Top-1 and Top-2 denote the highest- and second-highest-\nranked teachers for each student. \"Oracle\" corresponds to the ground-truth best and second-best teachers.\nResults\nAs shown in Table 7, both the best and second-best teachers selected by Rank-Surprisal Ratio yield\nstrong post-training reasoning performance, achieving average results close to oracle teachers and outperforming\nother selection methods. Notably, as also observed in our ablation study (Table 5), RSR remains effective when\nusing only 200 trajectories per teacher for measurement, highlighting its robustness and practical value for\nidentifying suitable teachers in low-resource settings.\n6. Related Work\nKnowledge Distillation\nKnowledge distillation is a powerful approach for transferring knowledge from\nlarge models to smaller ones [15], and has been widely used in training LLMs [30]. Prior work shows that\nstronger teachers do not necessarily yield better students, often due to capability gaps [24, 46] or off-policy\ndata [5]. Recent studies address this by better aligning teacher supervision with student behavior, achieving an\nimplicit balance through approaches such as on-policy distillation [2], integration with reinforcement learning\n[25, 49], SFT optimization [37, 42], teaching assistants [8, 27], and interleaved sampling [31, 38]. By contrast,\nour work explicitly quantifies the trade-off between informativeness and alignment in distillation, enabling a\nprincipled identification of effective teacher data for a given student.\nSFT with Reasoning Trajectories\nLong CoT trajectories provide strong supervision for improving student\nmodels\u2019 reasoning performance via SFT [34, 35]. In line with findings in the general domain that high-quality\ndata improves SFT [41, 51], many studies focus on constructing high-quality CoT data, either by selecting\nbetter prompts [40, 44] or by filtering reasoning trajectories [6, 17, 23, 36, 43, 53]. Recognizing that optimal\nreasoning data may vary across students, recent work explores student-specific trajectory selection strategies\n[16, 18, 29]. Our work also studies student-specific data selection, but differs from prior work by measuring\ndata\u2013student suitability from the perspective of informative alignment and by conducting more comprehensive\nevaluations across a wider range of teacher models.\n7."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary\n\n**Main Contributions:**\n- **Rank-Surprisal Ratio (RSR):** Proposed a simple yet effective metric to measure the suitability of reasoning trajectories for student LLMs. RSR captures both alignment and informativeness by balancing low absolute probability with high-ranked tokens under the student model's prediction distribution.\n- **Systematic Study:** Conducted a comprehensive study across multiple teacher and student models (5000-problem set, 11 diverse teachers, 5 student models), demonstrating that stronger reasoning teachers do not necessarily yield better students. Highlighting the importance of data-student suitability in distillation.\n- **Correlation Analysis:** Showed strong correlation between RSR scores and post-training performance across different student models (average Spearman correlation of 0.86).\n- **Practical Utility:** Proved that RSR is effective for both trajectory selection and teacher selection, outperforming existing methods.\n\n**Methodology:**\n1. **Data Generation:** Generated reasoning trajectories from 11 diverse teachers for a set of 5000 math problems.\n2. **Student Models:** Evaluated the performance of student models (Qwen-3-8B, Qwen-3-30B-Thinking, Nemotron-Super, Deepseek-R1, Magistral-Small) trained on these trajectories.\n3. **Metrics Development:**\n   - Introduced two fundamental token-level measures: surprisal and rank.\n   - Analyzed the limitations of existing probability-based metrics like average surprisal and local surprisal.\n   - Conducted simulation studies to identify quantitative patterns related to suitable reasoning data.\n4. **RSR Definition:** Defined RSR as the ratio between a trajectory\u2019s average token-wise rank and its average negative log-likelihood (surprisal). It is straightforward to compute and interpret.\n\n**Key Findings:**\n1. **Data Suitability:** Trajectories from Nemotron-Super and Magistral-Small, which are less familiar to student models, perform poorly in training despite high surprisal values.\n2. **Effective Trajectories:** Effective trajectories should balance informativeness with alignment by having low absolute probability but relatively high-ranked tokens under the student model\u2019s prediction distribution.\n3. **Correlation Analysis:** RSR scores correlate strongly (average Spearman correlation of 0.86) with post-training performance across different student models.\n\n**Conclusions:**\n1. **Importance of Data Suitability:** The effectiveness of reasoning trajectories varies significantly across students, emphasizing the importance of data-student suitability.\n2. **RSR as a Metric:** RSR is an effective metric for identifying suitable reasoning trajectories by jointly capturing alignment and informativeness.\n3. **Practical Applications:** RSR can be used to select more effective training trajectories and identify more suitable teacher models in low-resource settings.\n\nThis work provides a principled approach to selecting effective reasoning data for student LLMs, overcoming the challenges posed by prior methods that primarily focus on high-likelihood trajectories."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "THE ROLE OF PROSODIC AND LEXICAL CUES IN TURN-TAKING WITH\nSELF-SUPERVISED SPEECH REPRESENTATIONS\nSam O\u2019Connor Russell, Delphine Charuau and Naomi Harte\nSchool of Engineering, Trinity College Dublin, Ireland\nABSTRACT\nFluid turn-taking remains a key challenge in human-robot\ninteraction.\nSelf-supervised speech representations (S3Rs)\nhave driven many advances, but it remains unclear whether\nS3R-based turn-taking models rely on prosodic cues, lexi-\ncal cues or both. We introduce a vocoder-based approach to\ncontrol prosody and lexical cues in speech more cleanly than\nprior work. This allows us to probe the voice-activity pro-\njection model, an S3R-based turn-taking model. We find that\nprediction on prosody-matched, unintelligible noise is simi-\nlar to accuracy on clean speech. This reveals both prosodic\nand lexical cues support turn-taking, but either can be used\nin isolation. Hence, future models may only require prosody,\nproviding privacy and potential performance benefits. When\neither prosodic or lexical information is disrupted, the model\nexploits the other without further training, indicating they are\nencoded in S3Rs with limited interdependence. Results are\nconsistent in CPC-based and wav2vec2.0 S3Rs. We discuss\nour findings and highlight a number of directions for future\nwork. All code is available to support future research.\nIndex Terms\u2014 self-supervised speech representations,\nturn-taking, prosody, lexical cues, interpretability\n1. INTRODUCTION\nTurn-taking is fundamental to human interaction, allowing\nconversations to flow smoothly [1].\nAs speech planning\ntakes longer than transitions between speakers, listeners pre-\ndict when to speak ahead of time, supported by lexical and\nprosodic cues [1]. Achieving fluid turn-taking is critical for\nhuman-robot interaction [2]. However, most systems detect\nend-of-turn silence, leading to unnatural delays [2].\nPredictive turn-taking models address this by predicting\nspeaker changes ahead of time [3, 4, 5]. Feature engineer-\ning in earlier models, e.g. GeMAPS [4], has since been re-\nplaced by self-supervised speech representations (S3Rs) [3,\n5].\nS3Rs encode rich phonetic, lexical and prosodic cues\n[6, 7, 8], highly relevant to turn-taking [1, 9]. Despite their\nefficacy, the richness of speech is collapsed into a single rep-\nresentation, making it challenging to establish what drives\nThis work is financially supported by Taighde \u00b4Eireann \u2013 Research Ire-\nland under Grant No. 13/RC/2106 P2 and 22/FFP-A/11059\nprediction. Prior work has shown S3R-based turn-taking is\nquite robust to the flattening of pitch or intensity [3]. Lexical\ninformation may therefore support prediction. Alternatively,\nas only pitch or intensity was flattened, the model may rely\non the unmodified cue. Furthermore, we recently found back-\nground noise severely impacts turn-taking prediction [10]. As\nnoise did not fully degrade lexical content, prosodic and lexi-\ncal cues in S3R may be partially interdependent.\nThese challenges reflect the wider issues around S3R use\nin speech processing [7]. Their black-box nature hinders in-\nterpretability, limiting their ability to comply with ethics stan-\ndards when decisions need to be explainable [7]. This has\nmotivated a number of methods for studying S3Rs. Probing\nprovides a layer-wise analysis, but labels are required as small\nclassifiers are trained with supervised learning [6]. Input cor-\nruption can test sensitivity to a specific cue by removing it,\nwithout the need for labels [11]. A downside is common ma-\nnipulations like low-pass filtering disrupt both intelligibility\nand prosody [12]. Neuroscience routinely uses vocoders to\ncleanly isolate prosody while removing intelligibility, or vice\nversa [13, 14]. To our knowledge, these techniques have not\nbeen applied to turn-taking or S3R interpretability.\nMotivated by the need for greater S3R interpretability and\nrecent work in turn-taking, we ask: (i) do S3Rs support turn-\ntaking primarily via prosodic cues, lexical cues, or both?; and\n(ii) are prosodic and lexical cues in S3Rs interdependent? i.e.,\ncan one be disrupted whilst the other remains useful?\nOur contributions are threefold. Firstly, we introduce a\nvocoder method to cleanly control the prosodic and lexical in-\nformation in speech. Secondly, we apply this method to turn-\ntaking, showing that S3R encodes both cues independently:\nwhen one is removed, the other is exploited without further\ntraining. Finally, we show that unintelligible noise matched to\nspeech prosody achieves similar performance to clean speech.\nThis motivates future prosody-only models, which have per-\nformance and privacy benefits. We conclude with a discussion\nof our findings for both S3R and turn-taking.\n2. SELF-SUPERVISED SPEECH\nREPRESENTATIONS AND TURN-TAKING\nSelf-supervised speech representations (S3Rs) have driven\nadvances in speech processing as they capture rich repre-\narXiv:2601.13835v1  [cs.CL]  20 Jan 2026\nsentations of phonetics, prosody and semantics [6, 7]; all\nof which are relevant to turn-taking [1]. The voice-activity\nprojection (VAP) [3] model is a state-of-the-art transformer-\nbased turn-taking model leveraging S3R. The VAP model\nis widely studied [10, 11, 15], though questions concerning\nits operation remain. Firstly, which cues drive prediction?\nAn earlier study showed that when either pitch or intensity\nwas flattened, performance fell but remained above baseline\n[11]. Thus, lexical cues might dominate prediction, but as\nonly pitch or intensity was flattened, further work is needed.\nSecondly, it is unclear whether cues from speech captured\nby S3Rs are interdependent. Our recent study suggests inter-\ndependence: light noise, which disrupts prosody but leaves\nspeech mostly intelligible, leads to poor prediction [10].\nSeveral methods to better understand S3R models exist.\nProbing provides a layer-wise analysis [6]. Input corruption\ndirectly manipulates speech to understand how a cue is cap-\ntured by S3Rs. Common methods such as low-pass filter-\ning impact both intelligibility and prosody, limiting insights\n[3, 12]. Methods which more cleanly isolate cues are used\nin neuroscience to study language processing [13, 14] akin to\nthe highly entangled nature of S3Rs. Vocoders decompose\nspeech into spectral and amplitude components, enabling iso-\nlation of prosodic and lexical cues [13]. Yet to our knowledge,\nthey have not been applied to turn-taking with S3R.\n3. ISOLATING PROSODIC AND LEXICAL CUES\nWe explore S3R-based turn-taking with the CANDOR corpus\n[16] used in previous work [10]. It consists of 1,657 US En-\nglish dyads (850 hrs). Audio is 16kHz stereo (1 per speaker)\nand we used Amazon Transcribe for timing information.\nTo control prosodic and lexical cues, we use the WORLD\nvocoder [17]1. We generate prosody-matched noise by re-\nplacing the spectral envelope with pink noise (removing in-\ntelligibility), preserving pitch and amplitude. This isolates\nprosody by removing lexical information. We further isolate\npitch and intensity in two additional noises, which flatten ei-\nther the pitch or intensity to the utterance mean. We conduct\nprosodic manipulation preserving the spectral envelope (in-\ntelligibility) and flattening pitch, intensity, or both. A spec-\ntrogram of a sample utterance is in Figure 1, showing the ab-\nsence (middle) and preservation (bottom) of spectral informa-\ntion. We also generate background noise (babble, music and\nspeech=random utterances) by following our prior work [10].\nWe add noise to speech at controlled signal-to-noise ratios\n(SNRs) (-10:10 dB, step 2.5) [10]. We compute the word-\nerror rate (WER) with Whisper transcription [18] to quantify\nthe removal of lexical information, a common proxy for intel-\nligibility e.g. [19]. Samples and code are available2.\nWe use the VAP turn-taking model which predicts the\nnext 2 seconds of speaking activity (20Hz) [3, 15]. The front-\n130 s windows, 512 ffts, frame period 10 ms\n2github.com/russelsa/noise_generation_ICASSP-\nFig. 1: Spectrograms utterance \u201cwould you feel comfort-\nable\u201d. Top=original, middle=noise+prosody preserved, bot-\ntom=intelligible+prosody flat. Pitch=blue, intensity=yellow.\nend is a pre-trained S3R [20] (causal, frozen). The model\nhas 3 self- (1 per channel, tied weights) and 1 cross-attention\ntransformer layers (256d) [20]. We withhold 70 test sessions\nand cross validate on remaining sessions (5 folds). We train\n10 VAP models (Table 1) where \u2019mixed\u2019 demotes 25% of\ntraining sessions with 0 dB of noise / prosodic manipulation\n[10]. We train each for 10 epochs (lr=1e-4, batch=32) on an\nRTX 6000 GPU. We train a VAP variant with a wav2vec2.0\n[21] front-end (projected to 256d).\nTable 1: Overview of the 10 VAP model training conditions.\nModel description\nModel training data\n# Models\nClean speech\nClean speech\n1\nProsody-matched noise (no lexical cues)\nNoise matching pitch, intensity, and pitch+intensity\n3\nProsodic manipulation\nSpeech with flattened pitch, intensity and pitch+intensity\n3\nMixed\n75% clean + 25% noise matched to prosody\n1\nMixed\n75% clean + 25% prosody flattened speech\n1\nMixed\n75% clean + 25% background noise\n1\nWe evaluate models by extracting shifts (speaker change\nafter >200 ms silence; 83,158) and holds (no speaker change;\n206,830) following [3].\nS/H-Pred is a binary comparison\nof speech before shifts with speech before holds, whereas\nS-Pred compares pre-shift speech with mid-turn speech\n(83,158). We sum model probabilities over 200 ms, applying\na binary threshold (tuned on the validation set). We report\ntest set F1 and balanced accuracy with t-tests for significance.\n4. EXPLORING SELF-SUPERVISED\nREPRESENTATIONS IN TURN-TAKING\n4.1. Turn-taking in clean and manipulated speech\nWe first train and test the VAP model on clean speech (Table\n2). We note strong performance in line with prior work (80\nand 85% balanced accuracy for S/H-Pred and S-Pred) [10].\nWhen pitch and intensity are flattened, performance drops but\nremains above chance (e.g. 85 \u219268% for S-Pred). The\nmodel is more sensitive to flattening of intensity than pitch.\nProsodic noise removes lexical information whilst preserving\nprosody. Testing on prosodic noise yields a larger drop (85 \u2192\n60%), though performance remains above chance.\nOverall, the model is more robust to the removal of\nprosodic information than lexical. Performance drops when\nEnvironmental noise\nSpeech\nBabble\nMusic\nProsodic manipulation\nF0 flattened, intensity preserved\nF0 preserved, intensity flattened\nF0 and intensity flattened\nProsody\nmatched noise\nF0 preserved, intensity flattened\nF0 flattened, intensity preserved\nF0 and intensity preserved\nFig. 2: VAP model performance (S/H-Pred metric, 5-fold average \u00b195% c.i.) trained on clean speech (A) and a mixture of\nclean and manipulated speech (B). Balanced accuracy for each speech manipulation by test set signal-to-noise ratio (SNR).\neither cue is removed but above-chance performance demon-\nstrates the model can exploit whichever cue remains available.\nTable 2: VAP model performance on clean speech and on\nacoustic manipulations, grouped by their preservation of lex-\nical content and prosody (5-fold average, \u00b1 std \u2713=preserved,\nx=flattened. \u2713\u2713\u2713= original speech. F1 (w)=weighted).\nMetric\nTest set contains\nF1 (w)\nF1 (Hold)\nF1 (Shift)\nBal. Acc. (%)\nLexical\nPitch\nIntensity\nS-Pred\n\u2713\n\u2713\n\u2713\n0.86\n0.84\n0.87\n85 \u00b1 0\nx\nx\n\u2713\n0.58\n0.54\n0.62\n58 \u00b1 2\nx\n\u2713\nx\n0.58\n0.52\n0.64\n58 \u00b1 2\nx\n\u2713\n\u2713\n0.60\n0.56\n0.64\n60 \u00b1 2\n\u2713\nx\n\u2713\n0.76\n0.74\n0.80\n76 \u00b1 1\n\u2713\n\u2713\nx\n0.71\n0.67\n0.74\n71 \u00b1 1\n\u2713\nx\nx\n0.68\n0.63\n0.72\n68 \u00b1 1\nS/H-Pred\n\u2713\n\u2713\n\u2713\n0.83\n0.88\n0.70\n80 \u00b1 1\nx\nx\n\u2713\n0.67\n0.81\n0.33\n59 \u00b1 1\nx\n\u2713\nx\n0.66\n0.80\n0.31\n57 \u00b1 2\nx\n\u2713\n\u2713\n0.69\n0.81\n0.37\n61 \u00b1 1\n\u2713\nx\n\u2713\n0.76\n0.84\n0.56\n72 \u00b1 1\n\u2713\n\u2713\nx\n0.72\n0.81\n0.46\n66 \u00b1 1\n\u2713\nx\nx\n0.70\n0.80\n0.43\n63 \u00b1 1\n4.2. Effect of prosodic manipulation and noise\nNext, we explore how removing prosodic/lexical manipula-\ntions impacts intelligibility. Figure 3 shows the WER vs SNR.\nAdditive prosody-matched noise removes lexical content (in\ngreen, WER \u2192100%), whereas prosodic manipulation pre-\nserves lexical cues (in orange, WER < 20%). Background\nnoise degrades lexical content as the SNR decreases (blue).\nWe then test the clean speech-trained VAP model on\nspeech with prosodic/lexical manipulation. Figure 2 A shows\nS/H-pred accuracy vs test set SNR3. At high SNR, flattening\nof prosody has no significant impact on accuracy (80% vs\n80% p > 0.05) except when the intensity contour is flattened\n(80% \u219273%).\nAlthough no lexical information is pre-\nserved, at -10dB SNR accuracy is 66% in prosody-matched\nnoise (WER > 100%). Background noise (blue) has similarly\nhigh WER yet accuracy is 52%. Thus, these results support\nprosody as an independent turn-taking cue.\n3S-pred omitted for space, clean speech omitted for clarity (Table 2)\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nSNR (dB)\n20\n40\n60\n80\n100\nWER (%)\nWER vs. SNR\nFig. 3: Word error rate for each manipulation (WER, clamped\nto 100%) at each SNR in decibels (dB). P=pitch, I=intensity.\nMean per speaker \u00b195% c.i. Legend as above.\nWe then train models on a mix of clean and distorted\nspeech with results in Figure 2 B. The increased performance\nshows the model exploits prosodic and lexical cues more ef-\nfectively (orange and green). Background noise performance\ndid not improve in low SNR (blue), where lexical informa-\ntion is removed. At high SNR there is some increased perfor-\nmance, yet it is lower than the performance of other models.\nThus either prosodic or lexical information must be preserved\nfor robust performance. When one is removed, the other can\nbe flexibly used by the model. This shows that prosodic and\nlexical cues are not interdependent in S3Rs.\n4.3. Prosody and lexical cues in turn-taking prediction\nWe next train models exclusively on manipulated speech (Ta-\nble 3). We express results as a percentage of the accuracy\nof the model trained and tested on clean speech in Figure 4,\nestablishing to what extent prosody and lexical cues are in-\ndependent predictors. Noise matching the original prosody\nachieves 87-91% of clean accuracy, comparable to models\ntrained on prosody flattened speech (89-96%). Hence isolat-\ning prosodic and lexical cues yields accuracy close to clean\nspeech, supporting these cues as independent predictors.\n4.4. Generalisation to wav2vec2.0\nWe train a variant with wav2vec2.0 encoder [21] finding\nprosodic or lexical cues can be removed with minimal impact\nTable 3: VAP model performance, trained and tested on either\nclean speech or speech manipulated speech (5-fold average \u00b1\nstd. \u2713=preserved, x=flattened. F1 (w)=weighted).\nMetric\nTest + train sets contain\nF1 (w)\nF1 (Hold)\nF1 (Shift)\nBal. Acc. (%)\nLexical\nPitch\nIntensity\nS-Pred\n\u2713\n\u2713\n\u2713\n0.86\n0.84\n0.80\n85 \u00b1 0.4\nx\nx\n\u2713\n0.77\n0.74\n0.75\n77 \u00b1 1.7\nx\n\u2713\nx\n0.72\n0.68\n0.81\n71 \u00b1 0.8\nx\n\u2713\n\u2713\n0.78\n0.75\n0.84\n78 \u00b1 1.5\n\u2713\nx\n\u2713\n0.82\n0.80\n0.83\n82 \u00b1 1.8\n\u2713\n\u2713\nx\n0.81\n0.78\n0.83\n80 \u00b1 1.5\n\u2713\nx\nx\n0.74\n0.71\n0.70\n74 \u00b1 3.4\nS/H-Pred\n\u2713\n\u2713\n\u2713\n0.83\n0.88\n0.70\n80 \u00b1 1\nx\nx\n\u2713\n0.72\n0.82\n0.43\n68 \u00b1 1.2\nx\n\u2713\nx\n0.70\n0.81\n0.49\n64 \u00b1 1.0\nx\n\u2713\n\u2713\n0.73\n0.82\n0.62\n69 \u00b1 3.2\n\u2713\nx\n\u2713\n0.82\n0.85\n0.55\n76 \u00b1 1.4\n\u2713\n\u2713\nx\n0.75\n0.83\n0.53\n71 \u00b1 2.4\n\u2713\nx\nx\n0.75\n0.83\n0.52\n70 \u00b1 3.0\nFig. 4: Percentage of clean speech model accuracy achieved\nby when tested (top) and trained (bottom) on prosody-\nmatched noise and speech with flat prosody.\non performance (93% vs 92%). Unlike the earlier models and\nhuman turn-taking, wav2vec2.0 embeddings contain informa-\ntion from future frames, leading to unrealistic performance.\nResults affirm prior findings are not limited to a single S3R.\nTable 4: Wav2vec2.0 trained on clean speech (5-fold average\n\u00b1 std. \u2713=preserved, x=flattened. F1 (w)=weighted).\nMetric\nTest sets contain\nF1 (w)\nF1 (Hold)\nF1 (Shift)\nBal. Acc. (%)\nLexical\nPitch\nIntensity\nShift/hold\n\u2713\n\u2713\n\u2713\n0.94\n0.95\n0.89\n93 \u00b1 0\nx\n\u2713\n\u2713\n0.92\n0.94\n0.85\n92 \u00b1 1\n\u2713\nx\nx\n0.92\n0.94\n0.85\n92 \u00b1 1\n5. DISCUSSION OF KEY FINDINGS\nTurn-taking remains a key unsolved problem in human-robot\ninteraction.\nSelf-supervised speech representations (S3Rs)\nhave transformed turn-taking and speech processing, though\ninterpretability remains challenging. In this paper, we take\na new approach to analysis, isolating prosodic and lexi-\ncal information in speech.\nWe showed a VAP turn-taking\nmodel trained on noise matched to original prosody retains\n91% of the accuracy of a model trained and tested on clean\nspeech. Thus prosody alone can function as a turn-taking\ncue.\nSimilarly, speech with flat prosody and just lexical\ncues supports turn-taking with high accuracy. Performance\nis still better when both prosodic and lexical cues are present\n(clean speech).\nTo answer of our first research question,\nboth prosody and lexical cues can function as independent\nturn-taking predictors, and both support prediction.\nOur second question explored whether prosodic and lexi-\ncal cues are interdependent in S3Rs. Performance is remark-\nably similar in unintelligible speech matched to the original\nprosody (\u224870%), even when the model was trained on clean\nspeech (\u224866%). Hence, an S3R-based turn-taking model\ncan rely on prosody when lexical information is degraded and\nvice-versa. These cues are therefore not interdependent in\nS3R. We showed this was also true for a wav2vec2.0 model.\nOur work has revealed much about S3R-based turn-taking\nand S3Rs, raising a number of directions for future work.\nOur method isolates these cues more cleanly than prior work\n[12]. It should therefore be used to clarify the extent to which\nS3Rs exploit lexical information in outperforming engineered\nprosodic features, not addressed in prior work [22]. Secondly,\nbased on our finding that prosody alone supports turn-taking,\nlightweight prosody-based models should be considered. An-\nother advantage is that as we have shown, isolating prosody\nremoves useful lexical information. This has significant pri-\nvacy advantages e.g. sensitive topics removed. Furthermore,\nas both the spectral envelope and pitch contours display in-\ndividual speaker differences [23], using the prosodic contour\nreduces, but does not fully eliminate, identifiable information.\nFinally, future work should explore cross-linguistic generali-\nsation of S3Rs with our method, building on prior work [15].\nA limitation is the potential for residual phonetic infor-\nmation to persist in prosody-matched noise, though this is un-\nlikely, as all spectral information is removed. Furthermore,\nthe WER value confirms no useful information persists. Fi-\nnally, we only consider English in the CANDOR corpus and\nfuture work should consider other corpora.\nHowever, this\ncorpus is large (850 hr) and representative, containing many\nspeakers across \u22481600 dyads of casual talk.\n6. CONCLUSION\nWe revealed that self-supervised speech representation (S3R)\nindependently encode prosodic and lexical cues: when one\nis disrupted, S3R-based turn-taking models reliably use the\nother without additional training.\nWe further showed that\nthe prosodic contour is sufficient for turn-taking, with per-\nformance on noise matched to prosody nearly matching clean\nspeech performance. Our findings enhance S3R interpretabil-\nity, motivating future work in prosody-only turn-taking mod-\nels, which have important advantages including privacy.\n7. REFERENCES\n[1] Stephen C Levinson and Francisco Torreira, \u201cTiming in\nturn-taking and its implications for processing models\nof language,\u201d Frontiers in psychology, 2015.\n[2] Gabriel Skantze,\n\u201cTurn-taking in conversational sys-\ntems and human-robot interaction: a review,\u201d Computer\nSpeech & Language, vol. 67, pp. 101178, 2021.\n[3] Erik Ekstedt and Gabriel Skantze, \u201cVoice activity pro-\njection: Self-supervised learning of turn-taking events,\u201d\nin Interspeech 2022, 2022, pp. 5190\u20135194.\n[4] Matthew Roddy, Gabriel Skantze, and Naomi Harte,\n\u201cInvestigating speech features for continuous turn-\ntaking prediction using lstms,\u201d\nin Interspeech 2018,\n2018, pp. 586\u2013590.\n[5] Siyan Li, Ashwin Paranjape, and Christopher Manning,\n\u201cWhen can I speak? predicting initiation points for spo-\nken dialogue agents,\u201d in SIGDIAL, 2022.\n[6] Ankita Pasad, Ju-Chieh Chou, and Karen Livescu,\n\u201cLayer-wise analysis of a self-supervised speech repre-\nsentation model,\u201d in ASRU 2021. IEEE, 2021.\n[7] David Gimeno-G\u00b4omez, Catarina Botelho, Anna Pom-\npili, Alberto Abad, and Carlos-D Mart\u00b4\u0131nez-Hinarejos,\n\u201cUnveiling interpretability in self-supervised speech\nrepresentations for parkinson\u2019s diagnosis,\u201d IEEE Jour-\nnal of Selected Topics in Signal Processing, 2025.\n[8] Bogdan Vlasenko and Mathew Magimai Doss, \u201cMulti-\nmodal Prosody Modeling: A Use Case for Multilingual\nSentence Mode Prediction,\u201d in Interspeech, 2025.\n[9] Anne Cutler and Mark Pearson,\n\u201cOn the analysis of\nprosodic turn-taking cues,\u201d in Intonation in discourse.\n1985.\n[10] Sam O\u2019Connor Russell and Naomi Harte, \u201cVisual Cues\nSupport Robust Turn-taking Prediction in Noise,\u201d in In-\nterspeech 2025, 2025, pp. 1073\u20131077.\n[11] Erik Ekstedt and Gabriel Skantze,\n\u201cHow much does\nprosody help turn-taking? investigations using voice ac-\ntivity projection models,\u201d in SIGDIAL, 2022.\n[12] Phoebe Parsons and Heming Str\u00f8mholt Bremnes and\nKnut Kvale and Torbj\u00f8rn Svendsen and Giampiero\nSalvi,\n\u201cEffects of Prosodic Information on Dialect\nClassification Using Whisper Features,\u201d in Interspeech\n2025, 2025.\n[13] Erik C Brown, Otto Muzik, Robert Rothermel, Csaba\nJuh\u00b4asz, Aashit K Shah, Darren Fuerst, Sandeep Mittal,\nSandeep Sood, and Eishi Asano,\n\u201cEvaluating signal-\ncorrelated noise as a control task with language-related\ngamma activity on electrocorticography,\u201d Clinical Neu-\nrophysiology, vol. 125, no. 7, pp. 1312\u20131323, 2014.\n[14] Faizah Mushtaq, Ian M Wiggins, P\u00b4adraig T Kitterick,\nCarly A Anderson, and Douglas EH Hartley, \u201cEvaluat-\ning time-reversed speech and signal-correlated noise as\nauditory baselines for isolating speech-specific process-\ning using fnirs,\u201d PLoS One, vol. 14, no. 7, 2019.\n[15] Koji Inoue, Bing\u2019er Jiang, Erik Ekstedt, Tatsuya Kawa-\nhara, and Gabriel Skantze,\n\u201cMultilingual turn-taking\nprediction using voice activity projection,\u201d\nin LREC-\nCOLING 2024, Torino, Italia, May 2024.\n[16] Andrew Reece et al., \u201cThe candor corpus: Insights from\na large multimodal dataset of naturalistic conversation,\u201d\nScience Advances, vol. 9, no. 13, pp. eadf3197, 2023.\n[17] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa,\n\u201cWorld: a vocoder-based high-quality speech synthesis\nsystem for real-time applications,\u201d IEICE TRANSAC-\nTIONS on Information and Systems, 2016.\n[18] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever, \u201cRobust\nspeech recognition via large-scale weak supervision,\u201d in\nInternational conference on machine learning. PMLR,\n2023, pp. 28492\u201328518.\n[19] Jeongsoo Choi, Minsu Kim, and Yong Man Ro, \u201cIn-\ntelligible lip-to-speech synthesis with speech units,\u201d in\nInterspeech 2023, 2023, pp. 4349\u20134353.\n[20] Morgane Riviere, Armand Joulin, Pierre-Emmanuel\nMazar\u00b4e, and Emmanuel Dupoux, \u201cUnsupervised pre-\ntraining transfers well across languages,\u201d\nin ICASSP\n2020. IEEE, 2020, pp. 7414\u20137418.\n[21] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli,\n\u201cwav2vec 2.0: A framework for\nself-supervised learning of speech representations,\u201d Ad-\nvances in neural information processing systems, vol.\n33, pp. 12449\u201312460, 2020.\n[22] Sofoklis Kakouros and Haoyu Chen, \u201cSounding Like\na Winner? Prosodic Differences in Post-Match Inter-\nviews,\u201d in Interspeech 2025, 2025, pp. 4188\u20134192.\n[23] Masato Akagi and Taro Ienaga,\n\u201cSpeaker individual-\nity in fundamental frequency contours and its control,\u201d\nJournal of the Acoustical Society of Japan."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary\n\n#### Main Contributions\nThis paper introduces a vocoder-based approach to cleanly control prosodic and lexical cues in speech, which allows for probing the voice-activity projection (VAP) model\u2014a self-supervised speech representation (S3R)-based turn-taking model. The key contributions of this work are:\n1. **Isolation Method**: Introduce a vocoder method to isolate prosodic and lexical information more cleanly than prior approaches.\n2. **Turn-Taking Performance Analysis**: Show that S3Rs encode both cues independently, with either cue being sufficient for robust turn-taking prediction.\n3. **Performance Comparison**: Demonstrate that unintelligible noise matched to speech prosody achieves similar performance to clean speech, indicating the importance of both prosodic and lexical cues in turn-taking.\n\n#### Methodology\nThe authors utilized a CANDOR corpus (1657 US English dyads) for their experiments. They employed a vocoder-based WORLD system to generate prosody-matched noise by replacing spectral envelopes with pink noise, thereby isolating prosodic information while preserving intelligibility. Additional noises were created by manipulating pitch and intensity independently or in combination.\n\nThe VAP model was used as the turn-taking predictor, leveraging self-supervised speech representations (S3Rs). The authors trained multiple variants of the VAP model on clean speech and manipulated speech scenarios to evaluate performance under various conditions. These included prosodic manipulation, lexical manipulation, mixed manipulations, and background noise addition.\n\n#### Key Findings\n1. **Prosodic Cues Dominance**: When either pitch or intensity was flattened (prosodic manipulation), the model's accuracy dropped but remained above chance levels.\n2. **Lexical Information Impact**: The removal of prosodic cues alone led to a significant drop in performance, indicating that lexical information is crucial for robust turn-taking prediction.\n3. **Intelligible Noise Matching**: Unintelligible noise matched to speech prosody achieved similar performance to clean speech, suggesting both prosodic and lexical cues are critical for accurate turn-taking.\n\n#### Conclusions\nThe study reveals that S3Rs encode both prosodic and lexical cues independently. When either cue is disrupted, the model can still exploit the remaining cue without further training. This finding supports the idea that future models might only require prosodic information, providing potential privacy benefits. The work also highlights the importance of isolating these cues for better interpretability in S3R-based turn-taking models.\n\n#### Future Directions\n1. **Prosody-Only Models**: Develop lightweight prosody-only models to leverage performance gains without relying on lexical cues.\n2. **Cross-Linguistic Generalization**: Explore cross-linguistic generalization of S3Rs using the proposed methodology.\n3. **Ethical Considerations**: Investigate how S3Rs can be used ethically, especially in contexts where explainability is crucial.\n\n### Conclusion\nThis research provides valuable insights into the role of prosodic and lexical cues in turn-taking with self-supervised speech representations. By isolating these cues more cleanly than prior work, this study advances our understanding of S3R-based models and opens up new directions for future research."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Learning to Explain: Supervised Token Attribution from Transformer\nAttention Patterns\nGeorge Mihaila\nUniversity of North Texas\ngeorgemihaila@my.unt.edu\nAbstract\nExplainable AI (XAI) has become critical as\ntransformer-based models are deployed in high-\nstakes applications including healthcare, legal\nsystems, and financial services, where opacity\nhinders trust and accountability. Transform-\ners\u2019 self-attention mechanisms have proven\nvaluable for model interpretability, with atten-\ntion weights successfully used to understand\nmodel focus and behavior (Xu et al., 2015);\n(Wiegreffe and Pinter, 2019). However, exist-\ning attention-based explanation methods rely\non manually defined aggregation strategies and\nfixed attribution rules (Abnar and Zuidema,\n2020a); (Chefer et al., 2021), while model-\nagnostic approaches (LIME, SHAP) treat the\nmodel as a black box and incur significant com-\nputational costs through input perturbation. We\nintroduce Explanation Network (ExpNet), a\nlightweight neural network that learns an ex-\nplicit mapping from transformer attention pat-\nterns to token-level importance scores. Unlike\nprior methods, ExpNet discovers optimal atten-\ntion feature combinations automatically rather\nthan relying on predetermined rules. We evalu-\nate ExpNet in a challenging cross-task setting\nand benchmark it against a broad spectrum of\nmodel-agnostic methods and attention-based\ntechniques spanning four methodological fami-\nlies.\n1\nIntroduction\nTransformer architectures (Vaswani et al., 2017)\nhave revolutionized Natural Language Processing,\nachieving state-of-the-art performance across di-\nverse tasks. Despite their success, transformers\nremain complex and difficult to interpret, as predic-\ntions emerge from multiple layers of self-attention\nand nonlinear transformations. This opacity raises\ncritical concerns for transparency and trustworthi-\nness, particularly when models are deployed in\nsensitive domains such as healthcare diagnostics,\nlegal decision support, and financial risk assess-\nment (Doshi-Velez and Kim, 2017).\nRigorous,\nhuman-centered interpretability has therefore be-\ncome essential for accountable deployment of these\npowerful yet opaque models. Existing XAI ap-\nproaches fall into two main categories. Model-\nagnostic methods such as LIME (Ribeiro et al.,\n2016) and SHAP (Lundberg and Lee, 2017) treat\nmodels as black boxes, offering broad applicability\nbut ignoring internal reasoning and incurring sig-\nnificant computational costs. In NLP specifically,\nthese perturbation-based methods face a fundamen-\ntal challenge: masking or replacing words creates\nsemantically invalid examples that lose linguistic\nmeaning (Feng et al., 2018). The transformer\u2019s\nself-attention mechanism offers an alternative ex-\nplanation source. Attention weights have proven\nvaluable for interpretability since their introduc-\ntion (Bahdanau et al., 2014); (Xu et al., 2015);\n(Wiegreffe and Pinter, 2019), leading to numerous\nattention-based explanation methods (Abnar and\nZuidema, 2020a); (Qiang et al., 2022). However,\nthese methods rely on hand-crafted heuristic rules\nor fixed aggregation functions to extract explana-\ntions from attention patterns.\nOur contribution treats attention-based explana-\ntion as a learning problem. This learned approach\ngeneralizes across tasks and consistently outper-\nforms established baseline methods while maintain-\ning computational efficiency. Our work contributes:\n(1) a novel framework that learns attention-based\nexplanations from human rationales rather than\nrelying on fixed heuristics, (2) demonstration of\ncross-task explainability generalization in NLP, (3)\ncomprehensive ablation studies justifying architec-\ntural choices, and (4) extensive empirical evalua-\ntion showing state-of-the-art performance.\n2\nRelated Work\n2.1\nModel-Agnostic Explanation Methods\nModel-agnostic techniques treat models as black\nboxes, requiring only input-output access. LIME\narXiv:2601.14112v1  [cs.CL]  20 Jan 2026\n(Ribeiro et al., 2016) learns local linear approxima-\ntions by probing models on perturbed inputs, while\nSHAP (Lundberg and Lee, 2017) provides game-\ntheoretic Shapley value-based attributions with for-\nmal guarantees of consistency and local accuracy.\nWhile offering broad applicability, these methods\nface limitations in NLP contexts.\nThey ignore\nmodel internals including transformer attention\nmechanisms, incur significant computational costs\nthrough extensive model queries, and perturbation-\nbased approaches can produce out-of-distribution\ninputs that confound causal interpretation (Feng\net al., 2018).\n2.2\nAttention as Explanation\nAttention mechanisms have proven valuable for\ninterpretability since their introduction, with at-\ntention weights widely visualized to understand\nmodel focus (Bahdanau et al., 2014; Xu et al.,\n2015; Vig, 2019). Research has shown that at-\ntention patterns capture meaningful linguistic phe-\nnomena including syntactic dependencies and se-\nmantic relationships (Wiegreffe and Pinter, 2019).\nThis motivates attention-based explanation meth-\nods that leverage these internal signals, which are\ncommon to transformer architectures and unavail-\nable to model-agnostic approaches.\n2.3\nGradient-Based and Attention-Based\nMethods\nGradient-based methods leverage backpropaga-\ntion to attribute importance. Integrated Gradients\n(Sundararajan et al., 2017a) accumulates gradients\nalong paths from baseline to actual inputs, satis-\nfying axioms including sensitivity and implemen-\ntation invariance, but operates independently of\nattention mechanisms.\nMethods combining gradients with attention\nleverage complementary signals from backpropaga-\ntion and attention mechanisms. Gradient-weighted\napproaches multiply attention scores by loss gra-\ndients, analogous to Grad-CAM in computer vi-\nsion (Selvaraju et al., 2016; Barkan et al., 2021).\nAttCAT (Qiang et al., 2022) integrates attention\nweights, token gradients, and skip connections.\nLayer-wise Relevance Propagation (LRP) methods\nconservatively redistribute output scores through\nsystematic backpropagation (Bach et al., 2015).\nTransformer-LRP (Chefer et al., 2020) adapted\nLRP for transformers, Generic Attention Explain-\nability (GAE) (Chefer et al., 2021) extended it to\nencoder-decoder models, and Mask-LRP (Song\net al., 2024) refined the approach by masking unin-\nformative heads.\nAttention aggregation approaches combine at-\ntention signals from multiple heads or layers using\npredetermined rules. Attention Rollout and Atten-\ntion Flow (Abnar and Zuidema, 2020b) aggregate\nattention across layers to trace token influence, ad-\ndressing entangled representations in deeper lay-\ners (Brunner et al., 2019). These approaches im-\nproved correlation with ablation studies through\nmulti-layer composition rules.\nThese methods extract explanations through pre-\ndetermined procedures\u2014gradient computations,\npropagation algorithms, or aggregation func-\ntions\u2014that are specified apriori based on math-\nematical or heuristic principles. Each method em-\nbodies specific assumptions about how to derive\ntoken importance from model internals.\n2.4\nExplanation Evaluation\nEvaluation of explanation quality faces fundamen-\ntal challenges. Faithfulness metrics assess whether\nexplanations reflect model reasoning through per-\nturbation (DeYoung et al., 2019), but suffer from\ncritical problems in NLP: token removal creates\nout-of-distribution inputs (Feng et al., 2018), archi-\ntectural redundancy enables compensation where\nalternative pathways maintain predictions despite\nremoving important features (Voita et al., 2019;\nSerrano and Smith, 2019), and circular validation\nwithout ground truth undermines validity (Hooker\net al., 2018). Plausibility metrics, which measure\nalignment with human rationales (Jacovi and Gold-\nberg, 2020), provide a more principled evaluation\nframework for high-stakes applications where hu-\nman interpretability is paramount (Doshi-Velez and\nKim, 2017). Our approach embraces this paradigm\nby learning explanations directly from human ra-\ntionales.\n2.5\nPositioning ExpNet\nExpNet builds upon attention-based explanation\nmethods, leveraging the insight that transformer\nattention patterns contain valuable signals for in-\nterpretability (Bahdanau et al., 2014; Wiegreffe\nand Pinter, 2019; Abnar and Zuidema, 2020b).\nRather than applying fixed aggregation rules or\nheuristics, ExpNet treats explanation extraction as\na supervised learning problem, training on human-\nannotated rationales to discover which attention\nconfigurations correlate with human-judged to-\nken importance. This approach enables cross-task\ngeneralization and direct optimization for human-\naligned explanations while reducing the need for\nhuman rationales when applied to new datasets.\n3\nMethodology\n3.1\nOverview and Base Model\nWe use BERT-base (Devlin et al., 2019) as our\ntransformer model for all experiments. BERT pro-\ncesses input text by tokenizing it into subword\nunits and prepending a special classification to-\nken [CLS] to each sequence. During classification,\nBERT\u2019s final hidden state corresponding to [CLS]\nis passed to a classification head to produce the\nprediction. This design makes [CLS] a natural\naggregation point: through self-attention across\nall layers, [CLS] gathers task-relevant information\nfrom input tokens.\nOur goal is to learn which input tokens are\nimportant for BERT\u2019s decisions by analyzing at-\ntention patterns involving [CLS]. Specifically, we\ntrain a lightweight neural network\u2014ExpNet\u2014that\nmaps BERT\u2019s self-attention patterns to token-level\nimportance scores aligned with human rationales.\nExpNet treats BERT as a frozen feature extrac-\ntor: we do not modify BERT\u2019s weights, only ex-\ntract and process its attention patterns. While we\nuse BERT-base for experimental validation, Exp-\nNet\u2019s approach generalizes to other encoder-based\ntransformers with similar aggregation mechanisms\n(e.g., RoBERTa, ALBERT, DeBERTa) and can be\nadapted to decoder-based or encoder-decoder-base\nmodels with appropriate modifications.\n3.2\nAttention-Based Feature Extraction\nWe extract features from BERT\u2019s final self-\nattention layer (layer 12), which contains H = 12\nattention heads. For each token j in the input se-\nquence, we extract two types of attention-based\nfeatures from each head h \u2208{1, . . . , H}:\nTask-to-Token Attention:\nThe attention weight\nfrom [CLS] to token j , denoted A(h)\n[CLS],j. This\nmeasures how strongly BERT\u2019s aggregation mech-\nanism attends to token j when forming its predic-\ntion. High values suggest [CLS] draws information\nfrom token j.\nToken-to-Task Attention:\nThe attention weight\nfrom token j to [CLS] , denoted A(h)\nj,[CLS]. This\nmeasures how much token j attends to the task\nrepresentation. High values suggest token j is con-\ntextualizing itself relative to the classification ob-\njective.\nThese\ntwo\nperspectives\ncapture\nthe\nnon-\nsymmetric nature of self-attention relationships,\nwhere Ai,j \u0338= Aj,i. A token may be important\neither because [CLS] focuses on it (task-to-token:\n[CLS] extracts information from the token) or be-\ncause the token focuses on [CLS] (token-to-task:\nthe token contextualizes itself relative to the task).\nPrior attention-based explanation methods often\naveraged attention weights across heads (Wiegreffe\nand Pinter, 2019) or used only one attention direc-\ntion, potentially discarding information valuable\nfor cross-task generalization. By preserving indi-\nvidual head contributions from both directions, Ex-\npNet learns which attention patterns indicate impor-\ntance\u2014patterns that transfer across tasks because\nattention heads capture general linguistic relation-\nships (syntax, semantics, discourse structure) inde-\npendent of specific classification objectives (Voita\net al., 2019; Clark et al., 2019).\nThese two perspectives capture asymmetric at-\ntention relationships: a token may be important ei-\nther because [CLS] focuses on it (task-to-token) or\nbecause it focuses on [CLS] (token-to-task). Prior\nattention-based explanation methods often aver-\naged attention weights across heads (Wiegreffe and\nPinter, 2019) or used only one attention direction.\nBy preserving individual head contributions from\nboth directions, we enable ExpNet to learn which\nattention patterns are most indicative of token im-\nportance.\nEach token j is represented by a feature vector\nconcatenating all attention values:\nfj =\n\u0002\nA(1)\n[CLS],j, . . . , A(H)\n[CLS],j,\nA(1)\nj,[CLS], . . . , A(H)\nj,[CLS]\n\u0003\n(1)\nFor BERT-base with H = 12 heads, fj is a 24-\ndimensional vector. The entire ExpNet pipeline is\nillustrated in Figure 1.\n3.3\nExpNet Architecture\nExpNet is a simple feed-forward network that maps\nthe feature vector fj to an importance score for\ntoken j. The architecture consists of:\n\u2022 Input layer: 24 dimensions (2H attention fea-\ntures)\n\u2022 Hidden layer: 16 units with ReLU activation\n\u2022 Output layer: Single sigmoid unit producing\nimportance probability \u02c6yj \u2208[0, 1]\nFormally, ExpNet computes:\nhj = ReLU(W1fj + b1)\n\u02c6yj = \u03c3(w\u22a4\n2 hj + b2)\n(2)\nwhere \u03c3(x) = 1/(1 + e\u2212x) is the sigmoid func-\ntion, and \u02c6yj represents the predicted probability\nthat token j is important.\n3.4\nTraining Procedure\nSupervision from Human Rationales:\nWe train\nExpNet as a token-level binary classifier using\nhuman-annotated rationales.\nFor each training\nexample, humans mark which words justify the\nlabel. Since human annotations are at the word\nlevel while BERT operates on subword tokens, we\nproject labels: if any subtoken of a word is marked\nimportant by humans, all subtokens of that word\nreceive label yj = 1; otherwise yj = 0.\nTraining on Correct Predictions Only:\nWe\ntrain ExpNet exclusively on instances where\nBERT\u2019s prediction is correct. This ensures the\nexplainer learns attention patterns associated with\nsuccessful reasoning. Training on mispredictions\ncould introduce noise, as attention patterns may be\nunreliable when the model makes errors (DeYoung\net al., 2019). At inference time, ExpNet can still\nexplain any prediction (correct or incorrect).\nFocal Loss for Class Imbalance:\nToken impor-\ntance labels are highly imbalanced: most tokens\nare not important. To address this, we use focal\nloss (Lin et al., 2017) instead of cross-entropy:\nLfocal = \u2212\u03b1t(1 \u2212pt)\u03b3 log(pt)\n(3)\nwhere pt = \u02c6yj if yj = 1, else pt = 1 \u2212\u02c6yj. The\nmodulating factor (1 \u2212pt)\u03b3 down-weights easy\nexamples, focusing learning on hard-to-classify to-\nkens. We set \u03b1 = 0.6 and \u03b3 = 2 following (Lin\net al., 2017).\nInference and Thresholding:\nAt test time, we\napply a threshold of 0.5 to \u02c6yj to produce binary\nimportance predictions. To ensure ExpNet iden-\ntifies at least one important token per example\n(even when all scores fall below 0.5), we select\nthe highest-scoring token as important if no tokens\nexceed the threshold. This reflects the assumption\nthat every prediction has at least some supporting\nevidence and prevents degenerate cases with empty\nexplanations.\nHyperparameters:\nWe train ExpNet for 50\nepochs using Adam optimizer with learning rate\n0.001 and batch size 32. No learning rate schedule\nor warmup is used. These hyperparameters were\nchosen to ensure stable convergence across all three\ndatasets.\n4\nExperiments\nWe evaluate ExpNet on three NLP classification\ntasks (Table 1), examining the challenging scenario\nof cross-domain (cross-task) explainability. This\nchapter describes the datasets, experimental train-\ning setups, baseline explanation methods, and eval-\nuation metrics.\n4.1\nDatasets and Annotation\nWe evaluate ExpNet on three NLP classification\ntasks with distinct characteristics. For each task,\nwe fine-tune a separate BERT-base classifier (De-\nvlin et al., 2019) that serves as the frozen feature\nextractor.\nHateXplain (Mathew et al., 2020) includes na-\ntive human rationales. We use only hate and of-\nfensive examples. SST-2 (Socher et al., 2013) and\nCoLA (Warstadt et al., 2018) lack word-level ra-\ntionales. We generated training annotations us-\ning Claude-3.5-Sonnet following a systematic pro-\ncess: calibration on HateXplain, prompt refinement\nwith human-annotated 30-example seed sets, and\nlarge-scale annotation. Critically, test sets for both\nSST-2 and CoLA were independently annotated\nby two human annotators, achieving substantial\ninter-annotator agreement (Table 2). Complete an-\nnotation methodology and reliability analysis are\nprovided in Appendix A.\nFollowing standard practice (DeYoung et al.,\n2019), we apply correct-prediction filtering: ex-\nplanations are evaluated only on instances where\nthe classifier\u2019s prediction matches the gold label,\nensuring we explain true decision-making rather\nthan error patterns.\n4.2\nCross-Task Generalization Experimental\nDesign\nThe evaluation employs a leave-one-task-out proto-\ncol where ExpNet is trained on the combined ratio-\nnale data from two tasks and evaluated on the held-\nout third task. No task-specific hyperparameter\ntuning is performed; all training parameters remain\nfixed across experiments to ensure fair cross-task\ngeneralization assessment. The three experimental\nconfigurations are:\nFigure 1: ExpNet complete pipeline from BERT\u2019s attention patterns to importance predictions.\nDataset\nTask\nSource\nRationales\nHateXplain\nHate speech\nSocial media\nHuman\nSST-2\nSentiment\nMovie reviews\nLLM + Human\nCoLA\nGrammaticality\nLinguistic\nLLM + Human\nTable 1: Dataset overview and annotation sources.\n\u2022 Train on (SST-2 + CoLA) \u2192Test on HateX-\nplain\n\u2022 Train on (SST-2 + HateXplain) \u2192Test on\nCoLA\n\u2022 Train on (CoLA + HateXplain) \u2192Test on\nSST-2\nThis cross-task design demonstrates that Exp-\nNet learns generalizable explanation patterns rather\nthan task-specific heuristics.\n4.3\nBaseline Explanation Methods\nWe benchmark ExpNet against thirteen established\nexplainability methods representing fundamentally\ndifferent approaches to generating explanations.\nLIME (Ribeiro et al., 2016) learns a sparse lin-\near surrogate model around each instance by prob-\ning the classifier on perturbed inputs, treating the\nmodel as a black box. Similarly, SHAP (Lund-\nberg and Lee, 2017) adopts a model-agnostic ap-\nproach, framing attribution as a Shapley-value prob-\nlem from cooperative game theory to estimate each\nfeature\u2019s marginal contribution. Unlike these black-\nbox methods, Integrated Gradients (Sundararajan\net al., 2017b) leverages model internals by com-\nputing gradients along a straight-line path from a\nbaseline to the input, integrating to obtain attribu-\ntions. Building on this gradient-based foundation,\nLayer-wise Relevance Propagation (LRP) (Bach\net al., 2015) back-propagates output relevance to\nthe input layer using conservation rules, while Ful-\nlLRP (Voita et al., 2019) modifies this by stopping\npropagation at the final transformer block rather\nthan continuing to raw embeddings, making it more\nsuitable for transformer architectures. CAM (Sel-\nvaraju et al., 2016) takes a different gradient-based\napproach by leveraging the final hidden layer to lo-\ncalize important tokens, and Grad-CAM (Barkan\net al., 2021) extends this idea by using gradients\nwith respect to the final hidden layer to weight\ntoken contributions.\nSeveral methods combine multiple signals for\nmore robust attribution. Generic Attention Ex-\nplainability (GAE) (Chefer et al., 2021) merges at-\ntention gradients with relevance propagation rules\nto obtain token attributions, effectively bridging\ngradient-based and propagation-based approaches.\nMask-LRP (MGAE) (Song et al., 2024) extends\nDataset\nTrain/Val\nTest\nTest Annotation\nSST-2\n5,971/395\n436\nHuman (2 ann., \u03b1=0.84)\nCoLA\n2,426/101\n161\nHuman (2 ann., \u03b1=0.64)\nHateXplain\n8,042/734\n752\nHuman (3 ann., \u03b1=0.46)\nTable 2: Final dataset splits. CoLA uses only unacceptable examples; HateXplain excludes neutral class.\nthe LRP framework by first identifying and mask-\ning non-informative attention heads (e.g., syntactic\nor positional heads), then propagating relevance\nonly through retained heads for cleaner attributions.\nAttCAT (Qiang et al., 2022) fuses per-block at-\ntention weights with input-token gradients to form\nattentive class-activation maps, combining atten-\ntion mechanisms with gradient-based localization.\nFinally, two methods rely purely on attention mech-\nanisms without gradients or propagation: RawAtt\n(Abnar and Zuidema, 2020b) directly uses aver-\naged self-attention weights from the last layer as\ntoken-importance scores, representing the simplest\nattention-based approach, while Attention Roll-\nout (Abnar and Zuidema, 2020b) builds on this\nby aggregating attention weights across all layers\nthrough matrix multiplication to capture long-range\ndependencies throughout the network. Finally, we\ninclude a naive RandomBaseline, which ignores\nmodel internals and independently marks each to-\nken as important with probability equal to the em-\npirical positive rate in the training data, providing\na trivial lower bound for comparison.\nTo enable fair comparison with binary human\nrationales, we binarize continuous baseline scores\nusing top-K selection following standard practice\n(DeYoung et al., 2019), where K is the average ra-\ntionale length per dataset (K=3 for SST-2, K=15 for\nCoLA, K=9 for HateXplain). Detailed binarization\nprocedures are provided in Appendix A.3.\n5\nResults\nWe assess explanation quality using token-level F1-\nscore and AUROC, measuring alignment between\npredicted rationales and human-annotated ground\ntruth. F1-score is appropriate for this task as it bal-\nances precision and recall when identifying impor-\ntant tokens, penalizing both missed rationales and\nspurious highlights. AUROC provides a threshold-\nindependent assessment of ranking quality, evaluat-\ning how consistently methods order true rationale\ntokens ahead of non-rationale tokens across all de-\ncision boundaries. Together, these metrics capture\nboth binary classification accuracy (F1) and con-\ntinuous ranking performance (AUROC) of expla-\nnation methods. We report dataset-level metrics\nfollowing standard practice in rationale evaluation\n(DeYoung et al., 2019). Complete metric defini-\ntions and formulas are provided in Appendix B.\n5.1\nQuantitative Evaluation\nWe compare ExpNet against thirteen established\nbaseline methods. This comprehensive comparison\nensures robust evaluation against both classic post-\nhoc explainers and recent state-of-the-art methods.\nTable 3 reports token-level F1 scores with confi-\ndence intervals across the three benchmarks under\nthe cross-dataset setting. ExpNet achieves the high-\nest F1 across all three tasks, demonstrating substan-\ntially better alignment with human rationales than\nall thirteen baselines. The cross-task generalization\nis particularly notable: ExpNet was never trained\non the held-out task, yet consistently outperforms\nmethods specifically tuned for those datasets. On\nCoLA, ExpNet achieves F1 of 0.468 \u00b1 0.079, rep-\nresenting a 31% relative improvement over the best\nbaseline (GradCAM: 0.356). On HateXplain, Ex-\npNet reaches 0.473 \u00b1 0.007, outperforming the\nstrongest baseline (GradCAM: 0.396) by 19%.\nEven on SST-2, where baseline performance is\nmore competitive, ExpNet\u2019s F1 of 0.398 \u00b1 0.024\nexceeds the best baseline (GAE/MGAE: 0.350) by\n14%. Detailed results are provided in Appendix C.\nThe baseline results reveal distinct performance\npatterns across explainer categories.\nOn Ha-\nteXplain, gradient-based methods (GradCAM:\n0.396 \u00b1 0.008) and propagation-based techniques\n(GAE/MGAE: 0.391 \u00b1 0.007) lead among base-\nlines. Model-agnostic methods show more variable\nperformance, with LIME achieving 0.347 \u00b1 0.033\non SST-2 but only 0.290 \u00b1 0.007 on HateXplain,\nlikely due to sampling noise and difficulty captur-\ning complex token interactions in subjective hate\nspeech contexts. Interestingly, on SST-2, Integrated\nGradient (0.287 \u00b1 0.032) underperforms even raw\nattention weights (RawAt: 0.327\u00b10.029), suggest-\ning that simple attention patterns can sometimes\nbe more indicative than gradient-based approxima-\nExplainer\nSST-2\nCoLA\nHateXplain\nRandomBaseline\n0.258\n0.387\n0.293\nSHAP\n0.330 \u00b1 0.033\n0.330 \u00b1 0.056\n0.276 \u00b1 0.007\nLIME\n0.347 \u00b1 0.033\n0.323 \u00b1 0.053\n0.290 \u00b1 0.007\nIntegrated Gradient\n0.287 \u00b1 0.032\n0.342 \u00b1 0.054\n0.345 \u00b1 0.008\nRawAt\n0.327 \u00b1 0.029\n0.353 \u00b1 0.054\n0.362 \u00b1 0.007\nRollout\n0.133 \u00b1 0.025\n0.347 \u00b1 0.055\n0.356 \u00b1 0.007\nLRP\n0.339 \u00b1 0.030\n0.355 \u00b1 0.054\n0.372 \u00b1 0.008\nFullLRP\n0.218 \u00b1 0.030\n0.336 \u00b1 0.055\n0.346 \u00b1 0.007\nGAE\n0.350 \u00b1 0.030\n0.354 \u00b1 0.053\n0.391 \u00b1 0.007\nCAM\n0.243 \u00b1 0.032\n0.355 \u00b1 0.054\n0.332 \u00b1 0.007\nGradCAM\n0.234 \u00b1 0.031\n0.356 \u00b1 0.056\n0.396 \u00b1 0.008\nAttCAT\n0.280 \u00b1 0.034\n0.345 \u00b1 0.055\n0.340 \u00b1 0.007\nMGAE\n0.350 \u00b1 0.030\n0.354 \u00b1 0.053\n0.391 \u00b1 0.007\nExpNet\n0.398 \u00b1 0.024\n0.468 \u00b1 0.079\n0.473 \u00b1 0.007\nTable 3: F1-score results for ExpNet compared to baseline methods under cross-dataset evaluation. ExpNet is\ntrained on two datasets and evaluated on the held-out third dataset: SST-2 performance when trained on CoLA +\nHateXplain, CoLA performance when trained on SST-2 + HateXplain, and HateXplain performance when trained\non SST-2 + CoLA. For all trained methods we report F1-Score with \u00b1 95% confidence intervals; RandomBaseline is\nreported as a single deterministic run.\ntions for certain tasks.\nExpNet\u2019s superior performance stems from di-\nrect supervision on human rationales. Methods like\nRawAt and Rollout (Jain and Wallace, 2019) rely\non hand-crafted aggregation heuristics that demon-\nstrate can be misleading. Gradient-propagation\napproaches such as MGAE and GAE follow fixed\nrelevance rules, but without training on human ra-\ntionales they miss subtle language patterns that\nhumans recognize as salient. Even sophisticated\nhybrids like AttCAT, which combines attention\nweights, skip connections, and feature gradients,\nfall short of ExpNet\u2019s learned mapping from atten-\ntion to importance.\nFigure 2 shows the AUROC of each explainer\u2019s\ntoken-level ranking on held-out datasets. While Ex-\npNet performs competitively, achieving AUROC\nvalues consistently above 0.7, it does not always\nachieve the top AUROC. ExpNet is the top per-\nformer on SST-2, but some gradient-based base-\nlines achieve marginally higher AUROC on CoLA\nand HateXplain. This pattern reveals an important\ndistinction: ExpNet\u2019s threshold-based F1 scores\nare consistently the highest, but its continuous im-\nportance scores are not always the most separa-\nble in a ranking sense. ExpNet excels at selecting\nthe correct set of important tokens (high precision\nand recall for the binary decision), while certain\ngradient-based methods may preserve relative rank-\ning quality across all tokens more effectively. This\ncomplementary strength reflects ExpNet\u2019s training\nobjective, which optimizes for accurate binary ra-\ntionale prediction rather than perfect ranking across\nall importance levels.\nDespite this AUROC trade-off, ExpNet remains\nhighly robust across all three tasks, outperforming\nattention-only baselines by substantial margins and\nranking competitively against more complex attri-\nbution methods like AttCAT and GAE. The con-\nsistency of ExpNet\u2019s F1 improvements across di-\nverse tasks\u2014sentiment analysis (SST-2), grammat-\nical acceptability (CoLA), and hate speech detec-\ntion (HateXplain)\u2014demonstrates that the learned\nattention-to-rationale mapping generalizes beyond\ndataset-specific artifacts to capture fundamental\npatterns in how human-important features manifest\nin transformer attention.\n5.2\nAblation Study\nTo understand the contribution of different archi-\ntectural components in ExpNet, we conduct abla-\ntion studies across all three datasets using various\nfeature combinations. Table 4 presents F1-score\nresults comparing five variants: Hidden State (HS)\nfeatures alone, combined HS and ExpNet\u2019s atten-\ntion features (HS + ExpNet), task-to-token atten-\ntion only, token-to-task attention only, and the com-\nplete ExpNet architecture.\nTask-Specific Patterns.\nOn SST-2, task-to-\ntoken attention alone (0.397) nearly matches full\nFigure 2: AUROC values across datasets show ExpNet consistently achieves competitive ranking performance\n(often above 0.7), generally assigning higher scores to important tokens and lower scores to unimportant ones more\neffectively than most baselines.\nMethod\nSST-2\nCoLA\nHateXplain\nHS\n0.352\n0.198\n0.334\nHS + ExpNet\u2019s Attention\n0.268\n0.276\n0.362\nTask-to-Token Attention\n0.397\n0.391\n0.484\nToken-to-Task Attention\n0.269\n0.384\n0.503\nExpNet (Full)\n0.398\n0.468\n0.473\nTable 4: Ablation study F1-scores across three datasets.\nExpNet (0.398), indicating sentiment analysis re-\nlies primarily on task-driven attention patterns.\nCoLA demonstrates ExpNet\u2019s strongest advantage,\nwith the full model (0.468) substantially outper-\nforming the best component (0.391) showing that\ngrammatical analysis requires the complete bidirec-\ntional mechanism. HateXplain reveals that token-\nto-task attention (0.503) outperforms full ExpNet\n(0.473), suggesting hate speech detection benefits\nmost from bottom-up attention where offensive to-\nkens directly influence classification.\nCross-Dataset Insights. While ExpNet does\nnot achieve the best individual performance on all\ndatasets, it provides consistent, stable performance\nacross diverse domains. This validates the architec-\ntural design for task-agnostic generalization.\n5.3\nComputational Efficiency\nBeyond explanation quality, computational effi-\nciency is critical. ExpNet is substantially faster\nthan most baselines while maintaining superior ex-\nplanations, since it only requires a single forward\npass of a lightweight MLP over pre-computed at-\ntention features, with no gradients, sampling, or\nperturbations at inference time (see Appendix D\nfor throughput details).\n6\nConclusion\nExpNet shows that transformer attention can be\nsystematically refined into human-aligned expla-\nnations through supervised learning. Across three\nNLP tasks\u2014SST-2 (sentiment), CoLA (acceptabil-\nity), and HateXplain (hate speech)\u2014ExpNet con-\nsistently outperforms all thirteen baselines, with\nF1 gains of 13.9%, 24.8%, and 19.4% over the\nbest method in each domain. It also achieves task-\nagnostic generalization: training on two datasets\nand evaluating on the third still yields superior\nperformance to heuristic- or perturbation-based\nmethods, while remaining computationally effi-\ncient enough for real-time deployment.\nExpNet exhibits complementary strengths across\nmetrics: it achieves the highest F1 scores on all\ntasks, while its AUROC is strongest on SST-2 and\ncompetitive on CoLA and HateXplain. Future work\ncan extend this framework to decoder-only trans-\nformers (e.g., GPT-style LLMs) to enable efficient,\ninterpretable attributions in modern large language\nmodels.\nLimitations\nExpNet relies on human-annotated rationales dur-\ning training, which constrains its immediate appli-\ncability to tasks with existing explanation datasets.\nHowever, our cross-dataset experiments demon-\nstrate that ExpNet trained on available annotated\ntasks can generalize to new domains, suggest-\ning a path forward: training on diverse annotated\ndatasets enables zero-shot explanation on unanno-\ntated tasks. Future work should systematically eval-\nuate performance across broader domain ranges to\ncharacterize generalization boundaries.\nOur experiments focus on encoder-only BERT-\nbase models and datasets ranging from 872 to 1,924\ninstances. While this scope enables controlled eval-\nuation, ExpNet\u2019s behavior on larger-scale datasets,\nmore complex encoder architectures, and decoder-\nonly models (including modern LLMs) remains an\nopen empirical question. Extending to these set-\ntings represents a natural next step rather than a\nfundamental limitation; the core approach of learn-\ning from attention patterns generalizes architec-\nturally, though implementation details may require\nadaptation.\nExpNet operates at the token level, which natu-\nrally aligns with transformer tokenization but may\nnot capture phrase-level semantics for multi-word\nexpressions like \"not bad\" or \"over the moon.\" This\nrepresents a common challenge in token-attribution\nmethods rather than an ExpNet-specific issue. Ex-\ntensions to model phrasal importance explicitly\ncould enhance semantic alignment, though this\nintroduces additional complexity in aggregating\ntoken-level attention signals.\nThe framework assumes attention mechanisms\nprovide sufficient signal for explanation, which\nholds for many transformer architectures but may\nbe less complete for models that distribute compu-\ntation heavily through feed-forward layers or skip\nconnections. This attention-centric design choice\noffers computational efficiency and interpretability\nbut naturally focuses on one component of trans-\nformer reasoning. Hybrid approaches combining\nattention with hidden state or gradient information\ncould broaden coverage, as our ablation studies sug-\ngest different architectural components contribute\nvariably across tasks.\nExpNet\u2019s explanations necessarily reflect the\ncharacteristics of training rationales, including an-\nnotator subjectivity and any biases present in hu-\nman judgments. Research shows that different\nannotators highlight different tokens (Herrewij-\nnen et al., 2024), particularly across demographic\ngroups. Rather than viewing this as a flaw spe-\ncific to ExpNet, we recognize it as an inherent\nproperty of learning from human demonstrations:\nthe model learns patterns from its training signal.\nThis means ExpNet\u2019s outputs should be contex-\ntualized as \"explanations aligned with annotator\nreasoning patterns\" rather than ground truth. This\ncharacteristic applies broadly to supervised expla-\nnation methods and highlights the importance of\ndiverse, representative annotation processes. Ad-\nditionally, extractive rationales capture explicitly\nhighlighted tokens but cannot represent implicit\nreasoning or world knowledge, a fundamental lim-\nitation of token-level annotation schemes rather\nthan the learning approach.\nWhile we provide comprehensive quantitative\nevaluation (F1, AUROC, precision, recall) and com-\nputational benchmarks, complementing these met-\nrics with human-subject studies would strengthen\ninterpretability claims. Such studies could assess\nwhether ExpNet\u2019s explanations improve user un-\nderstanding, decision-making, or trust calibration\nbeyond annotation alignment. This represents valu-\nable future work to validate practical utility in de-\nployed systems.\nThese considerations frame productive research\ndirections\u2014extending domain coverage, scaling\nto diverse architectures, enriching annotation prac-\ntices, and validating with end users\u2014that build\nupon ExpNet\u2019s demonstrated effectiveness at learn-\ning generalizable explanation patterns from trans-\nformer attention.\nReferences\nSamira Abnar and Willem Zuidema. 2020a. Quantify-\ning attention flow in transformers. In Annual Meeting\nof the Association for Computational Linguistics.\nSamira Abnar and Willem Zuidema. 2020b. Quantify-\ning attention flow in transformers. In Annual Meeting\nof the Association for Computational Linguistics.\nGiuseppe Attanasio, Eliana Pastor, Chiara Di Bonaven-\ntura, and Debora Nozza. 2022. ferret: a framework\nfor benchmarking explainers on transformers. In\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary\n\n**Introduction to the Paper**\nThe paper focuses on developing an Explainable AI (XAI) method, specifically designed to understand and interpret transformer-based models in high-stakes applications such as healthcare, legal systems, and financial services. These applications often suffer from opacity, which hinders trust and accountability. Transformer architectures have proven valuable for model interpretability due to their self-attention mechanisms, which allow understanding of the model's focus and behavior.\n\n**Main Contributions**\n1. **ExpNet**: Introduces a lightweight neural network (ExpNet) that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike existing methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules.\n2. **Cross-Task Generalization**: Demonstrates cross-task explainability generalization in Natural Language Processing (NLP), showing the method's ability to generalize across different NLP tasks without task-specific tuning.\n3. **Comprehensive Ablation Studies**: Conducts detailed ablation studies to justify architectural choices, providing insights into which components of ExpNet are most critical for performance.\n\n**Methodology**\nThe paper uses BERT-base as its transformer model and focuses on analyzing attention patterns involving the [CLS] token (used for aggregation across layers). The key features extracted from BERT\u2019s final self-attention layer include:\n1. **Task-to-Token Attention**: Measures how strongly BERT's aggregation mechanism attends to a specific token.\n2. **Token-to-Task Attention**: Evaluates how much a token focuses on the task representation.\n\n**Training Procedure**\nExpNet is trained as a binary classifier using human-annotated rationales, with supervision from these annotations guiding its learning process. The training procedure includes:\n1. Supervision: Training ExpNet using human-annotated rationales.\n2. Correct-Prediction Filtering: Ensuring explanations are evaluated only on instances where the classifier's prediction matches the gold label.\n3. Focal Loss for Class Imbalance: Addressing class imbalance by down-weighting easy examples and focusing learning on hard-to-classify tokens.\n\n**Experiments**\nThe paper evaluates ExpNet across three NLP classification tasks (HateXplain, SST-2, CoLA) using distinct datasets with human-annotated rationales. The experiments employ a leave-one-task-out protocol to assess cross-task generalization. \n\n**Results and Findings**\n1. **Quantitative Evaluation**: ExpNet achieves the highest F1 scores across all three tasks, demonstrating better alignment with human rationales than existing baselines.\n2. **Ablation Study**: Reveals that task-to-token attention alone can achieve nearly optimal performance on SST-2, while full ExpNet consistently outperforms other variants.\n\n**Conclusion**\nExpNet shows that transformer attention patterns can be systematically refined into human-aligned explanations through supervised learning. The method outperforms existing baselines across three NLP tasks and demonstrates robustness to new domains without task-specific tuning. Future work should extend this framework to decoder-only transformers, enabling efficient and interpretable attributions in modern large language models.\n\n**Limitations**\n1. **Human-annotated Rationales**: ExpNet relies on human-annotated rationales during training, limiting its immediate applicability to tasks with existing explanation datasets.\n2. **Computational Efficiency**: While faster than most baselines, ExpNet's efficiency is still a concern for real-time deployment.\n\n**Future Work**\nExpNet\u2019s framework can be extended to decoder-only transformers and further studies should systematically evaluate performance across broader domain ranges."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "RL-BioAug: Label-Efficient Reinforcement Learning\nfor Self-Supervised EEG Representation Learning\nCheol-Hui Lee1, Hwa-Yeon Lee1, and Dong-Joo Kim1,2,3\nAbstract\u2014 The quality of data augmentation serves as a\ncritical determinant for the performance of contrastive learning\nin EEG tasks. Although this paradigm is promising for utilizing\nunlabeled data, static or random augmentation strategies often\nfail to preserve intrinsic information due to the non-stationarity\nof EEG signals where statistical properties change over time.\nTo address this, we propose RL-BioAug, a framework that\nleverages a label-efficient reinforcement learning (RL) agent to\nautonomously determine optimal augmentation policies. While\nutilizing only a minimal fraction (10%) of labeled data to\nguide the agent\u2019s policy, our method enables the encoder to\nlearn robust representations in a strictly self-supervised manner.\nExperimental results demonstrate that RL-BioAug significantly\noutperforms the random selection strategy, achieving substantial\nimprovements of 9.69% and 8.80% in Macro-F1 score on\nthe Sleep-EDFX and CHB-MIT datasets, respectively. Notably,\nthis agent mainly chose optimal strategies for each task\u2014for\nexample, Time Masking with a 62% probability for sleep stage\nclassification and Crop & Resize with a 77% probability for\nseizure detection. Our framework suggests its potential to replace\nconventional heuristic-based augmentations and establish a new\nautonomous paradigm for data augmentation. The source code\nis available at https://github.com/dlcjfgmlnasa/RL-BioAug.\nI. INTRODUCTION\nHealthcare and EEG analysis have achieved remarkable\nperformance improvements due to rapid advancements in\ndeep learning [1], [2]. Despite this progress, many studies\nrely on supervised learning, fundamentally limited by the\nneed for large-scale datasets labeled by experts [3]. The\nresulting substantial costs and time requirements serve as\na primary bottleneck for the practical deployment of deep\nlearning models in clinical settings [4].\nTo\naddress\nthis\nchallenge,\nself-supervised\nlearning\n(SSL)\u2014particularly contrastive learning\u2014has gained attention\nfor its ability to learn representations without labels in\nEEG analysis [5], [6]. The core principle of contrastive\nlearning is to maximize the similarity between positive pairs\ngenerated by data augmentation [7]. Consequently, the choice\nof augmentation technique is a critical factor that determines\nthe invariance that the model has to learn [8]. While the\nstandardized strategies are well-established in computer vision,\na unified methodology lacks in EEG analysis due to the\ncomplex nature of the signals [9]. Notably, data augmentation\nis not just an auxiliary tool for generalization as in supervised\n1The authors are with the Department of Brain and Cognitive Engineering,\nKorea University, Seoul, South Korea.\n2Dong-Joo Kim is with the Interdisciplinary Program in Precision Public\nHealth, Korea University, Seoul, South Korea.\n3Dong-Joo Kim is with the Department of Neurology, Korea Univer-\nsity College of Medicine, Seoul, South Korea (Corresponding author:\ndongjookim@korea.ac.kr)\nlearning; it is a fundamental component in contrastive learning\nthat shapes that quality of the representations [7], [10].\nHowever, the non-stationarity of EEG signals poses a\nsignificant challenge in defining an optimal augmentation\nstrategy [11]. Data augmentation hinges on a delicate balance\nbetween inducing sufficient diversity and preserving intrinsic\nintegrity; yet, a static and uniform approach that neglects\nsignal states is liable to distort the inherent patterns of data\nin EEG analysis tasks (e.g., sleep stage classification, seizure\ndetection). For instance, high-intensity noises are relatively\ntolerable in the Wake state with frequent EMG artifacts [12].\nIn contrast, REM sleep or Seizure intervals are fragile to the\nsame intensity where preserving subtle waveform variations\nis critical [12], [13]. These strong distortions can lead to the\nloss of intrinsic features, ultimately degrading the quality\nof representation learning [14], [6]. Therefore, an adaptive\nstrategy that dynamically adjusts augmentation types based\non the signal state is essential.\nWe validated the effectiveness of the proposed method\nusing two EEG benchmarks: the Sleep-EDFX [15] and\nCHB-MIT [16] datasets. Experimental results confirmed that\nour framework achieves superior classification performance\ncompared to random or fixed augmentation strategies.\nII. METHODS\nIn this study, we propose RL-BioAug, a framework\nthat simultaneously optimizes contrastive learning and data\naugmentation policies for EEG signals. The framework allows\nthe encoder and the agent to jointly optimize their parameters,\ndriving the cooperative learning process toward the most\neffective data augmentation strategy.\nA. Cooperative Optimization Loop\nRL-BioAug operates on a cyclic structure at every training\nstep in the following sequence: State Observation \u2192Action\n\u2192SSL Step: encoder update via contrastive loss \u2192RL\nStep: agent update via policy gradient.\n1) Non-causal Environments and Context-based Decision:\nRL is generally premised on causal assumption that actions\nof an agent alter the distribution of future states. In contrast,\napplying augmentation to a specific sample in our environment\ndoes not influence the physical properties of subsequent,\nindependent samples. Given this non-causal nature, we\ndesigned the agent to focus solely on analyzing the immediate\ncontext rather than formulating long-term future plans.\n2) Reinforcement Learning Problem Formulation: The key\ncomponents of this framework are defined as follows:\narXiv:2601.13964v1  [cs.LG]  20 Jan 2026\nFig. 1.\nOverview of the RL-BioAug Framework. The process consists of four distinct phases: (1) State Observation, where the state is extracted via a\nfrozen encoder; (2) Action, where the agent selects an optimal strong augmentation; (3) SSL Step, which updates the encoder using contrastive loss between\nweak and strong views; and (4) RL Step, which evaluates the representation quality using the Soft-KNN consistency score on a reference set to reward the\nagent.\n\u2022 State (st) \u2013 A state is defined as the embedding vector\nextracted from the freeze encoder ( f\u03b8) at the current time\nstep. This vector encapsulates the semantic information\nof the original signal and provides it to the agent. We\nemployed ResNet18-1d [17] as the encoder.\n\u2022 Action (at) \u2013 The agent selects and executes the optimal\nstrong augmentation from a discrete action space (A),\nwhich consists of five pre-defined transformations: Time\nMasking, Time Permutation, Crop & Resize, Time Flip,\nand Time Warp.\n\u2022 Reward (rt) \u2013 To quantitatively assess whether the\nselected action contributes to the improvement of the\nencoder\u2019s representation capability, we utilize the Soft-\nKNN consistency score.\nThe detailed workflow of RL-BioAug is organized as\nfollows: Section II-B describes the agent that performs actions.\nSections II-C and II-D present the overall training process\nincluding SSL and RL steps.\nB. Agent: Transformer-based Policy Network\nThe agent selects its augmentation policy by leveraging\npast augmentation history and reward feedback. To achieve\nthis, we employ a Transformer-based policy network (\u03c0\u03d5),\nwhich is renowned for its efficacy in processing sequential\ndata.\n\u2022 Feature Fusion \u2013 The input to the agent comprises\nthe current state representation (st) and the sequence\nof past K actions and rewards (i.e., {at\u2212K,...,at\u22121}\nand {rt\u2212K,...,rt\u22121}). These components are processed\nthrough an FC layer to be transformed into an integrated\ncontext vector.\n\u2022 Sequence Analysis \u2013 Positional embeddings are added\nto the integrated sequence before it is fed into the\nTransformer. The self-attention mechanism captures the\nhistorical context of policy-reward dynamics to infer the\ncurrent representation state.\n\u2022 Action Selection \u2013 The representation vector at the final\ntime step t is passed through an FC layer to generate\nan action probability distribution. Finally, the action is\nselected via Top-K Sampling, which effectively balances\nexploration and stability.\nC. Self-Supervised Learning Step: Encoder Update via Con-\ntrastive Loss\nThe selected action is immediately applied to the original\nsignal x to generate a strongly augmented view xstrong. This\nview is then paired with a weakly augmented view xweak\nfor contrastive learning. Notably, the encoder parameters are\ninstantaneously updated based on the view generated by the\nagent. We selected SimCLR [7] as our contrastive learning\nframework, given its high sensitivity to the quality of data\naugmentation [7] [10].\n\u03b8 \u2190\u03b8 \u2212\u03b7enc\u2207\u03b8LInfoNCE(f\u03b8(xweak), f\u03b8(xstrong))\n(1)\nLInfoNCE = \u2212log\nexp(sim(zweak,zstrong)/\u03c4)\n\u22112N\nk=1 \u22ae[k\u0338=i] exp(sim(zweak,zk)/\u03c4)\n(2)\nhere, Eq. (1) presents the update rule with learning rate\n\u03b7enc. With the encoded representations z = f\u03b8(x), Eq. (2)\naligns positive pairs (zweak,zstrong) while repelling negatives\n(zk) scaled by temperature \u03c4.\nThis process ensures that the agent\u2019s action induces\nstructural changes in the encoder\u2019s representation space,\nproviding a direct causal basis for the reward calculation.\nD. Reinforcement Learning Step: Agent Update via Policy\nGradient\n1) Reward Definition via Soft-KNN: We utilize Soft-\nKNN consistency score to quantitatively assess whether the\nagent\u2019s selected action contributes to improving the encoder\u2019s\nrepresentation capability. This score measures how tightly\nthe embeddings of the current batch form clusters with same-\nclass neighbors from a distinct reference set in the embedding\nspace.\nBased on the cosine similarity sim(zi,z j) between the\nembedding of the current sample zi and the embedding of\na sample z j in the reference set, the probability P(yi = c|zi)\nthat sample i belongs to a specific class c is derived as a\nsoftmax-weighted sum of the labels of the top-K nearest\nneighbors:\nP(yi = c|zi) = \u2211j\u2208Ni exp(sim(zi,z j)/\u03c4)\u00b7\u22ae(yj = c)\n\u2211k\u2208Ni exp(sim(zi,zk)/\u03c4)\n(3)\nHere, Ni denotes the set of K nearest neighbors of zi within\nthe reference set, and \u22ae(\u00b7) is an indicator function that returns\n1 if the condition is met and 0 otherwise. The final reward\nrt received by the agent is defined as the probability value\ncorresponding to the actual ground-truth label (ytrue) of the\nsample, providing a dense and continuous feedback signal\nbetween 0 and 1:\nrt = P(yi = ytrue|zi) = \u2211j\u2208Ni exp(sim(zi,z j)/\u03c4)\u00b7\u22ae(yj = ytrue)\n\u2211k\u2208Ni exp(sim(zi,zk)/\u03c4)\n(4)\n2) REINFORCE++ Optimization: To ensure the stable\nconvergence of the policy network in noisy EEG-signal\nenvironments, we adopt the REINFORCE++ algorithm [18].\na) Variance Reduction via Advantage: To suppress the\nvariance of the policy gradient, we utilize the advantage\nfunction At. We define the baseline bt as the average reward\nacross the mini-batch B and subtract it from the observed\nreward rt to compute the advantage:\nAt = rt \u2212bt,\nwhere\nbt = 1\n|B|\n|B|\n\u2211\ni=1\nri\n(5)\nb) Entropy Regularization: To prevent the agent from\nprematurely converging to a deterministic policy that relies\non a limited set of augmentations, we incorporate an entropy\nregularization term. We add the entropy of the policy\ndistribution H(\u03c0\u03d5) to the objective function with a coefficient\n\u03b3 = 0.1. This explicit regularization encourages the agent to\nmaintain high entropy, thereby promoting active exploration\nof diverse augmentation strategies.\nc) Final Objective Function: To define the optimization\nprocess, we apply a gradient descent step to update the policy\nnetwork parameters \u03d5 with a learning rate \u03b7agent:\n\u03d5 \u2190\u03d5 \u2212\u03b7agent \u2207\u03d5LPolicy(\u03d5)\n(6)\nThe objective function LPolicy(\u03d5) is formulated to maximize\nthe expected advantage while maintaining policy entropy:\nLPolicy(\u03d5) = \u2212E\nh\nlog\u03c0\u03d5(at|st)At\ni\n\u2212\u03b2\u03b3H(\u03c0\u03d5)\n(7)\nhere, the policy entropy term H(\u03c0\u03d5), which encourages\nexploration over the action space A, is explicitly defined\nas:\nH(\u03c0\u03d5) = \u2212\u2211\na\u2208A\n\u03c0\u03d5(a|st)log\u03c0\u03d5(a|st)\n(8)\nFinally, \u03b2 serves as a decaying factor that controls the\ntrade-off between exploration and exploitation. As training\nprogresses, \u03b2 is gradually reduced to allow the agent to\nshift its focus from exploring new strategies to exploiting the\nlearned optimal policy.\nIII. EXPERIMENTS\nA. Dataset Description\nWe utilized two EEG datasets with distinct characteristics\nto evaluate the generalizability and robustness of our proposed\nframework. For both datasets, we applied a 0.5\u201340 Hz band-\npass filter and performed z-normalization.\n\u2022 Sleep-EDFX [15] \u2013 We employed the Sleep-Cassette\nsubset of the Sleep-EDFX dataset for sleep stage\nclassification and used the single Fpz-Cz channel. In\naccordance with The American Academy of Sleep\nMedicine standards, we merged the N3 and N4 stages\nto reconfigure the data into five classes: Wake, N1, N2,\nN3, and REM.\n\u2022 CHB-MIT [16] \u2013 We utilized the CHB-MIT Scalp EEG\ndataset for epileptic seizure detection. The single Fp1-F7\nchannel was selected. The signals were segmented into\n4-second windows, and the labels were reconstructed for\nbinary classification: Seizure and Background.\nB. Experiment Protocol\nWe designed an experimental protocol to validate the\neffectiveness of RL-BioAug.\n1) Data Splitting (Subject-independent) \u2013 To evaluate ro-\nbust generalization performance, we adopted a subject-\nindependent split strategy, strictly partitioning training\nand testing datasets by subject. 80% of the subjects\nwere allocated for training, while the remaining 20%\nwere allocated for testing.\n2) Phase 1: Training RL-BioAug \u2013 In Phase 1, the\nRL agent learns the optimal data augmentation policy.\nFig. 2.\nExperiment Protocol.\nTABLE I\nCOMPARISON OF AUGMENTATION STRATEGIES ON SLEEP-EDFX AND CHB-MIT DATASETS.\nMethod\nAugmentation\nStrategy\nSleep-EDFX\nCHB-MIT\nB-ACC\nMF1\nB-ACC\nMF1\nSingle\nTime Masking\n65.52\n61.83\n68.50\n63.15\nTime Permutation\n65.33\n60.82\n67.45\n61.80\nCrop & Resize\n64.63\n60.40\n69.12\n64.25\nTime Flip\n64.22\n59.52\n66.82\n60.40\nTime Warp\n62.49\n57.80\n65.10\n58.95\nComposite\nRandom Selection\n64.48\n59.86\n68.05\n62.70\nRL-BioAug (Ours)\n72.72\n69.55\n76.24\n71.50\n- The best results in each column are shown in bold, while the second-best results are underlined.\nGiven that the reward function (Soft-KNN Consistency\nScore) relies on ground-truth labels, we trained the\nagent using only a small fraction (10%) of the labeled\ndata from the training set. Additionally, we utilized a\nportion of this subset as the Reference set to compute\nthe reward. This demonstrates that meaningful policy\nlearning is achievable with minimal supervision.\n3) Phase 2: Contrastive Learning \u2013 In Phase 2, the\nencoder is trained while the pre-trained agent remains\nfrozen. We utilized the entire training dataset for SSL,\ncompletely excluding labels. This design ensures that\nthe model learns representation solely through the\naugmented views generated by the agent.\nC. Data Augmentation\n1) Strong Augmentation: The RL-BioAug agent selects\none technique from a pool of strong augmentations that\nsignificantly alter the temporal or structural characteristics of\nthe signals. This approach encourages the encoder to maintain\ninvariance against challenging views during contrastive learn-\ning. The details of the five strong augmentation techniques\nare as follows:\n\u2022 Time Masking masks arbitrary time intervals with\nzeros. It simulates electrode artifacts (e.g., pop-outs)\nand compels the model to predict the missing intervals\nbased on the surrounding context.\n\u2022 Time Permutation segments the signal into multiple\nintervals and randomly shuffles their order. It encourages\nthe model to learn the global structure and sequential\ninformation of the data.\n\u2022 Crop & Resize crops a local temporal region and\nresizes it to the original length. It prevents reliance on\nglobal patterns and forces the model to focus on detailed\nwaveform features.\n\u2022 Time Flip reverses the time axis. It induces the model to\nlearn features that remain robust even when the temporal\nflow of the signal is inverted.\n\u2022 Time Warp arbitrarily accelerates or decelerates the\nspeed of signal. It helps overcome inter-subject speed\nvariations, allowing the model to learn features which\nare robust to changes in temporal duration.\n2) Weak Augmentation: Weak augmentation serves as an\nanchor view that preserves the semantic integrity of the\noriginal data. To achieve this, we applied a combination\nof Jittering and Scaling to the original signal. The details of\nthe two weak augmentation techniques are as follows:\n\u2022 Jittering injects gaussian noise with a standard deviation\nof 0.01 into the original signal. It simulates inherent\nsensor noise.\n\u2022 Scaling randomly scales the signal amplitude within a\nrange of approximately 2%. It addresses inter-subject\nmagnitude variations, ensuring the model remains invari-\nant to minor amplitude fluctuations.\nD. Evaluation Metrics\nWe employed Balanced Accuracy (B-ACC) and Macro\nF1-Score (MF1) as performance metrics to deal with the\nclass imbalance present in the datasets. The details of each\nmetric are as follows:\n\u2022 Balanced Accuracy (B-ACC) is defined as the arith-\nmetic mean of recall for each class. This metric is\nsuitable for evaluating the model\u2019s overall discriminative\ncapability in scenarios with severe data imbalance,\nensuring that performance is not biased toward majority\nclasses.\nB-ACC = 1\nC\nC\n\u2211\ni=1\nRecalli\n(9)\n\u2022 Macro F1-Score (MF1) is defined as the unweighted\nmean of per-class F1-scores. This metric treats all\nclasses equally, thereby fairly reflecting classification\nperformance on minority classes.\nMF1 = 1\nC\nC\n\u2211\ni=1\n2\u00b7Precisioni \u00b7Recalli\nPrecisioni +Recalli\n(10)\nHere, C denotes the number of classes.\nIV. RESULT\nA. Comparison of Augmentation Strategies\nAs presented in Table 1, the performance of single\naugmentation techniques varied significantly depending on the\ndataset characteristics. For Sleep-EDFX, Time Masking, which\npreserves temporal context, demonstrated the most effective\nperformance (B-ACC: 65.52%; MF1: 61.83%). For CHB-MIT,\nCrop & Resize, which emphasizes local patterns, achieved\nsuperior performance (B-ACC: 69.12%, MF1: 64.25%). In\nTABLE II\nIMPACT OF TOP-K SAMPLING STRATEGY.\nTop-K\nSleep-EDFX\nCHB-MIT\nB-ACC\nMF1\nB-ACC\nMF1\n1\n71.04\n68.18\n74.85\n69.82\n2\n71.35\n68.50\n75.60\n70.95\n3\n72.72\n69.55\n76.24\n71.50\n4\n72.34\n68.83\n75.92\n71.10\n5\n71.23\n68.32\n75.15\n70.45\n- The best results in each column are shown in bold, while the\nsecond-best results are underlined.\ncontrast, Time Warp, which distorts temporal dynamics,\nyielded poor results on both datasets. This strategy recorded\nthe lowest B-ACC of 62.49% and 65.10%, respectively.\nConsequently, the Random Selection strategy revealed clear\nlimitations, as it indiscriminately applies even detrimental\naugmentations without considering the data context. In\ncontrast, RL-BioAug achieved remarkable performance on\nboth Sleep-EDFX (B-ACC: 72.72%, MF1: 69.55%) and\nCHB-MIT (B-ACC: 76.24%, MF1: 71.50%). These results\nrepresent a substantial improvement of approximately 7.1%p\nto 7.7%p across both metrics compared to the best performing\nsingle baseline for each dataset. These findings validate that\nRL-BioAug adaptively selects optimal augmentations which\npreserve semantic integrity based on each sample.\nB. Ablation Experiments\n1) Impact of Top-K Sampling Strategy: Table\nII sum-\nmarizes the model performance with respect to the Top-K\nsampling strategy. The model yielded the highest performance\nwith K = 3 on both Sleep-EDFX (B-ACC: 72.72%, MF1:\n69.55%) and CHB-MIT (B-ACC: 76.24%, MF1: 71.50%).\nSpecifically, when K = 1, the selection was restricted to the\nsingle policy with the highest reward, thereby limiting the\ndiversity of augmentations and resulting in relatively lower\nperformance (e.g., Sleep-EDFX B-ACC: 71.04%). Conversely,\nincreasing K to 4 or higher introduced unnecessary noise by\nincorporating sub-optimal policies with lower rewards into\nthe training process (e.g., B-ACC: 71.23% at K = 5). These\nfindings suggest that selecting the Top-3 policies represents\nthe optimal balance, between ensuring augmentation diversity\nand maintaining training efficiency.\n2) Efficacy of Reward Mechanism: Table III compares the\nmodel performance across different reward mechanisms. The\nresults indicate that the Soft-KNN-based reward approach\nachieved the highest performance. This method ensured\nsuperior stability and enabled the agent to effectively converge\non optimal policies by providing dense rewards. Consequently,\nit reached (B-ACC: 72.72%, MF1: 69.55%) on Sleep-EDFX\nand (B-ACC: 76.24%, MF1: 71.50%) on CHB-MIT, outper-\nforming other reward mechanisms. In contrast, despite the\nadvantage of operating without labels, the SSL loss approach\ntends to prefer trivial augmentations that are easy to learn.\nThis led to the lowest performance (e.g., Sleep-EDFX B-ACC:\nTABLE III\nEFFICACY OF REWARD MECHANISM ON\nSLEEP-EDFX AND CHB-MIT DATASETS.\nReward\nSleep-EDFX\nCHB-MIT\nB-ACC\nMF1\nB-ACC\nMF1\nSSL Loss\n63.02\n59.82\n67.92\n62.50\nAccuracy\n67.85\n64.29\n72.15\n66.82\nSoft-KNN (Ours)\n72.72\n69.55\n76.24\n71.50\n- The best results in each column are shown in bold, while the\nsecond-best results are underlined.\nFig. 3.\nTraining dynamics of the data augmentation policy on (A) the\nSleep-EDFX dataset and (B) the CHB-MIT dataset.\n63.02%). The accuracy-based method is limited in its ability\nto finely tune the policy network due to the sparsity of the\nreward signal (e.g., Sleep-EDFX B-ACC: 67.85%).\nC. Analysis of Policy Networks\nTo validate the adaptability of the agent, we visualized\nthe evolution of action probabilities during training (Fig. 3).\nThe results confirm that the policy network autonomously\nconverges toward optimal augmentation strategies tailored to\nthe characteristics of each dataset.\nSpecifically, for the Sleep-EDFX (Fig. 3(A)), the selection\nprobability of Time Masking gradually increased, eventually\nstabilizing at approximately 62%. This aligns with the results\nin Table I, implying that the encoder\u2019s ability to infer global\nstructure from the surrounding context is critical for sleep\nstage classification. In contrast, for the CHB-MIT (Fig. 3(B)),\nCrop & Resize was adopted as the dominant strategy, reaching\na selection proabbility of 77%. This corresponds with the\ndomain knowledge that seizure detection relies on local\nwaveform patterns rather than global structures [16], [19].\nCollectively, these findings demonstrate that RL-BioAug\nis capable of autonomously identifying and applying optimal\ndata augmentation techniques tailored to data characteristics,\neven without prior domain knowledge.\nV. DISCUSSION & CONCLUSION\nThe experimental results demonstrate that an adaptive\nstrategy, actively exploring optimal transformations based\non individual data characteristics, is essential for robust rep-\nresentation learning in EEG signals. This marks a significant\ndeparture from conventional static methods relying on expert\nheuristics [6]. Specifically, our analysis (Table I, Fig. 3)\nreveals that Time Masking is the most effective strategy\nfor the Sleep-EDFX dataset, where capturing long-range\ntemporal context is crucial. In contrast, Crop & Resize yielded\noptimal performance for the CHB-MIT dataset, which requires\ncapturing local anomalies such as seizures [16], [19].\nIn contrast, the Random Selection strategy failed to match\nthe optimal single-technique performance, as it applies\naugmentations stochastically without regard for intrinsic data\ncharacteristics. This underscores that effective augmentation\nstrategies are highly task-dependent. RL-BioAug\u2019s superiority\nstems from the agent\u2019s ability to autonomously learn safe\nand valid transformations. By preserving signal integrity,\nit effectively eliminates the uncertainty associated with\nsubjective human intervention.\nDespite these contributions, our study presents several\nlimitations. First, the agent\u2019s action space is confined to a\npre-defined discrete pool. While effective at selecting combi-\nnations, the model cannot yet create novel transformations\ntailored to the signal\u2019s inherent characteristics. Second, the\ncomputational overhead inherent to RL imposes constraints\non real-time deployment, particularly in resource-limited\nenvironments. Third, the reward mechanism relies on partial\nsupervision. Given the prevalence of label noise in bio-signals\n[20], dependence on potentially imperfect ground truth risks\nintroducing negative bias into the policy learning process.\nFuture research will extend validation beyond EEG to\ndiverse bio-signals such as electrocardiograms (ECG) and\nelectromyograms (EMG). Furthermore, we aim to advance\nbeyond pre-defined discrete pools by establishing a dy-\nnamic generation pipeline. By exploring continuous action\nspaces and leveraging generative networks, our goal is to\nautonomously synthesize transformations that adapt to the\nsignal\u2019s characteristics.\nIn conclusion, RL-BioAug achieved superior performance\non both the Sleep-EDFX and CHB-MIT datasets by au-\ntonomously learning optimal data augmentation policies that\npreserve semantic structure without expert intervention. These\nresults underscore the framework\u2019s robust generalizability\nacross diverse pathological conditions and clinical tasks.\nUltimately, this work establishes a critical foundation for\nextending unsupervised medical data analysis to label-free\nenvironments.\nACKNOWLEDGMENT\nREFERENCES\n[1] Y. Roy, H. Banville, I. Albuquerque, A. Gramfort, T. H. Falk, and\nJ. Faubert, \u201cDeep learning-based electroencephalography analysis: a\nsystematic review,\u201d Journal of neural engineering, vol. 16, no. 5, p.\n051001, 2019.\n[2] A. Craik, Y. He, and J. L. Contreras-Vidal, \u201cDeep learning for\nelectroencephalogram (eeg) classification tasks: a review,\u201d Journal\nof neural engineering, vol. 16, no. 3, p. 031001, 2019.\n[3] H. Banville, O. Chehab, A. Hyv\u00e4rinen, D.-A. Engemann, and A. Gram-\nfort, \u201cUncovering the structure of clinical eeg signals with self-\nsupervised learning,\u201d Journal of Neural Engineering, vol. 18, no. 4, p.\n046020, 2021.\n[4] D. Kostas, S. Aroca-Ouellette, and F. Rudzicz, \u201cBendr: Using transform-\ners and a contrastive self-supervised learning task to learn from massive\namounts of eeg data,\u201d Frontiers in Human Neuroscience, vol. 15, p.\n653659, 2021.\n[5] C.-H. Lee, H. Kim, H.-j. Han, M.-K. Jung, B. C. Yoon, and D.-J.\nKim, \u201cNeuronet: A novel hybrid self-supervised learning framework\nfor sleep stage classification using single-channel eeg,\u201d arXiv preprint\narXiv:2404.17585, 2024.\n[6] E. Eldele, M. Ragab, Z. Chen, M. Wu, C. K. Kwoh, X. Li, and\nC. Guan, \u201cTime-series representation learning via temporal and\ncontextual contrasting,\u201d arXiv preprint arXiv:2106.14112, 2021.\n[7] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple framework\nfor contrastive learning of visual representations,\u201d in International\nconference on machine learning.\nPmLR, 2020, pp. 1597\u20131607.\n[8] T. Xiao, X. Wang, A. A. Efros, and T. Darrell, \u201cWhat should not be\ncontrastive in contrastive learning,\u201d in International Conference on\nLearning Representations, 2020.\n[9] C. Rommel, J. Paillard, T. Moreau, and A. Gramfort, \u201cData augmenta-\ntion for learning predictive models on eeg: a systematic comparison,\u201d\nJournal of Neural Engineering, vol. 19, no. 6, p. 066020, 2022.\n[10] Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola, \u201cWhat\nmakes for good views for contrastive learning?\u201d Advances in neural\ninformation processing systems, vol. 33, pp. 6827\u20136839, 2020.\n[11] A. Y. Kaplan, A. A. Fingelkurts, A. A. Fingelkurts, S. V. Borisov,\nand B. S. Darkhovsky, \u201cNonstationary nature of the brain activity\nas revealed by eeg/meg: methodological, practical and conceptual\nchallenges,\u201d Signal processing, vol. 85, no. 11, pp. 2190\u20132212, 2005.\n[12] R. B. Berry, R. Brooks, C. Gamaldo, S. M. Harding, R. M. Lloyd,\nS. F. Quan, M. T. Troester, and B. V. Vaughn, \u201cAasm scoring manual\nupdates for 2017 (version 2.4),\u201d pp. 665\u2013666, 2017.\n[13] P. Boonyakitanont, A. Lek-Uthai, K. Chomtho, and J. Songsiri, \u201cA\nreview of feature extraction and performance evaluation in epileptic\nseizure detection using eeg,\u201d Biomedical Signal Processing and Control,\nvol. 57, p. 101702, 2020.\n[14] B. K. Iwana and S. Uchida, \u201cAn empirical survey of data augmentation\nfor time series classification with neural networks,\u201d Plos one, vol. 16,\nno. 7, p. e0254841, 2021.\n[15] B. Kemp, A. H. Zwinderman, B. Tuk, H. A. Kamphuisen, and\nJ. J. Oberye, \u201cAnalysis of a sleep-dependent neuronal feedback loop:\nthe slow-wave microcontinuity of the eeg,\u201d IEEE Transactions on\nBiomedical Engineering, vol. 47, no. 9, pp. 1185\u20131194, 2000.\n[16] A. H. Shoeb, \u201cApplication of machine learning to epileptic seizure onset\ndetection and treatment,\u201d Ph.D. dissertation, Massachusetts Institute of\nTechnology, 2009.\n[17] Z. Wang, W. Yan, and T. Oates, \u201cTime series classification from scratch\nwith deep neural networks: A strong baseline,\u201d in 2017 International\njoint conference on neural networks (IJCNN).\nIEEE, 2017, pp. 1578\u2013\n1585.\n[18] J. Hu, \u201cReinforce++: A simple and efficient approach for aligning\nlarge language models,\u201d arXiv preprint arXiv:2501.03262, 2025.\n[19] E. Niedermeyer and F. L. da Silva, Electroencephalography: basic\nprinciples, clinical applications, and related fields. Lippincott Williams\n& Wilkins, 2005.\n[20] D. Karimi, H. Dou, S. K. Warfield, and A. Gholipour, \u201cDeep learning\nwith noisy labels: Exploring techniques and remedies in medical image\nanalysis,\u201d Medical image analysis, vol. 65, p. 101759, 2020."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning\"\n\n#### Main Contributions:\n- **Proposed Framework**: The paper introduces RL-BioAug, a framework that leverages reinforcement learning (RL) to autonomously determine optimal augmentation policies in self-supervised EEG representation learning. This approach aims to improve the quality of representations by preserving intrinsic information from non-stationary EEG signals.\n  \n- **Label-Efficient Learning**: By using only 10% labeled data, RL-BioAug achieves superior performance compared to random or fixed augmentation strategies on both Sleep-EDFX and CHB-MIT datasets. The framework significantly outperforms the baseline methods by achieving improvements of 9.69% and 8.80% in Macro-F1 score for these datasets.\n\n#### Methodology:\n- **RL-BioAug Framework**: RL-BioAug operates on a cyclic structure where the encoder and agent jointly optimize their parameters to learn effective data augmentation strategies.\n  \n- **Agent Design**: The agent is designed as a Transformer-based policy network that selects its augmentation policies based on past history and reward feedback. It employs feature fusion, sequence analysis, and action selection mechanisms.\n\n- **Contrastive Learning**: The framework uses SimCLR for contrastive learning, updating the encoder parameters based on the selected augmentation actions to ensure structural changes in the representation space.\n  \n- **Reward Mechanism**: RL-BioAug utilizes a Soft-KNN consistency score as its reward function. This method ensures stable convergence of the policy network and provides dense feedback signals.\n\n#### Key Findings:\n- **Adaptive Strategies**: The agent autonomously selects optimal augmentation strategies tailored to each task, such as Time Masking for sleep stage classification and Crop & Resize for seizure detection.\n  \n- **Performance Comparison**: RL-BioAug outperforms random or fixed augmentation strategies on both Sleep-EDFX and CHB-MIT datasets. Specifically, it achieves superior performance in terms of Balanced Accuracy (B-ACC) and Macro F1-Score.\n\n#### Conclusions:\n- **Adaptive Data Augmentation**: The framework demonstrates the effectiveness of adaptive data augmentation strategies that preserve semantic integrity based on individual sample characteristics.\n  \n- **Robustness and Generalizability**: RL-BioAug achieves superior performance across diverse pathological conditions and clinical tasks, highlighting its robust generalizability.\n\n- **Future Work**: Future research should extend validation to other bio-signals like ECG and EMG, establish a dynamic generation pipeline for continuous action spaces, and explore techniques to mitigate label noise in bio-signals.\n\n### Summary\nRL-BioAug is an innovative framework that uses reinforcement learning to optimize data augmentation policies in self-supervised EEG representation learning. By leveraging only 10% labeled data, it significantly outperforms existing methods on critical datasets like Sleep-EDFX and CHB-MIT, achieving substantial improvements in classification performance. The framework's adaptive strategies ensure robust and effective representations that are tailored to the specific characteristics of each task."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "IEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n1\nVision Also You Need: Navigating\nOut-of-Distribution Detection with Multimodal\nLarge Language Model\nHaoran Xu\u2217, Yanlin Liu\u2217, Zizhao Tong\u2217, Jiaze Li, Kexue Fu, Yuyang Zhang,\nLongxiang Gao, Senior Member, IEEE, Shuaiguang Li, Xingyu Li, Yanran Xu, Changwei Wang\nAbstract\u2014Out-of-Distribution (OOD) detection is a critical\ntask that has garnered significant attention. The emergence of\nCLIP has spurred extensive research into zero-shot OOD detec-\ntion, often employing a training-free approach. Current methods\nleverage expert knowledge from large language models (LLMs)\nto identify potential outliers. However, these approaches tend to\nover-rely on knowledge in the text space, neglecting the inherent\nchallenges involved in detecting out-of-distribution samples in the\nimage space. In this paper, we propose a novel pipeline, MM-\nOOD, which leverages the multimodal reasoning capabilities of\nMLLMs and their ability to conduct multi-round conversations\nfor enhanced outlier detection. Our method is designed to\nimprove performance in both near OOD and far OOD tasks.\nSpecifically, (1) for near OOD tasks, we directly feed ID images\nand corresponding text prompts into MLLMs to identify potential\noutliers; and (2) for far OOD tasks, we introduce the sketch-\ngenerate-elaborate framework: first, we sketch outlier exposure\nusing text prompts, then generate corresponding visual OOD\nsamples, and finally elaborate by using multimodal prompts.\nExperiments demonstrate that our method achieves significant\nimprovements on widely used multimodal datasets such as Food-\n101, while also validating its scalability on ImageNet-1K.\nIndex Terms\u2014Out-of-distribution detection, multimodal large\nlanguage models, zero-shot learning, CLIP, anomaly detection,\nvision-language models\nI. INTRODUCTION\nAmidst the continuous developments in deep learning, var-\nious [1] expert models have achieved corresponding success\nacross diverse domains. However, there is a rising focus\n\u2020 Changwei Wang is the corresponding author (changweiwang@sdas.org).\n\u2217Equal contribution.\nHaoran Xu is with Zhejiang University. Yanlin Liu is with Tsinghua\nUniversity. Zizhao Tong is with University of Chinese Academy of Sciences.\nJiaze Li is with Zhejiang University. Kexue Fu, Longxiang Gao, Shuaiguang\nLi and Changwei Wang are with the Key Laboratory of Computing Power Net-\nwork and Information Security, Ministry of Education, Shandong Computer\nScience Center (National Supercomputer Center in Jinan), Qilu University of\nTechnology (Shandong Academy of Sciences), Jinan, 250013, China; Shan-\ndong Provincial Key Laboratory of Computing Power Internet and Service\nComputing, Shandong Fundamental Research Center for Computer Science,\nJinan, 250014, China; Yuyang Zhang is with University of Electronic Science\nand Technology of China. Xingyu Li is with Mohamed bin Zayed University\nof Artificial Intelligence. Yanran Xu is with RWTH Aachen University.\nThis work was supported by the Shandong Provincial Key R&D\nProgram\nNo.2025KJHZ013,\nNational\nKey\nR&D\nProgram\nof\nChina\nNo.2025YFE0216800, Shandong Provincial University Youth Innovation and\nTechnology Support Program No.2022KJ291, Shandong Provincial Natural\nScience Foundation for Young Scholars Program No.ZR2025QC1627 and\nQilu University of Technology (Shandong Academy of Sciences) Youth\nOutstanding Talent Program No. 2024QZJH02, Shandong Provincial Natural\nScience Foundation for Young Scholars (Category C) No.ZR2025QC1627.\non addressing model generalization in current research. It is\nwell known that expert models in deep learning are often\ntrained on closed datasets and then tested on expert-specific\ntasks which requires that training and testing datasets share\nidentical distribution. Nevertheless, in complex open-world\nscenarios, the distribution of the training set and the test set is\noften considerable inconsistency, even substantial divergence,\nwhich often leads to the failure of the expert model in the\nreal scenario. In some special scenarios, such as autonomous\ndriving scenarios [2], the impact of these errors and failures\ncan be profoundly serious. Therefore, it is equally important to\nidentify out-of-distribution (OOD) samples and maintain good\nperformance on ID samples.\nPrevious methods usually focus on detecting OOD examples\nonly using closed dataset in training to predict OOD samples\nby lower confidence or higher energy. However, these methods\nare often less effective in the real scene. ATOM [3] claimed\nthat selecting more challenging outlier samples can help the\nmodel learn a better decision boundary [4] between ID and\nOOD. ID-like [5] proposed that the sampling ID-like samples\ncome from the ID samples without auxiliary outliers dataset,\nbecause most challenging OOD samples can improve the\nperformance. Recently, MCM [6] introduces the zero-shot\nOOD detection method which borrows large-scale VLMs, i.e.,\nCLIP [7].\nFurther, EOE [8] extends the training-free framework [9]\nby addressing the limitation of MCM in understanding out-\nof-distribution (OOD) samples. It leverages a large language\nmodel (LLM) to generate textual descriptions for OOD sam-\nples, thereby enhancing the model\u2019s ability to envision poten-\ntial outlier exposure under a zero-shot OOD detection setting.\nTo sum up, the key to improve OOD detection is how\nto effectively generate outlier samples, regardless of whether\nthe approach is training-free or involves fine-tuning. Hence,\nEOE attempts to leverage the reasoning ability of LLMs to\naddress this challenge without other auxiliary datasets, but it\noverlooks one of the most remarkable properties of LLMs:\nmulti-round conversations. Meanwhile, with the advancement\nof multimodal large language models (MLLMs), multimodal\nreasoning has demonstrated significant potential.\nIn this paper, we take a step further in EOE and employ\nmultimodal large language models (MLLMs) to address this\nchallenge. For near OOD task and far OOD task, we proposal a\ndetailed and targeted pipeline which harnesses the Multimodal\nreasoning and Multi-round conversation capabilities\n[10] to\narXiv:2601.14052v1  [cs.CV]  20 Jan 2026\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n2\nText space\n\u201cSiamese\u201d\nImage space\nOOD\ndetection\nAuxiliary\nOOD datasets\n\u201cToy Dogs\u201d\n\u201cSmall Dogs\u201d\n\u201cDomestic Cats\u201d\n\u2026\nAdversarial\nTraining\n(a) ATOM\n\u2026\nID samples\nOOD\ndetection\nText space\n\u201cSiamese\u201d\nImage space\nPrompt\nLearning\n(b) ID-like\nLLM\nText space\n\u201cSiamese\u201d\nImage space\nOOD\ndetection\nEnvision\nOOD classes\n\u201cWorking Dogs\u201d\n\u201cMedium Dogs\u201d\n\u201cHairless Cats\u201d\n\u2026\n(c) EOE\nMLLM\nDiffusion\nN Rounds\n(Optional)\nOOD\ndetection\nText space\n\u201cSiamese\u201d\nImage space\nEnvision\nOOD classes\n(d) MM-OOD (Ours)\nFig. 1. Illustrations of (a) ATOM, (b) ID-like, (c) EOE, and (d) MM-OOD (Ours). Previous methods such as (a), (b), and (c) typically utilized knowledge\nfrom a single modality (either text or vision) or relied on auxiliary datasets. Our approach stimulates the multimodal reasoning and multi-round dialogue\ncapabilities of MLLMs.\nenvision potential outlier exposure for Out-Of-Distribution\ndetection, termed MM-OOD.\nTechnically, we straightforwardly feed ID images and cor-\nresponding text prompt into MLLMs for near OOD task to\npropose potential outlier exposure. Nonetheless, in far OOD\ntasks, we find that the outlier samples generated by MLLMs\nwhen feeding images tend to be very similar to the input\nimage space, leading to the failure of the pipeline in handling\nfar OOD tasks. Upon analysis, we infer that the outlier\nexposure provided by MLLMs is biased toward sampling\nnear the input image space. When ID samples are fed into\nMLLMs, this bias restricts the model\u2019s imaginative explo-\nration of its stored knowledge. This observation motivates\nus to employ pretrained generative models to produce OOD\nsamples into MLLMs. Thus, we design the sketch-generate-\nelaborate framework which is the form of multi-round con-\nversations: sketch outlier exposure by text prompts, generate\ncorresponding visual OOD samples, and elaborate by visual\nOOD samples and fine-grained text prompts. Our contributions\ncan be summarized as follows. First, we propose leveraging\nthe multi-modal reasoning and multi-round conversation ca-\npability of multimodal large language models (MLLMs) to\nenhance outlier exposure. Second, we introduce MM-OOD,\nwhich incorporates differentiated designs for near OOD and\nfar OOD tasks: for the former, we feed the in-distribution (ID)\nimage and text prompt directly into MLLMs; for the latter,\nwe propose a sketch-generate-elaborate framework. Third, our\nmethod MM-OOD consistently outperforms current state-of-\nthe-art baselines across extensive experiments.\nII. RELATED WORKS\nA. Traditional OOD Detection.\nTraditional out-of-distribution (OOD) detection has been\nextensively studied in both visual [11] and textual [12]\u2013[14]\nclassification tasks. Existing methods improve OOD detection\nthrough various strategies, including logit-based scoring [15],\n[16], distance-based detectors [17], robust representation learn-\ning [18], and pseudo-OOD sample generation [19]. These\napproaches can be broadly categorized into classification-\nbased [20], [21], density-based [22], [23], and reconstruction-\nbased [24] techniques.\nClassification-based methods rely on a well-trained in-\ndistribution (ID) classifier and construct a scoring function to\nidentify OOD samples. This scoring function may be derived\nfrom the input, hidden [25]\u2013[29], output [15], [16], [30], or\ngradient spaces [31], [32]. However, when the label space of\nthe test data differs from that of the training data, conventional\nOOD detection methods typically require either retraining the\nmodel from scratch or fine-tuning, which incurs substantial\ncomputational overhead.\nB. Zero-Shot OOD Detection\nIn the zero-shot setting, which leverages vision-language\nmodels (VLMs) without requiring further training on in-\ndistribution (ID) data, zero-shot out-of-distribution OOD de-\ntection methods have shown promising results. However, re-\ncent works such as MCM [33] rely on closed-set ID labels\nand underutilize CLIP\u2019s open-world potential. ZOC [34] and\nCLIPN [35] address the open-world setting but require ad-\nditional auxiliary datasets, limiting their scalability. Despite\ntheir effectiveness, these methods have not fully harnessed\nopen-set OOD detection without additional auxiliary datasets.\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n3\nText\nEncoder\nEnvision\nN Rounds\n\u2026\nDiffusion\nFar OOD Detection\n\u2026\nTerriers\nGuard Dogs\nLap Cats\nToy Dogs\n\u201cTerriers\u201d\nA photo of a ___\nText\nEncoder\nMultimodal Large Language Model (MLLM)\n\u2026\nSelect\nr\niI\nnear\nq\n\u2026\nEnvision\nBalinese\nBirman\nBombay\nKorat\nnear\nL\nImage\nEncoder\n\u2026\n\u2026\nInput\nSamples:\n\u201cSiamese\u201d\nID Label\n\u201cSiamese\u201d\nID Label\nA photo of a ___\n1\nfar\nq\n2\nfar\nq\n3\nfar\nq\nNear OOD Detection\n1\nK\nL\nI T +\n\uf0d7\n1\n1\nK\nI T +\n\uf0d7\n1\nK\nI T\n\uf0d7\n\u2026\n\u2026\n1\n1\nI T\n\uf0d7\n\u2026\n\u2026\n1\nnI\nT\n\uf0d7\nn\nK\nI\nT\n\uf0d7\n1\nn\nK\nI\nT +\n\uf0d7\nn\nK\nL\nI\nT +\n\uf0d7\n\u2026\n\u2026\nnI\n1I\n1T\n\u2026\n\u2026\nK\nT\n1\nK\nT +\nK\nL\nT +\n1T\n\u2026\n\u2026\nK\nT\n1\nK\nT +\nK\nL\nT +\nID\nLabel\nOOD\nLabel\nID\nLabel\nOOD\nLabel\n1\nfar\nL\n2\nfar\nL\n3\nfar\nL\nFig. 2. The pipeline of MM-OOD. We incorporate text prompts (qnear for near-OOD detection and q1\nfar\u2013q3\nfar for far-OOD detection) and input images into\nMLLMs to generate potential out-of-distribution (OOD) class labels. These generated labels, combined with in-distribution (ID) class labels, are jointly encoded\nvia a text encoder to construct a classifier for OOD detection.\nIt constrains the future application and development of these\nmethods in practice.\nC. LLM-Guided Outlier Detection\nWith the recent development of Large Language Models\n(LLMs), LLM-guided Outlier Detection methods are also\nemerging, such as EOE [8]. EOE leveraged the expert knowl-\nedge and reasoning capabilities of LLMs to envision potential\noutliers and tackle the OOD detection problem. However,\nrecent advancements in Multimodal Large Language Models\n(MLLMs) [36], [37] have led to improvements in multimodal\nreasoning [38], [39]. EOE cannot fully utilize the potential\nof MLLM by using only textual reasoning capabilities, which\nlimits the further improvement of the method. Our method,\nMM-OOD, innovatively uses MLLM-Guided Outlier Detec-\ntion to address this limitation.\nIn summary, our approach MM-OOD focuses on zero-shot\nOOD detection task without auxiliary datasets with leveraging\ninformation from both textual and visual domains to facilitate\nthe identification and analysis of potential outlier exposure.\nFigure. 1 shows the comparison between MM-OOD and other\nthree representative methods.\nIII. METHOD\nA. Preliminaries\nProblem Setup. Out-of-distribution (OOD) detection can be\nformulated as a binary classification problem [40], where\nthe goal is to determine whether a given input x originates\nfrom the in-distribution (ID) data distribution PID or an OOD\ndistribution POOD. This is mathematically expressed as:\nD\u03b8(x; PID, I, T ) =\n(\nID\nS(x) \u2265\u03b8\nOOD\nS(x) < \u03b8\n(1)\nwhere D(\u00b7) is the OOD detector, x denotes the input image,\nx \u2208X, X = {ID, OOD}, and PID defines the space of ID class\nlabels. The OOD detection score function S is derived from\nthe similarity between the visual representation I(x) and the\ntextual representation T (t), where t is the textual input to the\ntext encoder, such as \u201da photo of a ID class\u201d, and \u03b8 is the\nthreshold to distinguish ID/OOD classes.\nOOD Detection Tasks. In this paper, we refer to EOE setting\nand categorize OOD detection tasks into two types: near and\nfar OOD detection.\n\u2022 Near OOD detection focuses on identifying OOD\nclasses that are closely related to ID classes, especially\nin visually similar categories, such as \u201ddog\u201d and \u201dwolf\u201d.\n\u2022 Far OOD detection This involves identifying OOD\nclasses that are distant from ID classes within the label\nspace and semantic space.\nB. MM-OOD\nIn this section, we introduce our proposed method MM-\nOOD. The pipeline is shown in the Figure 2. Overall, we\ndivide MM-OOD into two steps: propose outlier class labels\nand calculate detection score based on outlier class labels.\nMM-OOD is mainly based on the first step to improve.\nSpecifically, we design corresponding structures based on the\ncharacteristics of different tasks.\nFor near OOD detection task, we directly feed ID images\nand corresponding text prompts into MLLMs to propose po-\ntential outlier exposure. On the other hand, for far OOD detec-\ntion task, we propose the sketch-generate-elaborate framework\nto reason outlier classes.\nNear OOD Detection Branch. For each ID label, it is\nnecessary to select the most representative image. Firstly, we\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n4\ncalculate the average feature vector by CLIP of all images.\nHere is corresponding equation:\n\u00affi = 1\nNi\nNi\nX\nj=1\nf(Iij),\n(2)\nwhere Ni is the number of images for ID label i and f(Iij)\nis the feature vector from image j of ith ID class label.\nThen, the image whose feature vector is closest to this\naverage is then designated as the representative image for that\nlabel.\nIr\ni = arg min\nIij\u2208Ii \u2225f(Iij) \u2212\u00affi\u2225,\n(3)\nwhere Ir\ni is the representative ID image in ID label. Subse-\nquently, we straightforwardly feed the ID class label and the\ncorresponding ID image into MLLM to generate No outlier\nclass labels. Assuming we have K ID class labels, we will\ngenerate No\u00d7K = L outlier class labels. Details of the prompt\ninput to MLLMs are presented in the Appendix.\nFar OOD Detection Branch. For far OOD detection task,\nwe conduct a series of experiments and explorations. We find\nthat when we directly feed both the ID image and ID class\nlabel into the MLLM, following the pipeline for the near-OOD\ntask, the results are below expectations. Through analysis, we\nbelieve that this is due to the induction bias of the MLLM.\nCompared to pure text input, feeding the ID image constrains\nthe MLLM\u2019s generative potential and causes it to sample near\nthe ID image space, which hinders performance on far OOD\ndetection tasks.\nBased on the aforementioned points, in order to achieve\nfar sampling of outlier samples, we first guide MLLM to\nsummarize these classes into M respective primary categories.\nAll subsequent operations are conducted using these M pri-\nmary categories, allowing us to avoid being restricted to local\noptimal solutions.\nTherefore,\nwe\nconsider\ngenerating\nout-of-distribution\n(OOD) images using generative models and feeding these\nOOD images into the MLLM instead of the ID images. The\nOOD images generated by the pretrained generative model\nserve as a demonstration of the OOD space in the image\ndomain, guiding the MLLM to generate outlier class labels\nthat are farther removed from the original ID space. Since the\nOOD images are distant from the ID image space, the MLLM\nshifts from near sampling to far sampling, which enhances\nMM-OOD\u2019s performance in far OOD detection tasks. Mean-\nwhile, the OOD images input into the MLLM are maximally\ndistinct from the ID image space, enabling our methodology to\noutperform our method with ID image input, EOE and random\nsampling method. Figure 3 demonstrates this. Furthermore, we\nutilize the multi-round conversation capability of MLLM to\npropose the sketch-generate-elaborate framework.\nSpecifically, the sketch operation initiates the MLLM to\nform a preliminary conception of the text label for the\nOOD sample. Subsequently, the generate operation directs the\ngenerative model to produce the corresponding OOD image\nbased on this text label. Finally, the elaborate operation inputs\nthe OOD image and the aforementioned text labels into the\nMLLM for the final generation. The framework essentially ap-\nplies the multi-modal and multi-round dialogue characteristics\ndistance\n\u201ccaldron\u201d\n\u201cchocolate cake\u201d\nMLLM\n\u201cchocolate mousse\u201d\nfood\ncooker\nfood\ncooker\nDiffusion\ndistance\nMLLM\nMLLM\n\u201cchocolate cake\u201d\n(a)\n(b)\nchocolate cake\nMLLM\nMLLM\n- chocolate mousse\n- chocolate pudding\n- dessert\n- pie\n- cookie\n- brownie\n- caldron\n- bucket\n- basin\n- pot\n- drum\n- bowl\nwith sd\noutlier class label\n(c)\nwithout sd\noutlier class label\nFig. 3.\nIllustration of outlier class label generation: (a) without stable\ndiffusion, (b) with stable diffusion, and (c) comparison between the two\napproaches.\nof MLLM, while also conforming to the logic of the chain\nof thought (CoT) [41]. We give an in-depth analysis and a\npractical example in the section IV-D. The specific prompt is\nprovided in Figure 4. The detailed algorithm flow is in the\nAppendix.\nDetection Score Calculation. In this section, we enhance the\nseparation between in-distribution (ID) and out-of-distribution\n(OOD) samples by computing a detection score. Building upon\nthe outlier exposure strategy proposed in EOE [8], we leverage\nthe generated outlier class labels to perform OOD detection.\nThe detailed formulation is given below.\nsi(x) =\nI(x) \u00b7 T (ti)\n|I(x)| \u00b7 |T (ti)|, ti \u2208Yid \u222aYoutlier\n(4)\nS(x) = max\ni\u2208[1,K]\nesi(x)\nPK+L\nj=1 esj(x)\n\u2212\u03b2 \u00b7\nmax\nk\u2208(K,K+L]\nesk(x)\nPK+L\nj=1 esj(x)\n(5)\nwhere \u03b2 is a hyperparameter. In our setting, we set it to 0.25.\nFinally, the OOD detector is followed by formula 1.\nIV. EXPERIMENTS\nA. Setups\nNear OOD Detection. We use ImageNet-10 and ImageNet-20\nas ID and OOD datasets in a complementary manner, where\nImageNet-10 serves as the ID dataset and ImageNet-20 as the\nOOD dataset, and vice versa. Both datasets are subsets of\nImageNet-1K. ImageNet-10 is constructed to mimic the class\ndistribution of CIFAR-10 [43], while ImageNet-20 consists of\n20 classes semantically similar to those in ImageNet-10. These\ndatasets were introduced by MCM [6].\nFar OOD Detection. For in-distribution (ID) datasets, we\nutilize Oxford-IIIT Pets [44], Stanford Cars [45], CUB-200-\n2011 [46], Food-101 [47], and ImageNet-1K [48]. Among\nthese, ImageNet-1K is used to demonstrate the scalability\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n5\nTABLE I\nCOMPARISON RESULTS WITH MCM [6], MAXLOGIT [42], ENERGY [16], AND THE MOST RECENT EOE [8] ARE REPORTED UNDER DIFFERENT\nDATASETS. BOLD FONTS HIGHLIGHT THE BEST ACCURACY. K DENOTES THE NUMBER OF CLASSES IN THE CORRESPONDING IN-DISTRIBUTION (ID)\nDATASET.\nOOD Dataset\nAverage\nID Dataset\nMethod\niNaturalist\nSUN\nPlaces\nTexture\nFPR95\u2193\nAUROC\u2191\nFPR95\u2193\nAUROC\u2191\nFPR95\u2193\nAUROC\u2191\nFPR95\u2193\nAUROC\u2191\nFPR95\u2193\nAUROC\u2191\nCUB-200-2011\nMCM\n31.48\n91.00\n7.39\n98.50\n9.73\n98.00\n7.05\n98.74\n13.91\n96.56\nMaxLogit\n15.31\n95.08\n0.01\n100.00\n0.11\n99.97\n0.00\n100.00\n3.86\n98.76\nEnergy\n15.92\n94.92\n0.01\n100.00\n0.10\n99.97\n0.02\n100.00\n4.01\n98.72\nEOE+LLaVA, ,L=4\u00d7K\n16.18\n94.63\n0.54\n99.83\n0.96\n99.77\n0.29\n99.92\n4.49\n98.54\nEOE+LLaVA, L=12\u00d7K\n14.80\n95.02\n0.16\n99.95\n0.30\n99.92\n0.02\n99.99\n3.82\n98.72\nOurs+LLaVA, ,L=4\u00d7K\n15.82\n94.77\n0.39\n99.88\n0.82\n99.81\n0.12\n99.96\n4.29\n98.61\nOurs+LLaVA, L=12\u00d7K\n15.23\n94.86\n0.17\n99.95\n0.33\n99.92\n0.02\n99.99\n3.94\n98.68\nStanford Cars\nMCM\n1.22\n99.52\n0.13\n99.92\n0.56\n99.81\n0.02\n99.97\n0.48\n99.81\nMaxLogit\n0.00\n100.00\n0.60\n99.85\n0.90\n99.74\n0.00\n100.00\n0.38\n99.90\nEnergy\n0.00\n100.00\n0.80\n99.74\n1.13\n99.65\n0.04\n99.99\n0.49\n99.84\nEOE+LLaVA, ,L=4\u00d7K\n0.00\n100.00\n0.08\n99.98\n0.35\n99.91\n0.00\n100.00\n0.12\n99.97\nEOE+LLaVA, L=12\u00d7K\n0.00\n100.00\n0.11\n99.98\n0.36\n99.91\n0.00\n100.00\n0.12\n99.97\nOurs+LLaVA, ,L=4\u00d7K\n0.00\n100.00\n0.09\n99.98\n0.36\n99.91\n0.00\n100.00\n0.11\n99.97\nOurs+LLaVA, L=12\u00d7K\n0.00\n100.00\n0.12\n99.97\n0.36\n99.91\n0.00\n100.00\n0.12\n99.97\nFood-101\nMCM\n5.85\n98.76\n1.07\n99.75\n2.03\n99.52\n2.87\n99.35\n2.95\n99.34\nMaxLogit\n1.34\n99.71\n0.19\n99.95\n0.62\n99.86\n5.58\n98.77\n1.93\n99.57\nEnergy\n1.31\n99.70\n0.22\n99.94\n0.61\n99.86\n8.59\n98.07\n2.68\n99.39\nEOE+LLaVA, ,L=4\u00d7K\n6.05\n98.63\n0.78\n99.78\n1.56\n99.61\n2.60\n99.43\n2.75\n99.36\nEOE+LLaVA, L=12\u00d7K\n4.62\n99.00\n0.66\n99.82\n1.33\n99.67\n2.27\n99.50\n2.22\n99.50\nOurs+LLaVA, ,L=4\u00d7K\n5.15\n98.83\n0.66\n99.81\n1.44\n99.63\n1.88\n99.54\n2.28\n99.45\nOurs+LLaVA, L=12\u00d7K\n2.00\n99.60\n0.28\n99.92\n0.86\n99.79\n1.34\n99.69\n1.12\n99.75\nOxford Pets\nMCM\n19.48\n96.43\n1.17\n99.73\n2.20\n99.52\n1.18\n99.72\n6.01\n98.85\nMaxLogit\n0.04\n99.97\n0.07\n99.98\n0.30\n99.92\n0.39\n99.89\n0.20\n99.94\nEnergy\n0.05\n99.98\n0.08\n99.98\n0.30\n99.93\n0.47\n99.89\n0.22\n99.94\nEOE+LLaVA, ,L=4\u00d7K\n0.36\n99.85\n0.73\n99.81\n1.29\n99.66\n0.48\n99.86\n0.71\n99.79\nEOE+LLaVA, L=12\u00d7K\n0.14\n99.92\n0.47\n99.88\n0.98\n99.75\n0.23\n99.91\n0.45\n99.86\nOurs+LLaVA, ,L=4\u00d7K\n0.11\n99.94\n0.43\n99.90\n0.83\n99.79\n0.21\n99.93\n0.40\n99.89\nOurs+LLaVA, L=12\u00d7K\n0.09\n99.94\n0.69\n99.83\n1.21\n99.69\n0.23\n99.90\n0.55\n99.84\nAverage\nMCM\n14.51\n96.43\n2.44\n99.48\n3.63\n99.21\n2.78\n99.45\n5.84\n98.64\nMaxLogit\n4.17\n98.69\n0.22\n99.95\n0.48\n99.87\n1.49\n99.67\n1.59\n99.54\nEnergy\n4.32\n98.65\n0.28\n99.92\n0.54\n99.85\n2.28\n99.49\n1.85\n99.47\nEOE+LLaVA, ,L=4\u00d7K\n5.65\n98.28\n0.53\n99.85\n1.04\n99.74\n0.84\n99.80\n2.02\n99.42\nEOE+LLaVA, L=12\u00d7K\n4.89\n98.49\n0.35\n99.91\n0.74\n99.81\n0.63\n99.85\n1.65\n99.51\nOurs+LLaVA, ,L=4\u00d7K\n5.27\n98.39\n0.39\n99.89\n0.86\n99.79\n0.55\n99.86\n1.77\n99.48\nOurs+LLaVA, L=12\u00d7K\n4.33\n98.60\n0.32\n99.92\n0.69\n99.83\n0.40\n99.90\n1.43\n99.56\nof our method on large-scale datasets. For out-of-distribution\n(OOD) evaluation, we employ large-scale datasets including\nTexture [49], Places [50], iNaturalist [51], SUN [52], and the\nOOD benchmark curated by MOS [53].\nMM-OOD Setups. In our framework, we adopt CLIP [7] as\nthe core model, utilizing ViT-B/16 as the image encoder and\na masked self-attention Transformer [54] as the text encoder.\nFor far OOD detection, we first leverage GPT-4 as the LLM\nto generate initial category labels. These labels serve as the\nfoundation for generating OOD class labels. We then employ\nLLaVA (v1.5-7b) as the MLLM to propose candidate OOD\nlabels. From this set of candidates, the OOD label most\ndissimilar to the primary category label is selected. To refine\nthe label generation process, we introduce Stable Diffusion\nv1.5, which is used to generate an image. This generated image\nis then input back into LLaVA, guiding it to produce a final\nOOD label that is distinct from the primary category, ensuring\nthat the generated outlier class is sufficiently differentiated.\nBaselines. We compare our method against state-of-the-art\nzero-shot OOD detection approaches, all based on CLIP as the\nbackbone. The experimental setup follows the configuration in\nEOE. Specifically, we evaluate our method alongside three key\nbaselines in the zero-shot setting: MaxLogit [42], Energy [16],\nand MCM [6]. These methods use the pre-trained CLIP model\nwithout fine-tuning, enabling a direct comparison with our\napproach.\nEvaluation Metrics. To assess performance, we use two\ncommon evaluation metrics: (1) FPR95, which measures the\nfalse positive rate of OOD samples when the true positive rate\nfor ID samples is fixed at 95%. A lower FPR95 value indicates\nbetter detection capability; (2) AUROC, which calculates the\narea under the receiver operating characteristic curve, with a\nhigher value reflecting better results.\nB. Main Results\nNear OOD Detection. Table V shows the performance\nof MM-OOD in near OOD detection on ImageNet-10\nand ImageNet-20. For ImageNet-10, MM-OOD achieves an\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n6\nQ:\nGiven\nthe\nbroad\ncategory\n[computer],\nsuggest\nsix\nother\ncategories\nthat\nare\ncompletely\ndifferent from [computer]. Please\nensure they belong to broader and\ndistinct domains and highlight the\none that is the most different or\ndistant.\nSecond Round\nA: Here are six categories that are\nvery different from [computer]:\n- chair\n- lamp\n- office supplies\n- pencil\n- cake\n- bicycle\nThe most different one is: cake\nForth Round\nA: There are 3 categories that visually\nresemble [computer], but they belong\nto\nbroader\nand\ndistinct\ndomains\noutside of [computer]:\n- grilled meat\n- chocolate Mousse\n- cheese\nThird Round\nQ: Please categorize these 1000\nitems into 300 broad categories\nbased on their characteristics\nusing common classification\nmethods.\nFirst Round\nUser\nMLLM\nQ:  Given the broad\ncategory [computer]\nand this image, focus on\nthe visual features of\nthe image. Use the\nimage as an example\nthat is unrelated to\nthe broad category\n[computer], and\nsuggest 3 categories\nthat visually resemble\n[computer] but belong\nto broader, distinct\ndomains unassociated\nwith [computer].\n- fish\n- bird\n- frog\n- snake\n- tape\n- bed\n- Computer\n- \u2026\u2026\nDiffusion\nModel\n\u201ccake\u201d\ngenerate\nimage\nFig. 4. Prompt design for multi-round conversation in the proposed MM-OOD framework under far out-of-distribution (OOD) detection.\nTABLE II\nZERO-SHOT FAR OUT-OF-DISTRIBUTION (OOD) DETECTION RESULTS ON IMAGENET-1K WITH VARYING NUMBERS OF PRIMARY CATEGORIES M.\nLOWER FPR95 AND HIGHER AUROC ARE BETTER.\nPrimary Category\nNumber\nMethod\niNaturalist\nSUN\nPlaces\nTexture\nAverage\nFPR95 \u2193AUROC \u2191FPR95 \u2193AUROC \u2191FPR95 \u2193AUROC \u2191FPR95 \u2193AUROC \u2191FPR95 \u2193AUROC \u2191\nM = 100\nLLaVA-1.5 (EOE)\n72.11\n72.43\n60.64\n83.97\n53.12\n87.88\n55.73\n86.50\n60.40\n82.69\nLLaVA-1.5 (Ours)\n72.83\n72.20\n59.79\n84.35\n50.68\n88.30\n55.05\n86.85\n59.59\n82.92\nM = 300\nLLaVA-1.5 (EOE)\n69.64\n74.67\n59.40\n84.24\n49.32\n88.72\n54.74\n86.61\n58.27\n83.56\nLLaVA-1.5 (Ours)\n68.89\n74.69\n58.37\n84.88\n48.29\n88.70\n53.79\n87.04\n57.34\n83.83\nM = 500\nLLaVA-1.5 (EOE)\n70.61\n73.46\n58.15\n84.82\n51.67\n87.63\n53.55\n87.15\n58.49\n83.27\nLLaVA-1.5 (Ours)\n70.26\n73.57\n58.81\n84.66\n48.68\n88.54\n53.88\n87.04\n57.91\n83.45\nFPR95 of 3.84%, improving by 3.17 percentage points com-\npared to EOE, which has an FPR95 of 7.01%, and by 9.97\npercentage points compared to the Energy baseline, which\nhas an FPR95 of 13.81%. Similarly, for ImageNet-20, MM-\nOOD reduces FPR95 to 5.18%, which is 1.19 percentage\npoints lower than EOE\u2019s 6.37% and 2.81 percentage points\nlower than Energy\u2019s 7.99%. In summary, MM-OOD provides\nconsistent improvements over both EOE and Energy across\ndifferent settings in the near OOD detection task.\nFar OOD Detection. Table I compares our method with\nstate-of-the-art zero-shot OOD detection approaches across\nfour ID datasets: CUB-200-2011, STANFORD-CARS, Food-\n101, and Oxford-IIIT Pet. The \u201dGround Truth\u201d results, which\nassume access to the true OOD class labels, represent an ideal\nscenario but are not available in real-world applications, setting\na theoretical upper bound. In average results, MM-OOD with\nL = 12 \u00d7 K outperforms all methods, achieving the best\nFPR95 of 4.33% and AUROC of 99.56%, surpassing EOE,\nMaxLogit, and Energy. On individual datasets, MM-OOD\nalso demonstrates strong performance, especially on CUB-\n200-2011, Food-101, and Oxford-IIIT Pet, where it achieves\nsuperior results compared to other methods. These results\nhighlight the effectiveness ofMM-OOD in far OOD detection,\nshowing consistent advantages in all evaluated scenarios.\nScaling to ImageNet-1K dataset. Table VIII presents the\nperformance of MM-OOD for far OOD detection on the\nImageNet-1K dataset, comparing it to the baseline method\nEOE across different primary category numbers M. This\nexperiment is important for demonstrating that MM-OOD\nremains effective even when applied to large-scale, real-world\ndatasets like ImageNet-1K. As shown in the table, MM-OOD\nachieves consistently better performance than EOE across all\nvalues of M. Specifically, for M = 100, MM-OOD improves\nFPR95 by 0.81% and AUROC by 0.23% compared to EOE.\nAt M = 300, MM-OOD shows improvements of 0.93% in\nFPR95 and 0.27% in AUROC. For M = 500, MM-OOD\nfurther improves FPR95 by 0.58% and AUROC by 0.18%.\nOverall, MM-OOD shows a improvement, with an average\nincrease of 0.77% in FPR95 and 0.23% in AUROC across all\nM values. These results suggest that MM-OOD is capable of\nhandling large-scale far OOD detection tasks effectively, even\nas the number of primary categories increases.\nComputational Efficiency. Our method MM-OOD\u2019s compu-\ntation time is similar to EOE. We conduct the experiments\nusing the Food-101 dataset as the ID dataset as shown in\nTable VI on one A100 GPU. We conclude that there is little\ndifference in time between EOE and MM-OOD. Additionally,\nas L increases, the performance of EOE improves, but MM-\nOOD shows even greater improvement. It shows that our\nmethod MM-OOD has scalability.\nC. Ablation Study\nWe study the impact of outlier class number L, hyperparam-\neter \u03b2, and different MLLMs (LLaVA, Qwen2-VL). Results\nconfirm the robustness and effectiveness of our design.\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n7\nTABLE III\nZERO-SHOT FAR OUT-OF-DISTRIBUTION (OOD) DETECTION RESULTS ON FOOD-101 USING LLAVA-1.5-7B AND QWEN2-VL WITH VARYING NUMBERS\nOF PRIMARY CATEGORIES M. THE OUTLIER LABEL COUNT IS SET TO 12 \u00d7 K, WHERE K DENOTES THE NUMBER OF IN-DISTRIBUTION (ID) CLASSES.\nPrimary Category\nNumber\nMethod\niNaturalist\nSUN\nPlaces\nTexture\nAverage\nFPR95 \u2193AUROC \u2191FPR95 \u2193AUROC \u2191FPR95 \u2193AUROC \u2191FPR95 \u2193AUROC \u2191FPR95 \u2193AUROC \u2191\nM = 10\nLLaVA-1.5 (EOE)\n2.42\n99.52\n1.88\n99.64\n0.17\n99.94\n1.57\n99.60\n1.24\n99.72\nLLaVA-1.5 (Ours)\n1.88\n99.64\n0.21\n99.94\n0.79\n99.80\n1.55\n99.63\n1.11\n99.75\nQwen2-VL (EOE)\n7.07\n98.44\n1.43\n99.68\n2.50\n99.45\n4.10\n98.68\n3.78\n99.10\nQwen2-VL (Ours)\n4.86\n98.98\n0.77\n99.82\n1.62\n99.62\n3.16\n98.98\n2.60\n99.31\nM = 20\nLLaVA-1.5 (EOE)\n4.62\n99.00\n0.66\n99.82\n1.33\n99.67\n2.27\n99.50\n2.22\n99.50\nLLaVA-1.5 (Ours)\n2.00\n99.60\n0.28\n99.92\n0.86\n99.79\n1.34\n99.69\n1.12\n99.75\nQwen2-VL (EOE)\n7.50\n98.32\n1.74\n99.62\n2.97\n99.35\n4.65\n98.56\n4.22\n98.96\nQwen2-VL (Ours)\n6.57\n98.51\n1.26\n99.71\n2.30\n99.47\n4.45\n98.53\n3.65\n99.10\nTABLE IV\nCOMPARISON RESULTS WITH RANDOM WORDS IN WORDNET ON FAR OOD DETECTION TASK.\nID Dataset\nMethod\niNaturalist\nSUN\nPlaces\nTexture\nAverage\nFPR95 \u2193\nAUROC \u2191\nFPR95 \u2193\nAUROC \u2191\nFPR95 \u2193\nAUROC \u2191\nFPR95 \u2193\nAUROC \u2191\nFPR95 \u2193\nAUROC \u2191\nCUB-200-2011\nOurs + Random\n16.72\n94.37\n0.04\n99.99\n0.42\n99.96\n0.02\n99.96\n4.30\n98.57\nOurs + LLaVA\n15.23\n94.86\n0.03\n99.99\n0.33\n99.98\n0.02\n99.99\n3.90\n98.69\nTABLE V\nZERO-SHOT NEAR OOD DETECTION RESULTS. THE BOLD INDICATES THE\nBEST PERFORMANCE, AND THE GRAY COLOR INDICATES THE BEST\nPERFORMANCE ON EVERY DATASET.\nMethod\nID: ImageNet-10\nID: ImageNet-20\nAverage\nOOD: ImageNet-20 OOD: ImageNet-10\nFPR95\u2193AUROC\u2191FPR95\u2193AUROC\u2191FPR95\u2193AUROC\u2191\nEnergy\n13.81\n97.04\n7.99\n98.45\n10.90\n97.75\nMaxLogit\n12.80\n97.31\n6.72\n98.65\n9.76\n97.98\nMCM\n5.72\n98.61\n9.05\n98.39\n7.39\n98.50\nEOE\n7.01\n98.47\n6.37\n98.83\n6.69\n98.65\nOurs\n3.84\n98.97\n5.18\n98.97\n4.51\n98.97\nGround Truth\n0.20\n99.80\n0.20\n99.93\n0.20\n99.87\nTABLE VI\nPERFORMANCE AND EFFICIENCY ON DIFFERENT L.\nMetrics\nL\n4\u00d7K\n6\u00d7K\n12\u00d7K\n40\u00d7K\n180\u00d7K\nEOE Time\n118m 36s 120m 28s 126m 14s 189m 16s 218m 34s\nMM-OOD Time\n125m 8s 124m 37s 117m 28s 184m 59s 247m 55s\nEOE FPR95\n2.14\n2.13\n1.07\n0.80\n1.25\nMM-OOD FPR95\n1.63\n1.39\n0.45\n0.75\n0.67\nEOE AUROC\n99.51\n99.51\n99.76\n99.83\n99.72\nMM-OOD AUROC\n99.64\n99.69\n99.90\n99.84\n99.86\nDifferent MLLMs. Table VII presents the zero-shot far\nOOD detection performance of MM-OOD and EOE across\ntwo models: LLaVA-1.5 and Qwen2-VL. In all cases, MM-\nOOD achieves better results than EOE, demonstrating greater\nrobustness in far OOD detection under various primary cat-\negory settings. In addition, we also validate the results on\nInternVL2.5, which continue to demonstrate the superiority\nof EOE. For further details, please see Appendix.\nFig. 5. Comparison of EOE and the proposed method under varying outlier\nclass label counts L = c \u00d7 K and threshold \u03b2 on the near out-of-distribution\n(OOD) detection task, where K denotes the number of in-distribution (ID)\nclasses.\nD. More Discussions\nThe relationship between multi-round conversation and\nCoT. In our setup, N is 1. In fact, it is only the parameter\nused for batch processing. Multi-round conversations refer\nto the sketch-generate-elaborate framework used for far-OOD\ndetection tasks. In fact, sketch-generate-elaborate framework\naligns with the Chain of Thought theory enhancing the\nmodel\u2019s instruction-following ability and accuracy through\nmore detailed and iterative dialogues. Let\u2019s give an example\nto demonstrate the benefits mentioned above. When using the\nEOE method with the InternVL2.5 model, the output may\nbe something like, \u201dI can\u2019t understand the content of the\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n8\nimage; the image you described includes...\u201d. In contrast, with\nour MM-OOD method, the OOD class result is accurately\ngenerated.\nUnknown whether it is far OOD task or near OOD task\nat test time. In some cases, it is not clear whether the OOD\nsample belongs to the far OOD task or near OOD task, making\nit impractical to select one of the branches of MM-OOD. To\naddress this challenge, we propose proportionally mixing the\nOOD samples envisioned by both the near and far branches\nduring the envisioning phase of the MLLM. A default mixing\nratio of 0.5 can be used.\nWhy not simply use random words on the far OOD\ntask. Based on the above, we know that MLLMs struggle to\nrecommend far-OOD labels effectively. Therefore, one might\nquestion the feasibility of using random words instead. As\nshown in Table IV, we designed the experiments and proved\nthat MM-OOD can perform better than the random sampling\nfrom WordNet. We contend that this issue arises primarily\nbecause random sampling can indiscriminately select both near\nand far samples. Random samples tend to be dispersed and\ninsufficiently concentrated, resulting in decreased performance\nand fluctuation in the far OOD task.\nV. CONCLUSION\nWe propose MM-OOD, a framework that leverages multi-\nmodal large language models (MLLMs) through multimodal\nreasoning and multi-round conversation to more efficiently\nobtain outlier samples, thereby improving out-of-distribution\n(OOD) detection performance."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper\n\n#### Main Contributions:\nThe paper introduces a novel pipeline called MM-OOD (Multimodal Out-Of-Distribution Detection), which leverages multimodal large language models (MLLMs) to enhance out-of-distribution (OOD) detection. The key contributions include:\n\n1. **Novel Pipeline Design**: Proposes a detailed and targeted pipeline for both near OOD and far OOD tasks, leveraging the multimodal reasoning and multi-round conversation capabilities of MLLMs.\n\n2. **Enhanced Outlier Exposure**: Introduces a sketch-generate-elaborate framework to generate outlier samples more effectively by using text prompts and visual inputs in multiple rounds with MLLMs.\n\n3. **Improved Performance Across Datasets**: Demonstrates significant improvements across various datasets, including ImageNet-1K, Food-101, CUB-200-2011, Stanford Cars, Oxford-IIIT Pets, and iNaturalist.\n\n4. **Scalability to Large-Scale Data**: Shows that MM-OOD remains effective even when applied to large-scale datasets like ImageNet-1K, achieving consistent improvements in performance across different primary category numbers (M).\n\n5. **Ablation Study**: Conducts an ablation study to validate the robustness and effectiveness of the proposed design by varying outlier class number L, hyperparameter \u03b2, and MLLMs.\n\n#### Methodology:\nThe MM-OOD pipeline is designed as follows:\n\n1. **Near OOD Detection Branch**: Directly feeds ID images and corresponding text prompts into MLLMs to propose potential outlier exposure.\n2. **Far OOD Detection Branch**: Introduces the sketch-generate-elaborate framework: first, uses text prompts to sketch outlier exposure; then generates corresponding visual OOD samples; finally, elaborates by using multimodal prompts.\n\n#### Key Findings:\n- MM-OOD consistently outperforms current state-of-the-art baselines across extensive experiments.\n- For near OOD detection, it achieves an FPR95 of 3.84%, which is a significant improvement over existing methods like EOE (FPR95 = 7.01%) and Energy (FPR95 = 13.81%).\n- In far OOD detection tasks, MM-OOD reduces the FPR95 to 4.51% for ImageNet-20 when using ID images from ImageNet-10, outperforming EOE and Energy.\n- The method is scalable to large-scale datasets like ImageNet-1K.\n\n#### Conclusions:\nThe paper concludes that MM-OOD effectively leverages the multimodal reasoning and multi-round conversation capabilities of MLLMs to generate more accurate outlier samples. This leads to improved OOD detection performance across various tasks and datasets, making it a promising approach for enhancing robustness in real-world applications."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "STREAM-VOICE-ANON: ENHANCING UTILITY OF REAL-TIME SPEAKER\nANONYMIZATION VIA NEURAL AUDIO CODEC AND LANGUAGE MODELS\nNikita Kuzmin1,2\u2217, Songting Liu1\u2217, Kong Aik Lee3, Eng Siong Chng1\n1Nanyang Technological University, Singapore\n2Institute for Infocomm Research, A\u22c6STAR, Singapore\n3The Hong Kong Polytechnic University, Hong Kong\ns220028@e.ntu.edu.sg, lius0114@e.ntu.edu.sg\nABSTRACT\nProtecting speaker identity is crucial for online voice applications,\nyet streaming speaker anonymization (SA) remains underexplored.\nRecent research has demonstrated that neural audio codec (NAC)\nprovides superior speaker feature disentanglement and linguis-\ntic fidelity.\nNAC can also be used with causal language models\n(LM) to enhance linguistic fidelity and prompt control for stream-\ning tasks. However, existing NAC-based online LM systems are\ndesigned for voice conversion (VC) rather than anonymization,\nlacking the techniques required for privacy protection. Building on\nthese advances, we present Stream-Voice-Anon, which adapts mod-\nern causal LM-based NAC architectures specifically for streaming\nSA by integrating anonymization techniques. Our anonymization\napproach incorporates pseudo-speaker representation sampling, a\nspeaker embedding mixing and diverse prompt selection strategies\nfor LM conditioning that leverage the disentanglement properties\nof quantized content codes to prevent speaker information leakage.\nAdditionally, we compare dynamic and fixed delay configurations to\nexplore latency-privacy trade-offs in real-time scenarios. Under the\nVoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves\nsubstantial improvements in intelligibility (up to 46% relative WER\nreduction) and emotion preservation (up to 28% UAR relative) com-\npared to the previous state-of-the-art streaming method DarkStream\nwhile maintaining comparable latency (180ms vs 200ms) and pri-\nvacy protection against lazy-informed attackers, though showing\n15% relative degradation against semi-informed attackers.\nIndex Terms\u2014 streaming speaker anonymization, neural audio\ncodec, voice conversion, privacy preservation, disentanglement\n1. INTRODUCTION\nSpeaker anonymization (SA) transforms speech to conceal speaker\nidentity while preserving usability for downstream tasks such as\nautomatic speech recognition (ASR) or speech emotion recogni-\ntion (SER). Streaming anonymization presents additional challenges\ndue to strict latency constraints that require real-time processing\nwith minimal delay. Furthermore, the source speaker\u2019s identity is\ntypically unknown beforehand, preventing offline anonymization\nstrategies that rely on speaker-specific adaptations, thus necessitat-\ning speaker-agnostic approaches.\nOffline SA approaches can be categorized into digital signal pro-\ncessing (DSP) methods [1, 2, 3, 4] and deep learning (DL) [5, 6, 7]\napproaches. DSP methods are computationally cheap, but provide\nlimited privacy and controllability. DL approaches are divided into\n\u2217Equal contribution.\nconventional pipelines using ASR to TTS cascades [8] or voice con-\nversion (VC) [9] methods based mostly on continuous features, and\nneural audio codec (NAC)-based [10, 11, 12] approaches leverag-\ning discrete acoustic tokens. Conventional DL methods suffer from\nspeaker information re-entanglement during vocoding. In contrast,\nNAC-based methods encode speech into quantized representations\nusing learned codebooks, enabling language model (LM) architec-\ntures to generate speech while maintaining clearer separation of lin-\nguistic content from speaker characteristics.\nReal-time anonymization is crucial for call centers, voice assis-\ntants, live legal recordings, and sensitive medical conversations [13].\nExisting streaming approaches [14, 15] mostly adapt conventional\noffline designs rather than leveraging modern LM architectures, in-\nheriting speaker information re-entanglement issues and requiring\nprivacy-enhancing techniques like k-means clustering that degrade\nutility. In contrast, LM-based streaming VC has progressed rapidly\n[16], with causal LMs enabling stable, low-latency zero-shot conver-\nsion while inheriting superior disentanglement traits of offline NAC\nmethods.\nInspired by advances in streaming VC, we adopt Stream-Voice-\nAnon, a real-time voice anonymization system that uses LM-based\nstreaming VC architectures. A causal streaming content encoder\nwith VQ bottleneck extracts speaker-invariant content tokens by\ndistilling self-supervised learning features from HuBERT [17, 18].\nA causal autoregressive LM converts these tokens into synthesized\nacoustic codes by conditioning on pseudo-speaker embeddings and\ncontent with acoustic tokens extracted from various prompt ut-\nterances.\nThe model employs a dual decoder architecture [19]:\na primary autoregressive transformer generates frame-level latent\nrepresentations, while a secondary lightweight transformer recon-\nstructs multiple codebook layers. The system incorporates config-\nurable frame delays (1-8 tokens) to balance latency and quality,\nachieving latencies as low as 180 ms on a 3060 RTX laptop GPU.\nEvaluation using the VoicePrivacy 2024 Challenge framework [20]\ndemonstrates that Stream-Voice-Anon1 provides privacy protection\ncomparable to the SOTA streaming pipeline DarkStream [15] while\nsignificantly enhancing speech quality.\n2. PROPOSED APPROACH\n2.1. Overview\nThe schematic of the proposed approach is illustrated in Figure 1.\nThe framework consists of a content encoder, an acoustic encoder, a\nspeaker encoder, an autoregressive voice conversion (ARVC) model\nbased on a dual-stage transformer architecture (Slow AR and Fast\n1Our demo page: https://paniquex.github.io/Stream-Voice-Anon\narXiv:2601.13948v1  [eess.AS]  20 Jan 2026\nContent Encoder\nAcoustic Encoder\nSlow AR\nFast AR\nFast AR\nCross Entropy Loss\nVQ bottleneck feature ct\nAcoustic codes at, 1:n\nSpeaker Encoder\nFrozen module\nTraining module\nQuantized linguistic\ncontent feature\nSpeaker embedding\nWait-for-start embedding\nWait-for-end embedding\nSlow AR output latent\nAcoustic codes of\ndifferent codebooks\nSpk embedding g\nReconstructed acoustic codes\n(a) Training\ng1\nContent Encoder\nAcoustic Encoder\nSlow AR\nFast AR\nSpeaker Encoder\nAcoustic Decoder\nAnonymized acoustic codes\nPrompt Pool\n...\np1\npK\nReference utterances\n\u2026\ngs  ~\nSource utterance\ngK\nAnonymized speech\nSelection strategy\nganon\nShuffle order\n(b) Inference\nFig. 1: Training and inference pipelines of the Stream-Voice-Anon.\nAR), and a prompt pool with selection strategy for pseudo-speaker\nconditioning during inference. The training and inference pipelines\nillustrate how these components work together to enable real-time\nSA while maintaining linguistic content and emotion preservation.\nThe architecture is detailed in subsequent sections.\n2.2. Content encoder\nWe implement the content encoder (Ce) which extracts speaker-\ninvariant content tokens {ct}T \u22121\nt=0 using strictly causal feature ex-\ntraction with zero look-ahead, following StreamVoice [16]. Unlike\nStreamVoice, we train an ASR encoder from scratch in parallel to\nguide bottleneck features and employ distillation on HuBERT [18]\nrepresentations. A VQ layer [21] on the encoder output discretizes\ncausal states using a learned codebook to obtain quantized tokens.\nThis VQ bottleneck reduces residual mutual information with the\nsource speaker while preserving phonetic content.\nCe is frozen\nduring the AR voice conversion training.\n2.3. Acoustic encoder\nThe acoustic encoder (Ae) processes the audio utterance and ex-\ntracts multi-codebook acoustic tokens {at,1:n}T \u22121\nt=0 that represent\nthe acoustic characteristics of the speech. Each frame t contains n\nacoustic codes from different codebooks, capturing various aspects\nof the audio signal. Ae is pre-trained and frozen during the training\nstage of AR voice conversion.\n2.4. Speaker encoder\nFor each training sample, we extract a speaker embedding g using a\npre-trained speaker verification model (Se) and insert it at the begin-\nning of the Slow AR sequence as a global condition.\n2.5. Autoregressive Voice Conversion (ARVC)\nFeature preparation. We select the Ae whose frame rate matches\nthat of Ce. Given a single training utterance, we then extract aligned\nsequences of quantized content tokens {ct}T \u22121\nt=0 from Ce, acoustic\ntokens {at,1:n}T \u22121\nt=0 from Ae and a global speaker embedding g from\nSe.\nInterleaved AR factorization. Following StreamVoice [16], to\nrespect streaming I/O, we model the joint distribution with an in-\nterleaved AR ordering over frames: at the frame level we alternate\ncontent and acoustics,\ng, c0, a0,1:n, c1, a1,1:n, c2, a2,1:n, . . . ,\nmeaning the model consumes ct from the source stream and emits the\ncorresponding acoustic codes at,1:n of the same frame before mov-\ning on. This interleaving mirrors real-time VC: the source speech\nis read frame-by-frame, and the converted acoustics are produced\nframe-synchronously.\nLatency\u2013quality trade-off via delayed emission. Because ct is\na heavily compressed (speaker-invariant) representation, emitting at\nimmediately after seeing ct is challenging without any look-ahead.\nStreamVoice+ [22] introduced a fixed frame-level delay so that the\nmodel first accumulates the future content context and only then\nstarts emitting acoustics. For example, with d=2 the interleaving\nbecomes\ng, c0, \u27e8w4s\u27e9, c1, \u27e8w4s\u27e9, c2, a0,1:n, c3, a1,1:n, c4, a2,1:n, . . .\nwhere \u27e8w4s\u27e9is a learned wait-for-start embedding occupying out-\nput steps until emission begins. However, this fixed-delay approach\nlacks flexibility during inference, as the model cannot dynamically\nadjust the quality-latency trade-off based on real-time requirements.\nTo address this, we introduce the dynamic-delay technique by sam-\npling d \u223cU{1, . . . , 8} per utterance, which teaches the decoder to\noperate under variable look-ahead. At inference time, d can be cho-\nsen flexibly to trade latency for recognition fidelity (smaller WER\nwith larger d).\nTwo-stage AR with per-frame codebook decoding. As each\nacoustic token at,1:n consists of multiple codebooks, predicting all\nn codebooks of a frame with a single AR is suboptimal. Inspired\nby FishSpeech [19] We therefore use a two-stage decoder: a Slow\nAR operates at the frame rate to produce a latent zt after receiving\nct (and previous outputs), and a lightweight Fast AR runs within the\nframe conditioned on zt to decode all n codebooks autoregressively:\nzt, at,1, at,2, . . . , at,n.\nTraining uses teacher forcing inside the frame and the overall loss is\nthe sum of cross-entropy over all codebooks and frames,\nLAR =\nT \u22121\nX\nt=0\nn\nX\nk=1\nCE(\u02c6at,k, at,k) ,\nwhere \u02c6at,k and at,k predicted logits and ground truth labels corre-\nspondingly.\n2.6. Inference-time anonymization techniques\nIn addition to the training-time disentanglement techniques de-\nscribed above that help protect privacy, we strengthen anonymiza-\ntion with an inference-time speaker embedding mixing scheme and\nprompt-based randomization as illustrated in Figure 1(b). First, we\nselect K utterances from the prompt pool P using one of our se-\nlection strategies. Before extracting tokens for the target utterances,\nwe shuffle the selected prompts in random order and then extract\nboth content and acoustic tokens from each prompt, concatenating\nthem together to form diverse acoustic contexts. We use extracted\nrepresentations to condition ARVC.\nIn parallel, we extract speaker embeddings {gi}K\ni=1 from se-\nlected prompt utterances using Se and compute their average. In-\nspired by [12], we then sample a speaker embedding gs from a Gaus-\nsian distribution and form the anonymized target embedding as a lin-\near combination:\nganon = \u03b1 1\nK\nK\nX\ni=1\ngi + (1 \u2212\u03b1)gs,\nwhere \u03b1 indicates the trade-off between staying close to the prompts\nand injecting randomness for even stronger privacy.\nIt is worth noting, all of the anonymization steps can be per-\nformed independently of source speaker utterance, thus allowing to\nprecompute all representations beforehand and use during streaming\ninference.\n3. EXPERIMENTAL SETUP\n3.1. Datasets\nFor training, we use LibriHeavy [23], CommonVoice [24]. The con-\ntent encoder requires paired speech\u2013text supervision; we therefore\nuse the official transcripts of LibriHeavy and tokenize them with\nthe OpenAI Whisper tokenizer to form target sequences for the ASR\nauxiliary. The ARVC model is trained on the same dataset as the\ncontent encoder but it does not use transcripts. The prompt pool (P)\ncomprises VCTK [25], ESD [26], VoxCeleb1 [27], CREMA-D [28].\nFor evaluation, we use LibriSpeech dev-clean and test-clean to\nassess privacy and ASR utility, and the IEMOCAP development and\nevaluation sets to assess emotion recognition utility.\n3.2. Evaluation Protocol\nWe follow the 2024 VoicePrivacy Challenge protocol [20]. Our first\nprivacy setting is the lazy-informed attacker (ASVeval): an automatic\nspeaker verification model based on ECAPA-TDNN [29], trained\non original LibriSpeech train-clean-360. Additionally, we report re-\nsults for a semi-informed attacker (ASVanon\neval ) using the same architec-\nture trained LibriSpeech train-clean-360 that has been anonymized\nat the utterance level to match the anonymization system. Auto-\nmatic speech recognition ASReval and speech emotion recognition\nSEReval models are pretrained on the original LibriSpeech train-960\nand IEMOCAP datasets respectively. We report EER for privacy,\nWER for intelligibility, and UAR for emotion preservation; higher\nEER and UAR are better, and lower WER is better. All scores are\ncomputed using the official VPC2024 evaluation pipeline [20].\n3.3. Model Configurations\nContent encoder. We use a streaming content encoder that converts\n44.1 kHz audio to 160-bin log-Mel features and downsamples by 4\ntimes, yielding \u223c21.5 Hz latents. The backbone is a lightweight\nConvNeXt [30] stack followed by an 8-layer decoder-only Trans-\nformer (model/FFN dims 512/1536, SwiGLU [31], RoPE [32]). A\nVQ layer with a vocabulary of 8192 entries produces the discrete\ncontent codes. All convolutions are causal to enable streaming.\nAcoustic encoder and decoder. A Firefly-GAN vocoder from\nFishSpeech [19] synthesizes audio from the acoustic codes. Its frame\nrate is aligned with the content encoder (\u223c21.5 Hz) and it uses 8\ncodebooks. Both encoder and decoder components employ causal\nconvolutions for streaming synthesis.\nSpeaker encoder.\nPre-trained CAM++ [33] and SparkTTS\nglobal tokenizer [34] are used to extract speaker embeddings.\nARVC. We adopt a two-stage ARVC: a Slow-AR (12 layers,\n768 hidden dimension/2304 feed-forward dimension) produces a\nper-frame state that conditions a Fast-AR (4 layers, 768 hidden\ndimension/2304 feed-forward dimension), which then autoregres-\nsively emits all 8 acoustic codebooks for that frame.\nTraining details. Both the content encoder and the autoregres-\nsive voice conversion (ARVC) model were trained on 8 NVIDIA\nH100 GPUs for 400,000 optimization steps. We used the AdamW\noptimizer with an initial learning rate of 1.0 \u00d7 10\u22124, decayed expo-\nnentially every optimization step. The batch size was 16 per GPU\n(effective batch size = 8 \u00d7 16 = 128).\nAnonymization details. We use \u03b1 = 0.9 in formula for ganon.\nOur prompt pool P consists of 4 datasets.\nWe employ several\nselection strategies to evaluate both anonymization effectiveness\nand emotion preservation capabilities:vctk-1fix selects one utterance\nfrom a fixed VCTK speaker; vctk-1rnd selects one random utterance\nfrom VCTK; vctk-4rnd selects four random utterances from VCTK;\ncross-ds-4rnd samples one utterance from each dataset (VCTK,\nVoxCeleb1, CREMA-D, ESD); cremad-emo-4rnd samples four\nCREMA-D utterances with specific emotions (angry, neutral, sad,\nhappy). Utterances in multi-prompt strategies randomly cropped to\n3 seconds each to maintain total duration under 12 seconds.\n4. RESULTS AND DISCUSSION\n4.1. Model comparison\nWe benchmark our model against the SOTA streaming system\nDarkStream and offline approach EASY. We compare with two\nDarkStream\u2019s configurations (Mel+CL and Wave+CL) at LA=140\nas these match our model\u2019s latency, since lower LA for DarkStream\ncauses even worse WER degradation. The results are shown in Table\n1. Compared with DarkStream, our model improves intelligibility,\nimproving WER relatively by up to 46% at comparable latency\nbudgets. For emotion recognition, our emotion-prompt conditioning\nboosts expressiveness by \u223c28% and the other prompt strategy yield\ngains of 15%. Privacy results are comparable: our model is slightly\nbetter under the lazy-informed attacker, while DarkStream is better\nunder the semi-informed attacker. Finally, offline baselines high-\nlight the remaining gap between online and offline anonymization,\nindicating room for future advances in streaming approaches.\nTable 1: Performance comparison of online and offline SA methods.\nNumbers in parentheses show relative improvement/degradation\nover the selected DarkStream model (green indicates improvement,\nred indicates degradation). For privacy fairness, we select models\nfor comparison only if they achieve close to 50% EER under the\nlazy-informed attacker scenario, indicating near-random guess per-\nformance by the attacker.\nModel\nType\nWER \u2193\nUAR \u2191\nEER \u2191\nlazy-informed\nEER \u2191\nsemi-informed\nEASY [35]\nOffline\n2.70\n63.81\n\u2013\n45.89\nDarkStream [15]\nMel+CL, LA=140, KM\nOnline,\n210 ms\n8.75 (0.0%)\n34.73 (0.0%)\n47.26 (0.0%)\n21.83 (0.0%)\nWave+CL, LA=140, KM Online,\n200 ms\n9.52(8.8%\u2191)\n34.49(0.7%\u2193)\n46.75(1.1%\u2193)\n22.68(3.9%\u2191)\nStream-Voice-Anon (Ours)\ncremad-emo-4rnd\nOnline,\n180 ms\n6.59(24.7%\u2193) 44.59(28.4%\u2191)\n46.53(1.5%\u2193)\n18.63(14.6%\u2193)\ncross-ds-4rnd\nOnline,\n180 ms\n4.71(46.2%\u2193) 39.94(15.0%\u2191)\n47.72(0.9%\u2191)\n18.98(13.1%\u2193)\n4.2. Prompt diversity\nIn this experiment, we study how prompt (target) conditioning di-\nversity affects privacy, emotion preservation and intelligibility. Our\nmotivation for increasing prompt diversity is that a semi-informed\nattacker can adapt by finetuning to an anonymization strategy, so\nvarying the prompts is crucial to mask source speaker identity\ncues.\nAs shown in Table 2 increasing prompt diversity (vctk-\n1fix\u2192vctk-4rnd\u2192cross-ds-4rnd) consistently raises EER against\nsemi-informed attackers, supporting the hypothesis that diversity\nhinders attacker adaptation to anonymization techniques. Gains un-\nder lazy-informed threat are relatively smaller as the attacker does\nnot adapt to anonymization techniques. Even a single fixed prompt\nutterance already provides anonymization close to random guess\nunder that threat model.\nTable 2: Impact of prompt selection strategies on anonymization\nperformance and utility preservation.\nSelection\nstrategy\nWER \u2193UAR \u2191\nEER \u2191\nlazy-inform\nEER \u2191\nsemi-inform\nvctk-1fix\n4.54\n39.71\n47.19\n15.92\nvctk-1rnd\n4.70\n40.55\n45.88\n15.00\nvctk-4rnd\n4.74\n40.36\n44.96\n16.35\ncross-ds-4rnd\n4.71\n39.94\n47.72\n18.98\n4.3. Latency vs privacy vs utility trade-off\nThis experiment shows latency vs privacy vs utility trade-off for dif-\nferent latency budgets on Figure 2. Across 130-440ms, privacy is\nessentially invariant to latency, showing that it does not weaken pro-\ntection. As expected, intelligibility improves with higher latency up\nto 180 ms and then flattens as the delay increases further. We then\ncompare the dynamic delay model with a fixed delay at 240 ms la-\ntency (d = 4), which achieves slightly lower WER but shows no pri-\nvacy boost. Therefore, while fixed delay offers a small ASR advan-\ntage, dynamic delay preserves privacy and allows to choose latency\nat inference time without retraining to meet different requirements.\n4.4. Latency and RTF\nTable 3 reports latency and RTF on the H200 server GPU and laptop\nRTX 3060. In H200, all settings are run in real time (RTF = 0.28 \u2192\n0.11) with 151-399 ms latency. On laptop GPU, RTF improves from\n0.93 to 0.35 with 180-464 ms latency. Increasing chunk size reduces\n100\n150\n200\n250\n300\n350\n400\n450\nLatency (ms)\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nEER\n44.6\n45.9\n45.6\n45.6\n45.5\ndelay=1\ndelay=2\ndelay=4\ndelay=8\nEER\nWER\n3.50\n4.00\n4.50\n5.00\n5.50\n6.00\nWER\n5.94\n4.70\n4.49\n4.47\n4.26\nFig. 2: Latency (x-axis) against privacy (EER \u2191, left y-axis) and\nintelligibility (WER \u2193, right y-axis) for Stream-Voice-Anon with d \u2208\n{1, 2, 4, 8} dynamic delay values. Bold \u00d7, \u00d7 markers indicate a\nfixed-delay model with d = 4 for comparison. Selection strategy is\nvctk-1rnd.\nTable 3: Latency and real-time factor (RTF) performance across dif-\nferent chunk sizes on server and laptop hardware configurations with\nd = 2.\nServer (H200)\nPC (Laptop 3060)\nChunk size Inference time RTF Latency Inference time RTF Latency\n46 ms\n13 ms\n0.28\n151 ms\n43 ms\n0.93\n180 ms\n92 ms\n17 ms\n0.18\n201 ms\n53 ms\n0.58\n237 ms\n276 ms\n31 ms\n0.11\n399 ms\n96 ms\n0.35\n464 ms\nRTF but increases latency, revealing a throughput\u2013responsiveness\ntrade-off. For applications prioritizing responsiveness, 46\u201392 ms\nchunks yield the lowest delays, whereas 276 ms chunk offer the\nlargest real-time margin when slightly higher latency is acceptable.\n5. CONCLUSION\nWe present Stream-Voice-Anon, a real-time SA system that success-\nfully adapts LM-based VC architectures for streaming privacy pro-\ntection. Our system demonstrates that NAC-based language model-\ning can be effectively extended to SA tasks while maintaining real-\ntime performance. Compared to the SOTA streaming baseline Dark-\nStream, our system achieves substantial improvements in intelligi-\nbility (46% WER relative reduction) and emotion preservation (28%\nrelative improvement) while maintaining comparable privacy protec-\ntion against lazy-informed attackers (47.26% vs 47.72%) and latency\n(200 ms vs 180 ms). Against semi-informed attackers, our system\nshows slight degradation in privacy (21.83% vs 18.98% EER), indi-\ncating room for improvement against adaptive threats. The dynamic\ndelay mechanism enables flexible latency-quality trade-offs without\nretraining, making it practical for various applications. However,\nthe system currently requires GPU acceleration and cannot operate\nin real-time on CPU-only hardware. Future work will focus on im-\nproving disentanglement techniques to narrow the gap with offline\nmethods, enhancing robustness against semi-informed attackers, op-\ntimizing for CPU deployment to enable broader accessibility.\n6. REFERENCES\n[1] Priyanka Gupta, Gauri P. Prajapati, Shrishti Singh, Madhu R. Kamble,\nand Hemant A. Patil,\n\u201cDesign of voice privacy system using linear\nprediction,\u201d in 2020 Asia-Pacific Signal and Information Processing\nAssociation Annual Summit and Conference (APSIPA ASC), 2020, pp.\n543\u2013549.\n[2] Jose Patino, Natalia Tomashenko, Massimiliano Todisco, Andreas\nNautsch, and Nicholas Evans,\n\u201cSpeaker anonymisation using the\nmcadams coefficient,\u201d in Interspeech 2021, 2021, pp. 1099\u20131103.\n[3] Lauri Tavi, Tomi Kinnunen, and Rosa Gonz\u00b4alez Hautam\u00a8aki, \u201cImprov-\ning speaker de-identification with functional data analysis of f0 trajec-\ntories,\u201d Speech Communication, vol. 140, pp. 1\u201310, May 2022.\n[4] Candy Olivia Mawalim, Shogo Okada, and Masashi Unoki, \u201cSpeaker\nanonymization by pitch shifting based on time-scale modification,\u201d in\n2nd Symposium on Security and Privacy in Speech Communication,\n2022, pp. 35\u201342.\n[5] Sarina Meyer, Florian Lux, Julia Koch, Pavel Denisov, Pascal Tilli, and\nNgoc Thang Vu, \u201cProsody is not identity: A speaker anonymization\napproach using prosody cloning,\u201d in ICASSP 2023 - 2023 IEEE In-\nternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2023, pp. 1\u20135.\n[6] Xiaoxiao Miao, Xin Wang, Erica Cooper, Junichi Yamagishi, and Na-\ntalia Tomashenko, \u201cSpeaker anonymization using orthogonal house-\nholder neural network,\u201d IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, vol. 31, pp. 3681\u20133695, 2023.\n[7] Nikita Kuzmin, Hieu-Thi Luong, Jixun Yao, Lei Xie, Kong Aik Lee,\nand Eng-Siong Chng, \u201cNtu-npu system for voice privacy 2024 chal-\nlenge,\u201d in 4th Symposium on Security and Privacy in Speech Commu-\nnication, 2024, pp. 72\u201379.\n[8] Sarina Meyer, Pascal Tilli, Pavel Denisov, Florian Lux, Julia Koch, and\nNgoc Thang Vu, \u201cAnonymizing speech with generative adversarial net-\nworks to preserve speaker privacy,\u201d in 2022 IEEE Spoken Language\nTechnology Workshop (SLT), 2023, pp. 912\u2013919.\n[9] Pierre Champion,\nAnonymizing Speech: Evaluating and Designing\nSpeaker Anonymization Techniques, Ph.D. thesis, Universit\u00b4e de Lor-\nraine, 2023.\n[10] Michele Panariello, Francesco Nespoli, Massimiliano Todisco, and\nNicholas Evans,\n\u201cSpeaker anonymization using neural audio codec\nlanguage models,\u201d in ICASSP 2024 - 2024 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP), 2024, pp.\n4725\u20134729.\n[11] Michele Panariello, Massimiliano Todisco, and Nicholas Evans, \u201cPre-\nserving spoken content in voice anonymisation with character-level\nvocoder conditioning,\u201d in 4th Symposium on Security and Privacy in\nSpeech Communication, 2024, pp. 12\u201316.\n[12] Jixun Yao, Nikita Kuzmin, Qing Wang, Pengcheng Guo, Ziqian Ning,\nDake Guo, Kong Aik Lee, Eng-Siong Chng, and Lei Xie, \u201cNpu-ntu\nsystem for voice privacy 2024 challenge,\u201d in 4th Symposium on Secu-\nrity and Privacy in Speech Communication, 2024, pp. 67\u201371.\n[13] Sarina Meyer and Ngoc Thang Vu, \u201cUse cases for voice anonymiza-\ntion,\u201d arXiv preprint arXiv:2508.06356, 2025.\n[14] Waris Quamer and Ricardo Gutierrez-Osuna, \u201cEnd-to-end streaming\nmodel for low-latency speech anonymization,\u201d in 2024 IEEE Spoken\nLanguage Technology Workshop (SLT), 2024, pp. 727\u2013734.\n[15] Waris Quamer and Ricardo Gutierrez-Osuna,\n\u201cDarkStream: real-\ntime speech anonymization with low latency,\u201d\narXiv e-prints, p.\narXiv:2509.04667, 2025.\n[16] Zhichao Wang, Yuanzhe Chen, Xinsheng Wang, Lei Xie, and Yup-\ning Wang,\n\u201cStreamvoice: Streamable context-aware language mod-\neling for real-time zero-shot voice conversion,\u201d\narXiv preprint\narXiv:2401.11053, 2024.\n[17] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakho-\ntia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, \u201cHuBERT:\nSelf-Supervised Speech Representation Learning by Masked Prediction\nof Hidden Units,\u201d IEEE Transactions on Audio, Speech and Language\nProcessing, vol. 29, pp. 3451\u20133460, Jan. 2021.\n[18] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu,\n\u201cSpeechtokenizer: Unified speech tokenizer for speech language mod-\nels,\u201d in The Twelfth International Conference on Learning Representa-\ntions, 2024.\n[19] Shijia Liao, Yuxuan Wang, Tianyu Li, Yifan Cheng, Ruoyi Zhang,\nRongzhi Zhou, and Yijin Xing,\n\u201cFish-speech:\nLeveraging large\nlanguage models for advanced multilingual text-to-speech synthesis,\u201d\n2024.\n[20] Natalia Tomashenko, Xiaoxiao Miao, Pierre Champion, Sarina Meyer,\nXin Wang, Emmanuel Vincent, Michele Panariello, Nicholas Evans,\nJunichi Yamagishi, and Massimiliano Todisco, \u201cThe voiceprivacy 2024\nchallenge evaluation plan,\u201d 2024.\n[21] Aaron Van Den Oord, Oriol Vinyals, et al., \u201cNeural discrete represen-\ntation learning,\u201d Advances in neural information processing systems,\nvol. 30, 2017.\n[22] Zhichao Wang, Yuanzhe Chen, Xinsheng Wang, Lei Xie, and Yuping\nWang, \u201cStreamvoice+: Evolving into end-to-end streaming zero-shot\nvoice conversion,\u201d IEEE Signal Processing Letters, 2024.\n[23] Wei Kang, Xiaoyu Yang, Zengwei Yao, Fangjun Kuang, Yifan Yang,\nLiyong Guo, Long Lin, and Daniel Povey, \u201cLibriheavy: A 50,000 hours\nasr corpus with punctuation casing and context,\u201d\nin ICASSP 2024-\n2024 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2024, pp. 10991\u201310995.\n[24] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer,\nR. Morais, L. Saunders, F. M. Tyers, and G. Weber, \u201cCommon voice: A\nmassively-multilingual speech corpus,\u201d in Proceedings of the 12th Con-\nference on Language Resources and Evaluation (LREC 2020), 2020,\npp. 4211\u20134215.\n[25] Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald, \u201cCstr\nvctk corpus: English multi-speaker corpus for cstr voice cloning toolkit\n(version 0.92),\u201d 2019.\n[26] Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li, \u201cSeen and un-\nseen emotional style transfer for voice conversion with a new emotional\nspeech dataset,\u201d in ICASSP 2021 - 2021 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp.\n920\u2013924.\n[27] A. Nagrani, J. S. Chung, and A. Zisserman, \u201cVoxceleb: a large-scale\nspeaker identification dataset,\u201d in INTERSPEECH, 2017.\n[28] Houwei Cao, David G. Cooper, Michael K. Keutmann, Ruben C. Gur,\nAni Nenkova, and Ragini Verma, \u201cCrema-d: Crowd-sourced emotional\nmultimodal actors dataset,\u201d IEEE Transactions on Affective Computing,\nvol. 5, no. 4, pp. 377\u2013390, 2014.\n[29] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck, \u201cEcapa-\ntdnn: Emphasized channel attention, propagation and aggregation in\ntdnn based speaker verification,\u201d in Interspeech 2020. Oct. 2020, ISCA.\n[30] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer,\nTrevor Darrell, and Saining Xie, \u201cA convnet for the 2020s,\u201d in Pro-\nceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2022, pp. 11976\u201311986.\n[31] Noam Shazeer, \u201cGlu variants improve transformer,\u201d arXiv preprint\narXiv:2002.05202, 2020.\n[32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and\nYunfeng Liu, \u201cRoformer: Enhanced transformer with rotary position\nembedding,\u201d Neurocomputing, vol. 568, pp. 127063, 2024.\n[33] Hui Wang, Siqi Zheng, Yafeng Chen, Luyao Cheng, and Qian Chen,\n\u201cCam++: A fast and efficient network for speaker verification using\ncontext-aware masking,\u201d in Interspeech 2023, 2023, pp. 5301\u20135305.\n[34] Xinsheng Wang, Mingqi Jiang, and et al.,\n\u201cSpark-tts: An efficient\nllm-based text-to-speech model with single-stream decoupled speech\ntokens,\u201d 2025.\n[35] Jixun Yao, Hexin Liu, Eng Siong Chng, and Lei Xie, \u201cEASY: Emotion-\naware Speaker Anonymization via Factorized Distillation,\u201d in Inter-\nspeech 2025, 2025, pp. 3219\u20133223."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThe paper presents Stream-Voice-Anon (SVA), a real-time speaker anonymization system that enhances utility by leveraging modern causal language model-based neural audio codec architectures for streaming tasks. Key contributions include:\n1. **Adapting NAC to Streaming SA**: The authors adapt existing NAC architectures specifically for streaming speaker anonymization, integrating anonymization techniques and improving upon the limitations of conventional offline approaches.\n2. **Improved Linguistic Fidelity and Privacy Protection**: SVA uses a dual decoder architecture with an acoustic encoder that synthesizes diverse acoustic codes conditioned on pseudo-speaker embeddings and content tokens from various prompt utterances. This approach enhances linguistic fidelity while maintaining speaker anonymity.\n3. **Dynamic Delay Mechanism**: The system introduces dynamic delay techniques to balance latency and quality, offering flexibility in trade-offs without requiring retraining.\n\n#### Methodology\nThe methodology involves:\n1. **Content Encoder**: Extracts speaker-invariant content tokens using a streaming content encoder that converts audio into 160-bin log-Mel features.\n2. **Acoustic Encoder**: Processes speech to extract multi-codebook acoustic tokens representing the speech's acoustic characteristics.\n3. **Speaker Encoder**: Uses pre-trained speaker verification models to generate global speaker embeddings for conditioning.\n4. **Autoregressive Voice Conversion (ARVC)**: Combines a Slow AR and Fast AR model, with two-stage decoding to condition all codebooks autoregressively using learned embeddings.\n\n#### Key Findings\n1. **Improved Intelligibility**: SVA achieves substantial improvements in intelligibility compared to the state-of-the-art streaming method DarkStream, reducing WER by up to 46% while maintaining comparable latency (180 ms vs 200 ms).\n2. **Emotion Preservation**: The system also significantly enhances emotion preservation with a relative improvement of up to 28% in UAR.\n3. **Privacy Protection Against Lazy-Informed Attackers**: SVA maintains privacy protection against lazy-informed attackers, achieving EER reduction by only 15% compared to DarkStream (47.26% vs 47.72%).\n\n#### Conclusions\nSVA demonstrates the effectiveness of leveraging NAC-based language models for streaming speaker anonymization tasks. It achieves substantial improvements in utility while maintaining comparable privacy protection and latency, outperforming state-of-the-art methods like DarkStream. The dynamic delay mechanism enables flexible trade-offs between latency and quality without requiring retraining. Future work will focus on improving disentanglement techniques to narrow the gap with offline methods and enhance robustness against adaptive threats.\n\n### Detailed Summary\n\n#### Introduction\nThe paper addresses the challenge of protecting speaker identity in online voice applications, particularly for streaming tasks where strict latency constraints necessitate real-time processing. It introduces Stream-Voice-Anon (SVA), a system that adapts modern causal language model-based neural audio codec architectures specifically for streaming speaker anonymization.\n\n#### Proposed Approach\n1. **Overview**: The framework consists of a content encoder, acoustic encoder, speaker encoder, and an autoregressive voice conversion (ARVC) model based on a dual-stage transformer architecture.\n2. **Content Encoder**: Extracts speaker-invariant content tokens using causal feature extraction with zero look-ahead.\n3. **Acoustic Encoder**: Processes audio to extract multi-codebook acoustic tokens representing the speech's acoustic characteristics.\n4. **Speaker Encoder**: Uses pre-trained speaker verification models to generate global speaker embeddings for conditioning.\n5. **ARVC Model**: Combines a Slow AR and Fast AR model, with two-stage decoding to condition all codebooks autoregressively using learned embeddings.\n\n#### Experimental Setup\n- **Datasets**: Uses Libriheavy, CommonVoice, and various speech datasets like VCTK, ESD, VoxCeleb1, CREMA-D for training.\n- **Evaluation Protocol**: Follows the 2024 VoicePrivacy Challenge protocol with a semi-informed attacker scenario to assess privacy and utility.\n\n#### Results and Discussion\n1. **Model Comparison**: SVA outperforms DarkStream in intelligibility (WER reduction by up to 46%) while maintaining comparable latency.\n2. **Prompt Diversity**: Increasing prompt diversity consistently raises EER against semi-informed attackers, supporting the hypothesis that diversity hinders attacker adaptation.\n3. **Latency vs Privacy vs Utility Trade-off**: SVA achieves substantial improvements in utility and privacy protection without compromising on latency.\n\n#### Conclusions\nSVA successfully adapts NAC-based language model architectures for streaming speaker anonymization tasks, demonstrating significant improvements in intelligibility and emotion preservation while maintaining comparable privacy protection against lazy-informed attackers. The dynamic delay mechanism enables flexible trade-offs between latency and quality without requiring retraining. Future work will focus on improving disentanglement techniques to narrow the gap with offline methods and enhance robustness against adaptive threats.\n\n### Summary\nStream-Voice-Anon (SVA) is a real-time speaker anonymization system that leverages modern causal language model-based neural audio codec architectures for streaming tasks. It achieves substantial improvements in intelligibility, emotion preservation, and privacy protection while maintaining comparable latency and utility compared to state-of-the-art methods like DarkStream. The dynamic delay mechanism enables flexible trade-offs between latency and quality without requiring retraining. Future work will focus on improving disentanglement techniques and enhancing robustness against adaptive threats."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Fantasy AIGC Team\nFantasyVLN:\nUnified\nMultimodal\nChain-of-\nThought Reasoning for Vision-Language Navigation\nJing Zuo1,2*\u00a7,\nLingzhou Mu1,3*\u00a7,\nFan Jiang1*\u2020\u2021,\nChengcheng Ma1,\nMu Xu1,\nYonggang Qi2\u2021\n1Fantasy AIGC Team,\n2Beijing University of Posts and Telecommunications,\n3Tsinghua University\njiangfan0576@gmail.com\nqiyg@bupt.edu.cn\nAbstract\nAchieving human-level performance in Vision-and-Language Navigation (VLN)\nrequires an embodied agent to jointly understand multimodal instructions and\nvisual-spatial context while reasoning over long action sequences. Recent works,\nsuch as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought\n(CoT) reasoning for improving interpretability and long-horizon planning. More-\nover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as\na promising pathway toward human-like navigation reasoning. However, existing\napproaches face critical drawbacks: purely textual CoTs lack spatial grounding and\neasily overfit to sparse annotated reasoning steps, while multimodal CoTs incur se-\nvere token inflation by generating imagined visual observations, making real-time\nnavigation impractical. In this work, we propose FantasyVLN, a unified implicit\nreasoning framework that preserves the benefits of CoT reasoning without explicit\ntoken overhead. Specifically, imagined visual tokens are encoded into a compact\nlatent space using a pretrained Visual AutoRegressor (VAR) during CoT reason-\ning training, and the model jointly learns from textual, visual, and multimodal\nCoT modes under a unified multi-CoT strategy. At inference, our model performs\ndirect instruction-to-action mapping while still enjoying reasoning-aware repre-\nsentations. Extensive experiments on LH-VLN show that our approach achieves\nreasoning-aware yet real-time navigation, improving success rates and efficiency\nwhile reducing inference latency by an order of magnitude compared to explicit\nCoT methods.\n1\nIntroduction\nVision-and-Language Navigation (VLN) aims to enable an embodied agent to follow natural-language\ninstructions and navigate complex visual environments Krantz et al. (2020); Wu et al. (2024);\nAnderson et al. (2018); Gu et al. (2022).\nSolving this task requires the joint understanding of\nsemantics from language and spatial geometry from visual observations, along with long-horizon\nreasoning to plan a sequence of actions. In particular, for multi-stage and long-horizon navigation\nscenarios as proposed in (Song et al., 2025), the ability to perform robust multimodal reasoning,\ni.e., to integrate linguistic intent with visual-spatial context over extended temporal dependencies, is\nespecially critical. Despite the progress made by recent multimodal large models, achieving effective\ncross-modal reasoning in VLN remains challenging due to the semantic\u2013spatial gap and the need for\ninterpretable yet sample-efficient reasoning mechanisms.\n\u2217Equal contribution.\n\u2020Project leader.\n\u2021Corresponding author.\n\u00a7Work done during internship at Fantasy AIGC Team.\n1\narXiv:2601.13976v1  [cs.CV]  20 Jan 2026\nFantasy AIGC Team\n\ud83d\udea9 Global Task:\nBring the toilet paper\nfrom the bedroom to\nthe toilet in the\nbathroom.\nSubgoal 1:\nFind the toilet paper.\nSubgoal 3:\nGo find the toilet in the bathroom.\nImagined Observation\nLegend\nPredicted action via T-CoT\nPredicted action\u00a0w/o CoT\nPredicted action\u00a0via V-CoT\nCross-mode alignment\u00a0constraint\nSemantic Plans\nSemantic Planning: To finish this\ntask, the agent should first find ...\nVisual Description: The historical\nobservations indicate that ... The\ncurrent visual inputs show that ...\nAction Planning: Move forward\nthree times to exit the bathroom, ...\nVisual Imagination: After executing\nthe planned actions, ... I should\nsee...\nSubgoal 2:\nGo to the bathroom.\n\ud83c\udfc1 Finish\nFigure 1: Overview of FantasyVLN. FantasyVLN is a VLN framework that integrates the strengths\nof textual and visual CoT reasoning modes, thereby jointly modeling semantic planning and spatial\nunderstanding.\nThe recent success of large language models (LLMs) has inspired the integration of Chain-of-\nThought (CoT) reasoning into embodied navigation to improve interpretability and long-horizon\ndecision-making. Methods such as NavCoT (Lin et al., 2025b) and NavGPT-2 (Zhou et al., 2024)\nemploy step-by-step textual reasoning to decompose navigation instructions or generate intermediate\nsubgoals. However, their reasoning remains confined to the textual modality, typically by translating\nobservations into captions, thereby limiting the joint modeling of semantic planning and spatial\nunderstanding, both essential for successful navigation.\nThis limitation is compounded by the\ndifficulty of annotating CoT supervision in VLN, as highlighted by EvolveNav (Lin et al., 2025a),\nwhere multiple valid action sequences often exist. Moreover, explicitly supervised CoT reasoning\ntends to overfit training distributions and generalize poorly to unseen environments.\nLately, works such as CoT-VLA (Zhao et al., 2025), VISTA (Huang et al., 2025), RBF++ (Chen\net al., 2025), OctoNav-R1 (Gao et al., 2025), and OmniNav (Xue et al., 2025) have extended CoT\nreasoning into visual or multimodal domains to better couple semantic and spatial reasoning for\ngeneralizability. While this multimodal CoT paradigm marks an important step forward, it also\nintroduces new challenges for long-horizon navigation. In particular, modeling reasoning chains\nacross both language and vision requires the model to iteratively generate and interpret imagined\nintermediate observations at each step, leading to severe token inflation. A typical reasoning step\nspanning 5\u20137 actions expands into over 3k\u20135k tokens, an order of magnitude larger than purely\ntextual CoTs (usually <500 tokens). This explosion in sequence length drastically increases both\ntraining and inference latency, rendering real-time navigation infeasible even on high-end GPUs.\nTo address these challenges, we propose a unified implicit reasoning framework that retains the\nbenefits of CoT-style reasoning while eliminating its explicit token overhead during inference. The\nkey idea is twofold: (i) During training, we encode the imagined observation tokens generated by\nmultimodal CoT reasoning into a compact latent space using a pretrained Visual AutoRegressive\n(VAR) model. This significantly reduces sequence length and training cost without compromising\nthe richness of visual reasoning. (ii) At inference, the agent performs direct instruction-to-action\nmapping while still leveraging reasoning-aware representations, inspired by the train-with-CoT,\ninfer-without-CoT paradigm of Aux-Think (Wang et al., 2025).\nConcretely, we introduce a unified multi-CoT training strategy that jointly learns from textual-only,\nvisual-only, and textual\u2013visual CoT modes using a special tag token to indicate each mode. This\ndesign unifies both the input format and model parameters within a single framework.\nDuring\ntraining, we align the action predictions from CoT-based reasoning modes with those from direct\nprediction (without CoT), enforcing modality-invariant reasoning representations. Consequently,\n2\nFantasy AIGC Team\nthe model learns implicit reasoning capabilities that generalize effectively without explicit CoT\nsupervision or overfitting to training distributions.\nTo this end, our contributions are summarized as follows: (i) We propose the first unified implicit\nCoT reasoning framework that integrates textual, visual, and multimodal CoT paradigms within a\nsingle model. Unlike prior explicit CoT methods, our approach trains with diverse reasoning modes\nbut performs inference without generating CoT sequences, achieving reasoning-aware yet real-time\nnavigation. (ii) We introduce a gating-based multi-CoT learning mechanism that allows seamless\nswitching among reasoning modes and direct action prediction under shared parameters. By aligning\nCoT-driven and direct action predictions, our model learns consistent, modality-invariant reasoning\nrepresentations. (iii) To reduce the token explosion in multimodal reasoning, we compress imagined\nobservation tokens into a compact latent space using a pretrained Visual AutoRegressor (VAR),\nimproving training efficiency while preserving semantic\u2013spatial reasoning capacity. (iv) Extensive\nexperiments on the challenging LH-VLN benchmark demonstrate that our method substantially\nimproves navigation success and efficiency in multi-stage and long-horizon scenarios, while reducing\ninference latency by an order of magnitude compared to explicit CoT approaches.\n(a) Non-CoT Reasoning\nObservation\nInstruction\nAction\nObservation\nInstruction\nAction\nT-CoT\n(b) Textual CoT Reasoning (Training Only)\nObservation\nInstruction\nAction\nV-CoT\n(c) Visual CoT Reasoning (Training Only)\nObservation\nInstruction\nV-CoT\nT-COT\nAction\n(d) Multimodal CoT Reasoning (Training Only)\nFantasyVLN\nCross-Mode Alignment Loss\nLegend\nSemantic Plans\nVisual Description:\u00a0 The historical\nimages shows... The current images\nshows a living room with a door on my\nright...\nReasoning: ... A right turn could\nlogically lead to the office based on\ntypical house layouts...\nVisual Imagination:\u00a0 ... After turning\nright, a doorway leading to the office\nshould become visible, with office-\nImagined Observation\nTextual Token\nVisual Token\nAction Token\nGating Token\nModel Input\nAR Feedback\nSemantic\nPlans\nVisual Description:\nThe historical images\nshows...\nReasoning: ... A right\nturn could logically\nlead to...\nVisual Imagination:\n... After turning right, a\ndoorway leading to...\nImagined\nObservation\n0\n0\n1\n0\n0\n1\n1\n1\nVAR Decode\nVAR Decode\nVAR Decode\nFigure 2: Overview of our unified multimodal Chain-of-Thought reasoning framework. The model\nsupports four reasoning modes under a shared architecture: (a) non-CoT reasoning for real-time\ninference, (b) textual CoT, (c) visual CoT enabled by VAR-compressed imagined observations, and\n(d) multimodal CoT combining textual and visual reasoning. A gating mechanism switches the model\nacross reasoning modes, while the action predictions from CoT modes are consistently aligned with\nthe non-CoT mode.\n2\nRelated Works\n2.1\nVision-and-Language Navigation\nEarly VLN models typically separate perception, instruction understanding, and action planning into\ndiscrete modules, and rely on imitation (Nguyen et al., 2019; Wang et al., 2022; Wu et al., 2020)\nor reinforcement learning (Xu et al., 2023; Wang et al., 2020) with auxiliary tasks such as progress\nmonitoring or instruction reweighting. However, these methods, built on panoramic observations\nin discrete environments (e.g., R2R (Anderson et al., 2018) and RxR (Ku et al., 2020)), suffer\nfrom poor semantic alignment and limited generalization in continuous or unseen environments\n(e.g.\nVLN-CE (Krantz et al., 2020)).\nTo address these limitations, recent studies have shifted\ntoward end-to-end navigation policy learning with pretrained vision-language models. For example,\nPoliformer (Zeng et al., 2025) introduces transformer-based on-policy reinforcement learning for\nvideo-level navigation. NaVid (Zhang et al., 2024a) and Uni-NaVid (Zhang et al., 2025a) extend\nthis paradigm by performing monocular video-based navigation without depth or maps and unifying\nmultiple embodied tasks. NaVILA (Cheng et al., 2024) further integrates VLN with legged robot\nlocomotion, achieving impressive cross-embodiment generalization. While achieving remarkable\nprogress on short-term tasks, they still struggle to reason and plan for long-horizon, multi-stage\n3\nFantasy AIGC Team\ntasks. More recently, CoT reasoning has emerged as a crucial paradigm for Embodied AI tasks. In\nVLN, NavGPT leverages the zero-shot CoT reasoning ability of GPT-4, while Aux-Think introduces\nauxiliary CoT supervision to internalize reasoning patterns during training.\nYet, existing CoT-\nbased VLN methods confine reasoning within a single modality, leaving multimodal CoT largely\nunexplored. In this paper, we follow the continuous environment setting and systematically investigate\nmultimodal CoT reasoning in VLN.\n2.2\nChain-of-Thought Reasoning\nChain-of-Thought (CoT) reasoning enables large language models (LLMs) to solve complex problems\nby explicitly generating intermediate steps (Wei et al., 2022). Subsequent variants, such as Self-\nConsistency (Wang et al., 2023) and Least-to-Most Prompting (Zhou et al., 2023), further enhance\nreasoning robustness and compositionality. Recent studies have extended CoT to vision-language\nmodels (Zhang et al., 2024b), which can be categorized into three types based on the modality of\nreasoning steps: Textual CoT, Visual CoT, and Multimodal CoT. Specifically, Textual CoT (Zhang\net al., 2024c) in VLMs typically follows the format of vanilla LLM CoT. Visual CoT methods,\nsuch as CoT-VLA (Zhao et al., 2025) and DreamVLA (Zhang et al., 2025b), generate future frames\nbefore action prediction in manipulation tasks, while Multimodal CoT (Cheng et al., 2025) jointly\npredicts paired textual and visual reasoning steps in multimodal tasks. To the best of our knowledge,\nFantasyVLN is the first unified CoT reasoning framework that integrates these three reasoning\nparadigms.\n3\nMethods\n3.1\nOverview\nWe propose FantasyVLN, a VLN framework that integrates multimodal reasoning modes as its\ncore design, while enabling implicit reasoning for efficient inference.\nAs shown in Figure 2,\nFantasyVLN internalizes diverse CoT reasoning patterns across modalities through end-to-end\njoint training, and enhances the non-CoT reasoning mode via a cross-mode alignment constraint.\nThis enables combining the advantages of both textual and visual CoT reasoning without incurring\nexplicit CoT reasoning time. Moreover, we perform visual CoT reasoning in the latent space of the\nVAR model (Tian et al., 2024), which improves training and inference efficiency compared with\npixel-space methods.\nBelow, we introduce the problem setup, cross-mode alignment constraint, unified multimodal implicit\nreasoning and latent visual CoT learning.\n3.2\nProblem Setup\nVLN aims to develop an embodied agent \u03c0\u03b8 that navigates continuous 3D environments O based on a\nnatural language instruction I and visual observations, which can be formulated as a non-Markovian\ntemporal decision problem. Let s0 denote the initial state, i.e., location and orientation, and U denote\nthe action space. At each timestep t, the agent \u03c0\u03b8 receives multi-view visual observations ot \u2208O\nand predicts future actions At \u2208U conditioned on the instruction I and historical observations\n{o\u2264t}. Subsequently, the predicted actions At are executed, transferring the agent \u03c0\u03b8 to a new state\naccording to the environment dynamics. This interaction process continues until a stop action is\nexecuted or the maximum step T is reached.\n3.3\nCompact Visual Chain-of-Thought\nConventional V-CoT reasoning predicts thousands of visual tokens at each reasoning step, resulting\nin low training efficiency and high inference latency. To address this issue, we present Compact\nVisual Chain-of-Thought (CompV-CoT), which trains Qwen2.5-VL to directly generate a compact\nset of visual tokens in the latent space of a pretrained VAR model, yielding a novel compressed\nvisual chain-of-thought representation with far fewer tokens. The VAR model follows a next-scale\nprediction paradigm to hierarchically encode visual information, achieving higher efficiency than\nconventional autoencoding approaches such as VAE, VQ-VAE (Gafni et al., 2022) or RAE (Zheng\n4\nFantasy AIGC Team\net al., 2025). Given a 256 \u00d7 256 image, the VAR model enables precise reconstruction using the\ncorresponding low-scale representations, which contain only 30 visual tokens. As shown in Table 1,\nVAR achieves a higher compression ratio under comparable reconstruction quality.\nTable 1: Comparison of compression ratio and reconstruction error (MSE) across different visual\ncompressors.\nCompressors\nComp. Ratio\nMSE\nRAE-DINOv2-B\n1/256\n0.012\nRAE-SigLIP2-B\n1/256\n0.011\nVAE\n1/64\n0.005\nVQVAE\n1/64\n0.007\nVAR\n1/2185\n0.039\nSpecifically, we employ the VAR model as the visual decoder of the VLM and perform V-CoT\nin the VAR latent space. The VLM first takes navigation instructions and visual observations as\ninput and then generates latent representations of future observations before predicting actions.\nThe VAR model finally decodes generated representations into pixel frames. We freeze the VAR\nmodel during training, while the VLM first learns to predict latent future observations and then\ninfers the corresponding actions. During inference, we use only the VLM to perform visual CoT-\nbased navigation without explicit VAR decoding. Owing to the highly efficient visual information\ncompression and non-display image decoding, the proposed CompV-CoT method improves both\ntraining and inference efficiency.\n3.4\nUnified Multimodal Chain-of-Thought\nBuilding on CompV-CoT, we further present a Unified Multimodal Chain-of-Thought (UM-CoT)\nframework that integrates textual, compressed visual, and multimodal reasoning within a single\nagent.\nTextual CoT in VLN.\nTextual CoT (T-CoT) models the agent\u2019s reasoning as an explicit semantic\nplanning process that bridges language understanding and action decision.\nInstead of directly\nmapping instructions to actions, the agent first generates textual intermediate reasoning steps bTt.\nThese reasoning steps then provide structured causal guidance for predicting subsequent actions\nb\nAt, enabling interpretable and more reliable decision-making. Specifically, the intermediate steps\ntypically involve inferring subgoals from the instruction, assessing progress through current and\nhistorical visual observations, and identifying actionable cues for achieving the next objective.\nCompV-CoT as Visual CoT.\nFor visual CoT (V-CoT), we directly adopt the CompV-CoT intro-\nduced in Sec. 3.3 as the visual reasoning mode in UM-CoT. In this setting, the agent imagines\nfuture observations in the VAR latent space by predicting compressed visual tokens, and then infers\nactions conditioned on the imagined latent trajectory. Compared with pixel-space prediction, this\nCompV-CoT design yields more efficient and stable visual reasoning.\nMultimodal CoT in VLN.\nMultimodal CoT (MM-CoT) is defined as a native combination of T-\nCoT and CompV-CoT, where the agent is required to generate paired textual\u2013visual reasoning steps.\nWe denote the multimodal reasoning trace as Mt = [Tt, Vt], which jointly encodes semantic plans\nand imagined future observations, and use it to guide subsequent action prediction.\nUnified Multimodal CoT Reasoning Framework.\nTo unify the above reasoning modes within a\nsingle framework, we introduce two binary gating signals gT and gV that control whether textual\nand visual reasoning is activated. Given I, {o\u2264t}, and (gT , gV), the agent jointly predicts reasoning\ntraces and actions:\n[ bRt, b\nAt] = \u03c0\u03b8\n\u0000I, {o\u2264t}, gT , gV\n\u0001\n,\n(1)\n5\nFantasy AIGC Team\nwhere\nbRt =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nNone,\nif (gT , gV) = (0, 0),\nbTt,\nif (gT , gV) = (1, 0),\nbVt,\nif (gT , gV) = (0, 1),\nc\nMt,\nif (gT , gV) = (1, 1).\n(2)\nThis gating mechanism allows a single policy to flexibly operate in CoT, T-CoT, CompV-CoT, and\nMM-CoT modes.\nJoint Training via Data Mixture.\nGiven the navigation instruction I, visual observations {o\u2264t}\nand the ground truth action At. To enable end-to-end training, we organize the expert navigation\ndataset D into five-tuples:\n[I, {o\u2264t}, Tt, Vt, At] \u2208D,\n(3)\nwhere Tt and Vt denote the ground truth textual reasoning steps and CompV-CoT visual reasoning\nsteps, respectively.\nWe employ Qwen-VL-Max to generate textual reasoning traces Tt.\nDuring\ntraining, (gT , gV) are uniformly sampled and integrated with I and {o\u2264t} to form the query, while\nthe answer is constructed according to Eq. (2) by selecting Rt \u2208{None, Tt, Vt, Mt} together with\nAt. The joint objective is defined as:\nLJoint = (\u00acgT \u2227\u00acgV) LCE\n\u0000 b\nAt, At\n\u0001\n+ (gT \u2227\u00acgV) LCE\n\u0000[bTt, b\nAt], [Tt, At]\n\u0001\n+ (\u00acgT \u2227gV) LCE\n\u0000[bVt, b\nAt], [Vt, At]\n\u0001\n+ (gT \u2227gV) LCE\n\u0000[ c\nMt, b\nAt], [Mt, At]\n\u0001\n,\n(4)\nwhere LCE denotes the causal cross-entropy loss.\nAlgorithm 1 Cross-Mode Aligned Joint Training\n1: Input: Dataset D, parameters \u03b8, learning rate \u03b7, alignment weight \u03bbalign\n2: Output: Trained parameters \u03b8\u2217\n3: while not converged do\n4:\n[I, {o\u2264t}, Tt, Vt, At] \u223cD\n5:\nb\nAt \u2190\u03c0\u03b8(I, {o\u2264t}, gT =0, gV=0)\n6:\n\u03b8 \u2190\u03b8 \u2212\u03b7\u2207\u03b8LCE( b\nAt, At)\n7:\ne\nAt \u2190sg\n\u0002\n\u03c0\u03b8(I, {o\u2264t}, gT =0, gV=0)\n\u0003\n8:\n[bTt, b\nAT\nt ] \u2190\u03c0\u03b8(I, {o\u2264t}, gT =1, gV=0)\n9:\n[bVt, b\nAV\nt ] \u2190\u03c0\u03b8(I, {o\u2264t}, gT =0, gV=1)\n10:\n[ c\nMt, b\nAM\nt ] \u2190\u03c0\u03b8(I, {o\u2264t}, gT =1, gV=1)\n11:\nCompute L\u2217\nJoint using Eq. (7)\n12:\n\u03b8 \u2190\u03b8 \u2212\u03b7\u2207\u03b8L\u2217\nJoint\n13: end while\n14: \u03b8\u2217\u2190\u03b8\n15: return \u03b8\u2217\n3.5\nCross-Mode Alignment Constraint\nTo prevant conflict between different reasoning modes, we introduce a Cross-Mode Alignment\nConstraint that regularizes the unified multimodal CoT training. The key idea is to use the non-CoT\nreasoning mode as a supervisory signal to align all CoT variants, thereby embedding diverse reasoning\nbehaviors into a shared latent policy. Let b\nAt, b\nAT\nt , b\nAV\nt , and b\nAM\nt\ndenote the action predictions from\nthe non-CoT, T-CoT, V-CoT, and MM-CoT reasoning modes, respectively. In each iteration, we first\noptimize the non-CoT reasoning mode with the objective:\nLnon-CoT = LCE\n\u0000 b\nAt, At\n\u0001\n,\n(5)\n6\nFantasy AIGC Team\nTable 2: Comparison of navigation accuracy across different VLN methods on LH-VLN benchmark.\nThe best and second-best results are marked in bold and underlined, respectively.\nCoT Modal\nMethods\nSR\nISR\nCSR\nCGT\nNone/ZS\nRandom\n0\n0\n0\n0\nGLM-4v prompt\n0\n0\n0\n0\nGPT-4 + NaviLLM\n0\n2.19\n1.45\n2.61\nMGDM\n0\n2.34\n1.65\n2.91\nVisual\nCoT-VLA\n0\n0\n0\n0\nWorldVLA\n0\n0\n0\n0\nTextual\nAux-Think\n0.65\n3.16\n2.04\n1.47\nunified multimodal\nFantasyVLN\n2.44\n11.01\n9.64\n8.99\nwhere\nb\nAt = \u03c0\u03b8\n\u0000I, {o\u2264t}, gT = 0, gV = 0\n\u0001\n.\n(6)\nWe then obtain the soft targets e\nAt by rerunning the forward process (6). Finally, we incorporate the\ncross-mode alignment constraint into the joint objective of unified multimodal CoT reasoning:\nL\u2217\nJoint = LAlign + LCoT,\n(7)\nwhere\nLAlign = LCE\n\u0000 b\nAT\nt , e\nAt\n\u0001\n+ LCE\n\u0000 b\nAV\nt , e\nAt\n\u0001\n+ LCE\n\u0000 b\nAM\nt , e\nAt\n\u0001\n,\n(8)\nand\nLCoT = LCE\n\u0000bTt, Tt\n\u0001\n+ LCE\n\u0000bVt, Vt\n\u0001\n+ LCE\n\u0000 c\nMt, Mt\n\u0001\n.\n(9)\nWe alternately minimize the non-CoT objective (5) and the cross-mode aligned joint objective (7)\nuntil the training losses Lnon-CoT and L\u2217\nJoint converge.\nDuring this alternating optimization, all\nreasoning modes operate on similar inputs, share network parameters, and are aligned to identical\nsupervisory signals, thereby implicitly embedding diverse CoT reasoning patterns into a unified\nlatent representation. The overall algorithm is presented in Algorithm 1.\n3.6\nVLN During Inference\nDue to the real-time demands of VLN and the inference latency introduced by explicit CoT token\ndecoding, we follow Aux-Think (Wang et al., 2025) and adopt the non-CoT reasoning mode during\ninference. Similar to Aux-Think (Wang et al., 2025), our framework serves as an implicit reasoning\nmechanism that internalizes diverse CoT patterns and implicitly enhances non-CoT reasoning through\ncross-mode alignment and joint training across reasoning modes.\n4\nExperiments\n4.1\nExperimental Setup\nBenchmark.\nWe evaluate FantasyVLN on the challenging LH-VLN (Song et al., 2025) bench-\nmark, which is characterized by multi-stage tasks and long navigation trajectories. On one hand,\nTable 3: Comparison of navigation accuracy across different reasoning mode combinations on LH-\nVLN.\nnon-CoT\nT-CoT\nV-CoT\nMM-CoT\nSR\nISR\nCSR\nCGT\n\u2713\n0\n2.01\n1.51\n1.55\n\u2713\n\u2713\n0.98\n8.26\n6.60\n6.15\n\u2713\n\u2713\n1.46\n11.19\n9.66\n8.84\n\u2713\n\u2713\n0.49\n7.77\n6.48\n8.89\n\u2713\n\u2713\n\u2713\n\u2713\n2.44\n11.01\n9.64\n8.99\n7\nFantasy AIGC Team\nTable 4: Comparison of inference efficiency across different CoT reasoning methods. The best\nresults are marked in bold.\nReasoning Mode\nMethods\nModel Size\nAPS\nExplicit\nCoT-VLA\n7B\n0.19\nImplicit\nWorldVLA\n7B\n1.02\nAux-Think\n8B\n0.97\nFantasyVLN\n7B\n1.03\nmulti-stage navigation requires the agent to sequentially reach multiple goals, imposing higher\ndemands on reasoning and planning. On the other hand, longer navigation trajectories amplify\ncumulative errors compared to shorter ones. Following the standard LH-VLN setting, we perform\nonline evaluation on the test set, where both the tasks and scenes are unseen.\nBaselines.\nWe compared the proposed FantasyVLN with several representative methods. They\ncan be divided into four categories: (i) textual CoT-based methods, Aux-Think (Wang et al., 2025);\n(ii) visual CoT-based methods, CoT-VLA and WorldVLA; (iii) memory-based methods, MGDM; (iv)\nother baselines provided by LH-VLN, GLM-4v prompt, NaviLLM and GPT-4 + NaviLLM. For fair\ncomparison, all methods are trained on the same LH-VLN training set, and the validation set is used\nto select the best checkpoint for each method. For Aux-Think and CoT-VLA, we implement their\nmethods based on the descriptions in their papers, as the training codes are not publicly available. For\nWorldVLA, we adapt the official implementation by modifying the preprocessing pipeline to support\ntraining on the LH-VLN dataset. For all other methods, we use the implementations provided by\nLH-VLN.\nMetrics.\nFollowing (Song et al., 2025), we use Success Rate (SR), Independent Success Rate\n(ISR), Conditional Success Rate (CSR), and CSR weighted by Ground Truth (CGT) to measure\nmulti-stage navigation accuracy. SR denotes the success rate of multi-stage task navigation, ISR\nrepresents the success rate of individual subtasks, CSR weights the ISR according to the success of\npreceding subtasks, and CGT further weights the CSR based on the length of the expert trajectory.\nMoreover, we introduce the Action Per Second (APS) to evaluate inference efficiency:\nAPS = Nact\nTnav\n,\n(10)\nwhere Nact denotes the total number of executed actions, and Tnav represents the total navigation time\nin seconds.\n4.2\nMain Results\nNavigation Accuracy.\nTable 2 presents the quantitative results of navigation accuracy across\ndifferent VLN methods on the LH-VLN benchmark. FantasyVLN achieves superior performance\nacross all metrics, with SR, ISR, CSR, and CGT of 2.44, 11.01, 9.64, and 8.99, respectively,\nsignificantly surpassing all baselines. Aux-Think shows suboptimal results in SR, ISR, and CSR,\nindicating that T-CoT enhances navigation robustness compared to non-CoT approaches. However,\nits performance still exhibits a notable gap compared to FantasyVLN, owing to the limitations of\nsingle-modal CoT modeling and the lack of an explicit\u2013implicit alignment mechanism. MGDM\nperforms relatively well among non-CoT baselines, particularly in CGT, suggesting that memory\nmechanisms offer limited yet tangible benefits. Overall, the results demonstrate that our unified\nmultimodal implicit reasoning framework is crucial for tackling the complex multi-stage VLN task.\nInference Efficiency.\nTo quantify the inference efficiency of different CoT reasoning methods,\nwe report APS in Table 4. Implicit reasoning models, including FantasyVLN, Aux-Think, and\nWorldVLA, exhibit comparable efficiency and outperform the explicit approach CoT-VLA by a\nsubstantial margin. This outcome is expected. Implicit reasoning predicts each action by decoding\na single token, while explicit reasoning requires generating CoT reasoning steps with thousands\nof tokens. Under similar model sizes, implicit CoT reasoning predicts approximately one action\n8\nFantasy AIGC Team\nper second, while explicit CoT reasoning yields only 0.19 actions per second. Therefore, implicit\nreasoning better satisfies the real-time requirements of the VLN task.\n4.3\nAblation Studies\nContribution of Each Reasoning Mode.\nFantasyVLN integrates diverse reasoning modes within\na unified framework. To verify the contribution of each reasoning mode to the overall framework,\nwe explore various combinations of non-CoT, T-CoT, V-CoT, and MM-CoT modes during training.\nAs shown in Table 3, combining any CoT reasoning mode with non-CoT reasoning consistently\nimproves navigation performance across all metrics. Integrating all four reasoning modes further\nenhances the overall performance.\nVAR Scale Selection.\nAs detailed in Section 3.3, we perform V-CoT in the latent space of VAR. To\nselect the optimal VAR scale for latent V-CoT learning, we conduct comprehensive ablation studies\non a subset of LH-VLN. We first report the ISR results across different VAR scales, ranging from 1 to\n10, as shown in Figure 3. The results show that scale 4 achieves the best performance. We attribute\nthis to smaller scales lacking sufficient visual information, while larger scales leading redundancy.\nTo validate this argument, we randomly sample 100 images from LH-VLN and employ a pretrained\nVAR model to reconstruct them. Specifically, VAR takes the ground truth latents up to a given scale\nas input and predicts the remaining latent scales. The reconstructed images are then obtained by\ndecoding both the ground truth and predicted latents together. As shown in Figure 4, the results are\nconsistent with our argument.\nEffect of Cross-Mode Alignment Constraint.\nWe introduce a cross-mode alignment constraint\ninto the joint training of FantasyVLN. Table 5 compares SR, ISR, CSR, and CGT performance\nwith and without this constraint. Without the cross-mode alignment constraint (\u2717), FantasyVLN\nexhibits weak navigation ability, achieving only marginal ISR, CSR, and CGT scores. Adopting\nthis constraint (\u2713) yields substantial improvements across all metrics, with SR increasing from 0 to\n2.44, ISR from 2.39 to 11.01, CSR from 1.19 to 9.64, and CGT from 1.28 to 8.99. This indicates\nthat cross-mode alignment is essential for FantasyVLN, a unified framework that integrates diverse\nreasoning modes.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nVAR Scale\n0\n2\n4\n6\n8\nISR\n0.00\n0.00\n3.76\n7.41\n6.00\n3.76\n3.76\n2.26\n0.75\n0.90\nFigure 3: ISR variation with respect to different VAR scales.\n9\nFantasy AIGC Team\nScale 1\nScale 2\nScale 3\nScale 4\nScale 5\nScale 6\nScale 7\nScale 8\nScale 9\nScale 10\nGT\nFigure 4: Qualitative comparison of image reconstruction results produced by the VAR model using\nlatent inputs across different scales. For each image, the VAR model receives the ground truth\nlatents up to a specified scale and predicts all remaining scales; the final reconstruction is obtained\nby decoding the combined ground truth and predicted latents.\nTable 5: Comparison of SR, ISR, CSR, and CGT performance with and without cross-mode align-\nment.\nAlignment Constraint\nSR\nISR\nCSR\nCGT\n\u2717\n0\n2.39\n1.19\n1.28\n\u2713\n2.44\n11.01\n9.64\n8.99\n4.4\nMore Results\nTraining Efficiency.\nAs shown in Table 2, existing visual CoT methods (e.g., WorldVLA) achieve\nlimited navigation accuracy on the LH-VLN benchmark, failing to generalize to long-horizon sce-\nnarios. To understand the underlying cause, we compare their training efficiency with our unified\nmultimodal formulation. As shown in Figure 5, WorldVLA exhibits slow and unstable convergence,\nrequiring over 10k iterations to reach moderate token prediction accuracy. This indicates that pixel-\nlevel V-CoT learning delivers weak gradient signals, as the model must reconstruct high-dimensional\nvisual tokens for each reasoning step. In contrast, FantasyVLN converges rapidly within a few\nthousand iterations, reflecting stable supervision and more efficient learning dynamics. This im-\nprovement stems from our CompV-CoT design, where visual reasoning operates in a compact latent\nspace encoded by the pretrained VAR compressor. By replacing dense pixel reconstruction with com-\npressed latent prediction, the model learns richer multimodal reasoning cues under a substantially\nlighter optimization burden. Overall, these results highlight that CompV-CoT not only enhances\nreasoning efficiency but also yields more stable and interpretable learning behavior, contributing to\nthe superior navigation accuracy of FantasyVLN in long-horizon tasks.\nExplicit vs. Implicit Reasoning.\nFantasyVLN supports both explicit and implicit reasoning,\nenabling us to systematically compare their effectiveness across different CoT modalities. As sum-\nmarized in Table 6, we evaluate the T-CoT, V-CoT, and MM-CoT models under two inference modes.\nFor each case, the model is jointly trained with its corresponding CoT mode and the direct prediction\npathway. During inference, using the CoT branch corresponds to explicit reasoning, while employing\nthe direct pathway corresponds to implicit reasoning. Overall, implicit reasoning consistently yields\nhigher navigation accuracy, particularly under multimodal settings. In MM-CoT, implicit inference\nachieves the best performance with 2.44 SR and 11.01 ISR, surpassing the explicit counterpart by a\nlarge margin. This result aligns with the observations in Aux-Think (Wang et al., 2025), suggesting\nthat explicit CoT decoding may amplify cumulative reasoning errors across long trajectories. We\nattribute this phenomenon to two key factors: (i) the limited training data of LH-VLN (only 18k\ntrajectory slices of five steps each) makes explicit CoT sequences prone to overfitting and error prop-\nagation; (ii) explicit reasoning expands temporal dependencies, causing misaligned textual or visual\nCoT tokens to accumulate deviations over time. In contrast, implicit reasoning benefits from cross-\nmode alignment during training, allowing the model to internalize reasoning cues while maintaining\nstable and efficient inference.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of \"FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation\"\n\n#### Main Contributions:\n- **Unified Implicit CoT Framework**: Proposes a unified implicit reasoning framework that integrates textual, visual, and multimodal Chain-of-Thought (CoT) modes within a single model. Unlike explicit CoT methods, it performs inference without generating CoT sequences, achieving real-time navigation while maintaining reasoning-aware representations.\n  \n- **Compact Visual Chain-of-Thought**: Introduces Compact Visual Chain-of-Thought (CompV-CoT), which trains the model to generate compact sets of visual tokens in a pretrained Visual AutoRegressor (VAR) latent space. This reduces sequence length and training cost without compromising visual reasoning richness.\n\n- **Unified Multimodal CoT Reasoning**: Develops a Unified Multimodal Chain-of-Thought (UM-CoT) framework that integrates textual, compressed visual, and multimodal reasoning modes within a single agent. It uses gating mechanisms to switch between different reasoning modes during inference.\n  \n- **Cross-Mode Alignment Constraint**: Introduces a cross-mode alignment constraint to regularize unified multimodal CoT training, embedding diverse reasoning behaviors into a shared latent policy.\n\n#### Methodology:\n1. **Problem Setup**:\n   - Defines the VLN task as a non-Markovian temporal decision problem where an agent navigates through continuous 3D environments based on natural language instructions and visual observations.\n   \n2. **Compact Visual Chain-of-Thought (CompV-CoT)**: \n   - Trains Qwen-VL-Max to directly generate compact sets of visual tokens in the VAR latent space, reducing sequence length and improving training efficiency compared to pixel-space methods.\n\n3. **Unified Multimodal CoT Reasoning Framework**:\n   - Introduces two binary gating signals (gT and gV) that control whether textual or visual reasoning is activated during inference.\n   \n4. **Cross-Mode Alignment Constraint**: \n   - Regularizes unified multimodal CoT training by using non-CoT reasoning mode as a supervisory signal to align all CoT variants, embedding diverse reasoning behaviors into a shared latent policy.\n\n#### Key Findings:\n1. **Improved Navigation Accuracy**:\n   - On the challenging LH-VLN benchmark, FantasyVLN achieves superior performance across multiple metrics (SR, ISR, CSR, CGT) compared to existing methods.\n   \n2. **Enhanced Inference Efficiency**:\n   - Compared to explicit CoT methods like CoT-VLA and WorldVLA, implicit reasoning models such as FantasyVLN outperform by a significant margin in terms of Action Per Second (APS).\n\n3. **Optimal VAR Scale Selection**: \n   - Comprehensive ablation studies reveal that using scale 4 for latent V-CoT learning yields the best performance.\n\n4. **Cross-Mode Alignment Importance**:\n   - The cross-mode alignment constraint is essential for unified multimodal reasoning, significantly improving navigation accuracy across all metrics.\n\n#### Conclusions:\nFantasyVLN represents a significant advancement in VLN by integrating diverse CoT modes within a single framework while maintaining real-time navigation capabilities and improved inference efficiency. Its ability to handle complex tasks with robust multimodal reasoning highlights its potential for future embodied AI applications."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Universal Approximation Theorem for\nInput-Connected Multilayer Perceptrons\nVugar Ismailov\u2217\nJanuary 21, 2026\nAbstract\nWe introduce the Input-Connected Multilayer Perceptron (IC\u2013MLP), a feedfor-\nward neural network architecture in which each hidden neuron receives, in addition\nto the outputs of the preceding layer, a direct affine connection from the raw in-\nput. We first study this architecture in the univariate setting and give an explicit\nand systematic description of IC\u2013MLPs with an arbitrary finite number of hidden\nlayers, including iterated formulas for the network functions. In this setting, we\nprove a universal approximation theorem showing that deep IC\u2013MLPs can approx-\nimate any continuous function on a closed interval of the real line if and only if the\nactivation function is nonlinear. We then extend the analysis to vector-valued in-\nputs and establish a corresponding universal approximation theorem for continuous\nfunctions on compact subsets of Rn.\n1\nIntroduction\nMultilayer perceptrons (MLPs) are widely used for function approximation, regression,\nclassification, and pattern recognition. An MLP consists of an input layer, one or more\nhidden layers, and an output layer. Each neuron forms a weighted sum of its inputs, adds\na bias, and then applies an activation function. From the approximation-theoretic point\nof view, an MLP generates a family of parametrized functions, and the main question is\nhow large this family is.\nA basic notion in this area is the universal approximation property. Roughly speaking,\na class of networks has this property if it can approximate every continuous function on\na compact set with arbitrary accuracy. The first results of this type were obtained for\nshallow networks, i.e., networks with a single hidden layer. For an input x \u2208Rd, a typical\nsingle-hidden-layer network computes a function of the form\nr\nX\ni=1\nci \u03c3(wi \u00b7 x + bi),\n(1)\nwhere r \u2208N, ci, bi \u2208R, wi \u2208Rd, and \u03c3 : R \u2192R is the activation function.\nThe\napproximation question asks when the set of all functions of the form (1) is dense in\nC(K) for every compact set K \u2282Rd.\n\u2217The author can be contacted at vugaris@mail.ru or vugaris@gmail.com.\n1\narXiv:2601.14026v1  [cs.LG]  20 Jan 2026\nShallow networks and universal approximation. The possibility of approximating\ncontinuous functions on compact subsets of Rd by networks of the form (1) has been\nstudied in a large number of works. Different approaches were used, including methods\nfrom harmonic analysis, functional analysis, and classical approximation theory.\nCarroll and Dickinson [1] used ideas related to the inverse Radon transform to establish\nuniversality results for certain single-hidden-layer neural networks. Gallant and White [2]\nconstructed a specific continuous, nondecreasing sigmoidal activation function (referred\nto as a \u201ccosine squasher\u201d) and showed that it yields density via Fourier series expansions.\nHere, a sigmoidal function is understood as a function \u03c3 : R \u2192R that tends to 0 as\nx \u2192\u2212\u221eand to 1 as x \u2192+\u221e.\nCybenko [3] and Funahashi [4] independently proved that feedforward networks with\na continuous sigmoidal activation function can approximate any continuous function on\ncompact subsets of Rd.\nCybenko\u2019s proof relies on methods from functional analysis,\nparticularly the Hahn\u2013Banach theorem and the Riesz representation theorem, while Fu-\nnahashi\u2019s approach is based on an integral representation due to Irie and Miyake [5],\nusing a kernel that can be expressed as a difference of two sigmoidal functions.\nHornik, Stinchcombe and White [6] provided another general universality theorem for\nfeedforward networks, based on an application of the Stone\u2013Weierstrass theorem. See also\nCotter [7] for a discussion of this theorem and its role in neural network approximation.\nIt is also important that universality can be obtained under various restrictions and\nin constructive forms. K\u02daurkov\u00b4a [9] showed that staircase-like functions of sigmoidal type\ncan approximate continuous functions on compact intervals in R, and used this idea\nin related results for deeper networks (see [8, 9]). Chen, Chen and Liu [10] extended\nCybenko\u2019s theorem by allowing bounded (not necessarily continuous) sigmoidal activation\nfunctions. A closely related result was obtained independently by Jones [11]. Other works\ninvestigated the effect of restricting weights and thresholds: Chui and Li [12] treated\ninteger weights and thresholds, while Ito [13] studied density results with unit weights.\nConstructive approximation schemes for special sigmoidal superpositions with a minimal\nnumber of terms, as well as their application to single- and two-hidden-layer networks with\nfixed weights, were developed in joint work with Guliyev [14\u201316]. A broader discussion of\nsuch restrictions, together with a comprehensive survey of related results, can be found\nin Pinkus [17] and in our monograph [18, Chapter 5].\nA natural direction, beyond sigmoidal activations, is to ask for general conditions on\n\u03c3 guaranteeing density. Many works established universality results for non-sigmoidal\nactivation functions; see, for example, Stinchcombe and White [19], Hornik [20], and\nMhaskar and Micchelli [21]. A complete characterization was obtained by Leshno, Lin,\nPinkus and Schocken [22]: for a continuous activation function, the family (1) is dense in\nC(K) for every compact K \u2282Rd if and only if \u03c3 is not a polynomial. Thus, for shallow\nnetworks, the universality question is, in a strong sense, completely understood.\nFrom shallow to deep networks. While the classical theory focuses on (1), modern\napplications often use several hidden layers. A standard two-hidden-layer feedforward\nnetwork computes a function of the form\nN2\nX\nj=1\nvj \u03c3\nN1\nX\ni=1\nwji \u03c3(ai \u00b7 x + bi) + dj\n!\n,\nwhere N1, N2 \u2208N and the remaining quantities are real parameters. Deeper networks\nare obtained by iterating this construction: each additional hidden layer applies an affine\n2\nmap to the outputs of the preceding layer and passes the result through \u03c3.\nThere are two related but distinct issues in the study of deep networks. The first is\nuniversality: does a given deep architecture approximate every continuous function on\ncompact sets? The second concerns efficiency: how the approximation error depends on\nthe depth and width, or how many parameters are needed to reach a given accuracy. The\npresent paper is devoted to the first issue, namely universality, and we briefly recall some\ndevelopments in this direction.\nUniversality can be achieved in many deep-network settings under explicit assump-\ntions on the activation function. To the best of our knowledge, Gripenberg [23] was the\nfirst to study deep networks with a bounded number of neurons in each hidden layer\nand to prove universality under the assumption that the activation function is continu-\nous, nonaffine, and twice continuously differentiable in a neighborhood of a point t with\n\u03c3\u2032\u2032(t) \u0338= 0.\nKidger and Lyons [24] later weakened this smoothness requirement. They showed that\nuniversality for deep and narrow networks still holds provided the activation function is\ncontinuous, nonaffine, and differentiable in a neighborhood of at least one point at which\nthe derivative is continuous and nonzero. Thus, in both [23] and [24], universality is\nobtained under local differentiability assumptions on the activation function, without\nrequiring any global smoothness.\nFor piecewise linear activations, Hanin and Sellke [25] obtained a sharp characteri-\nzation of universality for deep ReLU networks. They proved that, for input dimension\nd, a ReLU network with arbitrary depth can approximate every continuous function on\ncompact subsets of Rd if and only if every hidden layer has width at least d + 1 (that is,\ncontains at least d + 1 neurons).\nIn a different but related direction, Johnson [26] established a purely negative uni-\nversality result for a broad class of activation functions. He proved that if the activation\nfunction is uniformly continuous and can be approximated by a sequence of one-to-one\nfunctions, then layered feedforward networks in which each hidden layer has width at\nmost d cannot approximate all continuous functions on Rd, regardless of depth. This\nresult does not assert that universality holds for width d + 1 or larger; rather, it shows\nthat width strictly greater than d is necessary for universal approximation within this\nclass of activation functions.\nWe refer the reader to Goodfellow, Bengio and Courville [27] for background on neural\nnetwork architectures and motivations from machine learning, and to the recent mono-\ngraph by Petersen and Zech [28] for a mathematical analysis of deep neural networks,\nwith an emphasis on approximation theory, optimization theory, and statistical learning\ntheory.\nMotivation for input-connected architectures. In standard feedforward MLPs, the\nraw input x is used only in the first hidden layer.\nAfter that, deeper layers receive\ninformation only through the outputs of preceding layers. In this paper we introduce a\nsimple and natural modification: we allow each hidden neuron to access the raw input\ndirectly, in addition to the outputs of the previous layer. This creates an input-connected\narchitecture.\nWe call the resulting model the Input-Connected Multilayer Perceptron (IC\u2013MLP).\nFormally, in an IC\u2013MLP, every hidden neuron computes an expression of the form\n\u03c3\n\u0000(affine combination of previous-layer outputs) + (affine function of the raw input)\n\u0001\n,\nso that information from the input can enter at every depth. Standard MLPs are re-\n3\ncovered as a special case by suppressing these direct input connections beyond the first\nhidden layer. In this sense, IC\u2013MLPs form a strict extension of the classical feedforward\narchitecture, while preserving the basic feedforward structure.\nThe main point of this paper is that this architecture admits a clean and transparent\nuniversality theory. In particular, we obtain a universal approximation theorem whose\ncondition on the activation function is minimal and simple.\nOverview of results. Our analysis proceeds in two stages. We first consider the univari-\nate setting, where the input is a scalar x \u2208R. This case already captures the structural\nfeature that distinguishes IC\u2013MLPs from classical MLPs, and it allows a systematic de-\nscription of networks of arbitrary finite depth. For completeness, we describe the network\nfunctions for shallow and deep cases and then pass to the general L-layer architecture in\na unified notation.\nIn the univariate setting, we prove a universal approximation theorem giving a neces-\nsary and sufficient condition: deep IC\u2013MLPs approximate every continuous function on\na compact interval of the real line if and only if the activation function is nonlinear. This\nmirrors the classical \u201cif and only if\u201d characterizations known for shallow networks (com-\npare, for instance, [22]), but it is proved here for the input-connected deep architecture.\nWe then extend the analysis to vector inputs x \u2208Rn and show that the same equiv-\nalence holds for approximation of continuous functions on compact subsets of Rn. Thus,\nIC\u2013MLPs admit a universal approximation theorem in the multivariate setting as well.\nRelation to existing architectures. Architectures that allow the raw input to be fed\ndirectly to deeper layers have appeared previously in different contexts. Most notably,\nInput Convex Neural Networks (ICNNs) [29] introduce so-called passthrough connections\nthat link the input directly to hidden units in deeper layers. In that setting, such connec-\ntions are introduced to ensure that the network output is convex with respect to selected\ninputs. This convexity is enforced by requiring certain inter-layer weights to be nonnega-\ntive. The direct passthrough connections then compensate for restrictions imposed by the\nconvexity constraints. The motivation for input-connected architectures in the present\nwork is different and arises from approximation-theoretic considerations.\nThe IC\u2013MLP architecture studied in this paper also allows direct affine dependence on\nthe input at every depth. However, it is developed in a fundamentally different setting.\nNo convexity constraints are imposed, and the networks are evaluated in a standard\nfeedforward manner, rather than through optimization-based inference. In particular,\nthe affine input term at the output layer follows naturally from the input-connected\ndesign and is not added as a separate modeling assumption. From an approximation-\ntheoretic point of view, IC\u2013MLPs therefore form a strict extension of classical multilayer\nperceptrons.\nIt is also worth noting that related ideas of bypassing intermediate layers have been\nexplored in other deep learning architectures, such as residual networks [30] and densely\nconnected convolutional networks [31]. In those models, skip connections transmit in-\nformation from earlier hidden layers to later hidden layers, while the raw input appears\nexplicitly only at the first layer and does not enter deeper layers through independent\naffine maps. In contrast, both ICNNs and IC\u2013MLPs include direct connections from the\nraw input itself to deeper layers. As discussed in [29], this distinction is essential for\npreserving convexity in ICNNs. In the present work, we show that even without any\nconvexity constraints, direct input connections lead to a feedforward architecture with a\nparticularly transparent and sharp universality theory.\n4\n2\nScalar IC\u2013MLP Architecture\nIn this section we describe IC\u2013MLP architecture in the special case of a scalar input.\nThroughout this section, the input variable is a real number x \u2208R, and \u03c3 : R \u2192R\ndenotes a fixed continuous activation function. Although this setting is simpler than the\nvector-input case, it already captures the essential structural features of IC\u2013MLPs and\nserves as a transparent model for the general theory developed later in this paper.\nFor completeness, we describe explicitly the network architectures with zero, one, two,\nthree, and L hidden layers. This layer-by-layer presentation is intended to clarify the\nrole of direct input connections at each depth and to highlight the structural differences\nbetween IC\u2013MLPs and classical feedforward networks.\nZero Hidden Layer (Depth 0). We begin with the simplest possible architecture,\nnamely a network with no hidden layers.\nIn this case, the input signal x is passed\ndirectly to the output neuron, which applies an affine transformation.\nThe resulting\nnetwork function has the form\nH0(x) = c x + d,\nwhere c, d \u2208R are parameters of the output neuron. Such a network computes only linear\nfunctions of the input and is therefore incapable of approximating genuinely nonlinear\nfunctions. This trivial case is included mainly for completeness and as a baseline for\ncomparison with deeper networks.\nSingle Hidden Layer (Depth 1). We next consider an IC\u2013MLP with a single\nhidden layer consisting of N neurons. The input neuron outputs the raw input value\nx. Each hidden neuron receives this input, applies an affine transformation, and then\napplies the activation function \u03c3. More precisely, the output of the i-th hidden neuron is\ngiven by\nh1,i = \u03c3(a1,ix + b1,i),\ni = 1, . . . , N,\nwhere a1,i, b1,i \u2208R are parameters of that neuron.\nThe output neuron then combines all hidden-layer outputs and, in addition, receives\nthe raw input x directly through an affine connection. The network output is therefore\nH1(x) =\nN\nX\ni=1\nvi h1,i + c x + d =\nN\nX\ni=1\nvi \u03c3(a1,ix + b1,i) + c x + d,\nwhere vi, c, d \u2208R are output-layer parameters. Compared to a classical single-hidden-\nlayer MLP, the distinctive feature here is the additional direct affine term c x + d at the\noutput.\nTwo Hidden Layers (Depth 2). We now move to an IC\u2013MLP with two hidden\nlayers. The first hidden layer has N1 neurons and is identical in structure to the single-\nhidden-layer case, with outputs\nh1,i = \u03c3(a1,ix + b1,i),\ni = 1, . . . , N1.\nThe second hidden layer consists of N2 neurons. Each neuron in this layer receives all\noutputs from the first hidden layer and, simultaneously, the raw input x through an affine\nconnection. The output of the j-th neuron in the second hidden layer is therefore\nh2,j = \u03c3\n\u0010 N1\nX\ni=1\nw2,ji h1,i + a2,jx + b2,j\n\u0011\n,\nj = 1, . . . , N2,\n5\nwhere w2,ji, a2,j, b2,j \u2208R are parameters.\nAs before, the output neuron combines all second-layer outputs and includes a direct\naffine dependence on x. The overall network function is thus\nH2(x) =\nN2\nX\nj=1\nvj \u03c3\n\u0010 N1\nX\ni=1\nw2,ji \u03c3(a1,ix + b1,i) + a2,jx + b2,j\n\u0011\n+ c x + d.\nThis expression already illustrates the characteristic feature of IC\u2013MLPs: at each depth,\nnonlinear transformations of earlier features are enriched by fresh affine information com-\ning directly from the input.\nThree Hidden Layers (Depth 3). The same construction extends naturally to\ndeeper architectures. This case is included solely to make the recursive structure explicit.\nFor three hidden layers with N1, N2, and N3 neurons, respectively, the first two layers\nare defined as above, while the third hidden layer produces outputs\nh3,k = \u03c3\n\u0010 N2\nX\nj=1\nw3,kj h2,j + a3,kx + b3,k\n\u0011\n,\nk = 1, . . . , N3.\nThe output neuron again aggregates all third-layer outputs together with a direct affine\ncontribution from x, yielding\nH3(x) =\nN3\nX\nk=1\nvk \u03c3\n\u0010 N2\nX\nj=1\nw3,kj \u03c3\n\u0010 N1\nX\ni=1\nw2,ji \u03c3(a1,ix+b1,i)+a2,jx+b2,j\n\u0011\n+a3,kx+b3,k\n\u0011\n+c x+d.\nGeneral L-Hidden-Layer Scalar IC\u2013MLP. In full generality, an IC\u2013MLP with L\nhidden layers is defined recursively. If the \u2113-th hidden layer contains N\u2113neurons, then\nthe output of the j-th neuron in that layer is given by\nh\u2113,j = \u03c3\n\u0010 N\u2113\u22121\nX\ni=1\nw\u2113,ji h\u2113\u22121,i + a\u2113,jx + b\u2113,j\n\u0011\n,\n\u2113= 1, . . . , L.\nHere, h0,1 = x is understood as the output of the input neuron. The final output of the\nnetwork is computed by\nHL(x) =\nNL\nX\nj=1\nvj hL,j + c x + d.\nEquivalently, the function realized by an L-hidden-layer scalar IC\u2013MLP can be written\nin the nested form\nHL(x) =\nNL\nX\nj=1\nvj \u03c3\n\u0010 NL\u22121\nX\ni=1\nwL,ji \u03c3\n\u0000\u00b7 \u00b7 \u00b7 \u03c3(a1,ix + b1,i) \u00b7 \u00b7 \u00b7\n\u0001\n+ aL,jx + bL,j\n\u0011\n+ c x + d.\nThis architecture is illustrated schematically in Figure 1.\nRemark. A standard feedforward scalar MLP is obtained as a special case of the scalar\nIC\u2013MLP architecture by suppressing all direct input connections beyond the first hidden\nlayer. In the present notation, this corresponds to imposing\na\u2113,j = 0\nfor all hidden layers \u2113\u22652 and all neurons j.\nUnder this restriction, only the first hidden layer receives the raw input, and the archi-\ntecture reduces exactly to a classical scalar MLP. Consequently, every function realizable\nby a standard scalar MLP is also realizable by a scalar IC\u2013MLP, while the converse does\nnot hold in general.\n6\nx\nh1,1\nh1,N1\nh2,1\nh2,N2\nhL,1\nhL,NL\nHL(x)\nHidden layer 1\nHidden layer 2 Hidden layer L\nFigure 1:\nScalar Input-Connected Multilayer Perceptron (IC\u2013MLP). The scalar input x\nis fed directly to every hidden layer and to the output neuron (dashed arrows), in addition\nto the standard feedforward connections between successive layers (solid arrows).\n3\nUniversal Approximation Theorem for Univariate\nFunctions\nIn this section, we establish a universal approximation theorem for IC\u2013MLPs in the\nunivariate setting. The theorem below characterizes precisely when deep input connected\nneural networks are capable of approximating arbitrary continuous functions on compact\nsets of the real line.\nTheorem 1. Let \u03c3 : R \u2192R be continuous. The following statements are equivalent:\n1. \u03c3 is nonlinear.\n2. For every closed interval [\u03b1, \u03b2] \u2282R, every continuous function f : [\u03b1, \u03b2] \u2192R,\nand every \u03b5 > 0, there exist an integer L \u22651 and parameters of an L-hidden-layer\nIC\u2013MLP such that the corresponding network function HL satisfies\nsup\nx\u2208[\u03b1,\u03b2]\n|f(x) \u2212HL(x)| < \u03b5.\nIn other words, deep IC\u2013MLP networks can approximate continuous functions on\ncompact sets of R if and only if the activation function \u03c3 is nonlinear.\nProof. Necessity. Assume that \u03c3 is linear, that is,\n\u03c3(t) = At + B.\nWe show that every IC\u2013MLP computes a linear function of the input x.\nThe input neuron outputs x, which is linear. Suppose that all neurons in some layer\ncompute linear functions of x. Each neuron in the subsequent layer evaluates\n\u03c3\nX\nk\nwkhk(x) + cx + d\n!\n,\nwhere hk(x) are linear. The argument of \u03c3 is therefore linear in x, and since \u03c3 is linear,\nthe output is again linear. By induction on the depth, every IC\u2013MLP realizes a linear\n7\nfunction.\nConsequently, no nonlinear continuous function can be approximated, and\nuniversality fails.\nSufficiency. Assume now that \u03c3 is continuous and nonlinear.\nLet F denote the set of all functions R \u2192R realizable by finite IC\u2013MLPs.\nBy\nconstruction, F contains linear functions, and is closed under linear combinations, affine\nchanges of variables, and superposition with \u03c3.\nSince \u03c3 is nonlinear on R, the function \u03c3(ax + b) is nonlinear for every a \u0338= 0 and\nb \u2208R. Indeed, if \u03c3(ax + b) were linear for some a \u0338= 0, then composing with the inverse\naffine map y 7\u2192(y \u2212b)/a would imply that \u03c3 itself is linear on R, a contradiction.\nLet \u03c6 \u2208C\u221e\nc (R) be a nonnegative function with\nR\nR \u03c6(y) dy = 1, and define \u03c6\u03b5(t) =\n\u03b5\u22121\u03c6(t/\u03b5). Fix some a \u0338= 0 and b \u2208R, and set\nS\u03b5(x) := (\u03c3(ax + b) \u2217\u03c6\u03b5)(x) =\nZ\nR\n\u03c3(a(x \u2212y) + b) \u03c6\u03b5(y) dy.\nWe claim that S\u03b5 \u2208F. Indeed, each function \u03c3(a(x \u2212y) + b) belongs to F, and finite\nlinear combinations of such functions are realizable by IC\u2013MLPs. Moreover, since \u03c6\u03b5 has\ncompact support, the integral defining S\u03b5 can be approximated uniformly on any compact\nset K \u2282R by Riemann sums of the form\nS\u03b5,n(x) =\nn\nX\nk=1\n\u03bbk \u03c3(a(x \u2212yk) + b),\nwhere \u03bbk \u2208R and yk are points in the support of \u03c6\u03b5. Each S\u03b5,n is a finite linear combi-\nnation of functions from F, and\nsup\nx\u2208K\n|S\u03b5,n(x) \u2212S\u03b5(x)| \u2212\u21920\nas n \u2192\u221e.\nThus S\u03b5 is the uniform limit on compact sets of such finite linear combinations, and\ntherefore lies in the closure of F in C(R) with the topology of uniform convergence on\ncompact sets.\nMoreover, S\u03b5 \u2208C\u221e(R) because it is the convolution of the function \u03c3(ax + b) with\nthe smooth mollifier \u03c6\u03b5. By the standard properties of mollifiers,\nS\u03b5(x) \u2192\u03c3(ax + b)\nuniformly on compact sets as \u03b5 \u21920.\nFor sufficiently small \u03b5, the function S\u03b5 is nonlinear. Indeed, suppose by contradiction\nthat there exists a sequence \u03b5n \u21920 such that S\u03b5n is linear for all n. Then the uniform\nlimit on compact sets of S\u03b5n would be linear, which would imply that \u03c3(ax + b) is linear\non R. This contradicts the nonlinearity of \u03c3. Hence, there exists \u03b50 > 0 such that S\u03b5 is\nnonlinear for all 0 < \u03b5 < \u03b50.\nFix one \u03b5 > 0 such that S := S\u03b5 is nonlinear.\nThen there exists x0 \u2208R with\nS\u2032\u2032(x0) \u0338= 0. For \u03b4 > 0 define the scaled symmetric second difference\nT\u03b4(x) := S(x0 + \u03b4x) + S(x0 \u2212\u03b4x) \u22122S(x0)\n\u03b42 S\u2032\u2032(x0)\n.\nLet us first prove that the function x 7\u2192S(x0 + \u03b4x) belongs to F. Since S \u2208F, there\nexists a sequence (Fn)n\u22651 \u2282F such that Fn \u2192S uniformly on every compact subset of\nR. Define\nGn(x) := Fn(x0 + \u03b4x),\nx \u2208R.\n8\nEach function Gn belongs to F, because if Fn is realized by an IC\u2013MLP with input x,\nthen replacing the input by the affine map x 7\u2192x0 + \u03b4x (i.e. feeding \u03b4x and adding the\nbias x0 through the direct input connection) yields an IC\u2013MLP realizing x 7\u2192Fn(x0+\u03b4x).\nLet K \u2282R be compact and set\nK\u2032 := x0 + \u03b4K = {x0 + \u03b4x : x \u2208K},\nwhich is also compact. Then\nsup\nx\u2208K\nGn(x) \u2212S(x0 + \u03b4x)\n= sup\nx\u2208K\nFn(x0 + \u03b4x) \u2212S(x0 + \u03b4x)\n\u2264sup\ny\u2208K\u2032 |Fn(y) \u2212S(y)|.\nSince Fn \u2192S uniformly on K\u2032, the right-hand side tends to 0 as n \u2192\u221e. Thus Gn \u2192\nS(x0 + \u03b4x) uniformly on K. Since K was arbitrary, this shows that S(x0 + \u03b4x) \u2208F. The\nsame argument applies to S(x0 \u2212\u03b4x).\nWe have therefore proved that both S(x0 + \u03b4x) and S(x0 \u2212\u03b4x) belong to F. Since\nF is closed under linear combinations and contains constants, it follows that T\u03b4 \u2208F for\nevery \u03b4 > 0.\nWe claim that T\u03b4 \u2192x2 uniformly on compact sets as \u03b4 \u21920. Fix M > 0 and assume\n|x| \u2264M. By Taylor\u2019s theorem with remainder (using S \u2208C3),\nS(x0 \u00b1 \u03b4x) = S(x0) \u00b1 \u03b4x S\u2032(x0) + \u03b42x2\n2\nS\u2032\u2032(x0) \u00b1 \u03b43x3\n6\nS(3)(\u03be\u00b1),\nfor some \u03be\u00b1 between x0 and x0 \u00b1 \u03b4x. Adding the + and \u2212expansions cancels the linear\nterms and yields\nS(x0 + \u03b4x) + S(x0 \u2212\u03b4x) \u22122S(x0) = \u03b42x2S\u2032\u2032(x0) + R\u03b4(x),\nwhere\n|R\u03b4(x)| \u2264\u03b43|x|3\n6\n\u0000|S(3)(\u03be+)| + |S(3)(\u03be\u2212)|\n\u0001\n\u2264\u03b43M 3\n3\nsup\n|t\u2212x0|\u2264\u03b4M\n|S(3)(t)|.\nTherefore\nsup\n|x|\u2264M\n|T\u03b4(x) \u2212x2| \u2264\n\u03b4M 3\n3|S\u2032\u2032(x0)|\nsup\n|t\u2212x0|\u2264\u03b4M\n|S(3)(t)| \u2212\u21920\n(\u03b4 \u21920),\nwhich proves T\u03b4 \u2192x2 uniformly on [\u2212M, M]. Hence x2 \u2208F.\nSince the function x2 belongs to the closure of F, for any w in the closure, the\ncomposition\nw2 = x2 \u25e6w\nalso belongs to the closure. Indeed, for n \u2208N set Kn := [\u2212n, n]. Since w is continuous,\nthe set w(Kn) is compact. Since x2 \u2208F, we can choose Pn \u2208F such that\nsup\ny\u2208w(Kn)\n|Pn(y) \u2212y2| < 1\n2n.\nBy continuity of Pn, the function Pn is uniformly continuous on the compact set\nJn := {y \u2208R : dist(y, w(Kn)) \u22641}.\n9\nChoose \u03b4n > 0 with \u03b4n \u22641 so that\n|Pn(y) \u2212Pn(z)| < 1\n2n\nwhenever y, z \u2208Jn and |y \u2212z| < \u03b4n.\nSince w \u2208F, choose wn \u2208F such that\nsup\nx\u2208Kn\n|wn(x) \u2212w(x)| < \u03b4n.\nDefine\nQn := Pn \u25e6wn.\nThen Qn \u2208F. Moreover, for x \u2208Kn we have w(x) \u2208w(Kn) \u2282Jn and\ndist\n\u0000wn(x), w(Kn)\n\u0001\n\u2264|wn(x) \u2212w(x)| < \u03b4n \u22641,\nhence wn(x) \u2208Jn. Therefore,\n|Qn(x) \u2212w(x)2| = |Pn(wn(x)) \u2212w(x)2|\n\u2264|Pn(wn(x)) \u2212Pn(w(x))| + |Pn(w(x)) \u2212w(x)2|\n< 1\n2n + 1\n2n = 1\nn,\nand thus\nsup\nx\u2208Kn\n|Qn(x) \u2212w(x)2| < 1\nn.\nFinally, let K \u2282R be any compact set. Then K \u2282KN for some N \u2208N. For all\nn \u2265N we have K \u2282Kn, so\nsup\nx\u2208K\n|Qn(x) \u2212w(x)2| \u2264sup\nx\u2208Kn\n|Qn(x) \u2212w(x)2| < 1\nn \u2212\u2212\u2212\u2192\nn\u2192\u221e0.\nThus Qn \u2192w2 uniformly on K. Since K was arbitrary, Qn \u2192w2 uniformly on compact\nsets, i.e. w2 \u2208F.\nNext, for any u, v in the closure, their product can be expressed as\nuv = 1\n2\nh\n(u + v)2 \u2212u2 \u2212v2i\n,\nshowing that the closure is closed under pointwise multiplication. Iterating this argument,\nit follows that all monomials xn and hence all polynomials in x are contained in the closure\nof F.\nBy the Weierstrass approximation theorem, polynomials are dense in C([\u03b1, \u03b2]) for any\nclosed interval [\u03b1, \u03b2] \u2282R. Therefore, for any f \u2208C([\u03b1, \u03b2]) and \u03b5 > 0, there exists an\nIC\u2013MLP function HL such that\nsup\nx\u2208[\u03b1,\u03b2]\n|f(x) \u2212HL(x)| < \u03b5.\nThis completes the proof of the theorem.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "The paper introduces the Input-Connected Multilayer Perceptron (IC\u2013MLP), a variant of feedforward neural networks where each hidden neuron receives both its layer's outputs and a direct affine connection from the raw input. The study begins with scalar inputs, proving that deep IC\u2013MLPs can approximate any continuous function on a compact interval if and only if the activation function is nonlinear. This theorem generalizes to vector-valued inputs, establishing that IC\u2013MLPs can also approximate continuous functions on compact subsets of Rn.\n\nKey contributions include:\n1. **Universal Approximation Theorem**: Proves deep IC\u2013MLPs can approximate any continuous function on a compact interval or subset if the activation function is nonlinear.\n2. **Generalization to Vector Inputs**: Extends the results from scalar inputs to vector-valued inputs, showing that the theorem holds for functions defined over compact subsets of Rn.\n\nThe methodology involves:\n1. **Analyzing Scalar Case**: Detailed study of IC\u2013MLPs with zero, one, two, and L hidden layers.\n2. **Universal Approximation Theorem Proof**: Demonstrates that deep IC\u2013MLPs can approximate any continuous function if the activation function is nonlinear by constructing a sequence of functions realizable by such networks.\n\nThe main findings are:\n1. Deep IC\u2013MLPs with nonlinear activation functions can approximate any continuous function on compact sets.\n2. This property holds for both scalar and vector inputs, broadening the applicability of the theorem.\n\nConclusions emphasize that this work provides a comprehensive understanding of deep neural network architectures and their ability to approximate complex functions, particularly in non-linear scenarios. The results have implications for various applications requiring accurate function approximation within compact domains."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Centre for Cognitive Science (January 21, 2026)\n1\nMooneyMaker: A Python package\nto create ambiguous two-tone im-\nages\nLars C. Reining (corresp.)\n, Technical University of Darmstadt\nlars.reining@stud.tu-darmstadt.de\nThabo Matthies, Technical University of Darmstadt\nLuisa Haussner, Technical University of Darmstadt\nRabea Turon, Technical University of Darmstadt\nThomas S. A. Wallis , Technical University of Darmstadt; Center for Mind,\nBrain and Behavior (CMBB) Universities of Marburg, Giessen and Darm-\nstadt\nKeywords: vision, perception, perceptual organisation, Mooney Images\nMooney images are high-contrast, two-tone visual stimuli, created by thresholding photographic\nimages. They allow researchers to separate image content from image understanding, making\nthem valuable for studying visual perception. An ideal Mooney image for this purpose achieves\na specific balance: it initially appears unrecognizable but becomes fully interpretable to the ob-\nserver after seeing the original template. Researchers traditionally created these stimuli manually\nusing subjective criteria, which is labor-intensive and can introduce inconsistencies across studies.\nAutomated generation techniques now offer an alternative to this manual approach. Here, we\npresent MooneyMaker, an open-source Python package that automates the generation of am-\nbiguous Mooney images using several complementary approaches. Users can choose between\nvarious generation techniques that range from approaches based on image statistics to deep\nlearning models. These models strategically alter edge information to increase initial ambiguity.\nThe package lets users create two-tone images with multiple methods and directly compare the\nresults visually. In an experiment, we validate MooneyMaker by generating Mooney images using\ndifferent techniques and assess their recognizability for human observers before and after disam-\nbiguating them by presenting the template images. Our results reveal that techniques with lower\ninitial recognizability are associated with higher post-template recognition (i.e. a larger disam-\nbiguation effect). To help vision scientists build effective databases of Mooney stimuli, we provide\npractical guidelines for technique selection. By standardizing the generation process, Mooney-\nMaker supports more consistent and reproducible visual perception research.\n1\nIntroduction\nMooney images are defined as highly degraded two-tone images where each pixel is either\nblack or white, removing all texture and much edge information (see Fig. 1 for an example). Be-\ncause so much information is lost in the generation process, Mooney images are often initially un-\nrecognizable to human observers. This is because it is hard to tell whether an edge present in the\nMooney image corresponds to an object boundary or is just the result of a shadow or illumination\nchange in the template image (Hegd\u00b4e & Kersten, 2010; Moore & Cavanagh, 1998).\nThis makes Mooney images a useful tool in human vision science. For example, they can be\nused to study perceptual closure (Gr\u00a8utzner et al., 2010; Mooney, 1957; Verhallen & Mollon, 2016),\nholistic processing (Canas-Bajo & Whitney, 2020; Latinus & Taylor, 2005), object recognition (Dolan\net al., 1997; Imamoglu et al., 2012; Teufel et al., 2018) or face recognition (Canas-Bajo & Whitney,\n2022; Peterson et al., 2023; Schwiedrzik et al., 2018). Additionally, Mooney images can also serve\npractical purposes such as discriminating between humans and machines in CAPTCHA systems (Li\n& Wei, 2024) or user authentication (Castelluccia et al., 2017).\narXiv:2601.14077v1  [q-bio.NC]  20 Jan 2026\nCentre for Cognitive Science (January 21, 2026)\n2\nMooney images are also used to study the effects of top down processing, because to be able\nto recognize the content of many Mooney images, observers have to rely on prior knowledge\nor other forms of top-down processing, and to integrate information over larger spatial regions of\nthe image. This means that often, a Mooney image is only interpretable once the observer has\nbeen provided with additional information, e.g., by showing the original template image before\nthe Mooney image (Hegd\u00b4e & Kersten, 2010; Teufel et al., 2018).\nThe ideal Mooney image allows researchers to separate image content from image under-\nstanding (Teufel et al., 2018). In other words, the same Mooney image can produce very different\npercepts depending on whether the observer has prior knowledge about its content or not. This\nproperty is interesting for vision science research as it allows to investigate how prior knowledge\ninfluences visual perception (Dolan et al., 1997; Hegd\u00b4e & Kersten, 2010; Teufel et al., 2018). Thus,\nan ideal Mooney image can be defined by achieving a specific balance: it should be initially un-\nrecognizable to observers but become fully interpretable after additional information is provided.\nWhile Mooney images have been widely used, they come with the disadvantage of being\nmanually generated. In the study that introduced Mooney images, Mooney (1957) himself hand-\ncolored image regions black and white to create the images. Nowadays, researchers typically\ncreate Mooney images by first smoothing a grayscale image and then applying a global thresh-\nold for binarization (Reining & Wallis, 2024; Schwiedrzik et al., 2018). Smoothing is typically done by\napplying a Gaussian filter and is used to remove high-frequency details and noise from the image.\nDuring thresholding, the intensity of each pixel (often in the 8-bit range between 0 and 255) is com-\npared to a predefined threshold value. Pixels with intensities above the threshold are set to white\n(255), while those below are set to black (0). Previously, we have shown that different parameters\nduring smoothing and thresholding can have a significant impact on the interpretability of Mooney\nimages (Reining & Wallis, 2024). This is because depending on the choice of these parameters, in-\nformation relevant for interpretation can either be preserved or lost. This is especially the case if,\ndue to the choice of parameters, edges are distorted which would normally provide important\ncues like closure for object recognition (Teufel et al., 2018).\nAs the amount of smoothing and the threshold are typically selected manually and on an\nimage-by-image basis, creating Mooney images is not only time-consuming for large datasets but\nalso introduces variability and potential bias into the generation process. Different researchers\nmight choose different parameters based on their subjective judgment, leading to inconsistencies\nacross studies and laboratories.\nTo mitigate the impact of these issues, we previously investigated techniques to automatically\ncreate Mooney images (Reining & Wallis, 2024). For this, we chose predefined smoothing amounts\nand selected thresholds using modern image thresholding techniques. We found that the amount\nof smoothing has a larger impact on the interpretability of Mooney images than the choice of\nthresholding technique.\nIn this paper, we build upon these findings by developing a set of new Mooney image gener-\nation techniques that optimize over both the amount of smoothing and the selected threshold to\ncreate Mooney images which are either maximally ambiguous or maximally recognizable. This is\nachieved by using an optimization procedure that compares the edge maps of the template im-\nage and the generated Mooney image using different edge detectors. This decision is motivated\nby the fact that edges are crucial for object recognition (Biederman, 1987) and that, in Mooney\nimages, edge information is distorted as described above (Hegd\u00b4e & Kersten, 2010).\nWe then either minimize or maximize the difference between the edge map of the template and\nthe Mooney image by varying the amount of smoothing and the selected threshold. We evaluate\nthese techniques in a psychophysical experiment where human participants had to interpret the\ngenerated Mooney images before and after being provided with the template image as a cue.\nOur results show that the techniques designed to increase the initial ambiguity of Mooney images\nalso lead to a larger increase in interpretability after disambiguation by revealing the template.\nAll the techniques we developed in this and in our previous study are now part of MooneyMaker,\nthe Python package we introduce here. We provide an easy-to-use implementation of all tech-\nniques to generate Mooney images with the goal of enhancing reproducibility and comparability\nacross studies.\nCentre for Cognitive Science (January 21, 2026)\n3\n2\nThe MooneyMaker package\n2.1\nAPI overview\nThe package offers four core functions. The main function is generate mooney image, which takes\na template image and a selected Mooney image generation technique (described below) as in-\nput and returns and saves the generated Mooney image in the specified output directory. Be-\nfore the Mooney image generation process, the function automatically converts the input im-\nage to grayscale and resizes it so that no dimension exceeds 1024 pixels, while maintaining the\noriginal aspect ratio. The function also allows users to specify parameters such as the size of the\nGaussian smoothing kernel or other technique dependent parameters. Additionally, the function\nconvert folder to mooney images allows users to convert all images in a specified folder to Mooney\nimages and store them in an output folder. As described above, users can select the Mooney\nimage generation technique and specify technique dependent parameters.\nThe function plot technique comparison takes a template image and generates Mooney im-\nages using all available techniques, displaying them side-by-side for comparison. This function is\nuseful for visualizing the differences between the various Mooney image generation methods.\nFinally, the package includes the function get edge prediction. Users can use this function to\nobtain edge maps of images using the different edge detection algorithms (Canny, TEED, Diffu-\nsionEdge) used in the package. This function is particularly useful for understanding how different\nedge detection methods label the edges in both template and Mooney images.\n2.2\nMooney image generation techniques\nConsider once more that the high-level goal of MooneyMaker is to allow researchers to gen-\nerate two-tone images that appear ambiguous to humans, but can be readily understood once\na template image is shown. To generate ambiguous two-tone images using global thresholding\ntechniques, we hypothesize that one could find smoothing kernel sizes and thresholds that maxi-\nmally disrupt the edges of the template image. In other words, we are seeking a two-tone image\nwhose contours are unlike those of the template image. (Of course, this also runs the risk of being\ntoo ambiguous, producing images that are unrecognizable even after the template is shown).\nTo do this algorithmically, the approach we adopt here is to first detect edges in the template\nimage, then measure the edges in an initial two-tone image, then optimize the thresholding to\nmaximize the difference between the template and the two-tone (i.e. to disrupt the edges). As\ncomparison conditions to compare using our behavioural experiments, we also include similarity\ntechniques, in which the objective function is reversed and the optimization process finds thresholds\nto maximize the similarity.\nAltogether, MooneyMaker supports nine different techniques for generating Mooney images\nfrom template images. Six of these techniques are first introduced here and are optimization-based\ntechniques that either maximize or minimize edge similarity between the Mooney image and the\ntemplate image. The remaining three techniques are the ones described in our previous work\n(Reining & Wallis, 2024).\nIn the following, we describe all nine Mooney image generation techniques implemented in\nthe MooneyMaker package. Table 1 provides an overview of the six novel techniques introduced\nhere.\n2.2.1\nTechniques from Reining and Wallis (2024)\nAll three techniques described in our previous work (Reining & Wallis, 2024) use a fixed size of the\nGaussian smoothing kernel and differ only in the method used to select the binarization threshold.\nThe size of the smoothing kernel is preselected to be 15x15 pixel but can also be adjusted manually\nby the user. The size of the smoothing kernel k relates to the standard deviation \u03c3 of the Gaussian\naccording to \u03c3 = 0.3 \u00b7 ((k \u22121) \u00b7 0.5 \u22121) + 0.8. Thus, a kernel size of 15x15 pixels corresponds to a\nstandard deviation of \u03c3 = 2.6 pixels. This preselected standard deviation lies within the range of\nvalues used in previous studies (Ke et al., 2017; Reining & Wallis, 2024).\nCentre for Cognitive Science (January 21, 2026)\n4\nTechnique\nEdge Detection Method\nOptimization Objective\nCannyEdgeSimilarity\nCanny (Canny, 1986)\nMinimize L(ET , EM)\nCannyEdgeDisruption\nCanny (Canny, 1986)\nMaximize L(ET , EM) \u2212RM\nTEEDEdgeSimilarity\nTEED (Soria et al., 2023)\nMinimize L(ET , EM)\nTEEDEdgeDisruption\nTEED (Soria et al., 2023)\nMaximize L(ET , EM) \u2212RM\nDiffusionEdgeSimilarity\nDiffusionEdge (Ye et al., 2024)\nMinimize L(ET , EM)\nDiffusionEdgeDisruption\nDiffusionEdge (Ye et al., 2024)\nMaximize L(ET , EM) \u2212RM\nTable 1: Overview of the six novel Mooney image generation techniques introduced here. The techniques differ in the\nedge detection method used to compute the edge maps of the Mooney and template images, the optimization objective\n(similarity vs. disruption of the template image and Mooney image edge map), and whether regularization is applied. The\nexact formulation of the loss and regularization can be found in Equation 1 and Equation 2.\nMean\nThis technique uses a fixed size of the Gaussian smoothing kernel and selects the mean pixel inten-\nsity of the smoothed image as the binarization threshold.\nOtsu\nThis technique also uses a fixed size of the Gaussian smoothing kernel but selects the binarization\nthreshold using Otsu\u2019s method (Otsu, 1979). Otsu\u2019s method determines an optimal binarization\nthreshold that minimizes the intra-class variance of the black and white pixel classes.\nCannyMaxEdge\nThis technique uses the classical Canny edge detector (Canny, 1986) to find the threshold which\nmaximizes the number of detected edges in the Mooney image. This non-neural technique is\ndescribed in more detail in Reining and Wallis (2024). Users can manually adjust the size of the\nsmoothing kernel and the standard deviation of the Canny edge detector. While a small standard\ndeviation of the Canny edge detector will result in fine-grained edge maps, a larger standard de-\nviation will highlight more coarse edges. The default value is 1 pixel. The technique optimizes for\nthe maximum number of edges in the Mooney image.\n2.2.2\nNovel techniques\nThe following six techniques are first introduced here. All six techniques use optimization to select\nthe size of the Gaussian smoothing kernel and the binarization threshold that either maximizes or\nminimizes edge similarity between the Mooney image and the template image. The techniques\ndiffer only in the edge detection algorithm used to compute the edge maps of the Mooney and\ntemplate images. An overview of all six techniques is provided in Table 1.\nThe optimization is performed using a coarse-to-fine grid search over a predefined range of\nsmoothing kernel sizes and binarization thresholds. The range from which the size of the smoothing\nkernel is selected is 11x11 pixels to 39x39 pixels. This preselected range aligns with ranges of values\nused in previous studies (Ke et al., 2017; Reining & Wallis, 2024). The range from which the binariza-\ntion threshold is selected is an intensity between 20 and 190. Since each image has intensity values\nin the 8-bit range from 0 to 255, this range excludes very low and very high thresholds that would\nlead to mostly white or black Mooney images and reduces the size of the search space.\nThe techniques described in the following optimize for either similarity or disruption of edges\nbetween the Mooney image and the template image, using a modified version of the Hausdorff\ndistance (Dubuisson & Jain, 1994). This objective function measures the similarity between the edge\nmap of a Mooney image and the edge map of the template image and can be described by:\nL(ET , EM) = d(ET , EM) + d(EM, ET )\n2\n,\nwhere\nd(A, B) = 1\n|A|\nX\na\u2208A\nmin\nb\u2208B \u2225a \u2212b\u22252\n(1)\nHere, ET and EM are sets of edge pixels in the template and Mooney image respectively, and \u2225\u00b7\u22252\nis the Euclidean distance between two pixels. The function d(A, B) computes the average distance\nfrom each pixel in set A to the closest pixel in set B. The overall loss L(ET , EM) is the average of the\nCentre for Cognitive Science (January 21, 2026)\n5\ndistances computed in both directions, providing a symmetric measure of dissimilarity between the\ntwo edge maps. Since the measure of similarity is symmetric, it not only rewards edges detected in\nthe template image that are preserved in the Mooney image but also penalizes edges that were\ndetected in the Mooney image but not in the template image. The edge maps are computed\nusing a technique specific edge detection algorithm.\nOf course, a maximally-disruptive solution would be to make all pixels black or white (i.e. setting\nvery high or low thresholds). Because we want images that can be disambiguated after seeing\nthe template, this solution would be detrimental. We therefore subtract a regularization term from\nthe loss in the disruption case. The Mooney image specific regularization term is defined as:\nRM = \u03bbM \u00b7\nlog\n\u0012|WM|\n|BM|\n\u0013\f\f\f\f , where\n\u03bbM = L(ET , EM) \u2212minm\u2208M L(ET , Em)\n2\n(2)\nHere, |WM| and |BM| are the number of white and black pixels in the Mooney image respectively.\nIn other words, the regularization penalty is higher as the ratio of black to white pixels diverges from\none. In addition, we set the regularization weight \u03bbM as proportional to the distance of the current\nloss to the minimum loss over all Mooney images for this template image (M) generated during the\ngrid search. This ensures that the regularization has a stronger effect when the current loss is high\nand a weaker effect when the current loss is relatively low.\nThe six techniques differ in the edge detection algorithm used to compute the edge maps of\nthe Mooney and template images and whether they optimize for similarity or disruption of edges.\nExample edge maps computed with the different edge detection methods are shown in Fig. 2.\nCannyEdgeSimilarity\nThis technique uses the classical Canny edge detector (Canny, 1986) to compute the edge maps\nof the Mooney and template images. The Canny edge detector is a non-neural technique that\ndetects all low-level edges in the image. The technique optimizes for similarity of edges between\nthe Mooney image and the template image by minimizing the loss L(ET , EM).\nCannyEdgeDisruption\nThis technique also uses the Canny edge detector (Canny, 1986) to compute the edge maps of\nthe Mooney and template images. However, this technique optimizes for disruption of edges be-\ntween the Mooney image and the template image by maximizing the loss L(ET , EM) \u2212RM.\nTEEDEdgeSimilarity\nThis technique uses the Tiny and Efficient Model for the Edge Detection Generalization (TEED) (Soria\net al., 2023) to compute the edge maps of the Mooney and template images. TEED is a neural\nnetwork-based edge detection method that was trained on natural images and is able to detect\nmore semantically meaningful edges. This means, detected edges are sparser and less texture-like\ncompared to the Canny edge detector. The technique optimizes for similarity of edges between\nthe Mooney image and the template image by minimizing the loss L(ET , EM).\nTEEDEdgeDisruption\nThis technique also uses the TEED (Soria et al., 2023) to compute the edge maps of the Mooney\nand template images. It optimizes for disruption of edges between the Mooney image and the\ntemplate image by maximizing the loss L(ET , EM) \u2212RM.\nDiffusionEdgeSimilarity\nThis technique uses the DiffusionEdge model (Ye et al., 2024) to compute the edge maps of the\nMooney and template images. DiffusionEdge is a state-of-the-art diffusion based model that cap-\ntures object-level edges and contours in images. The technique optimizes for similarity of edges\nbetween the Mooney image and the template image by minimizing the loss L(ET , EM).\nDiffusionEdgeDisruption\nThis technique also uses the DiffusionEdge model (Ye et al., 2024) to compute the edge maps of\nthe Mooney and template images. It optimizes for disruption of edges between the Mooney image\nCentre for Cognitive Science (January 21, 2026)\n6\nMean\nTemplate\nCannyEdge\nDi\ufb00usionEdge\nDisruption\nDisruption\nFigure 1: Example stimuli from the three categories (object, natural scene, face) and their corresponding Mooney images\ngenerated with the five selected techniques (Mean, CannyEdgeSimilarity, CannyEdgeDisruption, DiffusionEdgeSimilarity,\nDiffusionEdgeDisruption). The image in the top row was taken from the THINGS dataset (Hebart et al., 2019). The image\nin the second row is from the SUN2012 dataset (Xiao et al., 2010). We took the picture in the third row ourselves, and the\ndepicted person has certified their consent.\nand the template image by maximizing the loss L(ET , EM) \u2212RM.\n2.3\nAvailability and documentation\nThe MooneyMaker Python package is open source (MIT License) and freely available on pypi\n(https://pypi.org/project/mooney-maker/) and GitHub (https://doi.org/10.5281/zenodo.18300248).\nThe repository contains detailed documentation on how to install and use the package, including\nexample notebooks demonstrating its functionality.\n3\nMethods\n3.1\nParticipants\nOverall, 30 students of TU Darmstadt took part in the experiment. 12 of them were female and\n18 male. All of them were naive observers who were not familiar with the purpose of the study.\nThey all had normal or corrected-to-normal vision. All participants certified their informed consent\nand were rewarded with one participation credit for a course assignment. All protocols conformed\nto Standard 8 of the American Psychological Association\u2019s Ethical Principles of Psychologists and\nCode of Conduct (2017) and to the Declaration of Helsinki (with the exception of Article 35 con-\ncerning preregistration in a public database). The experiments reported here were approved by\nthe Technical University of Darmstadt Ethics Commission (Application number EK 77/2022).\n3.2\nStimuli\n3.2.1\nSelection of template images\nAs our goal was to test the applicability of the MooneyMaker package to generate Mooney\nimages for different types of images, we selected object, natural scenes and face images as tem-\nCentre for Cognitive Science (January 21, 2026)\n7\nTemplate\nCanny\nTEED\nDi\ufb00usionEdge\nFigure 2: Example edge maps of the different edge detection methods. Shown are edge maps of the same image com-\nputed using the three different edge detection methods used in the MooneyMaker package: Canny (Canny, 1986), TEED\n(Soria et al., 2023), and DiffusionEdge (Ye et al., 2024). It is easily visible that the different methods capture different kinds of\nedges in the image. While Canny captures all low-level edges, TEED captures sparser and less texture-like edges, and Diffu-\nsionEdge captures mostly object-level edges and contours. The shown images are taken from the THINGS dataset (Hebart\net al., 2019), which is shared under a CC-BY license.\nplate images. Overall, we used 54 template images, which results in 18 images per category. An\nexample of each category with its corresponding Mooney versions is shown in Fig. 1. Within each\ncategory, we also balanced the level of clutter present in each image. As this was not of primary\ninterest for this study, further results on different clutter levels will not be reported here.\nImages of objects were selected from the THINGS dataset (Hebart et al., 2019). An image was\ncategorized as \u201cobject\u201d when the object was clearly and prominently depicted, either as the only\nobject in the scene, the largest and most central item, or shown repeatedly to dominate the frame.\nWe preselected images based on this definition, prioritizing objects that are generally recognizable\nacross individuals.\nNatural scene images were selected from the SUN2012 dataset (Xiao et al., 2010) and contained\na diverse range of environments, including both natural landscapes (e.g. forests, mountains) and\nman-made settings (e.g. urban streets, indoor spaces). Unlike object images, which feature a\nclearly defined item, the selected natural scene images depict broader, more complex environ-\nments composed of multiple elements. For example, an image of a caf\u00b4e might include chairs,\ntables, a counter, and background details, making the identification of any single object more\ndifficult and less intuitive. This complexity is characteristic of natural scenes and aligns with their\nintended role in this study.\nFinding an existing dataset suitable for the face category in this study proved challenging, as we\nnot only required high-quality (1000 \u00d7 1000 pixels or higher) images of faces, but also a diverse set of\nbackgrounds and lighting conditions to improve generalizability. Additionally, we required images\nwith clearly identifiable emotional expressions, as the experiment involved an emotion recogni-\ntion task. However, most available datasets with emotion labels are typically available only un-\nder standardized conditions, often with uniform backgrounds and controlled lighting. To ensure\ngreater variability, we therefore created the face stimuli ourselves using an iPhone 14 (iPhone 14\nmain camera; 26 mm, f/1.5, 9 MP; 3024 \u00d7 3024 px; 1:1 aspect ratio). A diverse group of individu-\nals, aged between 18 and 75 years, agreed to participate as models and we obtained permission\nfrom all of them for the use of their images. Each person appears only once in the final dataset, dis-\nplaying a distinct emotional expression: happy, neutral, angry, sad or surprised. These expressions\nare based on the basic emotion framework proposed by Ekman (1992), with a neutral expression\nadded and certain categories omitted. To ensure sufficient variability, the selection included close-\nup portraits as well as images showing the upper body and larger portions of the background (the\nmain source of clutter in face stimuli). We aimed for diversity in perspective and lighting conditions,\nincorporating frontal and side views as well as different illumination settings, with the overall goal\nCentre for Cognitive Science (January 21, 2026)\n8\nFixation cross\nStimuli presentation\n500ms\n1500ms\nTask 1\nTextbox\nTask 2\nEmotion task\nDifficulty rating\nTask 3\nFigure 3: Procedure of one trial in one of the two Mooney phases. Each trial consisted of two to three tasks. First, in the\ninterpretation task, participants were shown a Mooney image and then presented with an open-response text box in which\nthey typed what they saw in the image. If their response indicated that they saw a face, human or person, the emotion\nrecognition task followed, in which they selected one of five emotional expressions (happy, neutral, angry, sad, surprised).\nIf the response did not indicate a face, the emotion recognition task was skipped. Finally, participants were asked to give\na subjective rating about the difficulty of interpreting the Mooney image on a scale from 1 (very easy) to 5 (very difficult).\nEach participant completed two Mooney phases, one before and one after the template presentation phase (not shown\nhere).\nof representing faces in natural rather than posed environments. We included faces with different\nemotional expressions to evaluate human emotion recognition in Mooney images. Thus, we added\nan additional task: participants had to identify the emotion displayed by each face. However, the\ndependence on facial emotion expression is not of primary interest here and will not be discussed\nfurther.\n3.2.2\nMooney image generation\nWe generated Mooney images from each of the 54 templates using five different generation\ntechniques from the MooneyMaker package. As a baseline, we included the Mean technique with\na fixed Gaussian smoothing kernel size of 15x15 pixels. In addition, we selected four optimization-\nbased techniques. We choose CannyEdgeSimilarity and DiffusionEdgeSimilarity to represent tech-\nniques that optimize for similarity of edges between the Mooney image and the template image.\nWe selected CannyEdgeDisruption and DiffusionEdgeDisruption to represent techniques that opti-\nmize for disruption of edges between the Mooney image and the template image. We decided\nto include both Canny-based and DiffusionEdge-based techniques to compare a classic, non-\nneural edge detection method with a state-of-the-art diffusion based edge and contour detection\nmodel. All techniques are described in detail in section 2.\nThus, in total, we generated 270 Mooney images (54 template images \u00d7 5 techniques). All stimuli\nmeasured 1000 \u00d7 1000 pixels, which, given the experimental setup, corresponded to approximately\n12.7 \u00d7 12.7 degrees of visual angle.\n3.3\nProcedure and design\nIn this experiment we investigated five different Mooney image generation techniques and their\ninfluence on Mooney image disambiguation after template presentation.\nThe experiment consisted of three phases: two Mooney phases and one template presentation\n(disambiguation) phase in between. The two Mooney phases are also referred to as pre-template\nand post-template Mooney phase. In each Mooney phase, each participant saw 54 Mooney\nimages, one for each template image. The Mooney images were generated using one of the\nfive techniques, randomly assigned to each image for each participant. Thus, no participant saw\nCentre for Cognitive Science (January 21, 2026)\n9\nmultiple Mooney images created from the same template image using different techniques. The\norder of presentation of the Mooney images was randomized for each participant in each Mooney\nphase.\nEach trial in the Mooney phases followed the procedure shown in Fig. 3. First, a fixation cross\nwas presented for 500 milliseconds to center the gaze of the participants. Then, the Mooney image\nwas presented for 1500 milliseconds. This duration is the same time as used by Reining and Wallis\n(2024) and Teufel et al. (2018) and should be sufficient to interpret the black and white patches\nif possible. Immediately afterwards, participants were presented with an open-response text box.\nThey were instructed to type what they saw in the image (interpretation task) by typing one to\ntwo words. If their response indicated that they saw a \u201cface\u201d, \u201chuman\u201d or \u201cperson\u201d, they were\npresented with an additional emotion recognition task, in which they selected one of five emotional\nexpressions (happy, neutral, angry, sad, surprised). If the response did not indicate a face, the\nemotion recognition task was skipped. Finally, participants were asked to give a subjective rating\nabout the difficulty of interpreting the Mooney image on a scale from 1 (very easy) to 5 (very\ndifficult). After this, the next trial of the Mooney phase began. At no point during the experiment did\nparticipants receive feedback on their responses. After completing all 54 trials in the first Mooney\nphase, participants proceeded to the template presentation phase.\nThe goal of the template presentation phase was to establish a visual connection between the\nMooney images and their corresponding template images to allow for disambiguation. To achieve\nthis, the procedure used methods previously used in the study by Teufel et al. (2018). In each\ntrial of the disambiguation phase, the Mooney image and its original template image gradually\nfaded into and out of each other twice, with each transition lasting three seconds. To ensure that\nparticipants maintained their attention throughout the passive viewing, a gray circle appeared\nat a random location on the image between one and four seconds after the fading sequence\nended. The timing of this appearance followed a normal distribution to reduce predictability and\nprevent participants from anticipating the circle\u2019s appearance. Participants were instructed to\nclick on the circle to proceed to the next trial.\nFollowing the disambiguation phase, the experiment returned to a second Mooney phase (post-\ntemplate). The same Mooney images from the pre-template presentation were shown again, but\nin a newly randomized order. Aside from this reordering, the task remained identical. This design\nallowed for a direct comparison of perception before and after template presentation, enabling\nthe investigation of how prior exposure to the template influenced interpretation of the Mooney\nimages.\nParticipants had the opportunity to take short breaks between the three phases of the experi-\nment. The entire experiment lasted approximately 45 minutes per participant, excluding instructions\nand consent procedures.\n3.4\nEquipment\nThe code for the experiment and a analysis was implemented in Python (version 3.10.18) using\nPsychoPy (v2025.1.1). The stimuli were generated using our own MooneyMaker package.\nThe stimuli were presented on a ROG Swift OLED PG27AQDM monitor, connected to a computer\nrunning Ubuntu 22.04. The monitor had a spatial resolution of 3,840 \u00d7 2,160 pixels and a refresh rate\nof 240 Hz. A chin rest ensured a consistent viewing distance of 70 cm. The study took place in a\ndarkened laboratory at TU Darmstadt.\n3.5\nData analysis\n3.5.1\nPost-Processing of open responses\nTo evaluate the open responses from the interpretation task, we first had to post-process and\nclean the participants answers. Participants (all native German speakers) were instructed to re-\nspond in English but were allowed to respond in their German native language if they were not\nable to recall specific words. We translated all answers to English and corrected spelling mistakes.\nFurthermore, if multiple words were given by the participant, we tried to extract the most relevant\nword. This post-processing was done using Google\u2019s Gemini 1.5 Flash (Google, 2025). The used\nprompt is shown in Appendix A. We closely reviewed the post-processed answers to ensure that\nCentre for Cognitive Science (January 21, 2026)\n10\nthe meaning of the original response was preserved.\nIn the next step, we post-processed the \u201cground-truth\u201d labels of the template stimuli. The la-\nbeling process of the template stimuli varied depending on the dataset: images from the THINGS\ndataset (Hebart et al., 2019) already contained high-quality annotations, while self-created face\nimages were uni- formly labeled as \u201cface.\u201d For the SUN2012 dataset (Xiao et al., 2010), some original\nlabels were slightly simplified (e.g .\u201caccess road\u201d was simplified to \u201croad\u201d). Additionally, in cases\nwhere a large number of identical responses (at least 6) were provided by the participants and\nthese responses clearly and correctly described the image content but deviated from the label\n(e.g. \u201dlogs\u201d instead of \u201dwood\u201d), we added these as alternative labels for the respective images."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Academic Paper\n\n**Title:** MooneyMaker: A Python Package to Create Ambiguous Two-Tone Images\n\n**Authors:** Lars C. Reining, Thabo Matthies, Luisa Haussner, Rabea Turon, Thomas S. A. Ulrich, and Center for Mind, Brain and Behavior (CMBB) Universities of Marburg, Giessen, and Darmstadt.\n\n**Publication Date:** January 21, 2026\n\n**Keywords:** vision, perception, perceptual organization, Mooney Images\n\n#### Main Contributions\nMooneyMaker is an open-source Python package that automates the generation of ambiguous two-tone images (Mooney images). These images are created by thresholding photographic images and removing all texture and much edge information. The key contribution lies in its ability to generate these images using several complementary approaches, including image statistics-based methods and deep learning models.\n\n**Methodology**\nThe package supports nine different techniques for generating Mooney images from template images. Six of these techniques are optimization-based, aiming either to maximize or minimize the similarity between the Mooney image and the template image's edge maps. The remaining three techniques were previously described in a related study by Reining and Wallis (2024). \n\n**Key Findings**\n- Techniques with lower initial recognizability of Mooney images are associated with higher post-template recognition (i.e., larger disambiguation effect).\n- Visual comparison of different generation techniques reveals that some methods produce more ambiguous images initially but can be better understood after seeing the template image.\n- The package includes a function to plot technique comparisons and another to obtain edge maps for further analysis.\n\n**Conclusions**\nMooneyMaker supports more consistent and reproducible visual perception research. By standardizing the generation process, it helps researchers build effective databases of Mooney stimuli. Practical guidelines are provided for selecting techniques based on their performance in disambiguating images. The package is freely available online and can be used to enhance the reliability and comparability across studies.\n\n### Summary Breakdown\n\n#### Introduction\n- **Definition:** Mooney images are highly degraded two-tone images where each pixel is either black or white, removing all texture and much edge information.\n- **Purpose:** Useful for studying human vision science, including perceptual closure, holistic processing, object recognition, and face recognition. Also used to study effects of top-down processing.\n\n#### Methodology\n- **Package Overview:** MooneyMaker includes four core functions: generating Mooney images, comparing techniques side-by-side, obtaining edge maps, and visualizing results.\n- **Techniques:** Nine different methods are implemented, including optimization-based approaches that either maximize or minimize edge similarity between the Mooney image and template image's edge maps. Three additional techniques from previous work were also included.\n\n#### Key Findings\n- Techniques with lower initial recognizability of Mooney images show a larger increase in post-template recognition.\n- Visual comparison of different methods reveals varying levels of ambiguity initially but better interpretability after disambiguation.\n- The package includes functions for generating and visualizing Mooney images, as well as obtaining edge maps.\n\n#### Conclusions\n- **Reproducibility:** Standardizes the generation process to support more consistent and reproducible research in vision science.\n- **Guidelines:** Provides practical guidelines for selecting techniques based on their performance in disambiguating images.\n- **Availability:** The package is freely available online, making it accessible for researchers.\n\n### Conclusion\nMooneyMaker offers a comprehensive solution for generating Mooney images that are both ambiguous and interpretable. By automating the process and providing standardized methods, it facilitates more reliable and comparable research across studies."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution\nSamuel W. Remedios\nJohns Hopkins University\nBaltimore, MD, USA\nsamuel.remedios@jhu.edu\nZhangxing Bian\nJohns Hopkins University\nBaltimore, MD, USA\nShuwen Wei\nJohns Hopkins University\nBaltimore, MD, USA\nAaron Carass\nJohns Hopkins University\nBaltimore, MD, USA\nJerry L. Prince\nJohns Hopkins University\nBaltimore, MD, USA\nBlake E. Dewey\nJohns Hopkins University\nBaltimore, MD, USA\nFigure 1. Overview of our proposed MISR generalization to diffusion super-resolution. Our method generalizes existing methods for\nmulti-image super-resolution (MISR). The first column shows low-resolution (LR) anisotropic MRI measurements with 16\u00d7 downsampling\nalong orthogonal planes. For super-resolution results, the first row shows the existing methods for single-image super-resolution (SISR),\nwhich only use the LR measurement enclosed in the blue dotted box. The second row shows results for our proposed generalization of each\nmethod for MISR, which use both LR measurements enclosed in the pink solid box. All methods used the same 3D volumetric diffusion\nmodel, and all results are 3D volumetric images. A single sagittal slice is shown for viewing clarity. Volumetric PSNR and SSIM are overlaid\nin the yellow box, and zoomed inset regions are shown in red. MISR improves performance for all methods and better estimates the anatomy\nof the reference image, shown on the far right.\nAbstract\nDiffusion models are the current state-of-the-art for solving\ninverse problems in imaging. Their impressive generative\ncapability allows them to approximate sampling from a prior\ndistribution, which alongside a known likelihood function\npermits posterior sampling without retraining the model.\nWhile recent methods have made strides in advancing the ac-\ncuracy of posterior sampling, the majority focuses on single-\nimage inverse problems. However, for modalities such as\nmagnetic resonance imaging (MRI), it is common to acquire\nmultiple complementary measurements, each low-resolution\nalong a different axis. In this work, we generalize common\ndiffusion-based inverse single-image problem solvers for\nmulti-image super-resolution (MISR) MRI. We show that the\nDPS likelihood correction allows an exactly-separable gra-\ndient decomposition across independently acquired measure-\nments, enabling MISR without constructing a joint operator,\nmodifying the diffusion model, or increasing network func-\ntion evaluations. We derive MISR versions of DPS, DMAP,\nDPPS, and diffusion-based PnP/ADMM, and demonstrate\nsubstantial gains over SISR across 4\u00d7/8\u00d7/16\u00d7 anisotropic\ndegradations. Our results achieve state-of-the-art super-\nresolution of anisotropic MRI volumes and, critically, en-\n1\narXiv:2601.14030v1  [cs.CV]  20 Jan 2026\nable reconstruction of near-isotropic anatomy from routine\n2D multi-slice acquisitions, which are otherwise highly de-\ngraded in orthogonal views.\n1. Introduction\nSuper-resolution (SR) is a linear inverse problem that esti-\nmates a candidate high-resolution (HR) image given an ob-\nserved low-resolution (LR) image [2, 5, 9, 11, 17, 18, 20, 23\u2013\n25, 27, 31, 35, 37, 41\u201343, 47, 48, 59, 61\u201367, 69\u201371]. Math-\nematically, SR uses the observed LR image y \u2208Rm (the\nmeasurement) to estimate the HR image x \u2208Rn given the\nforward model\ny = Ax + \u03f5,\n(1)\nwhere A \u2208Rm\u00d7n is the linear forward operator and\n\u03f5 \u223cN(0, \u03c3Im). Since m < n, Eq. 1 is underdetermined\nand noisy, multiple solutions exist; the problem is ill-posed.\nSome form of regularization is required to select a solution.\nWhile classical approaches use hand-crafted regular-\nization priors such as total variation [61, 70] or spar-\nsity under some transform [6, 21, 49, 51], contemporary\ndata-driven approaches use denoisers as a plug-and-play\nprior [12, 32, 33, 55, 60]. This practice began with generic\ndenoising algorithms [8, 46, 58] but has recently found\ndeep theoretic connections with the family of diffusion\nmodels, including denoising diffusion probabilistic mod-\nels (DDPMs) [29], score-based models [56], flow mod-\nels [26, 40], and Schr\u00a8odinger bridges [19, 52]. These ap-\nproaches learn a generative model that transforms samples\nfrom one distribution (often a high-dimensional standard\nGaussian) to the data distribution. In this way, diffusion\nmodels act as a Bayesian prior on the desired data distribu-\ntion p(x). Accordingly, inverse problem solving algorithms\nleverage Bayes\u2019 rule to sample from the posterior:\np(x|y) \u221dp(y|x)p(x),\n(2)\nwhere the likelihood term p(y|x) is approximated by the\nknown measurement function Eq. 1.\nThis insight has spurred great developments in the use of\ndiffusion models for inverse problem solving. The recipe for\nsolving any inverse problem with Eq. 2 requires only three\ningredients: a known forward operator A, a pretrained diffu-\nsion model to approximate p(x), and a technique to sample\nfrom the posterior p(y|x)p(x). Most works assume that A\nis known, assume the distribution of \u03f5 is known, and use\nan existing well-trained diffusion model. Thus, algorithm\ndevelopment focuses on the sampling procedure. Diffusion\nPosterior Sampling (DPS) [15], which corrects samples on\nthe diffusion trajectory towards the data-consistent manifold\nvia a gradient update, is possibly the most well-known.\nMost work on SR has focused on single-measurement in-\nverse problem solving. However, in problem settings such as\nmagnetic resonance imaging (MRI), multiple measurements\nare common. For example, an MRI session may include\nmultiple scans along orthogonal axes. These occur for a\nmultitude of reasons, such as improving signal-to-noise ra-\ntio, mitigating motion artifacts, or due to special MRI pulse\nsequences designed for slice-wise acquisition. Such sessions\noften result in multiple image volumes that are HR along\ntwo axes (called in-plane) but LR along the third axis (the\nthrough-plane). These anisotropic image volumes are faster\nto acquire, but by trading signal-to-noise (SNR) and in-plane\nresolution for a degraded through-plane resolution, the image\nvolumes are not clinically useful when viewed through-plane.\nRadiologists therefore do not read anisotropic volumes in\nthe through-planes, as important anatomical structures are\nblurred and/or decimated.\nTo leverage the complementary information available\nin this real-world setting, we propose to generalize diffu-\nsion inverse problem solving approaches for the multi-image\nsuper-resolution (MISR) problem. In this paper, we show\nthat diffusion models can accurately estimate the underlying\nisotropic anatomy without retraining for specific resolutions.\nTo our knowledge, no prior work in MRI or diffusion inverse\nproblems has shown that multiple anisotropic volumes can\nbe restored to isotropy. This positions MISR diffusion as a\npractical, high-impact alternative to long 3D MRI scans.\nOur main contributions are as follows:\n\u2022 Likelihood separability for diffusion posterior sam-\npling.\nWe show that the likelihood gradient decom-\nposes exactly across independent measurements, permit-\nting MISR correction using only per-measurement op-\nerators. This enables multi-image posterior correction\nwithout a joint operator or modification of the diffusion\nmodel.\n\u2022 A unified MISR generalization of diffusion inverse\nsolvers. We derive MISR versions of DPS, DMAP, DPPS,\nand diffusion-based PnP/ADMM.\n\u2022 Noise weighting for heterogeneous MRI acquisi-\ntions. We introduce inverse-variance weighting of per-\nmeasurement gradients, enabling principled fusion of mea-\nsurements with different slice thicknesses, SNRs, and spa-\ntial resolutions.\n\u2022 State-of-the-art MISR MRI performance.\nAcross\n4 \u00d7 /8 \u00d7 /16\u00d7 through-plane degradations, our MISR ex-\ntensions substantially outperform their single-image coun-\nterparts, improving PSNR by 1 to 3 dB while producing\nanatomically faithful reconstructions. Evaluations were\nconducted with 64 network function evaluations (NFEs)\nfor the reverse diffusion process, yielding a total super-\nresolution time per volume of less than 60 seconds on an\nNVIDIA RTX 6000 ADA.\nWhen taken together, our contributions establish diffusion\nmodels as a practical, flexible, and high-quality approach for\nmulti-image MRI super-resolution without retraining.\n2\n2. Background\n2.1. Diffusion models\nDiffusion-based models are the current state of the art in\ngenerative modeling [7]. They include the denoising dif-\nfusion probabilistic model (DDPM) [29], the denoising\ndiffusion implicit model (DDIM) [54], score-based mod-\nels [14, 15], Schr\u00a8odinger bridges [10, 52, 57], and Brownian\nbridges [13, 34, 36]. There are also connections between\ndiffusion-based models and flow matching [26, 38, 39].\nDiffusion models establish a Markov chain of transition\nstates between the data distribution and a noise distribution,\ntypically a high-dimensional Gaussian. By convention, we\nwill denote a state at the data distribution by x0 and a state\nat the noise distribution by x1. Thus, arbitrary transition\nstates are denoted by xt for t \u2208[0, 1]. A state xs is closer to\nthe data distribution than xt whenever s < t. The forward\ndiffusion process describes how data diffuses into noise and\nis of closed form. The reverse diffusion process describes\nhow to denoise each transition state and is parameterized by\na neural network. The goal in training a diffusion model is\nto find parameters \u03b8 that allow the neural network f\u03b8(xt, t)\nto move from transition state xt at time t to a less-noisy state\nxs. Once trained, at inference time, a sample x1 is drawn\nand successively transformed through a series of transition\nstates until arriving at the data state x0.\nThe forward diffusion process defines xt as a linear com-\nbination between data and noise:\nxt = \u03b1tx0 + \u03b2t\u03f5,\n(3)\nwhere \u03b1t and \u03b2t are parameters for the noise schedule and\n\u03f5 \u223cN(0, I) by definition. The reverse diffusion process is\nthe following:\nxs = \u03b1s\u02c6x0 + \u03b2s\u02c6\u03f5,\n(4)\nwhere \u02c6\u03f5 = (xt \u2212\u03b1t\u02c6x0)/\u03b2t and \u02c6x0 is estimated by the dif-\nfusion network f\u03b8. Equation 4 is the DDIM sampler [53],\nwhich generalizes the DDPM framework [29] and allows\naccelerated and deterministic sampling. The full reverse\ndiffusion process arrives at t = 0 by iteratively applying\nEq. 4.\nWhile the methods presented in this paper are agnostic\nto the specific way \u02c6x0 is obtained (sample estimation, noise\nestimation, velocity estimation, etc. [26]), our work will\nspecifically make use of the flow estimation framework [26,\n38, 39]. That is, the network f\u03b8 estimates the flow-matching\nfield: \u02c6u = \u02c6\u03f5 \u2212\u02c6x0 = f\u03b8(xt, t), so\n\u02c6x0 = \u02c6\u03f5 \u2212f\u03b8(xt, t)\n(5)\nand\n\u02c6\u03f5 = f\u03b8(xt, t) + \u02c6x0.\n(6)\nDue to the empirical success of diffusion models, much\ntheoretical work has been done to connect schools of thought.\nThere are now many leading perspectives on diffusion mod-\nels, including the score, variational, flow-matching, and\nSchr\u00a8odinger bridge perspectives.\n2.2. 2D Magnetic Resonance Image Acquisition\nIn MRI, image volumes are rarely obtained alone. Usually,\nimaging sessions acquire multiple images of different con-\ntrasts, orientations, and resolutions. We will consider the\ncommon scenario of an MRI session where the same pulse\nsequence is used to acquire multiple images along orthogo-\nnal axes, each potentially with different slice thicknesses. In\nthis case, image volumes are formed by 2D acquisition (also\ncalled 2D MRI or 2D multi-slice MRI). In 2D MRI acquisi-\ntions, samples are acquired in 2D k-space after exciting a slab\nwithin the field of view using slice selection [4]. Different\nregions are selected by varying the slice selection parameters,\nresulting in a set of independently acquired slices. Each of\nthese slices is converted to the image domain with the inverse\nFourier transform and then stacked along the through-plane\naxis to create a 3D volume.\nSlice selection determines the through-plane resolution.\nThe physics of slice selection allow us to write the relation-\nship between HR and LR images along one dimension. In\nEq. 1, A can be written as a strided convolution using the\nslice selection profile and the separation between slices in\nmillimeters. In practice, the slice selection profile is propri-\netary and its closed-form equation is unknown. However,\nit is known that a Gaussian kernel serves as an reliable ap-\nproximation. In this work, we assume that the slice selection\nuses Gaussian profiles with slice separations and full-with-\nat-half-maxes equal to the scale factor.\n2.3. Diffusion posterior sampling for SISR\nDiffusion Posterior Sampling (DPS)[15] uses Jensen\u2019s ap-\nproximation for the likelihood term in Eq. 2: p(y | xt) \u2248\np(y | E[x0|xt]).\nThis is convenient, since computing\nE[x0|xt] is a conventional step in the reverse diffusion pro-\ncess (see Eq. 5). With this, DPS adjusts samples xs in the\ndiffusion process towards the data-consistent manifold:\nxs \u2212\u2207xt\u2225AE[x0|xt] \u2212y\u2225.\n(7)\nThe algorithm for implementing DPS is shown in Algo-\nrithm 1. It should be noted that DPS does not require the\nforward operator to be linear, only differentiable.\nMany methods have built on this initial formulation from\nDPS. We defer the reader to [16] for further reading, but\nbriefly summarize a few methods here. The denoising diffu-\nsion null-space model (DDNM) does a state-wise correction\nwith a hard projection for linear forward operators. Diffusion\nplug-and-play image restoration (DiffPIR) uses the proxi-\nmal algorithm half-quadratic splitting to update diffusion\nstates to be proximal to the observation. Diffusion posterior\nproximal sampling (DPPS) samples multiple candidates at\n3\neach state xt and selects the one with the smallest distance\nto the data-consistent manifold. Diffusion Maximum A Pos-\nteriori (DMAP) performs multiple sampling steps to better\napproximate a MAP sample.\n3. Method\nOur objective is to generalize diffusion priors to the multi-\nimage super-resolution (MISR) problem. We first formalize\nthe MISR observation model and derive a separable likeli-\nhood expression that permits multi-image data consistency\nwithout constructing a joint operator. We then analyze the\nimplications of this decomposition for DPS and describe\nMISR extensions of several existing solvers, including DPS,\nDMAP, DPPS, and diffusion-based PnP methods.\n3.1. MISR Preliminaries\nWe seek a single HR image x \u2208Rn that is consistent with\nmultiple LR measurements {yi \u2208Rmi}N\ni=1. This yields a\nset of equations:\nyi = Aix + \u03f5i,\n(8)\nwhere Ai \u2208Rmi\u00d7n and \u03f5i \u223cN(0, \u03c3iImi). Since all opera-\ntions are linear, it is possible to rewrite Eq. 8 as\ny = Ax + \u03f5,\n(9)\nwhere y =\n\u0002y1\n. . .\nyN\n\u0003\u22a4, A =\n\u0002A1\n. . .\nAN\n\u0003\u22a4, and\n\u03f5 =\n\u0002\u03f51\n. . .\n\u03f5N\n\u0003\u22a4. Although A is mathematically well-\ndefined, in MRI each Ai may operate on different grids\nwith different slice geometries, making explicit stacking\ncomputationally impractical.\n3.2. Likelihood separability in DPS\nThe joint negative log-likelihood of Eq. 8 is\nL(x) =\nN\nX\ni=1\n1\n2\u03c32\ni\n\u2225Aix \u2212yi\u22252\n2,\n(10)\nwhich is separable across observations.\nProposition 1. Let \u00b50(xt) = E[x0 | xt] be the diffusion\nmodel\u2019s estimation of the data sample from time t. Then, the\nlikelihood gradient to correct xt towards the data consistent\nmanifold is\n\u2207xtL(\u00b50(xt)) =\nN\nX\ni=1\n1\n\u03c32\ni\n\u2207xt\u2225Ai\u00b50(xt) \u2212yi\u22252\n2.\n(11)\nIn particular, each Ai contributes an independent correction\ndirection, and the combined gradient is their sum.\nProof. By independence of measurements,\nL(\u00b50) =\nN\nX\ni=1\nLi(\u00b50).\n(12)\nSince the diffusion network f\u03b8 is used only to generate\n\u00b50(xt) and the measurement operators Ai are linear and\ndo not couple measurements,\n\u2207xtL(\u00b50(xt)) =\nN\nX\ni=1\n\u2207xtLi(\u00b50(xt)).\n(13)\nWith substitution,\n\u2207xtL(\u00b50(xt)) =\nN\nX\ni=1\n1\n\u03c32\ni\n\u2207xt\u2225Ai\u00b50(xt) \u2212yi\u22252\n2\n(14)\nRemark.\nProposition 1 guarantees that MISR diffu-\nsion posterior sampling requires only independent per-\nmeasurement gradients without requiring a joint operator,\nmodification of the diffusion model, nor an increase in NFEs.\nThe nonlinearity of \u00b50 does not break linear separability\nbecause dependence on \u00b50 flows through the chain rule.\nThe separability of likelihoods under DPS has several\nimplications. First, as we will demonstrate in this work,\nwe can extend several approaches in the DPS family for\nMISR. Second, likelihood separability can generalize to any\ncomponents that are independent in A, not just multiple\nobservations. For example, in the SISR framework, we\ncould split A into independent rows and consider different\nregions of images as separate measurements. This extends to\nfurther norm-preserving linear maps, and so block-stacked\nwavelet transform matrices could allow splitting by wavelet\nsubband. We leave this exploration for future work. Finally,\nlikelihood separability allows for independent weights for\neach component. With even weights, all measurements are\nconsidered equally. However, some measurements may be\nless reliable, for example, with more noise than others.\nAlgorithm 1 DPS\nRequire: T, A, y, \u03b6t\n1: x1 = N(0, I)\n2: for t = 1 to 0 with T steps do\n3:\nxs \u223cp\u03b8(Xs | xt)\n4:\n\u00b50 := E[X0 | xt]\n5:\nxs = xs \u2212\u03b6t\u2207xt\u2225A\u00b50 \u2212y\u2225\n6: end for\n7: return x0\nMeasurement noise weighting\nEach measurement\nmay have its own noise strength; i.e., \u03c3i \u0338= \u03c3j for i \u0338= j. In\nsuch scenarios, it is disadvantageous to correct states towards\nall observations equally. Thus, we propose to inversely scale\nthe gradients by the strength of the noise for that observation.\n4\nAlgorithm 2 MISR DPS\nRequire: T, {Ai}N\ni=1, {yi}N\ni=1, {\u03c3i}N\ni=1, \u03b6t\n1: x1 = N(0, I)\n2: wi := (\u03c32\ni + \u03c3floor)\u22121\n3: wi := N \u00b7 wi/(PN\nj=1 wj)\n4: for t = 1 to 0 with T steps do\n5:\nxs \u223cp\u03b8(Xs | xt)\n6:\n\u00b50 := E[X0 | xt]\n7:\nxs = xs \u2212\u03b6t\u2207xt\nPN\ni=1\n\u0002\nwi \u00b7 \u2225Ai\u00b50 \u2212yi\u22252\n2\n\u0003\n8: end for\n9: return x0\n3.3. MISR inverse diffusion methods\nThe previous sections provided a framework for MISR that\nis applicable to existing approaches. Here, we outline how\nthese methods can be modified for noise-weighted MISR.\nMISR DPS\nWe describe MISR DPS in Algo-\nrithm 2.\nThe gradient update term is weighted by\nwi: \u2207xt\nPN\ni=1\n\u0002\nwi \u00b7 \u2225Aix0|t \u2212yi\u22252\n2\n\u0003\n, where wi \u221d(\u03c32\ni +\n\u03c3floor)\u22121 and \u03c3floor is a noise floor that the diffusion model\nf\u03b8 is robust to.\nMISR DMAP\nDMAP uses multiple samples and gradi-\nent steps to refine the approximation of state s from state t.\nSince it is algorithmically similar to DPS (compare DPS in\nAlgorithm 1 to DMAP in Algorithm 3), we are also able to\ngeneralize DMAP for MISR, shown in Algorithm 4.\nMISR DPPS\nTo implement MISR for DDPS, the se-\nlected candidate must have the smallest distance to all data-\nconsistent manifolds.\nMISR PnP\nPlug-and-play (PnP) methods that rely on\npriors are also amenable to MISR. The alternating direction\nmethod of multipliers (ADMM) uses variable splitting to\noptimize for data-consistency and prior-consistency [3]. The\ndata-consistency term is still written as the squared \u21132 norm,\nso Eq. 11 is still applicable. The same logic applies to\nDiffPIR, with half-quadratic splitting instead of ADMM.\nAlgorithm 3 DMAP\nRequire: T, K, A, y, \u03b6t, d\n1: x1 = N(0, I)\n2: for t = 1 to 0 with T steps do\n3:\nxs \u223cp\u03b8(Xs | xt)\n4:\n\u00b5s = E[Xs | xt]\n5:\nfor j = 1 to K do\n6:\n\u00b50 := E[X0 | xs]\n7:\nxs = xs \u2212\u03b6t\u2207xs\u2225A\u00b50 \u2212y\u2225\n8:\nxs = \u00b5s \u2212\n\u221a\nd\u03c3t\nxs\u2212\u00b5s\n\u2225xs\u2212\u00b5s\u22252\n2\n9:\nend for\n10: end for\n11: return x0\nAlgorithm 4 MISR DMAP\nRequire: T, K, {Ai}N\ni=1, {yi}N\ni=1, {\u03c3i}N\ni=1, \u03b6t, d\n1: x1 = N(0, I)\n2: wi := (\u03c32\ni + \u03c3floor)\u22121\n3: wi := N \u00b7 wi/(PN\nj=1 wj)\n4: for t = 1 to 0 with T steps do\n5:\nxs \u223cp\u03b8(Xs | xt)\n6:\n\u00b5s = E[Xs | xt]\n7:\nfor j = 1 to K do\n8:\n\u00b50 := E[X0 | xs]\n9:\nxs = xs \u2212\u03b6t\u2207xs\nPN\ni=1\n\u0002\nwi \u00b7 \u2225Ai\u00b50 \u2212yi\u22252\n2\n\u0003\n10:\nxs = \u00b5s \u2212\n\u221a\nd\u03c3t\nxs\u2212\u00b5s\n\u2225xs\u2212\u00b5s\u22252\n2\n11:\nend for\n12: end for\n13: return x0\n4. Experiments and Results\nOur experiments focus on super-resolution for anisotropic\nmagnetic resonance head images. We consider images that\nare 4\u00d7, 8\u00d7, and 16\u00d7 worse resolution along through-plane\naxes than in-plane axes. For HR image volumes of 1 mm3\nisotropic resolution, these scale factors correspond to 4,\n8, and 16 mm slice thicknesses. To enable quantitative\nreference-based distortion metrics, we perform simulations\nof data using Gaussian slice selection profiles with full-\nwidth-at-half-max and slice separation equal to the scale\nfactor. Following Eq. 8, we also add Gaussian noise with\n\u03c3i proportional to the voxel size due to the physics of how\nnoise interacts with slice selection [44]. Thicker slices have\nless noise. Specifically, we chose \u03c3i = 0.1/ki, where ki is\nthe scale factor for the LR image volume yi.\nWe used an open-source pre-trained non-latent 3D dif-\nfusion model for brain MRI [45]. This model is a flow-\nestimation model, and we used 64 DDIM steps for all\nmethods and all experiments. This model was trained on\nmore than 70, 000 T1-weighted brain volumes from 36 open\ndatasets, and additionally withheld two sites from training:\nAIBL [22] and SLEEP [68]. Thus, to avoid model bias,\nwe selected 50 subjects with 1 mm3 resolution from each\nof those two sites are our data cohort. For preprocessing,\nthese image volumes were padded and/or cropped to a voxel\nsize of 192 \u00d7 224 \u00d7 192 using the center-of-mass location\nfrom an HD-BET [30]-calculated brain mask, then linearly\nnormalized to the intensity range [\u22121, 1] using the volume\u2019s\nminimum and maximum values.\nWe conducted two experiments. First, we compared\nSISR to MISR with orthogonal planes. Then, we perform\nan ablation study and compare noise-weighting to uniform\nweighting in MISR DPS and MISR DMAP. For evalua-\ntion, we used PSNR, SSIM, and when comparing SISR\nto MISR we additionally calculated the Frechet Inception\n5\nFigure 2. Qualitative results for a representative subject from the AIBL dataset. Row-wise labels designate LR inputs as \u201cMeasurements\u201d\nand SR estimations named by method. Supercolumns group scale factors together. Within supercolumns, each column corresponds to SISR\nusing only the axial acquisition as input (blue), then MISR using axial and coronal (pink), and MISR using axial, coronal, and sagittal (green).\nThe HR reference is shown at the bottom of the figure. For all methods, the same area highlighting the folia of the cerebellum is zoomed into\nthe inset.\nDistance (FID) [28, 50]. To enable the computation of FID\nfor 3D volumetric images, we extract 2D slices from each\nvolume from all three cardinal planes, separated by 4 mm,\nand spanning the center 128 mm of the volume. These slices\nwere linearly normalized to the intensity range [0, 255] using\nthe volume\u2019s minimum and maximum values, quantized to\n8-bit integers, and saved to disk as .png files. The refer-\nence dataset for the FID computations for the AIBL cohort\nwas the SLEEP cohort, and vice-versa. This was done to\nprevent an abnormally low FID score due to similar intensity\nstatistics between, for example, true-HR-AIBL and super-\nresolved-AIBL images.\n4.1. SISR vs MISR\nIn Fig. 1 and Fig. 2, we show qualitative comparisons be-\ntween slices of representative volumes for SISR and MISR\nacross several methods. We emphasize that all methods op-\nerate directly in 3D, and 2D slices are shown for clarity. For\nMISR scenarios, there are more LR images to use as input,\nleading to improved results for all methods and scale factors.\nIn Fig. 3, we show four more representative subjects at 8\u00d7\nscale factor. With a second observation, these regions have\nmore correct anatomy. However, the distortion metrics do\nnot reflect geometric or anatomical errors; the approximately\n2 to 3 dB difference in PSNR is not sufficient to describe the\nimprovements gained from another measurement.\n6\nTable 1. Quantitative results for 100 subjects. For PSNR and SSIM, the mean\u00b1std. are reported. FID is computed as mentioned in Sec. 4 Best results by scale factor are bolded and\nsecond-best results are underlined.\nSISR\nMISR\nAxial\nAxial & Coronal\nAxial & Coronal & Sagittal\nScale\nMethod\nPSNR (\u2191)\nSSIM (\u2191)\nFID (\u2193)\nPSNR (\u2191)\nSSIM (\u2191)\nFID (\u2193)\nPSNR (\u2191)\nSSIM (\u2191)\nFID (\u2193)\n4\u00d7\nPnP-ADMM\n31.39 \u00b1 1.42\n0.880 \u00b1 0.014\n98.65\n33.30 \u00b1 1.11\n0.883 \u00b1 0.009\n84.97\n34.17 \u00b1 0.91\n0.884 \u00b1 0.008\n85.38\nDiffPIR\n27.92 \u00b1 1.26\n0.888 \u00b1 0.014\n65.44\n30.23 \u00b1 1.19\n0.895 \u00b1 0.009\n58.15\n32.14 \u00b1 1.25\n0.896 \u00b1 0.007\n53.97\nDPS\n32.78 \u00b1 2.03\n0.918 \u00b1 0.017\n41.28\n34.59 \u00b1 1.73\n0.928 \u00b1 0.012\n47.50\n35.52 \u00b1 1.44\n0.927 \u00b1 0.009\n51.76\nDPPS\n32.76 \u00b1 2.04\n0.918 \u00b1 0.017\n41.19\n34.59 \u00b1 1.74\n0.928 \u00b1 0.012\n47.30\n35.54 \u00b1 1.42\n0.927 \u00b1 0.009\n51.83\nDMAP\n33.08 \u00b1 2.14\n0.929 \u00b1 0.018\n32.07\n34.97 \u00b1 1.91\n0.940 \u00b1 0.014\n34.77\n35.98 \u00b1 1.64\n0.940 \u00b1 0.012\n37.18\n8\u00d7\nPnP-ADMM\n28.70 \u00b1 1.50\n0.844 \u00b1 0.027\n158.53\n31.51 \u00b1 1.59\n0.914 \u00b1 0.017\n85.09\n33.27 \u00b1 1.67\n0.936 \u00b1 0.013\n83.50\nDiffPIR\n25.18 \u00b1 1.29\n0.857 \u00b1 0.024\n91.66\n26.91 \u00b1 1.25\n0.898 \u00b1 0.016\n78.43\n27.82 \u00b1 1.30\n0.910 \u00b1 0.013\n70.85\nDPS\n30.04 \u00b1 2.11\n0.878 \u00b1 0.027\n39.58\n32.21 \u00b1 2.07\n0.922 \u00b1 0.019\n36.25\n33.65 \u00b1 2.13\n0.940 \u00b1 0.016\n34.38\nDPPS\n30.00 \u00b1 2.12\n0.877 \u00b1 0.027\n39.03\n32.18 \u00b1 2.08\n0.922 \u00b1 0.019\n35.84\n33.64 \u00b1 2.15\n0.940 \u00b1 0.016\n34.26\nDMAP\n30.36 \u00b1 2.16\n0.888 \u00b1 0.026\n38.97\n32.63 \u00b1 2.13\n0.931 \u00b1 0.019\n35.51\n34.17 \u00b1 2.22\n0.947 \u00b1 0.016\n32.66\n16\u00d7\nPnP-ADMM\n25.82 \u00b1 1.33\n0.751 \u00b1 0.037\n217.75\n28.53 \u00b1 1.62\n0.843 \u00b1 0.028\n126.23\n29.98 \u00b1 1.75\n0.881 \u00b1 0.024\n110.77\nDiffPIR\n23.07 \u00b1 1.21\n0.793 \u00b1 0.031\n116.23\n24.68 \u00b1 1.19\n0.856 \u00b1 0.022\n96.28\n25.46 \u00b1 1.20\n0.882 \u00b1 0.019\n85.93\nDPS\n27.56 \u00b1 1.95\n0.811 \u00b1 0.036\n44.61\n29.26 \u00b1 1.99\n0.859 \u00b1 0.028\n43.93\n30.50 \u00b1 2.08\n0.888 \u00b1 0.024\n42.75\nDPPS\n27.47 \u00b1 1.94\n0.810 \u00b1 0.037\n43.98\n29.25 \u00b1 2.01\n0.859 \u00b1 0.029\n42.99\n30.47 \u00b1 2.08\n0.888 \u00b1 0.024\n41.96\nDMAP\n28.04 \u00b1 2.03\n0.826 \u00b1 0.036\n44.43\n29.87 \u00b1 2.03\n0.877 \u00b1 0.026\n43.50\n31.25 \u00b1 2.14\n0.905 \u00b1 0.023\n41.89\n7\nFigure 3. Sagittal slices from four representative subjects are shown for 8\u00d7 scale factor LR inputs. Each displayed image is labeled for\nits contents. LR: low-resolution; AX: axial acquisition; COR: coronal acquisition; HR: ground-truth high-resolution; SISR: single-image\nsuper-resolution, using the axial acquisition as input; MISR: multi-image super-resolution, using the axial and coronal acquisitions as input.\nZoomed inset regions are located by the red box, highlighting anatomical differences that are not correctly recovered by SISR.\nWe show quantitative results for our 100 subjects in Ta-\nble 1. In correspondence with the qualitative results from\nFig. 2, DMAP outperforms all other methods across the\nboard, both in distortion metrics and FID. An interesting\nresult in Table 1 is the metrics\u2019 trends when adding more LR\nimages. As expected, PSNR and SSIM improve in all cases.\nHowever, for the 4\u00d7 scale factor, the FID score generally\nworsens for the DPS family of methods as more images are\nadded. This is potentially due to the perception-distortion\ntradeoff [1]; since the SR image \u02c6x is guided towards being\nconsistent with multiple images, its freedom in being a real-\nistic image is restricted. However, this phenomenon is not\npresent at 8\u00d7 and 16\u00d7 scale factors.\n4.2. Ablation: noise-weighting\nIn the previous experiment, each of the LR images used for\nMISR had the same scale factor. The noise present in the im-\nages had the same power, so the noise weights wi were equal\nfor all i and essentially absorbed into the gradient scalar\n\u03b6t. To evaluate the efficacy of the proposed noise weight-\ning, we compared with and without noise-weighting for\nmixed-resolution inputs: 4\u00d7 and 16\u00d7 scale factors for two\northogonal inputs. We used MISR DPS and MISR DMAP\nfor this evaluation and varied the noise by \u03c3i = \u03c3base/ki,\nfor \u03c3base = 0.15, 0.3, 0.45. These values correspond to ap-\nproximately 1\u00d7, 2\u00d7, and 3\u00d7 the average intensity standard\ndeviation in HR images (after normalization to [\u22121, 1]).\nIn Table 2, we show the results of this experiment. For\nboth MISR DPS and MISR DMAP, noise-weighting im-\nproves all metrics. As the noise level becomes more severe,\nthe effect of noise weighting becomes larger.\nTable 2. Quantitative results for our ablation without noise weight-\ning (w/o NW) and with noise weighting (w/ NW) for the two-image\n4\u00d7 and 16\u00d7 scale factors for both MISR DPS and MISR DMAP.\nw/o NW\nw/ NW\n\u03c3base\nPSNR (\u2191)\nSSIM (\u2191)\nPSNR (\u2191)\nSSIM (\u2191)\nDPS\n0.15\n32.5 \u00b1 2.4 0.90 \u00b1 0.01 32.5 \u00b1 2.4 0.90 \u00b1 0.01\n0.3\n29.8 \u00b1 1.6 0.78 \u00b1 0.01 29.9 \u00b1 1.6 0.79 \u00b1 0.01\n0.45\n27.0 \u00b1 1.1 0.63 \u00b1 0.02 27.5 \u00b1 1.2 0.66 \u00b1 0.02\nDMAP\n0.15\n32.9 \u00b1 2.6 0.91 \u00b1 0.02 32.9 \u00b1 2.6 0.91 \u00b1 0.02\n0.3\n30.0 \u00b1 1.6 0.80 \u00b1 0.01 30.1 \u00b1 1.7 0.80 \u00b1 0.01\n0.45\n27.2 \u00b1 1.2 0.65 \u00b1 0.01 27.5 \u00b1 1.3 0.67 \u00b1 0.01\n5. Conclusion\nWe presented an approach for MISR using diffusion models.\nWe showed that likelihood separation underneath the diffu-\nsion trajectory allows for a flexible multi-image framework\nand implemented it with noise-weighting for the diffusion\nposterior sampling family of inverse problem solvers. We\nconducted experiments at various scale factors with orthogo-\nnal acquisition planes and showed that MISR outperforms\nSISR both qualitatively and quantitatively, and additionally\nbetter preserves anatomy. Future work will address mis-\nalignment between measurements (though an estimated reg-\nistration can be folded in to the measurement A) as well as\nmulti-contrast acquisitions. To the best of our knowledge,\nthe results of this work are state-of-the-art in the field of\nMRI super-resolution for anisotropic image volumes. Our\n8\nresults indicate that MISR diffusion could make standard 2D\nMRI acquisitions usable in all viewing planes, potentially\nreducing the need for long, thin-slice 3D scans and restoring\nimage analytic quality for retrospectively accessed imaging\nsessions.\n9\nReferences\n[1] Yochai Blau and Tomer Michaeli. The perception-distortion\ntradeoff. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6228\u20136237, 2018. 8\n[2] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Di-\nmakis. Compressed sensing using generative models. In\nInternational Conference on Machine Learning, pages 537\u2013\n546. PMLR, 2017. 2\n[3] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan\nEckstein, et al. Distributed optimization and statistical learn-\ning via the alternating direction method of multipliers. Foun-\ndations and Trends\u00ae in Machine learning, 3(1):1\u2013122, 2011.\n5\n[4] Robert W. Brown, Y-C Norman Cheng, E. Mark Haacke,\nMichael R. Thompson, and Ramesh Venkatesan. Magnetic\nResonance Imaging: Physical Principles and Sequence De-\nsign (Second edition). Wiley, 2014. 3\n[5] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel. A\nreview of image denoising algorithms, with a new one. Multi-\nscale modeling & simulation, 4(2):490\u2013530, 2005. 2\n[6] Emmanuel J. Cand`es, Justin Romberg, and Terence Tao. Ro-\nbust uncertainty principles: Exact signal reconstruction from\nhighly incomplete frequency information. IEEE Transactions\non Information Theory, 52(2):489\u2013509, 2006. 2\n[7] Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangy-\nong Chen, Pheng-Ann Heng, and Stan Z. Li. A survey on\ngenerative diffusion models. IEEE Transactions on Knowl-\nedge and Data Engineering, 36(7):2814\u20132830, 2024. 3\n[8] Stanley H Chan, Xiran Wang, and Omar A Elgendy. Plug-and-\nplay ADMM for image restoration: Fixed-point convergence\nand applications. IEEE Transactions on Computational Imag-\ning, 3(1):84\u201398, 2016. 2\n[9] Chih-Wei Chang, Junbo Peng, Mojtaba Safari, Elahheh Salari,\nShaoyan Pan, Justin Roper, Richard LJ Qiu, Yuan Gao, Hui-\nKuo Shu, Hui Mao, et al. High-resolution MRI synthesis\nusing a data-driven framework with denoising diffusion prob-\nabilistic modeling. Physics in Medicine & Biology, 69(4):\n045001, 2024. 2\n[10] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou.\nLikelihood training of schr\u00a8odinger bridge using forward-\nbackward SDEs theory. In International Conference on Learn-\ning Representations, 2022. 3\n[11] Ming Cheng, Haoyu Ma, Qiufang Ma, Xiaopeng Sun, Weiqi\nLi, Zhenyu Zhang, Xuhan Sheng, Shijie Zhao, Junlin Li,\nand Li Zhang. Hybrid transformer and CNN attention net-\nwork for stereo image super-resolution. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1702\u20131711, 2023. 2\n[12] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune\nGwon, and Sungroh Yoon. ILVR: Conditioning method for\ndenoising diffusion probabilistic models. In 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n14347\u201314356, 2021. 2\n[13] Kyobin Choo, Youngjun Jun, Mijin Yun, and Seong Jae\nHwang. Slice-consistent 3D volumetric brain CT-to-MRI\ntranslation with 2D brownian bridge diffusion model. In In-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 657\u2013667. Springer,\n2024. 3\n[14] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul\nYe. Improving diffusion models for inverse problems using\nmanifold constraints. Advances in Neural Information Pro-\ncessing Systems, 35:25683\u201325696, 2022. 3\n[15] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann,\nMarc Louis Klasky, and Jong Chul Ye. Diffusion posterior\nsampling for general noisy inverse problems. In The Eleventh\nInternational Conference on Learning Representations, 2023.\n2, 3\n[16] Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. Dif-\nfusion models for inverse problems.\narXiv preprint\narXiv:2508.01975, 2025. 3\n[17] Giannis Daras, Joseph Dean, Ajil Jalal, and Alex Dimakis.\nIntermediate Layer Optimization for Inverse Problems using\nDeep Generative Models. In Proceedings of the 38th Interna-\ntional Conference on Machine Learning, pages 2421\u20132432.\nPMLR, 2021. 2\n[18] Giannis Daras, Yuval Dagan, Alex Dimakis, and Constantinos\nDaskalakis. Score-guided intermediate level optimization:\nFast Langevin mixing for inverse problems. In Proceedings\nof the 39th International Conference on Machine Learning,\npages 4722\u20134753. PMLR, 2022. 2\n[19] Valentin De Bortoli, James Thornton, Jeremy Heng, and Ar-\nnaud Doucet. Diffusion Schr\u00a8odinger Bridge with Applica-\ntions to Score-Based Generative Modeling. arXiv preprint\narXiv:2106.01357, 2021. 2\n[20] C. Dong, C. C. Loy, K. He, and X. Tang.\nImage super-\nresolution using deep convolutional networks. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 38(2):\n295\u2013307, 2016. 2\n[21] David L Donoho. Compressed sensing. IEEE Transactions\non Information Theory, 52(4):1289\u20131306, 2006. 2\n[22] Kathryn A. Ellis, Ashley I. Bush, David Darby, Daniela\nDe Fazio, Jonathan Foster, Peter Hudson, Nicola T. Lauten-\nschlager, Nat Lenzo, Ralph N. Martins, Paul Maruff, et al. The\nAustralian Imaging, Biomarkers and Lifestyle (AIBL) study\nof aging: methodology and baseline characteristics of 1112\nindividuals recruited for a longitudinal study of Alzheimer\u2019s\ndisease. International psychogeriatrics, 21(4):672\u2013687, 2009.\n5\n[23] Jinsheng Fang, Hanjiang Lin, Xinyu Chen, and Kun Zeng.\nA hybrid network of CNN and transformer for lightweight\nimage super-resolution. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages\n1103\u20131112, 2022. 2\n[24] Chun-Mei Feng, Yunlu Yan, Huazhu Fu, Li Chen, and Yong\nXu.\nTask transformer network for joint MRI reconstruc-\ntion and super-resolution.\nIn Medical Image Computing\nand Computer Assisted Intervention\u2013MICCAI 2021: 24th\nInternational Conference, Strasbourg, France, September 27\u2013\nOctober 1, 2021, Proceedings, Part VI 24, pages 307\u2013317.\nSpringer, 2021.\n[25] Cristhian Forigua, Maria Escobar, and Pablo Arbelaez. Super-\nformer: Volumetric transformer architectures for mri super-\nresolution. In International Workshop on Simulation and Syn-\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper\n\n#### Main Contributions\nThe paper introduces a novel approach to multi-image super-resolution (MISR) using diffusion models, which are state-of-the-art generative models in imaging tasks. The key contributions include:\n- **Likelihood Separability for Diffusion Posterior Sampling:** The authors show that likelihood gradients can be decomposed exactly across independent measurements without requiring joint operators or modifications of the diffusion model.\n- **Unified MISR Generalization:** They derive MISR versions of existing diffusion-based inverse solvers such as DPS, DMAP, and DPPS. \n- **Noise Weighting for Heterogeneous MRI Acquisitions:** The paper introduces a method to weight measurements based on their noise strength, enabling principled fusion of different slice thicknesses, SNRs, and spatial resolutions.\n- **State-of-the-Art Performance:** Across 4\u00d7/8\u00d7/16\u00d7 anisotropic degradations, the proposed MISR extensions outperform single-image super-resolution (SISR) methods by up to 3 dB in PSNR while producing anatomically faithful reconstructions.\n\n#### Methodology\nThe authors formalize the multi-image super-resolution problem and derive a separable likelihood expression that allows for consistent data across multiple measurements without constructing a joint operator. They analyze how this decomposition impacts diffusion posterior sampling (DPS) methods, including DPS, DMAP, DPPS, and diffusion-based PnP/ADMM approaches.\n\n#### Experiments\nThe experiments focus on super-resolution of anisotropic magnetic resonance head images with various scale factors (4\u00d7, 8\u00d7, and 16\u00d7). The authors use Gaussian slice selection profiles for simulations to enable quantitative distortion metrics. They compare SISR methods directly to MISR versions using orthogonal planes.\n\n#### Key Findings\n- **Quantitative Results:** Across different scale factors, the proposed MISR extensions outperform their SISR counterparts in PSNR and SSIM metrics by significant margins.\n- **Qualitative Comparisons:** The paper provides visual comparisons of representative subjects at various scale factors, showing that MISR methods can recover more correct anatomical details compared to SISR.\n\n#### Conclusions\nThe authors conclude that diffusion models offer a practical, high-quality approach for multi-image MRI super-resolution without requiring retraining. Their work positions MISR as an impactful alternative to long 3D MRI scans, particularly in clinical settings where anisotropic volumes are not useful when viewed through-plane. The proposed noise weighting method further enhances the performance of MISR methods.\n\n### Summary\nThe paper introduces a novel approach to multi-image super-resolution using diffusion models, leveraging likelihood separability and noise-weighting techniques. This results in improved performance across various scale factors compared to single-image super-resolution methods. The authors demonstrate that their MISR extensions achieve state-of-the-art results while preserving anatomical details, making it an impactful advancement for MRI imaging tasks."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Virtual Urbanism: An AI-Driven Framework for Quantifying Urban\nIdentity.\nA Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic\nEnvironments\nGlinskaya Maria\u00b9\n\u00b9Department of Architecture, The University of Tokyo, Tokyo, Japan\u200b\nAbstract\nThis paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for\nquantifying urban identity through the medium of synthetic urban replicas. The framework aims to\nadvance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study\nVirtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and\nLoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic\nurban sequences, excluding existing orientation markers to elicit core identity-forming elements.\nHuman-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II)  quantified area-level\nidentity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of\n~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of\nidentity levels across areas, while semantic analysis revealed culturally embedded typologies as core\nidentity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis,\noutlining a path toward automated, multi-parameter identity metrics.\nKeywords\ngenerative artificial intelligence, latent diffusion model, low-rank adaptation model, urban perception,\nurban identity\n1. Introduction\nRecent advances in Artificial Intelligence (AI) have rapidly expanded its presence across diverse domains,\nranging from natural-language processing supported by Large Language Models (LLMs) to visual synthesis\nenabled by Diffusion Models (DMs). These developments are reshaping creative, analytical, and interpretive\nmethodologies across architectural and urban studies [1], allowing researchers to move beyond\nconventional\u2014often reductionist, resource-bound\u2014modes of urban study, such as continuous elevation drawing\n[2], toward new, data-driven methods that focus on semantic understanding and AI-assisted decoding of urban\nidentity [3]. \u200b\nIn parallel, contemporary urban development is increasingly shaped by forces of globalization,\nstandardization, and economic optimization, where project priorities often centre on production efficiency,\nmarket positioning, and replicable design strategies [4]. Within such conditions, locally embedded urban\nidentity\u2014a naturally evolved configuration of perceptual cues and locally distinctive features, shaped by\nhistorical layering and cultural narratives, that define an area\u2019s visual\u2013spatial coherence and perceived\nauthenticity\u2014tend to be treated as secondary considerations, despite their critical role in shaping lived\nexperience and cultural continuity. This has heightened the importance of developing computational methods\ncapable of documenting, analysing, and preserving urban identity before it becomes diluted or lost. As a result,\nthe adaptation of machine-learning techniques to urban-perception research has become an emerging trajectory.\u200b\nWhile a number of studies have begun to incorporate AI-generated content as part of their analytical\nframework [5],[6], a recurring omission emerges: the unique analytical affordance offered by generative AI\u2014its\nability to generate large volumes of controlled, high-dimensional variation\u2014is rarely leveraged as an analytical\nasset. Existing approaches still mainly rely on a small number of manually selected outputs drawn from what is,\nin principle, a theoretically unbounded generative space. A more substantial methodological gap concerns the\nfact that AI is still predominantly used to supplement conventional analytical workflows rather than to establish\nnew ones, leaving its potential to function as a multimodal, variation-rich framework for urban analysis largely\nunexplored.\u200b\nThis work addresses these limitations by positioning generative AI as the methodological core of a novel\nanalytical framework, shifting urban inquiry from static imagery to dynamic synthetic environments and from\nAI-assisted to AI-driven analysis. In doing so, it enables the systematic revelation of previously inaccessible\npatterns in urban identity, with potential applicability across analytical urban research, applied design practice,\ncomputational urban identity archiving, and the identification of identity-rich urban environments.\u200b\nThe primary visual analytical materials and fine-tuned generative models produced in this study are publicly\navailable; details are provided in the Data Availability section.\u200b\n1.1 Core Methodological Approach\nThe main contribution of this paper is the introduction and demonstration of Virtual Urbanism (VU), a\nmultimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic\nurban replicas. Synthetic urban replicas are defined here as generatively produced, non-metric, non-literal\nreconstructions of real urban environments\u2014approximate representational models deliberately structured to\nfunction as an analytical medium, created to enable forms of insight not directly attainable through the real\nenvironment itself.\nThe main analytical aspect of synthetic replicas is their dynamic quality\u2014here defined as a state of constant\nfluctuation in the visual representation of form, enabled by AI-driven synthetic generative variation. In this\nwork\u2019s pilot study, this dynamic quality was operationalized by presenting the replicas as dynamic synthetic\nurban sequences. A dynamic synthetic urban sequence is defined as a series of images rendered within the\nsynthetic urban replica and subsequently re-generated through the diffusion model\u2014each frame undergoing\nimage-to-image generative transformation under a constrained, fine-tuned generative model\u2014to introduce\nmaximal AI-driven variation in the representation of the same form or environment. These frames are then\nassembled into a video sequence that produces dense perceptual fluctuation, resulting in a highly intensive\ndynamic analytical representational medium.\nAccordingly, here the sequence of appearances becomes the primary analytical tool for identifying core urban\nidentity\u2013forming elements\u2014recurrent perceptual cues, visual and spatial features, and typological references that\nfunction as primary anchors of an area\u2019s perceived authenticity and cohesive local urban identity\u2014building on\nthe concept of the interrepresentational approach originally introduced by Marcos Novak. This concept can be\ntraced to Novak\u2019s essay Liquid Architecture (1991) [7], in which he reinterprets Sartre\u2019s Principle of the Series\n[8]\u2014a philosophical position proposing that essence emerges through a sequence of appearances governed by\ninternal logic rather than subjective perception. Under this interpretation, identity is not fixed or static, but\nrevealed through successive manifestations.\nExtending this conceptual line, the VU framework leverages generative AI to construct synthetic urban\nreplicas of existing environments endowed with dynamic quality. By re-envisioning the urban environment as a\nsequence of successive manifestations generated through controlled, AI-driven variation of the same underlying\nform, VU aims to render the internal logic of urban identity perceptible through large volumes of systematic,\nhigh-dimensional repetition. In this formulation, AI-enabled successive generative appearances operate as an\nanalytical mechanism for revealing core urban identity structures, effectively rearticulating Novak\u2019s original\ninterrepresentational approach within contemporary urban research through the VU framework. The present\npilot study operationalizes this conceptual logic in a preliminary form.\n1.2 Research Overview\u200b\nAt the long-term level, the VU framework sets goals to advance the development of computationally tractable\nidentity metrics\u2014primarily encompassing (I) the formalization of urban identity as a measurable and\ncross-comparable metric, and (II) the quantification of the elements that define urban identity and the\ndetermination of their relative weight.\u200b\nThis paper represents the first applied stage of that agenda, specifically investigating the following objectives:\n(a) the capacity of generative models in replicating real urban settings;\n(b) the perceptual validity of these replications through human perceptual evaluation experiments;\n(c) the applicability of AI-driven synthetic urban replicas as an analytical medium for studying urban identity at\nthe visual, streetscape level.\nThe remainder of the paper is structured as follows. Section 2, Related Works, reviews prior research. Section\n3, Pilot Study \u201cVirtual Urbanism and Tokyo Microcosms\u201d, introduces the pilot study through which the VU\nframework is demonstrated. Where Section 4, Experiment, describes the method for constructing synthetic\nurban replicas, corresponding to objective (a). Section 5, Evaluation, outlines the method for extracting\nanalytical data through human perceptual evaluation addressing objective (b). And Section 6, Results and\nDiscussion, analyses the acquired data to generate insights addressing objective (c). Section 7, Limitations and\nChallenges, reflects on the methodological and technical constraints identified. Section 8: Conclusion,\nsummarizes the overall outcomes of the pilot study, and outlines directions for future development of the VU\nframework.\u200b\n2. Related Works\nThis review contextualizes generative models in urban studies by surveying generative urban design (GUD),\nemerging AI-based urban perception research, and related applications in architectural heritage and\nlocal-identity preservation. Each domain reveals distinct methodological gaps, which collectively delineate the\npositioning of the present study within the broader field.\n2.1 Generative Urban Design \u200b\nGenerative AI is currently finding growing implementation within architectural discourse, ranging from\nautomated ideation in GUD [9] to simulation [10], visualization [11], and human-in-the-loop design evaluation\n[12]. Particularly widely used in practical implementation are vision-based generative models, which have\nrecently experienced a shift from earlier versions\u2014Generative Adversarial Networks (GANs) [13], operating\nthrough a generator\u2013discriminator system\u2014to more refined, controllable, and accessible Denoising Diffusion\nModels (DDMs), such as Latent Diffusion [14], featuring true text-to-image generation and accompanied by\nmajor simplifications in user interface (UI) design. This transition has catalyzed a decisive shift, transitioning\nthe field from niche science to mainstream application.\u200b\nWhile mostly utilized during the Schematic Design stage of architectural project development [15],\napplications of DMs can be found across a wide range of domains in both urban research and production,\nranging from early-stage urban analysis [16] to controllable floorplan generation [17], architectural fa\u00e7ade\nsynthesis [18], visualization and rendering assistance [19], and further expanding into three-dimensional spatial\ngeneration [20].\u200b\nYet, as Hazbei and Cucuzzella [21] note, computational approaches often risk reducing architectural context\nto parametric constraints, overlooking the more intangible social, cultural, and perceptual dimensions that\nconstitute the essence of place. Their observation underscores a critical gap: current GUD practices widely rely\non AI for measurable optimization but often overlook the perceptual factors that define human experience of\nplace. This work responds to the challenge outlined above by contributing to the empirical exploration of the\ncapacity of generative models to articulate contextual qualities of the built environment and gauge urban\nidentity, thereby enabling qualitative, higher-dimensional perceptual factors to be incorporated into measurable\nparameters that can, in turn, be reintegrated into future GUD workflows.\n2.2 Urban Perception Studies\u200b\nA parallel trajectory of AI integration has emerged within urban perception studies, where machine learning is\nemployed to evaluate how humans perceive and emotionally respond to built environments. Representative\nstudies include Zhang et al. [22], predicting perceptual attributes from street-view imagery; Liu et al. [23],\nemploying geo-tagged photos to computationally reassess Lynch\u2019s imageability elements; and Ordonez and Berg\n[24], demonstrating cross-city prediction of perceptual judgments from visual data correlated with\nsocioeconomic indicators. \u200b\nWhat earlier generations of urban theorists pursued as qualitative exploration of city atmospheres\u2014for\ninstance, the once-unfinished ambitions of the Situationist International\u2019s psychogeographic maps and urban\nd\u00e9rives [25] \u2014is now, being reawakened, reinterpreted, and operationalized, translating affective and perceptual\nqualities of urban space into quantifiable visual data [22] and allowing perceptual urban phenomena to be\nexamined computationally at metropolitan and city-wide scales [26].\u200b\nDespite the rapid integration of AI-driven methods into this research trajectory, only limited studies have\nexamined qualitative urban characteristics through artificially generated media and their alignment with human\nperception, such as Tanigawa and Kobayashi\u2019s analysis of AI-generated streetscape imagery and perceived\nauthenticity [5], and Law et al.\u2019s perceptual alignment studies of synthetic architectural fa\u00e7ades [27]. Another\ntendency can be observed: with the rapid involvement of machine-based methods encouraging an accelerated\nexpansion of scale, research is progressively zooming out from the ground-level urban resolution toward\nmeta-scale perspectives, a pattern evident across the majority of studies reviewed above. \u200b\nIn response, this study takes a step back to explore meso-scale, context-sensitive applications of generative\nAI\u2014operationalized specifically through the introduction of an AI-driven analytical medium\u2014focusing on\nconfined urban areas, thus counterbalancing the field\u2019s prevailing shift toward city-scale analysis.\n2.3 Image-Generative AI for Architectural Heritage and Local Identity\n2.3.1 GAN-based\u200b\nThe domain that can be positioned at the convergence of GUD and Urban Perception Studies is the emerging\nuse of image-generative AI for preserving locality-specific architectural and cultural features. Early explorations\nin this area primarily relied on Generative Adversarial Networks (GANs). For instance, Bachl and Ferreira [28]\nproposed City-GAN, a conditional GAN trained to learn and reproduce architectural styles from city-specific\nimage datasets. Steinfeld [6] introduced GAN-Loci, designed to extract and replicate the implicit spatial\ncharacteristics of urban areas, implementing StyleGAN model [29]. Ali and Lee [30] presented iFACADE\u2014a\nCycleGAN-based [31] generator for urban infill, producing fa\u00e7ades style-mixed from adjacent buildings. While,\nSun et al. [32] proposed the CycleGAN-based method for identifying and reproducing historic architectural\nstyles to support urban renovation. While GAN-based studies established important early groundwork for\nlocality-aware generative workflows, their broader adoption has been limited by well-documented challenges,\nincluding training instability, lower detail fidelity, and restricted controllability [33].\u200b\n\u200b\n2.3.2 Diffusion-based \u200b\nDMs emerged as a more stable alternative, offering higher-resolution synthesis, multimodal conditioning, and\ngreater flexibility. Within this trajectory, Latent Diffusion Models (LDMs) [14] have become the basis for\nresearch-oriented workflows, with open architecture supporting detailed conditioning, parameter control, and\ndomain-specific fine-tuning. For instance, Law et al. [27] used Stable Diffusion (SD) to generate geographically\nplausible counterfactual fa\u00e7ades and evaluated them against geographical, objective, and affective descriptors\nvia AI- and human-based perceptual alignment. Jo et al. [19] developed an approach for generating alternative\narchitectural design options incorporating regional stylistic characteristics using SD fine-tuned via Low-Rank\nAdaptation (LoRA) technique [34] enabling parameter-efficient domain specialization.\nSeveral studies further demonstrate the suitability of SD and LoRA-based fine-tuning for heritage-oriented\napplications. Kuang et al. [35] introduced a framework integrating SD, LoRA, and ControlNet [36] to support\nautomated architectural heritage preservation in urban renewal projects. Shin [37] employed diffusion-based\nworkflows for heritage artefact 3D reconstruction. Guo et al. [38] applied LoRA-fine-tuned DM to the\ngenerative reconstruction of Yi ethnic embroidery patterns, illustrating the broader cultural-heritage relevance.\nThere is a simple but fundamental methodological distinction between the present study and the predominant\nbody of work outlined above. In prior studies, generative models are used primarily for reproduction, with\ngenerative output treated as the end goal of the process. In contrast, this study reproduces in order to analyse.\nSynthetic replication is not the end goal but the means to investigate contextual cues and to uncover new\nperceptual insights into urban identity.\n3. Pilot Study  \u201cVirtual Urbanism and Tokyo Microcosms\u201d\nTo demonstrate the proposed framework\u2019s potential, a pilot implementation\u2014Virtual Urbanism and Tokyo\nMicrocosms\u2014is presented, operationalized across nine Tokyo urban areas. In this study, the term Microcosms\nwas introduced to avoid administrative framings such as \u201cdistricts\u201d or \u201ccities,\u201d and instead denote perceptually\ncoherent urban units. Each study area was defined at a neighborhood-scale spatial extent, with boundaries\napproximating a walkable radius of approximately 1.5 km.\u200b\nTokyo was selected as the pilot site due to its intrinsic spatial complexity, defined by high visual density, a\nmultilayered urban fabric, and rapidly changing contextual conditions. Identifying core urban identity\u2013forming\nelements here presents a significant analytical challenge, making the city an appropriate testbed for evaluating\nthe capacities of the VU framework. \u200b\nThe selection of the case-study areas was intended to capture a spectrum of urban development trajectories,\nranging from organically evolved, community-driven environments to areas shaped by centralized planning and\ncorporate redevelopment. The nine case-study areas thus comprised: Shimokitazawa, Harajuku, Yanesen,\nKagurazaka, Asakusa, Ueno, Shibuya, Ikebukuro, and Roppongi.\n3.1 Pilot Study Workflow and Objectives\nThe pilot study consisted of three sequential components, corresponding to Sections 4, Experiment; 5,\nEvaluation; and 6, Results and Discussion. Together, these sections outline the construction, assessment, and\nimplementation of AI-driven synthetic urban replicas as an analytical medium within the VU framework.\n(Figure 1)\u200b\nSection 4, Experiment, details the methodology for constructing a Tokyo-based synthetic urban replica as a\nsingle three-dimensional base, subsequently operationalized into nine dynamic synthetic urban sequences as the\nstudy\u2019s primary analytical medium. The analytical advantage of this medium over direct real-world observation\nis twofold. First, its primary feature is dynamic quality, defined earlier in alignment with Novak\u2019s\ninterrepresentational approach. Dynamic repetition functioning as a perceptual amplifier, enabling latent patterns\nto emerge through sustained exposure rather than static inspection. Second, both the replica and the\ncorresponding sequences are deliberately constructed to omit existing orientation markers. Orientation\nmarkers\u2014here defined as visually salient reference elements such as landmarks, signage, or other distinctive\nfeatures that facilitate immediate place identification\u2014are removed to eliminate recognition driven by prior\nfamiliarity, collectively foregrounding deeper contextual cues.\u200b\nThe resulting sequences thus operate as a controlled, landmark-absent analytical medium through which urban\nidentity can be examined via human perceptual evaluation, the methodological procedure of which is detailed in\nSection 5, Evaluation. Section 6, Results and Discussion, subsequently presents and interprets the collected data.\u200b\n\u200b\nAlignment with Research Objectives\nWithin this structure, the pilot study directly addresses previously defined research objectives (a) and (b) by\ntesting both the technical capacity and the perceptual validity of AI-driven synthetic urban replicas. It further\nadvances objective (c)\u2014the assessment of AI-driven synthetic urban replicas as an analytical medium\u2014through\ntwo focused sub-objectives, designed specifically to implement the previously outlined long-term goals of the\nVU framework:\n(cI) to evaluate and compare Urban Identity Levels (UIL) across the selected study areas\u2014operationalized\nthrough the use of identification Accuracy Rate parameter per sequence as preliminary metric;\n(cII) to define the core urban identity\u2013forming elements of each area\u2014derived through semantic analysis of\ncue-emergent free-text survey responses collected during human perceptual evaluation sessions.\nTheir implementation and resulting analyses are discussed in Sections 5 and 6.\nFigure 1. Methodological Workflow of the Pilot Study.\n4. Experiment\nThe experiment follows a four-stage pipeline. First, LDM is fine-tuned through area-specific dataset creation\nand LoRA training, resulting in multiple area-specific LoRA adapters. These adapters are then employed to\nconstruct a synthetic urban replica and the corresponding dynamic synthetic urban sequences. The full\nmethodological pipeline is described in Sections 4.1\u20134.5.\n4.1 Model Pipeline Components\nThe Experiment is operationalized by leveraging the widely adopted LDM\u2014Stable Diffusion (SD) from\nStabilityAI, specifically SD version 1.5 (SD 1.5), trained on the LAION-5B dataset [39]. SD is a latent\ndenoising diffusion architecture that predicts and removes noise across timesteps. This study adopts the standard\nDDPM formulation introduced by Ho et al. [40] and extended to latent space by Rombach et al. [14], without\nmodifying the diffusion objective or noise-prediction mechanism, as model architecture evaluation falls outside\nthe scope of this work.\u200b\nProprietary generators (e.g., MidJourney, DALL\u00b7E) were excluded in favor of open-source SD 1.5, ensuring\nreproducibility and architectural flexibility. Although the more recent SDXL model [41] offers higher visual\nfidelity, it was not adopted, as the production phase of this study predated its release.\nBuilding on the base model, this experiment integrates LoRA [34] to enable targeted, lightweight fine-tuning\nby introducing a small set of trainable rank-decomposition matrices into the pretrained model, allowing specific\nstylistic or contextual attributes to be learned while keeping the original model weights frozen, providing\nefficient specialization with minimal computational cost. ControlNet [36] is further employed to condition\ngeneration on structured inputs\u2014such as depth maps, line drawings, or segmentation masks\u2014thereby enabling\nexplicit control over spatial layout and structural coherence.\n4.2 Dataset Creation \u200b\n4.2.1 Image dataset\u200b\nFor each of the nine areas, data collection focused on the visible surface appearance\u2014the \u201cexternal layers\u201d of\nurban form. In contrast to the common approach in urban-perception studies that rely on automated Street View\n(SV) image acquisition (e.g., via the SV Static API [42]), this work employed manually collected image datasets\nobtained through fieldwork. This approach enabled targeted capture of locality-representative environments and\nensured coverage of spatial conditions\u2014such as narrow streets and enclosed urban spaces\u2014that are\ninsufficiently represented or absent in SV imagery, yet critical to perceptual identity, particularly in Asian urban\ncontexts.\nAcross 3\u20134 field-visits per area, approximately 200\u2013300 photographs were captured, including street-level\nviews, fa\u00e7ade exteriors, and selected architectural details. Each dataset was subsequently curated to\napproximately 60\u201366 images\u2014a number identified as the minimal, computationally efficient dataset size,\nenabling LoRA models to reliably capture distinct local urban identity characteristics. Images were captured\nusing an iPhone 12 Pro in both portrait and landscape orientations (4032\u00d73024 px and 3024\u00d74032 px). No\nresizing was applied, as uniform rectification of images introduces geometric distortions that compromise\narchitectural proportions. In contrast, preliminary tests indicated that preserving native image dimensions\nimproves LoRA training fidelity and enables more accurate variation in scale and proportion during subsequent\ngeneration.\nDatasets were organized with a consistent typological distribution: approximately 63% street-view images,\n35% fa\u00e7ade images, and 2% architectural details or interiors. This composition\u2014combining street-views with\naxonometric fa\u00e7ade images\u2014effectively enabled the single LoRA model implementation to generate both\nfa\u00e7ade-focused and full-streetscape outputs in subsequent stages. \u200b\nFigure 2. Example structure of the image dataset used for training the Ikebukuro.safetensors LoRA model.\u200b\n4.2.2 Textual Dataset \u200b\nThe paired textual dataset was subsequently developed using manually produced captions, refined with\nChatGPT assistance, following a structured annotation logic (Figure 3). Although substantially more\ntime-consuming than fully automated methods (e.g., BLIP [43]), manual captioning was adopted as a deliberate\nmethodological decision. The explicit goal of this process was to encode controllable semantic tokens during\ntraining, enabling modulation of LoRA influence at inference time via targeted token repetition. This\nmechanism increases the activation of LoRA-specific features without changing model weights, allowing\nprecise control over aspects of the generated output\u2014such as viewpoint selection and building-scale variation\n(Figure 4). Importantly, explicit stylistic or interpretive identity labels were not encoded: both the training\ncaptions and subsequent prompt engineering were intentionally kept semantically neutral to avoid introducing\nmodel bias or prematurely evoking specific architectural styles (e.g., \u201cGothic\u201d), thereby allowing stylistic and\ncontextual features to emerge through the model\u2019s own inferential processes.\nFigure 3. Representative image\u2013text pairs demonstrating the structured annotation logic implemented for LoRA\ntraining, shown for street-view and axonometric fa\u00e7ade image types.\nFigure 4. Ground-truth image with manual annotation set and a corresponding generative output sample.\n4.3 LoRA Training Configuration\nFollowing dataset construction, nine LoRA models\u2014one for each of the nine study-areas\u2014were trained locally\nusing the v1-5-pruned.safetensors base model on an NVIDIA GeForce RTX 3080 GPU. The training time for a\nsingle model under this configuration was approximately four hours. The v1-5-pruned model was selected due\nto its optimisation for fine-tuning, efficient VRAM usage, and strong performance in both generalisation and\ndetailed adjustments. As the objective of this study was not to benchmark training configurations\u2014key\nparameters such as batch size, repeat count, learning rate, and optimizer type were adopted from settings\nreported in prior studies that demonstrated stable, high-quality results [44]. LoRA training was conducted using\nKohya-ss [45], a GUI-based training framework. Most settings remained at their default values, except for the\nparameters listed below:\u200b\n\u2022 Max Resolution: 768 \u00d7 768 pixels\u200b\n\u2022 Epochs: 12 \u200b\n\u2022 Batch Size: 2\u200b\n\u2022 Learning Rate: 0.00002\nThe nine area-specific LoRA models produced in this study are publicly available through an open GitHub\nrepository (see Data Availability section).\nFigure 5. Sample images generated during LoRA testing for Shibuya, Ueno, and Ikebukuro areas.\n4.4  Synthetic Urban Replica Construction\nWith the area-specific LoRA models established, the experiment advanced to the construction of the synthetic\nurban replica that underpins subsequent analytical stages. The replica is constructed in three stages, detailed in\nSections 4.4.1\u20134.4.3.\u200b\n4.4.1 Map\u200b\nThe base map was constructed as a controlled abstraction of a 1,500 \u00d7 1,500 m segment of Tokyo\u2019s urban fabric,\nincorporating the nine study areas included in the pilot. This approach preserved relative urban density and\nspatial organization while intentionally simplifying secondary environmental complexity, thereby providing a\ncomputationally manageable yet structurally representative base. Consistent with the overall generative pipeline,\nmap construction implemented SD 1.5 and LoRA and was organized into two steps:\u200b\nStep 1: Pooled-Area LoRA Fine-Tuning and Base Map Generation. LoRA training was first conducted\nusing Google Earth aerial imagery pooled from all nine study areas. For each area, 8 aerial samples were\nextracted and cropped to 1,500 \u00d7 1,500 m tiles, producing a total training dataset of 72 images. At this stage, the\nmodel learned shared urban characteristics across the nine areas, enabling the generation of synthetic base maps\nintegrating representative features from each zone. Approximately 400 low-resolution map samples were\ngenerated, with a single base map selected based on its capacity to represent common environmental\ncharacteristics, including relative urban density, scale, and spatial configuration.\u200b\nStep 2: Zone-Level LoRA Fine-Tuning and Sector Refinement. The selected base map was subdivided\ninto 200 \u00d7 200 m sectors (64 sectors in total), each assigned to one of the nine study areas based on visual and\ncontextual similarity. A second training phase produced 9 area-specific LoRA models, each trained on 66\nGoogle Earth aerial samples cropped to the sector scale. Each sector was then upscaled and refined using its\ncorresponding area-specific LoRA, resulting in a high-resolution synthetic map with localized spatial\ncharacteristics.\nFigure 6. Workflow for the production of a 1,500 \u00d7 1,500 m synthetic map integrating nine Tokyo case-study\nareas and its subsequent three-dimensional instantiation.\n4.4.2 3D\nThe subsequent stage in synthetic replica development involved transforming the base map from a\ntwo-dimensional representation into a three-dimensional model. This was achieved through manual tracing,\ncategorical height assignment, and data import into Blender, where building volumes were extruded using\nrandomized parameters constrained by zone-specific height limits. Height distributions were calibrated using\nreference data from the PLATEAU platform (MLIT) [46], ensuring contextual fidelity.\n4.4.3 Fa\u00e7ade\nIn the final stage, fa\u00e7ade imagery for the synthetic urban replica was generated using a fine-tuned SD 1.5 model.\nSequential camera placement within Blender was used to render depth maps for individual fa\u00e7ade segments,\nwhich were subsequently imported into SD. Fa\u00e7ade generation was performed through image-to-image\ngeneration workflow using area-specific LoRA models (see Sections 4.2, 4.3), with ControlNet depth-map\nbased conditioning. Generated fa\u00e7ade textures were then re-projected onto the 3D model, resulting in a\ncontextually aligned and visually unified surface representations. (Figure 8)\nFigure 7. Fa\u00e7ade generation workflow integrating Blender-based depth extraction and fine-tuned SD 1.5 model\nwith ControlNet conditioning.\nFigure 8. Samples of fa\u00e7ades generated during synthetic urban replica construction, categorized by case-study\narea and urban subzone.\nFigure 9. Constructed synthetic urban replica: a three-dimensional environment integrating nine Tokyo\ncase-study areas.\n4.5 Dynamic Synthetic Urban Sequence Production \u200b\nThis section addresses the production of the study\u2019s primary analytical medium: dynamic synthetic urban\nsequences. Within Blender, camera paths were defined inside the synthetic replica environment, with one path\ncorresponding to each case-study area. These paths were rendered as high\u2013frame-rate image sequences and\nexported for subsequent reprocessing through an image-to-image generation workflow in SD 1.5, implementing\nthe corresponding area-specific LoRA models and prompt configurations. Frame-wise generative transformation\nwas performed under constrained denoising conditions (\u2264 0.68), ensuring controlled variation within\nLoRA-defined representational boundaries. Through this process, minor shifts in viewpoint\u2014amplified by the\nelevated frame rate\u2014were transformed into successive manifestations of the same form or environment (Figure\n10).\u200b\nThe resulting dynamic synthetic urban sequences constitute the final analytical medium of the pilot study.\n(Figure 11). Each sequence was standardized as a 30-second video, comprising 385 frames per area (~13 fps),\nexhibiting the previously defined dynamic quality and landmark-absent conditions. In addition to their analytical\nfunction, the sequences form continuous urban-narrative traversals, subsequently used as stimulus material for\nhuman perceptual evaluation and public showcase sessions. The complete set of dynamic synthetic urban\nsequences is publicly available for viewing; access details are provided in the Data Availability section.\nFigure 10. Image-to-image diffusion\u2013based (SD 1.5, LoRA) transformation of a Blender-rendered urban replica\nfootage to dynamic synthetic urban sequence, illustrated through a subset of frame samples from two study\nareas.\nFigure 11. Representative frames from the dynamic synthetic urban sequences.\n5. Evaluation\nThis section outlines the methodology for perceptual evaluation conducted using the materials produced in\nSection 4, detailing the evaluation design and the introduced analytical parameters. \u200b\nUnlike conventional survey-based perceptual studies that rely on static stimuli and predefined evaluative\ndescriptors, the present work leveraged AI-driven dynamic synthetic urban sequences to frame the research\nsurvey as an immersive experience, with experimental materials functioning as a front-end communicative\nmedium that simultaneously supported data collection, interpretation, and dissemination (Figure 12). This\ndesign enabled open-ended, cue-emergent response elicitation, allowing urban identity to be examined through\nparticipant perception while effectively avoiding imposed taxonomies.\n\u200b\nFigure 12. Visual material produced within the pilot study, Tokyo Creative Forum 2025."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper\n\n**Title:** Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity  \n**Author:** Maria Glinskaya  \n**Department:** Department of Architecture, The University of Tokyo, Tokyo, Japan  \n\n#### Main Contributions\nThis paper introduces **Virtual Urbanism (VU)** as a multimodal AI-driven analytical framework to quantify urban identity through synthetic urban replicas. VU aims to advance computationally tractable urban identity metrics by leveraging generative artificial intelligence (AI) and diffusion models.\n\n**Key Findings:**\n1. **Synthetic Replicas:** The study produced nine dynamic synthetic urban sequences of Tokyo areas, each rendered as a series of images that undergo image-to-image generative transformation under constrained, fine-tuned generative models.\n2. **Human Evaluation Experiments:** Human evaluators assessed the replicas' perceptual legitimacy and quantified area-level identity using an Urban Identity Level (UIL) metric.\n3. **Core Elements Identification:** Semantic analysis revealed culturally embedded typologies as core elements forming urban identity.\n\n**Methodology:**\n- **Data Collection:** Manually collected image datasets for each of nine Tokyo areas, including street views, fa\u00e7ade exteriors, and architectural details. Textual descriptions were generated using ChatGPT.\n- **LoRA Training:** Multiple LoRA models (one per area) were trained on specific datasets to fine-tune the generative model's parameters.\n- **Replica Construction:** The base map was constructed as a controlled abstraction of Tokyo\u2019s urban fabric, incorporating nine study areas. Subsequent stages involved transforming this into a 3D model and generating fa\u00e7ade imagery.\n\n**Results:**\n- Mean identification accuracy for replicas was ~81%, confirming the validity of the synthetic representations.\n- Urban Identity Level (UIL) enabled assessment of identity levels across areas.\n- Semantic analysis revealed culturally embedded typologies as core identity-forming elements.\n\n**Conclusions:**\nVU positions generative AI at the methodological core, enabling systematic revelation of previously inaccessible patterns in urban identity. The framework's potential extends to applications such as applied design practice and computational archiving of urban identity. Future work will explore further development of VU to establish new multimodal, variation-rich analytical frameworks for urban analysis.\n\n### Summary\nThis paper introduces Virtual Urbanism (VU), a novel AI-driven framework for quantifying urban identity through synthetic replicas. The study produced nine dynamic sequences of Tokyo areas and used human evaluation experiments to assess perceptual validity and identify core elements forming urban identity. Results validate the synthesized representations, enabling assessment of urban identity levels across different areas. The paper concludes by outlining future directions for expanding VU's applicability in various analytical and applied contexts within urban studies."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "IIR-VLM: In-Context Instance-level Recognition\nfor Large Vision-Language Models\nLiang Shi1\nWei Li2\nKevin M Beussman2\nLin Chen2\nYun Fu1\n1Northeastern University\n2Wyze Labs, Inc.\nshi.lia@northeastern.edu, {wei.li, kevin.beussman, lchen}@wyze.com, yunfu@ece.neu.edu\nAbstract\nInstance-level recognition (ILR) concerns distinguishing\nindividual instances from one another, with person re-\nidentification as a prominent example. Despite the impres-\nsive visual perception capabilities of modern VLMs, we find\ntheir performance on ILR unsatisfactory, often dramatically\nunderperforming domain-specific ILR models.\nThis lim-\nitation hinders many practical application of VLMs, e.g.\nwhere recognizing familiar people and objects is crucial for\neffective visual understanding. Existing solutions typically\nlearn to recognize instances one at a time using instance-\nspecific datasets, which not only incur substantial data\ncollection and training costs but also struggle with fine-\ngrained discrimination. In this work, we propose IIR-VLM,\na VLM enhanced for In-context Instance-level Recognition.\nWe integrate pre-trained ILR expert models as auxiliary vi-\nsual encoders to provide specialized features for learning\ndiverse instances, which enables VLMs to learn new in-\nstances in-context in a one-shot manner. Further, IIR-VLM\nleverages this knowledge for instance-aware visual under-\nstanding. We validate IIR-VLM\u2019s efficacy on existing in-\nstance personalization benchmarks.\nFinally, we demon-\nstrate its superior ILR performance on a challenging new\nbenchmark, which assesses ILR capabilities across varying\ndifficulty and diverse categories, with person, face, pet and\ngeneral objects as the instances at task.\n1. Introduction\nInstance-level Recognition (ILR) [10, 16, 19, 27, 29\u201331] re-\nquires models to distinguish specific instances rather than\ngeneral categories.\nA prominent example is person re-\nidentification [7, 15, 29], which focuses on identifying the\nsame person across images from different cameras or in\nnew environments. This challenge of fine-grained match-\ning naturally extends to other categories, such as recogniz-\ning specific pets [21], vehicles [7], faces [4], or general ob-\njects [10, 16, 31]. Together, these tasks illustrate a funda-\nmental demand of visual perception, which is to differenti-\nate specific instances across diverse domains.\nDespite the impressive visual understanding capabilities\nof large vision-language models (VLMs), we find that they\nstruggle with instance-level recognition. When prompted to\nidentify the same instance from a small gallery of images,\neven state-of-the-art VLMs often fail to select the correct\ninstance, especially when the gallery images exhibit high\nvisual similarity. A wide range of real-world applications\ncan be affected by such an inability. For example, users\nof a smart-home camera may expect their VLM to reliably\ndistinguish between family members, or tell their own pet\napart from their neighbor\u2019s similar-looking pet. Instance-\nlevel recognition capabilities must serve as a cornerstone of\nsuch personalization features.\nTo enable the identification of instances, most existing\nsolutions leverage specialized, per-instance training [1, 2,\n14].\nWhile effective, this paradigm is relatively ineffi-\ncient, requiring the curation of per-instance datasets and\na repetitive training cycle for every new instance.\nBe-\nyond per-instance training, specialized methods like IDA-\nVLM [9] explore instance understanding without repetitive\nfine-tuning cycles. However, their demonstrated improve-\nment is largely confined to face datasets, showing limited\nsuccess on the diverse and challenging task of differenti-\nating instances with similar visual appearances, which our\nwork seeks to improve.\nWe overcome the limitations of existing work by cre-\nating In-context Instance-level Recognition VLM, or IIR-\nVLM, which is required to to learn and recognize new in-\nstances on-the-fly. Our framework achieves this with two\nprimary components. First, we elevate the VLM\u2019s percep-\ntion to fine-grained, instance-discriminating details by in-\ntegrating instance-level recognition expert models as aux-\niliary visual encoders. This component addresses the fact\nthat the VLM\u2019s original visual encoder, while strong on gen-\neral semantics, is not trained to distinguish fine-grained in-\nstances and thus lacks the discriminative power for highly\nsimilar instances. In light of this, we introduce an auxiliary\nencoder mechanism. Through an attention-based adapter,\n1\narXiv:2601.14188v1  [cs.CV]  20 Jan 2026\nwe fuse the highly instance-discriminative features from an\nexpert (e.g., an off-the-shelf person re-identification model)\nwith those from the VLM\u2019s original, general-purpose visual\nencoder. This process transfers the experts\u2019 ability to iden-\ntify subtle distinguishing clues, allowing the VLM\u2019s perfor-\nmance to become comparable to, or even surpass, that of the\nexperts themselves. We quantitatively demonstrate the aux-\niliary encoder\u2019s critical role on fine-grained tasks, showing\nits performance advantage over the baseline widens as the\ntasks involve more visually similar instances.\nSecond, we introduce a two-stage, lightweight train-\ning process to adapt the VLM to challenging ILR data.\nWe repurpose existing ILR datasets of diverse categories\ninto an instruction-tuning format, where each task consists\nof a query and a small image gallery, essentially asking\nthe model to learn instances from the gallery in-context.\nThis process provides a comprehensive training dataset that\nspans varying difficulty and instance categories. In Stage\n1, the model is trained simply to identify the matching in-\nstance from the gallery. Stage 2 then advances this capabil-\nity by training the model to not only match the instance, but\nalso leverage this new instance-level knowledge to perform\nrelated visual understanding tasks. This training paradigm\nempowers the VLM to identify and reason about any new\ninstance of different categories provided in-context, elim-\ninating the need for the costly, per-instance fine-tuning of\nprevious methods.\nExtensive experiments validate our IIR-VLM\u2019s capa-\nbilities, showing its clear advantages over existing meth-\nods on instance learning benchmarks [1, 9, 14]. We fur-\nther evaluate our method on our benchmark of challeng-\ning, fine-grained ILR tasks spanning multiple difficulty lev-\nels and categories, including persons, faces, pets, and gen-\neral objects. Strong performance on these tasks promises\nour model\u2019s effectiveness for real-world applications, such\nas smart-home cameras, where distinguishing specific in-\nstances is essential for all-around personalized visual un-\nderstanding.\nWe summarize our contribution as follows:\n(1) We propose a simple and effective method to in-\ntegrate instance-level recognition models as auxiliary vi-\nsual encoders to VLMs, infusing the VLM with powerful\ninstance-discriminating capabilities.\n(2) Based on the new architecture, we train IIR-VLM\nto perform In-context Instance-level Recognition through a\nlightweight, two-stage fine-tuning process, allowing VLMs\nto reliably learn and identify new instances on-the-fly.\n(3) We establish a comprehensive instance-level recog-\nnition (ILR) benchmark for VLMs, spanning varying dif-\nficulty levels and categories. On the benchmark, our IIR-\nVLM outperforms strong baselines and prior specialized\nmethods in both instance matching and instance-aware vi-\nsual understanding.\n2. Related Work\nInstance-level recognition and retrieval.\nInstance-\nlevel recognition (ILR) tasks [10, 19, 27, 30, 31] require\nmodels to identify specific instances from visually simi-\nlar candidates. The most prominent example is person re-\nidentification (Re-ID) [7, 28, 29, 33, 34], which focuses on\nmatching individuals across different views. In terms of\nmethod, most work leverage comparison of instance embed-\ndings, which are obtained by training models on instance\nclassification, and are often jointly optimized with metric\nlearning objectives to enforce separation between features\nof different instances. This core challenge of Re-ID has\nbeen extended to other specific domains, such as pet Re-\nID [21] and vehicle Re-ID [7]. Similarly, instance-level re-\ntrieval tasks are developed for domains like landmarks [19],\nartwork [30], and online products [16], which all share the\ncommon goal of fine-grained instance matching.\nMore recently, the field has explored non-category-\nspecific ILR [10, 31]. These methods aim to generate ro-\nbust identity embeddings for any object, not just those from\na predefined category. UnED [31] proposes to learn a sin-\ngle embedding model for diverse instance retrieval tasks.\nILIAS [10] builds a challenging benchmark for general in-\nstance retrieval, and verifies that pre-training on ILR tasks\nsignificantly improves the performance of general-purpose\nvisual encoders. This finding directly motivates our work,\nas we show effectiveness of an auxiliary ILR expert to sup-\nplement the VLM\u2019s general visual encoder.\nVision-language models and visual encoders.\nLarge\nvision-language models (VLMs) [3, 12, 13] have achieved\nremarkable success in general visual perception, excelling\nat tasks such as generating detailed captions, answering\nopen-ended questions, and performing complex visual rea-\nsoning.\nIn a typical VLM, a visual encoder maps fea-\ntures to the token space of a large language model (LLM)\nvia a projector, serving as the model\u2019s visual perception\nunit. While many different general-purpose visual encoders\nare considered in prior work [3, 12, 13], evidence [23]\nreveals that this choice has a selective impact on down-\nstream performance. Specifically, an encoder\u2019s pretext task,\nsuch as vision-language alignment [20, 25, 32] or self-\nsupervised learning [5, 17], determines its suitability for dif-\nferent VLM tasks. For example, VLMs based on language-\nsupervised encoders perform far better on OCR and chart-\nrelated tasks [23]. Furthermore, features from different en-\ncoders can be aggregated to complement each other and ac-\ncomplish tasks where single encoders fail [23, 24].\nIn this work, we argue that instance-level recogni-\ntion (ILR), much like OCR, is a highly specialized task.\nGeneral-purpose encoders perform poorly on it due to their\nlack of necessary, category-specific fine-grained features.\nWe demonstrate that a VLM\u2019s ILR capabilities can be en-\nhanced by complementing the general encoder with small,\n2\nspecialist models trained specifically for this purpose.\nVLM personalization.\nIn training VLMs to under-\nstand specific instances, existing work mainly relies on\nper-instance training [1, 2, 14]. This approach collects a\nsmall image set for each new instance and then fine-tunes\nthe VLM to associate those visual features with a unique\ninstance token, which the model uses to refer to that in-\nstance. Specifically, MyVLM [1] trains individual concept\nheads that indicate the existence of instances, before decid-\ning to include the instance token as part of its response.\nYo\u2019LLaVA [14] trains the instance token as well as a set\nof description tokens to help the VLM encode crucial in-\nformation of the instance. MC\u2019LLaVA [2] tackles multiple\ninstance appearances. While effective, per-instance train-\ning is considered inefficient. It usually requires not only\nthe curation of per-instance datasets including positive and\nnegative samples, but also additional training for every new\ninstance. Training on single instances also leads to strug-\ngles with fine-grained ILR tasks that require differentiating\nbetween highly visually similar images.\nA few studies, primarily in face understanding, pio-\nneer instance understanding without per-instance training.\nPLVM [18] uses additional general-purpose encoders for\ngallery image encoding. IDA-VLM [9] applies a specif-\nically designed encoder that involves explicit cross-image\ncomparison for face-related movie understanding.\nOur\nwork generalizes these approaches to diverse and challeng-\ning ILR tasks, identifying dedicated ILR-trained encoders\nas the critical component for succeeding at challenging,\nfine-grained instance-level recognition.\n3. Method\nThis paper proposes IIR-VLM, which advances large VLMs\nwith an In-context Instance-level Recognition capability.\nWe achieve this through a dual-component framework. We\nfirst integrate instance-level recognition models as auxiliary\nvisual encoders, which provides category-specific knowl-\nedge for instance discrimination (Sec. 3.2). This enhanced\nVLM is then efficiently fine-tuned in a two-stage process\n(Sec. 3.3): aligning the feature spaces through challenging\nin-context instance matching in Stage 1, and enhancing the\nmodel with instance-aware understanding tasks in Stage 2.\nThe complete pipeline is illustrated in Fig. 1.\n3.1. Preliminary\nA large Vision-Language Model (VLM) combines a visual\nencoder with a Large Language Model (LLM) through a\nprojector, and works to process interleaved visual and text\ninputs to perform various visual perception tasks. Specifi-\ncally, given a series of input text X = (x1, x2, \u00b7 \u00b7 \u00b7 , xn\u22121)\nand image inputs c, VLMs predict\nP(xn|f(c), x1, x2, \u00b7 \u00b7 \u00b7 , xn\u22121),\n(1)\nQuery\nGallery 1\nGallery 2\nGeneral-purpose Visual Encoder\nExpert Encoder\nLarge Language Model\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\nQ: What\u2019s happening in this Query\nimage?  Respond based on information\nprovided by the Gallery.\nA: [Product 1] is shown with its door open, displaying toast inside.\n\u00d7  \ud835\udc34\nEnhanced Token\nExpert Feature\nImage Token\nText Token\nFigure 1. Pipeline of our proposed IIR-VLM. IIR-VLM learns\ninstances in-context from the gallery and performs visual under-\nstanding tasks related to the query. All images are processed by\nboth the original general-purpose visual encoder and an instance-\nlevel recognition expert encoder, which is trained exclusively on\ninstance classification tasks. Expert features are re-weighted by\nan attention score A to indicate instance presence in each original\nfeature token. Re-weighted features are added to the original to-\nkens to provide discriminative features beneficial for ILR tasks.\nwhere f(\u00b7) denotes visual encoders, in order to decode re-\nsponses based on inputs.\nThis paper investigates and advances the capabilities of\nVLMs on the in-context instance-level recognition (ILR)\ntask [19, 27, 30]. ILR can be described as matching images\nthat depict the same instances across diverse categories, in-\ncluding persons, faces, and arbitrary objects. In a typical\nformulation, a query image cq is provided along with a\nset of gallery images (c1\ng, c2\ng, \u00b7 \u00b7 \u00b7 ck\ng), where the goal is to\nidentify which gallery image ci\ng shares the same \u201didentity\u201d,\ni.e., involves the same instance, as cq. Traditional solutions\ninvolve ILR model fe that are trained to extract instance-\ndiscriminating features, and images are matched through\nfeature similarity:\ny = argmaxi(Sim(fe(cq), fe(ci\ng)).\n(2)\nInstance-level recognition, and subsequently visual un-\nderstanding based on instance knowledge, are invaluable for\nVLMs to master, though we show that state-of-the-art mod-\nels perform poorly on the task, significantly underperform-\ning traditional ILR models. We refer to these traditional\n3\nmodels fe as experts, and show how they can be utilized to\nefficiently enhance VLMs on the task.\n3.2. Integrating ILR experts\nIntegration mechanism.\nOur method enhances the\nVLM\u2019s image features by injecting instance-aware features\nfrom ILR models.\nFor each image c containing a tar-\nget instance of the expert, the VLM\u2019s original, general-\npurpose encoder provides a multi-token feature f(c) =\n[f1(c), f2(c), \u00b7 \u00b7 \u00b7 fN(c)], while the expert encoder extracts a\nsingle, highly discriminative one-dimensional identity fea-\nture, fe(c). To integrate the features, fe(c) is first projected\nthrough a trainable MLP into f \u2032\ne(c), and then used as a query\nto calculate an attention score A against every token in f(c):\nAi = SoftmaxN (Sim(fi(c), f \u2032\ne(c))) .\n(3)\nInspired by the positional encoding mechanism [26],\nwhich injects spatial information into Transformer in-\nputs through additive modulation, we treat the attention-\nenhanced feature f \u2032\ne(c) as an \u201didentity embedding\u201d. This\nembedding is added back to the original token features, ef-\nfectively infusing fine-grained instance identity knowledge\ninto the VLM\u2019s visual representation:\nFi(c) = fi(c) + Ai \u00b7 f \u2032\ne(c).\n(4)\nThe original encoder\u2019s multi-token features f(c) are then\nreplaced by the instance-aware feature sequence F(c) =\n[F1(c), F2(c), \u00b7 \u00b7 \u00b7 FN(c)] for all subsequent VLM process-\ning. Multiple experts can be used in parallel to enhance\nimages containing different categories of instances.\nSelection of expert encoders.\nWe empirically show\nthat the identity embedding requires a model explicitly\ntrained on instance discrimination, instead of general visual\nencoders, for the best performance. We utilize off-the-shelf\nmodels for categories that are mature in instance learning,\nsuch as person [7, 35] and face [6]. For categories such\nas pet and household items, where off-the-shelf experts are\nreadily not available, we train a dedicated ILR expert to\nmeet this demand. Following ILIAS [10], we fine-tune a\ngeneral-purpose visual encoder with instance classification\nloss and metric learning losses, specifically a triplet loss that\npulls features of the same instance closer and pushes dif-\nferent instances away [7]. The models are fully-finetuned\nto achieve maximum performance. This step ensures that\nthe resulting encoder possesses the instance-discriminative,\nfine-grained feature that effectively supplements the origi-\nnal visual encoder.\n3.3. Data engine and two-stage training\nDataset formulation.\nThe training of VLMs requires\ndatasets in the format of instruction tuning. To that end, we\nrepurpose traditional instance-level recognition datasets [4,\n11, 16, 28, 29], which feature a large number of distinct in-\nstances and multiple images per instance. These images are\ngrouped and embedded into user-assistant conversations,\nwhere every conversation includes a query image (cq) and\na gallery of candidate images (G = {c1\ng, c2\ng, . . . , cK\ng }) that\npotentially contains matches for the queried instance. At\ninference time, the VLM is provided with the gallery in its\ncontext, and required to understand the query image based\non matching results.\nWhile some instances can be straightforwardly identi-\nfied based on their visual appearance, instance-level recog-\nnition often requires more fine-grained attention to subtle\nvisual cues, particularly when dealing with highly similar\ninstances.\nIn this work, we emphasize the capability of\nperforming challenging fine-grained tasks, which are not\nimmediately solvable by general visual perception. Lever-\naging general visual encoders, we modulate the difficulty\nlevel of the dataset by filling the gallery with strong dis-\ntractors, selected based on visual similarity to the query.\nSpecifically, we require Sim(f(ci\ng), f(cq)) > \u03c4 for some\npre-determined similarity threshold \u03c4.\nTwo-stage fine-tuning with ILR awareness.\nWith\nthe dataset and auxiliary expert encoders, we introduce\na lightweight, two-stage fine-tuning process to embed\nInstance-Level Recognition (ILR) awareness into the VLM.\nThis process minimizes computational cost and mitigates\noverfitting by freezing the parameters of both the LLM and\nthe visual encoders, and training only the projectors.\nStage 1 of the framework aims to establish the VLM\u2019s\nfoundational ability to recognize and distinguish instances\nin-context. The VLM is trained to correctly identify which\nimage in the gallery G matches the identity of the query\nimage cq provided in the prompt, essentially answering a\nmultiple-choice question. We aim to align the expert fea-\ntures with the VLM in this stage for effective knowledge\ntransfer. Stage 2 subsequently improves the model\u2019s capa-\nbility to instance-aware understanding. The VLM is tasked\nwith generating a detailed caption of the query image cq,\nleveraging the identity information present in the gallery G.\nFor example, in a conversation that includes Person 3 of the\ngallery, the model is trained to respond with specific, brack-\neted identity, such as [Person 3] walks into the\nroom wearing a blue shirt. This mechanism al-\nlows the model to anchor its generated text to the identity\nlearned in-context, and the bracketed token can readily be\nreplaced by an instance\u2019s name in practical applications.\n4. Experiments\nWe perform a series of experiments to verify the validity\nof IIR-VLM to learn instances in-context. In Sec. 4.1, we\noutline our experimental setup, including benchmark con-\nstruction, training configuration, and sources of expert en-\ncoders.\nWe evaluate our model on our challenging ILR\n4\n[Product 5] is displayed next to its instruction manual.\n[Person 3] appears to be attending an event or gathering.\n[Person 4] appears to be in a joyful or playful mood\n[Person 2] is wearing a black coat and a pair of black\ntrousers. He is walking with his hands in his pockets.\n[Person 3] with long hair wears a dark blue down\njacket. She is walking with a blue backpack.\n[Person 5] who wears an orange down jacket\nand a pair of blue jeans is walking.\n[Pet 4] is walking across a light-colored surface, passing by a dark object.\n[Pet 2] is standing on a light-colored floor, looking down at something.\n[Product 1] is fully stocked with various decorative items.\nQuery\nGallery\nQuery\nGallery\nFigure 2. Samples of our instance-level recognition benchmark and corresponding model outputs. We cover a wide span of categories,\nincluding household items (object), persons, faces, and pets. In each conversation, models take a group of images as input, where the\nleftmost image represents the query image at task, and five visually similar gallery images follow. Models are required to select the\ngallery image with the matching instance (in green), and generate captions of the query image referring to the correct instance.\nbenchmark in Sec. 4.2, compare it with existing VLM and\nper-instance methods in Sec. 4.3, and demonstrate strong\nresults in both matching and captioning. Finally, through\nablations in Sec. 4.4, we quantify the impact of expert en-\ncoders, task difficulty, and two-stage training on overall per-\nformance.\n4.1. Setup\nDatasets. We build our instruction-tuning training and test\ndata by re-purposing instance-level recognition datasets of\ndifferent domain. We put together datasets spanning four\ncategories, i.e. person, face, pet, and general objects. For\nthe person category, we utilize a set of public person Re-\nID dataset [11, 22, 28, 33, 34], which combines into 76k\nimages. We use the VGGFace [4] dataset for the face cate-\ngory. Around 60k images are collected for the pet category.\nThe Stanford Online Product [16] dataset, which contains\n120k images representing 23k instances across 12 house-\nhold categories, is used for learning general object ILR.\nFor instruction tuning, we select gallery length K = 5\nand similarity threshold \u03c4 = 0.5 by default to provide suf-\nficiently challenging data. All instruction tuning datasets\nare split into training and test sets such that there are no\noverlapping identities between the two splits. The test set,\nfeaturing 500 conversations per instance category, serves as\nthe cornerstone of our evaluation and will be released.\nFinally, we generate captions with bracketed subjects\n(see Sec. 3.3) for all query images using Gemini 2.5 Flash\nand train models to perform captioning in the second stage.\nImplementation details.\nWe use Qwen2.5-VL-3B-\nInstruct [3] as our base model. During training, the LLM\nand the original visual encoders are both kept frozen. Only\nprojectors between all encoders to the LLM are updated.\nImage inputs are presented as is to the VLM\u2019s original vi-\nsual encoder; however, when target instance exists, images\nare adjusted and potentially cropped to meet input require-\nments for expert encoders.\nDedicated ILR experts are selected based on cate-\ngory. We use off-the-shelf person re-identification models\nPLIP [35] for the person category, and the Arcface [6] face\nrecognition model for the face category. Experts for pet and\ngeneral object are fine-tuned from general-purpose visual\nencoders [17] on instance classification tasks.\n4.2. Challenging instance-level recognition\nFig. 2 provides representative examples from our instance-\nlevel recognition benchmark along with outputs from our\nmodel. The visually challenging nature of the benchmark is\nhighlighted by galleries containing highly similar negative\nsamples. Many test cases are challenging even to the human\neye at first glance; however, correct decisions can be made\nby focusing on subtle details. The model\u2019s success on these\ntasks is therefore determined by its ability to capture these\ncategory-specific, fine-grained details hidden in the images.\n5\nTable 1. Accuracy (%) of four different VLMs on our bench-\nmark, which tests models\u2019 capability on challenging instance-level\nrecognition (ILR) tasks across four categories.\nObject\nPerson\nFace\nPet\nAvg\nGemini 2.5 Pro\n89.8\n75.4\n72.8\n79.6\n79.4\nQwen2.5 VL-3B\n54.0\n52.6\n65.4\n49.6\n55.4\nQwen2.5 VL-7B\n70.8\n73.6\n84.8\n68.4\n74.4\nIDA-VLM [9]\n59.6\n42.0\n75.0\n51.4\n57.0\nStage 1 models: trained for recognition.\nOurs w/o Expert\n89.6\n90.4\n91.4\n86.7\n89.5\nOurs w/ Expert\n92.9\n92.6\n91.4\n88.3\n91.3\nStage 1 + 2 models: trained for recognition and caption.\nOurs w/o Expert\n89.4\n81.6\n85.8\n90.9\n86.9\nOurs w/ Expert\n91.2\n84.4\n86.2\n92.2\n88.5\nAs the samples and corresponding outputs demonstrate,\nour VLM successfully handles this complexity, consistently\nidentifying the correct matching instance (marked in green).\nFurthermore, the model validates the second stage train-\ning by producing captions for the query images (marked\nin black).\nThe generated captions are consistently cor-\nrect visual descriptions that cater to the instance\u2019s imme-\ndiate status, context, and background, rather than recit-\ning pre-existing knowledge of the instance.\nThe model\nalso correctly utilizes the bracketed subject (e.g., [Person\n3]), a format that allows for fast replacement of real,\nuser-specified names.\nIn real-world applications such as\nsmart-home visual assistants, the model can be provided in-\ncontext with a gallery of family members, important items,\npets, etc., and be expected to describe captured scenes based\non accurate knowledge of instances to remember.\nWe systematically evaluate the performance of VLMs on\nour four-category instance-level recognition (ILR) bench-\nmark.\nWe compare our IIR-VLM with state-of-the-art\nthe VLMs Gemini 2.5 Pro, Qwen2.5-VL-3B [3] (our base\nmodel), Qwen2.5-VL-7B [3], as well as IDA-VLM [9], a\nrecent specialized VLM proposed for instance learning and\nmovie understanding. We report the accuracy of finding the\ncorrect instance in Tab. 1.\nAmong the general state-of-the-art models, Gemini\n2.5 Pro scores the highest average baseline performance,\ndemonstrating particular strength in the object category,\nreaching 89.8%. The open-source Qwen2.5-VL models ex-\nhibit strong performance across both model sizes. Notably,\nthe 7B variant significantly improves accuracy, securing the\nsecond rank among the compared methods, directly follow-\ning the proprietary Gemini model.\nIDA-VLM [9] showcases its ILR capability by perform-\ning promisingly on the face category, scoring 75.0% and\nnotably surpassing Gemini. However, it is evidently less ca-\npable of generalizing its ILR capability to other categories,\nTable 2. Instance-level recognition accuracy (%) on the matching\ntasks of MM-ID [9], compared with IDA-VLM [9]\nFace\nAnimal\nBuilding\nVehicle\nAvg\nIDA-VLM [9]\n82.1\n58.3\n100\n30.0\n74.0\nIIR-VLM (Ours)\n92.9\n66.7\n100\n60.0\n85.1\nresulting in a lower overall average.\nWe separately report the performances of four variants of\nour VLMs, namely models trained with one or two stages,\nand models trained with and without experts. Training on\nmultiple-choice questions in Stage 1 provides maximized\nILR results, bringing the average accuracy to above 90%.\nTraining for the second stage enables a crucial ability to per-\nform visual understanding with instance knowledge. The\nnew capability incurs a modest performance drop of ap-\nproximately 2.2% on average, indicating an expected trade-\noff between maximizing ILR capabilities and general vi-\nsual understanding with instance knowledge. Nevertheless,\neven after this trade-off, our full model with the expert en-\ncoder maintains a strong average score of 88.5%, surpassing\nour base Qwen2.5-VL-3B model by 33.1% and leading all\nstate-of-the-art VLMs tested.\nAcross both stages, our results demonstrate the necessity\nand value of the expert encoder. VLMs trained with ILR\nexperts as auxiliary encoders consistently outperform their\ncounterparts that rely only on the original encoder across\nall categories. Quantitatively, the expert boosts the aver-\nage Stage 1 accuracy from 89.5% to 91.3%. By supple-\nmenting the general-purpose visual encoder with category-\nspecific knowledge on instance discrimination, the expert\nencoders allow for a strong instance-discriminating capabil-\nity, and establishes a new state-of-the-art for this instance-\nlevel recognition task.\n4.3. Comparison with existing solutions\nMatching tasks. In Tab. 2, we compare with IDA-VLM [9]\non their proposed MM-ID benchmark. MM-ID is a pioneer-\ning benchmark that aims to evaluate VLMs on memoriz-\ning and utilizing identity information of visual inputs, with\nparticular focuses on face understanding in movie scenes.\nWe test IIR-VLM on the matching task of MM-ID. De-\nspite not having trained on movie-relevant data, our method\nachieves superior accuracy across all evaluated categories,\ndemonstrating a promising generalization capability to var-\nious instance-level recognition problems.\nWe further compare IIR-VLM with per-instance training\nmethods MyVLM [1] and Yo\u2019LLaVA [14] on their bench-\nmark. Both benchmarks evaluate a model\u2019s ability to de-\ntect the presence of a specific, pre-trained concept within\na new image, reporting accuracy on both positive (concept\npresent) and negative (concept absent) samples. In our for-\n6\nTable 3. Comparison with per-instance training methods. Results\nindicates the accuracy (%) on correctly identifying the existence\nof specific instances in test images.\nPositive\nNegative\nWeighted\nMyVLM\u2019s benchmark.\nMyVLM [1]\n96.6\n90.9\n93.8\nYo\u2019LLaVA [14]\n97.0\n95.7\n96.4\nIIR-VLM (Ours)\n87.4\n90.5\n88.8\nYo\u2019LLaVA\u2019s benchmark.\nGPT-4V\n80.9\n99.2\n90.1\nYo\u2019LLaVA [14]\n94.9\n89.8\n92.4\nIIR-VLM (Ours)\n89.0\n97.4\n92.8\nTable 4. Comparison with MyVLM on instance-aware caption\ngeneration, with CLIP-Image measuring text-image alignment and\nCLIP-Text measuring similarity to ground truth captions.\nExamples\nCLIP-Image\nCLIP-Text\nMyVLM\n1\n24.20\n57.37\nMyVLM\n2\n24.91\n61.01\nMyVLM\n4\n25.42\n62.61\nIIR-VLM (Ours)\n1\n27.06\n69.81\nmulation, the task is analogous to a gallery size K = 1\nsetting. We train a variant of our model which does not as-\nsume the existence of query images in the gallery to meet\nthe demand of the task. Instead of utilizing multiple im-\nages to learn instance-specific tokens, our model learns all\nconcepts with a single image in-context.\nAs shown in Tab. 3, IIR-VLM achieves a weighted score\nof 92.8% on Yo\u2019LLaVA\u2019s benchmark, a result compara-\nble to Yo\u2019LLaVA\u2019s own score and outperforming GPT-4V.\nOn MyVLM\u2019s benchmark, our model pushes towards the\nscores of MyVLM [1] and Yo\u2019LLaVA [14], though a small\ngap remains. This gap stems largely from a lower accuracy\non positive samples, where a learned instance exists in the\nqueried image. This suggests that our model is more con-\nservative, exhibiting a higher tendency to deny a match even\nwhen the correct instance is present. We hypothesize that\nthis is a result of its training on our challenging ILR tasks,\nwhich has conditioned it to be highly sensitive to subtle vi-\nsual differences.\nWe emphasize that these comparable results are achieved\nwithout any costly per-instance training, which would re-\nquire complex data curation and training procedure for each\ninstance to learn. This strongly validates IIR-VLM, our in-\ncontext learning approach, as a more flexible and efficient\nparadigm. It demonstrates a clear potential to replace per-\ninstance training, especially as models scale and are trained\non more comprehensive, diverse ILR data.\nTable 5. Accuracy (%) on instance-level recognition tasks of vi-\nsual encoders and VLMs equipped with different encoders.\nModel\nAuxiliary Encoder\nStage 1\nStage 1+2\nILR with Embedding Similarity\nDINOv2\n\u2013\n58.5\n\u2013\nExpert\n\u2013\n91.4\n\u2013\nILR with VLM.\nVLM\nNone\n89.6\n89.4\nVLM\nDINOv2\n91.1\n90.2\nVLM\nExpert\n92.9\n91.2\nCaptioning tasks. We showcase the effectiveness of our\nStage 2-trained models in producing accurate captions for\nmatched instances, in comparison with existing work. Re-\nsults are shown in Tab. 4. Following MyVLM [1], generated\ncaptions are evaluated using CLIPScore [8], which mea-\nsures the alignment of the generated caption and the query\nimage (CLIP-Image). We also evaluate the caption\u2019s fidelity\nto the ground truth using the CLIP text encoder (CLIP-\nText).\nDespite learning instances in-context with only a\nsingle example image, our generated captions score higher\nthan MyVLM on both metrics, signaling accurate descrip-\ntion of the scene. When combined with successful instance-\nlevel recognition, the model is fully capable of identifying\nthe correct subject and performing identity-aware visual un-\nderstanding tasks.\n4.4. Ablation study\nWe study the effect of both critical components of our\nframework, namely introducing ILR expert as auxiliary en-\ncoders and two-stage training. We strengthen the argument\nof applying experts by testing the framework on varying dif-\nficulty levels of our ILR benchmark.\nVLM with expert encoders. We support the necessity\nof our dual-encoder design with ablation study in Tab. 5.\nWe study the performance on object [16] ILR tasks of\nstandalone visual encoders and VLMs with varying vi-\nsual encoder design. For a simple visual encoder like DI-\nNOv2 [17], we resort to traditional Re-ID protocols. We\napply it for instance-level recognition by extracting features\nand locating the gallery image with the highest similarity,\nas described in Sec. 3.1.\nIn order to have an ILR expert on general object, we\nfine-tune a general-purpose visual encoder, DINOv2 [17],\nwith instance classification and metric learning losses. We\nshow that the ILR-trained expert dramatically outperforms\nthe general-purpose DINO encoder, improving from 58.5%\nto above 90%. This confirms that while general-purpose vi-\nsual encoders extract rich semantics from visual inputs, they\nare insufficient for fine-grained ILR tasks. We study how\nthe features of these different encoders impact the VLM\u2019s\n7\n70\n75\n80\n85\n90\n95\nAccuracy (%)\n0.2\n0.5\n0.8\nDifficulty\n+0.8\n+2.2\n+3.1\nWith Expert\nWithout Expert\nFigure 3. Comparison between models with and without ILR ex-\npert encoder across different difficulty levels. Numbers on the\nright indicate the relative accuracy improvements.\nperformance as auxiliary encoders."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models\"\n\n#### Main Contributions:\n1. **Enhanced VLMs with Instance-Level Recognition (ILR) Capability**: The paper proposes IIR-VLM, a large vision-language model enhanced to perform in-context instance-level recognition (ILR). This capability allows the model to learn and recognize new instances on-the-fly, addressing limitations of existing models that require per-instance training.\n\n2. **Integration of ILR Experts as Auxiliary Visual Encoders**: The authors integrate instance-level recognition expert models as auxiliary visual encoders within VLMs. These experts provide specialized features for learning diverse instances, enabling the VLM to learn new instances in-context with minimal data collection and training costs. \n\n3. **Two-Stage Lightweight Training Process**: IIR-VLM employs a lightweight two-stage fine-tuning process that aligns feature spaces through challenging instance matching in Stage 1 and enhances understanding tasks in Stage 2. This approach minimizes computational cost and mitigates overfitting by freezing parameters during training.\n\n#### Methodology:\n1. **Preliminary Component**: The model combines the VLM's original visual encoder with expert-level recognition models, integrating specialized features for instance discrimination through a trainable multi-layer perceptron (MLP) and attention mechanism.\n\n2. **Two-Stage Training Process**:\n   - **Stage 1**: Aligns feature spaces by training the model to identify which gallery image matches the query image.\n   - **Stage 2**: Enhances understanding tasks by training the model to generate detailed captions using instance-level knowledge from the gallery, thereby leveraging new instance-level learning.\n\n#### Key Findings:\n- IIR-VLM outperforms state-of-the-art models on existing ILR benchmarks across multiple categories including persons, faces, pets, and general objects.\n- The addition of expert encoders significantly improves performance by up to 2.2% in Stage 1 accuracy, demonstrating the effectiveness of this approach.\n\n#### Conclusions:\nIIR-VLM demonstrates superior instance-level recognition capabilities compared to existing models that rely solely on per-instance training. By integrating ILR experts as auxiliary visual encoders and employing a two-stage lightweight fine-tuning process, IIR-VLM achieves robust performance across various challenging tasks without the need for extensive data collection or per-instance training.\n\nThe proposed framework not only enhances VLMs' ability to recognize instances in-context but also establishes a comprehensive benchmark for future research. This work paves the way for more personalized and effective visual understanding systems, particularly beneficial for applications such as smart-home cameras where distinguishing specific instances is crucial for personalized visual understanding."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Proceedings of Machine Learning Research \u2013 Under Review:1\u201318, 2026\nFull Paper \u2013 MIDL 2026 submission\nGeneralizing Abstention for Noise-Robust Learning in\nMedical Image Segmentation\nWesam Moustafa1,2\nwesam.h.moustafa@gmail.com\nHossam Elsafty1,2,3\nhossam.elsafty@iais.fraunhofer.de\nHelen Schneider1,2,3\nhelen.schneider@iais.fraunhofer.de\nLorenz Sparrenberg1,3\nsparrenberg@bit.uni-bonn.de\nRafet Sifa1,2,3\nrafet.sifa@iais.fraunhofer.de\n1 Institute of Computer Science, University of Bonn, Bonn, Germany\n2 Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS, Sankt Augustin,\nGermany\n3 Lamarr Institute for Machine Learning and Artificial Intelligence, Dortmund, Germany\nEditors: Under Review for MIDL 2026\nAbstract\nLabel noise is a critical problem in medical image segmentation, often arising from the\ninherent difficulty of manual annotation.\nModels trained on noisy data are prone to\noverfitting, which degrades their generalization performance. While a number of methods\nand strategies have been proposed to mitigate noisy labels in the segmentation domain,\nthis area remains largely under-explored. The abstention mechanism has proven effective\nin classification tasks by enhancing the capabilities of Cross Entropy, yet its potential\nin segmentation remains unverified. In this paper, we address this gap by introducing a\nuniversal and modular abstention framework capable of enhancing the noise-robustness of\na diverse range of loss functions. Our framework improves upon prior work with two key\ncomponents: an informed regularization term to guide abstention behaviour, and a more\nflexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate\nthe framework\u2019s versatility by systematically integrating it with three distinct loss functions\nto create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS\nand DSAD medical datasets show our methods consistently and significantly outperform\ntheir non-abstaining baselines, especially under high noise levels. This work establishes\nthat enabling models to selectively ignore corrupted samples is a powerful and generalizable\nstrategy for building more reliable segmentation models. Our code is publicly available at\nhttps://github.com/wemous/abstention-for-segmentation.\nKeywords: Abstention, Medical Image Segmentation, Label Noise, Noise-Robust Learning,\nLoss Functions.\n1. Introduction\nThe remarkable advancements in deep learning have revolutionized numerous fields, largely\npropelled by the availability of labelled datasets (Karimi et al., 2020; Zhang et al., 2020;\nGarcia-Garcia et al., 2017). However, the presence of label noise is a significant impediment\nto the generalizability of Deep Neural Networks (DNNs), as their immense capacity makes\nthem prone to memorizing incorrect labels, which harms their ability to generalize to unseen\ndata (Lienen and H\u00a8ullermeier, 2024; Gonz\u00b4alez-Santoyo et al., 2025; Schneider et al., 2024,\n2023a).\nThis problem is especially pronounced in medical image segmentation, where\n\u00a9 2026 CC-BY 4.0, W. Moustafa, H. Elsafty, H. Schneider, L. Sparrenberg & R. Sifa.\narXiv:2601.14039v1  [cs.CV]  20 Jan 2026\nMoustafa Elsafty Schneider Sparrenberg Sifa\nobtaining clean, pixel-level annotations is notoriously difficult and expensive, and where\nannotation errors can have direct clinical consequences (Karimi et al., 2020; Zhang et al.,\n2020; Xu et al., 2024). Training on such noisy labels leads to incorrect gradients, causing\nthe model to learn erroneous patterns and fail in critical applications (Marcinkiewicz and\nMrukwa, 2019).\nTo counteract label noise, a variety of robust learning methodologies have been developed,\nprimarily for classification tasks. These include noise filtering techniques (Gonz\u00b4alez-Santoyo\net al., 2025), loss reweighting strategies (Karimi et al., 2020), and curriculum learning (Lienen\nand H\u00a8ullermeier, 2024). While promising, these methods often introduce computational\ncomplexity or require strong assumptions about the noise characteristics (Gonz\u00b4alez-Santoyo\net al., 2025). Among the explored directions, noise-robust loss functions are a compelling\nalternative due to their simplicity, efficiency, and model-agnostic nature (Staats et al., 2025).\nBy leveraging properties like boundedness (Zhang and Sabuncu, 2018) and symmetry (Wang\net al., 2019), they modify the optimization objective to inherently limit the influence of\nnoisy examples and prevent overfitting (Toner and Storkey, 2023; Ding et al., 2024).\nDespite these advances, a research gap remains for robust learning specifically in image\nsegmentation, where existing methods often struggle to address the spatially-correlated\ninaccuracies inherent in annotation noise (Guo et al., 2025; Karimi et al., 2020). In this paper,\nwe propose to address this gap by adapting and generalizing the abstention mechanism, a\npowerful technique that has proven effective in mitigating label noise in classification (Karimi\net al., 2020; Thulasidasan et al., 2019; Schneider et al., 2025). The abstention mechanism\nempowers a DNN to abstain from making a prediction on confusing or unreliable samples\nby integrating an abstention option directly into the training process. Building upon the\nfoundational Deep Abstaining Classifier (DAC) (Thulasidasan et al., 2019) and its extension,\nthe Informed Deep Abstaining Classifier (IDAC) (Schneider et al., 2025), this paper makes\nseveral contributions to advance noise-robust medical image segmentation:\n\u2022 Adaptation of Abstention to Segmentation: We investigate the applicability of\nthe abstention mechanism to image segmentation by adapting the DAC and IDAC\nloss functions for this domain.\n\u2022 Enhanced and Generalized Abstention Definition: Our contribution improves\nand generalizes abstention, incorporating an informed regularization term guided by\nestimated noise rates \u02dc\u03b7 and a power-law-based \u03b1 auto-tuning algorithm.\n\u2022 Loss-Agnostic Integration and Novel Loss Functions: We integrate the enhanced\nabstention mechanism with other loss functions, including Generalized Cross Entropy\n(GCE) (Zhang and Sabuncu, 2018), Symmetric Cross Entropy (SCE) (Wang et al.,\n2019), and Dice Loss (Milletari et al., 2016), introducing three loss functions: the\nGeneralized Abstaining Classifier (GAC), the Symmetric Abstaining Classifier (SAC),\nand the Abstaining Dice Segmenter (ADS). ADS introduces architectural adaptations\nfor class-wise abstention and class-specific noise rates \u02dc\u03b7c.\n\u2022 Empirical Validation of Robustness and Versatility: Through empirical evalua-\ntions and quantitative analysis (Figure 1) on medical image datasets (CaDIS (Gram-\nmatikopoulou et al., 2022) and DSAD (Carstens et al., 2023)) under varying noise\nlevels, we show consistent superiority over non-abstaining baselines.\n2\nGeneralizing Abstention\nThe remainder of this paper reviews related work (Section 2), details our proposed abstention\nframework and novel loss functions (Section 3), outlines the experimental setup (Section 4),\nand presents a comprehensive evaluation of our results (Section 5) before concluding in\nSection 6.\n(a) Ground Truth\n(b) Dice\n(c) ADS\nFigure 1: The impact of our noise-robust abstention framework. On a CaDIS sample with 25% label\nnoise, the baseline Dice Loss (b) produces a noisy and inaccurate mask. In contrast, our proposed\nAbstaining Dice Segmenter (ADS) (c) yields a result that is visually cleaner and adheres more\nclosely to the ground truth (a).\n2. Related Work\nThe deployment of deep learning in medical imaging is frequently hampered by data\nimperfections, ranging from data scarcity and class imbalance (Tomar et al., 2025a,b) to\nmissing or uncertain annotations (Schneider et al., 2023b). Within this landscape, label noise\nremains a particularly pervasive challenge that has been explored through several avenues\n(Karimi et al., 2020). Early strategies operated at different granularities, seeking to identify\nand correct noisy labels. These included pixel-wise adaptive weight maps, as proposed by\nShi and Wu (2021), which dynamically adjust the contribution of each pixel to the loss,\nand graph-based label correction by Yi et al. (2022), which models spatial relationships\nto propagate corrections from reliable to unreliable pixels. Other frameworks operated at\nthe image level to assess overall annotation quality, with some combining both pixel- and\nimage-level perspectives to distill supervision more effectively (Shi and Wu, 2021; Zhu et al.,\n2019). Recognizing that segmentation noise is often not random but spatially correlated,\nother works have proposed explicit noise models. The Markov models from Yao et al. (2023),\nfor instance, simulate realistic boundary distortions, while methods like LVC-Net by Shu\net al. (2019) leverage local visual cues to guide the network away from incorrect labels during\ntraining.\nA significant approach exploits the intrinsic learning dynamics of deep networks, par-\nticularly the \u2018early-learning\u2019 phenomenon, where models tend to fit clean, simple patterns\nbefore eventually memorizing the noise present in incorrect labels (Liu et al., 2022; Ye et al.,\n2024). This observation has led to the development of adaptive correction methods like\nADELE by Liu et al. (2022), which detects the onset of memorization for each semantic\nclass to intervene at the optimal moment. In a similar vein, multi-network and co-training\nparadigms leverage the consensus or disagreement between two or more models to filter out\nnoisy signals. By using diverse architectures, these methods reduce the risk of confirmation\nbias, where a single model reinforces its own errors (Li et al., 2023; Rong et al., 2023).\n3\nMoustafa Elsafty Schneider Sparrenberg Sifa\nFurthermore, emerging paradigms reframe the problem by treating large-scale noisy\nlabels not as a hindrance but as a valuable resource. Pretraining strategies use massive,\nimperfectly labelled datasets to learn robust feature representations that can be fine-tuned\non smaller, clean datasets (Liu et al., 2025). Other methods use meta-learning to bootstrap\nrobust models, such as L2B by Zhou et al. (2024), which learns to dynamically weight the\ninfluence of observed labels and model-generated pseudo-labels during training.\nAn alternative and more fundamental approach that is most relevant to our work involves\ndesigning inherently robust loss functions. Instead of relying on external modules for noise\ndetection or correction, this strategy embeds noise tolerance directly into the optimization\nobjective (Karimi et al., 2020). Examples include the T-Loss from Gonzalez-Jimenez et al.\n(2023), which is based on the heavy-tailed Student-t distribution to reduce the influence of\noutliers, and the Active Negative Loss (ANL) framework proposed by Ye et al. (2024). Our\nwork contributes to this line of research, but with a distinct and more modular philosophy.\nInstead of designing a new loss function from scratch, we propose a modular mechanism\nthat can enhance the inherent robustness of existing losses.\nA promising strategy for mitigating the impact of label noise is to empower a model to\nabstain from making a prediction on samples it deems unreliable. This approach circumvents\nthe core problem of standard supervised learning, where a model is forced to commit to a\nprediction, potentially leading it to memorize erroneous labels. The concept was formally\nintroduced for deep learning in the Deep Abstaining Classifier (DAC) by Thulasidasan et al.\n(2019). The DAC framework enables abstention by augmenting a network\u2019s architecture\nwith an additional (k +1)-th output neuron, which explicitly represents the choice to abstain.\nIts corresponding loss function is defined as:\nLDAC(xj) = (1 \u2212pk+1)\n\u2212\nk\nX\ni=1\nti log\npi\n1 \u2212pk+1\n!\n+ \u03b1 log\n1\n1 \u2212pk+1\n(1)\nThe loss is composed of two competing terms. The first is a modified Cross Entropy\n(CE) loss scaled by (1 \u2212pk+1), which represents the confidence in not abstaining, while the\nclassification probability pi for each class i is re-normalized by the same factor. The second\ncomponent is a regularization term that directly penalizes the act of abstaining, where pk+1\nis the model\u2019s predicted probability of abstention (Thulasidasan et al., 2019).\nThe abstention penalty is controlled by the hyperparameter \u03b1. Crucially, DAC employs\nan adaptive auto-tuning schedule where, after an initial warm-up period, \u03b1 is initialized to\na small value and is linearly increased over the remaining epochs up to a predefined value,\n\u03b1final. This ramp-up strategy, detailed in Appendix A, acts as a form of curriculum learning,\ninitially permitting the model to ignore noisy samples and progressively forcing it to learn\nfrom more challenging data as its confidence grows (Thulasidasan et al., 2019).\nBuilding directly upon this foundation, Schneider et al. (2025) proposed the Informed\nDeep Abstaining Classifier (IDAC) to create a more targeted response to noise. IDAC refines\nthe abstention mechanism by incorporating an a priori estimation of the dataset\u2019s noise\nlevel, \u02dc\u03b7, directly into its regularization term. The IDAC loss functions is defined as:\nLIDAC(xj) = (1 \u2212pk+1)\n\u2212\nk\nX\ni=1\nti log\npi\n1 \u2212pk+1\n!\n+ \u03b1(\u02dc\u03b7 \u2212\u02c6\u03b7)2\n(2)\n4\nGeneralizing Abstention\nThe key innovation lies in replacing DAC\u2019s incremental penalty with a term that\nminimizes the divergence between the expected noise rate \u02dc\u03b7 and the model\u2019s current batch-\nwise abstention rate, \u02c6\u03b7. This provides a more direct supervisory signal, guiding the model to\nabstain on a fraction of samples that is consistent with the known level of label corruption\n(Schneider et al., 2025).\nWhile DAC and IDAC have demonstrated the profound effectiveness of abstention, their\napplication has been confined to the CE loss paradigm. Our work addresses this limitation by\nproposing a generalized abstention framework, establishing it as a modular tool to enhance\nthe robustness of a diverse range of loss functions.\n3. The Universal Abstention Framework\nBuilding upon the demonstrated efficacy of DAC and IDAC in mitigating label noise through\nabstention, we propose an enhanced and universal definition of the abstention mechanism that\ncan be readily adapted to virtually any underlying loss function, LX(xj). Our generalized\nabstaining loss is formulated as:\nLabstention(xj) = (1 \u2212pk+1)LX(xj) + \u03b1\nlog\n1 \u2212\u02dc\u03b7\n1 \u2212pk+1\n(3)\nWith this formulation, we introduce two critical innovations designed to provide greater\nflexibility and more targeted noise mitigation.\n3.1. Informed Regularization\nThe first improvement lies in the regularization term \u03b1\nlog\n1\u2212\u02dc\u03b7\n1\u2212pk+1\n. This term draws\ninspiration from IDAC by explicitly incorporating the expected noise rate \u02dc\u03b7 to guide the\nabstention behaviour. Unlike DAC, which pushes the abstention probability pk+1 toward\nzero, our term incentivises the model to maintain pk+1 in proximity to \u02dc\u03b7. This allows the\nmodel to continue abstaining on samples it confidently perceives as noisy, rather than being\nforced to make classification decisions that could elevate the risk of overfitting to noise. This\nenhanced definition of the regularization term is also flexible; if a reliable estimate for \u02dc\u03b7\nisn\u2019t available, setting \u02dc\u03b7 = 0 effectively reduces the term to its original DAC form, which\nhas already demonstrated its strength and effectiveness in combating label noise.\n3.2. Power-Law Auto-Tuning\nThe second and more significant enhancement concerns the \u03b1 auto-tuning algorithm.\nThe original algorithm proposed by DAC employed a linear ramp-up strategy for \u03b1 after a\nwarm-up phase, which, while effective, offered limited flexibility in controlling the learning\ntrajectory. Our refined approach replaces this with a simpler yet more powerful and flexible\nmethod. For every epoch e after an initial warm-up phase of L epochs out of a total E\nepochs, \u03b1 is dynamically calculated as:\n\u03b1 = \u03b1final \u2217\n\u0012 e \u2212L\nE \u2212L\n\u0013\u03b3\n(4)\n5\nMoustafa Elsafty Schneider Sparrenberg Sifa\nIn this equation, \u03b3 > 0 serves as a growth factor that precisely controls the rate at which \u03b1\nincreases throughout the abstention phase, as depicted in Appendix B.1. The behaviour\nof \u03b1 is modulated by \u03b3: if \u03b3 > 1 , \u03b1 exhibits a sublinear growth, increasing slowly at the\nbeginning of the abstention period and accelerating its growth towards the end of training.\nThis behaviour intensifies with larger values of \u03b3. Conversely, if \u03b3 < 1, \u03b1 experiences\nsuperlinear growth early in the abstention phase, with its rate of increase slowing down\nas training progresses. Setting \u03b3 = 1 yields a linear increment, akin to DAC\u2019s approach.\nThis formulation provides significant flexibility in penalizing and guiding the abstention\nbehaviour, enabling a more optimal balance between the model\u2019s learning from clean data\nand its strategic abstention from noisy or ambiguous samples.\n3.3. Novel Abstaining Loss Functions for Segmentation\nWe demonstrate our framework\u2019s versatility by creating three novel, noise-robust loss\nfunctions, one of which is tailored for segmentation.\n3.3.1. Abstaining Classifiers (GAC and SAC)\nWe first integrate our framework with two CE-based losses. The Generalized Abstaining\nClassifier (GAC) combines abstention with Generalized Cross Entropy (GCE) (Zhang and\nSabuncu, 2018), creating a dual defence where GCE\u2019s bounded loss attenuates noise on\nclassified samples, while abstention filters out the most corrupted ones. The Symmetric\nAbstaining Classifier (SAC) enhances Symmetric Cross Entropy (SCE) (Wang et al., 2019),\nempowering the model to completely disengage from highly suspect samples, rather than\nmerely re-balancing their influence. SAC can actively filter out the most egregious noisy\nexamples, allowing the symmetrical CE-RCE components to focus on refining predictions\nfor the more reliable data.\n3.3.2. Abstaining Dice Segmenter (ADS)\nOur most significant adaptation is the Abstaining Dice Segmenter (ADS)1, which integrates\nour framework with the region-based Dice loss. This required two fundamental architectural\nchanges to resolve the incompatibility between Dice\u2019s class-wise nature and standard pixel-\nwise abstention.:\n\u2022 Class-wise Abstention Head: We re-conceptualized the network\u2019s output to produce\nclass-wise abstention predictions. As illustrated in Figure 2(b), a specialized module\nuses Adaptive Average Pooling with an output size w \u2217w, followed by a Linear layer\nand sigmoid activation to output a unique abstention probability for each of the k\nclasses.\n\u2022 Class-specific Regularization: To complement the class-wise abstention, the reg-\nularization term in Equation (3) is modified to use a vector of class-specific noise\nestimates, \u02dc\u03b7c, instead of a global noise estimation \u02dc\u03b7. This provides granular control\nover the abstention behaviour for each class, which is crucial for complex medical\nsegmentation tasks.\n1. The Segmenter in ADS highlights its design for segmentation tasks, contrasting with the other \u2019Classifier\u2019\nlosses which can also be used for classification.\n6\nGeneralizing Abstention\nLast hidden layer\nOutput layer\n(a) Pixel-wise abstention\nAbstention head\nLast hidden layer\nSegmentation head\nAdaptive Average Pool\nLinear\n(b) Class-wise abstention\nFigure 2: Transforming the output layer from standard pixel-wise abstention (a) to our proposed\nclass-wise abstention head for ADS (b). The dimensions b, c, h, w represent batch size, number of\nclasses, height, and width, respectively. For the Adaptive Average Pool layer, w, w is the output size\n4. Experimental Setup\nWe validated our framework on two publicly available surgical datasets with distinct charac-\nteristics to demonstrate the generalizability of our approach.\n\u2022 Cataract Dataset for Image Segmentation (CaDIS): A benchmark featuring 4,670\nframes from cataract surgery with dense, high-quality pixel-wise annotations (Gram-\nmatikopoulou et al., 2022). For our experiments, we utilized the 8-class variant, which\ngroups surgical tools into a single class. Images were normalized and resized to 480x256.\n\u2022 Dresden Surgical Anatomy Dataset (DSAD): A more complex benchmark with 1,430\nframes from laparoscopic surgery (Carstens et al., 2023). This dataset presents a\ngreater challenge due to its intricate anatomical structures, sparse annotations (approx.\n82% background), and significant class imbalance. Images were normalized resized to\n480x384.\nTo rigorously assess noise robustness, we simulated realistic annotation errors with a\ntwo-pronged approach. Structural noise was introduced via morphological transformations\n(erosion and dilation) to simulate boundary inaccuracies, while semantic noise was injected\nvia stochastic label flipping to mimic annotator bias (Karimi et al., 2020; Zhang et al., 2020;\nMarcinkiewicz and Mrukwa, 2019; Li et al., 2023). We evaluated performance across five\ncalibrated noise levels for each dataset: 5-25% corruption for CaDIS and 3-15% for DSAD.\nWe used an NVIDIA A100 80GB to train an U-Net model (Ronneberger et al., 2015)\nwith a pretrained ResNet-50 (He et al., 2016) backbone for our experiments. Key training\nhyperparameters are detailed in Appendix C.1. To ensure statistical reliability, each ex-\nperiment was conducted five times with distinct random seeds. Hyperparameters for each\nloss function were optimized to yield the highest validation mean Intersection over Union\n(mIoU) on the highest noise level for each dataset, thereby maximizing noise resistance.\nCrucially, to ensure a fair comparison and rigorously isolate the impact of our abstention\nframework, the optimal hyperparameters found for the baseline GCE and SCE functions\nwere deliberately held constant for their respective abstaining versions, GAC and SAC.\nWhile likely suboptimal for our novel functions, this methodology ensures that any observed\n7\nMoustafa Elsafty Schneider Sparrenberg Sifa\nperformance gains are attributable solely to the abstention mechanism itself. Appendix C.2\ndetails the hyperparameters we used for each loss in our benchmarks.\n5. Evaluations and Visual Analysis\nThe efficacy of our proposed framework was validated through extensive experiments on the\nCaDIS and DSAD datasets. The results, summarized in Table 1 and Figure 3, demonstrate\nthe superior noise-robustness conferred by the abstention mechanism. As anticipated, all\nloss functions exhibited a performance decline with increasing label noise, underscoring\nthe universal challenge of learning from corrupted data. However, the critical distinction\nlies in the rate of this degradation. Our proposed abstaining loss functions consistently\ndemonstrated a more graceful performance decline and maintained a significant advantage\nover their respective non-abstaining baselines, particularly at high noise intensities. On\nthe CaDIS dataset at 25% noise, our Abstaining Dice Segmenter (ADS) emerged as the\ntop performer, achieving a 5.35% mIoU lead over the standard Dice Loss. Similarly, GAC\nand SAC surpassed their baselines, confirming the broad applicability of our framework.\nThis trend persisted on the more complex DSAD dataset, where despite lower overall mIoU\nscores, the abstaining variants maintained a clear and consistent performance advantage,\nhighlighting their robust effectiveness even in challenging segmentation scenarios. The flatter\ndegradation curves for the abstaining methods in Figure 3 highlight their superior resilience.\nTable 1: Average test mIoU (%) and standard deviation across 5 runs of a U-Net model trained on\nCaDIS and DSAD datasets. Gray background: Abstaining loss functions. (*): Our proposed\nnovel loss functions. Structure: The table is divided into four comparative groups (separated by\ndouble vertical lines); each group compares a baseline loss against its abstaining counterpart(s).\nBold: Indicates the best result within that specific group. For example, in the last group, we compare\nDice vs. ADS to isolate the impact of our framework on the Dice loss.\nDataset\nNoise rate\n\u03b7 (%)\nLoss function\nCE\nDAC\nIDAC\nGCE\nGAC*\nSCE\nSAC*\nDice\nADS*\nCaDIS\n0\n76.02\u00b10.70\n75.29\u00b10.79\n75.36\u00b10.73\n73.49\u00b13.27\n73.76\u00b12.80\n75.38\u00b10.75\n75.83\u00b10.62\n76.52\u00b10.47\n77.04\u00b10.37\n5\n73.67\u00b11.03\n73.14\u00b10.46\n72.89\u00b10.41\n72.83\u00b11.11\n71.73\u00b12.79\n73.41\u00b10.71\n73.51\u00b11.59\n73.48\u00b10.28\n75.22\u00b10.85\n10\n66.39\u00b10.17\n67.43\u00b10.49\n66.92\u00b10.49\n64.82\u00b10.86\n64.16\u00b12.57\n65.92\u00b10.91\n67.29\u00b11.65\n66.51\u00b10.61\n71.12\u00b10.55\n15\n64.15\u00b12.47\n65.85\u00b11.05\n64.87\u00b10.91\n64.81\u00b10.46\n64.44\u00b12.70\n62.16\u00b11.99\n65.48\u00b12.11\n67.31\u00b10.73\n70.80\u00b11.08\n20\n59.56\u00b11.21\n63.42\u00b10.87\n60.54\u00b12.27\n60.73\u00b11.41\n60.91\u00b11.64\n57.62\u00b14.22\n62.70\u00b10.31\n63.64\u00b10.82\n68.88\u00b10.49\n25\n52.27\u00b11.70\n60.63\u00b12.73\n58.19\u00b14.77\n55.71\u00b11.30\n59.46\u00b10.76\n55.08\u00b10.93\n61.27\u00b11.22\n61.04\u00b11.41\n66.39\u00b10.67\nDSAD\n0\n34.25\u00b12.50\n34.01\u00b10.96\n33.60\u00b10.72\n35.14\u00b11.65\n32.26\u00b10.53\n32.78\u00b11.19\n33.86\u00b11.83\n31.28\u00b10.87\n30.09\u00b11.10\n3\n33.69\u00b11.85\n33.67\u00b12.01\n32.76\u00b12.03\n33.84\u00b12.56\n32.94\u00b12.23\n32.11\u00b11.09\n30.90\u00b12.76\n30.83\u00b14.78\n28.64\u00b12.76\n6\n30.70\u00b12.47\n29.47\u00b11.97\n29.11\u00b12.10\n29.69\u00b11.96\n29.78\u00b14.27\n30.51\u00b12.16\n31.55\u00b12.43\n28.56\u00b11.00\n30.48\u00b13.61\n9\n24.65\u00b12.90\n24.58\u00b12.61\n23.47\u00b12.48\n22.95\u00b12.93\n28.84\u00b14.17\n28.02\u00b12.37\n28.55\u00b11.29\n19.04\u00b11.92\n26.23\u00b12.05\n12\n21.00\u00b13.15\n22.59\u00b14.35\n20.94\u00b11.86\n19.84\u00b12.89\n25.00\u00b14.13\n21.57\u00b10.67\n23.73\u00b10.68\n16.15\u00b11.49\n22.63\u00b10.51\n15\n14.41\u00b12.59\n17.69\u00b13.97\n16.24\u00b11.45\n14.12\u00b12.91\n20.01\u00b12.56\n15.31\u00b10.75\n15.91\u00b13.53\n14.65\u00b11.50\n18.05\u00b11.63\nA qualitative review of the segmentation masks, depicted in Figures 4 and 5 further\nsubstantiates these quantitative gains. Visual inspection of the CaDIS results revealed that\nmodels trained with our abstaining losses produced markedly cleaner and more accurate\nsegmentations than their baselines. Contours were sharper, noise artifacts were reduced,\nand overall structural coherence was improved. Most notably, ADS produced masks with\n8\nGeneralizing Abstention\nthe highest similarity the ground truth even at the highest noise level. While the inherent\ndifficulty of the DSAD dataset resulted in lower-quality predictions across all methods, the\nsame relative improvements were observed. The abstaining models consistently generated\nmore coherent masks with fewer spurious predictions and better-defined boundaries compared\nto their non-abstaining counterparts. This visual evidence confirms that the improvements\nin mIoU translate directly to more reliable and clinically relevant segmentation outputs.\n(a) CaDIS Dataset\n(b) DSAD Dataset\nFigure 3: Quantitative comparison of noise-robustness. The plots show the average test mIoU (%)\ndegradation as label noise \u03b7 increases. The flatter curves of the abstaining variants (solid lines)\ndemonstrate their superior resilience compared to non-abstaining baselines (dashed lines). Our\nproposed losses (GAC, SAC, ADS) are in bold.\n(a) Ground truth\n(b) CE\n(c) DAC\n(d) IDAC\n(e) GCE\n(f ) GAC\n(g) SCE\n(h) SAC\n(i) Dice\n(j) ADS\nFigure 4: Qualitative comparison on a CaDIS sample at 25% noise. Our proposed abstaining losses\n(GAC, SAC, ADS) produce masks with higher fidelity and fewer artifacts than their baselines.\nAbstaining losses are in bold.\n9\nMoustafa Elsafty Schneider Sparrenberg Sifa\n(a) Ground truth\n(b) CE\n(c) DAC\n(d) IDAC\n(e) GCE\n(f ) GAC\n(g) SCE\n(h) SAC\n(i) Dice\n(j) ADS\nFigure 5: Qualitative comparison on a challenging DSAD sample at 15% noise. Abstaining variants\n(in bold) yield masks with better structural coherence and fewer spurious activations than their\nbaselines.\n6. Conclusions\nIn this paper, we have established that the abstention mechanism is not merely a classifica-\ntion trick, but a fundamental and robust computer vision strategy whose capabilities are\nexceptionally beneficial for medical image segmentation. By introducing a universal frame-\nwork equipped with informed regularization and dynamic auto-tuning, we have successfully\ngeneralized abstention to function with diverse loss paradigms, including the region-based\nDice loss. This modularity allows for the creation of specialized noise-robust loss functions\nwithout requiring complex, computationally expensive changes to the underlying model\narchitecture.\nOur extensive validation on the CaDIS and DSAD datasets confirms that this approach\nyields consistent and significant improvements over standard baselines, particularly in high-\nnoise situations where traditional models fail. The ability of the ADS model to produce\nclean, anatomically coherent masks even when trained on 25% corrupted labels highlights\nthe practical value of allowing models to selectively ignore unreliable supervision.\nWhile our results are promising, we acknowledge that our evaluation relied on synthetic\nnoise injection, which serves as a controlled proxy for the complex, structured ambiguity often\nfound in clinical annotations. Furthermore, the current framework relies on a pre-estimated\nnoise rate hyperparameter. Future work will focus on validating this framework on datasets\nwith naturally occurring inter-rater variability and developing adaptive, data-driven methods\nto estimate the noise rate dynamically. Ultimately, this work provides a scalable pathway\ntoward building trustworthy diagnostic systems that can robustly learn from the imperfect\ndata realities of the medical domain.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation\"\n\n#### Main Contributions:\n- **Universal and Modular Abstention Framework**: The paper introduces a universal abstention framework capable of enhancing the noise-robustness of diverse loss functions. This framework includes two key components: an informed regularization term to guide abstention behavior, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty.\n\n#### Methodology:\n- **Adaptation to Segmentation**: The authors adapt existing abstention mechanisms (Deep Abstaining Classifier - DAC and Informed Deep Abstaining Classifier - IDAC) specifically for image segmentation tasks.\n- **Enhanced Abstention Definition**: They propose a more generalized definition of the abstention mechanism, incorporating an informed regularization term guided by estimated noise rates \\( \\tilde{\\eta} \\), and a power-law-based \u03b1 auto-tuning algorithm.\n- **Loss-Agnostic Integration**: The framework is integrated with various loss functions including Generalized Cross Entropy (GCE), Symmetric Cross Entropy (SCE), and Dice Loss, resulting in three new noise-robust variants: Generalized Abstaining Classifier (GAC), Symmetric Abstaining Classifier (SAC), and Abstaining Dice Segmenter (ADS).\n- **Empirical Validation**: The framework\u2019s robustness is empirically validated on two medical datasets (CaDIS and DSAD) under varying levels of label noise. Results demonstrate consistent superiority over non-abstaining baselines, especially at high noise levels.\n\n#### Key Findings:\n- **Noise Robustness**: The proposed abstention mechanisms consistently outperform their non-abstaining counterparts across different datasets and noise levels.\n- **Versatility**: The framework is versatile enough to integrate with various loss functions without requiring complex architectural changes.\n- **Clinical Relevance**: The ADS model demonstrates the practical value of allowing models to selectively ignore unreliable supervision, producing cleaner masks even when trained on corrupted data.\n\n#### Conclusions:\nThe paper establishes that enabling models to abstain from making predictions on noisy or ambiguous samples is a powerful and generalizable strategy for building more reliable segmentation models. The proposed framework provides a modular tool to enhance the robustness of diverse loss functions in medical image segmentation tasks. While promising, future work will focus on validating this approach with real-world datasets and developing adaptive methods to estimate noise rates dynamically.\n\n### Summary Key Points:\n- **Universal Framework**: A universal abstention mechanism for various loss functions.\n- **Enhanced Regularization and Auto-Tuning**: Improved regularization term and power-law-based auto-tuning algorithm.\n- **Novel Loss Functions**: Three new noise-robust variants (GAC, SAC, ADS).\n- **Empirical Validation**: Superior performance on CaDIS and DSAD datasets under varying noise levels.\n- **Clinical Relevance**: Demonstrates practical value in medical applications by allowing models to abstain from unreliable supervision."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Deep Learning Approaches to Quantum Error\nMitigation\nLeonardo Placidi1,4,5, Ifan Williams2, Enrico Rinaldi3,6, Daniel Mills2,\nCristina C\u02c6\u0131rstoiu2, Vanya Eccles2, and Ross Duncan2\n1Quantinuum K.K., Otemachi Financial City Grand Cube 3F 1-9-2 Otemachi, Chiyoda-ku\nTokyo, Japan, 1000004\n2Quantinuum Ltd., Terrington House, 13\u201315 Hills Road, Cambridge CB2 1NL, United\nKingdom\n3Quantinuum Ltd., Partnership House, Carlisle Place, London SW1P 1BX, United Kingdom\n4The University of Osaka, Graduate School of Engineering Science, 1-2, Machikaneyama,\nToyonaka, Osaka 560-8531, Japan\n5QIQB Center for Quantum Information and Quantum Biology, The University of Osaka, 1-2\nMachikaneyama, Toyonaka, Osaka 560-0043, Japan\n6Center for Quantum Computing, RIKEN, 2-1 Hirosawa, Wako, Saitama 351-0198, Japan\nJanuary 21, 2026\nAbstract\nWe present a systematic investigation of deep learning methods applied to\nquantum error mitigation of noisy output probability distributions from measured\nquantum circuits. We compare different architectures, from fully connected neural\nnetworks to transformers, and we test different design/training modalities, iden-\ntifying sequence-to-sequence, attention-based models as the most effective on our\ndatasets. These models consistently produce mitigated distributions that are closer\nto the ideal outputs when tested on both simulated and real device data obtained\nfrom IBM superconducting quantum processing units (QPU) up to five qubits.\nAcross several different circuit depths, our approach outperforms other baseline\nerror mitigation techniques. We perform a series of ablation studies to examine:\nhow different input features (circuit, device properties, noisy output statistics)\naffect performance; cross-dataset generalization across circuit families; and trans-\nfer learning to a different IBM QPU. We observe that generalization performance\nacross similar devices with the same architecture works effectively, without needing\nto fully retrain models.\n1\nIntroduction\nQuantum error correction enables exponential noise suppression [1\u20133], and has been\npartially demonstrated at small scales [4\u20138]. However practical-scale, fault-tolerant ap-\nplications remain out of reach. Quantum error mitigation (QEM) can enhance the utility\nof noisy quantum hardware [9\u201311], and may be used to mitigate the effect of residual\nlogical noise when using error correction on devices with limited sizes [12]. QEM does\nnot improve quantum state fidelity, but instead reduces noise bias in estimating quan-\ntities of interest. There are a variety of QEM methods, but they typically have similar\nworkflows [13] that require minimal increase in circuit size, and rely mostly on classical\n1\narXiv:2601.14226v1  [quant-ph]  20 Jan 2026\npost-processing of noisy samples from many different circuits. As a result, QEM requires\nexponentially more samples [14, 15].\nMachine learning (ML) has been applied to QEM in several ways [16].\nML can\nmimic existing error mitigation schemes with reduced overhead [17], it can guide which\nmodified circuits are run [18, 19], or can be used to train a neural network to map noisy\noutput samples to error-mitigated estimators [17, 20\u201328]. For instance, Clifford data\nregression [20, 28] uses classically simulatable circuits derived from the target circuit to\nlearn a linear map from noisy to ideal expectation values. Other supervised learning\napproaches use feed-forward neural networks to mitigate expectation values for quantum\nsimulation [21, 23, 24] or combinatorial optimization [22].\nSupervised ML has also been used for mitigating noise in the outcome probabilities.\nThe main motivation to address this task is its greater generality: all observables in the\ncomputational basis can be determined from the full distribution over outcomes. While\nwe can expect scalability bottlenecks at smaller systems than in the case of mitigating\nexpectation values, this setting provides a worst-case benchmark to stress-test ML for\nQEM. Previous studies used ML to correct complex non-linear readout noise that is not\nmitigated by standard state preparation and measurement (SPAM) correction methods\n[27]. Other works [25] used shallow random circuits to train a neural network to map\ninputs, including the noisy distribution, error rates, circuit depth, and the numbers\nof single- and two-qubit gates, to a correction for each outcome probability.\nWhile\npromising, these approaches are often demonstrated on restricted datasets and simple\nmodels, leaving open questions about how architectural choices influence generalizability\nand robustness, particularly on time-varying hardware data.\nIn this article, we perform a systematic study investigating the use of deep learn-\ning models to mitigate noise in quantum circuit outcome probabilities. Envisioning a\npipeline where models are pre-trained and then continuously refined based on expanded\ndatasets, we train the models using noisy simulation data, fine-tune using real hardware\ndata, and also perform transfer learning to data run on another quantum device. We\nuse a dataset of over 246, 000 unique circuits, which includes extensive hardware data\ncollected over a period of time of several months. This is a markedly larger and more\ndiverse dataset than has been used in prior work, enabling our models to account for\ntime drift and noise fluctuations. The dataset collection is facilitated by Quantinuum\nNexus [29].\nWe use two classes of 5-qubit circuits for training and testing: circuits\nconsisting of random gates, inspired by random circuit sampling [30]; and circuits con-\nsisting of random Pauli gadgets [31], motivated by dynamical simulation. The dataset\ngeneration is described in Section 2.\nWe compare a much wider selection of deep learning architectures and training meth-\nods than have been investigated to date, including models based on two output types:\nprediction models, which output the mitigated probability distribution, and correction\nmodels, which output corrections to the noisy probability distributions in the form of an\nelement-wise convolution. For each dataset, we separately optimize the models through\nextensive hyperparameter searches, as described in Section 3. Our study shows that\nthe prediction models perform best, and that attention-based models are particularly\neffective, improving the accuracy of the probability distributions for up to \u223c80% of\nthe circuits in our real hardware test sets. We also compare the strategy learned by our\nmodels to simple error mitigation schemes that modify the noisy probability distribution\nusing the available device noise data (without requiring additional circuit-dependent er-\nror characterization experiments). Our models outperform these approaches across a\nrange of circuit depths. The full comparative analysis results are shown in Section 4.\nThe models take the following input features: an encoding of the circuit; the noisy\nprobability distribution; and a characterization of the device (taken at the most recent\nround of device calibration). To probe how the inputs affect the performance of each\n2\nmodel, we perform ablation studies that selectively randomize different inputs.\nWe\ndemonstrate that the noisy probability distribution is by far the most important feature\nused by the models, with circuit and backend information acting only as secondary\nconditioning factors. Furthermore, removing the circuit information does not lead to\ndegradation in performance for most of the models (i.e recurrent neural networks [RNNs]\nor attention-based), indicating they have not learned to simply simulate the original\ncircuit.\nWe also perform transfer learning studies between different classes of circuits and\ndifferent hardware devices in Section 5. Among the architectures we studied, generative\nattention-based models are the most robust to dataset and hardware shifts. Models\ntrained with measurements from one device (ibm algiers) transfer effectively to an-\nother (ibm hanoi), without the need to retrain from scratch, as long as the hardware\ncharacterization data remains comparable. This may be explained by the importance of\nnoisy output statistics on performance. Transferring between random and Pauli circuit\ntypes is less successful, demonstrating that the models are dependent on the circuit type\ndue to differences in their output distributions.\nFinally, we emphasize that this study is limited to quantum circuits with few qubits,\nfor which the true outcome probability distributions are known. This restriction allows\nus to isolate the effects of input features, training strategy, and model architectures in\na controlled setting. Consequently, our analysis focuses on how these different architec-\ntural choices influence robustness and generalization. Some of the insights above may\npersist at scale, but otherwise would have been difficult to separate from scaling aspects.\nIn particular, the exponential dimensionality of the full probability distributions, and\nresolving its discriminating noise features with limited finite samples, will pose signifi-\ncant scaling challenges. However, our results suggest guiding principles to improve the\narchitectural design of ML-based error mitigation. In Section 6 we provide a detailed\ndiscussion of scalability constraints and suggest how to address these in future work by\nadapting our analysis to quantities like marginal distributions or local observables.\n2\nData\nIn this section, we discuss the data used to train our ML models, describing how our\ninput data and labels are generated and post-processed. The main features that we use\nare the following:\n\u2022 C - an encoding of the circuit to be run, described in Section 2.1;\n\u2022 B - the QPU properties and error calibration data, described in Section 2.2;\n\u2022 Pnoisy - the noisy output probability distribution of the circuit.\nPideal, a circuit\u2019s ideal probability distribution obtained from classical simulation, is\nused as a label in the supervised training process to compute the loss function and\nevaluate performance. Pnoisy, and Pideal, are described in Section 2.3.\n2.1\nCircuits\nWe use two application-motivated classes of circuits [13, 32].\nThese are both built\niteratively in blocks, which we call time steps, t, (which differ between circuit classes),\nup to a maximum time step, T. We vary T to obtain circuits with many different depths\nand gate numbers:\nPauli Synthetic versions of the product formula circuits seen in Hamiltonian simulation\nand quantum chemistry [33\u201335]. At each time step, a Pauli gadget [31] of the form\n3\nexp(\u2212i\u03b1P), where P \u2208{X, Y, Z, I}n, and the angle, \u03b1, are randomly chosen, is ap-\nplied to the circuit. The maximum time steps considered were T \u2208{3, 4, 5, 6, 7, 9}.\nSee Algorithm 1 for the complete description.\nRandom These are random circuits, seen in applications such as random circuit sam-\npling [36]. At each time step, a gate is randomly selected from the IBM native\ngateset\nG = {X, SX, Rz(\u03b1), CX},\n(1)\nand applied to a randomly selected qubit, or qubits in the case of the two-qubit\nCX gate. The maximum time steps considered are T \u2208{48, 64, 80, 96, 112, 144}.\nSee Algorithm 2 for the complete description.\nWe use Quantinuum\u2019s tket compiler [37] to compile each circuit onto the native\nconnectivity and gate set of IBM devices. No further optimization is performed. Each\ncircuit is then converted into a 3D array, Carray, of shape, (nl, nqubits, |G|) = (nl, 5, 5).\nEach of the nl layers in the array has an associated nqubits \u00d7 |G| matrix, with each\nqubit representing a row in this matrix (indexed starting from 1), and each operation a\ncolumn. The total number of layers depend on T and on the type of circuit, due to the\nfact that each layer may have multiple parallel operations on nqubits qubits. We use an\nencoding of each of the possible gates, as specified in Table 1. To create a homogeneous\ndataset of pre-defined shapes, Carray is padded with nqubits \u00d7 |G| zero matrices in order\nto be the same shape as the Carray of the maximal depth circuit with the largest T\nvalue.\nGate type\nEncoding\nX\n(1, 0, 0, 0, 0)\nSX\n(0, 1, 0, 0, 0)\nRz(\u03b1)\n(0, 0, (\u03b1 (mod 2))/2, 0, 0)\nCX (Control)\n(0, 0, 0, \u2212(1+(target-qubit index)), 0)\nCX (Target)\n(0, 0, 0, 1+(control-qubit index), 0)\nMeasure\n(0, 0, 0, 0, 1)\nIdle\n(0, 0, 0, 0, 0)\nTable 1: Encoding scheme for 5-qubit circuits compiled into the IBM native\ngateset G given in Eq. (1). \u03b1 is a real number representing an angle in units of \u03c0.\n2.2\nDevice Information\nWe used two 27-qubit superconducting transmon devices from IBM, ibm algiers and\nibm hanoi, of the same quantum chip generation. The backend information, B, consists\nof a bundle of parameters, retrieved via a public API [38], giving the calibrated properties\nof the QPU at a particular time:\nQubit properties: Qubit frequencies, and the T1 and T2 decay times.\nInstruction properties: Gate times, gate error rates, and measurement error rates.\nWe select a fixed subset of five qubits of the device, as shown in Fig. 1a. We discard\nthe parameters that refer to unused parts of the QPU, leaving 101 relevant parameters,\nwhich we store as a 1D array, B. In Figs. 1b and 1c we show the distribution of device\nproperties in our data set for both ibm algiers and ibm hanoi. These distributions are\n4\n6\n17\n0\n1\n4\n7\n10\n12\n15\n18\n21\n23\n2\n13\n24\n3\n5\n8\n11\n14\n16\n19\n22\n25\n26\n9\n20\n(a) Topology of ibm algiers\nand ibm hanoi. The dashed\nline encloses the five qubits\nused in this article.\n0\n50\n100\n150\n200\n250\n300\n350\nT1 [ s]\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\nDensity\nibm_algiers\nibm_hanoi\n0\n100\n200\n300\n400\n500\nT2 [ s]\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\nDensity\nibm_algiers\nibm_hanoi\n0.02\n0.04\n0.06\n0.08\n0.10\nReadout Error\n0\n20\n40\n60\n80\n100\nDensity\nibm_algiers\nibm_hanoi\n(b) Probability density functions of coherence times T1, T2\nand readout errors for ibm algiers and ibm hanoi, averaged\nacross qubits.\n10\n2\n10\n1\n100\n101\nCX Error [%]\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nDensity\nQubit Edge (0,1)\nibm_algiers\nibm_hanoi\n10\n2\n10\n1\n100\n101\nCX Error [%]\nQubit Edge (1,2)\nibm_algiers\nibm_hanoi\n10\n1\n100\n101\nCX Error [%]\nQubit Edge (2,3)\nibm_algiers\nibm_hanoi\n100\n101\nCX Error [%]\nQubit Edge (3,4)\nibm_algiers\nibm_hanoi\n(c) Probability density functions of CX error rates between each pair of\nqubits using a logarithmic scale.\nFigure 1: Device properties used as an input feature B for our models. Densities\nare normalised across backend instances and include temporal variations in calibration\ndata between periodic circuit executions.\ngenerated by collecting calibration data from the first online date of the device until the\ndate of the data-taking period, with an approximate daily frequency. Therefore, they\nare a collective representation of how device characteristics change with time and also\nwith space (on the device). We observe that the two-qubit error rates on edges (0, 1) and\n(1, 2) have distributions with only small overlap between the two devices, while other\ncalibration data is more similarly distributed.\n2.3\nOutput Probability Distributions\nWe have two kinds of datasets based on the aforementioned circuit types and devices:\none composed of simulated data, denoted as \u201cSimulated\u201d, and one composed of real\ndata, denoted as \u201cReal\u201d. In the Simulated datasets, Pnoisy is obtained from a noisy\nclassical simulation with the noise model parameters given by a randomly chosen B for\nibm algiers, collected at some random point in time. In the Real datasets, Pnoisy is\nobtained from running the circuit C on a real quantum device, and B stores the relevant\ncalibration data provided by the API at runtime.\nEach circuit in the Real datasets is run three different times (repeats), with the\nrepeats spaced out in time in a staggered fashion during the data-taking period (January-\nApril 2024). For Simulated datasets, the different repeats are achieved by sampling the\ndevice calibration data at different points in time (across the entire life span of the\nmachines) when building the noise models.\nThis ensures that the datasets contain\nsufficient variability to effectively characterize the correlation between B and the true\nnoise on the device, independent of the correlation of the circuit structure with the\nnoiseless output.\n15, 000 unique circuits are run per T value,1 using batches of 300\ncircuits with 20, 000 shots per circuit, with submission spaced out across the entire\n1Note that for ibm algiers not all jobs completed before the machine was retired, and therefore the\ntotal numbers of unique circuits ended up being less than this for some T values.\n5\n40\n60\n80\n100\n120\n140\nNumber of layers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nMetric\nNoise\nSignal\n(a) Pauli Simulated data.\n40\n60\n80\n100\n120\n140\nNumber of layers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nMetric\nNoise\nSignal\n(b) Pauli Real data.\n40\n60\n80\n100\n120\n140\nNumber of layers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nMetric\nNoise\nSignal\n(c) Random Simulated data.\n40\n60\n80\n100\n120\n140\nNumber of layers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nMetric\nNoise\nSignal\n(d) Random Real data.\nFigure 2: Noise and signal as a function of the number of layers in the circuit\nfor ibm algiers data.\ndata-taking period. In the case of Simulated data, 8, 000 unique circuits are run per T\nvalue, and we again run 20, 000 shots of each circuit on the noisy simulator, with each\ncircuit also run for three repeats. See Appendix A.1 for complete details on the number\nof circuits of each type.\nQualitatively, we expect error mitigation to work in a regime where the circuit in-\ncurs a significant level of noise, but not too much that it completely dominates the\ncomputation. To quantify this in our datasets, we refer to the Kullback-Leibler [39]\n(KL) divergence between the ideal and noisy distribution, KL(Pideal, Pnoisy), as noise \u2013\nand to the KL-divergence between the ideal distribution and the uniform distribution,\nKL(Pideal, Puniform), as signal. In Fig. 2 we show the signal and noise as a function of\ncircuit layers, a proxy for the number of two-qubit gates, which is the dominating source\nof noise in the systems.\nOf note in Figs. 2 and 3 is that the signal in Random circuits is smaller on average\nthan for Pauli circuits. This is to be expected because Random circuits converge more\nquickly to a Porter-Thomas distribution [40], whereas Pauli circuits tend to produce\nmore peaked distributions, on average. We expect this to impact the performance of\nour mitigation methods, because at a lower signal it will be more difficult for the model\nto extract discriminating features in Pnoisy.\nThe second feature of note is the region where signal and noise are similar in value,\nwhich is to say the region where the ideal distribution is as comparably close to the\nnoisy distribution as it is to the uniform distribution, or where the noisy distribution\nhas become uniform. In this region, we expect QEM to perform less effectively, because it\nwill be hard to extract useful information from Pnoisy. From Fig. 2, this threshold occurs\nafter approximately 80 layers (when running both types of circuits on real hardware).\n6\n20\n40\n60\n80\n100\n120\n140\nNumber of layers\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nSignal\n(a) Pauli Simulated data.\n20\n40\n60\n80\n100\n120\n140\nNumber of layers\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nSignal\n(b) Pauli Real data.\n20\n40\n60\n80\n100\n120\n140\nNumber of layers\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nSignal\n(c) Random Simulated data.\n20\n40\n60\n80\n100\n120\n140\nNumber of layers\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nSignal\n(d) Random Real data.\nFigure 3: Scatter plots of signal against the number of layers for ibm algiers\ndata. The signal value is divided into six equally-spaced bins, whose edges are repre-\nsented by dashed gray lines. Here we plot 10,000 points randomly sampled from each\ntest dataset.\nHowever, for Random circuits, there is still a significant overlap, even for a smaller\nnumber of layers.\nFinally, from Fig. 3 we see that the number of circuits with high signal is relatively\nsmall, and concentrated at lower numbers of layers, for both types of circuit. As such,\nwhile we expect the models to perform well on average for high-signal circuits, we would\nalso expect the variance to be higher.\n3\nMachine Learning Models and Training\nWe detail the ML models that we investigate in Section 3.1, and the training pipeline\nthat we use in Section 3.2. Our overall experiment design is outlined in Fig. 4.\n3.1\nModels\nThroughout this subsection we use the following shorthand:\n\u2022 xCB \u2208R3701 denotes the flattened circuit\u2013backend feature vector (i.e. circuit and\nbackend information).\n\u2022 XCB \u2208Rnl\u00d7132 denotes the embedded layer-wise circuit\u2013backend sequence.2\n2This representation includes a small zero padding at the end of the input vector, padding 126 real\nfeatures up to a fixed size of 132. The padded dimensions carry no information and are ignored by the\nmodels.\n7\nQPU\nEncode Circuit Structure\nNoisy Probabilities\nError Parameters\n|0i\nH\n|0i\n|0i\n|0i\nRZ(2\u2713)\n|0i\nH\nRZ(2\u21b5)\nH\nQUANTUM CIRCUIT\nMITIGATED\nPROBABILITY\nDISTRIBUTION\nML MODEL\nPnoisy\nPmit\nEVALUATION\n|0i\nH\n|0i\n|0i\n|0i\nRZ(2\u2713)\n|0i\nH\nRZ(2\u21b5)\nH\n|0i\nH\n|0i\n|0i\n|0i\nRZ(2\u2713)\n|0i\nH\nRZ(2\u21b5)\nH\n|0i\nH\n|0i\n|0i\n|0i\nRZ(2\u2713)\n|0i\nH\nRZ(2\u21b5)\nH\n|0i\nH\n|0i\n|0i\n|0i\nRZ(2\u2713)\n|0i\nH\nRZ(2\u21b5)\nH\n|0i\nH\n|0i\n|0i\n|0i\nRZ(2\u2713)\n|0i\nH\nRZ(2\u21b5)\nH\n|0i\nH\n|0i\n|0i\n|0i\nRZ(2\u2713)\n|0i\nH\nRZ(2\u21b5)\nH\nPRE-TRAINING + FINE-TUNING\nQPU\nNOISY\nEMULATION\nOR\nCPU\nPmit\nLOSS\nFUNCTION\nUpdate\nmodel\nCIRCUITS\nPideal\nML ARCHITECTURE\nc\nModel outputs correction\n\u229bPnoisy\nPmit\nMODEL\nRNN\nTF/TFM\nNOISY ENC\nPERCEIVER\nMLP\nENC\nPmit\nFigure 4: ML pipeline for mitigating errors in a quantum circuit\u2019s output\nprobability distribution.\n\u2022 Pnoisy \u2208R32 denotes the noisy probability distribution.\n\u2022 Pmit \u2208R32 denotes the mitigated probability distribution.\nWe categorize the models we train into two main classes, prediction and correc-\ntion, according to their output type. All model architectures, training workflows, and\nhyperparameters are detailed in Section B.\nPrediction Models\nPrediction models output the mitigated probability distribution as\nPmit = Model (C, B, Pnoisy) .\n(2)\nThese models aim to learn a direct mapping between the noisy and ideal outcomes. The\nfollowing architectures are explored:\nMultilayer Perceptron (MLP-PREDICTION): A baseline feedforward architec-\nture [41] that processes circuit, backend, and noisy output distribution features\njointly. The input vectors for the three features are first flattened and concatenated\nto give\nz = [xCB; Pnoisy] \u2208R3701+32.\n(3)\nThis is then passed through a sequence of k dense layers, hk, with batch normal-\nization (BN) and non-linearities\nh0 = z,\nhk+1 = \u03c3(BN(Wkhk + bk)) ,\nk = 0, 1, . . . , K \u22121.\n(4)\nwhere \u03c3 is an element-wise activation function, Wk is the weight matrix of layer\nk, and bk its corresponding bias vector. This is then passed to a final linear layer,\n8\nfollowed by a softmax3 layer over the 32-dimensional outcome space of the target\nnoiseless distribution\nPmit = softmax(Wouthfinal + bout).\n(5)\nThe MLP thus implements the direct mapping\n(xCB, Pnoisy) 7\u2212\u2192Pmit,\n(6)\nwithout sequential modelling, attention, or latent-space aggregation.\nCompared to the models that follow, the MLP applies non-linear function approx-\nimations to flattened inputs only, and does not leverage the inherent structure of\nquantum circuits. Despite its architectural simplicity, the MLP provides an es-\nsential baseline for quantifying the benefit of the more structured inductive biases\npresent in the following models. Further details are given in Section B.1.\nRecurrent Neural Network (RNN): A two-stream RNN architecture [41, 42] that\nencodes C, B, and Pnoisy separately, then fuses their representations to predict a\nmitigated 32-dimensional distribution.\nThe circuit and backend information are represented as a embedded sequence over\nnl layers such that XCB \u2208Rnl\u00d7132, which is processed by the first RNN stream\nHcirc = RNNcirc(XCB) \u2208Rnl\u00d7h,\n(7)\nand its final hidden state retained hcirc = Hcirc[nl] \u2208Rh. In parallel, the noisy\noutput distribution is processed by a second RNN stream, which directly produces\na hidden representation\nhnoisy = RNNnoisy(Pnoisy) \u2208Rh.\n(8)\nThis representation is concatenated with the final hidden state of the circuit\u2013backend\nstream, and mapped to a probability vector via a fully connected layer, followed\nby a softmax layer\nPmit = softmax\n\u0000W [hcirc; hnoisy] + b\n\u0001\n,\n(9)\nwhere W and b are again weight matrices and bias vectors.\nThus, the RNN\nimplements\n(XCB, Pnoisy) 7\u2212\u2192Pmit,\n(10)\nin a way that explicitly models depth- or time-dependent correlations in the circuit\nand backend, while conditioning on the noisy output distribution. Further details\nare provided in Section B.2.\nTransformer (TF): An encoder-decoder transformer model featuring an attention\nmechanism [43]. The input data consists of XCB alongside Pnoisy. Because the en-\ncoder and decoder operate on different feature dimensions, the architecture must\nbe adapted to allow the model to predict a single 32-dimensional mitigated distri-\nbution.\nThe encoder, Enc, processes the circuit sequence using standard self-attention\nmechanisms to capture global correlations among circuit layers, gates, and backend\n3The softmax function is defined component-wise as softmax(y)i = eyi/(P\nj eyj ).\n9\nfeatures. The encoder output is then projected into the 32-dimensional decoder\nfeature space via a linear map fc\nHenc = Enc(XCB) \u2208Rnl\u00d7denc,\n\u02dcHenc = fc(Henc) \u2208Rnl\u00d732,\n(11)\nwhere denc is the latent-space dimension. The decoder, Dec, takes as input the\nnoisy output distribution as a length-one sequence, and using causal masking,\ngenerates a hidden representation conditioned on the encoded circuit\nHdec = Dec\n\u0010\nP(1\u00d732)\nnoisy , \u02dcHenc\n\u0011\n\u2208R1\u00d732.\n(12)\nA final fully-connected (FC) [41] linear layer, h, maps this hidden state to a prob-\nability vector\nPmit = softmax\n\u0010\nh(Hdec)\n\u0011\n.\n(13)\nIn summary, the full TF model implements the same transformation as in Eq. (10),\nwhere the encoder and decoder operate in different feature spaces and are con-\nnected via the learned projection fc. Further details are given in Section B.4.\nTransformer with SPAM Mitigation (TFM): The TFM variant extends the pre-\nvious TF model by injecting SPAM-mitigated (which will be introduced in Sec-\ntion 4.2) intermediate distributions directly into the decoder, alongside the noisy\noutput distribution. This means that the decoder receives a multi-token sequence\n(e.g. [Pnoisy, m1], where m1 indicates the Spam-mitigated sequence), allowing the\nmodel to condition using previously mitigated data, while still leveraging the en-\ncoder\u2019s circuit-level representation. The structure of the encoder is unchanged,\nbut the decoder input is amplified to operate on a sequence encoding that has\npreviously been mitigated. This variant is also discussed in Section B.4.\nNoisy Encoder (NOISY ENC): An encoder-only model that operates solely on the\nnoisy output distribution. In the encoder, Pnoisy is treated as a length-one se-\nquence, embedded, and then passed through a number of self-attention layers,\nfollowed by a linear softmax layer that produces a mitigated distribution\nPmit = NoisyEnc(Pnoisy).\n(14)\nThis architecture does not use explicit circuit and backend information, and does\nnot have a separate decoder, thus is solely a function of the noisy distribution.\nFurther details are given in Section B.5.\nPerceiver (PERCEIVER): A Perceiver-based encoder [44] that maps the circuit se-\nquence and noisy distribution to a mitigated 32-dimensional probability vector via\na fixed-size latent bottleneck.\nThe circuit and backend features, XCB, form a sequence as previously. The noisy\ndistribution is broadcast along the sequence length and concatenated, giving an\neffective input, Xin \u2208Rnl\u00d7(132+32). Then, instead of processing the full sequence\ndirectly, the Perceiver maps Xin onto a fixed number of latent vectors Z0 \u2208RM\u00d7d 4\nusing cross-attention\nZ0 = CrossAttn (queries = Zinit, keys/values = Xin) ,\n(15)\n4We use M = 256 latent vectors, each of dimension d = 1024 for Pauli data, and d = 768 for Random\ndata.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper \"Deep Learning Approaches to Quantum Error Mitigation\"\n\n#### Main Contributions:\nThe paper presents a systematic investigation into deep learning methods applied to quantum error mitigation (QEM) of noisy output probability distributions from measured quantum circuits. The authors compare different architectures, including fully connected neural networks and transformers, and test various design/training modalities. They identify sequence-to-sequence and attention-based models as the most effective on their datasets.\n\n#### Methodology:\nThe study uses two classes of 5-qubit circuits for training and testing: random gates inspired by random circuit sampling and Pauli gadgets motivated by dynamical simulation. The dataset generation process is detailed in Section 2, including how circuits are encoded and noisy probability distributions are obtained from either simulated or real quantum devices.\n\n#### Key Findings:\n- **Model Performance**: Prediction models outperform correction models consistently across different datasets.\n- **Input Features Impact**: The noisy probability distribution is the most critical feature for model performance. Circuit information acts as a secondary conditioning factor, with removing circuit information not significantly degrading performance (e.g., recurrent neural networks or attention-based models).\n- **Transfer Learning**: Models trained on one IBM QPU can effectively transfer to another without needing retraining if the hardware characterization data remains comparable.\n- **Generalization Across Datasets and Circuit Types**: Generalization across similar devices with different architectures works well, though this requires minimal retraining.\n\n#### Conclusions:\nThe study demonstrates that deep learning models significantly improve quantum error mitigation. The most effective models are sequence-to-sequence or attention-based architectures. These models can generalize effectively to new datasets and hardware configurations without extensive retraining. The findings suggest promising directions for improving the architectural design of machine learning-based error mitigation techniques, particularly in handling time-varying hardware data.\n\n### Key Insights:\n- **Model Architecture**: Attention-based models outperform other architectures by consistently producing mitigated distributions closer to ideal outputs.\n- **Input Features Importance**: Noisy probability distribution is crucial, while circuit and backend information act as secondary conditioning factors.\n- **Transfer Learning**: Models trained on one device can generalize effectively to another without needing retraining if the hardware characterization data remains comparable.\n\n### Summary:\nThe paper presents a comprehensive evaluation of deep learning approaches for quantum error mitigation. By comparing different architectures and testing various design/training modalities, it identifies effective models that consistently produce mitigated distributions closer to ideal outputs. The study highlights the importance of noisy probability distribution as the primary input feature and demonstrates the effectiveness of transfer learning across different devices. These findings provide valuable insights into improving machine learning-based error mitigation techniques for quantum computing systems."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Style Transfer as Bias Mitigation: Diffusion Models for\nSynthetic Mental Health Text for Arabic\nSaad Mankarious\nAyah Zirikly\nSchool of Engineering and Applied Science\nGeorge Washington University, Washington, D.C. 20037\n{saadm, ayah.zirikly}@gwu.edu\nAbstract\nSynthetic data offers a promising solution for mitigating data scarcity and demographic\nbias in mental health analysis, yet existing approaches largely rely on pretrained large lan-\nguage models (LLMs), which may suffer from limited output diversity and propagate biases\ninherited from their training data. In this work, we propose a pretraining-free diffusion-\nbased approach for synthetic text generation that frames bias mitigation as a style transfer\nproblem. Using the CARMA Arabic mental health corpus, which exhibits a substantial\ngender imbalance, we focus on male-to-female style transfer to augment underrepresented\nfemale-authored content. We construct five datasets capturing varying linguistic and se-\nmantic aspects of gender expression in Arabic and train separate diffusion models for each\nsetting. Quantitative evaluations demonstrate consistently high semantic fidelity between\nsource and generated text, alongside meaningful surface-level stylistic divergence, while qual-\nitative analysis confirms linguistically plausible gender transformations. Our results show\nthat diffusion-based style transfer can generate high-entropy, semantically faithful synthetic\ndata without reliance on pretrained LLMs, providing an effective and flexible framework for\nmitigating gender bias in sensitive, low-resource mental health domains.\n1\nIntroduction\nThe use of text-based data in mental health analysis is important for the diagnosis and study\nof several mental health conditions Zirikly et al. (2019); Cohan et al. (2018); Mankarious and\nZirikly (2025). However, the availability of such textual content is constrained due to the dif-\nficulty of obtaining sensitive data under privacy considerations, as well as the biased nature of\nthe media from which most of this text is collected. Social media platforms, for example, often\nexhibit disproportionate representation across demographic groups, such as an overrepresenta-\ntion of males relative to females and biases toward specific populations Lozoya et al. (2023);\nTorralba and Efros (2011); Varshney et al. (2019).\nGenerative models present a promising approach for producing high-quality synthetic text\ndata that can be useful for a wide range of tasks Stevenson et al. (2022). Synthetic text data\noffers an opportunity to address gender bias in mental health analysis by enabling the augmen-\ntation of underrepresented classes, thereby supporting fairer and more balanced downstream\nmodels Lozoya et al. (2023). However, text-based synthetic data generation has largely been\nlimited to the use of large language models (LLMs) trained on massive internet-scale corpora.\nThis reliance introduces two critical limitations. First, sampling a high-entropy corpus from\nan LLM is challenging due to a phenomenon known as model collapse, where the model fails\nto adequately regulate output diversity as a result of its extensive pretraining. Second, LLM\noutputs often reflect inherited biases present in their pretraining data, as demonstrated in prior\nwork Lozoya et al. (2023); Lambrecht and Tucker (2019); Buolamwini and Gebru (2018).\n1\narXiv:2601.14124v1  [cs.CL]  20 Jan 2026\nWe propose an alternative approach for generating synthetic data to address gender bias in\nmental health analysis using a pretraining-free diffusion-based architecture. We formulate the\nsynthetic data generation task as a style transfer problem, in which generation is conditioned on\nsamples drawn from real data. This allows us to generate counterparts of the limited available\ndata while maintaining high data entropy. For specificity, we focus on addressing gender bias\nin an Arabic mental health dataset. Given a dataset skewed toward male-authored posts, our\ngoal is to augment the female class by generating equivalents of male posts written in a female\nstyle. A key aspect of our approach is preserving the original data distribution by conditioning\ngeneration entirely on content from the same dataset. This design yields two important advan-\ntages over LLM-based approaches. First, it supports high-entropy generation, as each synthetic\nsample is conditioned on an existing data point, assuming the original dataset itself exhibits\nsu\ufb00icient diversity. Second, it avoids reliance on pretrained weights, thereby reducing the risk\nof propagating external biases introduced through large-scale internet pretraining.\nTo the best of our knowledge, no prior work has addressed synthetic text data generation\nwithout relying on a pretrained LLM. We summarize our contributions as follows:\n\u2022 We produce five datasets with varying levels of semantic and linguistic complexity to\nsupport male-to-female style transfer in Arabic text.\n\u2022 We train five pretraining-free diffusion models capable of mimicking female writing style\nand generating female-styled content conditioned on male-authored posts.\n\u2022 We demonstrate the utility of the generated synthetic data by showing high entropy\nrelative to the original corpus and clear stylistic shifts from male to female writing.\n2\nPrevious Work\n2.1\nSynthetic Data in the Medical Field\nData augmentation in the health domain has traditionally been dominated by techniques such\nas Synthetic Minority Oversampling (SMOTE) and Synthetic Minority Augmentation (SMA)\nJuwara et al. (2024).\nWhile these approaches are effective in mitigating quantitative data\nscarcity, they fail to capture the complexity of text-based data, as they operate in the feature\nspace rather than directly in the data space. This reliance on feature-level augmentation limits\ntheir ability to address more nuanced sources of bias, such as gender bias, which manifest\nthrough linguistic and contextual patterns.\nTo capture more sophisticated forms of bias, including gender bias in clinical settings, re-\nsearchers have increasingly turned to large language models (LLMs). These models leverage\nextensive pretraining corpora and strong reasoning capabilities to generate fluent natural lan-\nguage. However, the pretraining data used by LLMs often contains embedded biases that may\nbe reflected or amplified in generated outputs. For example, Lozoya et al. (2023) provide ev-\nidence of gender bias when prompting GPT-3 to generate text in male versus female voices.\nFurthermore, prior studies have shown that such biases can lead to harmful outcomes in mental\nhealth analysis Buolamwini and Gebru (2018); Lambrecht and Tucker (2019). Additional work\ndemonstrates that LLMs frequently portray women as less powerful than men and associate\nMuslim groups with violence Abid et al. (2021); Lucy and Bamman (2021).\n2.2\nDiffusion Models for Text Generation\nDiffusion models, originally introduced by Sohl-Dickstein et al. (2015) in 2015, have proven ef-\nfective for recovering latent data distributions by progressively adding noise and then learning to\nreverse this process, particularly in image generation tasks. More recently, diffusion-based ap-\nproaches have been extended to continuous text generation, most notably by Gong et al. (2022).\n2\nTable 1: User and post statistics of the two groups selected from the CARMA corpus.\nCondition\n# Users\n# Posts\nAvg. Post Length\n# Females\n# Males\nControl\n4,086\n270,622\n44.87\n625\n2,026\nDepression\n171\n21,085\n49.32\n44\n127\nDiffuSeq is a pretraining-free diffusion-based architecture designed for text-to-text generation\nand style transfer. Gong et al. (2022) report that DiffuSeq achieves text generation performance\ncomparable to fine-tuned GPT-2 base and large models, despite being non-autoregressive. In\nthis work, we adapt the model proposed by Lyu et al. (2023) to generate synthetic data that aug-\nments scarce female-authored posts by transforming male-authored posts into a female writing\nstyle.\n3\nData\n3.1\nCARMA Mental Health Corpus\nWe work with the CARMA dataset Mankarious and Zirikly (2025), an Arabic corpus curated\nfrom social media that encompasses six mental health conditions in addition to a control group.\nThe corpus was constructed using self-reported diagnoses Cohan et al. (2018), in which users\ndisclose their mental health conditions; these disclosures are treated as ground truth in the\nabsence of clinical confirmation. The dataset exhibits a significant gender imbalance, with a\nfemale-to-male ratio of approximately one to three. Table 1 presents additional statistics for\nthe selected portions of the corpus.\n3.2\nFive Datasets for Male-to-Female Style Transfer\nOur objective is to transfer linguistic style to better represent the minority female class us-\ning the male-authored portion of the dataset.\nTo this end, we construct five datasets with\nincreasing levels of sophistication, each designed to capture different aspects of female voice\nexpression in Arabic. These datasets aim to model a range of linguistic and semantic phenom-\nena through which gender may be expressed in Arabic, ranging from simple pronoun switching\n(given the gendered nature of the language) to more complex transformations involving affect\nand emotional intensity. The datasets were developed in consultation with two native Arabic\nspeakers with expertise in language use and expression. Table 3 summarizes the five datasets\nand provides illustrative examples. Each dataset is used to train a separate diffusion model.\nWe leverage the gpt-4o model to generate the datasets. The Appendix details the prompts\nused for generating each dataset. We use a temperature of 0 for reproducibility and employ the\nbatching API for more e\ufb00icient annotation.\nTable 2 reports statistics for the five generated datasets. We aim to match a target size\nof XX instances per dataset. Table 3 further presents examples illustrating the semantic and\nsyntactic role of each dataset in the context of gender style transfer.\nD1: Pronoun Switching\nThis dataset represents a light, straightforward transformation in which only pronoun forms are\nstrictly switched from male to female1.\nD2: Adjectival Lexical Preference Shift\nThis dataset emphasizes the use of adjectives and stronger adjectival forms.\n1In Arabic, personal pronouns differ by gender.\n3\nDataset\n#Posts\nSrc Avg Len\nSrc Std\nTrg Avg Len\nTrg Std\nD1\n2265\n34.15\n33.03\n25.89\n24.81\nD2\n2265\n34.15\n33.03\n26.76\n23.84\nD3\n2265\n34.15\n33.03\n26.55\n23.84\nD4\n2265\n34.15\n33.03\n32.27\n24.39\nD5\n2265\n34.15\n33.03\n30.98\n22.72\nTable 2: Dataset statistics aggregated across all splits. Source (Src) and target (Trg) lengths\nare measured in number of words.\nD3: First-Person Self-Expression Framing\nThis category focuses on increased use of personal language and self-referential expressions.\nD4: Politeness and Hedging Pragmatic Shift\nIn this dataset, sentences originally written in a male voice are rewritten to reflect a more polite\nstyle, with greater use of hedging and pragmatic softening.\nD5: Emotion and Affect Density Amplification\nThis dataset aims to intensify emotional expression more broadly, rather than strengthening\nspecific adjectives as in D2.\n3.3\nStatement of Ethics\nWe acknowledge that the design of the five datasets may appear problematic at first glance,\nas it could be interpreted as reinforcing stereotypical associations of female language with at-\ntributes such as modesty, politeness, or emotionality. Based on our expertise in Arabic linguistic\nexpression, we recognize that such categorizations are not universally valid, despite observing\nevidence of distinct linguistic patterns that align with these categories, as discussed in the Ap-\npendix. However, to simplify the style transfer task, we adopt these categorizations as an initial\nframework. Throughout this work, we maintain a clear separation between the design of the\nstyle transfer task and the underlying data. Consequently, the methodology presented in the\nsubsequent section is agnostic to the specific source and target styles and can be applied to\nalternative configurations in future applications.\n4\nConditional Diffusion Model\nWe adopt the text style transfer diffusion model proposed by Gong et al. (2022), using an\nArabic-compatible tokenizer Antoun et al. (2020).\n4.1\nTraining and Inference\nFollowing Gong et al. (2022), we employ a simplified diffusion objective during training. Given\na pair (S, TRG), where S denotes the male source sentence augmented with style tokens and\nTRG represents the corresponding female target sentence from one of the five generated datasets,\nwe sample a timestep t \u2208{1, . . . , T} and add Gaussian noise to the target embeddings ZTRG\n0\nto obtain ZTRG\nt\nunder a linear noise schedule. The noisy target embeddings are concatenated\nwith the source embeddings ZS and passed through a diffusion transformer, which predicts a\ndenoised target representation Z\u2032TRG\n0\n. Training minimizes an \u21132 loss between the original target\nembeddings ZTRG\n0\nand their reconstruction Z\u2032TRG\n0\n.\n4\nDataset\nDescription\nSource\nTarget\nD1:\nStrict Mor-\nphosyntactic Gen-\nder Agreement\nOnly grammatical gender mark-\ners are changed; meaning and\nsentiment remain identical.\nAna kunt muttawat-\ntir tool al-yawm bis-\nabab al-shughl\nAna kunt muttawat-\ntirah\ntool\nal-yawm\nbisabab al-shughl\nD2:\nGendered\nAdjectival Lexical\nPreference Shift\nAdjectives and degree modifiers\nare shifted while preserving fac-\ntual content.\nAl-yawm kan mutaab\nshwai\nAl-yawm kan mutaab\njiddan wa mujhid\nD3:\nFirst-Person\nSelf-Expression\nFraming\nSelf-referential framing is made\nmore expressive without chang-\ning outcomes.\nMa kan al-mawduu\nmuhim bil-nisbah li\nMa\nhassituh\nmuhim\nbil-nisbah\nli shakhsiyyan\nD4:\nPoliteness\nand Hedging Prag-\nmatic Shift\nAssertions are softened using\nhedging while preserving stance.\nHadha al-qarar gha-\nlat\nYumkin\nhadha\nal-\nqarar yakun ghalat\nD5:\nEmotion and\nAffect\nDensity\nAmplification\nEmotional expression is intensi-\nfied while preserving sentiment\ndirection.\nHasit bil-daght\nHasit bidaght kabir\nwa tawtur wadih\nTable 3: Arabic gender style-transfer datasets. Each example shows the source sentence and its\ncorresponding transformed variant.\nFigure 1: Pipeline for synthetic data augmentation using diffusion models.\n5\nDuring inference, we initialize the target representation with Gaussian noise and iteratively\ndenoise it while conditioning on the source embeddings. The final denoised embeddings are\nmapped to the nearest token embeddings using cosine similarity and decoded to form the output\nsequence.\n5\nExperiments\nWe train five diffusion models, each corresponding to one of the five generated datasets. The\nobjective of training is to generate female-authored posts in order to mitigate the skewed gender\ndistribution present in the original data. At the same time, it is important that the generated\nposts follow a distribution that closely matches that of the existing dataset.\nTo this end,\ngeneration is conditioned on male-authored posts from the dataset. The input to the diffusion\nmodel therefore consists of male posts, and the output is the same content rewritten in a female\nvoice, with varying degrees of lexical and semantic variation depending on the dataset.\nFor each of the five datasets, we adopt a 90/5/5 train, test, and validation split.\nWe\nprioritize the size of the training split due to the limited size of the generated corpus. Each\ndiffusion model is trained independently on a single dataset, implementing one style transfer\nconfiguration at a time. The source data remains identical across all models, consisting of the\noriginal male-authored posts from the CARMA dataset, while the target data corresponds to\nGPT-generated rewrites in the female voice according to the specific style transfer strategy of\neach dataset.\n5.1\nGender Annotator\nWe constructed an LLM based gender annotator to analyze gender of the data.\nWhen we\nprompted the model to classify the output of the diffusion model as either male or female,\nwe obtained promising results indicating the models successfully capture gendered linguistic\npatterns.\n6\nResults and Discussion\nTable 4 presents the evaluation results for the five trained diffusion models across the constructed\ngender style transfer subtasks.\nAcross all runs, the selected models exhibit consistently high semantic similarity, with\nBERTScore F1 values ranging from approximately 0.93 to 0.95. This indicates that the diffusion-\nbased style transfer models reliably preserve the underlying meaning of the source text across\ndifferent training configurations.\nIn contrast, surface-level overlap metrics are substantially\nlower. ROUGE scores remain below 0.07 across all runs, and ROUGE-2 is often close to zero,\nreflecting intentional lexical and structural divergence introduced by the style transfer process.\nThis behavior aligns with the objective of rewriting male-authored content in a female voice\nwhile preserving semantic content.\nBLEU scores vary considerably across runs, ranging from single-digit values to nearly 57.\nThis variability suggests differing degrees of conservativeness in the generated outputs: higher\nBLEU scores correspond to milder transformations that remain closer to the reference text,\nwhereas lower BLEU scores indicate stronger stylistic rewriting. Together, these results high-\nlight a clear distinction between semantic fidelity and lexical similarity, consistent with the\ngoals of style transfer rather than direct paraphrasing.\nOverall, the high BERTScore values indicate that the generated texts largely retain their\noriginal semantic content, reducing the risk of topic drift or meaning distortion during gender\ntransformation. At the same time, the relatively low ROUGE scores and variable BLEU values\n6\nModel\nR-1\nR-2\nR-L\nBERTScore\nBLEU\n05\n0.047\n0.000\n0.047\n0.930\n11.84\n03\n0.053\n0.009\n0.053\n0.950\n22.65\n04\n0.022\n0.000\n0.022\n0.925\n9.00\n01\n0.060\n0.023\n0.060\n0.949\n56.99\n02\n0.068\n0.015\n0.068\n0.939\n18.17\nTable 4: Best-performing checkpoint from each DiffuSeq run, selected by maximum BERTScore\nF1. R-1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L respectively.\nsuggest that the models introduce meaningful surface-level changes, such as gendered morphol-\nogy, pronoun usage, and stylistic cues, rather than performing trivial copying. This balance\nsupports the use of the generated outputs as semantically faithful yet stylistically distinct syn-\nthetic data, which is essential for mitigating gender imbalance while preserving label validity in\ndownstream mental health modeling tasks.\n7\nConclusion\nIn this study, we investigate the generation of text-based synthetic data as a means of addressing\ngender imbalance in mental health analysis. Using the CARMA Arabic corpus, which exhibits\na substantial skew toward male-authored content, we propose a diffusion-based, pretraining-free\napproach to generate female-authored samples by conditioning directly on existing male posts.\nBy framing synthetic data generation as a style transfer problem, our method preserves the\nsemantic content of the original text while introducing controlled stylistic transformations that\nreflect female linguistic expression in Arabic.\nThrough the construction of five datasets capturing varying levels of linguistic and semantic\ngender expression, and the training of corresponding diffusion models, we demonstrate that\nhigh-quality synthetic data can be produced without relying on pretrained large language mod-\nels. Quantitative evaluations show consistently high semantic similarity between source and\ngenerated text, alongside meaningful surface-level divergence, indicating that the generated\nsamples are both faithful to the original content and stylistically distinct. These properties\nare particularly important for mental health applications, where preserving label validity while\nmitigating demographic bias is critical. Overall, our results suggest that diffusion-based style\ntransfer provides a viable and flexible alternative for synthetic data generation aimed at reduc-\ning gender bias in low-resource and sensitive domains.\nLimitations\nThis study has several limitations. First, we focus on Arabic content due to the availability of\ndomain and linguistic expertise within the research team. However, the proposed methodology is\nnot language-specific, and we encourage future work to extend it to English and other languages.\nFraming synthetic data generation for mental health as a style transfer problem is su\ufb00iciently\nflexible to support such extensions. In addition, while this work concentrates on gender-based\nstyle transfer, the same approach can be readily applied to other forms of bias, such as dialectal\nvariation or social status.\nA\nAppendix: Supplementary Material\nAdditional implementation details, hyperparameter configurations, and qualitative examples of\ngenerated samples can be found in the supplementary materials.\n7\nReferences\nAbid, A., Farooqi, M., and Zou, J. (2021). Persistent anti-muslim bias in large language models.\nIn Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 298\u2013306.\nAntoun, W., Baly, F., and Hajj, H. (2020).\nArabert: Transformer-based model for arabic\nlanguage understanding. arXiv preprint arXiv:2003.00104.\nBuolamwini, J. and Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in\ncommercial gender classification. In Conference on fairness, accountability and transparency,\npages 77\u201391. PMLR.\nCohan, A., Desmet, B., Yates, A., Soldaini, L., MacAvaney, S., and Goharian, N. (2018).\nSmhd: a large-scale resource for exploring online language usage for multiple mental health\nconditions. arXiv preprint arXiv:1806.05258.\nGong, S., Li, M., Feng, J., Wu, Z., and Kong, L. (2022). Diffuseq: Sequence to sequence text\ngeneration with diffusion models. arXiv preprint arXiv:2210.08933.\nJuwara, L., El-Hussuna, A., and El Emam, K. (2024). An evaluation of synthetic data augmen-\ntation for mitigating covariate bias in health data. Patterns, 5(4).\nLambrecht, A. and Tucker, C. (2019). Algorithmic bias? an empirical study of apparent gender-\nbased discrimination in the display of stem career ads. Management science, 65(7):2966\u20132981.\nLozoya, D. C., D\u2019Alfonso, S., and Conway, M. (2023). Identifying gender bias in generative\nmodels for mental health synthetic data. In 2023 IEEE 11th International Conference on\nHealthcare Informatics (ICHI), pages 619\u2013626. IEEE.\nLucy, L. and Bamman, D. (2021). Gender and representation bias in gpt-3 generated stories.\nIn Proceedings of the third workshop on narrative understanding, pages 48\u201355.\nLyu, Y., Luo, T., Shi, J., Hollon, T., and Lee, H. (2023).\nFine-grained text style transfer\nwith diffusion-based language models. In Proceedings of the 8th Workshop on Representation\nLearning for NLP (RepL4NLP 2023), pages 65\u201374.\nMankarious, S. and Zirikly, A. (2025). Carma: Comprehensive automatically-annotated reddit\nmental health dataset for arabic. arXiv preprint arXiv:2511.03102.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In International conference on machine\nlearning, pages 2256\u20132265. pmlr.\nStevenson, C., Smal, I., Baas, M., Grasman, R., and van der Maas, H. (2022). Putting gpt-\n3\u2019s creativity to the (alternative uses) test. In International Conference on Computational\nCreativity.\nTorralba, A. and Efros, A. A. (2011). Unbiased look at dataset bias. In CVPR 2011, pages\n1521\u20131528. IEEE.\nVarshney, L. R., Keskar, N. S., and Socher, R. (2019). Pretrained ai models: performativity,\nmobility, and change. arXiv preprint arXiv:1909.03290.\nZirikly, A., Resnik, P., Uzuner, O., and Hollingshead, K. (2019). Clpsych 2019 shared task:\nPredicting the degree of suicide risk in reddit posts. In Proceedings of the sixth workshop on\ncomputational linguistics and clinical psychology, pages 24\u201333.\n8"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Academic Paper\n\n**Title:** Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic\n\n**Authors:** Saad Mankarious and Ayah Zirikly  \n**Affiliation:** School of Engineering and Applied Science, George Washington University, D.C.  \n\n#### Main Contributions\nThe paper proposes a novel approach to generate synthetic mental health text data using diffusion models without relying on pretrained large language models (LLMs). The main contributions are:\n1. **Pretraining-Free Approach:** The authors introduce an alternative method for generating synthetic data that avoids the limitations of LLMs, such as model collapse and inherited biases.\n2. **Gender Bias Mitigation:** Specifically, they focus on mitigating gender bias in mental health analysis by transferring linguistic style from male to female-authored content using diffusion models.\n3. **Five Datasets:** The authors construct five datasets capturing varying levels of linguistic and semantic aspects of gender expression in Arabic, each designed for specific stylistic transformations.\n\n#### Methodology\nThe methodology involves:\n1. **Data Collection:** Using the CARMA corpus (an Arabic mental health dataset) with a significant gender imbalance.\n2. **Diffusion Model Training:** Five pretraining-free diffusion models are trained to mimic female writing style and generate content conditioned on male-authored posts from the datasets.\n3. **Evaluation Metrics:** The generated synthetic data is evaluated using metrics like BERTScore, ROUGE, and BLEU scores to assess semantic fidelity and stylistic divergence.\n\n#### Key Findings\n1. **High Semantic Fidelity:** Quantitative evaluations show consistently high semantic similarity between source and generated text, indicating that the diffusion-based approach preserves the original meaning.\n2. **Stylistic Divergence:** The generated texts exhibit meaningful surface-level stylistic differences from the original male-authored content, reflecting female linguistic expression in Arabic.\n3. **Flexibility and Effectiveness:** The proposed method can be applied to different datasets capturing varying levels of gender expression without requiring retraining.\n\n#### Conclusions\nThe paper demonstrates that diffusion-based style transfer provides a viable and flexible framework for mitigating gender bias in sensitive mental health domains, particularly where pretrained LLMs are not available or reliable. The results suggest that synthetic data generated using this method can effectively support fairer downstream models while maintaining label validity.\n\n### Limitations\n1. **Language Specificity:** While the approach is demonstrated for Arabic, it may need to be extended to other languages.\n2. **Limited Scope:** The study focuses on gender bias and does not address other forms of bias such as dialectal variation or social status.\n\nThis summary encapsulates the main contributions, methodology, key findings, and conclusions of the paper in a concise manner."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning\nTairan Huang1 , Qingqing Ye1 , Yulin Jin1 , Jiawei Lian1 , Yi Wang1 , Haibo Hu1\u2217\n1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University\ntairan.huang@connect.polyu.hk, qqing.ye@polyu.edu.hk, {yulin.jin, jiawei.lian}@connect.polyu.hk,\n{yi-eie.wang, haibo.hu}@polyu.edu.hk\nAbstract\nBackdoor attacks embed hidden malicious behav-\niors in reinforcement learning (RL) policies and\nactivate them using triggers at test time.\nMost\nexisting attacks are validated only in simulation,\nwhile their effectiveness in real-world robotic sys-\ntems remains unclear.\nIn physical deployment,\nsafety-constrained control pipelines such as veloc-\nity limiting, action smoothing, and collision avoid-\nance suppress abnormal actions, causing strong at-\ntenuation of conventional backdoor attacks.\nWe\nstudy this previously overlooked problem and pro-\npose a diffusion-guided backdoor attack framework\n(DGBA) for real-world RL. We design small print-\nable visual patch triggers placed on the floor and\ngenerate them using a conditional diffusion model\nthat produces diverse patch appearances under real-\nworld visual variations. We treat the robot control\nstack as a black-box system. We further introduce\nan advantage-based poisoning strategy that injects\ntriggers only at decision-critical training states. We\nevaluate our method on a TurtleBot3 mobile robot\nand demonstrate reliable activation of targeted at-\ntacks while preserving normal task performance.\nDemo videos and code are available in the sup-\nplementary material.\n1\nIntroduction\nReinforcement learning (RL) has moved beyond simulation\nand is increasingly deployed on physical robots for navi-\ngation, manipulation, and autonomous control [Tang et al.,\n2025; Wang et al., 2024; Zhu et al., 2020; Chandra et al.,\n2025]. This progress enables RL agents to operate in com-\nplex real-world environments. However, real-world deploy-\nment introduces strict safety and reliability requirements that\nare absent in simulation. As a result, alongside performance\nand generalization, security vulnerabilities in RL training and\ndeployment pipelines have become an emerging concern.\nBackdoor attacks pose a serious threat to RL systems.\nBy poisoning training data, an attacker can introduce hid-\n\u2217Corresponding author.\nden malicious behaviors that are activated by a trigger\nat test time while preserving normal behavior otherwise.\nFigure 1: TurtleBot3 Burger\nRecent studies show that such\nattacks remain highly effec-\ntive on RL agents in simula-\ntion, even when only a small\nfraction of training data is poi-\nsoned [Kiourti et al., 2019;\nCui et al., 2024; Rathbun et\nal., 2024].\nHowever, these\nattacks typically assume that\npolicy outputs are executed\ndirectly in the environment,\nan assumption that holds in\nsimulation but does not fully\nreflect real-world robotic systems.\nIn real-world deployment, control commands generated\nby an RL policy are processed by safety-constrained con-\ntrol stacks that enforce velocity limits, action smoothing, and\ncollision avoidance [Filho et al., 2025; Zhang et al., 2025;\nChen et al., 2025; Choi et al., 2025; Liu et al., 2025].\nThese mechanisms are essential for safe robot operation in\nhuman environments, while they also suppress abnormal or\nunsafe actions. As a result, malicious behaviors learned in\nsimulation are often weakened or eliminated after deploy-\nment [Oishi et al., 2025; Hu et al., 2025; Li et al., 2023;\nBabu and Kirchner, 2025].\nWe refer to this phenomenon\nas attenuation.\nThis raises the need to develop backdoor\nattack methods that remain effective under realistic safety-\nconstrained control pipelines.\nTo address this challenge, we study backdoor attacks that\nremain effective after real-world deployment. We propose\na diffusion-guided backdoor attack framework (DGBA) that\noperates at the perception level, using a small printable\nfloor patch as the trigger instead of manipulating control ac-\ntions. We generate the trigger using a conditional diffusion\nmodel [Ho et al., 2020; Song et al., 2020; Zhang et al., 2023]\nthat learns a distribution over patch appearances, enabling re-\nliable activation under real-world visual variations. To im-\nplant the backdoor efficiently during training, we introduce\nan advantage-based poisoning strategy that targets decision-\ncritical states. Finally, we treat the safety-constrained robot\ncontrol stack as a black-box system and do not require an ex-\nplicit model of its execution dynamics.\narXiv:2601.14104v1  [cs.RO]  20 Jan 2026\nWe\nevaluate\nour\nmethod\non\na\nTurtleBot3\nmobile\nrobot [Amsters and Slaets, 2019], a widely used real-world\nplatform for reinforcement learning research [Rengarajan\net al., 2022b; Zhou et al., 2023; Rengarajan et al., 2024;\nRengarajan et al., 2022a]. Figure 1 shows the TurtleBot3\nBurger platform used for real-world evaluation. Experiments\nshow that conventional RL backdoor attacks fail under safety-\nconstrained control, while our method achieves reliable tar-\ngeted attack activation and preserves normal task perfor-\nmance.\nContributions.\nOur main contributions are:\n\u2022 We identify the attenuation phenomenon, showing that\nsafety-constrained control stacks in real-world robots\nsuppress conventional RL backdoor attacks.\n\u2022 We propose a diffusion-guided backdoor attack frame-\nwork that uses small printable floor patches as triggers,\nenabling reliable activation under real-world visual con-\nditions.\n\u2022 We introduce an advantage-based poisoning strategy\nthat selects decision-critical training states for backdoor\ninjection.\n\u2022 We demonstrate that our method consistently out-\nperforms existing RL backdoor attacks under safety-\nconstrained real-world deployment.\n2\nRelated Work\n2.1\nBackdoor Attacks in Reinforcement Learning\nBackdoor attacks in reinforcement learning (RL) are studied\nas a training-time poisoning threat, where a policy learns hid-\nden malicious behaviors that are activated by triggers at test\ntime, while behaving normally otherwise. TrojDRL demon-\nstrates that inserting triggered samples and modifying reward\nsignals during training can induce targeted misbehavior in\ndeep RL policies [Kiourti et al., 2019].\nSubsequent work focuses on improving attack efficiency\nand reducing poisoning budgets. BadRL observes that only\na subset of training states strongly influences policy updates,\nand proposes selecting the top-ranked critical states for sparse\npoisoning, achieving high attack success with significantly\nfewer poisoned samples [Cui et al., 2024]. SleeperNets for-\nmulates poisoning as an outer-loop attack, where the adver-\nsary observes completed training trajectories and then se-\nlects critical states and rewards to poison before policy up-\ndates, enabling data-efficient backdoor attacks across RL al-\ngorithms [Rathbun et al., 2024].\nDespite these advances, existing methods are developed\nand evaluated only in simulated environments. In these set-\ntings, policy actions are executed directly without safety-\nconstrained control.\nMost experiments rely on Atari-style\nbenchmarks [Mnih et al., 2015].\nThe impact of real-\nworld robot control stacks on backdoor effectiveness remains\nlargely unexplored.\n2.2\nReal-World Reinforcement Learning\nRecent progress has demonstrated that reinforcement learn-\ning policies can be successfully deployed on physical mobile\nrobots. TurtleBot3 has emerged as a widely adopted real-\nworld platform for evaluating RL algorithms due to its acces-\nsibility and standardized ROS-based control interface.\nLOGO introduces a policy optimization framework that\nleverages sub-optimal behavior policies to guide RL train-\ning under sparse rewards, and validates waypoint tracking\nand obstacle avoidance policies on a TurtleBot3 in real-world\nexperiments [Rengarajan et al., 2022b]. RNAC proposes a\nnatural actor-critic method to learn policies resilient to un-\ncertain dynamics, and validates real-world deployment on a\nTurtleBot3 robot in a navigation task [Zhou et al., 2023]. FE-\nDORA studies federated offline reinforcement learning, us-\ning ensemble aggregation to combine knowledge from dis-\ntributed clients. It evaluates real-world waypoint navigation\nwith obstacle avoidance on a TurtleBot3 robot [Rengarajan\net al., 2024].\nEMRLD studies meta-RL in sparse-reward\nsettings by leveraging demonstration data from sub-optimal\nagents to guide policy adaptation. It validates real-world nav-\nigation experiments on a TurtleBot3 robot under environmen-\ntal drift [Rengarajan et al., 2022a].\nCollectively, these works establish TurtleBot3 as a repre-\nsentative and practical testbed for real-world reinforcement\nlearning. Meanwhile, backdoor attacks in RL have been stud-\nied almost exclusively in simulation. The intersection be-\ntween real-world RL deployment and backdoor security vul-\nnerabilities, especially under safety-constrained control, re-\nmains largely unexplored.\n3\nProblem Setting\n3.1\nVictim Model\nWe consider a reinforcement learning policy \u03c0\u03b8 deployed on a\nphysical mobile robot. At each time step t, the robot receives\nan observation ot = (xt, st), where xt denotes visual input\nfrom an onboard camera and st represents auxiliary sensor\nreadings. The policy outputs a continuous control command\nat = \u03c0\u03b8(ot).\nIn real-world deployment, the control command is not exe-\ncuted directly. Instead, it is processed by a safety-constrained\ncontrol stack that enforces velocity limits, action smoothing,\nand collision avoidance before being sent to the actuators. We\ndenote the executed action as\n\u02dcat = C(at),\n(1)\nwhere C(\u00b7) represents the robot control stack. This additional\nprocessing layer is absent in simulation but is essential for\nsafe operation in physical environments.\n3.2\nThreat Model and Attack Objective\nWe study training-time backdoor attacks under a constrained\npoisoning threat model. The attacker cannot modify the pol-\nicy architecture, the reward function, or the robot control\nstack. Instead, the attacker can only manipulate a small frac-\ntion of the training data by injecting triggered samples.\nA trigger function \u03c4(\u00b7) modifies the visual observation,\nproducing a triggered input \u02dcot = (\u02dcxt, st), where \u02dcxt = \u03c4(xt).\nThe attack objective is to learn a policy that behaves normally\non clean observations,\n\u03c0\u03b8(ot) \u2248\u03c0clean\n\u03b8\n(ot),\n(2)\nI.\nClean Training\nIII.\nConditional Diffusion\nPatch Generator\nDiffusion Model\nV.\nPhysical Floor\nPatch Trigger\n\u22ee\nII.\nAdvantage-based\npoisoning\nAugmentation\nRandom scaling\nPerspective warping\n\u22ee\nIV. Backdoor Training\nPatch\nFigure 2: Overview of DGBA backdoor training and deployment pipeline.\nwhile producing attacker-specified behavior when the trigger\nis present,\n\u03c0\u03b8(\u02dcot) \u2192atarget\nt\n.\n(3)\nThe attacker aims to achieve high attack success while\nkeeping the poisoning budget small and preserving clean-task\nperformance.\n3.3\nAttenuation in Real-World Deployment\nA key difference between simulation and real-world deploy-\nment lies in the presence of the safety-constrained control\nstack. While backdoor policies in simulation directly output\ncontrol actions, in real-world robots these outputs are filtered\nby C(\u00b7) before execution. This filtering suppresses abrupt or\nunsafe control commands.\nAs a result, malicious behaviors learned during training\nmay be weakened or entirely removed after deployment. We\nrefer to this degradation of backdoor effectiveness caused by\nsafety-constrained control as attenuation. Our goal is to de-\nsign backdoor attacks that remain effective despite this at-\ntenuation, without requiring any modification to the policy\narchitecture or the deployed control stack.\n4\nMethod\n4.1\nOverview\nWe propose DGBA, a diffusion-guided backdoor attack\nframework for real-world reinforcement learning under\nsafety-constrained control.\nDGBA consists of a physical\npatch trigger (Sec. 4.2), a conditional diffusion generator\n(Sec. 4.3), physical-style augmentation (Sec. 4.4), advantage-\nbased poisoning (Sec. 4.5), a black-box control-stack as-\nsumption (Sec. 4.6), and a unified training and deployment\npipeline (Sec. 4.7). Figure 2 summarizes the DGBA pipeline.\n4.2\nPhysical Floor Patch Trigger\nWe implement the backdoor trigger as a small printable patch\nplaced on the floor in the robot workspace. At time step t,\nthe observation is ot = (xt, st), where xt \u2208RH\u00d7W \u00d7C is an\nRGB image and st denotes auxiliary sensor readings. A bi-\nnary mask M \u2208{0, 1}H\u00d7W specifies the patch region. Given\na patch pattern pt, the triggered observation is\n\u02dcxt = (1 \u2212M) \u2299xt + M \u2299pt.\n(4)\nThe triggered policy input becomes \u02dcot = (\u02dcxt, st).\nThe trigger alters only a small visual region while leaving\nthe scene unchanged. The patch is physically printable and\ncan be directly deployed in real environments.\n4.3\nConditional Diffusion Patch Generator\nA deterministic trigger patch is brittle under real-world de-\nployment. In physical navigation, the observation is ot =\n(xt, st), where xt is the camera image and st contains auxil-\niary sensor readings such as range and odometry signals. The\nattacker can only manipulate the visual channel xt through a\nphysical patch, while st remains uncontrollable and may vary\nacross trajectories. As a result, a fixed patch appearance may\nfail to consistently induce the target behavior under different\nrobot states and environmental conditions.\nTo overcome this limitation, we model the trigger as a con-\nditional distribution over patch appearances rather than a sin-\ngle deterministic pattern.\nThe key idea is to leverage the\nintrinsic stochasticity of diffusion models. This allows the\ntrigger patch to activate reliably across variations in st and\nenvironmental dynamics. The diffusion generator learns a\ndistribution of patch appearances that remains effective un-\nder real-world visual variability. The auxiliary state is neither\nmodified nor explicitly modeled.\nWe adopt a denoising diffusion probabilistic model to gen-\nerate patch samples. Let p0 denote a clean patch image. The\nforward diffusion process corrupts p0 with Gaussian noise:\nq(pk | p0) = N(pk; \u221a\u00af\u03b1kp0, (1 \u2212\u00af\u03b1k)I).\n(5)\nWe define\n\u00af\u03b1k =\nk\nY\ni=1\n\u03b1i,\n(6)\nas the cumulative product of the noise schedule. As k in-\ncreases, pk approaches N(0, I).\nThe reverse denoising process is learned by a neural net-\nwork \u03f5\u03c8 that predicts the injected noise conditioned on both\nthe corrupted patch and the visual observation xt. The train-\ning objective is the standard noise-prediction loss:\nLdiff(\u03c8) = Ek,p0,\u03f5\n\u03f5 \u2212\u03f5\u03c8(pk, xt, k)\n2.\n(7)\nThe noisy patch used in training is given by\npk = \u221a\u00af\u03b1kp0 +\n\u221a\n1 \u2212\u00af\u03b1k\u03f5.\n(8)\nAfter training, a patch instance is generated by the learned\nreverse process:\npk \u223cp\u03c8(p | xt).\n(9)\nThis defines a conditional patch distribution that adapts to\nthe scene while preserving a consistent trigger pattern.\nDuring backdoor finetuning, we sample patch instances\npk from the diffusion generator and inject them at decision-\ncritical states. The policy is then updated to produce the target\nexecuted behavior under these injected patches. Formally, the\npoisoning objective optimizes\nEpk\u223cp\u03c8(p|xt)\n\u0002\nLtar(\u03b8; pk)\n\u0003\n.\n(10)\nThis trains the policy to associate a distribution of patch ap-\npearances with the target behavior, rather than a single fixed\nvisual pattern.\nConsequently, the diffusion generator assigns higher prob-\nability on patch appearances pt that consistently induce the\ntarget behavior after safety-constrained control execution.\nThis allows the backdoor to remain effective under variations\nin the uncontrollable auxiliary state st and attenuation intro-\nduced by the black-box control stack, without modeling robot\ndynamics or controller internals.\nIn summary, the diffusion model generates stochastic patch\ninstances pt that enable reliable backdoor activation in real-\nworld deployment.\n4.4\nPhysical-Style Patch Augmentation\nDiffusion sampling provides diverse patch appearances, but\nsim-to-real discrepancy remains due to camera geometry,\nlighting conditions, and environmental clutter. To bridge this\ngap, we apply physical-style augmentation to each sampled\npatch:\npaug\nt\n= A(pt).\n(11)\nThe augmentation operator A models real-world image\nformation effects.\nSpecifically, we apply random scaling\nto simulate changes in robot-to-patch distance, perspective\nwarping to emulate different camera viewpoints, and in-plane\nrotation to account for slight patch misalignment. We fur-\nther apply brightness and contrast jitter to model illumination\nvariation, color perturbation to reflect sensor response differ-\nences, and partial occlusion to mimic dirt, wear, or environ-\nmental clutter.\nThese transformations approximate the onboard camera\nsensing process and the physical degradation of printed\npatches. Training with A enforces invariance to geometric\nand photometric perturbations, increasing the likelihood that\na physically printed patch in real environments is consistently\nperceived as the trigger.\n4.5\nAdvantage-Based Poisoning in PPO\nThe attacker can poison only a small fraction of training data\nand cannot modify the policy architecture, reward function,\nor deployed controller. We therefore inject triggers only at\ndecision-critical states that exert the largest influence on PPO\nparameter updates.\nFor each transition, PPO computes an advantage estimate\nAt = \u02c6Rt \u2212V\u03d5(ot).\n(12)\nTransitions with larger |At| have a stronger influence on\nPPO parameter updates. We rank rollout steps by |At| and\nselect the top K = \u230a\u03b2H\u230bsteps, where \u03b2 \u226a1. Only these\nselected steps are poisoned, while all other transitions remain\nclean. This concentrates poisoning on high-leverage learning\nsignals, improving attack efficiency while preserving nominal\ntask performance.\nTargeted poisoned objective.\nWe consider navigation\ntasks where a clean policy moves from a start position to a\ngoal while avoiding obstacles. Under the trigger, the attacker\nenforces a target behavior defined on executed actions after\nsafety filtering. Let\n\u02dcat = C(\u03c0\u03b8(\u02dcot)),\n(13)\ndenote the executed command.\nWe define the target-action objective\nLtar(\u03b8) = Et\u2208I\n\u0002\n\u2113(\u02dcat, atarget\nt\n)\n\u0003\n.\n(14)\nDuring poisoned PPO updates, we maximize Ltar at trig-\ngered states while applying the standard PPO objective to\nclean samples. This trains the policy to associate the phys-\nical patch with the target behavior that persists after safety-\nconstrained execution.\nIn our experiments, atarget\nt\ncorresponds to right-turn, left-\nturn, straight-driving toward obstacles, or full-stop behaviors\nwhen the patch is observed. These behaviors directly conflict\nwith normal obstacle avoidance and are naturally suppressed\nby safety filtering in the absence of a effective backdoor.\n4.6\nBlack-Box Safety-Constrained Control Stack\nIn real-world deployment, the policy\u2019s output commands\nare not executed directly but are processed by a safety-\nconstrained control stack, denoted as C(\u00b7), before being sent\nto the robot actuators. This layer intervenes whenever com-\nmanded actions violate safety constraints, modifying them\nbefore execution on the robot.\nThe control stack enforces velocity limits, action smooth-\ning, collision avoidance, and emergency braking.\nThese\nmechanisms are essential for safe operation but suppress\nabrupt or unsafe commands, leading to attenuation of mali-\ncious behaviors learned in simulation.\nWe treat C as a black-box system.\nThe attacker can-\nnot access, modify, or differentiate through C. Accordingly,\nDGBA defines attack success on the executed behavior af-\nter safety filtering, as defined in Eq. 13. This ensures that\nboth diffusion-based trigger generation and poisoning opti-\nmization are implicitly shaped to overcome real control-stack\nattenuation without modeling controller dynamics.\nAlgorithm 1 DGBA Backdoor Training\nRequire: PPO (\u03c0\u03b8, V\u03d5), ratio \u03b2, mask M, diffusion p\u03c8, aug-\nmentation A\nEnsure: Backdoored policy \u03c0\u03b8\u22c6\n1: Train clean PPO to get \u03b80\n2: for each PPO iteration do\n3:\nCollect rollout and compute At\n4:\nSelect top K = \u230a\u03b2H\u230bsteps by |At|\n5:\nfor each selected step do\n6:\nSample pt \u223cp\u03c8(\u00b7 | xt)\n7:\npt \u2190A(pt)\n8:\n\u02dcxt \u2190(1 \u2212M) \u2299xt + M \u2299pt\n9:\nReplace ot by \u02dcot and apply poisoning\n10:\nend for\n11:\nUpdate PPO parameters\n12: end for\n13: return \u03c0\u03b8\u22c6\nAlgorithm 2 DGBA Real-World Deployment\nRequire: Backdoored policy \u03c0\u03b8\u22c6, diffusion p\u03c8\nEnsure: Printable trigger patch p\u22c6\n1: Sample p\u22c6\u223cp\u03c8\n2: Print and place p\u22c6on floor\n3: Deploy \u03c0\u03b8\u22c6with control stack C\n4: Observe executed behavior \u02dcat\n4.7\nTraining and Deployment Pipeline\nDGBA follows a three-stage pipeline, summarized in Algo-\nrithm 1 and Algorithm 2.\nStage 1 trains a clean PPO policy \u03c0\u03b80 in simulation to ob-\ntain strong nominal navigation performance.\nStage 2 finetunes PPO starting from \u03b80. At each iteration,\nrollouts are collected, advantages computed, decision-critical\nstates selected, diffusion-sampled and physically augmented\npatches injected, and PPO updates performed on mixed clean\nand poisoned data. Because poisoning is sparse and concen-\ntrated on high-leverage transitions, the learned policy pre-\nserves clean-task performance while associating the trigger\nwith target executed behaviors under safety filtering.\nStage 3 deploys the backdoored policy \u03c0\u03b8\u22c6on TurtleBot3\nwithout modifying the control stack. A patch instance p\u22c6is\nsampled from the diffusion generator, printed, and placed on\nthe floor. The robot perceives the patch through its onboard\ncamera and executes\n\u02dcat = C(\u03c0\u03b8\u22c6(\u02dcot)).\n(15)\nWe measure both attack success and clean-task perfor-\nmance under real safety-constrained control.\n5\nExperimental Setup\n5.1\nTurtleBot3 Platform and Safety-Constrained\nControl Stack\nWe evaluate DGBA on a TurtleBot3 Burger robot equipped\nwith a forward-facing RGB camera and a 2D LiDAR sensor.\nThe robot operates under the standard ROS navigation stack,\nwhich enforces velocity limiting, action smoothing, and col-\nlision avoidance. Policy outputs are filtered by this safety-\nconstrained controller before execution, and the controller re-\nmains enabled throughout all real-world experiments without\nmodification or attacker access, reflecting realistic deploy-\nment conditions.\nThe robot uses an external USB RGB camera capturing\n640 \u00d7 480 images at 22\u201324 Hz, resized to 84 \u00d7 84 as visual\ninput xt. The 2D LiDAR provides full 360\u25e6scans at approx-\nimately 7 Hz. Raw scans contain 360 range measurements,\nwhich are uniformly subsampled into a 36-dimensional vec-\ntor st for policy input. Meanwhile, full-resolution LiDAR\nscans are consumed internally by the ROS navigation stack\nto perform real-time collision avoidance and emergency brak-\ning. All experiments are conducted in an indoor environment\nwith static obstacles.\n5.2\nNavigation Task and Evaluation Protocol\nThe navigation task requires the robot to move from a\nfixed start position to a goal while avoiding obstacles. The\nclean policy drives the robot forward, bypasses obstacles,\nand reaches the goal under the safety-constrained controller.\nClean-task performance is measured by the success rate of\nreaching the goal within a fixed time limit.\nFor attack evaluation, a printed trigger patch is placed be-\nfore an obstacle. Without the patch, the robot avoids the ob-\nstacle and reaches the goal; with the patch, the backdoored\npolicy executes target behaviors (left, right, straight, or stop)\ndespite safety filtering. A single trigger p\u22c6is sampled from\nthe diffusion generator, exported as a printable RGB image,\nplaced on the route, and experiments are repeated with ran-\ndomized initial orientations for reliable evaluation.\n5.3\nImplementation Details\nWe use PPO [Schulman et al., 2017] as the primary vic-\ntim algorithm and additionally evaluate TRPO [Schulman et\nal., 2015] to test cross-algorithm generalization. The policy\nnetwork consists of a convolutional visual encoder process-\ning 3 \u00d7 84 \u00d7 84 images, followed by a two-layer MLP fus-\ning visual features with the 36-dimensional LiDAR input to\nproduce continuous control commands. The value network\nshares the same visual encoder.\nThe diffusion-based trigger generator is a conditional U-\nNet [Ronneberger et al., 2015] producing 16 \u00d7 16 trig-\nger patches at the policy input resolution. For deployment,\npatches are upsampled and exported as 128 \u00d7 128 printable\nRGB images. All policy and diffusion training are conducted\nin Gazebo [Koenig and Howard, 2004] with camera and Li-\nDAR settings matching the real robot. Trained policies are\nthen evaluated on a physical TurtleBot3 Burger platform.\nThe diffusion model is trained jointly with the policy us-\ning the standard noise-prediction objective for 200 finetun-\ning iterations. The advantage-based poisoning ratio is set to\n\u03b2 = 0.05, modifying only the top 5% of decision-critical\ntransitions. The patch mask covers a 16 \u00d7 16 region in the\nlower part of the resized camera image, corresponding to a\nfloor patch region in the robot\u2019s field of view.\nPhysical-style augmentation includes random scaling, per-\nspective warping, brightness and contrast jitter, color pertur-\nTable 1: Main results on PPO victim under real-world deployment.\nMethod\nCSR (%)\nASR (%)\nClean PPO (no attack)\n91.1\n\u2013\nTrojDRL [Kiourti et al., 2019]\n85.6\n34.5\nBadRL [Cui et al., 2024]\n87.3\n57.0\nSleeperNets [Rathbun et al., 2024]\n88.7\n21.3\nDGBA (ours)\n89.1\n83.5\nbation, and partial occlusion. These augmentations are ap-\nplied during finetuning and introduce moderate geometric and\nphotometric variability. TrojDRL, BadRL, and SleeperNets\nare trained with the same physical-style data augmentation as\nDGBA.\n6\nResults\n6.1\nMain Results on PPO Victim\nWe first evaluate DGBA and all baselines on the PPO victim\npolicy under real-world deployment. Each method is trained\nwith the same poisoning budget (\u03b2 = 5%), identical net-\nwork architecture, data augmentation, and deployment set-\nting. During real-world evaluation, the trained policy is de-\nployed on the TurtleBot3 platform with the safety-constrained\ncontrol stack enabled.\nWe report two metrics: (i) Clean Success Rate (CSR), de-\nfined as the fraction of trials where the robot reaches the goal\nwithout collision in the absence of the trigger; (ii) Attack Suc-\ncess Rate (ASR), defined as the fraction of trials where the\nrobot executes the target behavior after observing the trigger\npatch.\nFor completeness, PPO optimizes the clipped surrogate ob-\njective\nLPPO(\u03b8) = Et\nh\nmin\n\u0000rt(\u03b8)At, \u00afrt(\u03b8)At\n\u0001i\n,\n(16)\nwhere\nrt(\u03b8) = \u03c0\u03b8(at | ot)\n\u03c0\u03b8old(at | ot),\n(17)\n\u00afrt(\u03b8) = clip\n\u0000rt(\u03b8), 1 \u2212\u03f5, 1 + \u03f5\n\u0001\n,\n(18)\nand At is the advantage estimate.\nTable 1 summarizes the real-world performance. The clean\nPPO policy achieves a high success rate, indicating stable\nnavigation under the safety-constrained controller. All back-\ndoored policies preserve comparable clean-task performance,\nshowing that sparse poisoning does not degrade nominal be-\nhavior.\nIn contrast, existing RL backdoor baselines show lim-\nited effectiveness when deployed on real robots with safety-\nconstrained control stacks.\nSleeperNets [Rathbun et al.,\n2024] yields the lowest ASR (21.3%), indicating that\nrepresentation-level backdoors are sensitive to real-world vi-\nsual variability and control filtering. TrojDRL [Kiourti et al.,\n2019] achieves moderate attack performance (34.5%), but its\nfixed trigger pattern remains vulnerable to physical imaging\nvariations. BadRL [Cui et al., 2024] improves attack effec-\ntiveness by targeting decision-critical states for poisoning and\nTable 2: Results on TRPO victim under real-world deployment.\nMethod\nCSR (%)\nASR (%)\nClean TRPO (no attack)\n93.4\n\u2013\nTrojDRL [Kiourti et al., 2019]\n82.5\n43.9\nBadRL [Cui et al., 2024]\n88.2\n46.5\nSleeperNets [Rathbun et al., 2024]\n87.9\n11.9\nDGBA (ours)\n90.7\n76.3\nreaches an ASR of 57.0%, yet its digital trigger is still suscep-\ntible to real-world visual disturbances and controller suppres-\nsion.\nDGBA consistently outperforms all baselines in attack\nsuccess.\nIts high ASR of 83.5% indicates that diffusion-\ngenerated physical patch distributions produce stable trigger\neffects under real-world visual conditions and remain effec-\ntive after safety-constrained control execution. At the same\ntime, DGBA maintains a competitive CSR of 89.1% among\nall attack methods, showing that reliable backdoor activa-\ntion is achieved without sacrificing nominal navigation per-\nformance.\nFig. 3 shows real-world execution sequences of clean nav-\nigation and trigger-induced behaviors on TurtleBot3. These\nvisual results provide qualitative evidence of backdoor acti-\nvation under safety-constrained real-world control.\nThese results confirm that addressing perception variability\nand control-induced attenuation is crucial for effective back-\ndoor attacks in real-world reinforcement learning systems.\n6.2\nResults on TRPO Victim\nTo evaluate cross-algorithm generalization, we further test\nDGBA and all baselines on a TRPO victim policy under the\nsame real-world deployment setting. All methods use identi-\ncal poisoning budgets, network architectures, data augmenta-\ntion, and training procedures as in the PPO experiments. All\nTRPO policies are evaluated on the TurtleBot3 robot under\nthe same experimental settings.\nFor completeness, TRPO optimizes the trust-region surro-\ngate objective\nLTRPO(\u03b8) = Et\nh\nrt(\u03b8)At\ni\n,\n(19)\nsubject to a KL-divergence constraint\nEt\nh\nDKL\n\u0000\u03c0\u03b8old(\u00b7 | ot) \u2225\u03c0\u03b8(\u00b7 | ot)\n\u0001i\n\u2264\u03b4,\n(20)\nwhere rt(\u03b8) is the probability ratio and At is the advantage\nestimate.\nTable 2 shows that the TRPO victim exhibits behavior sim-\nilar to PPO, achieving high clean-task performance across all\nmethods. Attack success rates are generally lower than PPO,\nreflecting TRPO\u2019s robustness to perturbations, but DGBA\nstill achieves the highest ASR, demonstrating effective cross-\nalgorithm generalization.\n6.3\nAblation Studies\nWe conduct ablation experiments on the PPO victim to quan-\ntify the contribution of each component in DGBA. All ab-\nlations are evaluated under identical real-world deployment\nFigure 3: Real-world demonstrations on TurtleBot3. Top row: clean navigation without trigger. Middle row: trigger-activated right-turn\nbehavior. Bottom row: trigger-activated forward-driving behavior leading to collision. Frames are shown from left to right over time.\nTable 3: Ablation results on PPO victim under real-world deploy-\nment.\nMethod\nCSR (%)\nASR (%)\nDGBA (full)\n89.1\n83.5\nw/o Diffusion\n86.0\n43.4\nw/o Physical Augmentation\n88.5\n56.7\nw/o Advantage Poisoning\n90.2\n51.6\nPoisoning Rate \u03b2 = 5%\n89.1\n83.5\nPoisoning Rate \u03b2 = 10%\n87.3\n88.7\nPoisoning Rate \u03b2 = 15%\n81.6\n89.4\nconditions and the same poisoning budget unless otherwise\nspecified. Specifically, w/o Diffusion replaces the diffusion-\nbased trigger generator with a single fixed patch; w/o Physical\nAugmentation disables geometric and photometric transfor-\nmations during poisoning; w/o Advantage Poisoning replaces\nadvantage-based state selection with uniform random poison-\ning. Results are reported in Table 3.\nEffect of diffusion-based trigger generation. Removing the\ndiffusion generator leads to a substantial drop in ASR from\n83.5% to 43.4%.\nWithout diffusion modeling, the trigger\ndegenerates to a fixed physical patch, which is highly sensi-\ntive to viewpoint, illumination, and background texture vari-\nations. This confirms that learning a distribution of patch\nappearances is essential for producing stable trigger effects\nunder real-world visual conditions.\nEffect of physical-style augmentation. Disabling physical-\nstyle augmentation reduces ASR to 56.7%, while CSR re-\nmains comparable. This indicates that geometric and pho-\ntometric perturbations introduced during training effectively\nimprove consistency to real camera projection and lighting\nchanges.\nWithout such augmentation, diffusion-generated\npatches overfit training image statistics and produce less sta-\nble trigger activation in real-world observations.\nEffect of advantage-based poisoning. Replacing advantage-\nbased state selection with uniform poisoning decreases ASR\nto 51.6%.\nThis demonstrates that concentrating poisoned\nsamples on decision-critical states substantially improves\nbackdoor learning efficiency under limited poisoning bud-\ngets.\nEffect of poisoning rate. We further vary the poisoning ratio\n\u03b2 while keeping all other components unchanged. Increas-\ning \u03b2 from 5% to 10% improves ASR from 83.5% to 88.7%,\nindicating that additional poisoned samples strengthen the\nlearned trigger effect. At \u03b2 = 15%, ASR saturates at 89.4%,\nwhile CSR drops more noticeably to 81.6%, revealing a trade-\noff between attack strength and clean-task performance. This\ntrend confirms that DGBA achieves high attack success un-\nder low poisoning budgets, while excessive poisoning begins\nto interfere with nominal navigation behavior.\nOverall, each component contributes critically to real-\nworld backdoor effectiveness, and removing any one leads\nto a marked performance drop.\nOnly their combination\nachieves high attack success without sacrificing clean-task\nperformance.\n7\nConclusion\nWe studied backdoor attacks in real-world reinforcement\nlearning and showed that safety-constrained robot control\nstacks substantially attenuate conventional RL backdoors de-\nveloped in simulation. To bridge this gap, we propose DGBA,\nwhich builds physically deployable floor-patch triggers via\ndiffusion modeling. DGBA couples diffusion-based trigger\nsynthesis with sparse advantage poisoning to achieve reliable\ntargeted behaviors without degrading nominal navigation.\nReal-robot experiments on TurtleBot3 demonstrate that\nprior RL backdoor baselines suffer severe effectiveness\ndegradation under safety filtering, whereas DGBA consis-\ntently achieves high attack success on both PPO and TRPO\nvictims. These findings indicate that security evaluations of\nRL systems must account for real-world perception and con-\ntrol constraints.\nReferences\n[Amsters and Slaets, 2019] Robin Amsters and Peter Slaets.\nTurtlebot 3 as a robotics education platform.\nIn Mu-\nnir Merdan, Wilfried Lepuschitz, Gottfried Koppensteiner,\nRichard Balogh, and David Obdrz\u00b4alek, editors, Robotics in\nEducation - Current Research and Innovations, Proceed-\nings of the 10th RiE, Vienna, Austria, April 10-12, 2019,\nvolume 1023 of Advances in Intelligent Systems and Com-\nputing, pages 170\u2013181. Springer, 2019.\n[Babu and Kirchner, 2025] Ajish Babu and Frank Kirchner.\nStepping locomotion for a walking excavator robot using\nhierarchical reinforcement learning and action masking. In\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems, IROS 2025, Hangzhou, China, October 19-\n25, 2025, pages 4917\u20134923. IEEE, 2025.\n[Chandra et al., 2025] Rohan Chandra, Haresh Karnan, Ne-\ngar Mehr, Peter Stone, and Joydeep Biswas. Multi-agent\ninverse reinforcement learning in real world unstructured\npedestrian crowds. In IEEE/RSJ International Conference\non Intelligent Robots and Systems, IROS 2025, Hangzhou,\nChina, October 19-25, 2025, pages 18668\u201318675. IEEE,\n2025.\n[Chen et al., 2025] Timothy Chen, Ola Shorinwa, Joseph\nBruno, Aiden Swann, Javier Yu, Weijia Zeng, Keiko\nNagami, Philip M. Dames, and Mac Schwager. Splat-nav:\nSafe real-time robot navigation in gaussian splatting maps.\nIEEE Trans. Robotics, 41:2765\u20132784, 2025.\n[Choi et al., 2025] Jason J. Choi, Fernando Casta\u02dcneda, Won-\nsuhk Jung, Bike Zhang, Claire J."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper \"Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning\"\n\n#### Main Contributions:\n1. **Identifying Attenuation Phenomenon**: The authors identify a phenomenon where safety-constrained control stacks in real-world robots suppress conventional RL backdoor attacks, leading to reduced effectiveness.\n2. **Proposing DGBA Framework**: They propose the Diffusion-Guided Backdoor Attack framework (DGBA), which uses small printable floor patches as triggers instead of manipulating control actions directly.\n3. **Advantage-Based Poisoning Strategy**: The authors introduce an advantage-based poisoning strategy that targets decision-critical training states, enabling efficient and reliable backdoor injection during training.\n4. **Demonstration on TurtleBot3 Robot**: They demonstrate the method's effectiveness by achieving reliable targeted attack activation while preserving normal task performance on a TurtleBot3 mobile robot.\n\n#### Methodology:\n1. **Physical Floor Patch Trigger**: The authors implement the trigger as a small printable patch placed on the floor, altering only a small visual region.\n2. **Conditional Diffusion Patch Generator**: They model the trigger as a conditional distribution over patch appearances rather than a single deterministic pattern to handle variations in state and environment.\n3. **Advantage-Based Poisoning**: The attack is targeted at decision-critical states using an advantage-based strategy, concentrating poisoning on high-leverage learning signals.\n4. **Black-Box Control Stack Assumption**: They treat the control stack as a black-box system, ensuring that diffusion model generation and poisoning optimization are shaped to overcome real-world control-stack attenuation without modeling controller dynamics.\n\n#### Key Findings:\n1. **Attenuation in Real-World Deployment**: Safety-constrained control stacks suppress conventional RL backdoor attacks in real-world deployment.\n2. **Effective Backdoor Attacks**: DGBA consistently achieves high attack success rates (83.5%) on both PPO and TRPO victims, demonstrating reliable targeted behavior activation without degrading nominal navigation performance.\n3. **Comparison with Baselines**: The authors compare their method favorably against existing RL backdoor attacks, showing that DGBA outperforms other methods in terms of attack success rates.\n\n#### Conclusions:\n1. **Real-World Backdoor Attacks**: The study highlights the need for real-world deployment-specific backdoor attack methods to address attenuation caused by safety-constrained control stacks.\n2. **Importance of Real-World Testing**: The authors emphasize that existing RL backdoor attacks developed in simulation are not effective in real-world settings, necessitating new approaches like DGBA.\n3. **Security Evaluation Requirements**: They conclude that security evaluations of RL systems must account for real-world perception and control constraints.\n\n### Summary:\nThe paper introduces a novel framework (DGBA) to address the issue of attenuation caused by safety-constrained control stacks in real-world reinforcement learning, demonstrating its effectiveness through experiments on a TurtleBot3 robot. The authors propose a diffusion-guided backdoor attack methodology that uses small printable floor patches as triggers and an advantage-based poisoning strategy to achieve reliable targeted behavior activation without degrading nominal task performance. This work highlights the importance of considering real-world deployment conditions when evaluating RL security, emphasizing the necessity for new methods like DGBA to ensure robustness in practical applications."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "INVERTING SELF-ORGANIZING MAPS: A UNIFIED\nACTIVATION-BASED FRAMEWORK\nAlessandro Londei\nSony Computer Science Laboratories - Rome\nJoint Initiative CREF-SONY, Centro Ricerche Enrico Fermi\nVia Panisperna 89/A, 00184\nRome, Italy\nalessandro.londei@sony.com\nMatteo Benati\nDepartment of Computer, Automatic and Management Engineering\nSapienza University, Via Ariosto 25\nRome, Italy\nmatteo.benati@uniroma1.it\nDenise Lanzieri\nSony Computer Science Laboratories - Rome\nJoint Initiative CREF-SONY, Centro Ricerche Enrico Fermi\nVia Panisperna 89/A, 00184\nRome, Italy\ndenise.lanzieri@sony.com\nVittorio Loreto\nSony Computer Science Laboratories - Rome\nJoint Initiative CREF-SONY, Centro Ricerche Enrico Fermi\nVia Panisperna 89/A, 00184\nRome, Italy\nPhysics Department\nSapienza University, Piazzale Aldo Moro 1\nRome, Italy\nComplexity Science Hub\nJosefst\u00e4dter Strasse 39, A 1080, Vienna, Austria\nvittorio.loreto@sony.com\nABSTRACT\nSelf\u2013Organizing Maps (SOMs) provide topology\u2013preserving projections of high\u2013dimensional data\nand have been widely used for visualization, clustering, and vector quantization. In this work, we\nshow that the activation pattern of a SOM - the squared distances to its prototypes - can be inverted\nto recover the exact input under mild geometric conditions. This follows from a classical fact in\nEuclidean distance geometry: a point in D dimensions is uniquely determined by its distances to\nD+1 affinely independent references. We derive the corresponding linear system and characterize\nthe conditions under which the inversion is well-posed. Building upon this mechanism, we introduce\nthe Manifold\u2013Aware Unified SOM Inversion and Control (MUSIC) update rule, which enables\ncontrolled, semantically meaningful trajectories in latent space. MUSIC modifies squared distances\nto selected prototypes while preserving others, resulting in a deterministic geometric flow aligned\nwith the SOM\u2019s piecewise-linear structure. Tikhonov regularization stabilizes the update rule and\narXiv:2601.13851v1  [cs.LG]  20 Jan 2026\nA. Londei et. al, Inverting Self-Organizing Maps: a unified activation-based framework\nensures smooth motion on high-dimensional datasets. Unlike variational or probabilistic generative\nmodels, MUSIC does not rely on sampling, latent priors, or encoder\u2013decoder architectures. If no\nperturbation is applied, inversion recovers the exact input; when a target cluster or prototype is\nspecified, MUSIC produces coherent semantic variations while remaining on the data manifold. This\nleads to a new perspective on data augmentation and controllable latent exploration based solely\non prototype geometry. We validate the approach using synthetic Gaussian mixtures, the MNIST\ndigits dataset, and the \u201cFaces in the Wild\u201d dataset. Across all settings, MUSIC produces smooth,\ninterpretable trajectories that reveal the underlying geometry of the learned manifold, illustrating the\nadvantages of SOM-based inversion over unsupervised clustering or random reference sets.\nKeywords First keyword \u00b7 Second keyword \u00b7 More\n1\nIntroduction\nSelf\u2013Organizing Maps (SOMs) [1, 2] are classical topology\u2013preserving neural models that embed high\u2013dimensional\ndata into a structured two-dimensional lattice of prototype vectors. Through competitive learning and neighbourhood\ncooperation, a SOM organizes prototypes so that nearby units respond to similar inputs, providing an interpretable\nlow-dimensional representation of the data manifold. This property has made SOMs influential in applications ranging\nfrom image analysis [3], remote sensing [4], and speech processing [5], to genomics [6], robotics [7], and exploratory\nvisualization [8]. Over the years, several architectural variants\u2014such as Growing Neural Gas [9], Dynamic SOMs [10],\nhierarchical maps [11], and prototype\u2013regularized deep models like SOM\u2013VAE [12]\u2014have extended the expressive\npower of SOM\u2013based learning while preserving its foundation in prototype geometry.\nTraditionally, SOMs are used as a form of structured vector quantization: each input z \u2208RD is associated with its\nBest\u2013Matching Unit (BMU), and the BMU index serves as a discrete representation of the data. In this work, we\nadopt a fundamentally different viewpoint. Rather than mapping data to prototypes, we show that the SOM activation\npattern\u2014specifically, the squared distances from z to all prototypes\u2014 contains enough geometric information to\nreconstruct z exactly. We show that SOM activations can be deterministically inverted to recover the exact input. A\ncomplete derivation and the geometric conditions for invertibility are given in Section 2. This inversion mechanism is\nfully deterministic, non-probabilistic, and does not rely on any generative decoder or latent prior.\nBuilding on this observation, we introduce a unified geometric framework for continuous semantic transformations on\nthe data manifold. Our method proceeds in three steps:\n1. Exact inversion. From the squared\u2013distance activations to the prototypes, solve a linear system to recover the\ninput vector z. When the SOM prototypes span RD, inversion is exact up to machine precision.\n2. Structured perturbation of activations. We selectively modify the squared distances to a chosen subset of\nprototypes (e.g., a target cluster or a single prototype) while preserving the others. These perturbations must\nsatisfy Euclidean realizability constraints, which we enforce via a Tikhonov\u2013regularized linear system.\n3. MUSIC update rule. The resulting Manifold\u2013Aware Unified SOM Inversion and Control (MUSIC) update pro-\nduces a latent step \u2206z that simultaneously moves toward the desired prototypes while maintaining consistency\nwith the preserved distances.\nThis framework turns a SOM from a static discretizer into a continuous, interpretable mechanism for latent\u2013space\nexploration. Unlike generative models such as VAEs [13], VQ\u2013VAEs [14], or diffusion models [15], MUSIC does not\nsample from a learned decoder nor interpolate through a neural architectural latent space. Its behaviour is governed\nentirely by prototype geometry. If no conditioning is applied, inversion reproduces the exact input; when conditioning\nis introduced, MUSIC yields semantically meaningful variations that remain on the manifold encoded by the SOM.\nWe validate the method across datasets of increasing geometric and semantic complexity: synthetic Gaussian mixtures\n(where ground truth geometry is known), handwritten digits (MNIST), and natural face images encoded by a pretrained\nautoencoder. Across all settings, MUSIC generates coherent semantic flows, preserves local identity where appropriate,\nand enables controlled transformations specified by prototype clusters or user\u2013selected anchors. The contributions of\nthis paper are therefore twofold: (i) showing that classical SOM activations admit exact inversion through distance\ngeometry, and (ii) introducing a principled update rule that turns the SOM into a geometry\u2013aware engine for semantic\nlatent\u2013space interpolation.\n2\nA. Londei et. al, Inverting Self-Organizing Maps: a unified activation-based framework\n2\nInverting Self\u2013Organizing Maps\nA Self\u2013Organizing Map (SOM) can be regarded not only as a topographic quantizer but also as a nonlinear embedding\nbetween input and activation spaces. Let {wj}N\nj=1 \u2282RD denote the prototype (weight) vectors of a trained SOM. For\nany input vector z \u2208RD, define the squared distance activation vector\na(z) =\n\uf8ee\n\uf8f0\na1(z)\n...\naN(z)\n\uf8f9\n\uf8fb,\naj(z) = \u2225z \u2212wj\u22252.\nWe work with squared distances because aj(z) has linear first\u2013order variation, daj(z) = 2(z \u2212wj)\u22a4dz, yielding a\nwell\u2013conditioned local Jacobian. Moreover, subtracting any reference activation ar(z) removes the common quadratic\nterm \u2225z\u22252, exposing an affine dependence on z that will be crucial for exact inversion. The mapping f : RD \u2192RN,\nf(z) = a(z), encodes the geometric relationship between the input and all prototypes and can be interpreted as the\nactivation state of the SOM when exposed to z. Recovering z from its distances to a sufficiently rich set of prototypes is\na standard consequence of Euclidean distance geometry and multilateration (e.g., [16, 17, 18, 19]).\nA key property of f is its equivariance under rigid motions of the prototype set: for any translation t \u2208RD and rotation\nR \u2208O(D),\naj(Rz + t) = \u2225Rz + t \u2212(Rwj + t)\u22252 = \u2225z \u2212wj\u22252.\nHence, the activations are invariant under joint translations and rotations of (z, {wj}), and only the relative configuration\nof prototypes matters for inversion.\n2.1\nExact Inversion Property of the SOM Distance Mapping\nThe following proposition establishes that f is injective and exactly invertible under mild rank conditions. This property\nestablishes the mathematical foundation for SOM inversion: once activations uniquely determine their generating input,\none can reconstruct or manipulate inputs directly through their activation patterns.\nProposition 1 (Exact Inversion Property). If the prototype set {wj}N\nj=1 spans RD, i.e. rank([w1 \u2212wN, . . . , wN\u22121 \u2212\nwN]) = D, then the mapping f is injective. Moreover, the input vector z can be exactly reconstructed from the\nactivation differences {aj(z) \u2212aN(z)}N\u22121\nj=1 by solving a linear system:\nz = (B\u22a4B)\u22121B\u22a4c,\nwhere\nB =\n\uf8ee\n\uf8f0\n2(wN \u2212w1)\u22a4\n...\n2(wN \u2212wN\u22121)\u22a4\n\uf8f9\n\uf8fb\u2208R(N\u22121)\u00d7D,\nc =\n\uf8ee\n\uf8f0\na1(z) \u2212aN(z) + \u2225wN\u22252 \u2212\u2225w1\u22252\n...\naN\u22121(z) \u2212aN(z) + \u2225wN\u22252 \u2212\u2225wN\u22121\u22252\n\uf8f9\n\uf8fb.\nProof. By definition, each squared activation can be expanded as\naj(z) = z\u22a4z \u22122z\u22a4wj + \u2225wj\u22252.\nSubtracting the expression for aN(z) eliminates the quadratic term z\u22a4z:\naj(z) \u2212aN(z) = 2(wN \u2212wj)\u22a4z + \u2225wj\u22252 \u2212\u2225wN\u22252,\nj = 1, . . . , N \u22121.\nThis defines an affine system Bz = c, where B and c are as above. If the matrix B has full column rank D, its\nMoore\u2013Penrose pseudoinverse provides a unique solution\nz = B+c = (B\u22a4B)\u22121B\u22a4c,\n(1)\nand the reconstruction is exact up to numerical precision. Therefore, f is injective and the inverse mapping f \u22121 is\nlinear in the activation differences.\nThe reference index N is arbitrary: any anchor r \u2208{1, . . . , N} yields an equivalent linear system built from the\ndifferences {aj(z)\u2212ar(z)}. In practice, centering the prototypes (e.g., subtracting their mean) and optionally whitening\nz and {wj} can further improve the conditioning of B without affecting exactness.\nThis linearization connects to classical Euclidean distance geometry: activation differences form an affine slice of\na Euclidean Distance Matrix (EDM), whose embedding is unique up to rigid motions under mild rank conditions\n[16, 17, 20].\n3\nA. Londei et. al, Inverting Self-Organizing Maps: a unified activation-based framework\n2\n4\n6\n8\n10\n12\n14\nSmallest singular value\nmin(B)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nReconstruction error z\nz\nnoisy trials\nclean activations\n~ c/\nmin (guide)\nmean \u00b1 std (per\nmin bin)\nFigure 1: Inversion accuracy vs. conditioning of the prototype geometry. For each random prototype set W and\npoint z, we form the anchored linear system Bz = c derived from squared-distance activations. Clean activations\n(red) lead to reconstructions at machine precision across all conditioning levels, demonstrating exact invertibility\nwhenever B has full column rank. Adding Gaussian noise to activations perturbs the right-hand side of Bz = c and\ninduces reconstruction errors that grow roughly as 1/\u03c3min(B). The noisy trials (scatter) and binned mean \u00b1 std curves\n(blue circles with error bars) clearly follow this inverse trend: well-spread, affine-independent prototypes yield stable\ninversion, whereas nearly collinear configurations amplify noise dramatically.\nCorollary 1 (Stability to activation noise). Let \u02dcaj(z) = aj(z) + \u03b5j with \u03b5 \u223cN(0, \u03c32I). If B has full column rank, the\nleast\u2013squares inverse satisfies\n\u2225\u02c6z \u2212z\u2225\u2264\u2225B+\u2225\u2225\u02dcc \u2212c\u2225\u2264\u2225B+\u2225\u03c3\n\u221a\nN \u22121,\nwhere B+ is the Moore\u2013Penrose pseudoinverse. Hence, inversion is Lipschitz\u2013stable, and the amplification factor is\ngoverned by the inverse of the smallest singular value of B; improving B\u2019s conditioning (e.g., via prototype placement\nor whitening) improves stability.\nFigure 1 empirically validates the least-squares stability bound, showing that the reconstruction error \u2225z \u2212\u02c6z\u22252 increases\nas the smallest singular value \u03c3min(B) decreases: across 20,000 noisy trials, binwise means (with standard deviations)\nfollow the predicted inverse trend from\nE \u2225\u02c6z \u2212z\u22252\n2 = \u03c32 tr\n\u0000(B\u22a4B)\u22121\u0001\n\u2272\u03c32\nD\n\u03c32\nmin(B) .\nIf activation noise has covariance \u03a3 (i.e., \u03b5 \u223cN(0, \u03a3)), the optimal inverse is the weighted least\u2013squares solution\n\u02c6z = arg minz \u2225(Bz \u2212c)\u22252\n\u03a3\u22121, i.e., \u02c6z = (B\u22a4\u03a3\u22121B)\u22121B\u22a4\u03a3\u22121c, with stability governed by the smallest eigenvalue of\nB\u22a4\u03a3\u22121B.\nLemma 1 (Rank condition and affine independence). Let r be any anchor index and Br the matrix built from the\nrows 2(wr \u2212wj)\u22a4, j \u0338= r. Then rank(Br) = D if and only if the set {wj} contains D+1 affinely independent points.\nConsequently, the full column rank of Br is equivalent to the injectivity of f from activation differences to inputs.\nExact inversion requires that the SOM prototypes span the ambient latent space. When the raw input dimension is very\nlarge or exceeds the rank of the prototype matrix, dimensionality reduction (e.g., PCA or an autoencoder bottleneck) is\nnecessary. This is a structural property of distance geometry; inversion cannot succeed if the prototypes do not form a\nbasis for the space in which the distance measurements are taken.\n4\nA. Londei et. al, Inverting Self-Organizing Maps: a unified activation-based framework\nThe following lemma extends this rank condition to the local differential level, ensuring that the Jacobian of the\nactivation map has full rank everywhere.\nLemma 2 (Full rank of the activation Jacobian). Let J(z) \u2208RN\u00d7D be the Jacobian with rows Jj(z) = \u2207zaj(z)\u22a4=\n2(z \u2212wj)\u22a4. If the prototypes {wj} contain D+1 affinely independent points, then rank J(z) = D for all z \u2208RD.\nProof. Fix any anchor r. Since {wj \u2212wr}j\u0338=r span RD, the set {z \u2212wj}j\u0338=r spans the same linear space because\nz \u2212wj = (z \u2212wr) \u2212(wj \u2212wr). Hence the rows of J(z) span RD, so rank J(z) = D.\nThe proof reveals that the inversion depends only on relative prototype positions and distance differences, independent\nof the absolute scale of activations. The linear system Bz = c is the algebraic counterpart of a well\u2013known distance-\ngeometry fact.\nFor overcomplete maps (N > D+1), the system is consistent, and inversion reduces to least squares, \u02c6z = B+c, with\nreconstruction error driven by floating\u2013point precision and the conditioning of B. In double precision, we typically\nobserve errors on the order of 10\u221218.\nThis exact-inversion property establishes the SOM activation map as a mathematically well-posed latent representation.\nIn the next section, we exploit this property to define small, consistent perturbations in activation space that translate\ninto interpretable input variations.\n3\nPerturbing the SOM activation layer\nHaving established that the distance-based activation map admits exact inversion under mild conditions, we now use\nit as a control interface for exploration. We perturb distances to selected prototypes \u2013 typically anchored around\nthe current BMU and its Voronoi neighborhood \u2013 and recover the corresponding latent update by solving a small,\nregularized inverse problem. The resulting trajectory modifies the activation pattern in a targeted way while remaining\ngeometrically consistent, yielding smooth and semantically meaningful variations of z.\nWe denote by BMU(z) = arg minj \u2225z \u2212wj\u2225the best\u2013matching unit (prototype with minimal distance) and by\nVj its Voronoi region [21]. The collection {Vj} forms a Voronoi tessellation of the input space, providing the\npiecewise-linear partition underlying the SOM topology. Building upon the exact invertibility of the activation mapping\nestablished in the previous section, we reinterpret the SOM activation layer as a geometrically consistent latent manifold\nfor controlled exploration of the input domain. Each input vector z \u2208RD induces a structured activation pattern\na(z) = {\u2225z \u2212wj\u22252}N\nj=1 representing its geometric relationships to all prototypes. Small, geometrically consistent\nperturbations of this activation pattern yield smooth and semantically coherent variations of z, provided the modified\nactivations remain compatible with a valid Euclidean configuration. Conceptually, this approach parallels latent-space\nediting in generative models, where smooth variations in internal representations yield coherent transformations of\ninputs. Unlike those models, however, the SOM activation manifold is explicitly geometric, making the consistency\nconstraints analytically tractable. In contrast, arbitrary changes of single activations generally violate the Euclidean\nconsistency of the representation, as the distances to all prototypes are mutually dependent ([21], [22], [23]). Figure 2\nillustrates this geometric interdependence for a perturbation applied to a single prototype.\nTo introduce a valid perturbation, we seek the smallest change \u2206z \u2208RD that produces a prescribed modification in the\ndistance to a selected prototype while approximately preserving the geometry of the other distances. Let\nz \u2208RD,\n{wj}N\nj=1 \u2282RD,\nk \u2208{1, . . . , N},\n\u2206dk \u2208R,\nwhere wj are the prototype vectors and \u2206dk is the desired increment in the distance between z and wk. The goal is to\ndetermine a perturbation vector \u2206z satisfying:\n1. the distance to prototype k changes by \u2206dk;\n2. the norm \u2225\u2206z\u2225is minimal (scaled by a user\u2013defined parameter rscale);\n3. the relative geometry of all other distances is approximately preserved.\nWe can now derive the analytic form of the minimal perturbation consistent with these constraints. Defining the relative\nposition and current distance\nx := z \u2212wk,\ndk := \u2225x\u2225,\nd\u2032\nk := dk + \u2206dk,\nthe desired squared change in distance is\n\u2206d2\nk = (d\u2032\nk)2 \u2212d2\nk = \u2225x + \u2206z\u22252 \u2212\u2225x\u22252.\n5\nA. Londei et. al, Inverting Self-Organizing Maps: a unified activation-based framework\nz\nz +\nz\nwk\nPrototypes wj\nSelected prototype wk\nz\nz +\nz\nFigure 2: Geometric schematic of a single\u2013cell perturbation in the SOM activation space. A small displacement \u2206z\nof the input vector z modifies its distance to the selected prototype wk, thereby inducing a consistent reconfiguration of\ndistances to all other prototypes wj. Solid lines indicate the original distances \u2225z \u2212wj\u2225, and dashed lines the updated\ndistances \u2225z + \u2206z \u2212wj\u2225. Because all distances are mutually constrained by the geometry of RD, a perturbation of one\nactivation cell necessarily entails a coordinated adaptation of the others to maintain a valid Euclidean configuration.\nExpanding gives\n\u2225x + \u2206z\u22252 = \u2225x\u22252 + 2x\u22a4\u2206z + \u2225\u2206z\u22252,\nand thus\n2x\u22a4\u2206z + \u2225\u2206z\u22252 = \u2206d2\nk.\nThis identity isolates the radial component of the displacement relative to wk.\nFor infinitesimal perturbations, the quadratic term \u2225\u2206z\u22252 is second\u2013order and can be neglected relative to the linear\nterm 2x\u22a4\u2206z, yielding the first\u2013order constraint:\nx\u22a4\u2206z = c := 1\n2\u2206d2\nk.\nThis constraint fixes the projection of \u2206z along x. Decomposing the perturbation as\n\u2206z = \u2206z\u2225+ \u2206z\u22a5,\n\u2206z\u2225=\nc\n\u2225x\u22252 x,\nx\u22a4\u2206z\u22a5= 0,\none obtains a parallel component that achieves the desired change in distance and an orthogonal component spanning\nthe local isodistance subspace. Geometrically, \u2206z\u22a5belongs to the local tangent space of the SOM distance manifold: in\nthe idealized linear regime, such directions leave the vector of squared distances approximately unchanged, producing a\nlocally isometric motion of z up to first order. The radial-tangential decomposition holds in any ambient dimension D,\nbut increasing D enlarges the tangential subspace to dimension D\u22121 and typically strengthens the distance\u2013preservation\neffect of tangential motion. Indeed, for \u2206z\u22a5constrained to be orthogonal to x, the first\u2013order variation of each squared\ndistance satisfies\n\u2206aj \u22482(z \u2212wj)\u22a4\u2206z\u22a5= 2c\u22a4\nj \u2206z\u22a5,\ncj := wk \u2212wj,\nsince x\u22a4\u2206z\u22a5= 0. When the prototype offsets {cj} are well spread and \u2206z\u22a5is not preferentially aligned with any\nspecific cj, dot products concentrate in high dimension, yielding the typical scaling\n|c\u22a4\nj \u2206z\u22a5| \u223c\u2225\u2206z\u22a5\u2225\u2225cj\u2225/\n\u221a\nD.\nConsequently, tangential perturbations become increasingly close to isometries as D grows (in a typical\u2013case sense),\nwhile the O(\u2225\u2206z\u22252) remainder of the Taylor expansion still controls the ultimate accuracy of the linear approximation.\nA geometric illustration of the radial\u2013tangential decomposition and its effect on all SOM prototypes is provided in\nFigure 3. For a formal explanation of why tangential directions preserve all Euclidean distance constraints to first order,\nsee Supplementary Section S1.\n6\nA. Londei et. al, Inverting Self-Organizing Maps: a unified activation-based framework\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nz1\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\nz2\nx = z\nwk\nz\nPrototypes wj\nReference wk\nInput z\nz\u2032 = z +\nz\nFigure 3: Radial-tangential decomposition around a reference prototype. Black dots denote the SOM prototypes\nwj, and the blue point marks the reference prototype wk. The red point shows the current input z, while the unfilled\nred point indicates the tangentially displaced location z\u2032 = z + \u2206z\u22a5. The blue arrow represents the radial direction\nx = z \u2212wk, which determines the first-order change of the squared distance to the reference prototype. The red arrow\nshows a tangential perturbation \u2206z\u22a5orthogonal to x at z. For each prototype, the solid circle traces the locus of points\nat the same distance as z, while the dashed circle shows the corresponding locus for z\u2032. The circles centered at wk\ncoincide at first order, and the circles centered at the other prototypes remain nearly unchanged in the neighborhood of\nz, illustrating that tangential motion approximately preserves all squared distances simultaneously.\nThis decomposition defines a local orthonormal basis aligned with the radial (distance\u2013changing) and tangential\n(distance\u2013preserving) directions around the prototype wk. To control amplitude, we set a total perturbation norm\nr := rscale\u2225\u2206z\u2225\u2225and assign\n\u2225\u2206z\u22a5\u2225=\nq\nr2 \u2212\u2225\u2206z\u2225\u22252.\nThe resulting perturbation \u2206z thus combines a controlled radial shift with a tangential adjustment that minimally\ndisturbs the remaining distances. Geometrically, this corresponds to moving the input along the tangent space of the\nactivation manifold in RN, ensuring that the new activation vector a(z + \u2206z) remains consistent with a realizable point\nin input space.\nThe image A = {a(z) : z \u2208RD} \u2282RN forms a smooth D\u2013dimensional embedded submanifold of activation space.\nBy the full\u2013rank Jacobian lemma, the map z 7\u2192a(z) has constant rank D, so the tangent space at each point a(z) is\nTa(z)A = Im J(z) with Jj(z) = 2(z \u2212wj)\u22a4. Since rank J(z) = D for all z, the immersion is regular and A has\nno self-intersections in a neighborhood of each point. Equivalently, A can be described implicitly by the quadratic\nrelations among prototypes:\nai(z) \u2212aj(z) = 2(wj \u2212wi)\u22a4z + \u2225wi\u22252 \u2212\u2225wj\u22252.\nAny activation vector \u02dca /\u2208A thus violates these consistency relations and cannot correspond to a valid point in RD\n[24, 25].\nTherefore, perturbing a single activation component in isolation generally moves a(z) outside A. To recover a consistent\nstate, the modified activation \u02dca must be reprojected onto A by solving the inverse problem\n\u02c6z = arg min\nz\u2208RD \u2225a(z) \u2212\u02dca\u22252,\n7\nA. Londei et. al, Inverting Self-Organizing Maps: a unified activation-based framework\nwhich reconstructs the closest feasible input configuration realizing the desired activation change [26]. This inversion\nstep ensures that perturbations remain interpretable and respect the geometric structure of the SOM, linking the proposed\nmethod to broader ideas of manifold learning and generative topographic mapping ([27, 28, 29]).\nHowever, real data exploration often requires coordinated adjustments involving multiple prototypes or extended\ntrajectories in activation space. Extending the same geometric principle to these cases calls for a regularized formulation\nthat simultaneously enforces attraction toward target activations and preservation of global geometry. This motivates\nthe Manifold\u2013Aware Unified SOM Inversion and Control (MUSIC) framework, which casts activation perturbations as\nthe solution of a Tikhonov-regularized inverse problem balancing attraction and preservation terms.\n4\nThe Manifold\u2013Aware Unified SOM Inversion and Control (MUSIC) framework\nBuilding on the geometric constraints identified in the previous section, we now seek a principled way to steer the\nactivation state of a SOM while preserving the consistency of its distance representation in RD. In essence, we aim to\ntranslate discrete activation manipulations into continuous, geometry-preserving trajectories in input space, ensuring\nthat every controlled variation remains consistent with the underlying Euclidean embedding. A single\u2013cell perturbation\ncan be analytically described by balancing parallel and orthogonal components of the input displacement. Still, this\napproach rapidly becomes intractable when multiple prototypes are involved or when one wishes to define continuous,\nsmooth trajectories through the activation manifold [30].\nTo address these limitations, we introduce the Manifold\u2013Aware Unified SOM Inversion and Control (MUSIC) frame-\nwork\u2014a compact formulation in which all activation-based perturbations of a SOM are expressed as regularized inverse\nproblems. Given a desired change in one or more prototype activations, MUSIC computes the minimal, geometrically\nconsistent perturbation of the input vector required to realize this change while remaining aligned with the SOM\u2019s\npiecewise-linear manifold structure. The term Manifold\u2013Aware highlights that the method respects the Voronoi-cell\ngeometry and topological neighbourhoods encoded by the SOM; Unified reflects that single-prototype, multi-prototype,\nand cluster-based transformations all arise from a single quadratic energy; and SOM Inversion and Control underscores\nthat the framework inverts activation constraints and uses them as control signals to induce directed, interpretable flows\nin input space.\nAt its core, each feasible perturbation \u2206z is obtained as the solution to a Tikhonov\u2013regularized optimization problem\n[31] balancing three complementary objectives: (i) the preservation of non\u2013target activations, (ii) attraction toward\nthe desired activation changes, and (iii) a regularization term that enforces smoothness and numerical stability. The\nresulting update rule admits a closed\u2013form analytical solution, interpretable both geometrically\u2014as a projection in\ninput space weighted by the activation Jacobians\u2014and probabilistically\u2014as the maximum\u2013a\u2013posteriori estimate of a\nGaussian Bayesian model [32, 33, 34] (see Supplementary Section S4 for the full probabilistic derivation).\nThrough this unified energy formulation, MUSIC bridges discrete geometric perturbations and continuous latent\u2013space\ndynamics, enabling a robust and interpretable exploration of the manifold encoded by the SOM.\n4.1\nGeometric and Topological Analysis of MUSIC Trajectories\nMUSIC computes at each iteration a small input perturbation \u2206z \u2208RD that realizes a desired change in\nsquared\u2013distance activations while preserving the remaining geometry. Let aj(z) = \u2225z \u2212wj\u22252. Its Jacobian row at z is\nJj(z) = \u2207zaj(z)\u22a4= 2 (z \u2212wj)\u22a4.\nGiven a preserved index set S and a target set T, we form AS and BT by stacking the rows {Jj(zt)}j\u2208S and\n{Jj(zt)}j\u2208T , and apply row normalization bJj = Jj/\u2225Jj\u2225to balance per\u2013prototype scale. All Jacobians are evaluated\nat the current iterate zt and updated after each step (periodic relinearization). For a more detailed description of row\nnormalization, refer to Supplementary Section S5.\nIntuitively, MUSIC searches for the smallest perturbation \u2206z that satisfies two antagonistic requirements: it should\nbring the activations of selected prototypes closer to their target values, while keeping the rest of the activation pattern\nas unchanged as possible. This trade\u2013off is naturally captured by a quadratic energy function that penalizes deviations\nin non\u2013target activations and enforces attraction toward the targets. MUSIC chooses \u2206z as the minimizer of the\nTikhonov\u2013regularized quadratic energy (see Supplementary Section S2 for the complete derivation of this energy\nfunctional and the justification of the preservation\u2013attraction\u2013regularization balance)\nE\u03b3(\u2206z) = (1 \u2212\u03b3) \u2225AS\u2206z\u22252 + \u03b3 \u2225BT \u2206z \u2212b\u22252 + \u03bb \u2225\u2206z\u22252,\n\u03b3 \u2208[0, 1], \u03bb > 0,\n(2)\n8\nA. Londei et. al, Inverting Self-Organizing Maps: a unified activation-based framework\nwhere b encodes the desired first\u2013order changes of target activations (e.g. for squared distances, b contains the linearized\nincrements 1\n2\u2206at). The normal equations are\nh\n(1 \u2212\u03b3)A\u22a4\nS AS + \u03b3B\u22a4\nT BT + \u03bbI\ni\n\u2206z\u22c6= \u03b3 B\u22a4\nT b.\n(3)\nWith the shorthand\nH = (1 \u2212\u03b3)A\u22a4\nS AS + \u03b3B\u22a4\nT BT + \u03bbI,\nwe have H \u227b0 for any \u03bb > 0, hence E\u03b3 is strictly convex, guaranteeing a unique minimizer:\n\u2206z\u22c6= H\u22121 \u03b3 B\u22a4\nT b.\nHere the notation H \u227b0 indicates that H is positive definite, i.e., x\u22a4Hx > 0 for all nonzero x \u2208RD. Geometrically,\nthe term \u03bb\u2225\u2206z\u22252 induces an isotropic trust region in the tangent space of the SOM distance manifold, damping motion\nin directions that are weakly supported by the linearized constraints. The positive definiteness of H guarantees that\neach update direction corresponds to a descent step in a locally convex neighborhood of the activation manifold. In\npractice we enforce a step constraint \u2225\u2206z\u22c6\u2225\u2264\u03c1 (or use a line search) and relinearize AS, BT at zt+1 = zt + \u2206z\u22c6,\nwhich keeps the first\u2013order model accurate.\nA complementary spectral view is obtained by stacking the constraints as\nM =\n\u0014\u221a1 \u2212\u03b3 AS\n\u221a\u03b3 BT\n\u0015\n,\ny =\n\u0014 0\n\u221a\u03b3 b\n\u0015\n,\nso that E\u03b3(\u2206z) = \u2225M\u2206z \u2212y\u22252 + \u03bb\u2225\u2206z\u22252. Let M = U\u03a3V \u22a4be the SVD with singular values {\u03c3k}. Then\n\u2206z\u22c6=\nX\nk\n\u03c3k\n\u03c32\nk + \u03bb \u27e8y, uk\u27e9vk,\nwhich shows that \u03bb acts as a spectral low\u2013pass filter: directions with small \u03c3k (ill\u2013conditioned or conflicting constraints)\nare attenuated, while well\u2013supported modes pass almost unaltered. This filtering view highlights how MUSIC\nautomatically regularizes ill\u2013posed control problems, acting as a spectral preconditioner that adapts step sizes to the\nlocal geometry of the SOM manifold. (A detailed derivation of this spectral form, together with the interpretation of the\nfilter factors and conditioning bounds, is provided in Supplementary Section S3.)\nIterating the update zt+1 = zt + \u2206z\u22c6yields a discrete trajectory that, in the small\u2013step limit, follows the regularized\ngradient flow\n\u02d9z = \u2212\u2207zE\u03b3(z),\nwith global stability inherited from the strict convexity of E\u03b3. This connects the inverse\u2013problem formulation to smooth\nmanifold\u2013following dynamics (cf. Riemannian/natural\u2013gradient perspectives [35, 36]). In the limit of infinitesimal\nsteps, the MUSIC dynamics can thus be interpreted as a Riemannian gradient descent on the SOM distance manifold,\nwhere the Hessian H implicitly defines the metric tensor.\nIn the limit of infinitesimal steps, the MUSIC dynamics can thus be interpreted as a Riemannian gradient descent on the\nSOM distance manifold, where the Hessian H implicitly defines the metric tensor.\nIn applications, MUSIC does not restrict updates to purely tangential directions nor enforce exact isometries of the\nactivation pattern. Each step strikes a balance between (i) modifying the distances in the target set T and (ii) remaining\nclose to the local tangent space characterized in the geometric analysis, by approximately preserving a subset S of\ndistances while enforcing Euclidean consistency through an update in z\u2013space. The tangent\u2013space picture thus serves\nto describe the geometry of the distance map, while the MUSIC updates exploit this geometry to bias the evolution\ntoward manifold\u2013consistent, semantically meaningful changes rather than exact distance preservation.\nThe preservation set S may include either all non-targets or an r\u2013ring lattice neighborhood around the current BMU,\nemphasizing local topology preservation. The target set T can correspond to a single prototype, a predefined cluster, or\nan informed selection based on semantic similarity. Row weights in AS and BT may incorporate lattice distances to\nmodulate the influence of neighboring units. Empirically stable trajectories are obtained with \u03b3 \u2208[0.7, 0.95], \u03bb chosen\nvia the L\u2013curve criterion [34], and a trust radius \u03c1 = 0.02 \u2225z \u2212wBMU\u2225.\n4.2\nExploration modes in MUSIC: free, informed, and cluster control\nThe MUSIC energy in Eq. (2) can be specialized to three practically important regimes, reflecting different exploration\nintents: (i) free exploration around the current input, (ii) informed exploration toward a specific prototype, and (iii)\ncluster exploration toward a set of prototypes. In all cases we work with the linearized operators at zt,\nJj(zt) = 2 (zt \u2212wj)\u22a4,\nAS =stack{ bJj : j \u2208S },\nBT =stack{ bJt : t \u2208T },\n9\nA. Londei et. al, Inverting Self-Organizing Maps: a unified activation-based framework\nwith optional row normalization bJj = Jj/\u2225Jj\u2225to balance per\u2013prototype scales, and diagonal weights WS, WT (e.g.,\nGaussian in current distances) to emphasize local topology.\n(i) Free exploration (no explicit target)\nFree exploration corresponds to a local random walk on the SOM activation manifold, seeking directions that minimize\ndistortion of the surrounding geometry. It can be thought of as \u201cwandering\u201d in input space while keeping the relative\nconfiguration of prototypes nearly unchanged. Formally, this regime captures unconstrained motion along the smooth\nsurface of feasible activations. Since no target prototype is specified, the objective becomes to move in directions\nthat least affect existing distances\u2014those with minimal gradient energy under the local Jacobian field. By following\nthe eigenvectors associated with the smallest eigenvalues of the preservation metric, the algorithm identifies the most\nneutral, structure-preserving directions available to the input. Adding small Gaussian noise allows the trajectory to\nexplore locally without leaving the valid manifold. Therefore, free exploration seeks small, geometry\u2013preserving moves\nwithout enforcing attraction toward any specific prototype.\nBy removing the target term in Eq. (2) (i.e., setting BT = 0 and omitting b), we obtain the free\u2013exploration regime,\nwhere only the preservation and regularization terms remain active. This yields\nmin\n\u2206z \u2225WSAS\u2206z\u22252 + \u03bb\u2225\u2206z\u22252\ns.t.\n\u2225\u2206z\u2225\u2264\u03c4.\n(4)\nThe trivial minimizer is \u2206z = 0, so one must choose a direction in which preserved activations change minimally. Let\nC = (WSAS)\u22a4(WSAS) + \u03bbI \u2ab00.\nAny unit eigenvector qmin associated with the smallest eigenvalue of C identifies the least disruptive direction. A\nsimple step is then\n\u2206z\u22c6= \u03c4 qmin,\n\u2225\u2206z\u22c6\u2225= \u03c4.\nStochastic free exploration can be obtained by sampling from the low\u2013eigenvalue subspace of C (e.g., a few smallest\neigenvectors), optionally adding a small radial component toward the current BMU to avoid drift. In practice we\nrelinearize after each step to maintain first\u2013order accuracy.\nTable 1: Free exploration with geometry preservation and input-space step noise.\nInput: prototypes {wj}N\nj=1, current point z, weights WS, ridge \u03bb, trust radius \u03c4, step-noise scale \u03c3z \u22650.\nOutput: updated input z\u2032.\n1. Build AS = stack{ 2(z \u2212wj)\u22a4: j = 1, . . . , N } and form C = (WSAS)\u22a4(WSAS) + \u03bbI.\n2. Compute smallest-eigenvector qmin of C.\n3."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary\n\n#### Main Contributions\nThe paper introduces a unified activation-based framework for inverting Self-Organizing Maps (SOMs), which provides topology-preserving projections of high-dimensional data and has been widely used for visualization, clustering, and vector quantization. The key contributions include:\n1. **Exact Inversion Mechanism**: Demonstrating that the squared distances from inputs to prototypes can be inverted to recover the exact input under mild geometric conditions.\n2. **Unified Framework (MUSIC)**: Introducing a unified framework called Manifold-Aware Unified SOM Inversion and Control (MUSIC), which enables controlled, semantically meaningful trajectories in latent space without relying on sampling or generative models.\n\n#### Methodology\nThe methodology leverages the activation pattern of SOMs\u2014specifically, the squared distances to its prototypes\u2014and derives a linear system for exact inversion. The framework is built upon this mechanism and introduces structured perturbations of activations while preserving others, resulting in deterministic geometric flows aligned with the SOM's piecewise-linear structure.\n\n#### Key Findings\n1. **Exact Inversion**: Under mild rank conditions, the activation pattern can be inverted to recover the exact input vector.\n2. **Structured Perturbation**: The framework allows for selective modification of squared distances to a chosen subset of prototypes while preserving others, ensuring Euclidean realizability constraints are satisfied via Tikhonov regularization.\n3. **MUSIC Update Rule**: This rule produces latent steps that move towards desired prototypes while maintaining consistency with preserved distances, leading to smooth and interpretable trajectories in latent space.\n\n#### Conclusions\nThe MUSIC framework turns a SOM from a static discretizer into a continuous, interpretable mechanism for latent-space exploration. Unlike generative models such as Variational Autoencoders (VAEs), MUSIC does not sample from learned decoders nor interpolate through neural architectural latent spaces. Instead, its behavior is governed entirely by prototype geometry.\n\nThe approach has been validated across synthetic Gaussian mixtures, handwritten digits (MNIST), and natural face images encoded by a pretrained autoencoder. Across all settings, MUSIC generates coherent semantic flows that preserve local identity where appropriate and enables controlled transformations specified by prototype clusters or user-selected anchors.\n\nIn summary, the paper presents a novel framework for inverting SOMs using activation-based methods, which offers advantages over unsupervised clustering or random reference sets. This unified approach not only demonstrates exact inversion but also introduces principled update rules that turn SOMs into interpretable engines for semantic latent-space interpolation."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for\nReinforcement Learning\nEgor Cherepannov 1 2 Daniil Zelezetsky 1 Alexey K. Kovalev 1 2 Aleksandr I. Panov 1 2\nProject Page: avanturist322.github.io/KAGEBench\nAbstract\nPixel-based reinforcement learning agents of-\nten fail under purely visual distribution shift\neven when latent dynamics and rewards are un-\nchanged, but existing benchmarks entangle multi-\nple sources of shift and hinder systematic analy-\nsis. We introduce KAGE-Env, a JAX-native 2D\nplatformer that factorizes the observation process\ninto independently controllable visual axes while\nkeeping the underlying control problem fixed. By\nconstruction, varying a visual axis affects perfor-\nmance only through the induced state-conditional\naction distribution of a pixel policy, providing a\nclean abstraction for visual generalization. Build-\ning on this environment, we define KAGE-Bench,\na benchmark of six known-axis suites comprising\n34 train\u2013evaluation configuration pairs that isolate\nindividual visual shifts. Using a standard PPO-\nCNN baseline, we observe strong axis-dependent\nfailures, with background and photometric shifts\noften collapsing success, while agent-appearance\nshifts are comparatively benign. Several shifts pre-\nserve forward motion while breaking task com-\npletion, showing that return alone can obscure\ngeneralization failures. Finally, the fully vector-\nized JAX implementation enables up to 33M en-\nvironment steps per second on a single GPU, en-\nabling fast and reproducible sweeps over visual\nfactors. Code: https://avanturist322.\ngithub.io/KAGEBench/.\n1. Introduction\nReinforcement learning (RL) agents trained from high-\ndimensional pixel observations are brittle to changes in ap-\npearance, lighting, and other visual nuisance factors (Cetin\net al., 2022; Yuan et al., 2023; Klepach et al., 2025). Policies\nthat perform well in-distribution can degrade sharply under\n1MIRIAI, Moscow, Russia\n2Cognitive AI Systems Lab,\nMoscow, Russia.\nCorrespondence to:\nEgor Cherepanov\n<cherepanov.e@miriai.org>.\nPreprint. January 21, 2026.\nFigure 1. Representative observations from KAGE-Env illus-\ntrating controlled, known-axis visual variation. Each panel\ndiffers along one or more explicitly configurable axes, including\nbackground imagery and color, agent appearance and animation,\nmoving distractors, photometric filters, and dynamic lighting ef-\nfects, while task semantics and underlying dynamics are held fixed.\npurely visual distribution shifts, even when task semantics,\ntransition dynamics, and rewards are unchanged (Staroverov\net al., 2023; Kachaev et al., 2025; Mirjalili et al., 2025).\nThis brittleness poses a fundamental obstacle to real-world\ndeployment, where observations inevitably vary due to view-\npoint changes, illumination, surface appearance, and sen-\nsor noise while the control-relevant latent state remains\nfixed (Raileanu et al., 2020; Kostrikov et al., 2020; Kir-\nilenko et al., 2023; Korchemnyi et al., 2024; Yang et al.,\n2024; Ugadiarov et al., 2026). As a result, pixel-based RL\npolicies that rely on incidental visual correlations can fail\nabruptly despite convergence, undermining reliability in\nrobotics, autonomous navigation, and interactive environ-\nments (Stone et al., 2021; Yuan et al., 2023). More broadly,\nvisual generalization is needed wherever models must ro-\nbustly extract information from visual structure, even in\nscientific texts and figures (Sherki et al., 2025).\nDespite substantial progress in representation learning (Ma-\nzoure et al., 2021; Rahman & Xue, 2022; Ortiz et al., 2024)\nand data augmentation (Laskin et al., 2020; Raileanu et al.,\n2020; Hansen & Wang, 2021), understanding visual general-\nization failures remains challenging. A central obstacle lies\nin evaluation benchmarks, which often entangle multiple\n1\narXiv:2601.14232v1  [cs.LG]  20 Jan 2026\nKAGE-Bench: Known-Axis Visual Generalization\nTrain\nStandard Evalutation\nKAGE-Bench\nEval\nTrain\nChange in\nonly one\nparameter\nChange in only one\nparameter\nChange in all three\nparameters!\nChange in all three\nparameters!\nEval\nFigure 2. KAGE-Bench: Motivation. Existing generalization\nbenchmarks entangle multiple sources of visual shift between train-\ning and evaluation, making failures difficult to attribute. KAGE-\nBench factorizes observations into independently controllable axes\nand constructs train\u2013evaluation splits that vary one (or a selected\nset) of axes at a time, enabling precise diagnosis of which visual\nfactors drive generalization gaps. The observation vector notation\n|\u03c8\u27e9is used for intuition only.\nvisual and structural changes such as background appear-\nance, geometry, dynamics, and distractors (Cobbe et al.,\n2020; Stone et al., 2021; Yuan et al., 2023). In these set-\ntings, train\u2013evaluation performance gaps cannot be cleanly\nattributed to specific sources of shift, and failures may re-\nflect visual sensitivity, altered task structure, or interactions\nbetween confounded factors. Compounding this issue, many\npixel-based RL environments are computationally expen-\nsive to simulate, limiting large-scale ablations and slowing\nhypothesis testing.\nWe address these limitations with KAGE-Bench (Known-\nAxis Generalization Evaluation Benchmark), a visual gen-\neralization benchmark in which sources of distribution\nshift are isolated by construction. KAGE-Bench is built\non KAGE-Env (Figure 1), a JAX-native (Bradbury et al.,\n2018) 2D platformer whose observation process is factor-\nized into independently controllable visual axes while la-\ntent dynamics and rewards are held fixed (see Figure 2).\nUnder this known-axis design, each axis corresponds to a\nwell-defined component of the observation kernel, and any\ntrain\u2013evaluation performance difference arises solely from\nhow a fixed observation-based policy responds to different\nrenderings of the same latent states, enabling unambiguous\nattribution of visual generalization failures.\nSystematic analysis of visual generalization requires eval-\nuating many controlled shifts at scale. KAGE-Env is im-\nplemented entirely in JAX with end-to-end jit compila-\ntion and vectorized execution via vmap and lax.scan,\nenabling efficient large-batch simulation on a single accel-\nerator. In practice, this design scales up to 216 parallel\nenvironments on one GPU and achieves up to 33M environ-\nment steps per second (see Figure 3), making exhaustive\nsweeps over visual parameters and fine-grained diagnosis\nof generalization behavior feasible.\n1https://colab.research.google.com/\n20\n22\n24\n26\n28\n210\n212\n214\n216\nNumber of environments nenvs\n10\n3\n10\n4\n10\n5\n10\n6\n10\n7\n10\n8\nSteps Per Second\nH100 (80GB)\nA100 (80GB)\nV100 (32GB)\nApple M3 Pro\nT4 (15GB)\n(a) Easy configuration.\n20\n22\n24\n26\n28\n210\n212\n214\nNumber of environments nenvs\n10\n3\n10\n4\n10\n5\n10\n6\n10\n7\nSteps Per Second\nH100 (80GB)\nA100 (80GB)\nV100 (32GB)\nApple M3 Pro\nT4 (15GB)\n(b) Hard configuration.\nFigure 3. Environment stepping throughput vs. parallelism.\nEnvironment stepping throughput (steps per second, higher is bet-\nter) as a function of the number of parallel environments nenvs for\nKAGE-Env across heterogeneous hardware backends. GPU re-\nsults are shown for NVIDIA H100 (80 GB), A100 (80 GB), V100\n(32 GB), and T4 (15 GB, Google Colab1), with CPU-only results\non an Apple M3 Pro laptop. (a) Easy configuration: lightweight\nsetup with all visual generalization parameters disabled. (b) Hard\nconfiguration: most demanding setup with all visual generaliza-\ntion parameters enabled at maximum values.\nBuilding on this environment, we construct six visual gen-\neralization suites comprising 34 train\u2013evaluation configu-\nration pairs, each targeting a specific visual axis. Using\nthese suites, we demonstrate that visual generalization is\nstrongly axis-dependent and identify classes of visual shifts\nthat reliably induce severe performance degradation, even\nfor a standard PPO-CNN baseline (Schulman et al., 2017).\nWe summarize our main contributions as follows:\n1. KAGE-Env, a JAX-native RL environment with\n93 explicitly controllable parameters, config-\nurable via a single .yaml file and vectorized\nto reach up to 33M environment steps per second\nwith 216 parallel environments on a single GPU.\n2. KAGE-Bench, a benchmark that isolates visual\ndistribution shifts by construction via six known-\naxis suites and 34 train\u2013evaluation configuration\npairs with fixed dynamics and rewards.\n3. Empirical diagnosis of visual generalization:\nusing a PPO-CNN baseline, we quantify how\nvisual generalization behavior differs across axes\nand identify classes of visual shifts that reliably\ninduce severe performance degradation.\n2. Related Work\nVisual generalization in RL.\nVisual generalization stud-\nies whether policies trained from pixel observations retain\nperformance when the observation process changes while\nlatent dynamics and rewards remain fixed. Prior work shows\nthat agents often overfit incidental visual features, leading\nto substantial train\u2013test gaps across a wide range of envi-\nronments and settings (Cobbe et al., 2019; Beattie et al.,\n2016; Xia et al., 2018; Ortiz et al., 2024). A common expla-\n2\nKAGE-Bench: Known-Axis Visual Generalization\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nStep\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nTrain\nEval\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nStep\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nTrain\nEval\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nStep\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nTrain\nEval\nFigure 4. Examples of visual generalization gaps. Success rate for three train\u2013evaluation pairs showing (left) negligible, (middle)\nmoderate, and (right) severe generalization gaps.\nnation is that standard architectures and objectives exploit\nspurious visual correlations, such as background textures\nor color statistics, rather than learning task-relevant invari-\nances (Cobbe et al., 2020; Hansen & Wang, 2021; Stone\net al., 2021). Accordingly, many approaches have been pro-\nposed to improve robustness, including data augmentation,\nauxiliary representation learning objectives, and regulariza-\ntion methods (Laskin et al., 2020; Raileanu et al., 2020;\nMazoure et al., 2021; Raileanu & Fergus, 2021; Cobbe\net al., 2021; Wang et al., 2020; Bertoin & Rachelson, 2022;\nBertoin et al., 2022; Zisselman et al., 2023; Rahman & Xue,\n2022; Jesson & Jiang, 2024). KAGE-Env and KAGE-Bench\nprovide diagnostic infrastructure for this literature by en-\nabling fast, controlled, axis-specific evaluation that isolates\nchanges in the observation kernel.\nBenchmarks for visual generalization in RL.\nA range of\nbenchmarks study visual generalization in pixel-based RL,\ndiffering in task domains and in how explicitly they isolate\nsources of visual variation. RL-ViGen (Yuan et al., 2023)\nspans multiple domains, including locomotion, manipula-\ntion, navigation, and driving, with shifts in textures, lighting,\nviewpoints, layouts, and embodiments. Hansen & Wang\n(2021) evaluates continuous control under controlled ap-\npearance changes such as color randomization and dynamic\nvideo backgrounds. Obstacle Tower (Juliani et al., 2019)\nand LevDoom (Tomilin et al., 2022) consider 3D settings\nwhere many factors vary jointly, making attribution of fail-\nures to specific visual causes difficult. Related benchmarks\nsuch as DMC-VB (Ortiz et al., 2024) and Distracting Meta-\nWorld (Kim et al., 2024) introduce task-irrelevant visual\ndistractors while keeping task dynamics fixed.\nAmong widely used benchmarks, Procgen (Cobbe et al.,\n2020) relies on procedural generation, so train\u2013test gaps\ntypically reflect entangled shifts in appearance and scene\ncomposition rather than isolated visual factors. The Dis-\ntracting Control Suite (DCS) (Stone et al., 2021) introduces\nexplicit distraction axes but is limited to a small set of fac-\ntors, and broad axis-wise sweeps are costly in its underly-\ning continuous-control simulator. KAGE-Env and KAGE-\nBench complement these benchmarks by explicitly factoriz-\ning the observation process into independently controllable\nvisual axes. KAGE-Env uses a simple platformer to re-\nduce optimization and exploration confounds, while KAGE-\nBench constructs train\u2013evaluation splits that vary specified\naxes (e.g., backgrounds, sprites, distractors, filters, and light-\ning) with fixed dynamics and rewards, enabling systematic,\naxis-specific attribution of generalization failures.\nFast and scalable evaluation in RL.\nEvaluating gener-\nalization in RL is sample intensive, as reliable conclusions\nrequire averaging over random seeds, environment instances,\nand distribution shifts. In visual generalization benchmarks,\nthis leads to combinatorial scaling Nsteps \u00d7 Nseeds \u00d7 Nshifts,\noften compounded by checkpointing and hyperparameter\nsweeps, making evaluation costly in CPU-bound simulators.\nRecent work addresses this bottleneck through accelerator-\nnative RL systems, where environment stepping is imple-\nmented as compiled, vectorized computation on GPUs or\nTPUs. Examples include JAX-based simulators such as\nBrax (Freeman et al., 2021), Jumanji (Bonnet et al., 2023),\nXLand-MiniGrid (Nikulin et al., 2024), CAMAR (Pshen-\nitsyn et al., 2025), and Craftax (Matthews et al., 2024), as\nwell as GPU-native platforms such as ManiSkill3 (Tao et al.,\n2024), MIKASA-Robo (Cherepanov et al., 2025), and Warp-\nDrive (Lan et al., 2021). By eliminating host-side control\nflow, these systems achieve orders-of-magnitude through-\nput. However, high throughput alone does not yield diag-\nnostic evaluation of visual robustness. Benchmarks such as\nProcgen and DCS do not support exhaustive, axis-isolated\nsweeps over rendering factors, limiting failure attribution.\nKAGE-Env combines the accelerator-native paradigm with\nexplicit factorization of the observation process into indepen-\ndently controllable axes, enabling large-batch, reproducible\nevaluation of known-axis visual shifts under fixed latent\ndynamics and rewards.\n3. Background\nPartially Observable Markov Decision Processes.\nWe\nconsider episodic control with horizon T in a partially ob-\nservable Markov decision process (POMDP). Each environ-\nment instance is indexed by a visual configuration \u03be \u2208\u039e\nand defined as M\u03be = (S, A, P, r, \u2126, O\u03be, \u03c10, \u03b3), where S\nis the latent (control-relevant) state space, A is the action\n3\nKAGE-Bench: Known-Axis Visual Generalization\nspace, P(\u00b7 | s, a) is the transition kernel, r(s, a) is the re-\nward function, \u2126is the observation space, O\u03be(\u00b7 | s) is the\nobservation (rendering) kernel parameterized by \u03be, \u03c10 is\nthe initial state distribution, and \u03b3 \u2208[0, 1) is the discount\nfactor. At each timestep t, the environment occupies a latent\nstate st \u2208S. An observation is generated according to\not \u223cO\u03be(\u00b7 | st), ot \u2208\u2126\u2286{0, . . . , 255}H\u00d7W \u00d73. Based on\nthis observation, the agent selects an action at \u2208A, receives\nreward r(st, at), and transitions to st+1 \u223cP(\u00b7 | st, at).\nA key structural property enforced throughout this work is\nthat the transition kernel P and reward function r are inde-\npendent of the visual configuration \u03be. All dependence on\n\u03be is confined to the observation kernel O\u03be. Consequently,\nthe same latent state st may give rise to different observa-\ntions under different values of \u03be, while inducing identical\ndynamics and rewards. Visual generalization concerns the\nbehavior of policies under such changes in the observation\nprocess, with the underlying control problem held fixed.\nPolicies and return.\nWe focus on reactive pixel-based\npolicies that map observations directly to action distribu-\ntions: \u03c0(a | o). The expected discounted return of a policy\n\u03c0 in environment M\u03be is\nJ(\u03c0; M\u03be) = E\ns0\u223c\u03c10,\not\u223cO\u03be(\u00b7|st),\nat\u223c\u03c0(\u00b7|ot),\nP\n\"T \u22121\nX\nt=0\n\u03b3tr(st, at)\n#\n.\n(1)\nVisual generalization.\nWe study generalization under\nshifts in visual parameters that affect observations but not\nthe underlying control problem. Let \u039e denote the space of\nvisual configurations, and let Dtrain and Deval be probability\ndistributions over \u039e. Each \u03be \u2208\u039e induces a visual POMDP\nM\u03be through its observation kernel O\u03be, while sharing the\nsame latent dynamics P and reward function r.\nA pixel policy \u03c0(a | o) is trained using environments with\n\u03be \u223cDtrain and evaluated under \u03be \u223cDeval. For any distri-\nbution D over \u039e, we define the expected performance\nJ(\u03c0; D) = E\u03be\u223cD[J(\u03c0; M\u03be)] .\n(2)\nWe refer to this setting as visual generalization when the\nshift from Dtrain to Deval changes only the observation\nkernels O\u03be, while preserving the latent state space, transition\ndynamics, and reward function.\nKnown-axis visual shifts.\nKAGE-Bench focuses on\nknown-axis visual generalization. Each visual configuration\nis decomposed as \u03be = (\u03beaxis, \u03berest), where \u03beaxis specifies a\ndesignated axis of visual variation (e.g., background appear-\nance, agent sprites, lighting, filters), and \u03berest contains all\nremaining parameters. By construction, any performance\ndifference between training and evaluation can therefore be\nattributed to changes in the observation process along the\nspecified visual axis, rather than to changes in task struc-\nture, dynamics, or rewards. This intuition is formalized and\njustified in Section 4 and Appendix A.\nEvaluation metrics.\nGiven Dtrain and Deval, we re-\nport in-distribution and out-of-distribution performance,\nJ(\u03c0; Dtrain) and J(\u03c0; Deval), and define the return-based\ngeneralization gap\n\u2206(\u03c0) = J(\u03c0; Dtrain) \u2212J(\u03c0; Deval).\n(3)\nWhile \u2206(\u03c0) provides a coarse measure of performance\ndegradation under visual shift, it is insufficient to fully char-\nacterize generalization behavior. The discounted return\naggregates multiple effects, including reward shaping, ex-\nploration inefficiency, and penalty terms, and may obscure\nwhether an agent nearly solves the task or fails catastroph-\nically. In particular, if a policy fails under both training\nand evaluation configurations, the return gap can be small\ndespite the absence of task competence.\nFor this reason, we complement return-based evaluation\nwith additional trajectory-level metrics that are measurable\nfunctions of the latent state trajectory, including distance\ntraveled, normalized progress toward the goal, and binary\ntask success. These metrics distinguish partial progress\nfrom complete failure and provide a more fine-grained view\nof visual generalization behavior. Their precise definitions\nand empirical use are described in Section 6.\n4. Known-axis visual generalization\nThis section states the formal principle behind KAGE-\nBench. In our construction (Section 3), the latent control\nproblem is fixed and only the renderer changes: \u03be affects\nperformance only through the induced state-conditional ac-\ntion law obtained by composing the observation kernel with\nthe pixel policy. The goal is to make this channel explicit\nand to justify the benchmark protocol: (i) constructing suites\nthat intervene on a single visual axis, and (ii) evaluating not\nonly return but also trajectory-level metrics such as distance,\nprogress, and success.\nFrom pixel policies to state-conditional behavior.\nA re-\nactive pixel policy \u03c0(\u00b7 | o) maps observations to actions\nand does not directly specify an action distribution condi-\ntioned on the latent state s. However, in a visual POMDP\nM\u03be, the observation kernel O\u03be(\u00b7 | s) induces a distribution\nover rendered observations for each latent state. Composing\nthese kernels yields a well-defined state-conditional action\ndistribution by marginalizing the intermediate observation:\ns\nO\u03be(\u00b7|s)\n\u2212\u2212\u2212\u2212\u2212\u2192o\n\u03c0(\u00b7|o)\n\u2212\u2212\u2212\u2212\u2192a.\n(4)\nUnder our construction (and for reactive policies), this com-\nposition is the only mechanism by which the visual con-\n4\nKAGE-Bench: Known-Axis Visual Generalization\nfiguration \u03be can affect control, since P and r are invariant\nacross \u03be. Figure 5 illustrates this marginalization in a con-\ncrete discrete example.\nDefinition 4.1 (Induced state policy). Fix \u03be \u2208\u039e, observa-\ntion kernel O\u03be(\u00b7 | s), and reactive pixel policy \u03c0(\u00b7 | o). The\ninduced state policy \u03c0\u03be is defined by\n\u03c0\u03be(a | s) :=\nZ\n\u2126\n\u03c0(a | o) O\u03be(do | s),\n\u2200s \u2208S, \u2200a \u2208A.\n(5)\nFor a fixed pixel policy \u03c0, the map \u03be 7\u2192\u03c0\u03be summarizes the\neffect of visual variation on state-conditional behavior. In\nparticular, changing \u03be changes \u03c0\u03be while leaving the latent\ncontrol problem (S, A, P, r, \u03c10, \u03b3) unchanged.\nVisual shift is equivalent to induced policy shift.\nThe\nnext theorem formalizes the reduction used throughout\nKAGE-Bench: executing \u03c0 in the visual POMDP M\u03be in-\nduces the same latent state\u2013action law as executing \u03c0\u03be in\nthe latent MDP M.\nTheorem 4.2 (Visual generalization reduces to induced pol-\nicy shift). Fix any \u03be \u2208\u039e and reactive pixel policy \u03c0(\u00b7 | o),\nand let \u03c0\u03be be defined by Definition 4.1. Then:\n1. (Conditional action law.) \u2200t \u22650, \u2200a \u2208A,\nPM\u03be,\u03c0(at = a | st) = \u03c0\u03be(a | st)\na.s.\n(6)\n2. (Equality in law of state\u2013action processes.) The state\u2013\naction process (st, at)t\u22650 induced by executing \u03c0 in M\u03be\nhas the same law as the state\u2013action process induced by\nexecuting \u03c0\u03be in the latent MDP M.\n3. (Return equivalence.) Consequently,\nJ(\u03c0; M\u03be) = J(\u03c0\u03be; M).\n(7)\nTheorem 4.2 is purely representational: it does not assume\noptimality and it does not modify the control problem. A\nuseful consequence is the identity, for any \u03be, \u03be\u2032 \u2208\u039e,\nJ(\u03c0; M\u03be) \u2212J(\u03c0; M\u03be\u2032) = J(\u03c0\u03be; M) \u2212J(\u03c0\u03be\u2032; M), (8)\nwhich states that a visual train\u2013evaluation gap for a fixed\npixel policy \u03c0 is exactly a performance difference between\ninduced state policies in the same latent MDP. This is the\nformal basis for attributing failures to the observation pro-\ncess: since (P, r) are unchanged, any degradation under\n\u03be \u2192\u03be\u2032 must be explained by how the renderer changes the\ninduced state-conditional behavior \u03c0\u03be.\nWhy known-axis suites enable axis-specific attribution.\nKAGE-Bench constructs axis-isolated suites by decompos-\ning \u03be = (\u03beaxis, \u03berest) and pairing train and evaluation con-\nfigurations that differ only in the designated axis:\n\u03betrain = (\u03betrain\naxis , \u03berest) and \u03beeval = (\u03beeval\naxis, \u03berest).\n(9)\nObservation Kernel\nPixel Policy\nAction Space\nObservation Space\nInduced\nState Policy\nFigure 5. Induced state policy. The renderer O\u03be(\u00b7 | s) maps\na latent state to an observation distribution, and the pixel policy\n\u03c0(\u00b7 | o) maps observations to actions. Their composition defines\n\u03c0\u03be(\u00b7 | s) by marginalizing o.\nEquivalently, for all s \u2208S the paired renderers satisfy\nO\u03betrain(\u00b7 | s) = O(\u00b7 | s; \u03betrain\naxis , \u03berest) and O\u03beeval(\u00b7 | s) =\nO(\u00b7 | s; \u03beeval\naxis, \u03berest), so the only change in the observation\nprocess is along \u03beaxis. Under this controlled-intervention\ndesign, the induced policies \u03c0\u03betrain and \u03c0\u03beeval differ only\nthrough this axis-dependent change in O\u03be. Therefore, by\nEquation 8, the measured gap isolates how that visual axis\nperturbs the induced state-conditional behavior of \u03c0.\nTrajectory-level consequences and evaluation metrics.\nBy Item 2 of Theorem 4.2, the latent state\u2013action trajec-\ntory has the same law under (M\u03be, \u03c0) and (M, \u03c0\u03be), so the\nreduction applies to any measurable trajectory functional,\nnot only return. We therefore report distance, progress, and\nsuccess in addition to episodic return: these are functions of\nthe latent trajectory exposed by KAGE-Env for evaluation,\nand their gaps under \u03be \u2192\u03be\u2032 admit the same induced-policy\ninterpretation. Unlike return, which can mask completion\nfailures due to reward shaping, these metrics separate partial\nprogress from task completion.\nCorollary 4.3 (Equivalence of trajectory-level evaluation\nmetrics). Fix \u03be \u2208\u039e and reactive \u03c0(\u00b7 | o), and let \u03c0\u03be be\nthe induced state policy. Let (st, at)t\u22650 \u223c(M\u03be, \u03c0) and\n(\u02dcst, \u02dcat)t\u22650 \u223c(M, \u03c0\u03be). Then for any measurable functional\nF : (S \u00d7 A)N \u2192R,\nF\n\u0000(st, at)t\u22650\n\u0001\nd= F\n\u0000(\u02dcst, \u02dcat)t\u22650\n\u0001\n,\nand in particular EM\u03be,\u03c0[F] = EM,\u03c0\u03be[F] whenever the\nexpectation is well-defined.\nCorollary 4.4 (Specialization to KAGE-Bench metrics). As-\nsume the latent state contains a one-dimensional position\nvariable xt \u2208R with initial position xinit and task comple-\ntion threshold D > 0. For a fixed horizon T (or terminal\ntime), define Fdist := xT \u2212xinit, Fprog := xT \u2212xinit\nD\n, and\nFsucc := I{xT \u2212xinit \u2265D}. Then each metric has the\nsame distribution under (M\u03be, \u03c0) and (M, \u03c0\u03be), and in par-\nticular EM\u03be,\u03c0[F] = EM,\u03c0\u03be[F], F \u2208{Fdist, Fprog, Fsucc}.\nAll proofs are deferred to Appendix A.\n5\nKAGE-Bench: Known-Axis Visual Generalization\n\u0004\nimport jax\nfrom kage_bench import (\nKAGE_Env,\nload_config_from_yaml,\n)\n# Create environment with custom config\nenv = KAGE_Env(\nload_config_from_yaml(\"custom_config.yaml\")\n)\n# Vectorize and JIT compile\nreset_vec = jax.jit(jax.vmap(env.reset))\nstep_vec = jax.jit(jax.vmap(env.step))\n# Initialize 65,536 parallel environments\nN_ENVS = 2**16\nkeys = jax.random.split(\njax.random.PRNGKey(42), N_ENVS\n)\n# Reset all at once\nobs, info = reset_vec(keys)\nstates = info[\"state\"]\n# Parallel step: Samples one random discrete\n# action per env in [0, 7] (bitmask actions)\nactions = jax.random.randint(\nkeys[0], (N_ENVS,), 0, 8\n)\n# obs.shape: (65536, 128, 128, 3)\nobs, rewards, terms, truncs, info \\\n= step_vec(states, actions)\nstates = info[\"state\"]\n\u0006\n\u0005\nCode 1. Python (JAX) usage. The environment is configured from\na .yaml file (e.g., custom config.yaml); the code shows\nJAX-vmap/jit batched reset/step over 216 parallel envs.\n5. KAGE-Environment\nKAGE-Env (Figure 1, Code 1) is a JAX-native RL environ-\nment designed for controlled evaluation of visual general-\nization. It implements the visual-POMDP interface from\nSection 3: configurations \u03be \u2208\u039e parameterize the renderer\nO\u03be(\u00b7 | s) while the latent control problem is held fixed.\nTask and interface.\nKAGE-Env is an episodic 2D side-\nscrolling platformer with horizon T and a push-scrolling\ncamera. At each timestep t, the agent observes a single RGB\nimage ot \u2208{0, . . . , 255}H\u00d7W \u00d73, with default resolution\nH = W = 128, and selects an action at from a discrete\naction space A = {0, . . . , 7}. Actions are encoded as a\nbitmask over three primitives: LEFT = 1, RIGHT = 2,\nand JUMP = 4. Policies interact with the environment\nexclusively through pixels; the latent simulator state is not\navailable to the policy and is exposed only via the info\ndictionary for logging and evaluation.\nReward and termination.\nLet xt \u2208R denote horizontal\nposition and xmax\nt\n:= max0\u2264k\u2264t xk the furthest position\nreached so far. The per-step reward is\nrt = \u03b11 max{0, xt+1 \u2212xmax\nt\n}\n|\n{z\n}\nfirst-time forward progress\n\u2212\n\u0010\n\u03b12 I[JUMP(at)] + \u03b13 + \u03b14 I[idle(xt, xt+1)]\n\u0011\n|\n{z\n}\npenalties\n,\n(10)\nwhere I[JUMP(at)] indicates the jump bit is active in at, \u03b13\nis a per-timestep time cost, and idle(xt, xt+1) flags lack of\nhorizontal progress. Episodes terminate only by time-limit\ntruncation at T = episode length.\nRendering assets and visual parameters.\nKAGE-Env\nprovides a library of visual assets and rendering controls\nfor constructing visual variation. Assets include 128 back-\nground images (Appendix, Figure 43) and 27 animated\nsprite skins for the agent and non-player characters (Ap-\npendix, Figure 44); when sprites are disabled, entities can\nbe rendered as geometric shapes (9 types) with a palette of\n21 colors. The renderer further exposes photometric and\nspatial transformations (e.g., brightness, contrast, gamma,\nhue, blur, noise, pixelation, vignetting) and lighting/overlay\neffects such as dynamic point lights with configurable count,\nintensity, radius, falloff, and color.\nConfiguration interface.\nAll parameters are specified\nthrough a single .yaml configuration file (Code 2).\nA configuration \u03be\n\u2208\n\u039e is organized into groups\nbackground,\ncharacter,\nnpc,\ndistractors,\nfilters, effects, layout, and physics. These\ngroups include rendering parameters (affecting only O\u03be)\nas well as optional control parameters (affecting P or r).\nKAGE-Env exposes both for extensibility; isolation of\npurely visual shifts is enforced by the KAGE-Bench pairing\nprotocol (Section 6).\n6. KAGE-Benchmark\nKAGE-Bench is a benchmark protocol built on top of\nKAGE-Env. It specifies how environment configurations\nare selected and paired to evaluate known-axis visual gener-\nalization. Concretely, KAGE-Bench defines a set of train\u2013\nevaluation configuration pairs (\u03betrain, \u03beeval) such that the\n\u0004\nbackground:\nmode: \"image\"\nimage_paths:\n- \"src/kage/assets/backgrounds/bg-1.jpeg\"\n- \"src/kage/assets/backgrounds/bg-64.jpeg\"\n- \"src/kage/assets/backgrounds/bg-128.jpeg\"\nparallax_factor: 0.5\nswitch_frequency: 0.0\ncharacter:\nmode: \"sprite\"\nsprite_paths:\n- \"src/kage/assets/sprites/clown\"\n- \"src/kage/assets/sprites/skeleton\"\nenable_animation: true\nanimation_fps: 12.0\nnpc:\nmode: \"sprite\"\nsprite_dir: \"src/kage/assets/sprites\"\nfilters:\nbrightness: 0.0\nhue_shift: 0.0\n\u0006\n\u0005\nCode 2.\nYAML configuration.\nKAGE-Env is config-\nured via a single .yaml file; shown is a small excerpt of\ncustom config.yaml. We show only a small part of all con-\nfiguration parameters; for details, see Appendix G.\n6\nKAGE-Bench: Known-Axis Visual Generalization\nTable 1. Axis-level summary of KAGE-Bench results (mean\u00b1SEM). During training of each run, we record the maximum value attained\nby each metric. For each configuration, these per-run maxima are averaged across 10 random seeds, and the resulting per-configuration\nvalues are then averaged across all configurations within each generalization-axis suite. We report Distance, Progress, Success Rate\n(SR), and Return for train and eval configurations, along with the corresponding generalization gaps (mean\u00b1SEM). Generalization\ngaps are color-coded: green indicates smaller gaps (better generalization), while red indicates larger gaps (worse generalization).\n\u2206Dist. = Dist.train\u2212Dist.eval\nDist.train\n\u00d7 100%, \u2206Prog. = Progresstrain\u2212Progresseval\nProgresstrain\n\u00d7 100%, \u2206SR = SRtrain\u2212SReval\nSRtrain\n\u00d7 100%, \u2206Ret. = |Ret.train \u2212Ret.eval|.\nEvaluation on train config\nEvaluation on eval config\nGeneralization gap\nDistance\nProgress\nSR\nReturn\nDistance\nProgress\nSR\nReturn\n\u2206Dist., %\n\u2206Prog., %\n\u2206SR, %\n\u2206Ret., (abs.)\nAgent\n396.5\u00b126.8\n0.81\u00b10.05\n0.76\u00b10.06\n-292.73\u00b177.21\n386.9\u00b126.1\n0.79\u00b10.05\n0.60\u00b10.06\n-408.8\u00b188.5\n2.4\n2.5\n21.1\n116.1\nBackground\n463.4\u00b110.4\n0.95\u00b10.02\n0.90\u00b10.02\n-118.3\u00b132.6\n322.7\u00b147.5\n0.66\u00b10.10\n0.42\u00b10.13\n-935.8\u00b1249.6\n30.5\n30.5\n53.3\n691.0\nDistractors\n413.5\u00b122.9\n0.84\u00b10.05\n0.81\u00b10.05\n-178.0\u00b141.5\n397.0\u00b123.6\n0.81\u00b10.05\n0.56\u00b10.11\n-307.0\u00b178.1\n4.0\n3.6\n30.9\n129.0\nEffects\n426.3\u00b115.6\n0.87\u00b10.03\n0.82\u00b10.03\n-224.3\u00b164.0\n337.7\u00b110.8\n0.69\u00b10.02\n0.16\u00b10.06\n-725.1\u00b165.6\n20.8\n20.7\n80.5\n500.8\nFilters\n431.2\u00b119.3\n0.88\u00b10.04\n0.83\u00b10.04\n-204.8\u00b159.6\n380.6\u00b118.1\n0.78\u00b10.04\n0.11\u00b10.04\n-652.4\u00b170.5\n11.7\n11.4\n86.8\n447.6\nLayout\n452.3\u00b10.0\n0.92\u00b10.00\n0.86\u00b10.00\n-118.6\u00b10.0\n434.1\u00b10.0\n0.89\u00b10.00\n0.32\u00b10.00\n-279.5\u00b10.0\n4.0\n3.3\n62.8\n160.9\nunderlying control problem is identical (P train = P eval,\nrtrain = reval) and the two configurations differ only in a\ndesignated subset of rendering parameters.\nBenchmark construction.\nWe first conduct a pilot sweep\nover KAGE-Env\u2019s rendering parameters using a standard\nPPO-CNN, adopted from the CleanRL (Huang et al., 2022)\nlibrary2, trained from a single RGB frame. Hyperparam-\neters are reported in Appendix, Table 4. This sweep mea-\nsures how individual rendering parameters affect out-of-\ndistribution performance when the control problem is fixed.\nBased on these results, we curate 34 train\u2013evaluation con-\nfiguration pairs that exhibit a range of generalization be-\nhavior, including both severe and mild gaps. The selected\npairs are grouped into six suites corresponding to distinct\nvisual axes: agent appearance, background, distractors, ef-\nfects, filters, and layout. In each pair, exactly one parameter\nwithin the target axis is changed between train and evalua-\ntion, while all other parameters are held fixed. Easier pairs\nare intentionally retained as sanity checks, ensuring that the\nbenchmark distinguishes lack of generalization from lack\nof task competence.\nEvaluation protocol and metrics.\nFor each train\u2013\nevaluation configuration pair, we run 10 independent train-\ning seeds and periodically evaluate the current policy on\nboth configurations. For each run and metric, we record\nthe maximum value attained over training, average these\nmaxima across seeds to obtain per-configuration results,\nand then average within each suite to produce the axis-level\nsummaries in Table 1. We use the maximum-over-training\nstatistic to assess whether a visual generalization gap is\nin principle mitigable by a given method. Because gener-\nalization performance can be non-monotonic and peak at\ndifferent iterations across runs, this aggregation provides an\nupper envelope on achievable transfer and avoids confound-\ning results with arbitrary checkpoint selection.\nGeneralization gap.\nWe define the visual generalization\ngap as the performance difference between the training and\nevaluation configurations of a pair. Figure 4 illustrates three\n2https://github.com/vwxyzjn/cleanrl\ncharacteristic regimes observed in KAGE-Bench: (i) negli-\ngible gap, where train and eval performance coincide; (ii)\nmoderate gap, where partial transfer occurs; and (iii) severe\ngap, where evaluation performance collapses despite strong\ntraining performance. Full learning curves for all 34 config-\nuration pairs and all suites are reported in Appendix C.\n7. Results\nTable 1 reports axis-level results for PPO-CNN under\nour maximum-over-training protocol: for each seed we\ntake the maximum of each metric over training check-\npoints, then average across 10 seeds and finally across\nconfiguration pairs within an axis.\nFigure 6 comple-\nments this summary with representative difficulty-scheduled\nevaluations:\n(left) we train on a black background\nand evaluate on progressively richer backgrounds (black,\nblack+white, black+white+red, black+white+red+green,\nblack+white+red+green+blue); (right) we train with no dis-\ntractors and evaluate with increasing numbers of same-as-\nagent distractors (0, 1, 2, 3, 5, 7, 9, 11), where distractors\nmatch the agent\u2019s shape and color. Across suites, training\nsuccess rises rapidly, while evaluation success often satu-\nrates substantially lower, revealing persistent train\u2013eval gaps\nunder purely visual shifts with fixed dynamics and rewards.\nGeneralization is strongly axis-dependent (mean\u00b1SEM).\nRanking axes by success-rate degradation, the largest gaps\n0.00\n0.25\n0.50\n0.75\n1.00\nStep\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\ntrain\nval_b\nval_bw\nval_bwr\nval_bwrg\nval_bwrgb\n0.00\n0.25\n0.50\n0.75\n1.00\nStep\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\ntrain\nval_0\nval_1\nval_2\nval_3\nval_5\nval_7\nval_9\nval_11\nFigure 6. Visual generalization gaps in single-axis shifts. Each\npanel shows training success rate (blue) and evaluation on pro-\ngressively harder visual variants (colored curves). (Left) Back-\ngrounds: trained on black background, evaluated with cumulative\ncolor additions (black \u2192black+white \u2192black+white+red \u2192etc.).\n(Right) Distractors: trained without distractors, evaluated with\nincreasing numbers of same-as-agent distractors. Full results are\npresented in the Appendix B, Figure 7.\n7\nKAGE-Bench: Known-Axis Visual Generalization\nTable 2. Per-configuration results for KAGE-Bench (mean\u00b1SEM). Each row corresponds to a train-evaluation configuration pair within\na known-axis suite. For each run, we record the maximum value attained by each metric during training; these maxima are then averaged\nacross 10 random seeds."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning\n\n#### Main Contributions\nKAGE-Bench introduces a novel benchmark designed to isolate and evaluate visual generalization in reinforcement learning (RL) environments. The key contributions are:\n1. **KAGE-Env**: A JAX-native 2D platformer environment that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed.\n2. **KAGE-Bench**: A benchmark protocol comprising six known-axis suites with 34 train-evaluation configuration pairs, isolating individual visual shifts and enabling precise diagnosis of generalization failures.\n3. **Empirical Diagnosis**: Using a standard PPO-CNN baseline, empirical analysis quantifies how visual generalization behavior differs across axes and identifies classes of visual shifts that reliably induce severe performance degradation.\n\n#### Methodology\n**KAGE-Env** is designed to isolate visual shifts by factorizing the observation process into independent visual axes while keeping latent dynamics and rewards fixed. The environment supports vectorized execution via JAX's `vmap` and `lax.scan`, enabling up to 33 million environment steps per second on a single GPU.\n\n**KAGE-Bench** is constructed from KAGE-Env, defining six known-axis suites with 34 train-evaluation configuration pairs. Each pair varies only one (or a selected set) of axes at a time, allowing precise analysis of which visual factors drive generalization gaps.\n\n#### Key Findings\n1. **Strong Axis-Dependent Failures**: Pixel-based RL agents often fail under purely visual distribution shifts even when task semantics and underlying dynamics are unchanged.\n2. **Background and Photometric Shifts**: Background imagery and photometric filters significantly affect performance, often collapsing success despite training on the same latent states.\n3. **Agent Appearance Shifts**: Agent appearance changes have a more benign impact compared to background and photometric variations.\n4. **Task Completion Failures**: Moving distractors can break task completion while maintaining forward motion, highlighting the complexity of visual generalization.\n\n#### Conclusions\n1. **Visual Generalization is Strongly Axis-Dependent**: The benchmark demonstrates that visual generalization failures are strongly tied to specific visual axes and configurations.\n2. **Effective Evaluation Protocol**: KAGE-Bench provides a robust framework for evaluating visual generalization, enabling fast and reproducible sweeps over visual factors.\n3. **Diagnostic Power**: Empirical analysis using the PPO-CNN baseline allows for precise characterization of how different visual shifts affect policy performance.\n\nKAGE-Bench offers a powerful toolset for researchers to systematically study and address visual generalization in RL environments, providing insights into the robustness and limitations of pixel-based policies under varying visual conditions."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Intermittent time series forecasting: local vs global\nmodels\nStefano Damato1*, Nicol\u00b4o Rubattu1, Dario Azzimonti1,\nGiorgio Corani1\n1*Scuola Universitaria Professionale della Svizzera Italiana (SUPSI),\nIstituto Dalle Molle di Studi sull\u2019Intelligenza Artificiale (IDSIA),\nLugano, Switzerland.\n*Corresponding author(s). E-mail(s): stefano.damato@supsi.ch;\nAbstract\nIntermittent time series, characterised by the presence of a significant amount of\nzeros, constitute a large percentage of inventory items in supply chain. Probabilis-\ntic forecasts are needed to plan the inventory levels; the predictive distribution\nshould cover non-negative values, have a mass in zero and a long upper tail.\nIntermittent time series are commonly forecast using local models, which are\ntrained individually on each time series. In the last years global models, which\nare trained on a large collection of time series, have become popular for time\nseries forecasting. Global models are often based on neural networks. However,\nthey have not yet been exhaustively tested on intermittent time series.\nWe carry out the first study comparing state-of-the-art local (iETS, TweedieGP)\nand global models (D-Linear, DeepAR, Transformers) on intermittent time series.\nFor neural networks models we consider three different distribution heads suitable\nfor intermittent time series: negative binomial, hurdle-shifted negative binomial\nand Tweedie. We use, for the first time, the last two distribution heads with\nneural networks.\nWe perform experiments on five large datasets comprising more than 40\u2019000 real-\nworld time series. Among neural networks D-Linear provides best accuracy; it\nalso consistently outperforms the local models. Moreover, it has also low compu-\ntational requirements. Transformers-based architectures are instead much more\ncomputationally demanding and less accurate. Among the distribution heads, the\nTweedie provides the best estimates of the highest quantiles, while the negative\nbinomial offers overall the best performance.\nKeywords: Global models, Intermittent time series, Probabilistic forecasting,\nTweedie distribution, Quantile loss\n1\narXiv:2601.14031v1  [stat.ML]  20 Jan 2026\n1 Introduction\nIn supply chain and demand planning, it can be necessary to forecast a large number of\nintermittent time series (Kolassa 2016; Boylan and Syntetos 2021), i.e., non-negative\ntime series which contain zeros and are typically count-valued. Decision making, for\ninstance to set the safety amounts (Kolassa 2016), requires probabilistic forecasts.\nSuch forecasts call for specific modelling approaches, whose predictive distribution\ncover non-negative values, has a mass in zero and a long upper tail. Most forecasting\nmodels are instead designed for smooth time series, which are real-valued and assume\nGaussian errors.\nProbabilistic models for intermittent time series, including the most recent ones\n(Svetunkov and Boylan 2023; Damato et al. 2025), are trained locally, i.e., on individual\ntime series. A recent approach is instead to train global forecasting models (Montero-\nManso and Hyndman 2021; Hewamalage et al. 2022), by pooling the data of all series\nin a data set and fitting a single forecasting function (Januschowski et al. 2020).\nIt has been theoretically and empirically shown that global models perform equally\nwell or even better than a collection of local models, even if the time series of the\ndata set are unrelated (Montero-Manso and Hyndman 2021). Indeed, neural networks\nobtained only mixed results when trained locally; instead, they achieve state of the art\nperformance (Benidis et al. 2022) if trained globally. So far, the empirical comparisons\nbetween local and global models has mostly focused on point forecast, rather than on\npredictive distributions, and on smooth time series. Moreover, some results involving\ndeep learning forecasting models considered inadequate data set and benchmarks, as\npointed out by Hewamalage et al. (2023).\nWe carry out the first comparison of probabilistic global and local models for\nintermittent time series, assessing both predictive accuracy and computational require-\nments. As local models, we select the simple in-sample quantiles and two advanced\nmodels: iETS (Svetunkov and Boylan 2023) and TweedieGP (Damato et al. 2025).\nSuch models provide challenging baselines for the neural networks models introduced\nlater.\nThere is no established global model architecture for forecasting intermittent time\nseries. We compare feed-forward neural networks (Babai et al. 2020), DeepAR (Salinas\net al. 2020) and three types of transformers: vanilla (Vaswani et al. 2017), Informer\n(Zhou et al. 2021) and Autoformer (Wu et al. 2021). Moreover, we consider D-Linear\n(Zeng et al. 2023), a relatively shallow architecture which compares favourably to\ntransformers-based models on smooth time series. We leave for future research the\ncomparison with LLM-based forecasting models (Tan et al. 2024).\nOn global models we also compare different distribution heads suitable for inter-\nmittent time series (for local models, such analysis has been already carried out in\nprevious papers). We consider the negative binomial distribution, widely used with\nlocal models (Kolassa 2016, 2022; Long et al. 2025), the hurdle-shifted negative bino-\nmial (HSNB, Feng (2021)) and the Tweedie distribution (Dunn and Smyth 2005). To\nthe best of our knowledge, we use the Tweedie and the HSNB distribution head for\nthe first time. The Tweedie has been used in (Damato et al. 2025) as predictive dis-\ntribution for local models, showing better estimates of the highest quantiles compared\n2\nto the negative binomial distribution. We assess both the accuracy and the computa-\ntional cost and the reliability of the models. on more than 40.000 time series from five\ndata sets from retail and supply chain.\n2 Probabilistic models for intermittent time series\nWe consider a dataset containing n time series and we denote the i-th time series\nby yi, i = 1, . . . , n and its value at time t by yi\nt. We denote by yi\ns:t the sequence of\nobservations of the time series from time s to t:\nyi\ns:t := {yi\ns, . . . , yi\nt}.\nWe denote the length of the training set by T and the forecast horizon by h. A\nprobabilistic forecast consists of a predictive distribution (or forecast distribution) for\nthe next h time instants of the i-th time series\np(yi\nT +1, . . . , yi\nT +h | yi\n1:T ).\nA probabilistic model is a function which returns the above forecast distribution.\nModels comprise learnable parameters, which we call model parameters. We differen-\ntiate between local and global models depending on how the model parameters are\nlearned: local models are trained on individual time series, while global models are\ntrained on a large set of time series (Januschowski et al. 2020).\n2.1 Local models\nIntermittent time series are commonly forecast (Hyndman and Athanasopoulos 2021,\nSec 13.8) using local models, i.e., by learning an independent model on each time series.\nSuch models can be trained with little amount of training data but do not exploit the\navailability of large amount of time series. Among the local models we select a baseline\n(in-sample quantiles) and two advanced approaches (iETS and TweedieGP).\nIn-sample quantiles\nThis model assumes the time series to be constituted by i.i.d. samples. The predic-\ntive distribution for each time step is the empirical distribution of the data; thus, the\nquantile of the h-steps ahead predictive distribution equals the quantile of the training\ndata. In-sample quantiles are immediate to compute; despite their simplicity, they pro-\nvide competitive probabilistic forecasts for intermittent time series, see, e.g., Kolassa\n(2016); Spiliotis et al. (2021); Long et al. (2025).\niETS\niETS (Svetunkov and Boylan 2023) specializes the ETS model (Hyndman and Athana-\nsopoulos 2021, Chap.8) to intermittent time series. Based on Croston\u2019s decomposition\n(Croston 1972), iETS identifies, for each observation yt, the demand dt and the\n3\noccurrence ot:\ndt =\n(\nyt\nif yt > 0\nundefined\notherwise,\not = 1(yt > 0),\nwhere 1 stands for the indicator function. This decomposition implies that yt = ot \u00b7dt,\nfor each t. iETS forecasts demand and occurrence with independent multiplicative\nexponential smoothing models. The best model for the occurrence is selected among\nfive different candidates via AIC.\nIts predictive distribution is bimodal, being constituted by a mass in zero (the\nBernoulli probability of non-occurrence) and a Gamma distribution for the positive\nvalues of demand. The multi-step ahead mean forecast is obtained via closed-form\nformulae, but h-step ahead probabilistic forecasts are obtained with autoregressive\nsampling; this is fast since the model is simple.\nTweedieGP\nTweedieGP (Damato et al. 2025) is a Bayesian model for forecasting intermittent time\nseries. It is trained with a Bayesian update: a Gaussian Process prior (Rasmussen\nand Williams 2006) is set on the latent function, the likelihood is assumed to be a\nTweedie distribution (see Sec. 2.4) and the posterior distribution of latent function\ndistribution is computed with Bayes theorem. Fast training times are achieved thanks\nto variational methods (Hensman et al. 2015). Damato et al. (2025) compares Tweedie\nand negative binomial likelihood for the Gaussian process; they report better forecasts\nwith the Tweedie, especially on the highest quantiles.\nTo forecast h-steps ahead, the latent function distribution is computed in closed\nform for the next h steps. For each future time step, samples of the latent function\nare drawn and set as the mean of the Tweedie likelihood to sample the predictive\ndistribution. This way, the predictive distribution of TweedieGP accounts for the\nuncertainty of the GP latent variable.\n2.2 Global models\nA global model is trained using all the time series of the data set (Hewamalage et al.\n2022). We focus on global univariate models, which forecast each time series indepen-\ndently. We consider global models based on neural networks (Benidis et al. 2022),\nas in general they achieve the best performance. With a slight abuse of terminol-\nogy, we divide them into shallow (feed-forward and D-Linear) and deep (DeepAR and\ntransformers-based architectures). Our models are probabilistic and thus they have a\ndistribution head; first, we introduce the architectures and then we discuss the distri-\nbution heads. Since they are trained using a large amount of data, global models do\nnot receive the entire series as input for training and prediction, but a only subset of\ndata points (context window).\nFeed-forward neural networks (FNNs)\nWe use the same architecture of Montero-Manso and Hyndman (2021), namely 5\nlayers of 32 units each, with ReLu activations. While the model of Montero-Manso\n4\nand Hyndman (2021) only returns point forecasts, we adapt it to intermittent demand\nby equipping it with a distribution head.\nThe model jointly outputs the parameters of the predictive distributions of the\nnext h steps, \u03b8T +1, . . . , \u03b8T +h, which we call distributional parameters. We refer to this\nmodel as a multi-horizon architecture. The probabilistic forecast for the next h-steps\nis immediate to compute given the context window.\nD-Linear\nD-Linear (Zeng et al. 2023) decomposes the time series into trend and remainder,\nusing a moving average kernel with unit stride. Then, each signal passes through a\nlinear layer; the sum of their outputs composes the last layer before the distribution\nhead. As the FNNs, D-Linear is multi-horizon, i.e., it returns the parameters h-steps-\nahead predictive distribution. The trend-remainder decomposition is inspired by the\nAutoformer (Wu et al. 2021).\nDeepAR\nRecurrent neural networks (RNNs) are competitive global models, particularly when\nLSTM cell units are used (Hewamalage et al. 2021; Bandara et al. 2019). DeepAR\n(Salinas et al. 2020) is a LSTM-based model, designed for probabilistic forecasting. To\nforecast intermittent time series, Salinas et al. (2020) couples this architecture with\na negative binomial distribution head; we will instead consider different distribution\nheads.\nDeepAR adopts autoregressive sampling to forecast h-steps ahead. To do so, it\ndraws a value from the predictive distribution for time t+1; it then appends the sam-\npled value to the context window and treats it as an input to compute the predictive\ndistribution for time t + 2. It repeats this process up to time t + h; the final output is\na sample path of a future trajectory. Autoregressive sampling with such a large model\nis, however, computationally demanding; the number of simulated trajectories should\nto be decided by considering also the computational cost.\nTransformers\nThe transformer (Vaswani et al. 2017) consists of an encoder-decoder structure: the\nencoder processes the input sequence to generate hidden representations, while the\ndecoder generates the output sequence based on these representations. Multi-head\nattention (Vaswani et al. 2017) enables the model to capture dependencies across long\ndistances in a sequence. We refer to this model as the vanilla transformer, being it\nthe standard formulation.\nThe model predicts the parameters of the one-step-ahead predictive distribution\nand adopts autoregressive sampling for the following time steps. This applies also to\nthe Informer and Autoformer discussed in the following.\nInformer\nThe multi-head self-attention component of the transformer is computationally expen-\nsive. The Informer (Zhou et al. 2021) speeds up the computations in forecasting by\nsparsifying the matrices of the self-attention. This decreases the the cost of passing\n5\nthe data from quadratic, vanilla transformer, to log-linear with respect to the context\nlength.\nAutoformer\nThe Autoformer (Wu et al. 2021) is another transformer-based architecture designed\nfor forecasting. It decomposes the original time series into trend and remainder; this\ndecomposition is applied also by D-Linear. This decomposition is performed repeatedy\nacross different steps of a transformer. Additionally, Autoformer replaces the self-\nattention with an auto-correlation mechanism, which uses the Fast Fourier Transform\nto focus on the data with the strongest temporal correlations.\n2.3 Distribution heads\nOur neural networks are probabilistic and thus have a distribution head. Denoting by\nh \u2208Rd the last layer of the model before the output, we linearly map h on the value of\neach parameter \u03b8 of the forecast distribution. We denote the parameters of the linear\nmap by lin\u03b8.\nSome distributional parameters need to be non-negative or to lie within the unit\ninterval. In these cases, after the linear map, we either apply the softplus function\n(softplus(x) = log(1 + ex)) or the sigmoid function (sigmoid(x) =\n1\n1+e\u2212x ). In the\nfollowing we discuss the relevant forecast distributions and how we designed their\ndistribution heads.\n2.4 Forecast distributions\nA distribution for intermittent time series should have both a probability mass in zero\nand a long upper tail to quantify the probability of large spikes in demand.\nNegative binomial\nThe negative binomial has both characteristics; it is thus often used with intermittent\ndemand (Kolassa 2016; Long et al. 2025). We write:\nY \u223cNegBin(r, p),\nwhere p \u2208(0, 1) and r > 0. The distribution can have a long upper tail even when\nits mean is low, because of overdispersion: Var[Y ] = E[Y ]\np\n> E[Y ]. For this flexibil-\nity it generally preferred (Kolassa 2016) to the Poisson distribution when modeling\nintermittent time series; it is is anyway bound to be unimodal.\nThe distribution head is:\nr = softplus(linr(h)),\np = sigmoid(linp(h)).\nHurdle-shifted negative binomial\nThe data of intermittent time series have sometimes a bimodal distribution, with a\npeak mass in zero and a second positive mode corresponding to most frequent demand\n6\nsize. It thus makes sense to consider a bimodal distribution head for the predictive\ndistribution. We consider two bimodal distribution heads: the hurdle-shifted negative\nbinomial (HSNB) and the Tweedie. Both are used here for the first time.\nThe hurdle-shifted negative binomial (HSNB) (Feng 2021) is a bimodal generaliza-\ntion of the negative binomial. An independent Bernoulli random variable provides the\nprobability (1\u2212\u03c0) of zero; the negative binomial distribution, observed with probabil-\nity \u03c0, is shifted by one unit to model the positive demand. Given Z \u223cNegBin(r, p) and\nO \u223cBer(\u03c0), the hurdle-shifted negative binomial variable is given by Y = O \u00b7 (1 + Z),\nwhich we denote by:\nY \u223cHSNB(\u03c0, r, p)\nwith P(Y = 0) = 1 \u2212\u03c0 and for y > 0, P(Y = y) = \u03c0 \u00b7 P(Z = y \u22121). Its parameters\nare \u03b8 = {\u03c0, r, p}. The shape of the shifted negative binomial, controlled by r and p, is\nindependent from the mass in zero, controlled by \u03c0. The distribution head is:\n\u03c0 = sigmoid(lin\u03c0(h)), r = softplus(linr(h)), p = sigmoid(linp(h)).\nTweedie\nThe Tweedie distribution (Dunn and Smyth 2005) is bimodal with a peak mass in\nzero and a continuous long-tailed density on the positive real values. Specifically, Y\nis distributed as a Tweedie if it is an exponential dispersion model (J\u00f8rgensen 1987)\nwith the following power relationship between mean and variance:\nVar[Y ] = \u03d5 E[Y ]\u03c1,\nwhere \u03d5 > 0 is the dispersion and \u03c1 \u2208(1, 2) is the power. We denote the mean by\n\u00b5 = E[Y ] > 0 and we write\nY \u223cTw(\u00b5, \u03d5, \u03c1).\nDifferently from the HSNB, all distributional parameters affect both the proba-\nbility in zero and the density of the positive demand. Another difference is that its\ndensity is continuos (Fig. 1). Thus, it can be the only suitable distribution head if\nthe intermittent time series is real-valued. To the best of our knowledge, ours is the\nfirst implementation of neural networks with a Tweedie distribution head. This is\nparametrised by \u03b8 = {\u00b5, \u03d5, \u03c1} with:\n\u00b5 = softplus(lin\u00b5(h)), \u03d5 = softplus(lin\u03d5(h)), \u03c1 = 1 + sigmoid(lin\u03c1(h)).\nThe Tweedie loss is often used to train neural networks (Jeon and Seong 2022) and\ntree models (Januschowski et al. 2022). However, it is only a rough approximation of\nthe Tweedie density and models trained in this way only return point forecasts; see\nDamato et al. (2025) for a detailed discussion. Here, we use a fully evaluated Tweedie\ndistribution.\nTraining\nWe denote the training set length as T and the data set of time series as y1:T +h. We\ntrain the models on y1:T \u2212h; we use yT \u2212h+1:T as validation set and yT :T +h as test\n7\n0\n5\n10\n15\ny\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\np(y)\nNegative binomial\n0\n5\n10\n15\ny\nHSNB\n0\n5\n10\n15\ny\nTweedie\nFig. 1: Every distribution above has mean 3 and variance 15. This uniquely determines\nthe parameters of the negative binomial, while different parameterizations are possible\nfor the Tweedie and the HSNB. The distributions in dark grey have approximately the\nsame mass in zero and similar shape. Instead, the light grey distributions are examples\nof bimodal HSNB and Tweedie. The HSNB is count-valued, while the Tweedie is\nabsolutely continuous on positive values.\nset. Both training and forecast are performed by processing batches of time series. A\nbatch B is a collection (i, wi), where i is the index of a time series and wi \u2208{1, . . . , T}\nis the last index of the context window. Model parameters are trained by minimizing\nthe negative log-likelihood of the predictive distribution on the training set summed\nover the different batches:\nL = \u2212\nX\n(i,wi)\u2208B\nh\nX\nt=1\nlog p(yi\nwi+t|\u03b8i\nwi+t)\n(1)\nwhere \u03b8i\nwi+t is the output of an evaluation of the model at yi\nwi\u2212c+1:wi and c is the\ncontext length, which defines how far the training looks back. This loss L is minimized\nby using Adam optimizer (Kingma and Ba 2017). We use the negative log-likelihood\non the validation set as a criterion for early stopping.\nScaling\nWhen training global models, it is important to bring all time series on the same scale\n(Montero-Manso and Hyndman 2021; Salinas et al. 2020). Dealing with intermittent\ntime series, we cannot simply scale the time series before training: integer-valued\npredictive distributions can be only evaluated on integer data.\nWe overcome the problem as follows. Denoting by si is the scale factor of the i-th\ntime series, we divide the autoregressive input of i-th time series by si. To evaluate\nthe likelihood function of Eq. (1) and to generate the forecasts on the test set we\nrescale by si the parameters of the predictive distribution which are scale-dependent;\nsee Appendix B for more details.\n8\nWe use the mean of the non-zero values of the i-th time series as si. Instead, the\nmean of the time series is not a suitable scale factor: on intermittent time series it can\nmuch smaller than one, thus inflating the positive values.\n3 Experiments\nModel name\nImplementation\nForecasting approach\niETS\nsmooth (R)\nAutoregressive (fast, 10,000 samples)\nTweedieGP\nTweedieGP (python)\nMulti-horizon (10,000 samples)\nFNN\ngluonts (python)\nMulti-horizon (10,000 samples)\nD-Linear\ngluonts (python)\nMulti-horizon (10,000 samples)\nDeepAR\ngluonts (python)\nAutoregressive (200 samples)\nTransformers\nHuggingFace transformers (python)\nAutoregressive (200 samples)\nTable 1: Summary of model implementations.\nWe list the used packages and forecasting approaches in Tab. 1. For local and\nshallow global models, we used no covariates. When training deep global models, we\ninclude time-based covariates such as lagged values, day of the week or month of the\nyear, time from the start of the time series, and time series IDs. We take the Tweedie\ndistribution head from Damato et al. (2025);it is based on PyTorch. We developed\nour PyTorch implementation of the HSNB distribution head [available in the code\nsubmitted to reviewers]. Additional details about the model\u2019s hyper-parameters are\nreported in Appendix C.\n3.1 Datasets\nWe consider the following datasets: M5: daily item sales from Walmart stores (Makri-\ndakis et al. 2022); UCI : daily item sales from an online store (T\u00a8urkmen et al. 2021);\nAuto: monthly vehicle sales (T\u00a8urkmen et al. 2021); Carparts: monthly car parts sales\n(Hyndman 2023); RAF: monthly spare parts demand for British Royal Air Force\n(Syntetos et al. 2005). In Tab. 2 we report some summary statistics including the aver-\nage demand interval (ADI) and squared coefficient of variation (CV2). Larger ADI\ncorrespond to sparser time series, while larger CV2 correspond to more volatile time\nseries.\nWe expect M5 to be the most favorable data set to large global models, as it\ncontains more than 30\u2019000 daily time series, each spanning over 5 years. Carparts\ncontains one order of magnitude less time series, which are also notably shorter (also\nbecause they are monthly); they have anyway similar intermittency to M5. UCI and\nRAF are the most intermittent datasets; the latter shows large demand spikes. Auto\nhas the shortest time series, with length of 18 months.\n9\nDataset\n# of ts\nFreq.\nT\nh\nContext\nAvg. ADI\nAvg. CV2\nM5\n30490\nD\n1941\n28\n112\n6\n0.4\nUCI\n1191\nD\n76\n14\n28\n10\n1.2\nAuto\n3000\nM\n18\n6\n12\n1\n0.4\nCarparts\n2493\nM\n45\n6\n12\n6\n0.3\nRAF\n5000\nM\n72\n12\n36\n10\n0.6\nTable 2: The considered datasets; \u201dD\u201d stands for daily and \u201dM\u201d for\nmonthly.\n3.2 Forecast evaluation\nWe evaluate probabilistic forecasts with the quantile loss, which is a proper scoring\nrule (Gneiting and Raftery 2007):\nQLq(y, \u02c6yq) =\n(\n2(1 \u2212q) \u00b7\n\u0000\u02c6yq \u2212y\n\u0001\nif y < \u02c6yq\n2q \u00b7\n\u0000y \u2212\u02c6yq\n\u0001\nif y \u2265\u02c6yq ,\nwhere \u02c6yq is the quantile q of the predictive distribution and y is the observation. We\nconsider q \u2208{0.50, 0.80, 0.90, 0.95, 0.99}, evaluating the upper tail of the distribution\nbut not the lower quantiles, which are generally zero.\nThe quantile loss is scale-dependent. To average the results across time series, we\nscale it by the quantile loss of the in-sample quantile on training data (Damato et al.\n2025):\nsQLq(y, \u02c6yq) =\nQLq(y, \u02c6yq)\n1\nT\nPT\nt=1 QLq(yt, ISQq)\n.\n(2)\nWe also report the RMSSE, i.e., the root mean squared error between the observa-\ntion and the mean of the predictive distribution, scaled by the in-sample mean squared\nerror of the na\u00a8\u0131ve forecast, whose prediction equals the last observation (Makridakis\net al. 2022).\n4 Results\nComputational times\nHere we only discuss the computational times with the negative binomial head; those\nobtained with different distribution heads are similar (Appendix A). We report in\nFig. 2 the computational times for training and generating forecasts. To make sure\nthat all models run on the same machine, such times are recorded on the CPU of a\nMacbook Pro M3 laptop (with Apple M3 Pro chip, 11 cores, 36GB).\nTransformer-based models are 3-10 times slower than FNNs and D-Linear. Thus\nthey have the largest computational costs; yet, as we show in the following, they do\nnot produce the most accurate forecasts. We also notice that the training times of\nDeepAR are in line with those of vanilla transformer and Informer, but its prediction\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"Intermittent Time Series Forecasting: Local vs Global Models\"\n\n#### Main Contributions:\n- The paper introduces a comprehensive comparison between local (iETS, TweedieGP) and global models (D-Linear, DeepAR, Transformers) for intermittent time series forecasting. \n- It evaluates these models on five large datasets containing more than 40,000 real-world time series.\n- The study considers three different distribution heads suitable for intermittent time series: negative binomial, hurdle-shifted negative binomial, and Tweedie.\n\n#### Methodology:\n- **Local Models**: These include in-sample quantiles and advanced models like iETS (iNTermittent ETS) and TweedieGP. The paper selects these as baseline models.\n- **Global Models**: These are trained on a large collection of time series, often using neural networks architectures such as D-Linear, DeepAR, and Transformers.\n- **Distribution Heads**: For the first time, the study uses two novel distribution heads: hurdle-shifted negative binomial (HSNB) and Tweedie. The paper assesses their performance across multiple datasets.\n\n#### Key Findings:\n1. **Performance of Models**:\n   - **Neural Networks Models**: D-Linear provides the best accuracy among neural networks models, outperforming both local and global models.\n   - **Global Models (Transformers)**: While Transformers-based architectures are computationally demanding, they do not always produce the most accurate forecasts. Their training times align with those of vanilla transformers and Informer but their predictions can be less accurate.\n2. **Distribution Heads**:\n   - The Tweedie distribution head provides the best estimates for high quantiles.\n   - The negative binomial distribution offers overall the best performance across different datasets.\n\n#### Conclusions:\n- **Accuracy vs Computation**: While global models are computationally intensive, they often outperform local models in terms of accuracy. Neural networks-based global models like D-Linear show promise but require careful tuning and validation.\n- **Future Work**: The study suggests that future research should focus on comparing LLM (Large Language Models)-based forecasting models with neural network architectures.\n\nThis comprehensive evaluation provides valuable insights into the strengths and weaknesses of different model architectures for intermittent time series forecasting, highlighting the need for further exploration in this area."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic\nSpeech Synthesis\nYushen Chen1,2, Junzhe Liu1, Yujie Tu2,3, Zhikang Niu1,2\nYuzhe Liang1,2, Kai Yu1, Chunyu Qiang4, Chen Zhang4, Xie Chen1,2*\n1X-LANCE Lab, School of Computer Science, MoE Key Lab of Artificial Intelligence,\nJiangsu Key Lab of Language Computing, Shanghai Jiao Tong University\n2Shanghai Innovation Institute 3University of Chinese Academy of Sciences 4Kuaishou Technology\n{swivid,chenxie95}@sjtu.edu.cn\nAbstract\nA notable gap persists in speech synthesis re-\nsearch and development for Arabic dialects,\nparticularly from a unified modeling perspec-\ntive. Despite its high practical value, the in-\nherent linguistic complexity of Arabic dialects,\nfurther compounded by a lack of standardized\ndata, benchmarks, and evaluation guidelines,\nsteers researchers toward safer ground.\nTo\nbridge this divide, we present Habibi, a suite\nof specialized and unified text-to-speech mod-\nels that harnesses existing open-source ASR\ncorpora to support a wide range of high- to low-\nresource Arabic dialects through linguistically-\ninformed curriculum learning. Our approach\noutperforms the leading commercial service\nin generation quality, while maintaining exten-\nsibility through effective in-context learning,\nwithout requiring text diacritization. We are\ncommitted to open-sourcing the model, along\nwith creating the first systematic benchmark\nfor multi-dialect Arabic speech synthesis. Fur-\nthermore, by identifying the key challenges in\nand establishing evaluation standards for the\nprocess, we aim to provide a solid ground-\nwork for subsequent research. Resources at\nhttps://SWivid.github.io/Habibi/.\n1\nIntroduction\nArabic is a pluricentric language with rich regional\ndiversity (Schuppler et al., 2024; Djanibekov et al.,\n2025), spoken by more than 400 million natives\nof approximately 30 different varieties (Wikipedia,\n2025). From the perspective of research on speech\ntechnology, the inherent difficulties of the Arabic\nlanguage mainly lie in two aspects:\n\u2022 Linguistically, the Modern Standard Arabic\n(MSA), as the official and literary form, is not\ncommonly used in daily life, while various spo-\nken dialects for actual communications are ob-\nserved fusing and transferring largely and consis-\ntently. Different dialects exhibit distinct lexical\n*Corresponding author\nFigure 1: Our open-source unified-dialectal model\n(Habibi) outperforms ElevenLabs\u2019 commercial TTS ser-\nvice: the latest Eleven v3 (alpha) (11Labs-3a).\nand pronunciation conventions, resulting in rela-\ntively complex vocabulary and grammar.\n\u2022 Technically, the challenge lies in the fact that al-\nthough MSA is a high-resource language, many\nArabic dialects are underrepresented and consid-\nered low-resource. For different dialects, addi-\ntional text diacritization can help distinguish and\nindicate the correct pronunciation. However, the\nexisting labeled data or the in-the-wild transcrip-\ntions often lack these diacritics. Moreover, the\nmajority of the datasets are designed and curated\nfor the automatic speech recognition (ASR) task,\nconsisting of a large proportion of noisy speech\nsamples. Almost no ready-made clean datasets\nare available at scale for speech synthesis.\nThe current main trends in Arabic dialectal\nspeech research include: from MSA to ASR of\nlow-resource dialects (MGB-2, MGB-3, MGB-5\nchallenges from Ali et al., 2016, 2017, 2019, etc.),\nand studies aiming at expanding the dialectal cov-\nerage of Arabic ASR (Djanibekov et al., 2025; Ta-\nlafha et al., 2025; Omnilingual et al., 2025); from\n1\narXiv:2601.13802v1  [cs.CL]  20 Jan 2026\nthe centralized text-to-speech (TTS) in MSA (Hal-\nabi, 2016; Kulkarni et al., 2023; Toyin et al., 2023)\nto few models for specific or limited dialect sup-\nport (Laouirine et al., 2024; Doan et al., 2024), and\nthe results still fail to match the performance ex-\npected as achieved by recent zero-shot TTS models\nin languages such as English and Chinese. The\nunderwhelming performance of these models is un-\nderstandable given the limitations of existing Ara-\nbic speech datasets. For instance, ArVoice (Toyin\net al., 2025), the largest open-source corpus by\nfar specifically designed for TTS, contains fewer\nthan 100 hours of MSA-only data from 11 speak-\ners, much of which is synthetic. Meanwhile, the\nQASR corpus (Mubarak et al., 2021), primarily\nintended for the ASR task, features a highly imbal-\nanced gender distribution among its speakers. This\ncharacteristic, though likely unintentional, renders\nit unsuitable for training high-quality TTS models.\nApart from data scarcity, there remains a lack of\nimpetus regarding the Arabic dialectal TTS sys-\ntem itself, which is a common issue in the study\nof long-tail languages, stemming from scientific\ndisincentives and other factors (Omnilingual et al.,\n2025). Notably, to the best of our knowledge, re-\nsearch on unified-dialectal Arabic TTS is absent,\nnot to mention an open-source framework.\nFacing up to all aforementioned challenges, we\npresent our Habibi (\u00a8byb\u0017, \u201cmy dear friend\u201d here):\n\u2022 An open-source unified-dialectal Arabic text-to-\nspeech model covering to date more than 20 lan-\nguoids (ISO 639-3 codes) and 12 assignable re-\ngional identifiers. While being equally compet-\nitive on MSA, Habibi beats the current leading\ncommercial model on all 6 major dialect test sets.\n\u2022 The first systematic and standardized benchmark\nfor multi-dialect Arabic zero-shot speech syn-\nthesis, featuring at least 1,000 and up to 3,000\nutterances each subset of 7, paired with manually\ntranscribed and rigorously filtered text.\n\u2022 Rooted in linguistically-informed curriculum\nlearning from high to low-resource and from gen-\neral to dialect-aware training, our model enables\neffective in-context learning to capture subtle di-\nalectal features at inference, without diacritized\ntext, advancing beyond a proof of concept.\n\u2022 An extensively ablated and scalable solution for\nmulti- and unified-dialectal Arabic TTS, with\nwhich we deliver insights on the pros and cons of\ntraining a single, unified model versus separate,\nspecialized models for each dialect. We further\nshowcase the importance of incorporating dialect-\nspecific ASR models for more comprehensive\nand reliable evaluation.\n2\nMethodology\nIn this section, we introduce the whole framework\nof Habibi. We begin in Section 2.1 with restricted\nabbreviation usage in this paper referring to Arabic\nvariants. Section 2.2 details the creation of our\ntraining dataset, while Section 2.3 describes how\nwe standardize a benchmark for the multi-dialect\nArabic zero-shot TTS task. We also elaborate on\nthe key designs from the aspect of training strategy\nthat contribute to the effectiveness of our model in\nSection 2.4 and 2.5, namely linguistically-informed\ncurriculum learning and dialect-aware supervised\nfine-tuning, respectively.\n2.1\nTerminology\nIn the absence of a strict scientific definition of Ara-\nbic dialects, we adopted a practical convention for\nthe rest of the paper, referring to Arabic variants\nusing abbreviations in two types of different gran-\nularity. We emphasize that the terminology setup\ndoes not reflect any official classification. The two\ntypes are languoids (linguistic entities from ISO\n639-3) and regional identifiers (IDs) based on re-\ngions and countries (three letters capitalized).\nMSA\nModern Standard Arabic. This is an offi-\ncial variant of Arabic, commonly used in literary\nform, e.g., in news, books, and education. Coded\narb_Arab in ISO 639-3.\nSAU\nReferring to the various spoken dialects of\nArabic in Saudi Arabia, primarily Najdi (Central,\nars_Arab) and Hijazi (Western, acw_Arab), Khal-\niji or Gulf Arabic (Eastern, afb_Arab), with more\nincluding Baharna (abv_Arab).\nUAE\nEmirati Arabic, a group of Gulf Arabic\n(afb_Arab) varieties spoken by the Emiratis na-\ntive to the United Arab Emirates.\nALG\nDialects spoken in Algeria, including pri-\nmarily Algerian Arabic (arq_Arab), and Algerian\nSaharan Arabic (aao_Arab).\nIRQ\nDialects mainly spoken in Iraq and Syria,\nincluding Mesopotamian Arabic (acm_Arab), and\nNorth Mesopotamian Arabic (ayp_Arab).\nEGY\nEgyptian Arabic (arz_Arab) is the most\nwidely spoken vernacular Arabic variety in Egypt.\nSaidi Arabic (aec_Arab), or Upper Egyptian Ara-\nbic, is another spoken variety in the south.\n2\nDataset Name\nID\nHours Utterances\nMASC[MSA] (Al-Fetyani et al., 2023) MSA\n329.0\n152,405\nMGB-2 (Ali et al., 2016)\nMSA\n585.2\n215,958\nSADA (Alharbi et al., 2024)\nSAU\n187.0\n154,288\nMSA\n7.2\n4,090\nEGY\n2.1\n2,029\nLEV\n0.9\n947\nUNK\n219.2\n79,807\nMixat (Al Ali and Aldarmaki, 2024)\nUAE\n13.1\n4,746\nUAE-100K1\nUAE\n94.4\n50,035\nUAE-Nexdata2\nUAE\n0.6\n423\nMASC[EGY] (Al-Fetyani et al., 2023) EGY\n18.5\n15,910\nMGB-3 (Ali et al., 2017)\nEGY\n8.3\n3,165\nFLEURS (Conneau et al., 2023)\nEGY\n8.2\n2,824\nMGB-5 (Ali et al., 2019)\nMAR\n33.7\n26,382\nin-house\nALG\n64.4\n70,970\nIRQ\n58.9\n48,534\nUAE\n4.4\n2,413\nOmnilingual ASR Corpus\u2020\nSAU\n62.4\n8,819\n(Omnilingual et al., 2025)\nALG\n8.6\n1,482\nIRQ\n11.7\n1,726\nEGY\n25.1\n3,979\nMAR\n8.3\n1,659\nOMN\n20.7\n2,778\nTUN\n19.4\n2,632\nLEV\n2.5\n383\nSDN\n4.2\n612\nLBY\n14.8\n2,194\nDarija-S2T3,\u2020\nMAR\n30.6\n3,839\nDarijaTTS-clean4,\u2020\nMAR\n9.2\n10,292\nJordan-Audio5,\u2020\nLEV\n4.9\n4,016\nTotal\n1857.3\n879,339\nTable 1: Training dataset statistics. \u2020: expanded data for\ntraining; UNK: dialect info unknown or non-applicable.\nMAR\nMoroccan Arabic (ary_Arab), or Darija, is\nspoken by the majority of people in Morocco.\nOMN\nIncluding Omani Arabic (acx_Arab) and\nDhofari Arabic (adf_Arab), in our data sources.\nTUN\nTunisian Arabic (aeb_Arab), or simply\nTunisian, is a variety of Arabic spoken in Tunisia.\nLEV\nLevantine Arabic (apc_Arab), is literally\nspoken in the Levant, while also broadly including\nLevantine Bedawi Arabic (avl_Arab) here.\nSDN\nSudanese Arabic (apd_Arab) refers to the\nvarious varieties of Arabic spoken in Sudan.\nLBY\nLibyan Arabic (ayl_Arab) is a variety of\nArabic spoken in Libya and neighboring countries.\n1hf.co: AhmedBadawy11/UAE_100K\n2hf.co: Nexdata/UAE_Arabic_Spontaneous...\n3hf.co: adiren7/darija_speech_to_text\n4hf.co: KandirResearch/DarijaTTS-clean\n5hf.co: nadsoft/Jordan-Audio\nRegional\nBasic Data (hrs)\nAfter Expansion (hrs)\nIdentifier\nD1 as abbr.\nD2 as abbr.\nMSA\n921.5\n921.5\nSAU\n187.0\n249.3\nUAE\n112.4\n112.4\nALG\n64.4\n72.9\nIRQ\n58.9\n70.7\nEGY\n37.1\n62.2\nMAR\n33.7\n81.7\nOMN\n-\n20.7\nTUN\n-\n19.4\nLEV\n0.9\n8.3\nSDN\n-\n4.2\nLBY\n-\n14.8\nUNK\n219.2\n219.2\nTotal\n1635.0\n1857.3\nTable 2: Duration statistics comparing basic data (D1)\nand data after expansion (D2) across different dialects.\n2.2\nTraining Data\nWe first assembled training data from several ex-\nisting open-source ASR datasets:\nMASC (Al-\nFetyani et al., 2023) with MSA and Egyptian\nparts, SADA (Alharbi et al., 2024), MGB-2 (Ali\net al., 2016), MGB-3 (Ali et al., 2017), MGB-\n5 (Ali et al., 2019), FLEURS (Conneau et al.,\n2023), and Omnilingual ASR Corpus (Omnilin-\ngual et al., 2025). We also collected online avail-\nable datasets: UAE-100K, UAE-Nexdata, Darija-\nS2T, DarijaTTS-clean, and Jordan-Audio, where\nthe source links are provided in Table 1. We fi-\nnally supplemented these sources with internal data\nand manually transcribed publicly available speech\nrecordings.\nAfter acquiring a substantial volume of data cov-\nering multiple Arabic dialect variants, the quality\nof the preliminary fetched data was entirely uncen-\nsored and is likely contaminated with significant\nnoise, which is detrimental to TTS training. There-\nfore, we implemented a multi-step selective data\nprocessing procedure as follows.\nFor all datasets, we apply the speaking rate\n(Character Per Second, CPS) as a straightforward\nthreshold to filter out poisonous or erroneous data\npairs. This is based on several observations regard-\ning facts: specifically, a too small CPS often corre-\nsponds to either missing text labels or audio con-\ntaining long silent segments, while an overly large\nCPS may result from incorrect automatic subtitles\nunfiltered by the original datasets or from unusu-\nally short audio files that are probably corrupted\nduring transmission. This approach, while sim-\nple, is demonstrably effective. Manual verification\n3\nconfirms that CPS filtering removes a substantial\nportion of noisy instances. The filtering thresholds\nare empirically determined, with a lower bound of\n[4, 10] and an upper bound of [15, 25], adjusted per\ndataset. The guiding principle is consistent: the\nestimated corruption level of a dataset dictates the\naggressiveness of the applied thresholds.\nTo tackle low-SNR datasets, we employed a\nsource separation model (Luo and Yu, 2023) for\nspeech denoising, filtering out any samples that\nwere rendered silent post-separation. The efficacy\nof this approach involves a nuanced trade-off, as\nanalyzed in Section 3.7.\nA meticulous filtering procedure was applied ex-\nclusively to the MASC corpus (Al-Fetyani et al.,\n2023). This tailored processing was motivated by\ntwo key factors. First, MASC constitutes a signif-\nicant portion of the total data duration. Second,\nsince many audio clips in the MGB-2 corpus were\nrecorded at an effective sampling rate below 8 kHz,\nit was crucial to ensure that the higher-sampling-\nrate MASC is filtered properly to maintain suf-\nficient quality and reliably support the model to\nlearn MSA, the major Arabic variant. Concretely,\nwe utilized only the clean subset of MASC. With\nmanual inspection, we established several text-\npattern-based filtering rules. Since MASC was\ncrawled from YouTube channels, we observed that\nthe patterns of corrupted data were highly channel-\nspecific (e.g., recurring intro & outro music or\nsinging segments). After rule-based filtering, we\nmerged short adjacent audio segments that were\neach shorter than 6 seconds, creating longer clips\nup to 30 seconds in length. This process effec-\ntively redistributed the data from the originally\nconcentrated sub-10-second range toward longer\ndurations, producing clips better suited for TTS.\nThe statistics of our final usable dataset are pre-\nsented in Table 1 and 2, where Basic Data and Ex-\npanded Data are denoted as D1 and D2 (hereafter\nused for brevity), respectively.\n2.3\nMulti-Dialect Arabic TTS Benchmark\nA systematic and standardized evaluation bench-\nmark is crucial for assessing the success of large-\nscale model training and inference. As part of\nour work, we establish the first benchmark for the\nmulti-dialect Arabic zero-shot TTS task, covering 7\nArabic variants: MSA, SAU, UAE, ALG, IRQ, EGY, and\nMAR. All test sets are first processed according to\nthe same data preparation pipeline described in Sec-\ntion 2.2. We then select samples that satisfy three\ncriteria: (1) duration falls between 3 and 12 sec-\nonds; (2) transcriptions contain only Arabic script;\nand (3) for each sample, there exists another utter-\nance from the same speaker, which is required for\nstandard zero-shot TTS evaluation as it needs both\nreference and target speech from a single speaker.\nWe have, for each released subset, regarding its\nsource and size:\nMSA\n2902 samples from MGB-2 test set;\nSAU\n1375 samples from SADA val and test sets;\nUAE\n1048 samples from UAE-100K val set;\nALG\n1727 samples from in-house data;\nIRQ\n2198 samples from in-house data;\nEGY\n1024 samples from MGB-3 test set;\nMAR\n1083 samples from MGB-5 test set.\nAll samples comprising this benchmark should be\nexcluded from the training process if it is to be used\nfor evaluation.\n2.4\nLinguistically-Informed Curriculum\nLearning\nWe employ a two-stage curriculum learning strat-\negy to enhance the overall generative performance\nfor Arabic dialects.\nIn the first stage, we ini-\ntialize the model with weights from the F5-TTS\nmodel (Chen et al., 2025b), which is pre-trained on\napproximately 95K hours of large-scale Chinese\nand English data (detailed in Section 3.1). We then\nconduct supervised fine-tuning (SFT) using only\nMSA data. As a formal written standard, MSA\nfavors the model to grasp fundamental grammat-\nical norms and basic phonological structures of\nArabic, enabling a relatively stable transfer of text-\nto-speech modality mapping capability from the\noriginal Chinese and English proficiency to Arabic.\nThe endpoint of the first stage is determined em-\npirically by selecting the weights where the metric\nreflecting naturalness (e.g., UTMOS) converges\nto an optimum. This decision is made because,\nwhile pronunciation accuracy (e.g., WER) keeps\nimproving with more training, the generated audio\nprogressively adapts to the narrower distribution of\nthe SFT data\u2014largely sourced from ASR corpora,\nwhich have relatively lower SNR\u2014resulting in a re-\nduction of both speaker similarity and naturalness.\nIn the second stage, we build a complementary\nsuite of models through two parallel strategies, de-\ntailed in the ablation studies: specialized models re-\nfined via continued training on individual dialects;\nand unified models, enhanced by fine-tuning on all\ndialect data combined.\n4\nThrough this linguistically informed, step-by-\nstep learning approach, our model demonstrates\nhigh-quality zero-shot generative capabilities under\nboth training strategies.\n2.5\nDialect-Aware Supervised Fine-Tuning\nCurrently, mainstream speech synthesis models ex-\nhibit strong in-context learning capabilities. Partic-\nularly when both reference audio and paired tran-\nscription are provided as conditional inputs, these\nmodels can implicitly retrieve and effectively lever-\nage some text-to-acoustic matching information\nfrom known pairs during zero-shot inference (as\ndemonstrated and discussed in Section 3.6). How-\never, supporting an optional specifiable regional\nidentifier could potentially clarify inference direc-\ntions and assist in disambiguation in the process.\nTherefore, we augment the dictionary with spe-\ncial tokens for regional identifiers i denoting the\ni-th term in Section 2.1, thus the text sequence is:\ni , \u27e8, c1, c2, . . . , cj, . . . , cn, \u27e9,\n(1)\nwhere cj is the j-th character of the raw text of total\nlength n, \u27e8\u27e9wraps a certain dialectal text with two\nspecial tokens indicating the beginning and ending.\nBased on the results in Section 3.8, explicitly in-\ncorporating regional identifiers during training im-\nproves the model\u2019s inference ability overall\u2014even\nwhen identifiers are not present later. We hypothe-\nsize that this stems from two primary factors. First,\nthe curriculum-trained base model inherently pos-\nsesses robust in-context learning capabilities and\nresilience to diverse input patterns (identifiers are\nintroduced here via text sequence manipulation).\nSecond, training with identifiers likely helps the\nmodel better understand and internalize certain\ndialect-related patterns within data distribution.\n3\nExperiments\nFigure 1 clearly illustrates the strong performance\nof our open-source Habibi model suite, with full\nmetrics provided in Appendix 1.2. In this section,\nwe detail the experimental setup, present our main\nresults, and conduct ablation studies to validate the\nkey factors underpinning the model\u2019s efficacy.\n3.1\nBackbone Choice and Training Setup\nCurrent paradigms for zero-shot TTS primarily fall\ninto three categories: autoregressive (AR) mod-\nels (Wang et al., 2023; Meng et al., 2024; Wang\net al., 2025b; Ye et al., 2025; Chen et al., 2025a;\nRouard et al., 2025), non-autoregressive (NAR)\nmodels (Gao et al., 2023; Le et al., 2023; Eskimez\net al., 2024; Wang et al., 2024; Chen et al., 2025b;\nZhu et al., 2025; Wang et al., 2025a), and hybrid\nAR-NAR systems (Du et al., 2024; Anastassiou\net al., 2024; Guo et al., 2024; Zhou et al., 2025; Jia\net al., 2025; Zhang et al., 2025; Song et al., 2025;\nYu et al., 2025). To our knowledge, there is no\nclear evidence that existing audio and text encoder\nmodules prevailing in AR or hybrid systems can\neffectively capture the complex features of Arabic\ndialects without additional training. Given the need\nto avoid cascading errors, we adopted the open-\nsource F5-TTS framework (Chen et al., 2025b),\nwhich operates directly on mel spectrograms and\nraw text character sequences. This approach is not\nonly straightforward but has also been validated in\nmainstream languages such as Chinese and English;\nits potential for extension across a wider variety of\nlanguages is reflected in emerging contemporane-\nous work (Feng et al., 2025; Zhao et al., 2026) and\nwithin the broader speech community.\nRegarding specific configurations, we strictly\nfollow the default setup of F5-TTS v1 base training.\nTo ensure adequate convergence across dialects, we\ntrain all models to 200K updates, where a single\ntraining process typically elapses for approximately\n2 days on 8 NVIDIA H100 SXM GPUs.\n3.2\nEvaluation Setup\nAll experiments adhere to the benchmarking stan-\ndards defined in Section 2.3. We measure three\nconventional metrics: word error rate (WER) us-\ning ASR models, speaker similarity (SIM) leverag-\ning the speaker verification model WavLM (Chen\net al., 2022), and naturalness with UTMOS (Saeki\net al., 2022). Notably, we report two sets of WER\nscores: (1) WER-O, evaluated with Omnilingual-\nASR-LLM-7B model (Omnilingual et al., 2025)\n(v1 with a fixed batch size of 64); and (2) WER-S,\nderived from dialect-specific ASR models, most\nof which trained following VietASR (Zhuo et al.,\n2025) except for EGY and MAR (two XLSR fine-\ntuned models on Hugging Face6,7 are employed).\nThe rationale for introducing both is to enable a\nmore reliable conclusion, given that a multilingual\nmodel risks cross-dialect recognition bias (e.g., in-\ncorrectly \u201ccorrecting\u201d speech that lacks dialect fea-\ntures); conversely, specialized models often suffer\nfrom poor generalization and noise resistance.\n6IbrahimAmin/egyptian-arabic-wav2vec2-xlsr-53\n7boumehdi/wav2vec2-large-xlsr-moroccan-darija\n5\nID\nWER-O \u2193\nWER-S \u2193\nSIM \u2191\nUTMOS \u2191\nSpecial. Uni.D1-I Uni.D2-I Special. Uni.D1-I Uni.D2-I Special. Uni.D1-I Uni.D2-I Special. Uni.D1-I Uni.D2-I\nMSA\n7.88\n7.44\n7.71\n7.74\n7.61\n7.62\n0.756\n0.757\n0.757\n1.93\n1.93\n1.92\nSAU\n13.56\n13.75\n13.96\n10.42\n11.16\n11.13\n0.706\n0.695\n0.694\n1.71\n1.66\n1.68\nUAE\n4.97\n5.04\n5.15\n4.55\n4.78\n4.88\n0.774\n0.779\n0.782\n2.88\n2.71\n2.75\nALG\n30.98\n31.68\n31.18\n19.85\n23.57\n23.58\n0.755\n0.731\n0.732\n2.63\n2.43\n2.46\nIRQ\n17.66\n17.91\n17.12\n11.45\n11.82\n11.90\n0.784\n0.759\n0.759\n2.63\n2.44\n2.48\nEGY\n18.08\n19.04\n18.58\n15.42\n16.89\n16.44\n0.798\n0.787\n0.787\n2.32\n2.12\n2.14\nMAR\n46.43\n40.12\n40.02\n41.90\n42.63\n41.53\n0.701\n0.684\n0.686\n2.20\n1.96\n1.98\nTable 3: Comparing unified models trained on D1 (Uni.D1) and on D2 (Uni.D2) with specialized (Special.)\ncounterparts. The \u201c-I\u201d postfix indicates the application of a regional identifier during both training and inference.\nID\nUpd.\nWER-O \u2193WER-S \u2193\nSIM \u2191UTMOS \u2191\nMSA\nGT\n11.42\n10.92\n0.802\n2.11\n100K\n8.43\n7.85\n0.761\n2.01\n150K\n7.95\n7.67\n0.758\n1.98\n200K\n7.88\n7.74\n0.756\n1.93\nSAU\nGT\n28.36\n19.88\n0.724\n1.71\n100K\n14.47\n11.12\n0.702\n1.79\n150K\n13.62\n10.36\n0.707\n1.75\n200K\n13.56\n10.42\n0.706\n1.71\nUAE\nGT\n12.62\n11.80\n0.757\n2.63\n100K\n4.97\n4.55\n0.774\n2.88\n150K\n5.40\n5.03\n0.777\n2.77\n200K\n5.73\n5.49\n0.774\n2.66\nALG\nGT\n41.19\n26.57\n0.754\n2.27\n100K\n30.98\n19.85\n0.755\n2.63\n150K\n32.25\n19.91\n0.764\n2.54\n200K\n33.57\n20.12\n0.767\n2.48\nIRQ\nGT\n27.18\n18.05\n0.790\n2.42\n100K\n17.66\n11.45\n0.784\n2.63\n150K\n18.50\n11.42\n0.793\n2.54\n200K\n19.33\n11.60\n0.795\n2.47\nEGY\nGT\n22.70\n18.62\n0.820\n2.20\n100K\n18.08\n15.42\n0.798\n2.32\n150K\n18.35\n14.69\n0.801\n2.31\n200K\n18.44\n14.21\n0.797\n2.31\nMAR\nGT\n54.42\n55.23\n0.732\n1.88\n100K\n46.43\n41.90\n0.701\n2.20\n150K\n48.22\n42.33\n0.688\n2.06\n200K\n50.30\n44.05\n0.649\n2.03\nTable 4: Performance comparison of dialect-specific\nmodels across different training updates (Upd.).\nTo ensure a fair and unbiased comparison\nwith the leading commercial system, ElevenLabs\u2019\nEleven v3 (alpha), we adopted the following pro-\ntocol for selecting dialect-specific reference audio.\nFirst, we identified candidate voices in ElevenLabs\u2019\nofficial voice library of PVC voices, which are\nfine-tuned on over 30 minutes of speaker data ac-\ncording to their official documentation8, and se-\n8elevenlabs.io/docs/creative-platform/voices/\nvoice-cloning#professional-voice-cloning\nlected the most frequently used voice for each tar-\nget dialect. For each candidate, we performed sec-\nondary validation using a large language model to\nconfirm that the audio preview exhibited authentic\ndialect-specific characteristics in its transcription\n(extracted using ElevenLabs\u2019 Scribe v2). Once val-\nidated, this audio was designated as the reference\nfor comparative evaluation. If no suitable voice\nwas available in ElevenLabs\u2019 library, we applied\nthe same protocol to audio samples from our Habibi\nbenchmark. All selected ElevenLabs Voice IDs or\nHabibi benchmark entries, along with texts, are\nprovided in Appendix 1.1.\n3.3\nResults of Dialect-Specific Models\nSpecialized dialect models are trained on D1 (in\nTable 2). For SAU-specialized model, we use the\nentire SADA corpus. As shown in Table 4, spe-\ncialized models reasonably outperform the ground\ntruth (GT). We attribute this mainly to the supe-\nriority of the model design and the fact that GT\nsamples, sourced from ASR corpora, are of rela-\ntively higher noise levels compared to TTS outputs,\nwhich challenge ASR models. In the subsequent\nexperiments, we selected checkpoints with the low-\nest WER-O from each dialect for comparison.\n3.4\nResults of Unified-Dialectal Models\nAccording to Table 3, our unified dialect model\nhas achieved performance very close to, and in\nsome dialects even surpassed, that of specialized\nmodels. This result represents a significant break-\nthrough, demonstrating that we can achieve com-\npelling zero-shot generation capabilities even in\nthe complex and challenging task scenario of multi-\ndialect Arabic. This substantially narrows the gap\nin TTS technology for the Arabic language. Mean-\nwhile, the sustained advantages of specialized mod-\nels continue to motivate further exploration and\ninnovation.\n6\n3.5\nEffectiveness of Curriculum Learning\nIn addition to the direct comparisons in Sections 3.3\nand 3.4, we conducted a detailed ablation study to\nexamine the impact of our linguistically-informed\ncurriculum learning strategy. We evaluated sev-\neral training approaches: training from scratch, di-\nrect fine-tuning from a pre-trained Chinese-English\nbase model, and our proposed two-stage method,\nwhich first fine-tunes on Modern Standard Arabic\n(MSA) before learning dialectal distributions.\nZH-EN MSA Final WER-O\u2193WER-S\u2193SIM\u2191UTMOS\u2191\nInit.\nSFT\nSFT\nGround Truth\n28.36\n19.88\n0.724\n1.71\n\u2717\n\u2717\nSADA\n55.63\n47.54\n0.669\n1.60\n\u2713\n\u2717\nSADA\n14.69\n10.83\n0.702\n1.72\n\u2713\n\u2713\nSADA\n13.30\n10.20\n0.707\n1.71\nGround Truth (avg.)\n28.27\n23.01\n0.768\n2.17\n\u2713\n\u2717\nD1\n20.92\n18.66\n0.741\n2.27\n,\u2192Cont.\n20.39\n17.34\n0.744\n2.16\n\u2713\n\u2713\nD1\n19.50\n17.20\n0.742\n2.19\nTable 5: Effectiveness of curriculum learning. The\nsettings are defined by the inclusion of a Chinese &\nEnglish pretrained model initialization, first-stage MSA-\nbased SFT, and final-stage SFT with a selected corpus.\nThe mark \u201c,\u2192Cont.\u201d indicates continued training.\nAs shown in Table 5, training from scratch is\nsuboptimal, while initializing with the multilin-\ngual base model enables successful convergence.\nCrucially, our curriculum approach\u2014first learn-\ning foundational structure via MSA, then adapting\nto dialects\u2014achieves the highest overall perfor-\nmance, underscoring the effectiveness of a struc-\ntured, knowledge-informed learning trajectory.\nMoreover, we verified that this finding is not\nlimited to a single dialect (e.g., Saudi Arabic, as\nablated) but holds comprehensively from a unified\nmodeling perspective. Even when compared with\na model receiving twice the training updates but\nwithout the MSA fine-tuning stage (\u201c,\u2192Cont.\u201d in\nTable 5), our curriculum approach demonstrates\ngreater efficiency and superior performance.\n3.6\nEffectiveness of In-Context Learning\nTo investigate the in-context learning capabilities\nof our model, we conducted an ablation study in\nwhich the reference audio sequence was zeroed out,\ni.e., the prior contextual information was disrupted\nduring inference. As shown in Table 6, the absence\nof reference speech led to a marked degradation\nin both WER-O and WER-S scores. This behav-\nior suggests that the model depends critically on\nUni.D2-I\nMSA\nSAU\nUAE\nALG\nIRQ\nEGY\nMAR\nWER-O \u2193\nw/o Context 13.77 18.73 12.33 32.87 20.15 18.76 42.42\nw/ Context\n7.71 13.96\n5.15 31.18 17.12 18.58 40.02\nWER-S \u2193\nw/o Context 13.30 16.37 10.68 26.87 14.63 23.22 43.09\nw/ Context\n7.62 11.13\n4.88 23.58 11.90 16.44 41.53\nTable 6: Effectiveness of in-context learning during the\ninference of the unified dialectal TTS trained on D2\nwith regional identifiers (Uni.D2-I), as an example.\nSAU\nWER-O \u2193\nWER-S \u2193\nSIM \u2191\nUTMOS \u2191\nRatio\n0\n1\n0\n1\n0\n1\n0\n1\n0\n13.30 14.33 10.20 10.52 0.707 0.682 1.71 1.82\n0.618 13.56 14.33 10.42 10.51 0.706 0.691 1.71 1.90\n1\n14.13 14.58 10.67 10.69 0.700 0.692 1.73 1.91\nTable 7: Ablation results of data mixing ratios on SADA\nduring Saudi-specialized model training. Evaluated on\noriginal (ratio 0) and fully denoised (ratio 1) test sets.\nspeech-text cues from the reference context and\nleverages retrieved nuanced dialect-relevant acous-\ntic patterns in generation (see Section 2.5).\n3.7\nAblation of Data Mixing and Scaling\nAs described in Section 2.2, we applied a denoising\nenhancement model to the high-noise portion of our\ntraining data. To assess its impact, we conducted an\nablation study on SADA, which is the dataset used\nto train the Saudi-specialized TTS model. Specifi-\ncally, we varied the sampling ratio between original\nnoisy audio and denoised versions during training,\ntesting three configurations: (1) Ratio 0, using only\nthe original (noisy) sample; (2) Ratio 1, using only\nthe denoised sample; (3) Mixed sampling, select-\ning the denoised sample with a probability of 0.618.\nThe results, summarized in Table 7, show that train-\ning and inference on data from the same source\ndistribution typically yield the best performance.\nHowever, mixed training with denoised enhance-\nment achieves comparable results in terms of SIM\nand UTMOS on raw data while performing bet-\nter when pure clean speech is used as reference\nprompt.\nMotivated by these findings, we adopted the\nmixed-sampling approach with a fixed probabil-\nity of 0.618 as our standard training protocol. This\nratio was applied only to source data that had a\ncorresponding denoised version; cleaner datasets,\nas noted in Section 2.2, did not undergo denoising\npre-processing.\n7\nID\nWER-O \u2193\nWER-S \u2193\nSIM \u2191\nUTMOS \u2191\nUni.D1\nUni.D1-I\nUni.D1\nUni.D1-I\nUni.D1\nUni.D1-I\nUni.D1\nUni.D1-I\n0 \u27e8\u27e9\ni \u27e8\u27e9\n0 \u27e8\u27e9\ni \u27e8\u27e9\n0 \u27e8\u27e9\ni \u27e8\u27e9\n0 \u27e8\u27e9i \u27e8\u27e9\nMSA\n7.81\n7.64\n7.51\n7.44\n7.59\n7.60\n7.57\n7.61\n0.757\n0.757 0.758 0.757\n1.95\n1.93\n1.93\n1.93\nSAU\n13.88\n13.87 13.97 13.75\n11.12\n11.27 11.17 11.16\n0.696\n0.693 0.694 0.695\n1.68\n1.66\n1.66\n1.66\nUAE\n5.30\n5.19\n5.11\n5.04\n6.17\n4.94\n4.99\n4.78\n0.784\n0.773 0.774 0.779\n2.72\n2.71\n2.72\n2.71\nALG\n32.52\n28.86 29.16 31.68\n23.75\n23.69 23.84 23.57\n0.731\n0.726 0.725 0.731\n2.44\n2.46\n2.46\n2.43\nIRQ\n18.11\n17.98 17.71 17.91\n11.87\n12.06 11.82 11.82\n0.760\n0.759 0.758 0.759\n2.46\n2.45\n2.45\n2.44\nEGY\n18.67\n18.60 18.97 19.04\n17.38\n17.31 17.21 16.89\n0.786\n0.786 0.786 0.787\n2.13\n2.12\n2.12\n2.12\nMAR\n40.21\n36.13 36.65 40.12\n43.30\n43.79 43.26 42.63\n0.683\n0.685 0.684 0.684\n1.97\n1.98\n1.98\n1.96\nTable 8: Comprehensive comparison between unified dialectal models trained with or without regional identifiers\n(Uni.D1 and Uni.D1-I), and with different inference patterns (plain text, ID-agnostic, and ID-aware) for the latter.\nData\nHours WER-O\u2193WER-S\u2193SIM\u2191UTMOS\u2191\nMoroccan-Specialized Model\nGT[MAR]\n-\n54.42\n55.23\n0.732\n1.88\nD1[MAR]\n34\n46.43\n41.90\n0.701\n2.20\n+ DarijaTTS-clean 43\n45.70\n42.15\n0.699\n2.12\n+ Darija-S2T\n73\n44.57\n41.63\n0.693\n2.08\nEgyptian-Specialized Model\nGT[EGY-MGB2]\n-\n22.70\n18.62\n0.820\n2.20\nD1[EGY]\n37\n18.08\n15.42\n0.798\n2.32\n+ pseudo clean\n55\n18.12\n15.63\n0.794\n2.29\n+ noisy\n103\n18.29\n16.04\n0.791\n2.29\nGT[EGY-MASC]\n-\n6.06\n9.97\n0.773\n2.07\nD1[EGY]\n37\n3.58\n5.50\n0.790\n2.23\n+ pseudo clean\n55\n3.67\n5.57\n0.788\n2.23\n+ noisy\n103\n3.89\n5.96\n0.784\n2.24\nUnified Model (avg. score)\nGT[all]\n-\n28.27\n23.01\n0.768\n2.17\nD1[all]\n1635\n19.28\n16.92\n0.742\n2.18\nD2[all]\n1857\n19.10\n16.73\n0.742\n2.20\nTable 9: Ablation results of different data scaling strate-\ngies\u2014from aspects of average sample length, audio\nquality, and overall data scale\u2014on MAR, EGY, and full\nsets.\nFurthermore, to systematically examine the ef-\nfects of data scaling, we conducted three targeted\nablation studies focusing on data quality, utterance\nlength diversity, and overall dataset size (Table 9).\nThe results on MAR indicate that the richness of\nsample length is beneficial to the TTS model\u2019s per-\nformance (DarijaTTS-clean consists primarily of\nshort segments lasting a few seconds; Darija-S2T\ncontains mainly audio clips ranging from 20 to 30\nseconds). According to results on EGY, training\nwith clean D1 yields the best result, indicating that\ndata quality remains crucial for the TTS task (the\nnoisy is from the MASC noisy subset; the pseudo-\nclean is filtered with WER-O under 15% from the\nnoisy; EGY-MASC is a complementary test set drawn\nfrom MASC to reduce distributional bias in assess-\nment). Finally, expanding from D1 to D2 not only\nbroadens dialect coverage (see Table 2), but also\nenhances overall performance.\n3.8\nAblation of Regional Identifiers\nTable 8 indicates that explicitly specifying a re-\ngional identifier consistently improves WER perfor-\nmance, maintains SIM performance, and slightly\nreduces UTMOS scores. Overall, the use of re-\ngional identifiers facilitates the model\u2019s learning\nof dialectal distributions, thereby enhancing per-\nformance\u2014particularly for WER, which remains\na primary evaluation metric in the current stage of\nArabic TTS development.\nFurthermore, as discussed in Section 2.5, the\nmodel exhibits strong robustness to different infer-\nence templates. However, maintaining complete\nconsistency between training and inference (i.e.,\nthe ID-aware case, formulated as i \u27e8\u27e9) still yields\nthe overall best results. Additionally, it is worth\nnoting that WER-O and WER-S jointly provide\nmore comprehensive evaluation perspectives, as\nemphasized in Section 3.2. We argue that WER-\nO reflects the model\u2019s generalization capability to\nsome extent, while WER-S places relatively greater\nemphasis on dialect-specific textual faithfulness it-\nself, which is more desirable.\n4\nConclusion\nIn this work, we introduce Habibi, the first open-\nsource framework for unified-dialectal Arabic\nspeech synthesis. By leveraging linguistically in-\nformed curriculum learning and a rigorous data\ncuration pipeline, we demonstrate that high-quality\nzero-shot synthesis across diverse, low-resource\nArabic dialects is achievable, even when training\ndata is primarily repurposed from ASR corpora,\n8\nand without requiring text diacritization. Empiri-\ncally, our unified model attains performance compa-\nrable to dialect-specialized counterparts, and even\nsurpasses them on several dialect subsets under our\nstandardized benchmark. Moreover, Habibi outper-\nforms the strongest available commercial baseline,\nElevenLabs\u2019 Eleven v3 (alpha), across major di-\nalect test sets, highlighting the practical competi-\ntiveness of an open-source solution in this long-tail\nsetting."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "This paper presents Habibi, a suite of specialized and unified text-to-speech models for Arabic dialects. The main contributions include developing a comprehensive benchmark for multi-dialectal speech synthesis, leveraging existing ASR corpora to support diverse Arabic dialects, and establishing effective in-context learning strategies without requiring text diacritization.\n\n**Main Contributions:**\n1. **Unified-Dialectal Model**: Habibi is an open-source framework that supports more than 20 language variants (ISO 639-3 codes) and 12 regional identifiers across Arabic dialects, including MSA.\n2. **Benchmark Development**: The paper introduces the first systematic benchmark for multi-dialectal zero-shot speech synthesis, featuring at least 1,000 to 3,000 utterances per subset of seven dialects, paired with manually transcribed and rigorously filtered text.\n3. **Linguistically-Informed Curriculum Learning**: Habibi employs a two-stage curriculum learning strategy: first, initial training on MSA data; second, fine-tuning on all dialectal data combined to capture subtle dialectal features without diacritized text.\n\n**Methodology:**\nThe authors collected and processed various open-source ASR datasets (e.g., MASC, SADA) to create a robust training corpus. They applied strict filtering procedures, including CPS thresholding for noise reduction and manual rule-based filtering of corrupted data. The model was trained on these curated datasets using F5-TTS framework.\n\n**Key Findings:**\n1. **Performance Comparison**: Habibi outperforms the leading commercial service (ElevenLabs' Eleven v3) in generation quality across all major dialect test sets.\n2. **Curriculum Learning Effectiveness**: The two-stage curriculum learning strategy significantly improves model performance, particularly for WER and SIM metrics.\n3. **In-Context Learning Impact**: In-context learning without diacritized text is effective but slightly reduces UTMOS scores.\n\n**Conclusions:**\nHabibi demonstrates that high-quality zero-shot synthesis across diverse Arabic dialects is achievable even when training data comes from ASR corpora, and without requiring text diacritization. The model outperforms existing commercial baselines and provides a solid foundation for future research in multi-dialectal speech synthesis.\n\n**Future Work:**\nThe authors aim to expand Habibi's coverage by incorporating dialect-specific ASR models and further refine the evaluation standards established in this study."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "TRACTRLFUSION: A GPT-BASED MULTI-CRITIC POLICY FUSION FRAMEWORK FOR\nFIBER TRACTOGRAPHY\nAnkita Joshi1, Ashutosh Sharma1, Anoushkrit Goel1, Ranjeet Ranjan Jha2,\nChirag Ahuja3, Arnav Bhavsar1, Aditya Nigam1\n1 IIT Mandi, 2 IIT Patna, 3 PGIMER Chandigarh\nABSTRACT\nTractography plays a pivotal role in the non-invasive\nreconstruction of white matter fiber pathways, providing\nvital information on brain connectivity and supporting pre-\ncise neurosurgical planning.\nAlthough traditional methods\nrelied mainly on classical deterministic and probabilistic\napproaches, recent progress has benefited from supervised\ndeep learning (DL) and deep reinforcement learning (DRL)\nto improve tract reconstruction.\nA persistent challenge in\ntractography is accurately reconstructing white matter tracts\nwhile minimizing spurious connections. To address this, we\npropose TractRLFusion, a novel GPT-based policy fusion\nframework that integrates multiple RL policies through a\ndata-driven fusion strategy. Our method employs a two-stage\ntraining data selection process for effective policy fusion,\nfollowed by a multi-critic fine-tuning phase to enhance ro-\nbustness and generalization. Experiments on HCP, ISMRM,\nand TractoInferno datasets demonstrate that TractRLFusion\noutperforms individual RL policies as well as state-of-the-\nart classical and DRL methods in accuracy and anatomical\nreliability.\nIndex Terms\u2014 Diffusion MRI, Tractography, Reinforce-\nment Learning, Transformers\n1. INTRODUCTION\nWhite matter fiber tractography[1] is a key technique to study\nbrain structural connectivity, neurological disorders, and to\nsupport precise neurosurgical planning [2]. It enables in vivo\nand non-invasive mapping of white matter pathways using\ndiffusion MRI (dMRI) data. dMRI captures the anisotropic\ndiffusion (movement) of water molecules within white mat-\nter tissue, as water diffuses more readily along the axonal\nfibers. This directional dependence is typically modeled us-\ning fiber orientation distribution functions (fODFs), allowing\ninference of white matter pathways. Traditional tractography\nalgorithms are generally categorized as deterministic, proba-\nbilistic, and global approaches. Deterministic methods[3, 4]\nreconstruct streamlines by following the principal diffusion\ndirection at each step, but often fail in regions with crossing,\nbranching, or other complex fiber configurations. In contrast,\nprobabilistic methods[5] estimate a distribution of possible\npathways at each step, improving coverage, but at the cost\nof increased false positives (FPs). Global methods[6] attempt\nto handle the FP-FN trade-off in tractography[7, 8], improv-\ning overall consistency but at the cost of high computational\ncost and limited generalizability. Supervised Learning based\nmethods rely on ground-truth tracts for training. For example,\na Random Forest Classifier[9] predicts the next streamline di-\nrection from a set of candidate directions and past estimated\nsteps. Similarly, DL methods such as Learn-to-Track[10] and\nDeepTract[11] formulate tractography as a sequential deci-\nsion making task, employing GRUs to model streamline paths\nand probabilistic fiber orientation distributions, respectively.\nIn parallel, DRL methods[12, 13, 14, 15] learn tractography\npolicies through exploration rather than direct supervision.\nInterestingly, we observe a similar FP-FN trade-off\namong both deterministic and stochastic RL policies. This\nobservation motivates our investigation of the fusion of\nRL policies to improve tractography performance, a strat-\negy that has proven to be effective in other domains, such\nas robotics and AI games to improve generalization and\nrobustness[16, 17]. Traditional ensemble strategies for RL\npolicy, including decision-level aggregation (e.g., voting or\naveraging), often fail to capture semantic context or state his-\ntory. Conversely, more complex ensemble methods, such as\nEnsemble Policy Gradient or hierarchical ensembles, intro-\nduce significant system complexity due to agent synchroniza-\ntion, parameter sharing, and joint optimization requirements,\nmaking them difficult to implement, debug, and maintain.\nTo address these challenges, we propose TractRLFusion\n(Fig. 1), a data-driven fusion framework to combine multiple\nRL policies in tractography. In this work:\n\u2022 We introduce TractRLFusion, a novel GPT-based policy\nfusion framework that integrates Episodic Data Selection\n(EDS) to select tract-specific trajectories with anatomical\nprecision, enabling data-driven fusion.\n\u2022 We propose Multi-Critic Policy Fine-Tuning (MCPFT)\nto enhance policy fusion, achieving robust, scalable, and\ngeneralizable tractography across diverse datasets.\n\u2022 Extensive comparative evaluation against state-of-the-art\ntractography methods demonstrate its superior perfor-\nmance, effectiveness, and generalizability.\narXiv:2601.13897v1  [cs.LG]  20 Jan 2026\nFig. 1: TractRLFusion: An RL policy fusion framework for\ntractography, comprising EDS, FusionNet and MCPFT.\nTo our knowledge, this is the first policy-fusion frame-\nwork for tractography. Code will be released on GitHub.\n2. METHODOLOGY\nWe fuse three independently trained policies, namely TD3\n[18], SAC [19], and DDPG [16], to combine their comple-\nmentary strengths in tractography. While SAC learns in an\nexploration-driven manner, resulting in higher tract coverage\nbut increased false positives, TD3 and DDPG exhibit more\nconservative tracking behavior, yielding lower coverage with\nfewer false positives, as shown in Table 3. The TractRLFu-\nsion framework comprises three key components: Episodic\nData Selection (EDS), FusionNet, and Multi-Critic Policy\nFinetuning (MCPFT). To enable data-driven fusion, we sam-\nple trajectories from trained RL policies, where a trajectory\nrepresents a sequences of state, action, and return-to-go tu-\nples obtained from an RL policy. A trajectory of length T is\ndefined as: \u03c4 = (s0, a0, rtg0, s1, ....sT , aT , rtgT ).\n(i) EDS samples a mix of trajectory data from each of\nthe three policies to train FusionNet. (ii) FusionNet is then\ntrained in two stages on EDS data to learn the fusion policy.\n(iii) Finally, the FusionNet policy is refined using MCPFT to\nenhance stability and generalization.\nFusionNet learns a tract-specific policy for each targeted\nwhite matter bundle. To this end, we train and evaluate our\npolicies within tract-specific masks from [15]. Among the\nmajor tracts, we focus on the occipital and pre/post central\ngyri regions of the Corpus Callosum (CC), the left and right\nCorticospinal Tract (CST), the Arcuate Fasciculus (AF), and\nthe Cingulum (CG), following previous work[13].\nTract-Specific RL policy Training: Training is con-\nducted through exploration in an environment similar to [12],\nbut constrained within tract-specific masks of five training\nsubjects from the TractoInferno dataset[20] (Table 1). The\nreward is defined as the product of the action vector\u2019s (\u20d7at)\nalignment with the most aligned fODF peak (p) and its dot\nproduct with the previous tracking direction (\u20d7ut\u22121):\nrt =\nmax\n\u20d7pi (\u20d7pi \u00b7 \u20d7at)\n\u00d7 (\u20d7at \u00b7 \u20d7ut\u22121)\n(1)\nEpisode termination conditions are (i) exceeding 530 steps\n(\u2261200 mm fiber), (ii) leaving the tract mask, or (iii) an an-\ngle > 60 with the previous segment. Subsequent sections de-\nscribe each component of Tract-RLFusion in detail.\n2.1. Episodic Data Selection\nIt is the data preparation process for the pre-training and fine-\ntuning stages of FusionNet model. It involves following steps:\n(i) Tracking is performed using each trained policy (TD3,\nSAC, and DDPG) from a common batch of neighboring seed\npoints sampled from the tract mask of a given subject. This\nproduces streamlines {(x,y,z)} along with their corresponding\ntrajectory tuples (s, a, rtg).\n(ii) Trajectories containing fewer than 47 transitions\n(equivalent to approximately 20mm for a 1mm3 voxel size\nand 0.375 step size) are filtered-out. The remaining tracjec-\ntories then undergo batch-wise across-policy and within-\npolicy selection for subsequent processing.\n(iii) Within-policy selection: Fifteen reference stream-\nlines are selected from the atlas tract[21] using the farthest\nstreamline sampling. For each policy, we filter streamlines to\nretain only the trajectories that lie within a 5mm MDF (mean\ndirect flip) distance [22] from the reference streamlines, thus\nselecting the most anatomically accurate trajectories.\n(iv) Across policy selection selects one policy out of\nthe three (TD3, SAC, DDPG), based on the maximum ex-\npected Q-value (normalised to incorporate different scales) of\nits MDF-filtered trajectories. These trajectories are selected\nfor the finetuning dataset.\nIn contrast, for the pretraining\ndataset, Q-value-based selection is performed directly on the\nunfiltered trajectories.\nNeighboring seed points are selected in each batch, en-\nabling EDS to select the best trajectories from all regions of\nthe given tract. This process is repeated across all tracts of the\nfive training subjects, resulting in approximately 150000 pre-\ntraining and 50000 finetuning trajectories after length-based\nand random selection. In particular, the pretraining dataset\ncomprises trajectories from all tracts, whereas the finetuning\ndata consists of tract-specific trajectories.\n2.2. FusionNet (Architecture and Training)\nCentral to our framework, FusionNet is a 4-layer GPT [23]\n(one attention head, 128-dim embeddings, context length 40,\nReLU activation, and dropout rate of 0.1) that models RL\ntrajectories as sequences. It undergoes general pretraining\nand tract-specific fine-tuning on expert trajectories from EDS.\nSpecifically, the first three layers are trained on mixed tra-\njectories from the EDS pretraining data over 30 iterations,\nwhile the final layer is fine-tuned for 10 iterations using tract-\nspecific trajectories from the EDS finetuning data. This two-\nstage training strategy (Table 2) is designed to first capture\nbroad trajectory patterns and subsequently adapt to the fine-\ngrained tract-specific details. We employ a five-step angular\nloss (Ldistcos) between predicted (\u02c6a) and actual (a) actions:\nLdistcos =\nlcontext\u22122\nX\nt=2\n2\nX\ni=\u22122\ncos\u22121 (at+i \u00b7 \u02c6at+i)\n!\n(2)\nThe model was trained with AdamW optimizer, hyperpa-\nrameters tuned to maximize Accumulated sum of rewards and\nDice Scores. Additional training with supervised loss (Eq. 2)\nled to performance saturation, showing limited improvement.\n2.3. Multi-critic Policy Fine-tuning (MCPFT)\nWe fine-tune FusionNet using MCPFT in an actor-critic setup\nwith critic networks derived from SAC, DDPG, and TD3.\nLactor = Ldistcos +\nK=3\nX\nk=1\n\"\n\u2212\nlcontext\nX\nt=0\nQC\n(\u03c0k)(st, \u02c6at)\n#\n|\n{z\n}\nLcritic,k\n(3)\nThe actor (FusionNet) is optimized using the compos-\nite loss Lactor (Eq. 3), which combines the supervised loss\nLdistcos with the aggregated loss from our K = 3 critics,\nwhere each Lcritic,k is formulated to maximize the critics\u2019\nvalue estimates (Eq. 3). Standalone aggregated critic loss\n(P3\nk=1 Lcritic,k) caused a performance drop after a few up-\ndates, followed by saturation below the prior levels.\nOur\nobservations during training indicate the ability of Ldistcos to\npreserve prior actor knowledge, while the critic term further\nrefines its policy. We observed that delayed critic updates\nprevent saturation from unstable value estimates. Training\nparameters are summarized in Table 2.\n2.4. Tracking Process\nUsing the FusionNet policy, tracking is performed for differ-\nent tracts within their respective tracking masks, by seeding at\n7 seeds per voxel. Return-to-go (rtg0) is initialized to 300 for\nFusionNet, and dataset-specific step sizes (0.375mm for Trac-\ntoInferno, 0.468mm for HCP, 0.75mm for ISMRM) are set.\nAs a postprocessing step, tract filtering is performed using\nFast Streamline Search [24] with atlas reference tracts [25].\nCategory\nDetails\nState\nFeatures\n- 45 SH coefficients (6 neighbors+1)\n- Last 4 past directions (12 values)\n- Tract mask value (6 neighbors+1)\n- Concatenated to: 334 dimensions\nArchitectures\n(Actor, Critics)\n- 3-layer ReLU-activated FCNNs\n- Neurons per hidden layer: 1024\nTrain Data [20]\n- IDs:1030, 1079, 1119, 1180, 1198\nHyperparameters - Batches: 50 per subject\n- Batch size: 4096 transitions\n- Seeding: 7/voxel; Step size: 0.375\nPolicy\nHyperparameters\n- TD3: \u03b7 = 8.56e-6, \u03c3 = 0.334, \u03b3 = 0.776\n- SAC: \u03b7 = 3.7e-5, \u03c3=0.4, \u03b3=0.89, \u03b1=0.076\n- DDPG: \u03b7 = 8.56e-6, \u03c3 = 0.35, \u03b3 = 0.5\nTable 1: Summary of Features, Architecture, Train Subjects,\nand Hyperparameters for training Tract-specific RL policies.\nCategory\nDetails\nFusionNet\nTraining\n- Optimizer: AdamW (\u03b7: 1e-4)\n- Batch Size: 128; Updates/iter: 1e4\n- Iters:30(General),10(Tract-specific)\n- Warmup-Steps: 1e4\nMCPFT-based\nFinetuning\n- \u03b7: 1e-4 ; Batch Size: 512 ; Iters: 25\n- Updates/iter: 1000 (actor), 1 (critic)\nTable 2: Summary of FusionNet training parameters.\n3. EXPERIMENTS AND RESULTS\nThis section presents evaluation on test subjects from Trac-\ntoInferno (1160, 1078, 1061, 1159, 1171), HCP (930449,\n959574, 992774, 987983) and ISMRM. We compare with:\nClassical DET [1] and PROB (iFOD1) [26]; RL policies SAC,\nTD3, and DDPG[12, 13] employed in TractRLFusion, and\ntheir decision-level ensembles \u03c0maxQ (Max Q-value action)\nand \u03c0Avg(average of actions), TractSeg [27] and [15], incor-\nporating SAC (best performing) in their framework (TRLFS).\nSince PROB contributed to create TractoInferno ground truth,\nit may be inherently favored. Moreover, reference stream-\nlines for HCP [27] are made using iFOD2, a close successor\nof PROB. Hence it is evaluated on ISMRM. TractOracle [28]\nwas left out, as it is tailored for WM/GM\u2013interface seeding.\nComparative analysis reveal the following notable insights:\n\u2022 Our FusionNet model consistently ranks best across all\ndatasets and tracts as shown in Table 3.\n\u2022 Fusion policy (FusionNet) outperforms individual RL\npolicies (SAC, TD3, DDPG) and their RL-Ensemble\nmethods (\u03c0maxQ and \u03c0avg). Individual RL policies show\na high OR-OL tradeoff with SAC achieving highest OL\nwith worst OR, and DDPG and TD3 achieving low OL\nand OR. The Ensemble\u2019s Q-value approach yields sub-\noptimal tracking, while FusionNet\u2019s learned policy better\nbalances OL-OR trade-offs, resulting in the best Dice.\n\u2022 FusionNet* (FusionNet tested on TractSeg masks [27])\nsurpasses TractSeg across all HCP tracts, e.g., achieving a\nDice score of 74.1 versus TractSeg\u2019s 70.9 for AF, demon-\nstrating superior tracking even within TractSeg\u2019s masks.\n\u2022 FusionNet* and TractSeg achieve highest scores on HCP,\nas TractSeg was trained on HCP data to obtain those\nmasks which were also involved in making reference\nstreamlines of HCP. However, masks from [15] were\navailable for all three datasets (TractoInferno, HCP, and\nISMRM), preferred for a consistent comparison across\nthese datasets. FusionNet\u2019s superior performance within\nboth masks demonstrates its robustness to variations in\nmask quality for tract-specific tractography.\n\u2022 Spurious connections made by SAC and PROB in left CG\nand CC Oc encircled in red (Fig. 2(a,b)) show lower OR\nby our method, consistent with the scores in table 3.\n\u2022 In Fig. 2(a), PROB struggles to reconstruct upper portion\nof CST right, at pyramidal decussation (intersection of left\n& right CST) compared to FusionNet.\n\u2022 Similar visual trend was observed in these tracts for other\nsubjects, but we limit for brevity.\n(a) CG and CST/PYT\nCG\nCST/PYT\nData Algorithm\nDice\u2191OL\u2191\nOR\u2193\nDice\u2191OL\u2191\nOR\u2193\nDET\n53.9\n44.1\n18.9\n62.9\n51.8\n11.7\nSAC\n56.2\n47.3\n20.2\n67.7\n60.8\n17.4\nDDPG\n48.0\n35.8\n12.8\n66.1\n60.4\n20.4\nHCP\nTD3\n49.2\n37.5\n14.7\n55.7\n46.2\n15.8\nTRLFS\n54.0\n41.6\n12.3\n61.6\n54.1\n15.7\n\u03c0maxQ\n56.3\n47.5\n20.7\n67.9\n59.5\n14.9\n\u03c0avg\n56.0\n46.6\n19.6\n69.2\n63.7\n17.6\nFusionNet\n56.6\n47.2\n19.2\n69.4\n62.5\n17.0\nTractSeg\n76.8\n73.1\n17.2\n80.8\n73.2\n7.8\nFusionNet* 77.0\n86.7\n38.2\n82.2\n79.8\n14.4\nDET\n59.7\n56.8\n33.0\n71.5\n77.8\n39.8\nSAC\n63.1\n70.9\n55.9\n73.0\n70.3\n22.0\nDDPG\n59.9\n55.7\n30.4\n69.2\n60.7\n14.3\nTractoInferno\nTD3\n57.5\n52.4\n29.9\n69.6\n62.1\n15.9\nTRLFS\n62.5\n63.3\n39.8\n69.8\n64.1\n19.1\n\u03c0maxQ\n63.5\n71.5\n53.9\n73.9\n70.6\n21.1\n\u03c0avg\n62.3\n71.2\n60.9\n73.2\n71.4\n23.5\nFusionNet\n64.0\n65.9\n41.6\n74.5\n73.0\n24.9\nDET\n57.6\n52.2\n28.9\n43.1\n38.7\n40.8\nPROB\n62.6\n62.4\n37.1\n50.3\n50.7\n49.1\nSAC\n62.7\n64.9\n41.5\n49.7\n50.3\n50.8\nDDPG\n57.7\n52.9\n30.0\n36.2\n31.4\n39.9\nISMRM\nTD3\n52.2\n43.2\n22.3\n36.9\n31.6\n39.0\nTRLFS\n61.1\n58.2\n32.1\n49.1\n51.1\n57.3\n\u03c0maxQ\n62.8\n65.1\n41.6\n50.2\n49.8\n48.8\n\u03c0avg\n62.8\n59.5\n30.1\n46.7\n45.4\n49.1\nFusionNet\n63.9\n65.4\n39.1\n52.6\n54.3\n51.2\n(b) AF and CC\nAF\nCC\nData Algorithm\nDice\u2191OL\u2191\nOR\u2193\nDice\u2191OL\u2191\nOR\u2193\nDET\n53.4\n41.3\n10.3\n70.9\n77.2\n41.2\nSAC\n54.6\n42.4\n11.4\n70.4\n80.4\n48.1\nDDPG\n50.6\n37.9\n8.3\n65.0\n66.2\n37.3\nHCP\nTD3\n51.7\n39.5\n9.6\n64.9\n66.2\n37.2\nTRLFS\n50.9\n38.0\n8.2\n67.6\n67.5\n30.9\n\u03c0maxQ\n54.8\n42.3\n10.9\n70.4\n81.6\n50.1\n\u03c0avg\n52.3\n40.5\n12.7\n70.1\n78.4\n46.1\nFusionNet\n55.5\n43.8\n12.3\n72.4\n81.8\n44.8\nTractSeg\n70.9\n59.5\n8.1\n76.8\n68.5\n11.7\nFusionNet* 74.1\n66.6\n13.0\n77.4\n78.6\n23.9\nDET\n50.0\n44.0\n35.8\n49.7\n39.5\n19.6\nSAC\n52.3\n52.4\n52.1\n54.6\n48.3\n28.6\nDDPG\n46.3\n39.3\n28.6\n49.6\n40.4\n22.1\nTractoInferno\nTD3\n45.3\n39.6\n37.6\n48.1\n38.3\n20.9\nTRLFS\n47.5\n43.5\n43.2\n46.9\n37.0\n21.1\n\u03c0maxQ\n52.7\n51.9\n47.3\n54.2\n47.0\n30.7\n\u03c0avg\n52.1\n51.9\n49.5\n53.3\n47.3\n31.5\nFusionNet\n53.2\n51.3\n39.1\n56.5\n50.5\n28.8\nTable 3: Performance comparison (%) for CG, CST/PYT, AF,\nCC tracts on HCP, TractoInferno, and ISMRM datasets. Dice,\nOL, OR are averaged across test subjects. DET, PROB re-\nsults from [15]; SAC, DDPG, TD3 from [13] were used to\ntrain our network, compared with same specifications. SAC\nbased TRLFS[15] trained on EDS. TractSeg and FusionNet*\nuse TractSeg masks, evaluated on HCP. Best Dice is in bold.\nNote: While simpler ensembling methods (\u03c0maxQ and \u03c0Avg)\nachieve lower scores than FusionNet, more complex RL en-\nsembling techniques often introduce stability and scalability\nchallenges. TractRLFusion addresses (this) fusion by learn-\ning a robust fusion policy in a data-driven manner, which sim-\nplifies integration compared to more traditional RL ensemble\nmethods. We illustrate our method with three policies (SAC,\nDDPG, TD3) to address the overlap\u2013overreach trade-off, but\nthe framework is modular and can incorporate additional poli-\ncies to capture broader tracking behaviors.\nFig. 2: Visualization of the right CST (a) and a segment of the\nleft CG (b) from HCP subject 992774, and the occipital CC\n(c) from TractoInferno subject 1160. The proposed method\nis compared with PROB, SAC (other top-performers), and\nground-truth tracts, with dashed circles marking key regions.\nAblation Study: The ablation study underlines the im-\nportance of EDS and MCPFT. EDS involves a within-policy\nand across-policy (EDSacross\u2212\u03c0) selection of training trajec-\ntories, whereas EDSacross\u2212\u03c0 selects trajectories across dif-\nferent policies using only Q-value. Table 4 reveals that pre-\ntraining and finetuning on only EDS (ii) performs worse than\n(EDSacross\u2212\u03c0) (i). Pretraining on coarser EDSacross\u2212\u03c0 and\nfinetuning on EDS (iii) performs the best. Hence FusionNet\nis trained on data from (iii), followed by MCPFT, whose re-\nsults in Table 3 show effectively lower OR with higher OL, a\ndesirable outcome in tractography, leading to a higher Dice.\nTract\n(i) EDSacross\u2212\u03c0\n(PT & FT)\n(ii) EDS\n(PT & FT)\n(iii) EDSacross\u2212\u03c0\nPT & EDS FT\nDice OL\nOR\nDice OL\nOR\nDice OL\nOR\nPYT\n70.4\n67.3\n23.4\n58.6\n45.7\n8.9\n70.9\n68.3\n23.9\nCG\n59.2\n58.6\n39.4\n42.9\n32.1\n10.7\n61.4\n59.5\n34.5\nAF\n47.3\n39.7\n27.3\n23.3\n14.4\n5.3\n48.7\n43.5\n38.9\nCC\n67.7\n74.1\n44.2\n60.6\n50.3\n14.9\n68.6\n70.2\n36.4\nTable 4: Mean (%) FusionNet scores on TractoInferno test\ndata comparing pretraining (PT) & finetuning (FT) strategies.\n4. CONCLUSION\nTractRLFusion enables efficient and effective white mat-\nter tracking by fusing complementary strengths of different\nactor-critic frameworks. Our ablation studies highlight the\neffectiveness of the proposed two-stage data curation, where\na strong baseline policy is first developed using a combina-\ntion of broad and anatomically-aware data selection (EDS),\nand subsequently refined through the MCPFT process. Trac-\ntRLFusion demonstrates strong generalization, being trained\nsolely on TractoInferno and evaluated on HCP and ISMRM.\nFinally, powered by FusionNet, TractRLFusion lays the\ngroundwork for a foundational tractography model with the\npotential to extend across additional datasets and integrate\ndiverse specialized policies, advancing the field of White\nMatter Tractography.\n5. COMPLIANCE WITH ETHICAL STANDARDS\nThis study used open-access human data from TractoInferno\n[20] and the Human Connectome Project [27] in accordance\nwith their licenses, and publicly available phantom data from\nthe ISMRM 2015 Tractography Challenge [29].\n6. ACKNOWLEDGMENTS\nThis work was supported by SERB Core Research Grant\nProject No: CRG/2020/005492, IIT Mandi.\nReferences\n[1] PJ Basser, S Pajevic, et al., \u201cIn vivo fiber tractography using\ndt-mri data,\u201d Magnetic resonance in medicine, vol. 44, no. 4,\npp. 625\u2013632, 2000.\n[2] Walid I Essayed et al., \u201cWhite matter tractography for neuro-\nsurgical planning: A topography-based review of the current\nstate of the art,\u201d NeuroImage: Clinical, vol. 15, 2017.\n[3] Peter J Basser,\n\u201cFiber-tractography via diffusion tensor mri\n(dt-mri),\u201d in Proceedings of the 6th Annual Meeting ISMRM,\nSydney, Australia, 1998, vol. 1226, p. 14.\n[4] Guang Cheng et al., \u201cTractography from hardi using an in-\ntrinsic unscented kalman filter,\u201d IEEE transactions on medical\nimaging, vol. 34, no. 1, pp. 298\u2013305, 2014.\n[5] Jeffrey I Berman et al., \u201cProbabilistic streamline q-ball trac-\ntography using the residual bootstrap,\u201d Neuroimage, vol. 39,\nno. 1, pp. 215\u2013222, 2008.\n[6] Marco Reisert et al.,\n\u201cGlobal fiber reconstruction becomes\npractical,\u201d Neuroimage, vol. 54, no. 2, pp. 955\u2013962, 2011.\n[7] Kurt G Schilling et al.,\n\u201cChallenges in diffusion mri\ntractography\u2013lessons learned from international benchmark\ncompetitions,\u201d MRI, vol. 57, pp. 194\u2013209, 2019.\n[8] Koji Kamagata et al., \u201cAdvancements in diffusion mri tractog-\nraphy for neurosurgery,\u201d Investigative Radiology, vol. 59, no.\n1, pp. 13\u201325, 2024.\n[9] Peter F Neher et al., \u201cA machine learning based approach to\nfiber tractography using classifier voting,\u201d in MICCAI 2015,\nProceedings, Part I 18. Springer, 2015, pp. 45\u201352.\n[10] Philippe Poulin et al., \u201cLearn to track: deep learning for trac-\ntography,\u201d\nin MICCAI 2017, Part I 20. Springer, 2017, pp.\n540\u2013547.\n[11] Itay Benou et al., \u201cDeeptract: A probabilistic deep learning\nframework for white matter fiber tractography,\u201d in MICCAI,\n2019. Springer, 2019, pp. 626\u2013635.\n[12] Antoine Th\u00b4eberge et al., \u201cTrack-to-learn: A general framework\nfor tractography with deep reinforcement learning,\u201d MIA, vol.\n72, pp. 102093, 2021.\n[13] Antoine Th\u00b4eberge et al., \u201cWhat matters in reinforcement learn-\ning for tractography,\u201d MIA, vol. 93, pp. 103085, 2024.\n[14] Antoine Th\u00b4eberge et al.,\n\u201cTractoracle:\ntowards an\nanatomically-informed reward function for rl-based tractogra-\nphy,\u201d in MICCAI. Springer, 2024, pp. 476\u2013486.\n[15] A Joshi et al., \u201cTract-rlformer: A tract-specific rl policy based\ndecoder-only transformer network,\u201d in ICPR. Springer, 2024,\npp. 258\u2013275.\n[16] TP Lillicrap,\n\u201cContinuous control with deep reinforcement\nlearning,\u201d arXiv preprint arXiv:1509.02971, 2015.\n[17] Teodor V Marinov et al.,\n\u201cOffline imitation learning from\nmultiple baselines with applications to compiler optimization,\u201d\narXiv preprint arXiv:2403.19462, 2024.\n[18] S Fujimoto et al., \u201cAddressing function approximation error in\nactor-critic methods,\u201d in ICML. PMLR, 2018, pp. 1587\u20131596.\n[19] Tuomas Haarnoja et al., \u201cSoft actor-critic: Off-policy max-\nimum entropy deep reinforcement learning with a stochastic\nactor,\u201d in ICML. PMLR, 2018, pp. 1861\u20131870.\n[20] Philippe Poulin et al.,\n\u201cTractoinferno-a large-scale, open-\nsource, multi-site database for machine learning dmri tractog-\nraphy,\u201d Scientific Data, vol. 9, no. 1, pp. 725, 2022.\n[21] Fang-Cheng Yeh et al.,\n\u201cPopulation-averaged atlas of the\nmacroscale human structural connectome and its network\ntopology,\u201d Neuroimage, vol. 178, pp. 57\u201368, 2018.\n[22] Yang Li, Ma, et al., \u201cFarthest streamline sampling for the uni-\nform distribution of forearm muscle fiber tracts from diffusion\ntensor imaging,\u201d arXiv preprint arXiv:2306.13969, 2023.\n[23] Alec Radford et al., \u201cImproving language understanding by\ngenerative pre-training,\u201d 2018.\n[24] Etienne St-Onge et al., \u201cFast streamline search: An exact tech-\nnique for diffusion mri tractography,\u201d Neuroinformatics, vol.\n20, no. 4, pp. 1093\u20131104, 2022.\n[25] Francois Rheault, \u201cPopulation average atlas for recobundlesx,\u201d\nMay 2023.\n[26] J-Donald Tournier et al., \u201cMrtrix: diffusion tractography in\ncrossing fiber regions,\u201d International journal of imaging sys-\ntems and technology, vol. 22, no. 1, pp. 53\u201366, 2012.\n[27] J Wasserthal et al., \u201cTractseg-fast and accurate white matter\ntract segmentation,\u201d NeuroImage, vol. 183, pp. 239\u2013253, 2018.\n[28] Jeremi Levesque, Antoine Th\u00b4eberge, Maxime Descoteaux, and\nPierre-Marc Jodoin, \u201cExploring the robustness of tractoracle\nmethods in rl-based tractography,\u201d Medical Image Analysis, p.\n103743, 2025.\n[29] Klaus H Maier-Hein et al.,\n\u201cThe challenge of mapping the\nhuman connectome based on diffusion tractography,\u201d Nature\ncommunications, vol. 8, no. 1, pp. 1349, 2017."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography**\n\n**Main Contributions**\nThe paper introduces TractRLFusion, a novel framework that fuses multiple reinforcement learning (RL) policies to improve tractography. This fusion is achieved through data-driven strategies and multi-critic fine-tuning phases. The authors propose Episodic Data Selection (EDS), Multi-Critic Policy Fine-Tuning (MCPFT), and FusionNet architecture, which integrate different RL policies effectively.\n\n**Methodology**\nTractRLFusion combines three independently trained RL policies: TD3, SAC, and DDPG. EDS samples trajectories from these policies to train FusionNet in two stages. The first stage uses mixed data for general pre-training, while the second fine-tunes on specific tract data. MCPFT refines the policy further, enhancing robustness and scalability across diverse datasets.\n\n**Key Findings**\nExperiments with HCP, ISMRM, and TractoInferno datasets show that TractRLFusion outperforms individual RL policies and state-of-the-art methods in accuracy and anatomical reliability. Specifically:\n- FusionNet consistently ranks best across all datasets.\n- FusionNet outperforms individual RL policies by balancing OL (Orientation Likelihood) and OR (Occlusion Rate).\n- FusionNet achieves superior Dice scores even within TractSeg masks, demonstrating robustness to mask quality variations.\n\n**Conclusions**\nTractRLFusion successfully fuses complementary strengths of different actor-critic frameworks. The two-stage data curation process using EDS and MCPFT highlights the importance of effective policy fusion. Its strong generalization capability is demonstrated by training solely on TractoInferno and evaluating on HCP and ISMRM datasets.\n\n**Future Work**\nThe framework's modularity allows incorporating additional policies, expanding its applicability to diverse specialized policies in the field of White Matter Tractography."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "UNIFIED UNBIASED VARIANCE ESTIMATION FOR MMD:\nROBUST FINITE-SAMPLE PERFORMANCE WITH IMBALANCED\nDATA AND EXACT ACCELERATION UNDER NULL AND\nALTERNATIVE HYPOTHESES\nShijie Zhong\nSchool of Power and Energy\nNorthwestern Polytechnical University\nXi\u2019an, Shanxi 710129\nzhongsj@mail.nwpu.edu.cn\nJiangfeng Fu\nSchool of Power and Energy\nNorthwestern Polytechnical University\nXi\u2019an, Shanxi 710129\nfjf@nwpu.edu.cn\nYikun Yang\nSchool of Power and Energy\nNorthwestern Polytechnical University\nXi\u2019an, Shanxi 710129\nyikunyang@mail.nwpu.edu.cn\nJanuary 21, 2026\nABSTRACT\nThe maximum mean discrepancy (MMD) is a kernel-based nonparametric statistic for two-sample\ntesting, whose inferential accuracy depends critically on variance characterization. Existing work\nprovides various finite-sample estimators of the MMD variance, often differing under the null and\nalternative hypotheses and across balanced or imbalanced sampling schemes. In this paper, we study\nthe variance of the MMD statistic through its U-statistic representation and Hoeffding decomposition,\nand establish a unified finite-sample characterization covering different hypotheses and sample\nconfigurations. Building on this analysis, we propose an exact acceleration method for the univariate\ncase under the Laplacian kernel, which reduces the overall computational complexity from O(n2) to\nO(n log n).\n1\nIntroduction\nThe fundamental problem of nonparametric two-sample testing is to detect differences between two probability\ndistributions given access only to samples drawn from them. Maximum Mean Discrepancy (MMD) is a kernel-based\nnonparametric statistic proposed for this task [1, 2]. When equipped with a characteristic kernel, MMD defines a metric\non the space of probability distributions, and thus equals zero if and only if the two distributions are identical [3].\nThe asymptotic behavior of the unbiased MMD statistic is, however, non-trivial. Under the null hypothesis, the statistic\nis a degenerate U-statistic, with a limiting distribution given by an infinite weighted sum of chi-square random variables.\nUnder the alternative hypothesis, it converges to a Gaussian distribution [2, 4, 5]. In practice, sample sizes from the two\ndistributions often differ. Existing work has addressed various special cases: Gretton et al. [2] provided the variance\nunder the null hypothesis with equal sample sizes; Sutherland and Deka [6] characterized the unbiased MMD estimator\nas a U-statistic under balanced sampling; and Wei et al. [7] proposed a method under the alternative hypothesis with\nunequal sample sizes, treating the MMD estimator as a unified generalized U-statistic [8, 9]. In this context, a unified\nformulation capable of providing unbiased variance estimation under both hypotheses while accommodating unequal\nsample sizes remains of significant theoretical and practical interest.\narXiv:2601.13874v1  [stat.ML]  20 Jan 2026\nA PREPRINT - JANUARY 21, 2026\nTo address these limitations, our approach employs the Hoeffding decomposition to explicitly construct orthogonal\nprojection terms, providing a streamlined representation of the MMD variance estimator. This formulation implicitly\nincorporates the covariance components present in [7], while simplifying the overall structure. The orthogonal\nstructure further enables a unified and smooth modeling of the variance transition across the null and alternative\nhypotheses. Beyond its theoretical elegance, the unbiased estimator\u2019s algebraic clarity facilitates a concise, tensorized\nimplementation in modern deep learning frameworks [10, 11], offering a practical and interpretable objective for kernel\noptimization aimed at maximizing test power [12, 13, 14, 15].\nBeyond the theoretical perspective, accurately estimating the MMD variance poses significant computational challenges,\nas naive summation incurs quadratic complexity, straining both memory and computational resources and limiting the\npractical deployment of recent theoretical advances [16]. Existing acceleration strategies exhibit clear trade-offs: global\napproximation methods, such as subsampling or random feature projections, scale well to high-dimensional data but\nintroduce inherent approximation errors [17, 18]. Analytical acceleration offers an alternative, restructuring independent\nsummations into ordered linear recursions [19, 20], a strategy previously applied in classical statistics for measures\nlike Kendall\u2019s tau and distance covariance to reduce complexity from O(n2) to O(n log n) [21, 22]. In the MMD\ncontext, Bodenham and Kawahara [23] leveraged the semigroup property of the Laplace kernel to achieve linear-time\nacceleration of the MMD mean (euMMD) for univariate settings. However, this computational advantage has not yet\nbeen extended to unbiased variance estimation, where handling complex second-order terms, including Frobenius\nnorms of kernel matrices, norms of row-sum vectors, and cross-matrix interactions, prevents existing linear-recursion\nschemes from providing exact results [6].\nTo bridge this gap, we exploit the analytical properties of the Laplace kernel [22, 23] to reformulate all terms in the\nvariance expression as a system of linear prefix sums. This reformulation enables exact computation of the unbiased\nMMD variance with quasi-linear time complexity O(n log n).\nThe remainder of this paper is organized as follows. Section 2 reviews the necessary background, including the\ndefinition of MMD, U-statistics, and the Hoeffding decomposition, as well as existing unbiased MMD acceleration\ntechniques based on sorting methods. Section 3 derives the variance expression via the Hoeffding decomposition and\npresents the corresponding unbiased estimator. Section 4 introduces an accelerated variance computation method based\non the Laplace kernel and the sorting methods. Finally, Section 5 presents numerical experiments demonstrating the\nproposed method\u2019s performance.\n2\nBackgroud\n2.1\nMaximum Mean Discrepancy\nAssume X, X\u2032\u223cP and Y , Y \u2032\u223cQ are independent copies. The maximum mean discrepancy is defined as ([2, 16]):\nMMD2 (X, Y ) = EX,X\u2032 [k (X, X\u2032)] + EY ,Y \u2032 [k (Y , Y \u2032)] \u22122EX,Y [k (X, Y )]\n(1)\nGiven i.i.d. finite samples X = {xi}n\ni=1 and Y = {yj}m\nj=1 from P and Q, respectively, an unbiased estimator of the\nsquared MMD is given by ([1]):\n\\\nMMD\n2(X, Y) =\n1\nn(n \u22121)\nX\ni\u0338=i\u2032\nk(xi, xi\u2032) +\n1\nm(m \u22121)\nX\nj\u0338=j\u2032\nk(yj, yj\u2032) \u2212\n2\nnm\nX\ni,j\nk(xi, yj)\n(2)\nwhich is a linear combination of two single-sample U-statistics and one generalized U-statistic ([5]).\nThe effectiveness of MMD hinges on the use of characteristic kernels, for which the kernel mean embedding uniquely\nrepresents probability distributions in the associated reproducing kernel Hilbert space (RKHS) ([3, 24]). Under this\ncondition, MMD induces a proper metric on distributions, ensuring that any difference between them is detectable\n([25, 26]).\nIn practice, translation-invariant kernels are particularly appealing due to their simple analytic form and favorable\ncomputational properties, as characterized by Bochner\u2019s theorem. Two commonly used examples are the Gaussian\nkernel and the Laplace kernel\nkG(x, x\u2032; \u03c3) = exp\n\u0000\u2212(x \u2212x\u2032)2/2\u03c32\u0001\n(3)\nkL(x, x\u2032; \u03c3) = exp (\u2212|x \u2212x\u2032|/\u03c3)\n(4)\nwhere the kernel parameter \u03c3 controls the scale of the feature space and directly influences the test power. A common\nchoice is the median heuristic[27], where \u03c3 is set to the median of all pairwise sample distances. This approach requires\nno prior knowledge of the underlying distributions and is widely regarded as a standard baseline in the literature.\n2\nA PREPRINT - JANUARY 21, 2026\nFinally, although \\\nMMD\n2 enjoys favorable theoretical properties, its computation relies on the explicit construction of\nkernel matrices, resulting in a computational complexity of O(d(n + m)2). Letting N = n + m, this complexity can be\nwritten as O(dN 2)([23]). In large-scale datasets or scenarios where moderate sample sizes require repeated evaluations,\nsuch quadratic computational cost severely limits the practical applicability of MMD-based statistical inference.\n2.2\nU-statistics and Hoeffding Decomposition\nDepending on the sample source, U-statistics can be classified into single-sample U-statistics and generalized U-statistics.\nBoth types form the theoretical foundation of the MMD estimator ([5]).\nLet finite samples be X = {xi}n\ni=1 i.i.d. from X \u223cP and Y = {yj}m\nj=1 i.i.d. from Y \u223cQ, where X and Y are\nindependent. Consider a function \u03b8(X) = EX[h(x1, . . . , xr)], whose unbiased estimator is\nUn =\n\u0012n\nr\n\u0013\u22121\nX\n1\u2264i1<\u00b7\u00b7\u00b7<ir\u2264n\nh(xi1, . . . , xir),\n(5)\nwhere r denotes the order of the U-statistic and n the sample size.\nDefine the conditional expectation projections as\nhc(xi1, . . . , xic) =\nZ\nh(xi1, . . . , xic, xic+1, . . . , xir) dP(xic+1) \u00b7 \u00b7 \u00b7 dP(xir),\n1 \u2264c \u2264r.\n(6)\nand \u02dch(xi1, . . . , xir) = h(xi1, . . . , xir) \u2212\u03b8(X) is the centered kernel .\nThe orthogonal projection components gc are recursively defined by ([28, 29]):\ng1(xi1) = \u02dch1(xi1),\ng2(xi1, xi2) = \u02dch2(xi1, xi2) \u2212g1(xi1) \u2212g1(xi2),\ngc(xi1, . . . , xic) = \u02dchc(xi1, . . . , xic) \u2212\nc\u22121\nX\ns=1\nX\n1\u2264i1<\u00b7\u00b7\u00b7<is\u2264c\ngs(xi1, . . . , xis),\nc \u22653.\n(7)\nThese components satisfy the zero-mean property: EX[gc(xi1, . . . , xic)] = 0.\nDefine Sc,n = P\n1\u2264i1<\u00b7\u00b7\u00b7<ic\u2264n gc(xi1, . . . , xic), then the Hoeffding decomposition yields by ([30]):\nUn \u2212\u03b8(X) =\nr\nX\nc=1\n\u0012r\nc\n\u0013\u0012n\nc\n\u0013\u22121\nSc,n.\n(8)\nFor parameters involving interactions between two distributions \u03b8(X, Y ) = E[h(x1, . . . , xr1; y1, . . . , yr2)], the corre-\nsponding generalized U-statistic is defined as\nUn,m =\n1\n\u0000 n\nr1\n\u0001\u0000m\nr2\n\u0001\nX\n1\u2264i1<\u00b7\u00b7\u00b7<ir1\u2264n\nX\n1\u2264j1<\u00b7\u00b7\u00b7<jr2\u2264m\nh(xi1, . . . , xir1; yj1, . . . , yjr2),\n(9)\nwhere r1 and r2 denote the orders of the U-statistic for the two samples, respectively.\nFor the generalized U-statistic Un,m defined in Eq. 9, its asymptotic variance are discussed in [31]. Following the\nrecursive logic of the single-sample case, we specify the orthogonal projection components gc1,c2 for the generalized\ncase as follows.\nLet \u02dch = h \u2212\u03b8(X, Y ) be the centered kernel. For 0 \u2264c1 \u2264r1 and 0 \u2264c2 \u2264r2, we define the joint conditional\nexpectation projections as:\n\u02dchc1,c2(xi1, . . . , xic1; yj1, . . . , yjc2) = E[\u02dch | xi1, . . . , xic1, yj1, . . . , yjc2].\n(10)\nThe orthogonal projection components gc1,c2 are recursively defined as:\ngc1,c2(xi1, . . . , xic1; yj1, . . . , yjc2) = \u02dchc1,c2 \u2212\nX\n0\u2264s1\u2264c1\n0\u2264s2\u2264c2\n0<s1+s2<c1+c2\nX\nSs1,s2\ngs1,s2(\u00b7),\n(11)\n3\nA PREPRINT - JANUARY 21, 2026\nwhere Ss1,s2 represents all subsets of size s1 from {xi1, . . . , xic1} and size s2 from {yj1, . . . , yjc2}. These components\nsatisfy E[gc1,c2] = 0 for all c1 + c2 > 0.\nDefine the partial sums as:\nSc1,c2,n,m =\nX\n1\u2264i1<\u00b7\u00b7\u00b7<ic1\u2264n\nX\n1\u2264j1<\u00b7\u00b7\u00b7<jc2\u2264m\ngc1,c2(xi1, . . . , xic1; yj1, . . . , yjc2).\n(12)\nThe generalized Hoeffding decomposition is then expressed as:\nUn,m \u2212\u03b8(X, Y ) =\nX\nc1=0,...,r1\nc2=0,...,r2\n(c1,c2)\u0338=(0,0)\n\u0012r1\nc1\n\u0013\u0012r2\nc2\n\u0013\u0012n\nc1\n\u0013\u22121\u0012m\nc2\n\u0013\u22121\nSc1,c2,n,m.\n(13)\n2.3\nLossless acceleration of the unbiased MMD estimator with the Laplace kernel\nFor univariate samples, directly computing Eq. 2 requires O(N 2) computational complexity. Based on several lemmas\nfor the Laplacian kernel presented in Appendix A, Bodenham and Kawahara[23] proposed a lossless acceleration\nstrategy that exploits the ordering structure of the samples, reducing the complexity to O(N log N). The procedure is\nsummarized as follows.\nFirst, by applying Lemmas 1 and 2 in Appendix A, Eq. 2 can be rewritten in the following form:\n\\\nMMD\n2\neu (X, Y) = 2 (\u03b11T1 + \u03b12T2 \u2212\u03b13T3)\n(14)\nwhere the coefficients are defined as \u03b11 = 1\n\u000e\n[n (n \u22121)], \u03b12 = 1\n\u000e\n[m (m \u22121)], \u03b13 = 1\n\u000e\n[nm]. The summation\nterms correspond to the lower triangular parts of the kernel matrices and are given by T1 = Pn\ni=2\nPi\u22121\ni\u2032=1 k (xi, xi\u2032),\nT2 = Pm\nj=2\nPj\u22121\nj\u2032=1 k (yj, yj\u2032), T3 = Pn\ni=1\nPm\nj=1 k (xi, yj).\nTo compute T1, the sample sequence X is first sorted in ascending order, i.e., x1 \u2264x2 \u2264\u00b7 \u00b7 \u00b7 \u2264xn. Define the\nforward cumulative sum RX\ni\n= Pi\u22121\ni\u2032=1 k (xi, xi\u2032) and the exponential difference between adjacent samples DX\ni\n=\nexp (\u2212(xi \u2212xi\u22121)/\u03c3). According to Proposition 4 in the appendix A, the following recurrence relation holds:\nRX\ni =\n\u0000RX\ni\u22121 + 1\n\u0001\n\u00b7 DX\ni .\n(15)\nAs a result, T1 can be computed in linear time as\nT1 =\nn\nX\ni=2\nRX\ni .\n(16)\nSimilarly, for T2, sorting Y in ascending order y1 \u2264y2 \u2264\u00b7 \u00b7 \u00b7 \u2264ym yields\nT2 =\nm\nX\nj=2\nRY\ni .\n(17)\nFor the cross-sample term T3, a merged-set strategy is adopted. Let Z = X \u222aY be the combined sample set, and sort it\nas z1 \u2264z2 \u2264\u00b7 \u00b7 \u00b7 \u2264zn+m. Define the total lower-triangular sum over the merged set as T4 = Pn+m\nk=2\nPk\u22121\nk\u2032=1 k (zk, zk\u2032).\nBy set decomposition, T4 contains all terms in T1, T2, and T3, and thus T3 = T4 \u2212T1 \u2212T2.\nThe quantity T4 can be efficiently computed using the same linear recurrence RZ\nk =\n\u0000RZ\nk\u22121 + 1\n\u0001\n\u00b7 DZ\nk , which gives\nT4 =\nn+m\nX\nk=2\nRZ\nk .\n(18)\nThe core logic of the above procedure is summarized in Algorithm 1, which achieves a computational complexity of\nO(n log n).\n4\nA PREPRINT - JANUARY 21, 2026\nAlgorithm 1 TriSSL: Triangular Sorted Sum of Lapliacians\nRequire: Ordered sample sequence S = {s1, . . . , sl}, kernel parameter \u03c3\nEnsure: Lower-triangular summation result T\n1: R1 \u21d00; T \u21d00\n2: for i = 2 to l do\n3:\nDi \u21d0exp\n\u0000\u2212(si \u2212si\u22121)/\u03c3\n\u0001\n4:\nRi \u21d0(Ri\u22121 + 1) \u00b7 Di\n5:\nT \u21d0T + Ri\n6: end for\n7: return T\n3\nUnbiased estimator of Maximum Mean Discrepancy variance\n3.1\nVariance derivation of the unbiased MMD estimator\nBased on the theory of U-statistics and the Hoeffding decomposition presented in Section 2.2, this subsection provides\na rigorous derivation of the variance of the unbiased estimator \\\nMMD\n2.\nFirst, we define the components in Eq. 1 as\n\u03b8X := EX,X\u2032 [k (X, X\u2032)] ; \u03b8Y := EY ,Y \u2032 [k (Y , Y \u2032)] ; \u03b8XY := EX,Y [k (X, Y )] .\nCorrespondingly, the components in Eq. 2 are denoted as\nA :=\n1\nn(n \u22121)\nX\ni\u0338=i\u2032\nk(xi, xi\u2032); B :=\n1\nm(m \u22121)\nX\nj\u0338=j\u2032\nk(yj, yj\u2032); C :=\n1\nnm\nX\ni,j\nk(xi, yj).\nAccording to the Hoeffding decomposition theorem (Eq. 8 and 13), the deviation of each component can be expanded\ninto a sum of linear terms and second-order residual terms.\nFor the within-X component,\nA \u2212\u03b8X = 2\nn\nn\nX\ni=1\ng1,A (xi) +\n2\nn (n \u22121)\nX\n1\u2264i<i\u2032\u2264n\ng2,A (xi, xi\u2032)\n(19)\nwhere g1,A (xi) = EX\u2032 [k (xi, X\u2032)] \u2212\u03b8X, g2,A (xi, xi\u2032) = k (xi, xi\u2032) \u2212g1,A (xi) \u2212g1,A (xi\u2032) \u2212\u03b8X.\nSimilarly, for the within-Y component,\nB \u2212\u03b8Y = 2\nm\nm\nX\nj=1\ng1,B (yj) +\n2\nm (m \u22121)\nX\n1\u2264j<j\u2032\u2264m\ng2,B (yj, yj\u2032)\n(20)\nwhere g1,B (yj) = EY \u2032 \u0002\nk\n\u0000yj, Y \u2032\u0001\u0003\n\u2212\u03b8Y , g2,B (yj, yj\u2032) = k (yj, yj\u2032) \u2212g1,B (yj) \u2212g1,B (yj\u2032) \u2212\u03b8Y .\nFor the cross-sample component,\nC \u2212\u03b8XY = 1\nn\nn\nX\ni=1\ng1,CX (xi) + 1\nm\nm\nX\nj=1\ng1,CY (yj) +\n1\nnm\nn\nX\ni=1\nm\nX\nj=1\ng2,C (xi, yj)\n(21)\nwhere g1,CX (xi) = EY (k (xi, Y )) \u2212\u03b8XY , g1,CY (yj) = EX (k (X, yj)) \u2212\u03b8XY , g2,C (xi, yj) = k (xi, yj) \u2212\ng1,CX (xi) \u2212g1,CY (yj) \u2212\u03b8XY .\nUnlike the approach in [7], which directly applies the general properties of generalized U-statistics by treating the entire\nMMD as a single monolithic entity, we leverage the natural form of the MMD estimator as a combination of distinct\nU-statistic components. By combining Eq. 19, 20, and 21, the deviation of the unbiased estimator can be expressed as:\n\\\nMMD\n2 \u2212MMD2 = T1 + T2\n(22)\nhere, T1 denotes the sum of first-order projection terms,\nT1 = 2\nn\nn\nX\ni=1\n(U (xi) \u2212E [U (xi)]) + 2\nm\nm\nX\nj=1\n(V (yj) \u2212E [V (yj)])\n(23)\n5\nA PREPRINT - JANUARY 21, 2026\nwhere the influence functions are defined as\nU (xi) := EX\u2032 \u0002\nk\n\u0000xi, X\u2032\u0001\u0003\n\u2212EY [k (xi, Y )]\n(24)\nV (yj) := EY \u2032 \u0002\nk\n\u0000yj, Y \u2032\u0001\u0003\n\u2212EX [k (X, yj)]\n(25)\nThe term T2 corresponds to the second-order residual terms,\nT2 =\n2\nn (n \u22121)\u03b62,A +\n2\nm (m \u22121)\u03b62,B \u2212\n2\nnm\u03b62,C\n(26)\nwhere \u03b62,A = P\n1\u2264i<i\u2032\u2264n g2,A (xi, xi\u2032), \u03b62,B = P\n1\u2264j<j\u2032\u2264m g2,B (yj, yj\u2032), \u03b62,C = Pn\ni=1\nPm\nj=1 g2,C (xi, yj).\nSince different orders in the Hoeffding decomposition are orthogonal, i.e., Cov(H1, H2) = 0 ([30]), the variance of\n\\\nMMD\n2 admits the exact decomposition\nVar\n\u0010\n\\\nMMD\n2\u0011\n= Var (T1) + Var (T2)\n(27)\nwhere the variance of the linear term Var(T1) captures the dominant fluctuations of the sample means, while the\nvariance of the remainder term Var(T2) characterizes the second-order interactions induced by the kernel.\n3.2\nSimplification and unbiased estimation of the variance\n3.2.1\nSimplification and unbiased estimation of first-order projection terms\nTo translate the theoretical decomposition in Eq. 27 into a practical and computable form, we leverage the fundamental\nindependence between the two sample sets, and the orthogonal representation allows for a decoupled estimation of the\nvariance components. Specifically, for the first-order term T1, the mutual independence of X and Y ensures that the\ncovariance between the sample-specific fluctuations vanishes. Consequently, the variance of the linear term simplifies\nto the weighted sum of marginal variances:\nd\nVar(T1) = 4(n \u22122)\nn(n \u22121)\nd\nVar(U) + 4(m \u22122)\nm(m \u22121)\nd\nVar(V),\n(28)\nwhere d\nVar(U) and d\nVar(V) represent the population variances of the influence functions for distributions P and\nQ, respectively. To realize this empirically, we replace the continuous conditional expectations with their discrete\nsample-based counterparts:\nFor a sample point xi \u2208X, the continuous projection U(xi) is discretized using sample averages as\n\u02c6U(xi) =\n1\nn \u22121\nX\ni\u2032\u0338=i\nk(xi, xi\u2032) \u22121\nm\nm\nX\nj=1\nk(xi, yj)\n(29)\nThe unbiased estimator of the variance Var(U(X)) is therefore given by the sample variance:\nd\nVar(U) =\n1\nn \u22121\nn\nX\ni=1\n\u0010\n\u02c6U(xi) \u2212\u00af\u02c6U\n\u00112\n(30)\nwhere \u00af\u02c6U = Pn\ni=1 \u02c6U(xi)\n\u000e\nn and an analogous estimator can be obtained for \u02c6U(xi) and d\nVar(V):\n\u02c6V(yj) =\n1\nm \u22121\nX\nj\u2032\u0338=j\nk(yj, yj\u2032) \u22121\nn\nm\nX\ni=1\nk(xi, yj)\n(31)\nd\nVar(V) =\n1\nm \u22121\nm\nX\nj=1\n\u0010\n\u02c6V(yj) \u2212\u00af\u02c6V\n\u00112\n(32)\nwhere we avoid the appearance of covariance terms (which appear in [7]) by computing the variance via continuous\nprojections, thereby simplifying the structure of the unbiased estimator.\n6\nA PREPRINT - JANUARY 21, 2026\n3.2.2\nSimplification and unbiased estimation of second-order residual terms\nParallel to the treatment of the linear term, the variance of the second-order remainder T2 can be simplified by exploiting\nthe mutual orthogonality of the g2 components across different sample pairs. Since g2,A, g2,B, and g2,C are constructed\nto be zero-mean and mutually uncorrelated under the independent sampling of X and Y , the variance Var(T2) avoids\nany cross-product terms. This allows us to expand the second-order variance into a weighted sum of the dispersion of\neach interaction kernel:\nVar(T2) = Var\n\u0012\n2\u03b62,A\nn(n \u22121)\n\u0013\n+ Var\n\u0012\n2\u03b62,B\nm(m \u22121)\n\u0013\n+ Var\n\u00122\u03b62,C\nnm\n\u0013\n(33)\nAs an illustrative example for the within-sample component A, consider the second-order sum \u03b62,A\n=\nP\n1\u2264i<i\u2032\u2264n g2,A(xi, xi\u2032). Since g2,A is a centered orthogonal projection component, for any two distinct index\npairs i, i\u2032 and j, j\u2032, if the pairs do not coincide exactly, then Cov(g2,A(xi, xi\u2032), g2,A(xj, xj\u2032)) = 0. As a consequence,\nthe variance of the sum equals the sum of the variances of the individual terms:\nVar\n\u0012\n2\u03b62,A\nn(n \u22121)\n\u0013\n=\n4\nn2(n \u22121)2 \u00b7\n\u0012n\n2\n\u0013\nVar(g2,A)\n=\n4\nn2(n \u22121)2 \u00b7 n(n \u22121)\n2\nE[g2\n2,A] =\n2\nn(n \u22121)E[g2\n2,A]\n(34)\nSimilarly, for the cross-sample mixed component C, since xi and yj are independent, the covariance is non-zero only\nwhen the indices coincide exactly:\nVar\n\u00122\u03b62,C\nnm\n\u0013\n=\n4\nn2m2 \u00b7 (nm)E[g2\n2,C] =\n4\nnmE[g2\n2,C]\n(35)\nAt this stage, Eq. 33 can be reformulated as\nVar(T2) =\n2\nn(n \u22121)E[g2\n2,A] +\n2\nm(m \u22121)E[g2\n2,B] +\n4\nnmE[g2\n2,C]\n(36)\nTo further simplify, Consider the orthogonality of first-order components (Section 2.2) and the nesting property\nof conditional expectations, E[Z \u00b7 E[Z|X]] = E[(E[Z|X])2], which allows the squared second-order orthogonal\ncomponents to be simplified. Taking g2,A as an example, where g2,A(x, x\u2032) = k(x, x\u2032) \u2212g1,A(x) \u2212g1,A(x\u2032) \u2212\u03b8X.\nThe expectation of the squared term and expanding yields\nE[g2\n2,A] = E[k(x, x\u2032)2] + E[g1,A(x)2] + E[g1,A(x\u2032)2] + \u03b82\nX\n(37)\n\u22122E[k(x, x\u2032) \u00b7 g1,A(x)] \u22122E[k(x, x\u2032) \u00b7 g1,A(x\u2032)] \u22122\u03b8XE[k(x, x\u2032)]\n(38)\n+ 2E[g1,A(x)g1,A(x\u2032)] \u22122\u03b8XE[g1,A(x)] \u22122\u03b8XE[g1,A(x\u2032)]\n(39)\nUsing the facts that E[g1,A(x)] = 0, E[k(x, x\u2032) \u00b7 g1,A(x)] = E[g1,A(x)2], and E[g1,A(x)g1,A(x\u2032)] = 0, the above\nexpression simplifies to\nE[g2\n2,A] = E[k(x, x\u2032)2] \u22122EX\nh\n(EX\u2032[k(X, X\u2032)])2i\n+ \u03b82\nX,\n(40)\nanalogous derivation can be obtained for E[g2\n2,B]:\nE[g2\n2,B] = E[k(y, y\u2032)2] \u22122EY\nh\n(EY \u2032[k(Y , Y \u2032)])2i\n+ \u03b82\nY\n(41)\nConsider the mixed component g2,C(x, y) = k(x, y) \u2212g1,CX(x) \u2212g1,CY (y) \u2212\u03b8XY , which arises from the interaction\nbetween samples X and Y. Since P and Q are independent, the first-order projection terms g1,CX(x) and g1,CY (y) are\nalso independent. Expanding the squared term and eliminating zero-mean cross terms yields\nE[g2\n2,C] = E[k(x, y)2] + E[g2\n1,CX] + E[g2\n1,CY ] + \u03b82\nXY\n\u22122E[k(x, y)g1,CX(x)] \u22122E[k(x, y)g1,CY (y)] + 2\u03b8XY E[k(x, y)]\n(42)\nUsing again the projection property E[k(x, y)g1,CX(x)] = E[g2\n1,CX + \u03b8XY g1,CX] = E[g2\n1,CX], the expression\nsimplifies to\nE[g2\n2,C] = E[k(x, y)2] \u2212EX\nh\n(EY [k(X, Y )])2i\n\u2212EY\nh\n(EX[k(X, Y )])2i\n+ \u03b82\nXY\n(43)\n7\nA PREPRINT - JANUARY 21, 2026\nBy applying the inclusion-exclusion principle, the unbiased estimator can be expressed entirely in terms of unconstrained\nmatrix operations ([6]). Specifically, using the falling factorial (n)k and the centered kernel matrices \u02dcKXX and \u02dcKY Y\n(with zero diagonals), the expectation E[g2\n2] can be written as a linear combination of the squared Frobenius norms, the\nsquared row and column sums, and the squared total sum of the kernel matrices.\n\\\nE[g2\n2,A] =\n1\n(n)2\n\u2225\u02dcKXX\u22252\nF \u2212\n2\n(n)3\n\u0010\n\u2225\u02dcKXX1\u22252\n2 \u2212\u2225\u02dcKXX\u22252\nF\n\u0011\n+\n1\n(n)4\n\u0010\n(1\u22a4\u02dcKXX1)2 \u22124\u2225\u02dcKXX1\u22252\n2 + 2\u2225\u02dcKXX\u22252\nF\n\u0011\n(44)\n\\\nE[g2\n2,B] =\n1\n(m)2\n\u2225\u02dcKY Y \u22252\nF \u2212\n2\n(m)3\n\u0010\n\u2225\u02dcKY Y 1\u22252\n2 \u2212\u2225\u02dcKY Y \u22252\nF\n\u0011\n+\n1\n(m)4\n\u0010\n(1\u22a4\u02dcKY Y 1)2 \u22124\u2225\u02dcKY Y 1\u22252\n2 + 2\u2225\u02dcKY Y \u22252\nF\n\u0011\n(45)\n\\\nE[g2\n2,C] =\n1\nnm\u2225KXY \u22252\nF \u2212\n1\nn(m)2\n\u0000\u2225K\u22a4\nXY 1n\u22252\n2 \u2212\u2225KXY \u22252\nF\n\u0001\n\u2212\n1\nm(n)2\n\u0000\u2225KXY 1m\u22252\n2 \u2212\u2225KXY \u22252\nF\n\u0001\n+\n1\n(n)2(m)2\n\u0000(1\u22a4\nn KXY 1m)2 \u2212\u2225KXY 1m\u22252\n2 \u2212\u2225K\u22a4\nXY 1n\u22252\n2 + \u2225KXY \u22252\nF\n\u0001\n(46)\n3.2.3\nThe unbiased estimator of MMD variance\nUsing the simplification results provided in Eq. 30, 32, 44, 45, and 46, the full analytical expression of Var(\\\nMMD\n2) is\ngiven by\nd\nVar\n\u0010\n\\\nMMD\n2\u0011\n= 4(n \u22122)\nn(n \u22121)\nd\nVar(U) + 4(m \u22122)\nm(m \u22121)\nd\nVar(V)\n+\n2\nn (n \u22121)\n\\\nE[g2\n2,A] +\n2\nm (m \u22121)\n\\\nE[g2\n2,B] +\n4\nnm\n\\\nE[g2\n2,C]\n= 4(n \u22122)\nn(n \u22121)2\nn\nX\ni=1\n\u0010\n\u02c6U(xi) \u2212\u00af\u02c6U\n\u00112\n+ 4(m \u22122)\nm(m \u22121)2\nm\nX\nj=1\n\u0010\n\u02c6V(yj) \u2212\u00af\u02c6V\n\u00112\n+\n2\n(n)2\n2\n\u2225\u02dcKXX\u22252\nF +\n2\n(m)2\n2\n\u2225\u02dcKY Y \u22252\nF +\n4\nn2m2 \u2225KXY \u22252\nF\n\u2212\n4\n(n)2(n)3\n\u0010\n\u2225\u02dcKXX1\u22252\n2 \u2212\u2225\u02dcKXX\u22252\nF\n\u0011\n\u2212\n4\n(m)2(m)3\n\u0010\n\u2225\u02dcKY Y 1\u22252\n2 \u2212\u2225\u02dcKY Y \u22252\nF\n\u0011\n\u2212\n4\nn2m(m)2\n\u0000\u2225K\u22a4\nXY 1n\u22252\n2 \u2212\u2225KXY \u22252\nF\n\u0001\n\u2212\n4\nm2n(n)2\n\u0000\u2225KXY 1m\u22252\n2 \u2212\u2225KXY \u22252\nF\n\u0001\n+\n2\n(n)2(n)4\n\u0010\n(1\u22a4\u02dcKXX1)2 \u22124\u2225\u02dcKXX1\u22252\n2 + 2\u2225\u02dcKXX\u22252\nF\n\u0011\n+\n2\n(m)2(m)4\n\u0010\n(1\u22a4\u02dcKY Y 1)2 \u22124\u2225\u02dcKY Y 1\u22252\n2 + 2\u2225\u02dcKY Y \u22252\nF\n\u0011\n+\n4\nn(n)2m(m)2\n\u0000(1\u22a4\nn KXY 1m)2 \u2212\u2225KXY 1m\u22252\n2 \u2212\u2225K\u22a4\nXY 1n\u22252\n2 + \u2225KXY \u22252\nF\n\u0001\n(47)\nRemark 1. Under the null hypothesis H0 : P = Q, the unbiased MMD statistic exhibits a pronounced \"degenerate\"\nbehavior. According to Eq. (47), its theoretical variance decomposes into a sum of the first-order projection term T1\nand the second-order residual term T2. For T1,\nU (x) := EX\u2032 \u0002\nk\n\u0000x, X\u2032\u0001\u0003\n\u2212EY [k (x, Y )] \u22610\n(48)\nV (y) := EY \u2032 \u0002\nk\n\u0000y, Y \u2032\u0001\u0003\n\u2212EX [k (X, y)] \u22610.\n(49)\nConsequently Var(T1) \u22610, and the population variance of the first-order term T1 drops from O(n\u22121) to O(n\u22122)([2,\n7]).\nHowever, it is important to emphasize that this theoretical degeneracy does not imply the disappearance of empirical\nprojection fluctuations. In finite sample observations, we compute the empirical projection bU(xi) and bV(yj) based\non sample averages (Eq. 29 and 31). Due to cross-coupling effects among samples, the empirical projection retains\nsubstantial variance. As a result, the estimator in Eq. ?? is generally larger than the one in [2].\n8\nA PREPRINT - JANUARY 21, 2026\n4\nAccelerated computation of Maximum Mean Discrepancy\n4.1\nSeveral Propositions and Lemma for acceleration\nUnder the Laplace kernel kL(x, x\u2032) = exp(\u2212|x \u2212x\u2032|/\u03c3) and the Lemmas in the Appendix A, the following Propositions\nand Lemma hold.\nProposition 1. Let {x1, . . . , xn} \u2282R be sorted such that x1 \u2264x2 \u2264\u00b7 \u00b7 \u00b7 \u2264xn, and consider the Laplacian kernel.\nDefine R1 = Ln = 0. For i = 2, . . . , n and i = n \u22121, . . . , 1, respectively, let\nDi := exp\n\u0000\u2212[xi \u2212xi\u22121]\n\u000e\n\u03c3\n\u0001\n,\nRi := (Ri\u22121 + 1) Di,\nLi := (Li+1 + 1) Di+1.\n(50)\nThen, for each i = 1, . . . , n, the off-diagonal row sum of the Laplace kernel matrix satisfies\nX\nj\u0338=i\nkL(xi, xj) = Ri + Li.\n(51)\nAlgorithm 2 give the prefix-suffix accumulators for Laplace kernel.\nAlgorithm 2 Prefix-suffix accumulators for Laplace kernel\nRequire: Ordered sample sequence S = {s1, . . . , sl}, kernel parameter \u03c3\nEnsure: Prefix accumulators {Ri}l\ni=1 and suffix accumulators {Li}l\ni=1\n1: R1 \u21d00; Ll \u21d00\n2: for i = 2 to l do\n3:\nDi \u21d0exp\n\u0000\u2212(si \u2212si\u22121)/\u03c3\n\u0001\n4:\nRi \u21d0(Ri\u22121 + 1) \u00b7 Di\n5: end for\n6: for i = l \u22121 downto 1 do\n7:\nDi+1 \u21d0exp\n\u0000\u2212(si+1 \u2212si)/\u03c3\n\u0001\n8:\nLi \u21d0(Li+1 + 1) \u00b7 Di+1\n9: end for\n10: return {Ri}l\ni=1, {Li}l\ni=1\nProposition 2. Let x1 \u2264\u00b7 \u00b7 \u00b7 \u2264xn and y1 \u2264\u00b7 \u00b7 \u00b7 \u2264ym be two sorted samples. Let Z = {z1, . . . , zn+m} denote their\nmerged sorted sequence, and let \u2113(k) \u2208{X, Y} indicate the origin of zk. Define \u2206k = zk \u2212zk\u22121 for k \u22652, and\nconsider the Laplacian kernel. Define the prefix accumulator Ax\u2192y(k) recursively by\nAx\u2192y(1) = 0,\n(52)\nAx\u2192y(k) = exp\n\u0000\u2212\u2206k\n\u000e\n\u03c3\n\u0001\nAx\u2192y(k \u22121) + I{\u2113(k) = Y},\n(53)\nwhere I{\u00b7} denotes the indicator function.\nSimilarly, define the suffix accumulator Zx\u2192y(k) by\nZx\u2192y(n + m) = 0,\n(54)\nZx\u2192y(k) = exp\n\u0000\u2212\u2206k+1\n\u000e\n\u03c3\n\u0001\nZx\u2192y(k + 1) + I{\u2113(k) = Y},\n(55)\nfor k = n + m \u22121, . . . , 1.\nThen, for any xi corresponding to index k in the merged sequence Z, the i-th row sum of the cross-kernel matrix satisfies\nm\nX\nj=1\nkL(xi, yj) = Ax\u2192y(k) + Zx\u2192y(k).\n(56)\nThe associated algorithm 3 computes the cross-kernel prefix and suffix accumulators for the sequences X and Y.\nRemark 2. When both sequences X and Y are given but not necessarily sorted, the algorithm first sorts each sequence\nindividually. The computational complexity is therefore dominated by these sorting steps:\n9\nA PREPRINT - JANUARY 21, 2026\nAlgorithm 3 Cross-kernel prefix and suffix accumulators\nRequire: Sorted sequences Xs = {x1, . . . , xn}, Ys = {y1, . . . , ym}; kernel parameter \u03c3\nEnsure: Prefix and suffix accumulators: Ax\u2192y, Zx\u2192y, Ay\u2192x, Zy\u2192x\n1: Merge Xs and Ys into sorted sequence Z = {z1, . . . , zn+m}\n2: Record origin of each zk as \u2113(k) \u2208{X, Y}\n3: Compute \u2206k = zk \u2212zk\u22121 for k = 2, . . . , n + m\n4: Ax\u2192y[1] \u21d00, Ay\u2192x[1] \u21d00\n5: Zx\u2192y[n + m] \u21d00, Zy\u2192x[n + m] \u21d00\n6: for k = 2 to n + m do\n7:\nif \u2113(k) = Y then\n8:\nAx\u2192y[k] \u21d0exp(\u2212\u2206k/\u03c3) \u00b7 Ax\u2192y[k \u22121] + 1\n9:\nelse\n10:\nAx\u2192y[k] \u21d0exp(\u2212\u2206k/\u03c3) \u00b7 Ax\u2192y[k \u22121]\n11:\nend if\n12:\nif \u2113(k) = X then\n13:\nAy\u2192x[k] \u21d0exp(\u2212\u2206k/\u03c3) \u00b7 Ay\u2192x[k \u22121] + 1\n14:\nelse\n15:\nAy\u2192x[k] \u21d0exp(\u2212\u2206k/\u03c3) \u00b7 Ay\u2192x[k \u22121]\n16:\nend if\n17: end for\n18: for k = n + m \u22121 downto 1 do\n19:\nif \u2113(k) = Y then\n20:\nZx\u2192y[k] \u21d0exp(\u2212\u2206k+1/\u03c3) \u00b7 Zx\u2192y[k + 1] + 1\n21:\nelse\n22:\nZx\u2192y[k] \u21d0exp(\u2212\u2206k+1/\u03c3) \u00b7 Zx\u2192y[k + 1]\n23:\nend if\n24:\nif \u2113(k) = X then\n25:\nZy\u2192x[k] \u21d0exp(\u2212\u2206k+1/\u03c3) \u00b7 Zy\u2192x[k + 1] + 1\n26:\nelse\n27:\nZy\u2192x[k] \u21d0exp(\u2212\u2206k+1/\u03c3) \u00b7 Zy\u2192x[k + 1]\n28:\nend if\n29: end for\n30: return Ax\u2192y, Zx\u2192y, Ay\u2192x, Zy\u2192x\n\u2022 Sorting X and Y requires O(n log n) and O(m log m) operations, respectively, where n = |X| and m = |Y|.\n\u2022 Once sorted, the merged sequence Z = X \u222aY can be obtained in linear time O(n + m).\n\u2022 The row sums of the X-only and Y-only kernel matrices can be computed in linear time O(n) and O(m)\nusing recursive prefix and suffix accumulators.\n\u2022 The row sums of the cross-kernel matrix are obtained in a single forward and backward sweep over the merged\nsorted sequence, also requiring O(n + m) operations.\n\u2022 Proposition 2 and Eq. 18 handle cross terms differently by explicitly storing per-row sums, but this does not\nchange the overall complexity.\nTherefore, the overall time complexity of euMMD in this setting is O(n log n + m log m), dominated by sorting, while\nall subsequent accumulation and row sum computations remain linear. This is consistent with [23].\nLemma 3. The p-th power of the Laplace kernel is equivalent to a Laplace kernel with bandwidth scaled by a factor of\np. That is,\nkL(x, x\u2032; \u03c3)p = kL(x, x\u2032; \u03c3/p) = exp\n\u0000\u2212p|x \u2212x\u2032|\n\u000e\n\u03c3\n\u0001\n(57)\nProof. By the exponential power rule, [exp (\u2212|x \u2212x\u2032|/\u03c3)]p = exp (\u2212p|x \u2212x\u2032|/\u03c3).\n4.2\nAccelerated computation of variance components\nWe also name our method euMMD, for efficient univariate Maximum Mean Discrepancy [23], and extend it to include\naccelerated variance estimation. Let x1 \u2264\u00b7 \u00b7 \u00b7 \u2264xn and y1 \u2264\u00b7 \u00b7 \u00b7 \u2264ym be sorted samples. Define the prefix and suffix\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper \"UNIFIED UNBIASED VARIANCE ESTIMATION FOR MMD\"\n\n#### Main Contributions:\n- **Unified Formulation**: The paper proposes a unified formulation for unbiased variance estimation under both null and alternative hypotheses, accommodating different sample configurations.\n- **Hoeffding Decomposition Utilization**: It leverages Hoeffding decomposition to explicitly construct orthogonal projection terms, providing a streamlined representation of the MMD variance estimator. This approach simplifies the overall structure while incorporating covariance components present in existing work.\n- **Exact Acceleration for Laplacian Kernel**: The authors introduce an exact acceleration method for the univariate case under the Laplacian kernel, reducing computational complexity from \\(O(n^2)\\) to \\(O(n \\log n)\\).\n\n#### Methodology:\n1. **Hoeffding Decomposition**:\n   - The paper derives the variance expression via Hoeffding decomposition and presents a corresponding unbiased estimator.\n   - It defines orthogonal projection components recursively based on U-statistics, which enables smooth modeling of variance transition across null and alternative hypotheses.\n\n2. **Simplification and Unbiased Estimation**:\n   - For first-order projection terms, it leverages the independence between samples to simplify the variance calculation using discrete sample-based counterparts.\n   - For second-order residual terms, it exploits orthogonality among different sample pairs to avoid cross-product terms and simplifies the variance expression into a weighted sum of interaction kernels.\n\n3. **Laplacian Kernel Acceleration**:\n   - The authors introduce an exact acceleration method for the Laplacian kernel, reducing computational complexity from \\(O(n^2)\\) to \\(O(n \\log n)\\), which is achieved through prefix and suffix accumulators.\n   \n#### Key Findings:\n- The proposed unified formulation provides a comprehensive framework that accommodates different hypotheses and sample configurations, offering significant theoretical and practical benefits.\n- The exact acceleration method for the Laplacian kernel reduces computational complexity from quadratic to linear time, making it more feasible in large-scale datasets or scenarios requiring repeated evaluations.\n\n#### Conclusions:\n- The paper presents a unified and robust approach to unbiased variance estimation under MMD, addressing limitations of existing work by providing a streamlined representation and exact acceleration methods.\n- This unified formulation is particularly valuable for handling imbalanced samples and offers practical benefits in terms of computational efficiency."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "2026-01-21\nWork in progress.\nTwinBrainVLA\nTwinBrainVLA\n: Unleashing the Potential of Generalist VLMs for\nEmbodied Tasks via Asymmetric Mixture-of-Transformers\nBin Yu1,2,* Shijie Lian2,4,* Xiaopeng Lin2,5,* Yuliang Wei1,\u2020 Zhaolong Shen2,6 Changti Wu2,7\nYuzhuo Miao1,2 Xinming Wang2,8 Bailing Wang1 Cong Huang2,3 Kai Chen2,3,9,\u2020\n1HIT\n2ZGCA\n3ZGCI\n4HUST\n5HKUST(GZ)\n6BUAA\n7ECNU\n8CASIA\n9DeepCybo\na https://github.com/ZGC-EmbodyAI/TwinBrainVLA\nAbstract\nStandard Vision-Language-Action (VLA) models typically \ufb01ne-tune a monolithic Vision-\nLanguage Model (VLM) backbone explicitly for robotic control. However, this approach\ncreates a critical tension between maintaining high-level general semantic understanding\nand learning low-level, \ufb01ne-grained sensorimotor skills, often leading to \"catastrophic for-\ngetting\" of the model\u2019s open-world capabilities.\nTo resolve this con\ufb02ict, we introduce\nTwinBrainVLA\n, a novel architecture that coordinates a generalist VLM retaining uni-\nversal semantic understanding and a specialist VLM dedicated to embodied proprioception\nfor joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains\nrobust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied\nperception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This\ndesign allows the Right Brain to dynamically query semantic knowledge from the frozen\nLeft Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-\nMatching Action Expert to generate precise continuous controls. Extensive experiments on\nSimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior\nmanipulation performance compared to state-of-the-art baselines while explicitly preserv-\ning the comprehensive visual understanding capabilities of the pre-trained VLM, offering a\npromising direction for building general-purpose robots that simultaneously achieve high-\nlevel semantic understanding and low-level physical dexterity.\n1\nIntroduction\nThe pursuit of embodied arti\ufb01cial intelligence has recently converged on the paradigm of Vision-\nLanguage-Action (VLA) models (Kim et al., 2024; Intelligence et al., 2025; NVIDIA et al., 2025). By\ngrafting robotic control heads onto pre-trained Vision-Language Models (VLMs), these systems inherit\nthe rich semantic reasoning and open-world generalization capabilities of large-scale internet data. This\nuni\ufb01ed approach promises a robot that can not only recognize a \"coffee mug\" but also understand the\ninstruction \"place the mug on the coaster\" and execute the corresponding motor commands.\nHowever, a fundamental tension lies at the heart of current VLA architectures: the misalignment\nbetween the optimization objectives of semantic pre-training and robotic \ufb01ne-tuning. Pre-trained\nVLMs are originally optimized for general semantic understanding and dialogue. In contrast, during\nVLA \ufb01ne-tuning, the backbone is repurposed to provide precise, spatially-grounded conditioning for the\nAction Expert to enable precise robotic control. Forcing a single VLM backbone to satisfy these diver-\ngent objectives simultaneously inevitably precipitates \"catastrophic forgetting\"(Hancock et al., 2025a;\nZhang et al., 2026; Zhou et al., 2025; Fang et al., 2025), where the model sacri\ufb01ces its general linguistic\ncapabilities to adapt to the specialized demands of robotic actuation. More critically, this catastrophic\n*Equal contribution\n\u2020Corresponding author\nWork done at Zhongguancun Academy (Beijing).\n1\narXiv:2601.14133v1  [cs.RO]  20 Jan 2026\nforgetting undermines the fundamental premise of the VLA paradigm, which aims to leverage the general\nworld understanding acquired during VLM pre-training to enable generalized robotic control. However,\ncurrent \ufb01ne-tuning methodologies paradoxically strip the VLM of this very capability.\nTo resolve this con\ufb02ict, we draw inspiration from the biological principle of hemispheric lateraliza-\ntion, where the human brain allocates distinct cognitive functions to specialized hemisphereslogic and\nlanguage to the left, spatial attention and motor coordination to the right. We propose that VLA mod-\nels should mimic this decoupling. By separating general semantic understanding from embodied action\nestimation, we cY-an achieve a system that is both cognitively capable and physically skillful.\nVLM as Backbone\n(specialist-only)\nLeft\nBrain\n(generalist)\nRight\nBrain\n(specialist)\nAction Expert\nKV\nAction Expert\na) Vanilla VLA\nb) TwinBrainVLA\nimage token\ntext token\nstate info\nFigure 1: Architectural comparison between\nVanilla VLA and TwinBrainVLA.\nIn this paper, we introduce TwinBrainVLA\n, a\nnovel VLA framework that orchestrates two isomorphic\nVLM pathways via an asymmetric dual-stream joint train-\ning strategy. Structurally, our model consists of a \"Left\nBrain\" and a \"Right Brain\". The Left Brain is a frozen,\npre-trained VLM that retains robust open-world knowledge\nand instruction following capabilities. The Right Brain,\ninitialized with the same architecture, is fully trainable and\nspecialized for embodied perception. These two streams\nare seamlessly fused through a Asymmetric Mixture-of-\nTransformers (AsyMoT) mechanism, which allows for\njoint attention over hidden states without sharing param-\neters. Crucially, the specialized representations from the\nRight Brain serve as the primary condition for our action\nexpert utilizing \ufb02ow-matching for precise action genera-\ntion. This design ensures that the action expert receives\nspatially rich, task-aligned embeddings from the Right Brain, while the Left Brain explicitly preserves\nthe model\u2019s general semantic understanding capabilities.\nOur contributions are threefold:\n\u2022 We propose TwinBrainVLA\n, the \ufb01rst VLA architecture to explicitly decouple general seman-\ntic understanding and embodied perception via an asymmetric dual-stream design, resolving the\ntraining con\ufb02ict inherent in single-backbone VLAs.\n\u2022 We introduce the Asymmetric Mixture-of-Transformers (AsyMoT) mechanism for the informa-\ntion interaction of two isomorphic VLM pathways to build the VLA backbone, and employ an\nasymmetric parameter freezing strategy to achieve the joint training of the dual models.\n\u2022 Extensive comparative experiments and evaluations on the SimplerEnv and RoboCasa benchmarks,\ndemonstrates the effectiveness of the TwinBrainVLA architecture, the AsyMoT mechanism, and\nthe proposed training strategy.\n2\nRelated Work\nMultimodal understanding. The landscape of computer vision and natural language processing has\nbeen revolutionized by Vision Language Models (VLMs) (Liu et al., 2023; Bai et al., 2025b,a). By\nseamlessly integrating visual encoders with Large Language Models (LLMs) through sophisticated pro-\njection layers or adapter modules (Jiang et al., 2025a), these models exhibit emergent abilities in holistic\nscene understanding, visual question answering (VQA), and complex math reasoning. Given the in-\nherent limitations of general-purpose VLMs in spatial perception (Xiao et al., 2026; Dang et al., 2025;\nZhang et al., 2025b; Chen et al., 2025; Yang et al., 2025c), recent studies have increasingly focused on\nemploying post-training strategies to enhance spatial intelligence (Yang et al., 2025a; Tan et al., 2025;\nZhou et al., 2026; Zhu et al., 2025; Li et al., 2025) and construct specialized embodied foundation mod-\nels tailored for embodied scenarios (Team et al., 2025a; Yang et al., 2025a; Hao et al., 2025; Lin et al.,\n2025c).\n2\nVision-Language-Action (VLA) Models. Building upon the zero-shot generalization and rich semantic\npriors of VLMs, VLA models have emerged as a scalable paradigm for embodied intelligence (Pertsch\net al., 2025; Zheng et al., 2025a; Wen et al., 2025; Shukor et al., 2025; Zheng et al., 2025b; Lin et al.,\n2025b; Cai et al., 2026). By \ufb01ne-tuning pre-trained VLMs on large-scale robotic manipulation datasets,\nthese models learn to map multimodal instructions and observations to low-level control signals (Jiang\net al., 2025b; Fan et al., 2025; Zhai et al., 2025). To mitigate the degradation of general conversational\nskills during VLA training, recent approaches have integrated general visual-dialogue datasets (Mazza-\nglia et al., 2025; Yang et al., 2025b; Hancock et al., 2025b; Zhou et al., 2025) and reasoning mechanisms\ninto the VLA framework (Zawalski et al., 2025; Yin et al., 2025; Lin et al., 2025a; Qu et al., 2025a; Ye\net al., 2025; Huang et al., 2025; Guo et al., 2025; Lee et al., 2025; Huang et al., 2026).\n3\nMotivation: Catastrophic Forgetting in VLA Models\nStandard Vision-Language-Action (VLA) models typically adopt a transfer learning paradigm. Formally,\nlet a pre-trained Vision-Language Model (VLM) be parameterized by \u03b8pre, which has been aligned on\nmassive internet-scale image-text pairs to maximize the likelihood of generating semantic text responses\ny given an image v and instruction x. To adapt this backbone for robotic manipulation, the model is\n\ufb01ne-tuned on a robotic demonstration dataset Drobot = {(v, x, a)i}N\ni=1, where a represents the robot\u2019s\naction (e.g., end-effector pose or joint angles). The training objective shifts to minimizing the action\nprediction loss Laction:\n\u03b8\u2217= arg min\n\u03b8\n\u2211\n(v,x,a)\u2208Drobot\n\u2212log P(a|v, x; \u03b8),\n(1)\ninitialized with \u03b8 = \u03b8pre.\nWhile this process effectively imparts manipulation skillsenabling the model to control servos and\nexecute tasksit introduces a critical side effect known as catastrophic forgetting. The distribution of\nDrobot differs fundamentally from the diverse visual-linguistic data seen during pre-training (Kachaev\net al., 2025; Hancock et al., 2025b; Zhang et al., 2025a). As the parameters \u03b8 are updated to minimize\nthe high-frequency spatial error in action space, the delicate semantic alignment established in \u03b8pre is\noften overwritten. Consequently, the resulting VLA model suffers from a severe degradation in its orig-\ninal capabilities: it loses the ability to follow complex open-ended instructions, fails to describe visual\nscenes accurately, and exhibits weakened general semantic reasoning. This phenomenon turns the VLM\nbackbone from a \"generalist\" into a \"specialist\", sacri\ufb01cing its linguistic brain to gain a robotic body.\n4\nMethod: TwinBrainVLA\nIn this section, we present TwinBrainVLA\n, a uni\ufb01ed VLA framework designed to disentangle\nhigh-level semantic reasoning from \ufb01ne-grained sensorimotor control. As illustrated in Figure 2, our\narchitecture operates on an asymmetric dual-stream paradigm to resolve the \"catastrophic forgetting\"\ndilemma. The framework comprises three core components: (1) a frozen \"Left Brain\" that preserves\nopen-world visual-linguistic knowledge; (2) a trainable \"Right Brain\" that specializes in embodied mo-\ntor control; and (3) a Flow-Matching Action Expert that generates precise continuous actions. The two\nbrains interact via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism, enabling the\nspecialist to leverage generalist semantics without corrupting them. We detail the Dual-VLM architec-\nture and AsyMoT fusion strategy in Sec. 4.1, describe the \ufb02ow-matching policy formulation in Sec. 4.2,\nand \ufb01nally present our joint-training objective in Sec. 4.3.\n4.1\nAsymmetric Dual-VLM as Backbone\nThe backbone of TwinBrainVLA consists of two parallel Vision-Language Models (VLMs), denoted as\nthe \"Left Brain\" (ML) and the \"Right Brain\" (MR). Both streams are initialized with identical pre-\ntrained weights (e.g., Qwen3-VL series (Bai et al., 2025a)) but serve distinct roles during training.\nDual-Stream Inputs and State Encoding. The input to the system comprises a visual observation I,\na natural language instruction T, and the robot\u2019s proprioceptive state s \u2208Rds (e.g., joint angles and\n3\nLeft Brain\nRight Brain\nGeneralist\nSpecialist for robot\nGeneralist\nSpecialist for robot\nvision\nencoder\ntext\ntokenizer\ntask instruction\nvision\nencoder\ntext\ntokenizer\nstate\nencoder\ntask instruction\nKV\nKV\nAsymmetric\u00a0MoT\nAction Expert\nCondition\ndenoise\n-1.8\n3.7\n1.4\ncontinuous actions\nFlow-Matching Algorithm\na)\u00a0 Overview of TwinBrainVLA\nWq\nWk\nWv\nWq\nWk\nWv\nCausal Self-Attention\nindependent weights, joint attention\nLeft Brain Input\nRight Brain Input\nb)\u00a0 Asymmetric MoT\nTransformer Block\nTransformer Block\nFigure 2: The framework of TwinBrainVLA\n. (a) Overall Architecture. The model features an Asym-\nmetric Mixture-of-Transformers design composed of two distinct pathways: a frozen \"Left Brain\" (Generalist)\nfor semantic reasoning and a trainable \"Right Brain\" (Specialist) for embodied motor control. The Right Brain\nfuses visual, textual, and proprioceptive state inputs to provide conditioning for the Action Expert, which utilizes a\nFlow-Matching algorithm to denoise continuous robotic actions. (b) Asymmetric MoT Mechanism (AsyMoT).\nThrough causal self-attention, the trainable Right Brain attends to the frozen Key-Value (KV) pairs of the Left\nBrain, enabling the transfer of general semantic knowledge to the robotic control policy without catastrophic for-\ngetting.\nend-effector pose). The Left Brain functions as a pure semantic generalist. It receives only the vision\nand language tokens:\nH0\nL = [V(I); T (T)],\n(2)\nwhere V and T represent the vision encoder and text tokenizer, respectively.\nIn contrast, the Right Brain is responsible for embodied control and must be grounded in the robot\u2019s\nphysical state. To achieve this, we introduce a lightweight State Encoder \u03d5, modeled as a Multi-Layer\nPerceptron (MLP), which projects the low-level proprioceptive state s into the VLM\u2019s embedding space.\nThese state tokens are appended to the multimodal input sequence:\nH0\nR = [V(I); T (T); \u03d5(s)].\n(3)\nThis design ensures that the Right Brain explicitly attends to the robot\u2019s current con\ufb01guration, a critical\nrequirement for closed-loop control.\nAsymmetric Mixture-of-Transformers (AsyMoT). The core innovation of our backbone lies in how\nthese two streams interact. We propose the AsyMoT mechanism to fuse general semantic knowledge\ninto the robotic control stream. Let Hl\nL and Hl\nR denote the hidden states of the Left and Right Brains at\nlayer l, respectively.\nThe Left Brain remains frozen during training. Its self-attention mechanism operates independently to\npreserve pre-trained general capabilities:\nHl+1\nL\n= Attn(Ql\nL, Kl\nL, V l\nL) + FFN(Hl\nL),\n(4)\nwhere Q, K, V are derived from Hl\nL using frozen projection weights.\nThe Right Brain is trainable and employs an Asymmetric Joint Attention mechanism. It queries not\nonly its own context but also the semantic features from the Left Brain. Speci\ufb01cally, at each attention\nhead, the Right Brain computes its Query (QR), while the Key (K) and Value (V ) are constructed by\nconcatenating the representations from both brains:\nKjoint = [sg(Kl\nL) ; Kl\nR],\n(5)\nVjoint = [sg(V l\nL) ; V l\nR],\n(6)\nHl+1\nR\n= Softmax\n(Ql\nR(Kjoint)T\n\u221adk\n)\nVjoint + FFN(Hl\nR),\n(7)\n4\nwhere [ ; ] denotes concatenation along the sequence length dimension, and sg(\u00b7) indicates the stop-\ngradient operation.\nThis asymmetric \ufb02ow ensures a strict hierarchy: the Left Brain acts as a stable \"semantic anchor\"\nproviding high-level reasoning features, while the Right Brain dynamically fuses these semantics with\n\ufb01ne-grained proprioceptive cues to reason about spatial actions. The \ufb01nal hidden states of the Right\nBrain, Hfinal\nR\n, are then passed to the Action Expert.\n4.2\nFlow-Matching Action Expert\nTo enable high-precision, continuous control, we move beyond the discrete tokenization paradigm (Kim\net al., 2024) and adopt the state-of-the-art generative policy architecture utilized in recent foundation\nmodels like Isaac-GR00T (NVIDIA et al., 2025) and \u03c00 (Intelligence et al., 2025). Speci\ufb01cally, our\nAction Expert employs the Diffusion Transformer (DiT) architecture (Peebles and Xie, 2023) and is\ntrained via \ufb02ow matching.\nArchitecture and Conditioning. Following the design in Isaac-GR00T, the Action Expert operates as a\nconditional decoder that denoises action trajectories. It takes a noisy action sequence at and the current\ntimestep t as input. While the policy architecture aligns with established baselines, a key distinction\nin our framework lies in the source of the condition. We inject the spatially-rich representations from\nour trainable Right Brain, HR, into the DiT via cross-attention layers. This ensures that the proven\ncontrol capabilities of the DiT are guided explicitly by the embodied perception features extracted by\nour asymmetric backbone.\nFlow-Matching Formulation. We utilize the \ufb02ow-matching objective to train the policy, as demon-\nstrated effectively in prior work (NVIDIA et al., 2025). The process models a conditional probability\npath \ufb02owing from a standard Gaussian prior a0 \u223cN(0, I) to the ground-truth action distribution a1.\nThe vector \ufb01eld regresson loss is de\ufb01ned as:\nLFM(\u03c8) = Et,a0,a1\n[\n||v\u03c8(at, t, HR) \u2212(a1 \u2212a0)||2]\n,\n(8)\nwhere v\u03c8 is the DiT network, t \u223cU[0, 1], and the target vector \ufb01eld is the straight line a1 \u2212a0. During\ninference, actions are synthesized by solving the Ordinary Differential Equation (ODE) using an Euler\nsolver, translating the Right Brain\u2019s understanding into smooth robotic movements.\n4.3\nTraining Strategy\nOptimization Objective. Consistent with standard VLA \ufb01ne-tuning paradigms (NVIDIA et al., 2025),\nwe train TwinBrainVLA using exclusively the robotic action objective. We do not incorporate auxiliary\nNext-Token Prediction (NTP) losses or mix in general vision-language datasets during this stage. The\ntraining objective is to minimize the Flow-Matching loss:\nLtotal = LFM(\u03b8R, \u03c8, \u03d5; Drobot),\n(9)\nwhere Drobot represents the robotic demonstration dataset, and {\u03b8R, \u03c8, \u03d5} denote the trainable parameters\nof the Right Brain, Action Expert, and State Encoder, respectively. It is worth noting that in monolithic\nVLA architectures, relying solely on action loss typically leads to severe catastrophic forgetting of gen-\neral semantic capabilities. However, our dual-stream design structurally immunizes the model against\nthis degradation: the \"Right Brain\" is free to specialize entirely in control dynamics, while the frozen\n\"Left Brain\" implicitly safeguards the linguistic and semantic priors.\nAsymmetric Update Rule. We enforce a strict parameter update policy. Let \u03b8L be the parameters of the\nLeft Brain. During backpropagation, we set \u2207\u03b8L = 0. Gradients originating from the Action Expert \ufb02ow\nbackwards through the DiT (\u03c8), permeate the Right Brain (\u03b8R) via the conditioning signals, and update\nthe State Encoder (\u03d5). Crucially, at the AsyMoT fusion layer, the gradient \ufb02ow is explicitly blocked\n(stop-gradient) at the Key-Value interface from the Left Brain. This ensures that the Left Brain acts as a\nstable semantic anchor, providing consistent features KL, VL without its weights being perturbed by the\nhigh-variance gradients typical of robotic control tasks.\n5\n5\nExperiment\nTo comprehensively evaluate the ef\ufb01cacy of TwinBrainVLA, we conduct extensive experiments on two\nsimulation benchmarks: SimplerEnv (Li et al., 2024c) and RoboCasa (Nasiriany et al., 2024). Our\ntraining pipeline is built upon the starVLA framework (starVLA, 2025), distributed across 16 \u00d7 NVIDIA\nH100 GPUs. We strictly follow its default training protocols to ensure a fair comparison. More extensive\nsimulation benchmarks and real-world robotic experiments are in progress.\n5.1\nExperiments on SimplerEnv\nImplementation and Training Setup. To demonstrate the scalability and effectiveness of our architec-\nture, we instantiate TwinBrainVLA with two state-of-the-art VLM backbones: Qwen2.5-VL-3B-Instruct\nand Qwen3-VL-4B-Instruct. Consistent with our asymmetric design, both the frozen Left Brain and the\ntrainable Right Brain are initialized from these pre-trained checkpoints to ensure aligned feature spaces.\nFor the training data, we utilize two large-scale subsets from the Open X-Embodiment (OXE) dataset:\nthe Bridge-V2 and the Fractal dataset. We are actively working on scaling our training to the complete\nOXE dataset, and future experimental results will be reported in subsequent updates. Comprehensive\nimplementation details are provided in Appendix A.\nEvaluation. The benchmark consists of four manipulation tasks: \"put spoon on towel\", \"put carrot on\nplate\", \"stack green block on yellow block\", \"put eggplant in the yellow basket\". For each task, we eval-\nuate our VLA policy using the evaluation script provided by the SimplerEnv repository (Li et al., 2024c).\nTo mitigate randomness, we run 480 independent trials and report the mean performance (Avg@480).\nBaselines. We benchmark TwinBrainVLA against a comprehensive suite of state-of-the-art VLA poli-\ncies, including RT-1-X (ONeill et al., 2024), Octo (Team et al., 2024), OpenVLA (Kim et al., 2024),\nRoboVLM (Li et al., 2024b), TraceVLA (Zheng et al., 2025c), SpatialVLA (Qu et al., 2025b), Co-\ngACT (Li et al., 2024a), VideoVLA (Shen et al., 2025), \u03c00 (Black et al., 2024), and \u03c00.5 (Intelligence\net al., 2025). All baseline results are obtained from their original papers or of\ufb01cial reproductions to\nensure a fair and reliable comparison.\nTable 1: Results of evaluating the VLA models with the WidowX robot in the SimplerEnv simulation envi-\nronment. We highlight the best results in bold and the second-best results with underline.\nMethod\nPut Spoon\non Towel\nPut Carrot\non Plate\nStack Green Block\non Yellow Block\nPut Eggplant\nin Yellow Basket\nAverage\nRT-1-X (ONeill et al., 2024)\n0.0\n4.2\n0.0\n0.0\n1.1\nOcto-Base (Team et al., 2024)\n15.8\n12.5\n0.0\n41.7\n17.5\nOcto-Small (Team et al., 2024)\n41.7\n8.2\n0.0\n56.7\n26.7\nOpenVLA (Kim et al., 2024)\n4.2\n0.0\n0.0\n12.5\n4.2\nOpenVLA-OFT (Kim et al., 2025)\n12.5\n4.2\n4.2\n72.5\n23.4\nRoboVLM (Li et al., 2024b)\n50.0\n37.5\n0.0\n83.3\n42.7\nTraceVLA (Zheng et al., 2025c)\n12.5\n16.6\n16.6\n65.0\n27.7\nSpatialVLA (Qu et al., 2025b)\n20.8\n20.8\n25.0\n70.8\n34.4\nCogACT (Li et al., 2024a)\n71.7\n50.8\n15.0\n67.5\n51.3\nVideoVLA (Shen et al., 2025)\n75.0\n20.8\n45.8\n70.8\n53.1\n\u03c00 (Black et al., 2024)\n29.1\n0.0\n16.6\n62.5\n27.1\n\u03c00.5 (Intelligence et al., 2025)\n49.3\n64.7\n44.7\n69.7\n57.1\nQwenGR00T + Qwen3-VL-3B (starVLA, 2025)\n87.5\n50.0\n29.2\n54.2\n55.2\nQwenGR00T + Qwen3-VL-8B (starVLA, 2025)\n70.8\n38.9\n22.2\n65.3\n49.3\nQwenGR00T + Qwen2.5-VL-7B (starVLA, 2025)\n59.2\n30.8\n3.3\n44.2\n34.4\nIsaac-GR00T-N1.6-Bridge (Team et al., 2025b)\n64.5\n65.5\n5.5\n93.0\n57.1\nTwinBrainVLA + Qwen2.5-VL-3B-Instruct\n83.3\n41.7\n31.5\n77.1\n58.4\nTwinBrainVLA + Qwen3-VL-4B-Instruct\n73.0\n58.3\n33.3\n83.3\n62.0\nResults. The quantitative results on the SimplerEnv benchmark are presented in Table 1. Notably,\ndespite not undergoing large-scale pre-training for robotic action prediction, TwinBrainVLA achieves\nstate-of-the-art performance among all listed methods. Our framework demonstrates strong generaliz-\nability across different VLM families, attaining competitive success rates of 58.4% with Qwen2.5-VL-\n6\n3B-Instruct and 62.0% with Qwen3-VL-4B-Instruct. The latter con\ufb01guration surpasses the strongest\nbaseline, Isaac-GR00T-N1.6 (57.1%), by a notable margin of +4.9%, validating the effectiveness of our\nasymmetric dual-brain architecture in bridging high-level semantic understanding and low-level robotic\ncontrol.\n5.2\nExperiments on RoboCasa\nImplementation. We train TwinBrainVLA on the Humanoid Robot Tabletop Manipulation subset\nfrom the PhysicalAI-Robotics-GR00T-X-Embodiment-Sim dataset (NVIDIA et al., 2025). All other\nexperimental settings, including model initialization, distributed training infrastructure, and hyperparam-\neters, remain identical to those described in Sec. 5.1.\nEvaluation. We evaluate our policy on the RoboCasa GR1 Tabletop Benchmark, which comprises a\ndiverse suite of 24 tabletop manipulation tasks. These tasks involve complex interactions with articulated\nobjects and varying geometries, such as PnPBottleToCabinetClose (placing a bottle into a cabinet\nand closing the door), PnPCanToDrawerClose, and various microwave/toaster interaction scenarios. To\nensure statistical reliability while maintaining consistency with our previous experiments, we report the\nsuccess rate averaged over multiple independent trials (Avg@50) for each of the 24 tasks.\nTable 2: Results of evaluating the VLA models with the GR1 robot in the RoboCasa Tabletop simulation\nenvironment. We highlight the best results in bold and the second-best results with underline.\nTask\nIsaac-GR00T\nN1.6\nQwenGR00T\n+ Qwen3VL\nQwenPI\n+ Qwen3VL\nTwinBrainVLA\n+ Qwen2.5VL\nTwinBrainVLA\n+ Qwen3VL\nPnP Bottle To Cabinet Close\n51.5\n46.0\n26.0\n62.0\n74.0\nPnP Can To Drawer Close\n13.0\n80.0\n62.0\n66.0\n72.0\nPnP Cup To Drawer Close\n8.5\n54.0\n42.0\n46.0\n52.0\nPnP Milk To Microwave Close\n14.0\n48.0\n50.0\n52.0\n60.0\nPnP Potato To Microwave Close\n41.5\n28.0\n42.0\n56.0\n36.0\nPnP Wine To Cabinet Close\n16.5\n46.0\n32.0\n58.0\n46.0\nPnP Novel From Cuttingboard To Basket\n58.0\n48.0\n40.0\n50.0\n62.0\nPnP Novel From Cuttingboard To Cardboardbox\n46.5\n40.0\n46.0\n44.0\n46.0\nPnP Novel From Cuttingboard To Pan\n68.5\n68.0\n60.0\n64.0\n70.0\nPnP Novel From Cuttingboard To Pot\n65.0\n52.0\n40.0\n60.0\n66.0\nPnP Novel From Cuttingboard To Tieredbasket\n46.5\n56.0\n44.0\n52.0\n52.0\nPnP Novel From Placemat To Basket\n58.5\n42.0\n44.0\n56.0\n30.0\nPnP Novel From Placemat To Bowl\n57.5\n44.0\n52.0\n59.0\n54.0\nPnP Novel From Placemat To Plate\n63.0\n48.0\n50.0\n62.0\n64.0\nPnP Novel From Placemat To Tieredshelf\n28.5\n18.0\n28.0\n28.0\n38.0\nPnP Novel From Plate To Bowl\n57.0\n60.0\n52.0\n62.0\n60.0\nPnP Novel From Plate To Cardboardbox\n43.5\n50.0\n40.0\n52.0\n58.0\nPnP Novel From Plate To Pan\n51.0\n54.0\n36.0\n56.0\n56.0\nPnP Novel From Plate To Plate\n78.7\n70.0\n48.0\n68.0\n66.0\nPnP Novel From Tray To Cardboardbox\n51.5\n38.0\n34.0\n40.0\n46.0\nPnP Novel From Tray To Plate\n71.0\n56.0\n64.0\n56.0\n72.0\nPnP Novel From Tray To Pot\n64.5\n50.0\n44.0\n60.0\n56.0\nPnP Novel From Tray To Tieredbasket\n57.0\n36.0\n50.0\n42.0\n46.0\nPnP Novel From Tray To Tieredshelf\n31.5\n16.0\n28.0\n34.0\n28.0\nAverage\n47.6\n47.8\n43.9\n53.5\n54.6\nBaselines.\nWe compare TwinBrainVLA against several competitive baselines trained on the same\ndataset. Speci\ufb01cally, we include Isaac-GR00T-N1.6 (Team et al., 2025b), a large-scale VLA model\nthat demonstrates strong performance on humanoid manipulation tasks. Additionally, to directly assess\nthe advantages of our dual-brain architecture, we benchmark against two representative models from the\nstarVLA framework (starVLA, 2025): QwenGR00T and QwenPI, both instantiated with Qwen3-VL-\n7\n4B-Instruct backbones. These baselines follow similar training protocols and utilize the same dataset,\nenabling a fair comparison and highlighting the speci\ufb01c contributions of our asymmetric dual-VLM de-\nsign.\nResults. The quantitative results on the RoboCasa GR1 Tabletop Benchmark are presented in Table 2.\nTwinBrainVLA achieves the best performance across all 24 manipulation tasks, with our Qwen3-VL-\n4B-Instruct variant attaining an average success rate of 54.6%, followed closely by the Qwen2.5-VL-\n3B-Instruct variant at 53.5%. These results substantially outperform all baseline methods: our best\nmodel surpasses Isaac-GR00T-N1.6 (47.6%) by +7.0%, QwenGR00T (47.8%) by +6.8%, and QwenPI\n(43.9%) by +10.7%. These results reinforce our hypothesis that decoupling semantic understanding\nfrom embodied perception enables more effective learning of \ufb01ne-grained manipulation skills in complex\ntabletop scenarios.\n6\nConclusion\nIn this paper, we introduced TwinBrainVLA\n, a novel VLA framework that addresses the funda-\nmental tension between semantic generalization and embodied specialization in robotic learning. By\ndrawing inspiration from hemispheric lateralization, we proposed an asymmetric dual-stream architec-\nture that structurally decouples the visual cognitive process. Our AsyMoT mechanism effectively syn-\nergizes a frozen \"Left Brain\" for robust open-world reasoning with a trainable \"Right Brain\" for precise\nsensorimotor control, enabling the system to master complex manipulation skills without \"catastrophic\nforgetting\" of its general visual capabilities.\nLimitation and Future Work\nWhile TwinBrainVLA provides a powerful framework for decoupling semantic and embodied process-\ning, several limitations remain that outline our future research directions.\nA more decoupled model architecture. First, the current implementation of the Asymmetric Mixture-\nof-Transformers (AsyMoT) mechanism requires the Left and Right Brains to share an identical model ar-\nchitecture. This structural constraint ensures compatible hidden state dimensions for seamless joint atten-\ntion but limits the \ufb02exibility to pair heterogeneous modelsfor instance, synergizing a massive, reasoning-\nheavy VLM with a lightweight, high-frequency control model. We are actively exploring generalized\nfusion mechanisms, such as learnable projection layers or cross-attention adapters, to support backbones\nof varying sizes and architectures.\nMore diverse model checkpoints. Second, both streams in our current setup are initialized from stan-\ndard general-purpose VLM checkpoints (e.g., Qwen-VL). However, the \ufb01eld has arguably seen a surge in\nspecialized Embodied VLMs trained speci\ufb01cally for robotics. Integrating these specialized models into\nour dual-brain paradigmpotentially initializing the \"Right Brain\" with an embodied checkpoint while re-\ntaining a generalist \"Left Brain\"presents a promising avenue. We are currently investigating strategies to\neffectively adapt our framework to leverage these domain-speci\ufb01c post-trained weights.\nTraining on larger-scale data. Third, regarding data scale, our current training utilizes subsets of the\nOpen X-Embodiment dataset. We acknowledge that the capacity of our dual-stream architecture may\nnot be fully realized with limited data. We are in the process of scaling our pre-training to the complete\ndataset to further enhance the model\u2019s robustness and downstream performance.\nMore benchmarks and real-robot experiment. Finally, future work will also focus on extending our\nevaluation to a broader range of benchmarks, including real-robot scenarios, to comprehensively assess\nthe versatility of TwinBrainVLA.\nReferences\nShuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding,\nChang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui,\nShutong Jiang, Zhaohai Li, Mingsheng Li, and 45 others. 2025a. Qwen3-vl technical report.\n8\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang,\nJun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding,\nZheren Fu, Yiheng Xu, and 8 others. 2025b. Qwen2.5-vl technical report.\nKevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy\nGroom, Karol Hausman, Brian Ichter, and 1 others. 2024. \u03c00: A vision-language-action \ufb02ow model for general\nrobot control. arXiv preprint arXiv:2410.24164.\nJunhao Cai, Zetao Cai, Jiafei Cao, Yilun Chen, Zeyu He, Lei Jiang, Hang Li, Hengjie Li, Yang Li, Yufei Liu,\nYanan Lu, Qi Lv, Haoxiang Ma, Jiangmiao Pang, Yu Qiao, Zherui Qiu, Yanqing Shen, Xu Shi, Yang Tian,\nand 23 others. 2026. Internvla-a1: Unifying understanding, generation and action for robotic manipulation.\nPreprint, arXiv:2601.02456.\nShiqi Chen, Tongyao Zhu, Ruochen Zhou, Jinghan Zhang, Siyang Gao, Juan Carlos Niebles, Mor Geva, Junxian\nHe, Jiajun Wu, and Manling Li. 2025. Why is spatial reasoning hard for VLMs? an attention mechanism\nperspective on focus areas. In International conference on machine learning (ICML).\nRonghao Dang, Yuqian Yuan, Yunxuan Mao, Kehan Li, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, and Deli\nZhao. 2025. Rynnec: Bringing mllms into embodied world. Preprint, arXiv:2508.14160.\nShichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao,\nZhiyuan Xu, Meng Li, Qingjie Liu, Shanghang Zhang, Min Wan, and Jian Tang. 2025. Xr-1: Towards versatile\nvision-language-action models via learning uni\ufb01ed vision-motion representations. Preprint, arXiv:2511.02776.\nZhen Fang, Zhuoyang Liu, Jiaming Liu, Hao Chen, Yu Zeng, Shiting Huang, Zehui Chen, Lin Chen, Shanghang\nZhang, and Feng Zhao. 2025. Dualvla: Building a generalizable embodied agent via partial decoupling of\nreasoning and action. Preprint, arXiv:2511.22134.\nWenkai Guo, Guanxing Lu, Haoyuan Deng, Zhenyu Wu, Yansong Tang, and Ziwei Wang. 2025. Vla-reasoner:\nEmpowering vision-language-action models with reasoning via online monte carlo tree search.\nPreprint,\narXiv:2509.22643.\nAsher J. Hancock, Xindi Wu, Lihan Zha, Olga Russakovsky, and Anirudha Majumdar. 2025a. Actions as language:\nFine-tuning vlms into vlas without catastrophic forgetting. Preprint, arXiv:2509.22195.\nAsher J. Hancock, Xindi Wu, Lihan Zha, Olga Russakovsky, and Anirudha Majumdar. 2025b. Actions as language:\nFine-tuning vlms into vlas without catastrophic forgetting. Preprint, arXiv:2509.22195.\nXiaoshuai Hao, Lei Zhou, Zhijian Huang, Zhiwen Hou, Yingbo Tang, Lingfeng Zhang, Guang Li, Zheng Lu,\nShuhuai Ren, Xianhui Meng, Yuchen Zhang, Jing Wu, Jinghui Lu, Chenxu Dang, Jiayi Guan, Jianhua Wu,\nZhiyi Hou, Hanbing Li, Shumeng Xia, and 25 others. 2025. Mimo-embodied: X-embodied foundation model\ntechnical report. Preprint, arXiv:2511.16518.\nChi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, and Fu-En Yang.\n2026.\nFast-thinkact: Ef\ufb01cient vision-language-action reasoning via verbalizable latent planning.\nPreprint,\narXiv:2601.09708.\nChi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. 2025.\nThinkact:\nVision-language-action reasoning via reinforced visual latent planning. Preprint, arXiv:2507.16815.\nPhysical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail,\nMichael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman,\nBrian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, and 17 others. 2025. \u03c00.5: a vision-\nlanguage-action model with open-world generalization. Preprint, arXiv:2504.16054.\nJingjing Jiang, Chao Ma, Xurui Song, Hanwang Zhang, and Jun Luo. 2025a. Corvid: Improving multimodal\nlarge language models towards chain-of-thought reasoning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 3034\u20133046.\nTao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao,\nHuazhe Xu, and Hang Zhao. 2025b. Galaxea open-world dataset and g0 dual-system vla model. Preprint,\narXiv:2509.00576.\nNikita Kachaev, Mikhail Kolosov, Daniil Zelezetsky, Alexey K. Kovalev, and Aleksandr I. Panov. 2025. Don\u2019t\nblind your vla: Aligning visual representations for ood generalization. Preprint, arXiv:2510.25616.\nMoo Jin Kim, Chelsea Finn, and Percy Liang. 2025. Fine-tuning vision-language-action models: Optimizing\nspeed and success. Preprint, arXiv:2502.19645.\n9\nMoo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov,\nEthan P Foster, Pannag R Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burch\ufb01el, Russ Tedrake, Dorsa\nSadigh, Sergey Levine, Percy Liang, and Chelsea Finn. 2024."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper\n\n#### Main Contributions:\n1. **TwinBrainVLA Architecture**: The paper introduces TwinBrainVLA, a novel Vision-Language-Action (VLA) framework that decouples high-level semantic understanding from low-level embodied perception by using two isomorphic VLMs: a \"Left Brain\" and a \"Right Brain\". This design aims to resolve the fundamental tension between general-purpose vision-language models and specialized robotic control.\n2. **Asymmetric Mixture-of-Transformers (AsyMoT) Mechanism**: The authors propose AsyMoT, an asymmetric mechanism that integrates two VLMs without sharing parameters. It allows the \"Right Brain\" to dynamically query semantic knowledge from the frozen \"Left Brain\", providing rich conditioning for precise action generation.\n3. **Training Strategy**: TwinBrainVLA employs a dual-stream training strategy where the \"Left Brain\" is frozen and specialized for general semantic understanding, while the \"Right Brain\" is trainable and focused on embodied perception. The paper details this design in Sec. 4.1, describes the flow-matching policy formulation in Sec. 4.2, and presents the joint-training objective in Sec. 4.3.\n\n#### Methodology:\n- **Dual-VLM Backbone**: TwinBrainVLA consists of two parallel VLMs: a \"Left Brain\" (frozen) for semantic reasoning and a \"Right Brain\" (trainable) for embodied motor control.\n- **State Encoding**: The system integrates visual observations, natural language instructions, and proprioceptive states. The \"Left Brain\" processes vision and text tokens separately, while the \"Right Brain\" encodes low-level proprioceptive state information into VLM embedding space.\n- **Flow-Matching Action Expert**: The paper introduces a Flow-Matching Action Expert that generates precise continuous actions using Diffusion Transformer (DiT) architecture for high-precision control.\n\n#### Key Findings:\n1. TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines on benchmarks such as SimplerEnv and RoboCasa, demonstrating its effectiveness in complex tabletop tasks.\n2. The framework preserves comprehensive visual understanding capabilities of the pre-trained VLM while enabling precise robotic control.\n\n#### Conclusions:\n- **Performance**: TwinBrainVLA outperforms existing methods across various manipulation tasks, achieving state-of-the-art performance on SimplerEnv and surpassing strong baselines like Isaac-GR00T-N1.6.\n- **Generalizability**: The framework shows strong generalizability across different VLM families (Qwen2.5-VL-3B-Instruct vs Qwen3-VL-4B-Instruct).\n- **Limitations and Future Work**: While the paper highlights impressive results, it also identifies several limitations such as more decoupled model architectures, diverse model checkpoints, larger-scale data training, and broader benchmarking for real robots.\n\nThis architecture represents a promising direction towards building general-purpose robots that can simultaneously achieve high-level semantic understanding and low-level physical dexterity."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Asymmetric regularization mechanism for GAN training with Variational\nInequalities\nSpyridon C. Giagtzoglou\nMark H.M. Winands\nBarbara Franci\nAbstract\u2014 We formulate the training of generative adversar-\nial networks (GANs) as a Nash equilibrium seeking problem.\nTo stabilize the training process and find a Nash equilibrium,\nwe propose an asymmetric regularization mechanism based on\nthe classic Tikhonov step and on a novel zero-centered gradient\npenalty. Under smoothness and a local identifiability condition\ninduced by a Gauss\u2013Newton Gramian, we obtain explicit Lip-\nschitz and (strong)-monotonicity constants for the regularized\noperator. These constants ensure last-iterate linear convergence\nof a single-call Extrapolation-from-the-Past (EFTP) method.\nEmpirical simulations on an academic example show that, even\nwhen strong monotonicity cannot be achieved, the asymmetric\nregularization is enough to converge to an equilibrium and\nstabilize the trajectory.\nI. INTRODUCTION\nGenerative Adversarial Networks (GANs) are a machine-\nlearning model composed of two neural networks: the dis-\ncriminator and the generator [1]. The purpose of the model\nis to train the generator to create realistic data (images,\naudio) by playing a \u201cgame\u201d against the discriminator. In\nparticular, the training of the two neural networks is a two-\nplayer zero-sum game that can inherently be modelled as\na saddle-point problem. Saddle point problems are well\nstudied in the literature [2], [3] and find applications in\nconsensus problems, resource allocation or simply primal-\ndual problems [4], [5]. However, GANs highlighted some\ninstabilities, like mode collapse, that can be seen as \u201cbest-\nresponse\u201d pathologies. Moreover, the training dynamics are\noften rotational and poorly conditioned and the only available\nsolutions are basically ways to change the game\u2019s payoff to\nmake the learning process smoother [6], [7].\nWe rely instead on variational inequalities (VIs) and\nmonotone operator theory to establish the properties of\nthe operators involved and derive projected algorithms with\nconvergence guarantees. We adopt a VI viewpoint because it\nexposes the saddle operator connected to the GAN game and\nits geometry, it allows to handle constraints via projections\nin a principled way, and directly connects to convergence\nguarantees for a variety of methods. Since the VI for-\nmulation highlights the rotational dynamics of adversarial\ntraining, stabilization strategies are needed to make the train-\ning reliable.For this reason, we introduce a regularization\nS. C. Giagtzoglou was supported by the Dutch Research Council (NWO)\nM 1 grant, project number OCENW.M.22.433. S. C. Giagtzoglou and\nMark H.M. Winands are with the Department of Advanced Comput-\ning Sciences, Maastricht University, Paul-Henri Spaaklaan 1, 6229 GS\nMaastricht, The Netherlands (spyridon.giagtzoglou@maastrichtuniversity.nl;\nm.winands@maastrichtuniversity.nl). B. Franci is with the Department of\nMathematical Sciences, Politecnico di Torino, Corso Duca degli Abruzzi\n24, 10129 Torino, Italy (barbara.franci@polito.it).\nmechanism applied only on the discriminator side, that is,\nasymmetrically across the saddle blocks. This discriminator-\nonly block-asymmetric regularization adds curvature where\nit is most needed while preserving the target equilibrium\npoint. It also provides explicit analytic constants, specifi-\ncally Lipschitz continuity and strong-monotonicity constants\nfor the regularized operator, thereby enabling convergence\nguarantees for the proposed iterative method.\nAmong the available algorithms in the literature, we\npropose Extrapolation-from-the-Past (EFTP) [8] because,\nalthough made of two projection steps, it requires only\none gradient computation, hence providing a computation-\nally convenient alternative to the more common Forward-\nBackward (FB) [9] and ExtraGradient (EG) [10] schemes. In\nparticular, our contributions can be summarized as follows.\n\u2022 We introduce and analyze two explicitly asymmetric\nregularizations based on Tikhonov [11] and a zero-\ncentered input-gradient penalty on the discriminator\nside. The penalty vanishes at equilibrium, preserving\nthe original saddle point structure while improving\nconditioning.\n\u2022 Under smoothness and a local Gauss\u2013Newton iden-\ntifiability condition, we derive explicit Lipschitz and\nstrong-monotonicity constants for the regularized oper-\nator, showing that our regularization can infer stronger\nmonotonicity properties on the original saddle operator.\n\u2022 We propose EFTP as a training algorithm for GANs\nand show its convergence to a NE of the corresponding\ngame, even in the regularized case.\nWe validate our analysis with some numerical simulations\nshowing the effects of our proposed regularization in sta-\nbilizing the training process and the convergence properties\nof the proposed algorithm. Although our work takes GANs\nas a primary application, our results can be generalized to\nsymmetric regularization, hence retrieving, for instance, the\nclassic Tikhonov [11], [12]. As a consequence, the extension\nto n-players games is also possible, as long as some distinc-\ntion between the two sides (generator and discriminator) is\nkept for the asymmetric case.\nA. Notation\nLet R be the set of all real number and Rn represents\nn-dimensional Euclidean space.\nProbability: Let (X, X, pD) and (Z, Z, pZ) be proba-\nbility spaces. X and Z are the domains of data and latent\nvectors, respectively (e.g., X = Rdx, Z = Rdz). X and Z\nare their \u03c3-algebras, and pD and pZ are probability measures\narXiv:2601.13920v1  [cs.GT]  20 Jan 2026\non (X, X) and (Z, Z), respectively. We write x \u223cpD and\nz \u223cpZ for samples drawn from these distributions.\nMatrices: Given A \u2208Rn\u00d7n, we write \u03bbmin(A) and\n\u03bbmax(A) for its smallest and largest eigenvalues, respec-\ntively. Let Mn \u2282Rn\u00d7n be the set of symmetric matrices\nof dimension n. For A \u2208Mn: A \u2ab00 or A \u2208Mn\n+ means\nthat A is positive semidefinite (PSD), i.e., x\u22a4Ax \u22650 for\nall x \u2208Rn; A \u227b0 or A \u2208Mn\n++ means that A is positive\ndefinite (PD), i.e., x\u22a4Ax > 0 for all x \u0338= 0.For vectors,\n\u2225\u00b7 \u22252 denotes the Euclidean norm. For matrices A \u2208Rm\u00d7n,\n\u2225A\u2225denotes the spectral norm induced by \u2225\u00b7 \u22252, i.e. \u2225A\u2225=\nsupx\u0338=0\n\u2225Ax\u22252\n\u2225x\u22252\n= \u03c3max(A) =\np\n\u03bbmax(A\u22a4A). If A \u2208M,\n\u2225A\u2225= maxi |\u03bbi(A)|; if A \u2208Mn\n++, \u2225A\u2225= \u03bbmax(A).\nOperator Theory: For a nonempty closed convex set S,\nthe Euclidean projector is \u03a0S(x) := arg miny\u2208S \u2225y \u2212x\u22252.\nThe normal cone is NS(x) := {v : \u27e8v, y \u2212x\u27e9\u22640, \u2200y \u2208S}\nwhen x \u2208S, and empty otherwise. A function f : Rn \u2192R\nis \u00b5-strongly convex (concave) if f \u2212\u00b5\n2 \u2225\u00b7\u22252 (\u2212f \u2212\u00b5\n2 \u2225\u00b7\u22252),\n\u00b5 > 0, is convex. A mapping F : Rn \u2192Rn is: L-Lipschitz\ncontinous on S if \u2225F(u)\u2212F(v)\u2225\u2264L\u2225u\u2212v\u2225for all u, v \u2208S\nwith L > 0; monotone if \u27e8F(u) \u2212F(v), u \u2212v\u27e9\u22650 for all\nu, v \u2208S; \u00b5-strongly monotone if \u27e8F(u) \u2212F(v), u \u2212v\u27e9\u2265\n\u00b5\u2225u \u2212v\u22252 with \u00b5 > 0. Given P \u2208Mn\n+, the residual is\nRP (\u03c9) := P \u22121(\u03c9 \u2212\u03a0S(\u03c9 \u2212P F(\u03c9))).\nFor a mapping F : Rd \u2192Rm, we indicate with JF \u2208\nRm\u00d7d the Jacobian. For a twice continuously differentiable\nfunction f : Rd \u2192R we indicate with \u22072\nuuf \u2208M the\nHessian and with \u22072\nxyf the mixed partial derivatives.\nII. PROBLEM DESCRIPTION\nWe model the training of GANs as a two-player zero-sum\ngame between the generator G\u03b8 : Z \u2192X (player G) and\nthe discriminator D\u03c6 : X \u2192R (player D), with parameters\n\u03b8 \u2208\u0398 \u2282Rd\u03b8 and \u03c6 \u2208\u03a6 \u2282Rd\u03c6. The strategy sets \u0398 and\n\u03a6 capture architectural/regularization constraints We write\nS := \u0398 \u00d7 \u03a6 and \u03c9 = (\u03b8, \u03c6). The two players min\u2013max\noptimisation problem reads as\nmin\n\u03b8\u2208\u0398 max\n\u03c6\u2208\u03a6 L(\u03b8, \u03c6) := Ex\u223cpD\n\u0002\n\u03a8(D\u03c6(x))\n\u0003\n+ Ez\u223cpZ\n\u0002\n\u03a8(\u2212D\u03c6(G\u03b8(z)))\n\u0003\n,\n(1)\ntherefore this problem is a zero-sum game. A popular choice\nfor the objective is \u03a8(t) = \u2212log(1 + e\u2212t) but other choices\ncan be considered, as long as the following assumptions\n(Assumption 3 in particular) are satisfied.\nAccordingly, a pair (\u03b8\u22c6, \u03c6\u22c6) \u2208S is a Nash equilibrium iff\nL(\u03b8\u22c6, \u03c6) \u2264L(\u03b8\u22c6, \u03c6\u22c6) \u2264L(\u03b8, \u03c6\u22c6),\n\u2200(\u03b8, \u03c6) \u2208S. (2)\nThe min-max problem in (1) can be rewritten as a saddle\nproblem and solved by means of a VI. To this aim, let the\nsaddle operator associated to problem (1) be\nF(\u03c9) =\n\u0014 \u2207\u03b8L(\u03b8, \u03c6)\n\u2212\u2207\u03c6L(\u03b8, \u03c6)\n\u0015\n.\n(3)\nThen, the equilibria of the game in (1) can be characterized,\nthanks to the operator in (3), as the solutions of the VI(S, F)\nfind \u03c9\u22c6\u2208S s.t. \u27e8F(\u03c9\u22c6), \u03c9 \u2212\u03c9\u22c6\u27e9\u22650,\n\u2200\u03c9 \u2208S.\n(4)\nWe next state the standard conditions ensuring well-\nposedness of (1) and (4).\nAssumption 1 (Local Sets): \u0398\nand\n\u03a6\nare\nnonempty,\nclosed, and convex.\nNote that, under Assumption 1, the Euclidean projector\n\u03a0S, used in the later sections, is firmly non-expansive [13,\nsec 3.1].\nAssumption 2 (Smoothness of the base operator): The\nsaddle operator F in (3) is L0-Lipschitz continous on S.\nAssumption 3 (Curvature of the saddle objective): The\nobjective function L in (1) is \u00b5\u03b8-strongly convex in \u03b8 and\n\u00b5\u03c6-strongly concave in \u03c6 on S, with moduli \u00b5\u03b8, \u00b5\u03c6 \u22650\nRemark 1: Note that in Assumption 3 the parameters\n\u00b5\u03b8 and \u00b5\u03c6 may be zero. This means that the objective\nfunction might be merely convex/concave (not strongly con-\nvex/strongly concave). Consequently, the symmetric part of\nthe Jacobian JF is only positive semidefinite and the saddle\noperator F is, at best, monotone on S.\nA. Penalty Design\nWithout regularization, the GAN saddle field is typically\nrotational and ill-conditioned, therefore projected first-order\nmethods tend to oscillate or require impractically small steps\nsizes [14]\u2013[17]. To address these problems, we propose a reg-\nularization that injects curvature to suppress rotational drift\nand improve conditioning. The resulting field comes with\nexplicit Lipschitz and (strong) monotonicity constants, which\nguide stepsize choices and enable convergence guarantees.\nCombining the base operator in (3) with a penalty produces\nF\u03b3(\u03c9) =\n\u0014 \u2207\u03b8L(\u03b8, \u03c6) \u2212\u2207\u03b8B(\u03b8, \u03c6)\n\u2212\u2207\u03c6L(\u03b8, \u03c6) + \u2207\u03c6B(\u03b8, \u03c6)\n\u0015\n.\n(5)\nWe have a general, symmentric notation in (5), but later we\npropose an asymmetric one with \u2207B\u03b8(\u03b8, \u03c6) = 0. Therefore,\nwe regularize only the discriminator side of the game and\nanalyze the induced operator F\u03b3.\nRemark 2: Solving the regularized problem is not equiv-\nalent to solving the original one in general. In particular,\nsolutions of VI(S, F\u03b3) need not coincide with those of\nVI(S, F). However, if B vanishes at a saddle point of the\nunregularized operator, then F\u03b3(\u03c9\u22c6) = F(\u03c9\u22c6). Moreover,\nas \u03b3 \u21920, bounded solution sequences of VI(S, F\u03b3) have\ncluster points that solve VI(S, F).\nTo see this, let {\u03b3k}k>0 \u2286R be a sequence of regular-\nization parameters and let {\u03c9k}k>0 be the corresponding se-\nquence of solutions of VI(S, F\u03b3k). Assume {\u03c9k} is bounded\nwith \u03c9k \u2192\u00af\u03c9 along a subsequence. For any y \u2208S we have\n\u27e8F\u03b3k(\u03c9k), y \u2212\u03c9k\u27e9\u22650 and by uniform convergence F\u03b3k \u2192\nF on a compact set containing {\u03c9k} [18, Thm. 4.3]. There-\nfore, passing to the limit results in \u27e8F(\u00af\u03c9), y \u2212\u00af\u03c9\u27e9\u22650 for\nall y \u2208S, i.e., \u00af\u03c9 is a solution of VI(S, F) [3, Cor. 5.1.8].\nWe consider two forms of regularization. The terms are\nadded in the cost and later we differentiate to obtain the\ngradients of the regularization terms in (5).\na) Tikhonov regularization (Tik): Inspired by [11], let\nBTik(\u03b8, \u03c6) = \u03b3\n2 \u2225\u03c6\u22252.\n(6)\nCompared to the standard Tikhonov regularization, in our\ncase the term acts only on the discriminator block. Accord-\ningly, \u2207\u03c6BTik = \u03b3 \u03c6, \u22072\n\u03c6\u03c6BTik = \u03b3I, \u2207\u03b8BTik \u22610. This\npenalty introduces curvature in the \u03c6-subspace while leaving\nthe generator block unaffected.\nMoreover, if we also regularize the generator with the\nsame quadratic term, i.e.,\nBfull\nTik(\u03b8, \u03c6) = \u03b3\n2 (\u2225\u03b8\u22252 + \u2225\u03c6\u22252),\n(7)\nwe recover the classic (symmetric) Tikhonov regularization\nand the penalty adds identical curvature to both the generator\nand discriminator subspaces.\nb) Symmetric (zero-centered) Gradient Penalty (SGP):\nAs an alternative to the regularization introduced above, let\nBSGP(\u03b8, \u03c6) = \u03b3\n2 Ex\u223cpD\n\u0002\n\u2225\u2207xD\u03c6(x)\u22252\u0003\n+ \u03b3\n2 Ez\u223cpZ\n\u0002\n\u2225\u2207xD\u03c6(G\u03b8(z))\u22252\u0003\n.\n(8)\nThis regularization penalizes the input gradient of D\u03c6 toward\nzero over both real (x) and generated (z) supports. Despite\nits name [16], it is added exclusively to the discriminator\npayoff. By letting g(x; \u03c6) := \u2207xD\u03c6(x) and H\u03c6(x) :=\n\u22072\nx,\u03c6D\u03c6(x), and by direct differentiation,\n\u2207\u03c6BSGP = \u03b3(Ex\u223cpD[H\u03c6(x)\u22a4g(x; \u03c6)]\n+ Ez\u223cpZ[H\u03c6(G\u03b8(z))\u22a4g(G\u03b8(z); \u03c6)]).\n(9)\nUsing\nthe\nchain\nrule\nthrough\nx\u2032\n=\nG\u03b8(z)\ngives\n\u2207\u03b8BSGP\n=\n\u03b3 E\n\u0002\nJ\u03b8G\u03b8(z)\u22a4Hxx(x\u2032)\u22a4g(x\u2032; \u03c6)\n\u0003\n,\nwhere\nHxx(x) := \u22072\nxD\u03c6(x) and J\u03b8G\u03b8(z) is the generator Jaco-\nbian. Note that, although the regularization is added only on\nthe discriminator cost, it affects also the generator blocks.\nRemark 3: The gradients in (8) are with respect to the\ninput x, and not with respect to the parameter \u03c6. This choice\ntargets the function D\u03c6(\u00b7) in the data space: it penalizes\nsharpness of D\u03c6 on the real and generated supports and is\nzero whenever D\u03c6 is locally flat there. Consequently, the\npenalty vanishes at the equilibrium and preserves the target\nsaddle point [16, Rem. D.8]. In particular, since g(\u00b7; \u03c6\u22c6) \u22610\nat the equilibrium, (8) vanishes at the saddle point, thereby\npreserving the equilibrium in (2). This motivates the term\n\u201dzero-centered\u201d, since both its value and its first derivative\nvanish at (\u03b8\u22c6, \u03c6\u22c6). Compared with a parameter penalty (e.g.,\nTikhonov in (6)), the input-gradient penalty acts in data\nspace, it is invariant to simple reparameterizations, and, since\nit injects curvature away from the saddle point, it does not\nbias the equilibrium, i.e., \u03b3 need not be vanishing.\nB. Regularization analysis\nTo understand how the penalties modify the geometry of\nthe saddle field, we examine the curvature they induce on the\ndiscriminator block. Since the local behavior of the saddle\noperator is governed by its Jacobian, to certify explicit Lip-\nschitz continuity and strong\u2013monotonicity constants for the\nregularized mapping, we inspect the second-order structure\nof the proposed penalties.\nFor the curvature analysis on the discriminator block, let\nthe Gauss\u2013Newton (GN) surrogate [19, Sec. 10.3] of the\nHessian be \u22072\n\u03c6\u03c6BSGP \u2248\u03b3 JG(\u03c9), where\nJG(\u03c9) := E\n\u0002\nH\u03c6(x)\u22a4H\u03c6(x) + H\u03c6(x\u2032)\u22a4H\u03c6(x\u2032)\n\u0003\n(10)\nwith x being the real data inputs and x\u2032 being the generated\nsamples inputs.\nAssumption 4 (Gauss\u2013Newton identifiability): There exist\na neighborhood R \u2286S and a constant \u03bb0 > 0 such that\nJG(\u03c9) \u2ab0\u03bb0I, for all \u03c9 \u2208R.\nRemark 4: The Jacobian JF\u03b3 of F\u03b3 in (5) includes off-\ndiagonal mixed blocks stemming from \u22072\n\u03b8\u03c6(\u00b7) terms. These\ndrive rotational dynamics and can affect the desired strong-\nmonotonicity needed for our iterative scheme. Assumption 4\nenforces discriminator-side curvature guaranteeing that the\n\u03c6-block of the symmetric part of the Jacobian has a uniform\npositive bound (see Lemma 1) that offsets the rotational\ncoupling between generator and discriminator.\nAssumption 5 (Continuity): The\ndiscriminator\nD\u03c6\nis\ntwice continuously differentiable in both x and \u03c6 and on\nthe relevant supports of x \u223cpD and x\u2032 = G\u03b8(z), z \u223cpZ.\nLet \u03be be a generic input to indicate either a real sample\nx \u223cpD or a generated sample x\u2032 = G\u03b8(z) with z \u223cpZ.\nAssumption 6 (Bounded mixed Hessian): There exists a\nconstant CH > 0 such that \u2225H\u03c6(\u03be)\u2225\u2264CH for all \u03be \u2208\nsupp(pD) \u222aG\u03b8\n\u0000supp(pZ)\n\u0001\n, where H\u03c6(\u03be) := \u22072\nx,\u03c6D\u03c6(\u03be).\nAs a consequence E[\u2225H\u03c6(x)\u22252] \u2264C2\nH and E[\u2225H\u03c6(x\u2032)\u22252] \u2264\nC2\nH.\nLemma 1 (PSD and uniform bound for the Gramian):\nLet Assumptions 5\u20136 hold. Then JG(\u03c9) \u2ab00 and\n\u2225JG(\u03c9)\u2225\u22642C2\nH =: \u03ba.\n(11)\nProof:\nUsing the definition of JG(\u03c9) in (10), to\nshow that it is PSD, let v \u2208Rd\u03c6; then v\u22a4JG(\u03c9) v =\nE\n\u0002\n\u2225H\u03c6(x)v\u22252 + \u2225H\u03c6(x\u2032)v\u22252\u0003\n\u22650, hence JG(\u03c9) \u2ab00.\nThe bound in (11) instead follows by using linearity of\nexpectation and the triangle inequality:\n\u2225JG(\u03c9)\u2225= \u2225E[H\u03c6(x)\u22a4H\u03c6(x)] + E[H\u03c6(x\u2032)\u22a4H\u03c6(x\u2032)]\u2225\n\u2264\u2225E[H\u03c6(x)\u22a4H\u03c6(x)]\u2225+ \u2225E[H\u03c6(x\u2032)\u22a4H\u03c6(x\u2032)]\u2225\n\u2264E[\u2225H\u03c6(x)\u22a4H\u03c6(x)\u2225] + E[\u2225H\u03c6(x\u2032)\u22a4H\u03c6(x\u2032)\u2225].\nSince the spectral norm is convex, by Jensen inequality,\n\u2225JG(\u03c9)\u2225\u2264E[\u2225H\u03c6(x)\u22252] + E[\u2225H\u03c6(x\u2032)\u22252]. Finally by As-\nsumption 6 \u2225JG(\u03c9)\u2225\u2264C2\nH + C2\nH = 2C2\nH =: \u03ba. which\nproves the claim.\nWe can now show Lipschitz continuity and strong mono-\ntonicity of the reguralized operator in (5).\nProposition 1 (Lipschitz constant of F\u03b3): Under\nAssumptions 1, 2, 5 and 6, F\u03b3 in (5) is L-Lipschitz\ncontinous on S with\nL = L0 + \u03b3 \u03batot,\n\u03batot \u2208[\u03ba, \u03ba + \u02dc\u03ba\u03b8].\n(12)\nwhere \u03ba is as in (11), and \u02dc\u03ba\u03b8 \u22650.\nAlgorithm 1 Forward\u2013Backward\nRequire: initial \u03c90 \u2208R, stepsize/preconditioner P \u227b0\n1: for k = 0, 1, 2, . . . do\n2:\n\u03c9k+1 \u2190\u03a0R\n\u0000\u03c9k \u2212P F\u03b3(\u03c9k)\n\u0001\n3: end for\nAlgorithm 2 ExtraGradient\nRequire: initial \u03c90 \u2208R, stepsize/preconditioner P \u227b0\n1: for k = 0, 1, 2, . . . do\n2:\n\u02dc\u03c9k \u2190\u03a0R\n\u0000\u03c9k \u2212P F\u03b3(\u03c9k)\n\u0001\n\u25b7look-ahead\n3:\n\u03c9k+1 \u2190\u03a0R\n\u0000\u03c9k \u2212P F\u03b3(\u02dc\u03c9k)\n\u0001\n4: end for\nProof:\nBy Assumption 2, the operator F is L0-\nLipschitz on S. Therefore, it suffices to bound the Lips-\nchitz constant of the regularization contribution \u2207B(\u03c9) =\n( \u2212\u2207\u03b8B(\u03c9), \u2207\u03c6B(\u03c9) )T .\n(i) Discriminator-side contribution. For BTik in (6), we have\n\u2207\u03c6BTik(\u03c9) = \u03b3 \u03c6, whose Jacobian w.r.t. \u03c6 is \u03b3I; thus\n\u2225\u2207\u03c6BTik(\u03c9) \u2212\u2207\u03c6BTik(\u03c9\u2032)\u2225\u2264\u03b3 \u2225\u03c9 \u2212\u03c9\u2032\u2225.\nFor BSGP in (9), by the Gauss\u2013Newton surrogate in\n(10) and Lemma 1, the \u03c6-Jacobian of \u2207\u03c6BSGP satisfies\n\u22072\n\u03c6\u03c6BSGP(\u03c9)\n\u2248\n\u03b3 \u2225JG(\u03c9)\u2225\n\u2264\n\u03b3 \u03ba, with \u03ba as in\n(11). By the mean-value theorem for vector fields,\n\u2225\u2207\u03c6BSGP(\u03c9) \u2212\u2207\u03c6BSGP(\u03c9\u2032)\u2225\u2264\u03b3 \u03ba \u2225\u03c9 \u2212\u03c9\u2032\u2225.\nThus, in both BTik and BSGP cases, the discriminator-side\ncontribution is \u03b3 \u03ba-Lipschitz (with \u03ba = 1 for BTik and \u03ba\nfrom (11) for BSGP).\n(ii) Generator-side contribution. For BTik in (6) we have\n\u2207\u03b8BTik \u22610, hence \u02dc\u03ba\u03b8 = 0. For BSGP in (8) we have\n\u2207\u03b8BSGP(\u03c9) = \u03b3 E\n\u0002\nJ\u03b8G\u03b8(z)\u22a4Hxx(x\u2032)\u22a4g(x\u2032; \u03c6)\n\u0003\n. Under\nAssumptions 5\u20136 and bounded supports/parameters, J\u03b8G\u03b8,\nHxx and g are uniformly bounded and Lipschitz continuous\n(see Remark 6). Therefore, \u2207\u03b8BSGP is Lipschitz continuous\nwith some finite constant, i.e., \u02dc\u03ba\u03b8 < \u221e.\nCombining (i) and (ii), the regularization term is \u03b3(\u03ba+\u02dc\u03ba\u03b8)-\nLipschitz continuous. We have \u2225F\u03b3(\u03c9)\u2212F\u03b3(\u03c9\u2032)\u2225\u2264\n\u0000L0 +\n\u03b3 \u03ba + \u03b3 \u02dc\u03ba\u03b8\n\u0001\n\u2225\u03c9 \u2212\u03c9\u2032\u2225. Since \u02dc\u03ba\u03b8 \u22650, this yields (12) with\n\u03batot \u2208[\u03ba, \u03ba + \u02dc\u03ba\u03b8].\nRemark 5: The quantity \u02dc\u03ba\u03b8 in (12) is nonnegative by\ndefinition and it captures the \u03b8-dependence of B. In the case\nof the Tikhonov regularization in (6), \u02dc\u03ba\u03b8 = 0. For the SGP\nregularization in (8), \u02dc\u03ba\u03b8 > 0 whenever the generator Jacobian\nJ\u03b8G\u03b8 and the mixed Hessian H\u03c6 are uniformly bounded on\nthe supports of (x, z). This happens when x and z range\nover bounded sets (e.g., x \u2208[0, 1]dx, z \u2208[\u2212c, c]dz), G\u03b8(Z)\nis bounded, the parameter sets \u0398, \u03a6 are bounded, and G\u03b8 and\nD\u03c6 are twice continuously differentiable on their domains.\nProposition 2 (Strong monotonicity of F\u03b3): Under\nAssumptions 3 and 4, the Jacobian JF\u03b3 of the regularized\nmap F\u03b3 in (5) satisfies\nJF\u03b3(\u03c9) + JF\u03b3(\u03c9)\u22a4\n2\n\u2ab0diag\n\u0000\u00b5\u03b8I, (\u00b5\u03c6 + \u03b3\u03bb0)I\n\u0001\n(13)\nAlgorithm 3 EFTP\nRequire: initial \u03c90 \u2208R, stepsize/preconditioner P \u227b0\n1: y0 \u2190\u03c90;\nbF0 \u2190F\u03b3(y0) \u25b7warm-start: one oracle call\n2: for k = 0, 1, 2, . . . do\n3:\n\u03c9k+1 \u2190\u03a0R\n\u0000\u03c9k \u2212P bFk\n\u0001\n4:\nyk+1 \u2190\u03a0R\n\u0000\u03c9k+1 \u2212P bFk\n\u0001\n5:\nbFk+1 \u2190F\u03b3(yk+1)\n\u25b7one oracle call per loop\n6: end for\non R \u2286S. Therefore, F\u03b3 is \u00b5-strongly monotone on R with\n\u00b5 \u2265min{\u00b5\u03b8, \u00b5\u03c6 + \u03b3\u03bb0}.\n(14)\nProof: Let F\u03b3 be as in (5). Then, its Jacobian is\nJF\u03b3(\u03c9) =\n\u0014\nA\nBmix\n\u2212B\u22a4\nmix\nC\n\u0015\n\u03c9,\n(15)\nwhere A = \u22072\n\u03b8\u03b8L\u2212\u22072\n\u03b8\u03b8B, Bmix = \u22072\n\u03b8\u03c6L\u2212\u22072\n\u03b8\u03c6B and C =\n\u2212\u22072\n\u03c6\u03c6L + \u22072\n\u03c6\u03c6B. By Schwarz symmetry, the symmetric\npart is block diagonal:\nJF\u03b3(\u03c9)+JF\u03b3(\u03c9)\u22a4\n2\n= diag(\u22072\n\u03b8\u03b8L \u2212\n\u22072\n\u03b8\u03b8B, \u2212\u22072\n\u03c6\u03c6L + \u22072\n\u03c6\u03c6B). By Assumption 3, \u22072\n\u03b8\u03b8L \u2ab0\u00b5\u03b8I\nand \u2212\u22072\n\u03c6\u03c6L \u2ab0\u00b5\u03c6I. We consider the two regularization\nseparately.\na) For Tikhonov regularization, \u22072\n\u03c6\u03c6BTik\n=\n\u03b3I and\n\u22072\n\u03b8\u03b8BTik\n\u2261\n0,\nhence\non\nany\nregion\nin\nR,\nJF\u03b3(\u03c9)+JF\u03b3(\u03c9)\u22a4\n2\n\u2ab0diag\n\u0000\u00b5\u03b8I, (\u00b5\u03c6 + \u03b3)I\n\u0001\n.\nb) For BSGP, using the Gauss\u2013Newton surrogate in (10)\nand Assumption 4, \u22072\n\u03c6\u03c6BSGP \u2ab0\u03b3 JG(\u03c9) \u2ab0\u03b3\u03bb0I on\nR, hence\nJF\u03b3+JF \u22a4\n\u03b3\n2\n\u2ab0diag\n\u0000\u00b5\u03b8I, (\u00b5\u03c6 + \u03b3\u03bb0)I\n\u0001\n, on\nR \u2282S which is exactly (13).\nTherefore F\u03b3 is \u00b5-strongly monotone on R with \u00b5 \u2265\nmin{\u00b5\u03b8, \u00b5\u03c6 + \u03b3\u03bb0} [3, Proposition 2.3.2.(c)].\n\u25a1\nRemark 6: If \u00b5\u03b8\n=\n0 in Assumption 3, Proposition\n2 implies only \u00b5 \u22650 i.e., the operator F\u03b3 might be\nmonotone. Adding generator-side curvature, e.g., a Tikhonov\nterm \u03b3\n2 \u2225\u03b8\u22252 as in (7), the right-hand side of Equation (13)\nbecomes diag\n\u0000(\u00b5\u03b8 + \u03c4)I, (\u00b5\u03c6 + \u03b3\u03bb0)I\n\u0001\nand gives \u00b5\n\u2265\nmin{\u00b5\u03b8+\u03c4, \u00b5\u03c6+\u03b3\u03bb0} > 0, retrieving strong monotonicity.\nIII. ALGORITHMS AND CONVERGENCE\nWe consider three projected first-order methods to find a\nNash equilibrium of our saddle problem: Forward\u2013Backward\n(FB), ExtraGradient (EG), and Extrapolation from the Past\n(EFTP). The details of these iterative schemes are detailed\nin Algorithms 1, 2 and 3, respectively. Note that we use the\nregularized operator in (5) for these iterative schemes.\nFB is the classical projected gradient step, taking a single\nprojection along \u2212F\u03b3(\u03c9k) [9]. EG stabilizes the dynamics\nby evaluating at a look-ahead point \u02dc\u03c9k and updating with\nF\u03b3(\u02dc\u03c9k), which helps cancel rotational drift on monotone\nsaddle fields [10]. EFTP achieves similar stabilization while\nreusing a stored evaluation: after a warm start with one oracle\ncall, it needs only a single gradient call per iteration, reducing\ncomputational cost [8].\nA. Convergence analysis\nLet L be the Lipschitz constant of F\u03b3 on S and \u00b5 its\n(strong)-monotonicity constant on the region R \u2286S as in\nLemma 1 and Proposition 1. We recall that, according to\nRemark 6, \u00b5 might be 0.\nAssumption 7 (Stepsizes): Let R \u2286S and P \u2208Md\n++, the\nstep size sequences are such that for FB, \u2225P\u22252 < 2\u00b5\u03bbmin(P )\nL2\n;\nfor EG, \u2225P\u2225\u22641\nL; for EFTP, \u2225P\u2225\u2264\n1\n4L .\nRemark 7 (Scalar case): If P = \u03b7Id, then the bounds\nreduce to 0 < \u03b7 <\n2\u00b5\nL2 (FB), 0 < \u03b7 \u2264\n1\nL (EG), and 0 <\n\u03b7 \u2264\n1\n2L (EFTP). Using the explicit bound L = L0 + \u03b3 \u03batot\nfrom Proposition 1 produces a computable step-sizes.\nWe are now ready to state the convergence result for our\niterative schemes.\nTheorem 1 (FB vs. EG): Let Assumptions 1, 2 and 7\nhold.\n1) If F\u03b3 is strongly monotone (\u00b5 > 0) on R, then\nAlgorithm 1 converges to a solution \u03c9\u22c6of (4). The\nconvergence is Q-linear.\n2) If F\u03b3 is monotone (\u00b5 = 0) on S, then Algorithm 2\nconverges to a solution \u03c9\u22c6of (4). If, in addition, F\u03b3 is\nstrongly monotone on R, the convergence is Q-linear.\nProof:\n1) Consider the fixed-point map T(\u03c9)\n=\n\u03a0S\n\u0000\u03c9 \u2212P \u22121F\u03b3(\u03c9)\n\u0001\n. By [3, Thm. 12.1.2], and since F\u03b3 is\n\u00b5-strongly monotone and L-Lipschitz continuous on R, T is\na contraction in the P \u22121-induced norm. Therefore, \u03c9k+1 =\nT(\u03c9k) converges to the unique solution of VI(S, F\u03b3), and the\nBanach contraction theorem provides Q-linear convergence.\n2) If F\u03b3 is monotone on S and L-Lipschitz continuous,\nAlgorithm 2 generates a Fej\u00b4er-monotone sequence with re-\nspect to the solution set SOL(S, F\u03b3) and hence converges\nto a solution of VI(S, F\u03b3) [3, Thm 12.1.1], [13, \u00a75.8].\nIf, in addition, F\u03b3 is (locally) strongly monotone on R,\nthen EG converges Q-linearly for sufficiently small \u03b7 [3,\nThm. 12.6.4].\nRemark 8 (Region of validity R): Throughout\nSections\nII\u2013III, R \u2286S denotes a convex neighborhood of a solution\n\u03c9\u22c6on which our regularity assumptions hold and where\nthe\nconstants\nL\n(Lipschitz\ncontinuity)\nand\n\u00b5\n(strong\nmonotonicity) are valid. All convergence statements for FB,\nEG, and EFTP are made on R. If R = S, the results are\nglobal; otherwise they are local and require an initial point\n\u03c90 \u2208R.\nWe state the convergence of Algorithm 3 as a separate\nresult since, as far as we know, it is a new scheme for games.\nTheorem 2 (EFTP): Let Assumptions 1-7 hold.\n1) If F\u03b3 is monotone (\u00b5 = 0) on R, then Algorithm 3\nconverges to a solution \u03c9\u22c6of (4).\n2) If F\u03b3 is strongly monotone (\u00b5 > 0) on R, then\nAlgorithm 3 is a contraction on R and \u2225\u03c9k+1 \u2212\u03c9\u22c6\u2225\u2264\n\u2225\u03c9k \u2212\u03c9\u22c6\u2225with q \u2208(0, 1). Consequently, Algorithm 3\nconverges Q-linearly to a solution \u03c9\u22c6of (4). Moreover,\nthe number of iterations to reach \u2225\u03c9k+1 \u2212\u03c9\u22c6\u2225\u2264\u03b5\nscales as O(log(1/\u03b5)).\nProof:\nLet \u03c9\u22c6\u2208SOL(S, F\u03b3) be the (locally) unique\nsolution in R. Then, for 1), convergence follows mutatis\n\u22125\n0\n5\n10\n\u03b8\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n\u03d5\n(a) FB diverges (\u03b3=0)\nFB (iters=80)\n\u22120.15\n\u22120.10\n\u22120.05\n0.00\n0.05\n\u03b8\n\u22120.05\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n\u03d5\n(b) FB converges (\u03b3=2)\nFB (iters=57)\n\u22120.04\n\u22120.02\n0.00\n\u03b8\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n\u03d5\n(c) EG (\u03b3=2)\nEG (iters=110)\n\u22120.075\u22120.050\u22120.025 0.000 0.025\n\u03b8\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n\u03d5\n(d) EFTP (\u03b3=2)\nEFTP (iters=160)\nFig. 1: Trajectories with discriminator-only curvature: (a) FB\ndiverges for \u03b3 = 0; (b) FB converges for \u03b3 = 2; (c) EG\n(\u03b3 = 2); (d) EFTP (\u03b3 = 2). Blue dots indicate the starting\npoint and squares the end points; the star is the solution. The\nlegends report iteration till convergence.\nmutandis from [8, Theorem 1]. In particular, our conservative\nstep size \u2225P\u2225\u2264\n1\n4L\nensures the bound \u2225P\u2225\u2264\n1\n3L\nused\nin [8]. If, instead, we instead have strong monotonicity, anal-\nogously to [14, Theorem\u2019 1] which shows \u2225\u03c9k+1 \u2212\u03c9\u22c6\u2225\u2264\n\u00001 \u2212\n\u00b5\n4L\n\u0001\n\u2225\u03c9k \u2212\u03c9\u22c6\u2225leading to q \u2208(0, 1) by Assumption 7.\nAccordingly, by [3, Thm. 2.1.21(c)], the number of iterations\nto reach \u2225\u03c9k+1 \u2212\u03c9\u22c6\u2225\u2264\u03b5 scales as O(log(1/\u03b5)).\nIV. NUMERICAL SIMULATIONS\nTo illustrate our asymmetric design mechanism, we study\na bilinear toy example. We assume the discriminator is linear\nin the input, and specialize the GAN loss to L(\u03b8, \u03c6) =\nEx\u223cpD[\u03c6\u22a4x] \u2212Ez\u223cpZ[\u03c6\u22a4G\u03b8(z)] = \u03c6\u22a4\u0000E[x] \u2212E[G\u03b8(z)]\n\u0001\n.\nIn particular, we set E[x] = 0 and E[G\u03b8(z)] = \u2212a\u03b8\nwith a > 0. Since the discriminator is linear in its input,\ng(x; \u03c6) = \u2207xD\u03c6(x) = \u03c6 is constant in x. In this case, the\nzero-centered input-gradient penalty simplifies to\nBSGP(\u03b8, \u03c6) = \u03b3\n2 E\n\u0002\n\u2225g(x; \u03c6)\u22252 +\u2225g(G\u03b8(z); \u03c6)\u22252\u0003\n= \u03b3\n2 \u2225\u03c6\u22252,\nwhich exactly matches the Tikhonov regularization in (6).\nWith \u03c9 = (\u03b8, \u03c6)\u22a4\u2208R2, the associated saddle operator is\nF\u03b3(\u03c9) =\n\u0014\na \u03c6\n\u2212a \u03b8 + \u03b3 \u03c6\n\u0015\n,\nJF\u03b3 =\n\u0014 0\na\n\u2212a\n\u03b3\n\u0015\n.\n(16)\nNote that F\u03b3 is monotone for \u03b3 \u22650, but not strongly\nmonotone [3, Sec. 2.3]. We implemented (16) with a > 0\nand varied \u03b3 \u22650. For stepsize we used Assumption 7 with\nL as in Proposition 1. Under this premises, FB exhibits\noscillatory behavior and can stall unless \u03b3 is large enough\nand the stepsize is tuned conservatively, i.e., according to\nAssumption 7. This can be observed in Figure 1(a), with\n\u03b3 = 0 and Figure 1(b) with \u03b3 = 2. For EG and EFTP\ninstead, the second gradient step cancels the rotational drift\n0\n50\n100\n150\n200\nIteration\n10\u22129\n10\u22123\n103\n109\n1015\n1021\n1027\n1033\nResidual\nFB, \u03b3=0.0\nEG, \u03b3=0.0\nEFTP, \u03b3=0.0\nFB, \u03b3=0.5\nEG, \u03b3=0.5\nEFTP, \u03b3=0.5\nFB, \u03b3=2.0\nEG, \u03b3=2.0\nEFTP, \u03b3=2.0\nFig. 2: Residual vs. iterations on the bilinear toy for \u03b3 =\n{0, 0.5, 2}.\n0\n100\n200\n300\n400\nOperator evaluations of F\n10\u22129\n10\u22123\n103\n109\n1015\n1021\n1027\n1033\nResidual\nFB, \u03b3=0.0\nEG, \u03b3=0.0\nEFTP, \u03b3=0.0\nFB, \u03b3=0.5\nEG, \u03b3=0.5\nEFTP, \u03b3=0.5\nFB, \u03b3=2.0\nEG, \u03b3=2.0\nEFTP, \u03b3=2.0\nFig. 3: Residual as a function of gradient evaluations.\nand results in convergence (Figure 1(c)-(d)). Moreover, EFTP\nwith one oracle call per half step matches the stabilizing\neffect of EG on monotone fields while using stored gradients\nhence while being computationally lighter.\nThe residual plots in Figures 2\u20133 show how \u03b3 stabilizes\nthe adversarial dynamics: FB diverges for \u03b3 = 0, slows\nfor moderate curvature, and contracts linearly for \u03b3 = 2.\nFor all \u03b3, the field remains monotone but not strongly\nmonotone, so only look-ahead methods (EG, EFTP) achieve\nconvergence. In particular, EG and EFTP exhibit stable, near-\nlinear decrease that improves with larger \u03b3, whereas FB is\nunstable for small curvature and stabilizes as \u03b3 increases.\nConcerning the computational burden (Figure 3), EG uses\ntwo gradient steps per iteration and EFTP only one per half\nstep: both eliminate the rotational drift but EFTP achieves\nsimilar performances with fewer gradient calls.\nV. CONCLUSION\nWe studied GAN training through a VI formulation and\nintroduced a block-asymmetric, zero-centered input-gradient\npenalty applied to the discriminator only. This design pre-\nserves the target saddle point while certifiably improving\nconditioning. Under smoothness and a local Gauss\u2013Newton\n(GN) identifiability condition, we derived explicit Lipschitz\ncontinuity and strong-monotonicity constants for the regu-\nlarized operator and established linear convergence of EFTP\nscheme. Extending the analysis to stochastic oracles with\nvariance reduction or adaptive block preconditioners tasks\nwould further test the asymmetric design.\nREFERENCES\n[1] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial net-\nworks,\u201d Communications of the ACM, vol. 63, no. 11, pp. 139\u2013144,\n2020.\n[2] W. Li, X. Zeng, and L. Pavel, \u201cPrimal-dual accelerated mirror-\ndescent method for constrained bilinear saddle-point problems,\u201d IEEE\nTransactions on Automatic Control, 2025.\n[3] F. Facchinei and J.-S. Pang, Finite-dimensional variational inequalities\nand complementarity problems.\nSpringer, 2003.\n[4] D. Goldsztajn and F. Paganini, \u201cProximal regularization for the saddle\npoint gradient dynamics,\u201d IEEE Transactions on Automatic Control,\nvol. 66, no. 9, pp. 4385\u20134392, 2020.\n[5] Y. Huang, Z. Meng, J. Sun, and W. Ren, \u201cA unified distributed method\nfor constrained networked optimization via saddle-point dynamics,\u201d\nIEEE Transactions on Automatic Control, vol. 69, no. 3, pp. 1819\u2013\n1826, 2024.\n[6] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C.\nCourville, \u201cImproved training of wasserstein gans,\u201d Advances in neural\ninformation processing systems, vol. 30, 2017.\n[7] A. Jolicoeur-Martineau, \u201cThe relativistic discriminator: A key element\nmissing from standard gan,\u201d arXiv preprint arXiv:1807.00734, 2018.\n[8] L. D. Popov, \u201cA modification of the arrow-hurwicz method for search\nof saddle points,\u201d Mathematical notes of the Academy of Sciences of\nthe USSR, vol. 28, pp. 845\u2013848, 1980.\n[9] B. Franci and S. Grammatico, \u201cForward\u2013backward algorithms for\nstochastic nash equilibrium seeking in restricted strongly and strictly\nmonotone games,\u201d IEEE Transactions on Automatic Control, vol. 67,\nno. 9, pp. 4881\u20134893, 2022.\n[10] G. M. Korpelevich, \u201cThe extragradient method for finding saddle\npoints and other problems,\u201d Matecon, vol. 12, pp. 747\u2013756, 1976.\n[11] A. N. Tikhonov, A. V. Goncharsky, V. V. Stepanov, and A. G. Yagola,\nRegularization methods.\nDordrecht: Springer Netherlands, 1995, pp.\n7\u201363.\n[12] F. Fabiani and B. Franci, \u201cA stochastic generalized nash equilibrium\nmodel for platforms competition in the ride-hail market,\u201d in 2022 IEEE\n61st Conference on Decision and Control (CDC).\nIEEE, 2022, pp.\n4455\u20134460.\n[13] E. K. Ryu and S. Boyd, \u201cPrimer on monotone operator methods,\u201d\nApplied and Computational Mathematics, vol. 15, no. 1, pp. 3\u201343,\n2016.\n[14] G. Gidel, H. Berard, G. Vignoud, P. Vincent, and S. Lacoste-Julien, \u201cA\nvariational inequality perspective on generative adversarial networks,\u201d\narXiv preprint arXiv:1802.10551, 2018.\n[15] L. Mescheder, S. Nowozin, and A. Geiger, \u201cThe numerics of gans,\u201d\nAdvances in neural information processing systems, vol. 30, 2017.\n[16] L. Mescheder, A. Geiger, and S. Nowozin, \u201cWhich training methods\nfor gans do actually converge?\u201d in International conference on ma-\nchine learning.\nPMLR, 2018, pp. 3481\u20133490.\n[17] V. Nagarajan and J. Z. Kolter, \u201cGradient descent gan optimization is\nlocally stable,\u201d Advances in neural information processing systems,\nvol. 30, 2017.\n[18] A. Shapiro, \u201cSensitivity analysis of parameterized variational inequali-\nties,\u201d Mathematics of Operations Research, vol. 30, no. 1, pp. 109\u2013126,\n2005.\n[19] J. Nocedal and S. J. Wright, Numerical Optimization, 2nd ed.\nSpringer, 2006."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper\n\n**Title:** Asymmetric Regularization Mechanism for GAN Training with Variational Inequalities\n\n#### Main Contributions:\n1. **Formulation as a Nash Equilibrium Problem:** The authors formulate the training of Generative Adversarial Networks (GANs) as a Nash equilibrium seeking problem, which is well-studied in literature and can be modeled as a saddle-point problem.\n\n2. **Asymmetric Regularization Mechanism:** They propose an asymmetric regularization mechanism that applies only to the discriminator side, preserving the target equilibrium point while improving conditioning and introducing curvature where needed.\n\n3. **Explicit Constants for Convergence:** Under smoothness and local Gauss-Newton identifiability conditions, they derive explicit Lipschitz continuity and strong-monotonicity constants for the regularized operator, ensuring convergence guarantees for a single-call Extrapolation-from-the-Past (EFTP) method.\n\n4. **Empirical Validation:** Empirical simulations on an academic example show that even when strong monotonicity cannot be achieved, the asymmetric regularization is sufficient to converge to an equilibrium and stabilize the trajectory of training dynamics.\n\n#### Methodology:\n1. **VI Formulation:** The authors adopt a variational inequality (VI) viewpoint to expose the saddle operator connected to the GAN game and its geometry, handle constraints via projections in a principled way, and directly connect to convergence guarantees for various methods.\n\n2. **Regularization Design:** They introduce two forms of regularization:\n   - **Tikhonov Regularization (Tik):** Adds curvature only on the discriminator block.\n   - **Symmetric Gradient Penalty (SGP):** Penalizes the input gradient of D\u03c6 toward zero over both real and generated supports.\n\n3. **Operator Analysis:** By examining the curvature induced by these penalties, they derive explicit Lipschitz continuity and strong-monotonicity constants for the regularized operator under smoothness and local Gauss-Newton identifiability conditions.\n\n4. **Convergence Guarantees:** They establish convergence guarantees for a single-call EFTP method using these constants.\n\n#### Key Findings:\n1. **Stabilization of Training Dynamics:** The asymmetric regularization mechanism stabilizes the rotational dynamics often seen in adversarial training, making the training process more reliable and stable.\n2. **Explicit Constants:** By deriving explicit Lipschitz continuity and strong-monotonicity constants for the regularized operator, they provide a clear basis for understanding and controlling the convergence behavior of their proposed methods.\n\n#### Conclusions:\n1. **Block-Asymmetric Design:** The authors propose a block-asymmetric regularization mechanism applied only to the discriminator side, which preserves the target equilibrium point while improving conditioning.\n2. **Convergence Guarantees:** Under smoothness conditions, they show that their EFTP method converges linearly to a Nash equilibrium in the presence of asymmetric regularization.\n3. **Empirical Validation:** Empirical simulations demonstrate that even without strong monotonicity, the proposed regularization mechanism can still converge and stabilize the training process.\n\n#### Future Work:\n1. **Extension to Stochastic Oracles:** The authors suggest extending their analysis to stochastic oracles with variance reduction or adaptive block preconditioners.\n2. **Generalization to n-Player Games:** They indicate that their results can be generalized to symmetric regularization, thus extending the applicability of their work beyond GANs.\n\nThis summary encapsulates the main contributions, methodology, key findings, and conclusions of the paper on asymmetric regularization mechanisms for GAN training using variational inequalities."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "PREGEN:\nUNCOVERING\nLATENT\nTHOUGHTS\nIN\nCOMPOSED VIDEO RETRIEVAL\nGabriele Serussi\u2217,1,2, David Vainshtein*,1, Jonathan Kouchly*,1\nDotan Di Castro1, Chaim Baskin2\n1Bosch Center for AI, Israel\n2INSIGHT Lab, Ben-Gurion University of the Negev, Israel\nABSTRACT\nComposed Video Retrieval (CoVR) aims to retrieve a video based on a query\nvideo and a modifying text. Current CoVR methods fail to fully exploit modern\nVision-Language Models (VLMs), either using outdated architectures or requiring\ncomputationally expensive fine-tuning and slow caption generation. We introduce\nPREGEN (PRE GENeration extraction), an efficient and powerful CoVR frame-\nwork that overcomes these limitations. Our approach uniquely pairs a frozen,\npre-trained VLM with a lightweight encoding model, eliminating the need for any\nVLM fine-tuning. We feed the query video and modifying text into the VLM\nand extract the hidden state of the final token from each layer. A simple encoder\nis then trained on these pooled representations, creating a semantically rich and\ncompact embedding for retrieval. PREGEN significantly advances the state of the\nart, surpassing all prior methods on standard CoVR benchmarks with substantial\ngains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness\nacross different VLM backbones and exhibits strong zero-shot generalization to\nmore complex textual modifications, highlighting its effectiveness and semantic\ncapabilities.\n(a) Other VLM-based embedding methods\n(b) PREGEN (Ours)\nFigure 1: Comparison of other CoVR approaches, which finetune the VLM and use only the last\nlayer embedding, and PREGEN, which freezes the VLM and aggregates embeddings from all layers.\n1\nINTRODUCTION\nVideo understanding has been a fundamental challenge in computer vision, with early work focus-\ning on action recognition (Wang et al., 2011; Wang & Schmid, 2013), video classification (Karpathy\net al., 2014; Yue-Hei Ng et al., 2015), and object detection (Girshick et al., 2014; Redmon et al.,\n2016). As video content continues to grow across digital platforms, the need for effective video\nretrieval has become increasingly important. However, traditional keyword-based and visual sim-\nilarity approaches face challenges in effectively capturing complex user intents. Composed Video\n\u2217Equal Contributor.\n1\narXiv:2601.13797v1  [cs.CV]  20 Jan 2026\nRetrieval (CoVR) (Ventura et al., 2024a) addresses this challenge by defining the task of retrieving\na target video given both a reference video and a modifying text query, where a query expresses a\nspecific modification to a reference video, such as \u201dfind a video similar to this cooking demo, but\nwith a different cuisine.\u201d\nCoVR builds upon Composed Image Retrieval (CIR), which established the foundational approach\nof using both visual and textual inputs for retrieval tasks. Early work like TIRG (Vo et al., 2018)\ndeveloped image-text fusion architectures for CIR. Fashion-IQ (Wu et al., 2020) introduced the\nfirst CIR dataset with human-written relative captions, and demonstrated that combining reference\nimages with natural language modifications enables more effective image retrieval. Subsequent\nwork expanded CIR to general domains with datasets like CIRR (Liu et al., 2021). Recent zero-shot\napproaches like Pic2Word (Saito et al., 2023) and LinCIR (Gu et al., 2024) have leveraged vision-\nlanguage models such as CLIP (Radford et al., 2021) for composed retrieval without task-specific\ntraining.\nHowever, the transition from images to videos introduces substantial complexity that existing CIR\nmethods fail to address (Nguyen et al., 2024). Unlike static images, videos contain temporal se-\nquences with dynamic scene changes, object interactions, and narrative progressions that demand\nprocessing significantly richer semantic content (Simonyan & Zisserman, 2014; Goyal et al., 2017).\nThese challenges require novel architectures that can efficiently capture both the temporal dynamics\nof video content and the nuanced modifications expressed in natural language (Nguyen et al., 2024;\nXu et al., 2021).\nModern Vision-Language Models (VLMs) like LLaVA (Liu et al., 2023) and Qwen-VL (Bai et al.,\n2023) have demonstrated remarkable capabilities in understanding complex visual scenes and their\nrelationships to natural language descriptions. VLMs are trained on vast amounts of image-text and\nvideo-text pairs, and possess rich world knowledge about objects, scenes, actions, and their interac-\ntions. These strengths make VLMs a natural foundation for CoVR, which requires understanding\nhow textual modifications relate to video content.\nWhile significant progress has been made in curating large-scale datasets for CoVR, including\nWebVid-CoVR (Ventura et al., 2024a), FineCVR (Yue et al., 2025), and Dense WebVid-CoVR\n(Thawakar et al., 2025), the field faces a significant methodological gap. Current approaches fail to\nfully exploit the rich world knowledge encoded in modern VLMs. Existing methods either use out-\ndated models based on CLIP-style (Radford et al., 2021) joint-encoders (Yue et al., 2025; Hummel\net al., 2024), require extensive fine-tuning of large models (Kong et al., 2025), or rely on compu-\ntationally expensive caption generation processes that create bottlenecks for practical deployment\n(Thawakar et al., 2024; Hummel et al., 2024). This autoregressive caption generation approach\nbecomes particularly costly when scaling to large video databases with millions of videos.\nTo address these limitations, we introduce PREGEN (PRE GENeration extraction), a novel frame-\nwork that efficiently leverages frozen VLMs for CoVR without requiring fine-tuning or caption\ngeneration. Our key insight is that hidden states from the final token across all VLM layers con-\ntain complementary semantic information that, when properly aggregated, creates powerful embed-\ndings for video retrieval. We extract these multi-layer representations and process them through a\nlightweight Transformer encoder to produce compact yet semantically rich embeddings. Figure 1\nillustrates our approach compared to other VLM-based methods.\nOur contribution.\n\u2022 We observe that existing VLM-based embedding methods rely on single-layer representa-\ntions. This approach results in embeddings that fail to capture the full knowledge encoded\nacross the VLM.\n\u2022 Motivated by this observation, we propose PREGEN, an efficient embedding framework\nthat extracts hidden states from the final token across all VLM layers and aggregates them\nusing a lightweight Transformer encoder. This approach makes better use of VLM knowl-\nedge while avoiding expensive fine-tuning or caption generation.\n\u2022 We introduce a training strategy that precomputes hard negative batches by grouping\nqueries whose reference videos come from the same source. This approach preserves the\nbenefits of hard negatives while eliminating the costly process of online similarity search.\n2\n\u2022 We achieve significant state-of-the-art improvements with substantial performance gains.\nWe conduct an extensive ablation study to empirically validate the sources of our strong\nresults, demonstrating the effectiveness of our multi-layer pooling approach.\n2\nRELATED WORK\nComposed Image Retrieval (CIR).\nComposed Image Retrieval (CIR) has emerged as a funda-\nmental task in multimodal information retrieval. In CIR, the goal is to search for target images using\na composition of a reference image and a text modifier that describes desired changes.Wu et al.\n(2020) and ComposeAE (Anwaar et al., 2021) explored image-text fusion architectures and estab-\nlished early methods for learning joint image-text embeddings for CIR. More recent methods, such\nas InstructCIR (Zhong et al., 2024) and MagicLens (Zhang et al., 2024a), have utilized VLMs to\nhandle more complex and nuanced compositional modifications and achieve zero-shot performance\nwithout task-specific training.\nComposed Video Retrieval (CoVR).\nThe task of CoVR was introduced by Ventura et al. (2024a),\nwho released the first CoVR dataset, WebVid-CoVR, and established initial baselines using naive\nframe sampling paired with CLIP-based encoders. Notably, this simple approach struggles at cap-\nturing important temporal information, due to the static nature of CLIP-like architectures. Yue et al.\n(2025) used temporal pooling mechanisms and demonstrated improvements in temporal understand-\ning. Notably, they also intoduced FineCVR, a more complex CoVR dataset where modifications\nrequire a deeper understanding of video dynamics. This improves on WebVid-CoVR, where queries\ncan often be resolved using a single frame. Hummel et al. (2024) proposed a caption-based method\nthat generates intermediate text descriptions before matching. However, this approach creates com-\nputational bottlenecks of slow autoregressive text generation. UNITE (Kong et al., 2025) fine-tuned\nVLMs for multimodal retrieval tasks, using LoRA (Hu et al., 2022) to achieve competitive results\nwithout full retraining. Still, most current approaches either rely on outdated CLIP architectures or\nrequire expensive computational resources, leaving significant room for more efficient methods that\nfully leverage modern VLM capabilities.\nVision Language Models (VLMs).\nVLMs have revolutionized multimodal understanding by\nlearning unified representations across visual and textual modalities. Early approaches like CLIP\n(Radford et al., 2021) and BLIP (Li et al., 2022) introduced dual-encoder and encoder-decoder archi-\ntectures for cross-modal alignment and understanding. Modern VLM work has increasingly moved\ntoward generative architectures that demonstrate superior reasoning capabilities. Modern generative\nVLMs such as the LLaVA series (Liu et al., 2023; Zhang et al., 2025) and Qwen-VL series (Bai et al.,\n2023; Wang et al., 2024) have shown strong performance in complex multimodal tasks. By lever-\naging LLM backbones and sophisticated vision-language alignment techniques, VLMs are capable\nof using the vast real-world knowledge of LLMs to perform complex visual reasoning, generate\ndetailed descriptions, and understand intricate visual-textual relationships.\nIn the context of CoVR and CIR, these generative models offer advantages in understanding nu-\nanced compositional queries. Their autoregressive nature allows for more flexible reasoning about\nvisual-textual relationships, making them particularly well-suited for tasks that require composi-\ntional understanding. Methods like (Thawakar et al., 2024) and TFR-CVR (Hummel et al., 2024)\nuse VLMs to generate captions for the query and target video, thus crossing the modality gap of\nvision and text. Other methods like InstructCIR (Zhong et al., 2024) and UNITE (Kong et al., 2025)\ndirectly use the hidden representations of the VLM as embeddings for retrieval.\nUniversal multimodal retrievers and LMM-based retrieval.\nRecent work has explored univer-\nsal retrievers that can handle text, images, and mixed inputs within a single embedding space. These\nmethods convert multimodal LLMs into bi-encoders that generate unified embeddings for multiple\ndata modalities. E5-V (Jiang et al., 2024) extends the original E5 (Wang et al., 2022) framework to\nmultimodal settings by aligning different input modalities in a shared embedding space. The method\nconverts visual inputs into structured text descriptions and trains on instruction-formatted data pairs\nto learn cross-modal alignments. MM-Embed (Lin et al., 2025) builds on the NV-Embed (Lee et al.,\n2025) architecture, using modality-aware hard negative mining, and fine-tuning to improve perfor-\nmance on text retrieval benchmarks. GME (Zhang et al., 2024b) trains a multimodal embedder on\n3\nboth real and synthetically generated image-text pairs, studying how model size and training data\nvolume affect retrieval accuracy across different tasks.\n3\nMETHOD\nFigure 2: PREGEN extracts the hidden state of the last token at every VLM layer. These vectors are\nposition encoded and fed into a Transformer Encoder with a [CLS] token. The [CLS] token output\nis then projected through an MLP to produce the final embedding.\n3.1\nPROBLEM\nComposed Video Retrieval (CoVR) is the task of retrieving a target video from a large database given\na query consisting of a reference video and a modifying text description. Unlike standard video\nretrieval, which matches queries to targets based on visual similarity, CoVR requires understanding\nhow the query video should be modified based on the text. Formally, let Vq be a database of query\nvideos, and Vt a database of target videos. Given a query composed of a reference video vq \u2208Vq\nand modifying text tm that describes desired changes to the reference video, the goal is to retrieve\nthe target video vt \u2208Vt that best satisfies the modification specified by tm when applied to vq.\nThe training data consists of triplets (vq, tm, vt) where vq is the query video from Vq, tm is the\nnatural language modification text, and vt is the corresponding target video from Vt that matches\nthe described changes. The modifying text tm typically describes transformations such as changes\nin objects (\u201creplace the guitar with a piano\u201d), scenes (\u201cmove from outdoors to indoors\u201d), actions\n(\u201cwalking instead of running\u201d), or other semantic modifications that preserve the core context of the\nreference video. During inference, given a new query (vq, tm), the model ranks all videos in Vt by\ntheir similarity to the composed query.\n3.2\nMODEL ARCHITECTURE\nCurrent CoVR methods that leverage VLMs typically extract embeddings from only the final layer,\nmissing the hierarchical knowledge encoded across different layers of the model. Recent works\nshow that different layers in a VLM capture distinct semantic information. Early layers capture low-\nlevel visual and textual features, middle layers learn compositional relationships between visual\nand textual signals, and encode global information, and later layers integrate information for high-\nlevel reasoning and next token prediction, thus depending far less on raw visual tokens (Tao et al.,\n2024; Liu et al., 2024; Chen et al., 2024). To fully utilize this multi-layer knowledge, we propose\nPREGEN, which extracts and aggregates representations from all VLM layers. The full architecture\nis illustrated in figure 2.\n4\nGiven a query video vq and modifying text tm, we first process them through a frozen VLM to\nobtain joint video-text representations. Let fV LM denote the VLM with L layers. We feed the\nconcatenated input [vq; tm] through the VLM and extract the hidden state of the final token from\neach layer l \u2208{1, 2, . . . , L}, yielding hl \u2208Rd where d is the hidden dimension of the VLM.\nThe multi-layer features\nH = [h1, h2, . . . , hL]\nare first concatenated to a learnable hcls token. We then apply sinusoidal positional encodings\nbased on layer order, where earlier layers receive lower positional indices and later layers receive\nhigher indices. Specifically, we compute \u02dchl = hl + PE(l) for each layer representation, and\n\u02dchcls = hcls + PE(0) for the cls token, forming the input sequence\n\u02dc\nH = [\u02dchcls, \u02dc\nh1, . . . , \u02dc\nhL].\nThis encoding scheme enables the Transformer encoder to better distinguish between representations\nfrom different VLM layers and understand their sequential ordering.\nThe Transformer encoder takes as input the sequence \u02dc\nH and produces the attended representations\nZ = fV LM( \u02dc\nH) = [zcls, z1, \u00b7 \u00b7 \u00b7 , zL].\nWe extract the output of the encoder for the \u02dchcls token, which serves as an aggregated representation\nof all layer information. Finally, we project zcls through an MLP to obtain the final embedding:\ne = MLP(zcls),\nwhere e \u2208RD is the D-dimensional embedding used for retrieval.\nTarget video embeddings are generated using the same process, with the target video vt as the sole\ninput to the VLM. During retrieval, we compute cosine similarity between the query embedding eq\nand each target video embedding et in the database to rank candidates.\n3.3\nTRAINING\nWe train PREGEN using contrastive learning. The goal is to learn an embedding space where se-\nmantically similar query-target pairs have high similarity while dissimilar pairs have low similarity.\nWe also introduce a novel hard negative mining approach that leverages the inherent structure of\nCoVR datasets, allowing for efficient hard negative mining without the usual burden of additional\ncomputational overhead.\nContrastive learning.\nGiven a batch of of triplets B = {(vi\nq, ti\nm, vi\nt)}B\ni=1, the goal is to maximize\nthe similarity of each query embedding ei\nq to its corresponding target embedding ei\nt, while minimiz-\ning the similarity between the query embedding and all other target embeddings in the batch. Pairs\nof the form (ei\nq, ei\nt) are called positive pairs, while pairs of the form (ei\nq, ej\nt) such that i \u0338= j are\ncalled negative pairs.\nWe use the symmetric InfoNCE loss (Oord et al., 2018):\nL =\n1\n2B\nB\nX\ni=1\n\"\n\u2212log\nexp(sii/\u03c4)\nPB\nj=1 exp(sij/\u03c4)\n\u2212log\nexp(sii/\u03c4)\nPB\nj=1 exp(sji/\u03c4)\n#\n.\nwhere sij denotes the cosine similarity of ei\nq and ej\nt, and \u03c4 is a temperature hyperparameter. In-\nfoNCE is widely used for retrieval tasks, as it aims to maximize the similarity between positive pairs\nwhile simultaneously minimizing the similarity between all negative pairs in the batch.\nHard negative mining using source-based batching.\nHard negative mining is a training tech-\nnique that selects challenging negative examples for training. Specifically, instead of using random\nnegative samples, hard negative mining identifies negatives that are difficult for the current model to\ndistinguish from positive examples; typically, those with high similarity to the positive samples. By\ntraining on these difficult cases, the model learns a more discriminative embedding space, ultimately\n5\nresulting in more robust representations that achieve better performance on retrieval tasks. However,\nhard negative mining typically requires expensive online computation to identify challenging exam-\nples during training. We propose a preprocessing approach that achieves similar benefits without\nany additional computational overhead. During data preprocessing, we prioritize grouping training\ntriplets that share the same query video vq when constructing batches. This ensures that batches\ncontain multiple triplets with the same source video but different modifications whenever possible.\nSince target videos from the same source share visual similarity, they naturally serve as challenging\nnegatives that improve the model\u2019s discriminative capability without requiring explicit hard negative\nsearch. For an empirical validation of the benefits of this approach, see Section 4.2.\n4\nEXPERIMENTS\nIn this section, we conduct experiments aiming to answer these core questions:\n\u2022 Q1 Does aggregating representations from all layers improve retrieval performance com-\npared to using a single layer?\n\u2022 Q2 Does source-based hard negative mining improve model capabilities?\n\u2022 Q3 Is the method agnostic to different backbones?\n\u2022 Q4 Does the method generalize to more complex and detailed textual modifications without\nadditional training?\n\u2022 Q5 Do the different components of PREGEN contribute to its performance?\nAll experiments are conducted on 4 NVIDIA RTX PRO 6000 Blackwell Workstation Edition, using\nan AdamW optimizer. Full hyperparameter details, and dataset statistics are provided in Sections A\nand B. All results for PREGEN are reported using a Qwen2.5-VL 7B backbone (Bai et al., 2025)\nunless stated otherwise.\nOur codebase is publicly available at: https://anonymous.4open.science/r/PREGEN_\nCoVR.\n4.1\nTHE EFFECT OF USING ALL LAYERS\nSetup.\nTo evaluate the impact of using all layers (Q1), we compare PREGEN, as described in\nSection 3.2, to PREGEN when trained using only the last layer of the VLM. In this case, since\nthere is no longer a sequence of hidden states, we discard the Transformer encoder, and directly\nprocess the hidden state through an MLP. We perform evaluation on two available CoVR datasets:\nWebVid-CoVR (Ventura et al., 2024a), and FineCVR (Yue et al., 2025), reporting Recall@k for\nk \u2208{1, 5, 10, 50}. For WebVid-CoVR we report the baselines CoVR (Ventura et al., 2024b), CoVR-\n2 Ventura et al. (2024a), ECDE (Thawakar et al., 2024), Thawakar et al. (2025), and UNITE (Kong\net al., 2025), taken from (Kong et al., 2025). For FineCVR we report the baselines CoVR (Ventura\net al., 2024b), TFR-CVR (Hummel et al., 2024), FreestyleRet (Li et al., 2024), and FDCA (Yue\net al., 2025), taken from (Yue et al., 2025).\nResults.\nTables 1 and 2 establish PREGEN as state-of-the-art on WebVid-CoVR and FineCVR,\nsurpassing all other CoVR methods by huge margins. Notably, PREGEN outperforms the previous\nstate-of-the-art, UNITE (Kong et al., 2025), a method which also uses the hidden states of VLMs\nto generate embeddings. Conversely, when using a single layer of the VLM, PREGEN performs\npoorly, achieving the weakest results across all metrics for both datasets. This large disparity in\nperformance is a direct result of utilizing the full scope of information encoded across the layers of\nthe VLM, further validating the strength of our approach.\n4.2\nTHE EFFECT OF SOURCE-BASED HARD NEGATIVE MINING\nSetup.\nTo evaluate the impact of our source-based hard negative mining strategy (Q2), we com-\npare PREGEN trained with and without hard negative mining. We evaluate both configurations on\nWebVid-CoVR (Ventura et al., 2024a) and FineCVR (Yue et al., 2025), reporting Recall@k for\nk \u2208{1, 5, 10}.\n6\nTable 1: Results on WebVid-CoVR test set. The best results are highlighted.\nMethod\nR@1\nR@5\nR@10\nR@50\nCoVR\n53.1\n79.9\n86.9\n97.7\nCoVR-2\n59.8\n83.8\n91.3\n98.2\nECDE\n60.1\n84.3\n91.3\n98.7\nUNITEinstruct 7B\n72.5\n90.8\n95.3\n99.5\nPREGEN (1 layer)\n14.20\n30.71\n40.18\n67.45\nPREGEN\n99.73\n99.92\n99.96\n100.00\nTable 2: Results on FineCVR test set. The best results are highlighted.\nMethod\nR@1\nR@5\nR@10\nR@50\nTFR-CVR\n15.21\n40.12\n52.78\n81.75\nCoVR\n17.05\n41.57\n56.60\n85.56\nFreestyleRet\n20.39\n52.98\n68.37\n93.03\nFDCA-CLIP\n25.84\n55.84\n70.23\n94.33\nFDCA-BLIP\n26.79\n63.21\n78.65\n97.25\nPREGEN (1 layer)\n6.90\n12.21\n17.42\n39.95\nPREGEN\n96.38\n99.95\n99.98\n99.99\nResults.\nTable 3 demonstrates the gains added by using our source-based hard negative mining\nstrategy. On WebVid-CoVR, hard negative mining consistently improves performance across all\nmetrics, with gains of 0.98%, 1.17%, and 1.21% for Recall@{1,5,10} respectively. The improve-\nments are substantially more pronounced on FineCVR, where the method provides performance\nboosts of 16.05%, 2.98%, and 1.22% across the same metrics. This could suggest that our hard\nnegative mining approach may prove even more effective on more challenging datasets where per-\nformance has not reached saturation levels. We emphasize that this training strategy incurs no addi-\ntional computational costs, and can potentially be used across many more domains and tasks.\nTable 3: Results of PREGEN with and without source-based hard negative mining. Best results are\nin bold.\nWebVid-CoVR\nFineCVR\nMethod\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nPREGEN (No hard negative mining)\n98.75\n98.75\n98.75\n80.33\n96.97\n98.76\nPREGEN\n99.73\n99.92\n99.96\n96.38\n99.95\n99.98\n4.3\nTHE EFFECT OF DIFFERENT BACKBONES\nSetup.\nTo evaluate whether our method is agnostic to different VLM backbones (Q3), we compare\nPREGEN using four different Qwen-VL variants: Qwen2-VL 2B, Qwen2-VL 7B, Qwen2.5-VL 3B,\nand Qwen2.5-VL 7B (Wang et al., 2024; Bai et al., 2025). All models follow the same architecture\ndescribed in Section 3.2, with only the underlying VLM backbone being changed. We evaluate\nall variants on WebVid-CoVR (Ventura et al., 2024a), reporting Recall@k for k \u2208{1, 5, 10, 50}.\nWe report baseline results for UNITE (Kong et al., 2025) using Qwen2-VL 2B and Qwen2-VL 7B\nbackbones, taken from (Kong et al., 2025).\nResults.\nTable 4 demonstrates that PREGEN achieves consistently strong performance across all\ntested VLM backbones, with minimal variation in results despite the differences in model size and\n7\nTable 4: Results on WebVid-CoVR using different VLM backbones. The Backbone column specifies\nthe underlying VLM used for each method. Best results are in bold.\nMethod\nBackbone\nR@1\nR@5\nR@10\nR@50\nUNITEinstruct\nQwen2-VL 2B\n69.1\n88.4\n93.2\n99.1\nUNITEinstruct\nQwen2-VL 7B\n72.5\n90.8\n95.3\n99.5\nPREGEN\nQwen2-VL 2B\n95.89\n98.28\n98.59\n99.53\nPREGEN\nQwen2-VL 7B\n98.51\n98.71\n98.71\n98.75\nPREGEN\nQwen2.5-VL 3B\n97.81\n99.61\n99.8\n99.96\nPREGEN\nQwen2.5-VL 7B\n99.73\n99.92\n99.96\n100.00\nTable 5: Results on Dense WebVid-CoVR test set. Train Data column indicates the dataset used for\ntraining. Best results are in bold.\nMethod\nTrain Data\nR@1\nR@5\nR@10\nR@50\nThawakar et al. (2025)\nWebVid-CoVR\n48.08\n73.36\n81.06\n93.78\nThawakar et al. (2025)\nDense WebVid-CoVR\n71.26\n89.12\n94.56\n98.88\nPREGEN (Ours)\nWebVid-CoVR\n98.71\n98.75\n98.79\n98.79\narchitecture. All PREGEN variants substantially outperform existing state-of-the-art methods, with\nperformance differences between backbones remaining within 4% across all metrics. Notably, com-\nparing performance between the 2B and 7B variants of Qwen2-VL, our method shows an average\nrelative decrease of only 1.1% across Recall@k for k \u2208{1, 5, 10}, while UNITE shows a 3.2% av-\nerage relative decrease. This consistency across diverse backbone sizes and versions further demon-\nstrates the robustness of our approach, regardless of the underlying VLM architecture.\n4.4\nGENERALIZATION TO COMPLEX INSTRUCTIONS\nSetup.\nThawakar et al. (2025) published the Dense WebVid-CoVR dataset, a modified version of\nthe WebVid-CoVR dataset, where textual modifications are more fine grained. Unlike the original\ndataset\u2019s simple textual modifications (e.g., \u201dchange color to red\u201d), Dense WebVid-CoVR contains\ndetailed compositional descriptions that specify precise spatial relationships, temporal dynamics,\nand multi-object interactions within video scenes. To test generalization to complex instructions\n(Q4), we train PREGEN exclusively on the standard WebVid-CoVR. We then evaluate on the Dense\nWebVid-CoVR test set to assess its ability to handle significantly more detailed modifications. We\nuse the baselines of Thawakar et al. (2025), reporting Recall@k for k \u2208{1, 5, 10, 50}.\nResults.\nTable 5 reveals that PREGEN demonstrates exceptional generalization to more complex\ntextual instructions, experiencing less than 2% performance decrease across all metrics when mov-\ning from standard to dense modifications. In contrast, Thawakar et al. (2025) suffer a substantial\nperformance decrease, with drops as high as 23% compared to their results on the original WebVid-\nCoVR dataset. This demonstrates the utility of our approach in handling diverse and complex in-\nstructions, without requiring retraining. We attribute this robustness to our use of a large VLM\nbackbone, which provides significantly greater flexibility in understanding nuanced textual descrip-\ntions compared to the simpler BLIP backbone used by Thawakar et al. (2025).\n4.5\nEFFECT OF DIFFERENT COMPONENTS\nSetup.\nTo evaluate how much each component contributes to the strong performance of the method\n(Q5), we conduct an ablation study, testing variants of PREGEN with: (1) single-layer extraction\ninstead of multi-layer, (2) averaging encoder outputs instead of using the [CLS] token, (3) removal\n8\nTable 6: Ablation study results on WebVid CoVR. The best results are highlighted.\nMethod\nR@1\nR@5\nR@10\nR@50\nPREGEN (1 layer)\n14.20\n30.71\n40.18\n67.45\nPREGEN (avg over encoder outputs)\n98.71\n98.75\n98.79\n98.79\nPREGEN (No hard negative mining)\n98.75\n98.75\n98.75\n98.75\nPREGEN (No positional encodings )\n98.75\n98.75\n98.79\n98.79\nPREGEN\n99.73\n99.92\n99.96\n100.00\nof hard negative mining, and (4) removal of positional encodings. All experiments are conducted on\nthe WebVid-CoVR test set.\nResults.\nTable 6 demonstrates the importance of each component in PREGEN. As shown in our\nmain results, using only a single layer leads to severe performance degradation. Averaging encoder\noutputs instead of using the [CLS] token, training without hard negative mining, and removing po-\nsitional encoding, all lead to a small but consistent decrease in performance, validating our different\ndesign choices.\n5\nDISCUSSION\nWhile PREGEN achieves nearly perfect performance on current CoVR benchmarks, these results\nshould not be interpreted as indicating that composed video retrieval is a solved problem. Rather,\nwe believe these results indicate that current available benchmarks for CoVR do not mirror the chal-\nlenges posed by real-world video understanding tasks. For example, the WebVid-CoVR dataset\noften contains modifications that can be resolved using a single frame, rather than requiring under-\nstanding of temporal dynamics or complex semantic relationships Thawakar et al. (2024). FineCVR\nattempts to address this limitation by including more complex modifications. Indeed, performance\non this dataset was substantially lower compared to WebVid-CoVR (96.38% vs 99.73% Recall@1).\nThis difference highlights that future benchmarks should prioritize modifications that require more\nadvanced compositional reasoning, temporal causality, and complex spatial-temporal relationships.\nOur work reveals that current embedding and retrieval methods underutilize modern VLMs. Most\napproaches use outdated architectures or extract features from single layers, thereby neglecting\nmeaningful semantic information. We believe future research should explore how to better utilize the\ndifferent layers of VLM and LLM backbones, investigate semantic information distribution across\nmodel layers, and develop more efficient aggregation strategies. These findings can and should be\nadopted across other multimodal tasks where foundation models are underutilized. Similar multi-\nlayer extraction strategies could improve capabilities in tasks such as image-text retrieval, visual\nquestion answering, and cross-modal understanding.\n6\nCONCLUSION\nWe introduced PREGEN, a novel framework for Composed Video Retrieval that efficiently lever-\nages frozen vision-language models by extracting and aggregating hidden states from all layers of\nthe VLM. In contrast to previous methods that used VLMs, our approach eliminates the need for\nexpensive fine-tuning or caption generation while achieving substantial performance improvements.\nNamely, PREGEN surpasses all prior methods on the standard CoVR benchmarks WebVid-CoVR\nand FineCVR, with gains of +27.23 and +69.59 in Recall@1, respectively.\nWe introduce a training strategy we term source-based hard negative mining, which utilizes the\nstructure of CoVR datasets and improves training efficiency without additional computational over-\nhead. PREGEN exhibits strong performance across different VLM backbones and generalization to\ncomplex textual modifications, highlighting the robustness of our multi-layer feature aggregation.\n9\nREPRODUCIBILITY STATEMENT.\nWe have taken care to ensure the reproducibility of our results. Complete details of datasets, model\narchitectures, training settings, and hyperparameters are provided in the main text and Appendix.\nAll datasets used are publicly available benchmarks. All pretrained VLMs are taken from Hugging-\nFace repositories. We provide a public codebase with a complete implementation of all methods\npresented.\nETHICS STATEMENTS.\nThis work does not involve human subjects, personal or sensitive data, or applications in high-\nrisk domains. All datasets used in our experiments are publicly available benchmarks, and were\nused in compliance with their respective licenses. The large-language models used in our study are\npublicly released open-source models obtained through HuggingFace. Our work provides a general\nframework for Composed Video Retrieval, and does not target any harmful applications.\nUsage of Large Language Models in This Work.\nLLMs were used in this work for coding as-\nsistance, grammar refinement, and LaTeX formatting. Their use allowed the authors to invest more\ntime into meaningful research contributions.\nREFERENCES\nMuhammad Umer Anwaar, Egor Labintcev, and Martin Kleinsteuber. Compositional learning of\nimage-text query for image retrieval. In WACV, 2021.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, local-\nization, text reading, and beyond. arXiv, 2023.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan,\nPengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,\nHang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv,\n2025.\nLiang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang.\nAn image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-\nlanguage models. In ECCV, 2024.\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu-\nrate object detection and semantic segmentation. In CVPR, 2014.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne West-\nphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al.\nThe \u201dsomething something\u201d video database for learning and evaluating visual common sense. In\nICCV, 2017.\nGeonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang, and Sangdoo Yun. Language-only\nefficient training of zero-shot composed image retrieval. In CVPR, 2024.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022.\nThomas Hummel, Shyamgopal Karthik, Mariana-Iuliana Georgescu, and Zeynep Akata. Egocvr:\nAn egocentric benchmark for fine-grained composed video retrieval. In ECCV, 2024.\nTing Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang,\nDeqing Wang, and Fuzhen Zhuang. E5-v: Universal embeddings with multimodal large language\nmodels. arXiv, 2024.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**Summary of the Paper \"PREGEN: UNCOVERING LATENT THOUGHTS IN COMPOSED VIDEO RETRIEVAL\"**\n\n### **Main Contributions**\nThe paper introduces PREGEN (Pre GENeration extraction), a novel framework for Composed Video Retrieval (CoVR) that efficiently leverages frozen Vision-Language Models (VLMs). The key contributions are:\n1. **Efficient Use of VLMs**: PREGEN avoids the need for fine-tuning or caption generation, making it more efficient and scalable.\n2. **Multi-Layer Representation Extraction**: It extracts hidden states from all layers of the VLM, creating powerful embeddings that capture hierarchical semantic information.\n3. **Hard Negative Mining Strategy**: Introduces a source-based hard negative mining approach to improve model capabilities without additional computational overhead.\n\n### **Methodology**\nThe core methodology involves:\n1. **Hidden State Extraction**: The query video and modifying text are processed through the VLM, extracting hidden states from each layer's final token.\n2. **Aggregation of Representations**: These representations are aggregated using a lightweight Transformer encoder to create compact yet rich embeddings for retrieval.\n3. **Training Strategy**: A precomputed hard negative mining strategy is used, allowing efficient and effective negative example selection.\n\n### **Key Findings**\nThe key findings include:\n1. **State-of-the-Art Performance**: PREGEN significantly outperforms all prior methods on standard CoVR benchmarks (WebVid-CoVR and FineCVR), with gains of +27.23 in Recall@1 for WebVid-CoVR and +69.59 for FineCVR.\n2. **Robustness Across VLM Backbones**: The method achieves strong performance across different VLM backbones, showing robust generalization to more complex textual modifications without additional training.\n\n### **Conclusions**\nThe paper concludes by highlighting the effectiveness of PREGEN in Composed Video Retrieval (CoVR), demonstrating its ability to efficiently use frozen VLMs for retrieval tasks. It also emphasizes the importance of utilizing multi-layer feature aggregation and hard negative mining strategies, which contribute significantly to the method's robustness and performance gains.\n\n### **Future Work**\nThe research suggests that future work should focus on exploring how to better utilize different layers of VLMs and LLM backbones, investigating semantic information distribution across model layers, and developing more efficient aggregation strategies. This can be applied across other multimodal tasks where foundation models are underutilized."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "LIGHTONOCR: A 1B END-TO-END MULTILINGUAL\nVISION-LANGUAGE MODEL FOR STATE-OF-THE-ART OCR\nSaid Taghadouini\nAdrien Cavaill\u00e8s\nBaptiste Aubertin\nLightOn\nABSTRACT\nWe present LightOnOCR-2-1B, a 1B-parameter end-to-end multilingual vision\u2013language model\nthat converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR\npipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French\ndocuments, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench\nwhile being 9\u00d7 smaller and substantially faster than prior best-performing models. We further\nextend the output format to predict normalized bounding boxes for embedded images, introducing\nlocalization during pretraining via a resume strategy and refining it with RLVR using IoU-based\nrewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We\nrelease model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-\nbbox-bench evaluation under their respective licenses.\nModel:\nhttps://huggingface.co/collections/lightonai/lightonocr-2\nBlog:\nhttps://huggingface.co/blog/lightonai/lightonocr-2\nBenchmark:\nhttps://huggingface.co/datasets/lightonai/LightOnOCR-bbox-bench\n1\nIntroduction\nDespite decades of progress in Optical Character Recognition (OCR), from classical engines such as Tesseract [1],\nto deep neural sequence recognizers like CRNN [2], and Transformer-based models such as TrOCR [3], real-world\ndocuments remain challenging: reading order can be ambiguous in multi-column layouts, tables require consistent\nstructure, and scientific PDFs often mix dense typography with mathematical notation, figures, and noisy scans. A\ncommon production strategy is to rely on multi-stage pipelines (e.g., layout analysis, text detection, text recognition,\ntable extraction, and reading-order reconstruction), as in systems such as PaddleOCR [4, 5] and MinerU [6]. While\neffective in many settings, these pipelines couple multiple components and intermediate representations, making them\ncostly to adapt: improving performance on a new document distribution often requires additional annotations for\nintermediate tasks (layout regions, table structure, reading order) and coordinated changes across stages.\nEnd-to-end vision-language models (VLMs) reduce this engineering burden by learning extraction directly from pixels\nto structured text [7, 8, 9, 10, 11, 12]. This enables continuous improvement and specialization via straightforward fine-\ntuning, without retooling each stage of a pipeline. In this work, we present LightOnOCR-2, a compact 1B-parameter\nend-to-end multilingual VLM that achieves state-of-the-art OCR, outperforming substantially larger systems, and\nextends OCR with document image localization through predicted bounding boxes.\nIn this context, we release LightOnOCR, a compact, end-to-end model that delivers state-of-the-art document under-\nstanding with lightning speed and low cost. Competing systems(including newest releases) often rely on multiple\nmoving parts to boost performance, but this added complexity makes them brittle, difficult to train, and prone to break\nwhen adapting to new data or domains. LightOnOCR, on the other hand, is a single unified model \u2014 fully differentiable\nand easy to optimize end-to-end \u2014 capable of handling complex layouts such as tables, forms, receipts, and scientific\nnotation without fragile multi-stage pipelines.\nLightOnOCR-2-1B is a compact, end-to-end model that builds on our first release [13] with a substantially expanded\nand cleaner training mixture (2.5\u00d7 larger), with increased coverage of scans, French documents, and scientific content,\nsupported by an improved data curation pipeline. During pretraining, we train at higher resolution (maximum longest\narXiv:2601.14251v1  [cs.CV]  20 Jan 2026\nedge 1540px), apply data augmentation, and explicitly include empty pages to reduce looping behaviors and improve\nfull-page fidelity. We then apply Reinforcement Learning with Verifiable Rewards (RLVR) [14] to target persistent\nfailure modes that are difficult to address with supervised learning alone, including repetition loops, math rendering\nand formatting errors, and layout-sensitive consistency constraints enforced through unit-test style checks. We also\ntrain a variant to predict bounding boxes for embedded images. To avoid degrading OCR quality with naive supervised\nfine-tuning, we introduce coordinate supervision during pretraining via a resume strategy and then refine localization\nusing RLVR with IoU-based objectives. Finally, we leverage lightweight weight-space techniques, checkpoint averaging\nand task-arithmetic merging, to combine complementary gains and to control the trade-off between OCR quality and\nbounding box (bbox) accuracy.\nContributions\nTo summarize, we make the following contributions:\n\u2022 We release LightOnOCR-2-1B, a compact 1B-parameter end-to-end multilingual VLM for document parsing\nthat achieves a new state-of-the-art result on OlmOCR-Bench, outperforming substantially larger models (e.g.,\n9B-scale baselines).\n\u2022 We scale and improve the pretraining mixture (2.5\u00d7 larger) with stronger coverage of French and scientific\ndocuments, higher-resolution training (max longest edge 1540px), improved normalization (cleaner LATEX\nhandling), and robustness via data augmentations and explicit empty-page targets.\n\u2022 We introduce a nvpdftex-based arXiv curation pipeline to obtain pixel-aligned supervision from TEX sources,\nand use it both to strengthen scientific OCR supervision and to generate an automatic subset for our localization\nbenchmark.\n\u2022 We add image bounding box prediction in dedicated variants by introducing coordinate supervision during\npretraining and refining localization with RLVR using IoU-based objectives.\n\u2022 We introduce LightOnOCR-bbox-bench, a new benchmark for image localization in documents.\n\u2022 We show that lightweight weight-space techniques, checkpoint averaging and task-arithmetic merging, can\nimprove OCR and enable controlled trade-offs between OCR quality and bbox accuracy across released\ncheckpoints.\n2\nOverview\nIn this section, we describe the LightOnOCR architecture and summarize the training recipe differences between\nLightOnOCR-1B and LightOnOCR-2-1B.\n2.1\nArchitecture\nLightOnOCR is a compact 1B-parameter vision-language model composed of three main components: a vision encoder,\na multimodal projector, and a language model decoder. We train the model to perform OCR without task prompts at\ninference time, so the extraction behavior is embedded in the weights rather than controlled by an explicit prompt.\nVision\nEncoder\nWe\nemploy\na\nnative-resolution\nVision\nTransformer\ninitialized\nfrom\nthe\npretrained\nMistral-Small-3.1 [15] vision encoder weights. This encoder handles variable image sizes while preserving\nspatial structure, which is critical for documents with diverse aspect ratios and fine typographic details.\nMultimodal Projector\nTo bridge the vision and language modalities while controlling sequence length, we use a\ntwo-layer MLP with GELU activation that projects vision features into the language model\u2019s embedding space. Before\nprojection, we apply spatial merging with a factor of 2, effectively grouping 2 \u00d7 2 patches and reducing the number of\nvisual tokens by 4\u00d7. This keeps the overall token count tractable for high-resolution inputs while preserving sufficient\nspatial granularity. The projector is randomly initialized and trained from scratch.\nLanguage Model Decoder\nWe initialize the decoder from pretrained Qwen3 [16]. The decoder produces a single,\nlinearized representation of the page that preserves reading order while emitting structured tokens for non-text elements\n(e.g., ![image](image_N.png)). To simplify the interface between modalities, we remove the image-break and\nimage-end tokens and condition the decoder on a single contiguous block of visual tokens (after spatial merging),\nfollowed by text tokens. This yields a compact end-to-end VLM with a consistent generation format across datasets.\nInitialization\nBy initializing from strong pretrained components, LightOnOCR inherits robust visual representations\nand multilingual language modeling capabilities, enabling effective transfer to OCR with reduced training cost.\n2\nFigure 1: Model architecture.\n2.2\nLightOnOCR-1\nThe first version of LightOnOCR [13], established the core architecture described above.\nIt was developed\nusing supervised training on the PDF Association (PDFA) dataset [17], using transcriptions generated by\nQwen2-VL-72B-Instruct [18] as teacher supervision. Training used a maximum longest-edge resolution of 1024\npixels.\n2.3\nLightOnOCR-2\nLightOnOCR-2 keeps the same architecture but significantly updates both the data and the training recipe. We scale\nthe pretraining mixture from 17M to 43M pages (Section 3), with stronger coverage of scanned documents, scientific\nPDFs, and European languages with an emphasis on French. Supervision quality is improved by leveraging a stronger\nteacher, Qwen3-VL-235B-A22B-Instruct [16], and by improving conversion and normalization so that targets better\npreserve layout and clean LATEX formatting. We also increase the maximum longest-edge resolution from 1024 to 1540\npixels, improving legibility for small text and dense mathematical notation while retaining dynamic resizing for diverse\npage sizes. Beyond transcription, we train an image-localization variant that predicts bounding boxes for embedded\nimages by extending the output format with normalized coordinates; to avoid OCR regressions, we introduce coordinate\nsupervision during pretraining via a resume strategy and then refine localization with RLVR using IoU-based objectives\n(Section 4.2). Finally, we apply lightweight weight-space techniques, checkpoint averaging and task-arithmetic merging,\nto combine complementary gains across runs and to control trade-offs between OCR quality and bbox accuracy.\n3\n3\nDatasets and Preprocessing\n3.1\nDataset Scale and Composition\nBoth LightOnOCR versions are trained on large-scale document OCR corpora built primarily through distillation: a\nstrong vision\u2013language teacher produces naturally ordered transcriptions (Markdown with LATEX spans) from rendered\nPDF pages. Compared to LightOnOCR-1B, LightOnOCR-2-1B scales up the data mixture and improves supervision\nquality by upgrading the teacher model from Qwen2-VL-72B-Instruct [18] to Qwen3-VL-235B-A22B-Instruct,\nyielding more faithful mathematical notation and fewer formatting artifacts. The LightOnOCR-2-1B mixture combines\nteacher-annotated document pages from multiple permissibly usable sources, including scanned material for robustness,\nas well as auxiliary data to broaden layout coverage.\nIn addition to full pages, we include document-region crops (paragraphs, headers, abstracts, and general snippets)\nannotated with GPT-4o [19] to expose the model to varied formats, and we add explicit blank-page examples to enforce\na consistent target for empty inputs and mitigate looping or hallucination behaviors. To better cover scientific documents,\nwe also incorporate TEX-derived supervision obtained by compiling raw arXiv sources with an nvpdftex [20] pipeline\n(Section 3.3), and we complement the mix with publicly available OCR datasets for additional diversity [9]. We\nrelease the PDFA-derived annotated subset under license terms that match the underlying PDFA source data as\nlightonai/LightOnOCR-mix-0126.\nDuring large-scale annotation, we observed that the teacher occasionally emitted figure bounding box coordinates\neven when not explicitly prompted to do so. Manual inspection of a sampled subset showed these boxes to be highly\naccurate. To keep the base OCR objective strictly focused on transcription, we removed coordinate traces from the main\nsupervised training targets; however, we retained them as a separate supervision signal for the bounding-box addition\nprocedure described in Section 4.2.1. We also reformatted the coordinates to match our target output convention before\ntraining. We publicly release this extracted and normalized subset as lightonai/LightOnOCR-bbox-mix-0126.\n3.2\nNormalization Pipeline and data clean-up\nAs described above, our training corpus is assembled from heterogeneous sources (PDFA, scans, open mixtures,\narXiv renders) and multiple VLM teachers, which introduces systematic but superficial inconsistencies in the raw\ntranscriptions: stray Markdown code fences, watermark text, variable image placeholders, templated \u201cthis page is\nempty\u201d messages, and occasional format drift (e.g., LATEX environments or HTML fragments where Markdown is\nexpected). While these artifacts rarely affect human readability, they significantly increase target entropy and hurt both\ndeduplication and learning stability. We therefore apply a unified normalization pipeline that maps all sources to a\nsingle, canonical target format before mixing and training.\nConcretely, normalization is implemented as a sequence of lightweight, mostly deterministic transforms applied prior\nto hashing and filtering. We (i) sanitize the text (e.g., remove spurious Markdown ticks/code blocks and harmonize\nwhitespace), and apply source-specific cleanup when needed. Notably, we remove recurring watermark artifacts during\npreprocessing so they do not pollute the deduplication procedure. (ii) homogenize special cases such as full-page\nembedded images and blank pages by mapping them to fixed targets (respectively a single standard image placeholder\nand the empty string), and (iii) perform loop and repetition filtering and deduplication on the normalized text. To retain\ncontrol over the proportion of degenerate-but-common cases, we also separate dedicated pools of empty pages, which\nwere then re-injected at a chosen rate during training as mentioned in Section 3.1.\nFinally, we run a LATEX conversion and validation pass to enforce formatting invariants across the full mixture: LATEX\ncommands are restricted to math spans, headers and sections are converted to markdown, tables are standardized\n(we use HTML targets to reduce ambiguity), and math expressions are checked for KaTeX [21] compatibility. The\nconversion step emits structured metadata (success/partial/timeout, unresolved references, missing figure numbering,\nKaTeX compatibility), enabling simple, reproducible filtering rules when constructing the final training mixture.\n3.3\nnvpdftex data curation pipeline\nHigh-quality OCR supervision requires pairing rendered page images with faithful, layout-consistent transcriptions.\nIn earlier iterations we relied on a Nougat-style PDF parsing pipeline [7], but in practice we found it difficult to\nobtain sufficiently reliable (image, markup) pairs at scale for training: conversion errors, reading order issues, and\nimperfect alignment between rendered pages and extracted markup introduce noise that can hinder learning. For\nLightOnOCR-2, we revamped the arXiv extraction pipeline around nvpdftex, a recently released NVIDIA toolchain\nthat hooks directly into the pdfLATEX engine to produce pixel-aligned annotations without heuristic matching. Concretely,\nnvpdftex compiles TEX sources and emits (i) a PNG rendering of each page, (ii) structured text targets (Markdown\n4\nor HTML) for each region, (iii) pixel-accurate bounding boxes with semantic classes (e.g. headers/footers, captions,\ntables, formulas, pictures), and (iv) page-level metadata (e.g. image dimensions) [20, 22]. We additionally use this\npipeline to generate high-quality figure/image boxes for the arXiv subset used in our bbox RLVR experiments (Section\n4.2.2); these arXiv bounding boxes are used for RL supervision rather than included in the main pretraining mixture.\n3.4\nLightOnOCR-bbox-bench: New Image Localization Benchmark\nLightOnOCR-bbox-bench\nWhile OCR benchmarks are widely used, there are no standardized benchmarks that\nspecifically measure how well end-to-end vision-language models localize images within documents. To fill this gap, we\nintroduce LightOnOCR-bbox-bench, released with this work and composed of two subsets: (i) a manually reviewed\nsubset derived from OlmOCR-Bench [8] pages (290 samples) and (ii) an automatically annotated arXiv subset generated\nwith nvpdftex (565 samples) and filtered programmatically. We report F1 at IoU threshold 0.5, mean IoU, and count\naccuracy (exact match on the number of predicted boxes), and evaluate separately on the manual OlmOCR-derived\nsubset and the automatic arXiv+nvpdftex subset.\n4\nTraining\n4.1\nPretraining\nTraining setup\nWe train on a large mixed corpus of OCR datasets described in Section 3, with filtering to remove\nmissing references, numberless figures, KaTeX-incompatible samples, and long completions (capped at 3100 tokens).\nDocument pages are rendered at 200 DPI during training (except for crops which were already available as images)\nand resized to a maximum longest edge of 1540 pixels for computational efficiency. We apply moderate document\naugmentations such as bitmap corruption, erosion/dilation, small affine shears, shift\u2013scale\u2013rotate, 90\u25e6rotations, and\nmild grid distortions, so that a page receives at least one augmentation with probability 0.22 (and we use a more\naggressive regime for blank-page examples). We optimize a next-token prediction objective but mask part of the loss:\nthe loss is computed only on assistant tokens, excluding prompt tokens (system prompt and special tokens) as well as\nimage tokens. Optimization uses AdamW with no weight decay, a peak learning rate of 10\u22124, a cosine learning rate\nschedule with a 100-step warmup, a global batch size of 384 samples, and a maximum sequence length of 6144 tokens.\nTraining is distributed with DDP over 96 NVIDIA H100 GPUs (80 GB) using bf16 precision and FlashAttention-2 [23].\n4.2\nReinforcement Learning with Verifiable Rewards\nWe apply RLVR [14], where rewards are computed by automatic checks that can be evaluated deterministically on\nmodel outputs (e.g., binary unit tests on synthetic documents). This allows us to directly optimize for specific OCR\nfailure modes without having to annotate extra data.\nTraining\nsetup\nWe\nstart\nfrom\nthe\npretraining\ncheckpoints\n(LightOnOCR-2-1B-base\nfor\nOCR\nand\nLightOnOCR-2-1B-bbox-base for localization) and train with GRPO [24] for one epoch with AdamW at learn-\ning rate 4 \u00d7 10\u22125 and KL regularization strength \u03b2 = 0.01. We use group-scaled rewards with token-level importance\nsampling and sample multiple rollouts per prompt (28 for OCR and 14 for bbox). Training is implemented with the\nHugging Face TRL library [25], and rollouts are generated with vLLM for increased efficiency.\nWe apply two RLVR recipes: an OCR-focused variant that extends OlmOCR unit tests with additional rewards, and a\nbbox-focused variant that optimizes IoU-based localization rewards.\n4.2.1\nOlmOCR-2 Style RLVR Recipe\nWe build on the OlmOCR-2 RLVR recipe [9], where rewards are derived from synthetic OlmOCR-Bench\u2013style unit\ntests, and extend it with additional checks tailored to scientific documents. In addition to scoring completions by\nthe fraction of tests they pass, we (i) penalize low-entropy repetition loops using a compression-based heuristic and\nensuring proper EOS termination, (ii) reward mathematical correctness by extracting math spans and verifying they\nrender successfully with KaTeX, (iii) enforce clean math formatting by rejecting common artifacts such as HTML\ntags, misused Markdown italics for variables, and unbalanced LATEX delimiters or environments, and (iv) remove the\nfrontmatter reward that encourages the model to output document metadata at the top of its response. Finally, for\nthe headers/footers category, we change the absence reward to instead reward presence of headers, footers, and page\nnumbers, encouraging high-fidelity extraction of all visible content.\n5\n4.2.2\nBounding Box RLVR Recipe\nTo complement OCR with image localization,\nwe extend the output format for embedded images to\n![image](image_N.png)x1,y1,x2,y2, where coordinates are normalized to [0, 1000]. Throughout pretraining,\nthe model is already trained to emit the image placeholder ![image](image_N.png) whenever an image is present;\nbounding box learning therefore mainly adds where the image is, not whether it exists. We enable this capability by\nresuming pretraining from the base checkpoint and continuing on a mixture stage that includes bbox-annotated pages,\nproviding a cold start for RL. This pretraining integration preserves OCR performance, enabling joint OCR and image\nlocalization.\nWe then further improve image localization with RLVR using an IoU-based reward The full reward definition is given\nin Appendix D.\n4.2.3\nModel Averaging and Merging\nWe use checkpoint averaging by souping the last 5 checkpoints, which consistently outperforms any single check-\npoint. This yields our strong supervised pretraining baseline LightOnOCR-2-1B-base. We then apply RLVR\non top of this base to obtain our best OCR model LightOnOCR-2-1B. In parallel, we train a bbox-capable\nvariant by resuming supervised pretraining with coordinate annotations introduced during training, producing\nLightOnOCR-2-1B-bbox-base, and further refine it with bbox-focused RLVR to obtain LightOnOCR-2-1B-bbox.\nFinally, we apply task arithmetic merging, \u03b8merge = \u03b8base + \u03b1(\u03b8rl \u2212\u03b8base), to form LightOnOCR-2-1B-ocr-soup by\nmerging LightOnOCR-2-1B into the averaged base with \u03b1 = 0.4. We then construct LightOnOCR-2-1B-bbox-soup\nby merging LightOnOCR-2-1B-ocr-soup into the bbox-specialized checkpoint LightOnOCR-2-1B-bbox (used as\nthe base) with \u03b1 = 0.1, yielding an explicit OCR\u2013bbox trade-off without additional training. Our training targets retain\nheaders/footers, and in RLVR we flip OlmOCR\u2019s header/footer absence tests to reward presence of this text. We also\napplied the OCR RLVR recipe to LightOnOCR-1B-1025 to obtain LightOnOCR-1B-1025-GRPO for reference.\n5\nResults\nWe evaluate LightOnOCR-2-1B on OlmOCR-Bench [8] as our primary OCR benchmark. We additionally report\nOmniDocBench v1.0 [26] results in Appendix C.2. For localization, we evaluate on LightOnOCR-bbox-bench\n(Section 3.4). Unless otherwise stated, we use a single-pass evaluation without test-time heuristics (e.g., rotation sweeps\nor retries); inference details for LightOnOCR models are provided in Appendix B.\n5.1\nOlmOCR-Bench\nWe report OlmOCR-Bench results without the headers/footers category as originally defined, since it rewards omitting\nvisible content (e.g., titles and page numbers) whereas our objective is full-page transcription; systems that explicitly\nsuppress these regions can score well without improving full-page extraction quality, and even empty outputs can\nachieve a perfect score. We evaluate all models without test-time heuristics (e.g., retries or rotation correction) to reflect\nraw model behavior.\nTable 1 shows that LightOnOCR-2-1B achieves the highest overall score (83.2 \u00b1 0.9) among evaluated systems,\noutperforming substantially larger end-to-end models while using only 1B parameters and being end-to-end trainable.\nImprovements are broad across categories, with particularly strong scores on ArXiv, old scans math, and table-heavy\ndocuments, highlighting the benefit of higher-quality data, increased scientific coverage, and higher-resolution training.\nComparing LightOnOCR-2-1B-base to LightOnOCR-2-1B isolates the effect of RLVR, which improves overall\nperformance and reduces common generation failures such as repetition loops (Appendix C.4). Introducing bbox\nprediction (LightOnOCR-2-1B-bbox) yields a small OCR quality drop relative to the base checkpoint while enabling\nlocalization; task-arithmetic merging partially recover OCR performance and provide an explicit OCR\u2013bbox trade-off\n(bbox-soup). As we optimize for the presence of headers/footers, under the original OlmOCR-Bench definition\n(rewarding suppression), our headers_footers score decreases; see Appendix C.1 for details. When applying the\nsame recipe to our previous model, we observe similar gains on OCR performance, see LightOnOCR-1B-1025-GRPO\nin Table 1.\n5.2\nImage Bounding Box Detection\nTable 2 reports bounding-box detection on LightOnOCR-bbox-bench. The goal is to measure whether a compact\nend-to-end OCR model can localize embedded images while retaining strong transcription quality. We compare against\n6\nModel\nEnd-to-end Size (B) ArXiv Old Scans Math Tables Old Scans Multi-column Long Tiny Text Base\nOverall\nMistral OCR 3 API\n-\n-\n85.6\n69.7\n85.5\n43.5\n81.2\n88.5\n99.7 79.1 \u00b1 1.0\nGemini Flash 2\n-\n-\n32.1\n56.3\n61.4\n27.8\n58.7\n84.4\n94.0 59.2 \u00b1 1.1\nQwen2.5-VL-8B\n\u2713\n8\n63.1\n65.7\n67.3\n38.6\n68.3\n49.1\n98.3 64.3 \u00b1 1.2\nolmOCR v0.3.0\n\u00d7\n8\n78.6\n79.9\n72.9\n43.9\n77.3\n81.2\n98.9 76.1 \u00b1 1.1\nMonkeyOCR-pro-3B\n\u00d7\n3\n83.8\n68.8\n74.7\n36.1\n76.6\n80.1\n95.3 73.6 \u00b1 1.0\ndots.ocr\n\u00d7\n3\n82.1\n64.2\n88.3\n40.9\n82.4\n81.2\n99.5 76.9 \u00b1 1.0\nDeepSeekOCR\n\u2713\n3\n77.5\n74.5\n77.3\n33.1\n67.3\n83.0\n99.3 73.1 \u00b1 1.0\nChandra-9B\n\u2713\n9\n82.2\n80.3\n88.0\n50.4\n81.2\n92.3\n99.9 81.7 \u00b1 0.9\nolmOCR-2-8B\n\u2713\n8\n82.9\n82.1\n84.3\n48.3\n84.3\n81.4\n99.7 80.4 \u00b1 1.1\nMonkeyOCR-pro-1.2B\n\u00d7\n1.2\n80.5\n62.9\n71.1\n32.9\n68.3\n74.0\n92.6 68.9 \u00b1 1.1\nMinerU2.5\n\u00d7\n1.2\n76.6\n54.6\n84.9\n33.7\n78.2\n81.2\n83.5 70.4 \u00b1 1.0\nPaddleOCR-VL\n\u00d7\n0.9\n85.7\n71.0\n84.1\n37.8\n79.9\n85.7\n98.5 77.5 \u00b1 1.0\nLightOnOCR-1B-1025\n\u2713\n1\n81.4\n71.6\n76.4\n35.2\n80.0\n88.7\n99.5 76.1 \u00b1 1.1\nLightOnOCR-1B-1025-GRPO\n\u2713\n1\n86.5\n73.8\n74.5\n32.9\n85.1\n91.6\n99.7 77.7 \u00b1 1.0\nLightOnOCR-2-1B-base\n\u2713\n1\n84.9\n80.3\n86.7\n47.0\n84.6\n89.1\n99.8 81.8 \u00b1 0.9\nLightOnOCR-2-1B-bbox-base\n\u2713\n1\n84.6\n78.6\n84.7\n46.0\n83.8\n88.0\n99.8 80.8 \u00b1 0.9\nLightOnOCR-2-1B-bbox\n\u2713\n1\n86.9\n74.7\n88.6\n39.7\n85.0\n86.4\n99.8 80.2 \u00b1 0.9\nLightOnOCR-2-1B-bbox-soup\n\u2713\n1\n86.1\n77.9\n88.2\n41.2\n85.4\n87.3\n99.7 80.8 \u00b1 0.9\nLightOnOCR-2-1B-ocr-soup\n\u2713\n1\n86.8\n81.2\n89.0\n45.4\n84.2\n90.3\n99.7 82.4 \u00b1 0.9\nLightOnOCR-2-1B\n\u2713\n1\n89.6\n85.6\n89.0\n42.2\n84.8\n91.4\n99.6 83.2 \u00b1 0.9\nTable 1: OlmOCR-Bench results (headers/footers category excluded; see Appendix C.1). Per-column best is highlighted\nin blue and second best in bold. Results are taken from the corresponding published works; we additionally evaluate\nDeepSeekOCR and the Mistral OCR 3 API since they do not report OlmOCR-Bench numbers. Inference details for our\nmodels are provided in Appendix B.\nChandra-9B [12] under the same evaluation protocol and restrict evaluation to visual elements (figures/images), ignoring\nother layout categories.\nTable 2: Bounding box detection on LightOnOCR-bbox-bench.\nOlmOCR (290)\narXiv (565)\nModel\nF1@0.5\nIoU\nCount Acc.\nF1@0.5\nIoU\nCount Acc.\nChandra-9B\n0.75\n0.71\n75.2\n0.81\n0.77\n81.8\nLightOnOCR-2-1B-bbox\n0.78\n0.70\n83.8\n0.83\n0.77\n85.0\nLightOnOCR-2-1B-bbox-soup\n0.76\n0.67\n80.7\n0.82\n0.76\n85.1\nLocalization quality\nLightOnOCR-2-1B-bbox improves F1@0.5 and count accuracy over the 9B baseline on both\nsubsets, while achieving comparable mean IoU. This indicates reliable detection of both the presence and number of\nfigures, with accurate localization, despite the substantially smaller model size.\n5.3\nEfficiency\nBeyond accuracy, we measure throughput to characterize the practical speed\u2013quality trade-off of end-to-end OCR\nmodels. We measure inference efficiency by running the full OlmOCR-Bench evaluation (1,403 pages) end-to-end\nand reporting pages/sec as the total number of pages divided by the wall-clock time to complete the benchmark. We\nprefer pages/sec over tokens/sec, since tokenization and output lengths differ across models and formats, making\ntokens/sec less comparable. Each model was run using its official library and the inference parameters recommended by\nits respective authors to ensure a fair comparison. Table 3 compares LightOnOCR-2-1B against the main end-to-end\nbaselines; a full comparison with additional systems is provided in Appendix C.3.\nModel\nDtype\nSize (B)\nThroughput (pages/sec)\nLightOnOCR-2\nBF16\n1\n5.71\nolmOCR-2\nFP8\n8\n3.28\nChandra\nBF16\n9\n1.70\nTable 3: Inference throughput on a single NVIDIA H100 (80 GB).\n7\nThese results show that LightOnOCR-2 provides substantially higher throughput than larger end-to-end baselines,\nmaking it practical for high-volume document processing.\n6\nScope and Limitations\nLightOnOCR-2-1B models are designed for printed document understanding and perform particularly well on: (i)\nscientific PDFs, including dense typography and accurate LATEX math transcription (e.g., strong arXiv and \u201cold scans\nmath\u201d results in Table 1); (ii) scans of typed documents, including moderately degraded, noisy or rotated scans;\n(iii) European languages and Latin scripts, reflecting the distributional emphasis of the pretraining mixture; and (iv)\nlayout-heavy pages such as multi-column documents and long-form tables, where faithful reading order and structure\nare critical.\nDespite strong overall robustness, we note two important limitations. First, multilingual performance outside of\nEuropean / Latin-script languages is not fully supported: our training mix and normalization pipeline prioritize\nLatin-script documents, and some non-Latin scripts (e.g., CJK or Arabic) can exhibit degraded fidelity or inefficient\ntokenization compared to the in-scope languages (as shown in Appendix A it is as expected all the more evident\nfor pruned models). Second, handwritten text transcription remains inconsistent: while LightOnOCR-2-1B benefits\nfrom scan coverage, its supervision is primarily derived from printed or typeset sources, and cursive or unconstrained\nhandwriting is not a target use-case for the released checkpoints. We view these as promising directions for future work\nthrough targeted data collection and evaluation.\n7\nConclusion\nWe introduce LightOnOCR-2-1B a 1B-parameter end-to-end OCR VLM setting new state-of-the-art on OlmoOCR-\nBench. Building on top of the LightOnOCR architecture, setup and insight, we detailed the key factors behind its\nimproved performance: a substantially larger and cleaner pretraining mixture, stronger document normalization and\nconversion pipelines, and higher-resolution training with targeted augmentations. We further presented an image-\nlocalization variant trained to predict bounding boxes without degrading OCR by introducing coordinates during\npretraining and refining them with RLVR. Finally, we showed that lightweight weight-space techniques, checkpoint\naveraging and task-arithmetic merging, provide practical gains and enable explicit control of the OCR\u2013bbox trade-off.\nWe release model weights, datasets, and the LightOnOCR-bbox-bench benchmark to support reproducible research on\nhigh-fidelity document extraction and localization.\nAcknowledgments\nThis work was granted access to the HPC resources of IDRIS under GENCI allocations AS011016449, A0181016214,\nand A0171015706, enabling us to use the Jean Zay supercomputer. All RL experiments were run on our in-house H200\nnodes. We thank St\u00e9phane R\u00e9quena and the IDRIS support team for their valuable help. We also thank the LightOn\nteam for their support in making this release possible, with special thanks to Antoine Chaffin and Am\u00e9lie Chatelain for\ntheir help throughout the release process.\nReferences\n[1] Ray Smith. An overview of the tesseract ocr engine. In Proceedings of the International Conference on Document\nAnalysis and Recognition (ICDAR), 2007.\n[2] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequence\nrecognition and its application to scene text recognition. arXiv preprint arXiv:1507.05717, 2015.\n[3] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and\nFuru Wei. Trocr: Transformer-based optical character recognition with pre-trained models. arXiv preprint\narXiv:2109.10282, 2021.\n[4] PaddlePaddle. Paddleocr, 2025. Accessed 2026-01-17.\n[5] Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang,\nQingqing Dang, and Haoshuang Wang. Pp-ocr: A practical ultra lightweight ocr system. arXiv preprint\narXiv:2009.09941, 2020.\n[6] anonymous or see arXiv page. Mineru: An open-source solution for precise document content extraction. arXiv\npreprint arXiv:2409.18839, 2024.\n8\n[7] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for\nacademic documents. In International Conference on Learning Representations, 2024.\n[8] Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Christopher\nWilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models.\narXiv preprint arXiv:2502.18443, 2025.\n[9] Jake Poznanski, Luca Soldaini, and Kyle Lo. olmocr 2: Unit test rewards for document ocr. 2025.\n[10] Yumeng Li, Guang Yang, Hao Liu, Bowen Wang, and Colin Zhang. dots.ocr: Multilingual document layout\nparsing in a single vision-language model. arXiv preprint arXiv:2512.02498, 2025.\n[11] Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou,\nHongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Handong Zheng, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu,\nand Yanjun Ma. Paddleocr-vl: Boosting multilingual document parsing via a 0.9b ultra-compact vision-language\nmodel. arXiv preprint arXiv:2510.14528, 2025.\n[12] Datalab.\nChandra: Ocr model that handles complex tables, forms, handwriting with full layout.\nhttps:\n//github.com/datalab-to/chandra, 2025. GitHub repository, accessed 2026-01-16.\n[13] Said Taghadouini, Baptiste Aubertin, and Adrien Cavaill\u00e8s.\nLightonocr-1b: Making knowledge machine-\nreadable.\nhttps://www.lighton.ai/lighton-blogs/making-knowledge-machine-readable, 2025.\nLightOnOCR v1 blog post.\n[14] Nathan Lambert, Jacob Daniel Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman,\nLester James Validad Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria Graf,\nJena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Christopher Wilhelm, Luca Soldaini, Noah A.\nSmith, Yizhong Wang, Pradeep Dasigi, and Hanna Hajishirzi. T\u00fclu 3: Pushing frontiers in open language model\npost-training. ArXiv, abs/2411.15124, 2024.\n[15] Mistral AI. Mistral small 3.1. https://mistral.ai/news/mistral-small-3-1/, 2025.\n[16] Qwen Team. Qwen3 technical report, 2025.\n[17] Pixparse. Pdf association dataset. https://huggingface.co/datasets/pixparse/pdfa-eng-wds, 2025.\nHugging Face Datasets.\n[18] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang,\nWenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren\nZhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language model\u2019s perception of the world at any resolution.\narXiv preprint arXiv:2409.12191, 2024.\n[19] OpenAI. Gpt-4o system card. https://cdn.openai.com/gpt-4o-system-card.pdf, August 2024. Ac-\ncessed: 2026-01-14.\n[20] NVIDIA. nvtexlive: Fork of tex live with hooks to generate ocr annotations (nvpdftex toolchain). https:\n//github.com/NVIDIA/nvtexlive, 2025.\n[21] KaTeX Contributors. KaTeX: Fast math typesetting for the web. https://katex.org/, 2025. Version 0.16.27."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper\n\n**Title:** LightOnOCR-2-1B: A Compact 1 Billion Parameter End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR\n\n**Main Contributions:**\n1. **LightOnOCR-2-1B Model**: The paper introduces a compact, end-to-end multilingual vision-language model (VLM) named LightOnOCR-2-1B with 1 billion parameters. This model achieves state-of-the-art results on the OlmOCR-Bench benchmark while being significantly smaller and faster than larger models.\n\n2. **Multilingual Coverage**: The model is trained on a large-scale, high-quality dataset that includes French documents, scientific PDFs, and robust handling of scanned materials. It covers a broader range of document types compared to previous models.\n\n3. **End-to-End Localization**: The paper extends the output format of LightOnOCR-2-1B to predict normalized bounding boxes for embedded images in addition to text transcription. This localization is achieved through pretraining with a resume strategy and refinement using Reinforcement Learning with Verifiable Rewards (RLVR), which improves robustness.\n\n4. **Data Curation**: The authors introduce an nvpdftex-based data curation pipeline for arXiv sources, which provides pixel-aligned supervision to strengthen OCR supervision and generate automatic subsets for localization benchmarks.\n\n5. **Lightweight Techniques**: Lightweight weight-space techniques such as checkpoint averaging and task-arithmetic merging are used to combine complementary gains across different runs, enabling controlled trade-offs between OCR quality and bounding box (bbox) accuracy.\n\n6. **New Benchmark**: The paper introduces LightOnOCR-bbox-bench, a new benchmark for image localization in documents, which evaluates the model's ability to accurately localize embedded images within text pages.\n\n**Methodology:**\n- **Training Setup**: The model is trained on a large mixed corpus of OCR datasets using filtering techniques to remove missing references and incompatible samples. Document pages are rendered at 200 DPI during training with maximum longest edge resolution set to 1540 pixels.\n  \n- **Pretraining**: Pretraining involves applying moderate document augmentations such as bitmap corruption, affine shears, and grid distortions. The model is optimized using a next-token prediction objective but masks part of the loss by excluding prompt tokens and image tokens.\n\n- **Reinforcement Learning with Verifiable Rewards (RLVR)**: RLVR rewards are computed deterministically on model outputs for specific OCR failure modes without requiring additional data annotation. Training uses GRPO, a group-scaled reward scheme that samples multiple rollouts per prompt.\n  \n- **Normalization**: A unified normalization pipeline is applied to all sources before mixing and training, ensuring consistent target formats across different datasets.\n\n**Key Findings:**\n1. LightOnOCR-2-1B achieves state-of-the-art results on OlmOCR-Bench while being 9 times smaller and faster than larger models.\n2. The model's localization performance is improved using RLVR, yielding better bounding box detection accuracy compared to the base model without localization capabilities.\n\n**Conclusions:**\nThe paper presents LightOnOCR-2-1B as a compact, end-to-end multilingual vision-language model that outperforms previous state-of-the-art models on OCR benchmarks. The authors demonstrate improvements in both performance and efficiency through novel techniques such as lightweight weight-space merging and data curation pipelines.\n\n**Model Release:**\nThe model checkpoints are released under the Apache 2.0 license, along with the dataset and LightOnOCR-bbox-bench evaluation suite, also under their respective licenses."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Riemannian Liquid Spatio-Temporal Graph Network\nLiangsi Lu\u2217\nlu.liangsi.cn@gmail.com\nGuangdong University of Technology\nGuangzhou, China\nJingchao Wang\u2217\nethanwangjc@163.com\nPeking University\nBeijing, China\nZhaorong Dai\n13725167598@163.com\nSouth China Agricultural University\nGuangzhou, China\nHanqian Liu\n15899789962@163.com\nSun Yat-Sen University\nZhuhai, China\nYang Shi\u2020\nsudo.shiyang@gmail.com\nGuangdong University of Technology\nGuangzhou, China\nAbstract\nLiquid Time-Constant networks (LTCs), a type of continuous-time\ngraph neural network, excel at modeling irregularly-sampled dy-\nnamics but are fundamentally confined to Euclidean space. This\nlimitation introduces significant geometric distortion when rep-\nresenting real-world graphs with inherent non-Euclidean struc-\ntures (e.g., hierarchies and cycles), degrading representation qual-\nity. To overcome this limitation, we introduce the Riemannian\nLiquid Spatio-Temporal Graph Network (RLSTG), a framework\nthat unifies continuous-time liquid dynamics with the geometric\ninductive biases of Riemannian manifolds. RLSTG models graph\nevolution through an Ordinary Differential Equation (ODE) formu-\nlated directly on a curved manifold, enabling it to faithfully cap-\nture the intrinsic geometry of both structurally static and dynamic\nspatio-temporal graphs. Moreover, we provide rigorous theoreti-\ncal guarantees for RLSTG, extending stability theorems of LTCs\nto the Riemannian domain and quantifying its expressive power\nvia state trajectory analysis. Extensive experiments on real-world\nbenchmarks demonstrate that, by combining advanced temporal dy-\nnamics with a Riemannian spatial representation, RLSTG achieves\nsuperior performance on graphs with complex structures. Project\nPage: https://rlstg.github.io\nCCS Concepts\n\u2022 Computing methodologies \u2192Artificial intelligence; \u2022 In-\nformation systems \u2192Information systems applications.\nKeywords\nRiemannian Manifolds, Neural ODEs, Spatio-Temporal Graphs\nACM Reference Format:\nLiangsi Lu, Jingchao Wang, Zhaorong Dai, Hanqian Liu, and Yang Shi. 2026.\nRiemannian Liquid Spatio-Temporal Graph Network. In Proceedings of the\nACM Web Conference 2026 (WWW \u201926), April 13\u201317, 2026, Dubai, United\nArab Emirates. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/\n3774904.3792090\n\u2217Both authors contributed equally to this research.\n\u2020Yang Shi is the corresponding author.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nWWW \u201926, Dubai, United Arab Emirates.\n\u00a9 2026 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-2307-0/2026/04\nhttps://doi.org/10.1145/3774904.3792090\nTime\nContinuous-Time Spatio-Temporal Graph Network\nLiquid Spatio-Temporal Graph Network\nRiemannian Liquid Spatio-Temporal Graph Network\nRiemannian Embedding\nRiemannian Liquid ODE\nEuclidean Liquid\nODE\nLiquid ODE in tangent space\nof Riemannian manifold\nEmbedding\nTime\nTime\nEuclidean space\nODE\nRiemannian space\nFigure 1: Euclidean continuous-time models, including Liq-\nuid networks, distort non-Euclidean graph structures. RL-\nSTG reduces distortion by embedding graphs onto suitable\nRiemannian manifolds. The system\u2019s state evolves on this\ncurved space via a Liquid ODE formulated in the manifold\u2019s\ntangent spaces, enabling a more faithful representation.\n1\nIntroduction\nMany real-world dynamical systems evolve in continuous time,\nwith observations often collected at irregular intervals [32]. For\ninstance, user interactions in social networks occur at sporadic time\npoints, and sensor failures in traffic monitoring can lead to sparse\ndata [14]. Continuous-time models (e.g. CT-RNNs [11], Neural ODE\n[7]) are inherently well-suited for handling such irregularly and\nsparsely sampled time-series data [22, 33]. Liquid Time-Constant\nnetworks (LTCs), an advanced class of continuous-time neural ODE\nmodels [18, 19] that have been successfully extended to graphs [29].\nOwing to their adaptive gating mechanisms and learnable time\nconstants, LTCs have shown exceptional stability and expressivity\nin processing such data.\narXiv:2601.14115v1  [cs.LG]  20 Jan 2026\nWWW \u201926, April 13\u201317, 2026, Dubai, United Arab Emirates.\nLu et al.\nDespite their success in capturing complex temporal patterns,\nexisting continuous-time graph models share a fundamental limita-\ntion: they operate exclusively within Euclidean space [14, 32]. This\nassumption is problematic, as many real-world graphs are inher-\nently non-Euclidean [31]. For example, social networks and citation\ngraphs often exhibit tree-like hierarchies, while transportation net-\nworks and molecular graphs contain numerous cycles [6, 30]. Em-\nbedding these structures into the flat geometry of Euclidean space\ninevitably causes significant distortion, compressing hierarchies\nand stretching cycles, which degrades the quality of the learned\nrepresentations and harms the performance of downstream tasks.\nTo address this geometric distortion, the field of geometric deep\nlearning has proposed modeling graph data on Riemannian man-\nifolds, which offer a richer geometric representation [12]. Hyper-\nbolic manifolds, with their negative curvature, provide an ideal\nspace for embedding hierarchical structures with minimal distor-\ntion, while spherical manifolds are naturally suited for capturing\ncyclical patterns [6, 31]. However, existing Riemannian Graph Neu-\nral Networks (GNNs) have predominantly focused on static graphs\nor employed discrete-time evolution mechanisms [3, 41], the chal-\nlenge of continuous-time dynamics on manifolds has remained\nlargely unexplored.\nTo bridge this gap, we introduce the Riemannian Liquid Spatio-\nTemporal Graph Network (RLSTG), a model to unify LTCs with\nRiemannian geometry for spatio-temporal graphs. We design RL-\nSTG on the tangent spaces of the manifold, using mappings defined\nwithin the context of Riemannian geometry. As conceptualized in\nFigure 1, our approach is uniquely equipped to handle the dual\nchallenges of irregular temporal sampling and complex spatial ge-\nometry.\nOur main contributions are summarized as follows:\n\u2022 We develop a specialized, provably convergent ODE solver\nfor stiff dynamics on manifolds, extend the theoretical guar-\nantees of stability and universal approximation for LTCs to\nthe Riemannian domain and provide expressivity analyses.\n\u2022 We propose RLSTG, a framework integrates liquid continuous-\ntime dynamics with Riemannian geometry for spatio-temporal\ngraph representation learning.\n\u2022 We conduct extensive experiments on real-world bench-\nmarks, demonstrating that RLSTG is significantly superior\nin irregularly sampled data and graphs with non-Euclidean\nstructures.\n2\nRelated Work\nOur work integrates three research pillars: continuous-time neu-\nral models, spatio-temporal graph networks, and geometric deep\nlearning.\n2.0.1\nContinuous-Time and Spatio-Temporal Models. Continuous-\ntime model [7, 11, 22, 33], especially Liquid Time-Constant net-\nworks [19, 20] established a foundation for modeling continuous,\nirregularly sampled dynamics. However, these models and most\ncontinuous-time graph networks are fundamentally Euclidean [10,\n13, 14, 38]. This limitation is also shared by dominant discrete-\ntime models, which are further constrained by fixed time inter-\nvals [16, 28]. RLSTG overcomes these dual constraints by formulat-\ning a stable, liquid continuous-time model that evolves directly on\na geometrically appropriate manifold.\n2.0.2\nGeometric Graph Learning. Geometric deep learning miti-\ngates the metric distortion of embedding non-Euclidean graphs into\nflat space. Foundational work on non-Euclidean GNNs [1, 6, 12, 15,\n31] demonstrated superior performance for static non-Euclidean\ndata. Recently, these geometric concepts have been extended to dy-\nnamic graphs [3, 41], leveraging hyperbolic space for temporal link\nprediction and long-term dynamics. Nevertheless, these advanced\ngeometric approaches still predominantly rely on discrete-time\nupdate mechanisms. RLSTG bridges this gap by unifying LTCs\nwith Riemannian geometry, enabling a continuous-time dynamical\nsystem on a curved space.\n3\nPreliminaries\n3.0.1\nRiemannian Geometry Basics. Riemannian geometry pro-\nvides a rigorous framework for analyzing smooth manifolds en-\ndowed with smoothly varying inner products on their tangent\nspaces. A Riemannian manifold (M,\ud835\udc54) consists of a smooth man-\nifold M together with an inner product \ud835\udc54x(\u00b7, \u00b7) on each tangent\nspace \ud835\udc47xM that depends smoothly on x; since every \ud835\udc47xM is an\ninner-product space, one can define angles, lengths, and geodesics,\nwhere a geodesic is the shortest smooth curve joining two points\nx, y \u2208M. Canonical examples include constant-curvature spaces\nsuch as hyperbolic space H\ud835\udc5b, spherical space S\ud835\udc5b, and Euclidean\nspace E\ud835\udc5b(detailed in Appendix A). More complex geometries arise\nas Cartesian products of simpler factors, yielding a product mani-\nfold that supports heterogeneous structure (for example, H\ud835\udc5b\u00d7 S\ud835\udc5a).\nGiven smooth manifolds M1, . . . , M\ud835\udc58, the product manifold is\nP = M1 \u00d7 M2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 M\ud835\udc58.\n(1)\nA point x \u2208P is written x = [\ud835\udc651, . . . ,\ud835\udc65\ud835\udc58] with \ud835\udc65\ud835\udc56\u2208M\ud835\udc56, and the\ntangent space at x splits as a direct sum, reflecting the component-\nwise differential structure:\n\ud835\udc47xP \u001b \ud835\udc47\ud835\udc651M1 \u2295\u00b7 \u00b7 \u00b7 \u2295\ud835\udc47\ud835\udc65\ud835\udc58M\ud835\udc58.\n(2)\nAccordingly, any tangent vector v \u2208\ud835\udc47xP decomposes as v =\n[\ud835\udc631, . . . , \ud835\udc63\ud835\udc58] with \ud835\udc63\ud835\udc56\u2208\ud835\udc47\ud835\udc65\ud835\udc56M\ud835\udc56. If each factor M\ud835\udc56carries a distin-\nguished origin \ud835\udc5c\ud835\udc56\u2208M\ud835\udc56, the natural origin of the product is\no = [\ud835\udc5c1,\ud835\udc5c2, . . . ,\ud835\udc5c\ud835\udc58] \u2208P.\n(3)\nFor product spaces we adopt a decoupled geodesic distance that\naggregates the factor-wise distances,\n\ud835\udc51P(x, y) =\n\ud835\udc58\n\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc51M\ud835\udc56(\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56),\n(4)\nwhere \ud835\udc51M\ud835\udc56denotes the geodesic distance on M\ud835\udc56. Two fundamental\noperators mediate between points on a manifold and vectors in\nits tangent spaces: the exponential map expx : \ud835\udc47xM \u2192M and\nthe logarithmic map logx : M \u2192\ud835\udc47xM. On P these operators act\ncomponent-wise, implementing manifold-aware moves by indepen-\ndent factor updates:\nExpx(v) =\n\u0002\nexp\ud835\udc3e1\n\ud835\udc651 (\ud835\udc631), exp\ud835\udc3e2\n\ud835\udc652 (\ud835\udc632), . . . , exp\ud835\udc3e\ud835\udc58\n\ud835\udc65\ud835\udc58(\ud835\udc63\ud835\udc58)\n\u0003\n,\n(5)\nRiemannian Liquid Spatio-Temporal Graph Network\nWWW \u201926, April 13\u201317, 2026, Dubai, United Arab Emirates.\nLogx(y) =\n\u0002\nlog\ud835\udc3e1\n\ud835\udc651 (\ud835\udc661), log\ud835\udc3e2\n\ud835\udc652 (\ud835\udc662), . . . , log\ud835\udc3e\ud835\udc58\n\ud835\udc65\ud835\udc58(\ud835\udc66\ud835\udc58)\n\u0003\n.\n(6)\nThe orthogonal projection onto the product tangent space decom-\nposes factorwise:\nProj\ud835\udc47xP(u) =\n\u0002\nProj\ud835\udc47\ud835\udc651 M1 (\ud835\udc621), Proj\ud835\udc47\ud835\udc652 M2 (\ud835\udc622), . . . , Proj\ud835\udc47\ud835\udc65\ud835\udc58M\ud835\udc58(\ud835\udc62\ud835\udc58)\n\u0003\n.\n(7)\nParallel transport also decomposes by factors: if \ud835\udefe(\ud835\udc61) = [\ud835\udefe1(\ud835\udc61), . . . ,\n\ud835\udefe\ud835\udc58(\ud835\udc61)] is a smooth curve in P and v(\ud835\udc61) = [\ud835\udc631(\ud835\udc61), . . . , \ud835\udc63\ud835\udc58(\ud835\udc61)] is parallel\nalong \ud835\udefe(\ud835\udc61), then each \ud835\udc63\ud835\udc56(\ud835\udc61) is parallel along \ud835\udefe\ud835\udc56(\ud835\udc61) in M\ud835\udc56; denoting\nthe parallel transport in M\ud835\udc56from \ud835\udefe\ud835\udc56(0) to \ud835\udefe\ud835\udc56(1) by \ud835\udc43M\ud835\udc56\n\ud835\udefe\ud835\udc56(0)\u2192\ud835\udefe\ud835\udc56(1), the\nproduct transport is given by\n\ud835\udc43P\n\ud835\udefe(0)\u2192\ud835\udefe(1) [\ud835\udc631, . . . , \ud835\udc63\ud835\udc58] =\n\u0002\n\ud835\udc43M1\n\ud835\udefe1 (0)\u2192\ud835\udefe1 (1)\ud835\udc631, . . . , \ud835\udc43M\ud835\udc58\n\ud835\udefe\ud835\udc58(0)\u2192\ud835\udefe\ud835\udc58(1)\ud835\udc63\ud835\udc58\n\u0003\n,\n(8)\nand these operators together enable seamless transitions between\npoints and tangent directions, which are essential for defining and\nintegrating Riemannian ordinary differential equations on product\ngeometries.\n3.0.2\nGraph and Aggregation on Manifolds. We model a continuous-\ntime spatio-temporal graph as\ud835\udc3a(\ud835\udc61) = (V(\ud835\udc61), E(\ud835\udc61), N(\ud835\udc61), E(\ud835\udc61)), where\nV(\ud835\udc61) = {\ud835\udc631(\ud835\udc61), . . . , \ud835\udc63|V| (\ud835\udc61)} is the set of vertices, E(\ud835\udc61) \u2286V(\ud835\udc61) \u00d7\nV(\ud835\udc61) is the set of edges, N(\ud835\udc61) \u2208R|V|\u00d7\ud835\udc51\ud835\udc63is the node feature matrix,\nand E(\ud835\udc61) \u2208R| E|\u00d7\ud835\udc51\ud835\udc52is the edge feature matrix. To capture the data\u2019s\nintrinsic geometry, we embed these features onto a Riemannian\nmanifold P. Each node \ud835\udc63\ud835\udc56is represented by a learnable point x\ud835\udc56\u2208P,\nand each edge (\ud835\udc56, \ud835\udc57) by a point a\ud835\udc56\ud835\udc57\u2208P, mapped from their initial\nfeatures via the exponential map:\nx\ud835\udc56= \ud835\udf19\ud835\udc63(n\ud835\udc56) = Expo(\ud835\udc4a\ud835\udc63n\ud835\udc56+ b\ud835\udc63),\n(9)\na\ud835\udc56\ud835\udc57= \ud835\udf13\ud835\udc52(e\ud835\udc56\ud835\udc57) = Expo(\ud835\udc4a\ud835\udc52e\ud835\udc56\ud835\udc57+ b\ud835\udc52).\n(10)\nNode representations are updated via a message-passing scheme\nadapted for Riemannian manifolds. The process is conducted in\nthe tangent space at the origin o for computational efficiency and\ngeometric consistency. For each neighboring node \ud835\udc57\u2208N (\ud835\udc56), a\nmessage vector m\ud835\udc56\ud835\udc57is computed. This message is a function of the\nsource node, target node, and their connecting edge, all projected\ninto the tangent space:\nm\ud835\udc56\ud835\udc57= \ud835\udf0e\n\u0010\nMLP\n\u0010\u0002\nLogo(x\ud835\udc56), Logo(x\ud835\udc57), Logo(a\ud835\udc56\ud835\udc57)\n\u0003\u0011\u0011\n,\n(11)\nwhere \ud835\udf0eis a non-linear activation function (e.g. tanh). These mes-\nsage vectors are aggregated via summation. The resulting vector,\nwhich may not lie in the tangent space, is projected back to en-\nsure geometric validity. Finally, this projected vector is mapped\nfrom the tangent space to the manifold to produce the updated\nrepresentation for node \ud835\udc56:\nAGG(x)\ud835\udc56= Expo\n\u00a9\u00ad\n\u00ab\nProj\ud835\udc47oP\n\u00a9\u00ad\n\u00ab\n\u2211\ufe01\n\ud835\udc57\u2208N(\ud835\udc56)\nm\ud835\udc56\ud835\udc57\u00aa\u00ae\n\u00ac\n\u00aa\u00ae\n\u00ac\n.\n(12)\nThis message-passing mechanism allows for a rich, geometrically\nsound fusion of neighborhood information, forming the basis for\nour Riemannian graph differential equation.\n3.0.3\nLiquid Time-Constant Networks. Liquid time-constant net-\nworks (LTCs) are a class of continuous-time RNNs with superior\nexpressivity and stability [19]. Instead of an unstructured derivative,\nLTCs use an ODE whose parameters are modulated by an adaptive\ngating mechanism. The dynamics of a hidden state x\ud835\udc38(\ud835\udc61) \u2208E\ud835\udc5bare\ngoverned by:\n\ud835\udc51x\ud835\udc38(\ud835\udc61)\n\ud835\udc51\ud835\udc61\n= \u22121\ne\ud835\udf49sys\n\u2299x\ud835\udc38(\ud835\udc61) +ef(x\ud835\udc38(\ud835\udc61), I(\ud835\udc61),\ud835\udc61,\ud835\udf03) \u2299V\ud835\udc38,\n(13)\nwhere the driving input V is learnable and the key innovation is\nthe liquid time-constant e\ud835\udf49sys, which changes dynamically based on\nthe current state and input via a gating functionef(\u00b7):\ne\ud835\udf49sys =\n\ud835\udf49\n1 + \ud835\udf49\u2299ef(x\ud835\udc38(\ud835\udc61), I(\ud835\udc61),\ud835\udc61,\ud835\udf03)\n.\n(14)\nThe operator \u2299denotes the Hadamard product (element-wise\nmultiplication), and all divisions are also performed element-wise.\nThis state-dependent modulation allows LTCs to learn flexible tem-\nporal patterns while maintaining provably stable, bounded hidden\nstates. However, this formulation is fundamentally Euclidean; op-\nerations like vector addition and scaling are ill-defined on curved\nmanifolds. This necessitates a new formulation of liquid dynamics\nthat respects Riemannian geometry, which we introduce next.\n4\nRiemannian Liquid Spatio-Temporal Graph\nNetwork\nRLSTG is a continuous-time graph neural network that evolves node\nfeatures on a Riemannian manifold via a liquid time-constant ODE.\nWe describe the model\u2019s state representation, ODE formulation, and\nODE solver. We summarize key theoretical properties: the stability\nof the system and the advantage on expressive power.\n4.1\nRiemannian Liquid ODE Formulation\nWe generalize the principles of Liquid Time-Constant networks\n(LTCs) to evolve node states on a Riemannian manifold P. The\nstandard LTCs governs a state x\ud835\udc38in Euclidean space E via a leak\nterm (\u2212x\ud835\udc38) and a driving input (V). We establish a direct analogy\non the manifold. The state\u2019s decay towards the origin o is captured\nby the logarithmic map Logx(o) \u2208\ud835\udc47xP, a tangent vector at the\ncurrent state x pointing towards o. The driving input, represented\nas a vector V \u2208\ud835\udc47oP, is translated to the state\u2019s local frame via\nparallel transport, yielding \ud835\udc43o\u2192x(V) \u2208\ud835\udc47xP.\nThese substitutions yield the Riemannian Liquid Spatio-Temporal\nGraph (RLSTG) ODE. The dynamics for a node state x(\ud835\udc61) \u2208P are\ndefined entirely within its tangent space \ud835\udc47x(\ud835\udc61)P:\n\ud835\udc51x(\ud835\udc61)\n\ud835\udc51\ud835\udc61\n= Proj\ud835\udc47x(\ud835\udc61) P\n\u0012\u0012 1\n\ud835\udf49+ \ud835\udc53(\u00b7)\n\u0013\n\u2299Logx(\ud835\udc61)o+ \ud835\udc53(\u00b7) \u2299\ud835\udc43o\u2192x(\ud835\udc61) (V)\u0001 .\n(15)\nThe dynamics are modulated by a state-dependent gating func-\ntion \ud835\udc53(\u00b7), which is computed for each node \ud835\udc56as:\n\ud835\udc53(x(\ud835\udc61), N(\ud835\udc61), E(\ud835\udc61),\ud835\udc61,\ud835\udf03)\ud835\udc56=\ud835\udf0e(\ud835\udc4axLogo(AGG(x(\ud835\udc61))\ud835\udc56)+\n\ud835\udc4aI\ud835\udf19\ud835\udc63(N(\ud835\udc61)\ud835\udc56) + \ud835\udc4f),\n(16)\nwhere AGG(\u00b7) performs geometric aggregation of neighboring node\nstates as defined in Eq. (12),\ud835\udf19\ud835\udc63(\u00b7) is an encoder defined in Eq. (9) and\n\ud835\udf0eis a non-linear activation function. The function \ud835\udc53(\u00b7) adaptively\nWWW \u201926, April 13\u201317, 2026, Dubai, United Arab Emirates.\nLu et al.\ncontrols the system\u2019s behavior: it simultaneously modulates the\nrate of decay towards the stable equilibrium point o and scales the\ninfluence of a learnable driving input vector V. This dual role allows\nthe system to learn complex temporal dynamics, balancing stability\nwith responsiveness to new inputs.\n4.2\nGeodesic Decay ODE Solver\nNumerically integrating Eq. (15) is challenging due to stiffness\ncaused by the adaptive decay term [17]. Standard explicit solvers\nrequire prohibitively small step sizes for stability, while implicit\nmethods are too computationally expensive on manifolds.\nTo overcome this, we introduce the Geodesic Decay (GD) ODE\nsolver, a specialized integrator based on operator splitting [27, 35].\nThis technique decomposes the ODE\u2019s vector field into its non-stiff\n(driving) and stiff (decay) components. We handle the stiff decay\nanalytically for stability and integrate the non-stiff driving term\nefficiently, yielding a robust solution that avoids the limitations of\npurely explicit or implicit methods.\nThe solver update, illustrated in Figure 2, is a two-stage process.\nThe driving step advances the state to an intermediate point x\u2217:\nx\u2217= Expx(\ud835\udc61)\n\u0010\n\u0394\ud835\udc61\u00b7 \u0000\ud835\udc53(x(\ud835\udc61), N(\ud835\udc61), E(\ud835\udc61),\ud835\udc61,\ud835\udf03) \u2299\ud835\udc43o\u2192x\ud835\udc61(V)\u0001 \u0011\n.\n(17)\nThe GD step pulls x\u2217towards the origin using a state-dependent\ndecay factor \ud835\udefcto yield the final state:\n\ud835\udefc= \ud835\udc52\u2212\u0394\ud835\udc61\u00b7( 1\n\ud835\udf49+\ud835\udc53(x(\ud835\udc61),N(\ud835\udc61),E(\ud835\udc61),\ud835\udc61,\ud835\udf03)),\n(18)\nx(\ud835\udc61+ \u0394\ud835\udc61) = Expo\n\u0010\n\ud835\udefc\u2299Logo(x\u2217)\n\u0011\n.\n(19)\nThe complete forward propagation for a discrete time step \u0394\ud835\udc61\ud835\udc58\nfrom state x\ud835\udc58is thus:\nx\ud835\udc58+1 = Expo\n\u0010\n\ud835\udc52\u2212\u0394\ud835\udc61\ud835\udc58\n\u0002\n1\n\ud835\udf49+\ud835\udc53(\u00b7)\n\u0003\n\u2299\nLogo\n\u0010\nExpx\ud835\udc58\n\u0000\u0394\ud835\udc61\ud835\udc58( \ud835\udc53(\u00b7) \u2299\ud835\udc43o\u2192x\ud835\udc58(V) )\u0001\u0011\u0011\n,\n(20)\nwhere \ud835\udc53(\u00b7) = \ud835\udc53(x(\ud835\udc61), N(\ud835\udc61), E(\ud835\udc61),\ud835\udc61,\ud835\udf03).\nTheorem 1. Let the manifold be a product manifold P. Consider\nthe ordinary differential equation on P given by Eq. (15). The GD\nsolver defined by Eq. (20) is a first-order convergent method, where\nthe global error \ud835\udc38global over a finite time interval [0,\ud835\udc47] is bounded by:\n\ud835\udc38global \u2248\ud835\udc42(\u0394\ud835\udc61).\n(21)\nAs proven in Appendix B, this solver is first-order convergent.\nThe GD ODE solver therefore provides a robust and efficient frame-\nwork for integrating our stiff dynamical system on the manifold,\nmaking it exceptionally well-suited for deep learning architectures.\n4.3\nNumerical Stability of RLSTG\nEstablishing the numerical stability of RLSTG is critical for its reli-\nability. Riemannian embeddings, particularly in hyperbolic space,\nare commonly prone to numerical instability [6]. Therefore, pro-\nviding a rigorous theoretical guarantee for stability is not merely\na formality but a practical necessity. This requires generalizing\nthe stability analysis of Euclidean LTCs [19] to the non-trivial Rie-\nmannian setting, where curved geometry fundamentally alters the\nsystem dynamics.\n1.Driving step\n2.Geodesic decay\nFigure 2: GD ODE solver employs an operator splitting tech-\nnique to decouple the stiff dynamics. (1) Driving step. An\nexplicit Euler update advances the state from x(\ud835\udc61) to an in-\ntermediate point x\u2217based on the non-stiff driving input. (2)\nGD: The intermediate state x\u2217is analytically pulled towards\nthe origin o to yield the final state x(\ud835\udc61+ \u0394\ud835\udc61).\nTheorem 2. Let the state of a node, x(\ud835\udc61), evolve on a product\nRiemannian manifold P = \u00ce\ud835\udc58\n\ud835\udc56=1 M\ud835\udc56with origin o = (o1, . . . , o\ud835\udc58)\naccording to Eq. (15). We define the system time constant for each\ncomponent as (\ud835\udf49sys)\ud835\udc56=\n\ud835\udf49\ud835\udc56\n1+\ud835\udf49\ud835\udc56\ud835\udc53\ud835\udc56(\u00b7) . Then, for each component \ud835\udc56of the\nsystem, this quantity is bounded as follows:\n\ud835\udf49\ud835\udc56\n1 + \ud835\udf49\ud835\udc56\n\u2264(\ud835\udf49sys)\ud835\udc56\u2264\ud835\udf0f\ud835\udc56.\n(22)\nAs proven in Appendix C, this theorem provides a foundational\nguarantee for stability. By leveraging the [0, 1] range of the sigmoid\ngating function \ud835\udc53\ud835\udc56(\u00b7), we show that the adaptive time constant\ngoverning the system\u2019s decay rate is strictly bounded. This prevents\nvanishing or exploding decay rates, a crucial step towards ensuring\nthe overall boundedness of the state trajectory.\nWith the system time constant bounded, we establish our main\nstability result: the state trajectory itself is confined to a fixed region\non the manifold.\nTheorem 3. On the product manifold P = \u00ce\ud835\udc58\n\ud835\udc56=1 M\ud835\udc56, denote the\nnorm on M\ud835\udc56as \u2225\u00b7\u2225\ud835\udc56. Let the state x(\ud835\udc61) \u2208P evolve according to the Rie-\nmannian Liquid ODE. Assume that the condition\ud835\udc51\ud835\udc56(expo\ud835\udc56(\u00b1V\ud835\udc56), o\ud835\udc56) \u2265\n\ud835\udf49\ud835\udc56\u2225V\ud835\udc56\u2225\ud835\udc56is met for each component. We denote \ud835\udc45= \ud835\udc51(Expo(V), o)\nand \ud835\udc5f= \ud835\udc51(Expo(\u2212V), o). Then for any finite time interval [0,\ud835\udc47], the\nstate x(\ud835\udc61) is bounded within an annulus centered at the origin o:\n\ud835\udc5f\u2264\ud835\udc51(x(\ud835\udc61), o) \u2264\ud835\udc45.\n(23)\nThe proof in Appendix D analyzes the time evolution of the\nstate\u2019s geodesic distance to the origin, \ud835\udc51(x(\ud835\udc61), o). We show that the\nODE\u2019s vector field creates a potential well on the manifold, ensuring\nany trajectory is perpetually confined within the boundary.\n4.4\nExpressive power of RLSTG\n4.4.1\nUniversal Approximation Theorem. We establish the universal\napproximation capability for RLSTG, extending the known property\nfrom Euclidean models [11, 19, 21, 24, 25] to our framework on\ncurved Riemannian manifolds, thereby bridging a key theoretical\ngap.\nTheorem 4. Assume that the solution u(\ud835\udc61) of an ODE on Rie-\nmannian manifold exists and is unique on the time interval [0,\ud835\udc47],\nand the solution trajectory is contained within a compact subset \ud835\udc37\nRiemannian Liquid Spatio-Temporal Graph Network\nWWW \u201926, April 13\u201317, 2026, Dubai, United Arab Emirates.\nof an open set \ud835\udc46\u2282P. Given an arbitrary precision \ud835\udf00> 0 and a time\ninterval length \ud835\udc47> 0, there exists a RLSTG, with dynamics defined\nby Eq. (20), such that the sequence {x\ud835\udc58} generated by the discrete\nnetwork iterations satisfies:\nmax\n0\u2264\ud835\udc58\u2264\ud835\udc41\ud835\udc51(x\ud835\udc58, u(\ud835\udc61\ud835\udc58)) < \ud835\udf00.\n(24)\nThe complete proof is provided in Appendix E. The core strat-\negy is to project the target vector field onto a common tangent\nspace, where the classical universal approximation theorem can be\ninvoked. We show that RLSTG architecture is capable of realizing\nthis approximating network and bound the total error to prove that\nour model\u2019s trajectory can approximate the target trajectory to any\nspecified precision.\n4.4.2\nTrajectory Length as an Expressivity Metric. We quantify the\nmodel\u2019s expressive power via the length of its state trajectory in the\ntangent space. Let the global state be \ud835\udc4b(\ud835\udc61) = [\ud835\udc651(\ud835\udc61), . . . ,\ud835\udc65\ud835\udc5b(\ud835\udc61)] \u2208P\nand fix an origin \ud835\udc5c\u2208P so that Log\ud835\udc5cis well-defined on the (compact)\ndomain reached by the trajectories. Write the tangent representa-\ntion \ud835\udc4d(\ud835\udc61) = Log\ud835\udc5c(\ud835\udc4b(\ud835\udc61)) = [\ud835\udc671(\ud835\udc61), . . . ,\ud835\udc67\ud835\udc5b(\ud835\udc61)] \u2208\ud835\udc47\ud835\udc5cP \u001b R\ud835\udc51and use\nthe Euclidean norm \u2225\u00b7 \u2225\ud835\udc5con \ud835\udc47\ud835\udc5cP. The expressivity metric is the\ncurve length:\n\u2113(\ud835\udc4d) :=\n\u222b\ud835\udc47\n0\n\ud835\udc51\n\ud835\udc51\ud835\udc61\ud835\udc4d(\ud835\udc61)\n\ud835\udc5c\ud835\udc51\ud835\udc61.\n(25)\nA longer trajectory implies a greater capacity to capture complex dy-\nnamics. Our analysis establishes that the expected trajectory length\nis lower-bounded by the quality of the spatial graph representation,\ncaptured by the expected squared norm of node representations in\nthe tangent space, E[||\ud835\udc67||2]:\nE[\ud835\udc59(\ud835\udc4d(\ud835\udc61))] \u2265O(\ud835\udc36arch \u00b7 E[||\ud835\udc67||2]) \u00b7 \ud835\udc59(\ud835\udc3c(\ud835\udc61)),\n(26)\nwhere \ud835\udc36arch is an architecture-dependent constant and \ud835\udc59(\ud835\udc3c(\ud835\udc61)) is the\ninput signal\u2019s trajectory length.\nThis inequality directly links expressivity to metric distortion.\nEmbedding a graph into a space of mismatched curvature (e.g.,\na tree in Euclidean space) compresses the representation, reduc-\ning E[||\ud835\udc67||2] and thus limiting the model\u2019s expressive power. Con-\nversely, aligning the manifold\u2019s curvature with the graph\u2019s intrinsic\ngeometry (e.g., Hyperbolic for trees, Spherical for cycles) minimizes\ndistortion. This yields a more expansive representation with a larger\nE[||\ud835\udc67||2], thereby enhancing expressivity. The subsequent theorems\nformalize this geometric advantage for canonical graph structures.\nTheorem 5. For an RLSTG model processing a tree of depth \ud835\udc37, the\nlower bounds on the expected trajectory length E[\ud835\udc59(\ud835\udc4d(\ud835\udc61))] for embed-\ndings in Euclidean space E and Hyperbolic space H are, respectively:\nE[\ud835\udc59(\ud835\udc4d\ud835\udc38(\ud835\udc61))] \u2265O(\ud835\udc36arch \u00b7 \ud835\udc37) \u00b7 \ud835\udc59(\ud835\udc3c(\ud835\udc61)),\n(27)\nE[\ud835\udc59(\ud835\udc4d\ud835\udc3b(\ud835\udc61))] \u2265O(\ud835\udc36arch \u00b7 \ud835\udc372) \u00b7 \ud835\udc59(\ud835\udc3c(\ud835\udc61)).\n(28)\nProof. By the architectural trajectory\u2013length lower bound es-\ntablished earlier,\nE\n\u0002\n\u2113(\ud835\udc4d(\ud835\udc61))\n\u0003\n\u2265O\n\u0010\n\ud835\udc36arch \u00b7 E\n\u0002\n\u2225\ud835\udc67\u22252\u0003\u0011\n\u2113(\ud835\udc3c(\ud835\udc61)).\n(29)\nIt suffices to bound E\n\u0002\n\u2225\ud835\udc67\ud835\udc57\u22252\u0003\nin the two ambient geometries.\nFor the Euclidean case, embed the tree\ud835\udc47with \ud835\udc5bnodes into R\ud835\udc5bby\nfixing an orthonormal basis {\ud835\udc52\ud835\udc63}\ud835\udc63\u2208\ud835\udc49, setting \ud835\udc53(\ud835\udc5c) = \u00ae0 for the root\n\ud835\udc5c, and for a non-root \ud835\udc63with parent \ud835\udc5ddefining \ud835\udc53(\ud835\udc63) = \ud835\udc53(\ud835\udc5d) + \ud835\udc52\ud835\udc63. If\n\ud835\udc51\ud835\udc3a(\ud835\udc5c,\ud835\udc65\ud835\udc57) = \ud835\udc58and the unique path is (\ud835\udc630 = \ud835\udc5c, \ud835\udc631, . . . , \ud835\udc63\ud835\udc58= \ud835\udc65\ud835\udc57), then\n\ud835\udc53(\ud835\udc65\ud835\udc57) =\n\ud835\udc58\n\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc52\ud835\udc63\ud835\udc56,\n(30)\n\ud835\udc51\ud835\udc38(\ud835\udc5c,\ud835\udc65\ud835\udc57)2 =\n\ud835\udc58\n\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc52\ud835\udc63\ud835\udc56\n2\n=\n\ud835\udc58\n\u2211\ufe01\n\ud835\udc56=1\n\u2225\ud835\udc52\ud835\udc63\ud835\udc56\u22252 = \ud835\udc58= \ud835\udc51\ud835\udc3a(\ud835\udc5c,\ud835\udc65\ud835\udc57).\n(31)\nHence for a tree of depth \ud835\udc37,\nE\n\u0002\n\u2225\ud835\udc67\ud835\udc57\u22252\n\ud835\udc38\n\u0003\n= E\n\u0002\n\ud835\udc51\ud835\udc38(\ud835\udc5c,\ud835\udc65\ud835\udc57)2\u0003\n= E[\ud835\udc51\ud835\udc3a(\ud835\udc5c, \ud835\udc57)] = O(\ud835\udc37),\n(32)\nand inserting into (29) yields\nE[\u2113(\ud835\udc4d\ud835\udc38(\ud835\udc61))] \u2265O\u0000\ud835\udc36arch \u00b7 \ud835\udc37\u0001 \u2113(\ud835\udc3c(\ud835\udc61)).\n(33)\nFor the hyperbolic case, it is known that any finite tree admits\nan embedding into the hyperbolic plane with arbitrarily small dis-\ntortion [34], which gives dist(\ud835\udc53) \u22641 + \ud835\udf00. Consequently,\n\ud835\udc51H(\ud835\udc5c,\ud835\udc65\ud835\udc57) = \u0398\u0000\ud835\udc51\ud835\udc3a(\ud835\udc5c, \ud835\udc57)\u0001,\n\u2225\ud835\udc67\ud835\udc57\u2225\ud835\udc3b= \ud835\udc51H(\ud835\udc5c,\ud835\udc65\ud835\udc57) = \u0398\u0000\ud835\udc51\ud835\udc3a(\ud835\udc5c, \ud835\udc57)\u0001,\n(34)\nand for depth \ud835\udc37,\nE\n\u0002\n\u2225\ud835\udc67\ud835\udc57\u22252\n\ud835\udc3b\n\u0003\n= \u0398\u0000E\n\u0002\n\ud835\udc51\ud835\udc3a(\ud835\udc5c, \ud835\udc57)2\u0003\u0001 = \u0398(\ud835\udc372).\n(35)\nPlugging this into (29) gives\nE[\u2113(\ud835\udc4d\ud835\udc3b(\ud835\udc61))] \u2265O\u0000\ud835\udc36arch \u00b7 \ud835\udc372\u0001 \u2113(\ud835\udc3c(\ud835\udc61)).\n(36)\n\u25a1\nTheorem 6. Let \ud835\udc36\ud835\udc5bbe the \ud835\udc5b-cycle and let \ud835\udc4d\ud835\udc38(\ud835\udc61) and \ud835\udc4d\ud835\udc46(\ud835\udc61) denote\nthe RLSTG trajectories in Euclidean and Spherical latent geometries.\nThen\nE\n\u0002\n\u2113(\ud835\udc4d\ud835\udc38(\ud835\udc61))\n\u0003\n\u2265\ud835\udc36arch\n4\ud835\udf0b2 \ud835\udc5b2 \u2113(\ud835\udc3c(\ud835\udc61)) .\n(37)\nMoreover,\nE\n\u0002\n\u2113(\ud835\udc4d\ud835\udc46(\ud835\udc61))\n\u0003\n\u2265\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n\ud835\udc36arch\n12\n(\ud835\udc5b2 \u22121) \u2113(\ud835\udc3c(\ud835\udc61)) ,\n\ud835\udc5bodd,\n\ud835\udc36arch\n12\n(\ud835\udc5b2 + 2) \u2113(\ud835\udc3c(\ud835\udc61)) ,\n\ud835\udc5beven,\n(38)\nand therefore, for all \ud835\udc5b\u22653,\nE[\u2113(\ud835\udc4d\ud835\udc46(\ud835\udc61))]\nE[\u2113(\ud835\udc4d\ud835\udc38(\ud835\udc61))] \u2265\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\ud835\udf0b2\n3\n\u0012\n1 \u22121\n\ud835\udc5b2\n\u0013\n,\n\ud835\udc5bodd,\n\ud835\udf0b2\n3\n\u0012\n1 + 2\n\ud835\udc5b2\n\u0013\n,\n\ud835\udc5beven,\n(39)\nand in particular\nlim\n\ud835\udc5b\u2192\u221e\nE[\u2113(\ud835\udc4d\ud835\udc46(\ud835\udc61))]\nE[\u2113(\ud835\udc4d\ud835\udc38(\ud835\udc61))] = \ud835\udf0b2\n3 .\n(40)\nProof. By the generic lower bound (29), a universal constant\nis absorbed into \ud835\udc36arch\ud835\udc60\ud835\udc5c\ud835\udc61\u210e\ud835\udc4e\ud835\udc61E[\u2113(\ud835\udc4d(\ud835\udc61))] \u2265\ud835\udc36arch E[\u2225\ud835\udc67\ud835\udc57\u22252] \u2113(\ud835\udc3c(\ud835\udc61))\nholds for both geometries.\nEuclidean case. Place vertices at\ud835\udc53\ud835\udc38(\ud835\udc57) = \ud835\udc5f\ud835\udc38\n\u0000 cos(2\ud835\udf0b\ud835\udc57/\ud835\udc5b), sin(2\ud835\udf0b\ud835\udc57/\ud835\udc5b)\u0001,\n\ud835\udc5f\ud835\udc38=\n1\n2 sin(\ud835\udf0b/\ud835\udc5b) , so adjacent Euclidean distances equal 1. With the\nbase point at the center, \u2225\ud835\udc67\ud835\udc57\u2225\ud835\udc38= \ud835\udc5f\ud835\udc38for all \ud835\udc57, hence E[\u2225\ud835\udc67\ud835\udc57\u22252\n\ud835\udc38] =\n\ud835\udc5f2\n\ud835\udc38\u2265\n\ud835\udc5b2\n4\ud835\udf0b2 , since sin\ud835\udc65\u2264\ud835\udc65on [0, \ud835\udf0b/2], and thus\nE\n\u0002\n\u2113(\ud835\udc4d\ud835\udc38(\ud835\udc61))\n\u0003\n\u2265\ud835\udc36arch\n4\ud835\udf0b2 \ud835\udc5b2 \u2113(\ud835\udc3c(\ud835\udc61)) .\n(41)\nWWW \u201926, April 13\u201317, 2026, Dubai, United Arab Emirates.\nLu et al.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\nFigure 3: Expressive power analysis. (a) Mean trajectory length versus tree depth (D) in Euclidean and Hyperbolic spaces. (b)\nMean trajectory length versus cycle size (n) in Euclidean and Spherical spaces. (c, d) 2D Principal Component Analysis (PCA,\ndescribed in [19]) projections of the system trajectory for a tree graph (D=10) in Euclidean and Hyperbolic space. (e, f) 2D\nPCA projections for a cycle graph (n=10) in Euclidean and Spherical space. (g-j) Variance explained by the first four principal\ncomponents (bars) and their cumulative contribution (black line). Each plot corresponds to an experimental setting: (g) Tree in\nEuclidean, (h) Tree in Hyperbolic, (i) Cycle in Euclidean, and (j) Cycle in Spherical space. In (c-f), the color gradient on the\ntrajectory line indicates the temporal progression from the start to the end state.\nSpherical case. Embed \ud835\udc36\ud835\udc5bisometrically on \ud835\udc461 of radius \ud835\udc5f\ud835\udc46=\n\ud835\udc5b/(2\ud835\udf0b) so consecutive vertices have geodesic distance 1 [5]. Let\n\ud835\udc5a\u2208{0, 1, . . . , \u230a\ud835\udc5b/2\u230b} be the graph distance from a fixed base point\nalong the shorter arc; then \u2225\ud835\udc67\ud835\udc57\u2225\ud835\udc46= \ud835\udc5aand\nE[\u2225\ud835\udc67\ud835\udc57\u22252\n\ud835\udc46] =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n2\n2\ud835\udc58+ 1\n\ud835\udc58\n\u2211\ufe01\n\ud835\udc5a=1\n\ud835\udc5a2 = (2\ud835\udc58+ 1)2 \u22121\n12\n= \ud835\udc5b2 \u22121\n12\n,\n\ud835\udc5b= 2\ud835\udc58+ 1,\n2\n2\ud835\udc58\n\ud835\udc58\u22121\n\u2211\ufe01\n\ud835\udc5a=1\n\ud835\udc5a2 + 1\n2\ud835\udc58\ud835\udc582 = \ud835\udc5b2 + 2\n12\n,\n\ud835\udc5b= 2\ud835\udc58,\n(42)\nwhere\nE\n\u0002\n\u2113(\ud835\udc4d\ud835\udc46(\ud835\udc61))\n\u0003\n\u2265\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n\ud835\udc36arch\n12\n(\ud835\udc5b2 \u22121) \u2113(\ud835\udc3c(\ud835\udc61)) ,\n\ud835\udc5bodd,\n\ud835\udc36arch\n12\n(\ud835\udc5b2 + 2) \u2113(\ud835\udc3c(\ud835\udc61)) ,\n\ud835\udc5beven.\n(43)\nThe ratio bounds follow by division of the two estimates above.\n\u25a1\nWhen geometry aligns with structure, expressivity improves\u2014\ud835\udc372\nvs. \ud835\udc37on trees and, on cycles, \u0398(\ud835\udc5b2) in both geometries with a prov-\nable constant-factor advantage for Spherical. We empirically vali-\ndate these findings in Figure 3. For a tree graph (c, d), the trajectory\nin Hyperbolic space is visibly more complex and expansive than\nits constrained counterpart in Euclidean space. A similar observa-\ntion holds for a cycle graph (e, f), where the Spherical embedding\nenables a richer, more intricate trajectory. Finally, the bottom row\n(g-j) quantifies this complexity by showing the variance explained\nby the principal components. The higher cumulative variance ex-\nplained by the first few components in the geometrically-aligned\nspaces (h, j) compared to the Euclidean ones (g, i) indicates that\nthe learned dynamics are higher-dimensional and more expressive.\nTogether, these results provide a rigorous and visually intuitive\nconfirmation that aligning the model\u2019s geometry with the data\u2019s\nstructure is fundamental to unlocking its expressive potential.\n5\nExperiments\nTo validate the RLSTG framework, our empirical evaluation is cen-\ntered on two canonical tasks, node feature regression and edge\nlink prediction. These tasks are chosen to demonstrate the model\u2019s\ncapacity to capture complex spatial geometries and its effectiveness\nin handling evolving network topologies. The evaluation is com-\npleted by ablation studies that quantify the impact of the chosen\ngeometric space and GD ODE solver.\n5.1\nRLSTG Adaptation for Downstream Tasks\nRLSTG outputs \ud835\udc65\ud835\udc56\u2208P. Project to \ud835\udc47\ud835\udc5cP \u001b R\ud835\udc51and apply Euclidean\nheads.\nNode Regression.\n\u210e\ud835\udc56= Log\ud835\udc5c(\ud835\udc65\ud835\udc56) \u2208R\ud835\udc51, \u02c6\ud835\udc66\ud835\udc56= MLPreg(\u210e\ud835\udc56).\n(44)\nLink Prediction.\n\u210e\ud835\udc56= Log\ud835\udc5c(\ud835\udc65\ud835\udc56),\n\u210e\ud835\udc57= Log\ud835\udc5c(\ud835\udc65\ud835\udc57) \u2208R\ud835\udc51,\n(45)\n\u210e\ud835\udc56\ud835\udc57= [ \u210e\ud835\udc56;\u210e\ud835\udc57] \u2208R2\ud835\udc51,\n\ud835\udc60\ud835\udc56\ud835\udc57= MLPlink(\u210e\ud835\udc56\ud835\udc57),\n\ud835\udc5d\ud835\udc56\ud835\udc57= \ud835\udf0e(\ud835\udc60\ud835\udc56\ud835\udc57).\n(46)\n5.2\nNodes Features Regression\nWe evaluate RLSTG on node feature regression using three traffic\nforecasting benchmarks: METR-LA [28], PEMS03 [16], and PEMS04\n[16]. The METR-LA dataset is a widely-used real-world dataset\nfor traffic forecasting tasks. It contains traffic speed data collected\nRiemannian Liquid Spatio-Temporal Graph Network\nWWW \u201926, April 13\u201317, 2026, Dubai, United Arab Emirates.\nTable 1: MAE \u2193performance comparison for node fea-\ntures regression on traffic datasets, averaged over 5 separate\nruns. The best results are in bold and the second best are\nunderlined.\nMethod\nMETR-LA\nPEMS03\nPEMS04\nDCRNN\n4.55 \u00b1 0.14\n19.53 \u00b1 0.17\n26.03 \u00b1 0.24\nGMAN\n3.78 \u00b1 0.31\n18.44 \u00b1 0.35\n25.90 \u00b1 0.29\nAGCRN\n3.58 \u00b1 0.19\n18.21 \u00b1 0.08\n25.01 \u00b1 0.17\nSTGODE\n2.89 \u00b1 0.02\n17.70 \u00b1 0.05\n24.58 \u00b1 0.06\nSTG-NCDE\n2.93 \u00b1 0.04\n17.72 \u00b1 0.10\n24.46 \u00b1 0.09\nTGODE\n2.98 \u00b1 0.02\n17.69 \u00b1 0.07\n24.70 \u00b1 0.04\nLTC\n2.90 \u00b1 0.03\n17.74 \u00b1 0.04\n24.85 \u00b1 0.07\nRLSTG\n2.77 \u00b1 0.01\n17.49 \u00b1 0.02\n24.08 \u00b1 0.01\nfrom 207 sensors on the highways of Los Angeles County over a\nperiod of 4 months. The PEMS03 dataset, from the Caltrans Perfor-\nmance Measurement System (PeMS), records 3 months of traffic\nflow data from 358 sensors in a Californian district. The PEMS04\ndataset also originates from the Caltrans Performance Measure-\nment System (PeMS) and covers traffic data from 307 sensors over\n2 months in another district. We compare against several previ-\nous models: DCRNN [28], GMAN [43], AGCRN [2], STGODE [10],\nSTG-NCDE [8], TGODE [14], LTC [19].\nTo simulate realistic, non-uniform observation patterns, we sub-\nsample the time series, ensuring a minimum temporal distance\nof 3 between consecutive points. The task is to predict the fea-\ntures of the next sampled time step given the current one. We\nsubsample time indices T = {\ud835\udc611 < \u00b7 \u00b7 \u00b7 < \ud835\udc61\ud835\udc40} with a minimum\ngap of 3 steps to emulate irregular observations. Data are split\ntrain/val/test by time in an 8:1:1 ratio. For each \ud835\udc56, the input is\n(\ud835\udc41\ud835\udc56, \u0394\ud835\udc61\ud835\udc56) with \ud835\udc41\ud835\udc56\u2208R|\ud835\udc49|\u00d7\ud835\udc51and \u0394\ud835\udc61\ud835\udc56= \ud835\udc61\ud835\udc56+1 \u2212\ud835\udc61\ud835\udc56; the target is \ud835\udc41\ud835\udc56+1.\nLoss is LMAE =\n1\n|\ud835\udc49| \ud835\udc51\n\u02c6\ud835\udc41\ud835\udc56+1 \u2212\ud835\udc41\ud835\udc56+1\n1.\nThe performance comparison, measured by Mean Absolute Er-\nror (MAE), is summarized in Table 1. The results indicate that our\nproposed model consistently achieves a lower prediction error com-\npared to a wide range of existing methods across all traffic datasets.\nThis suggests that capturing the intrinsic geometry of the traffic\nnetwork leads to more accurate forecasts.\n5.3\nEdges Links Prediction\nWe conduct experiments on the ENRON social network dataset [4]\nfor links prediction task. The ENRON dataset is a dynamic graph\ndataset that records email communications among 184 employ-\nees over 3 years. We compare RLSTG with previous methods, in-\ncluding JODIE [26], DyRep [37], TGAT [40], TCL [39], TGN [32],\nGraphMixer [9], DyGFormer [42], HTGN [41], FreeDyG [36], HG-\nWaveNet [3].For the Enron email network, messages are grouped\nby day to form snapshots {(\ud835\udc49\ud835\udc56, \ud835\udc38\ud835\udc56)}\ud835\udc41\n\ud835\udc56=1. We predict edges at \ud835\udc61\ud835\udc56+1\ngiven the history up to and including \ud835\udc61\ud835\udc56. The timeline is split\n70%/15%/15% for train/val/test. Positives are \ud835\udc43\ud835\udc56+1 = \ud835\udc38\ud835\udc56+1. Negatives\nare drawn with historical negative sampling: from \ud835\udc3b\ud835\udc56= \u00d0\n\ud835\udc58\u2264\ud835\udc56\ud835\udc38\ud835\udc58,\nsample an equal-sized set \u02dc\ud835\udc41\ud835\udc56+1 \u2286\ud835\udc3b\ud835\udc56\\ \ud835\udc38\ud835\udc56+1 (hard negatives that\npreviously occurred but are absent at \ud835\udc61\ud835\udc56+1). We additionally report\nperformance on transductive (both endpoints seen in training) and\ninductive (at least one new node) subsets. The loss on balanced\npositives/negatives: LBCE = \u2212\n1\n| D\ud835\udc56+1|\n\u00cd\n(\ud835\udc62,\ud835\udc63,\ud835\udc66)\u2208D\ud835\udc56+1\nh\n\ud835\udc66log \u02c6\ud835\udc5d(\ud835\udc56+1)\n\ud835\udc62\ud835\udc63\n+\nTable 2: Average Precision \u2191for temporal link-prediction\nresults on ENRON datasets with historical negative sampling\nstrategy, averaged over 5 separate runs. The best results are\nin bold and the second best are underlined.\nMethod\nTransductive\nInductive\nJODIE\n69.89 \u00b1 0.21\n66.51 \u00b1 0.64\nDyRep\n71.13 \u00b1 0.52\n64.72 \u00b1 0.23\nTCL\n70.23 \u00b1 0.12\n67.88 \u00b1 0.43\nTGAT\n64.98 \u00b1 0.53\n61.12 \u00b1 0.17\nTGN\n73.18 \u00b1 0.32\n62.29 \u00b1 0.74\nGraphMixer\n72.88 \u00b1 0.32\n71.29 \u00b1 0.34\nDyGFormer\n75.16 \u00b1 1.22\n68.29 \u00b1 2.38\nHTGN\n79.87 \u00b1 0.78\n70.73 \u00b1 0.83\nFreeDyG\n79.98 \u00b1 0.54\n71.97 \u00b1 0.27\nHGWaveNet\n80.55 \u00b1 0.48\n71.42 \u00b1 0.38\nRLSTG\n83.14 \u00b1 0.21\n73.26 \u00b1 0.10\nTree-likeness\nLoops\nTCC\nStructure Stability\nTree-likeness\nTCC\nStructure Stability\nTree-likeness\nTCC\nStructure Stability\nTree-likeness\nTCC\nStructure Stability\nLoops\nLoops\nLoops\nFigure 4: Geometric analysis of benchmark datasets. Tree-\nlikeness is measured by 1\n\ud835\udeff. Loops is measured by ln(1 + \ud835\udefd1). \ud835\udeff\ndenotes Gromov hyperbolicity and \ud835\udefd1 the first Betti number.\n(1 \u2212\ud835\udc66) log \u00001 \u2212\u02c6\ud835\udc5d(\ud835\udc56+1)\n\ud835\udc62\ud835\udc63\n\u0001i\n, where D\ud835\udc56+1 = \ud835\udc43\ud835\udc56+1 \u222a\u02dc\ud835\udc41\ud835\udc56+1 and \u02c6\ud835\udc5d(\ud835\udc56+1)\n\ud835\udc62\ud835\udc63\nis the\nmodel\u2019s predicted probability for edge (\ud835\udc62, \ud835\udc63) at \ud835\udc61\ud835\udc56+1. Table 2 shows\nRLSTG demonstrates advantage in predicting future connections,\noutperforming other methods in both transductive and inductive\nscenarios. This highlights its robust generalization capability for\ndynamic graphs with evolving topologies.\n5.4\nAblation Studies\n5.4.1\nAblation 1: The Impact of Geometric Space. To isolate and\nquantify the contribution of the geometric representation space, we\nconducted a rigorous ablation study, with results presented in Table\n3. The analysis demonstrates that aligning the model\u2019s geometry\nwith the data\u2019s intrinsic structure is more critical to performance\nthan simply increasing representation capacity. Recognizing that\nreal-world graphs rarely conform perfectly to specific spaces, the\nselection of an appropriate product manifold is guided by a quanti-\ntative analysis of the data\u2019s intrinsic topological properties, aiming\nto mitigate geometric distortion. This analysis is detailed in the\nAppendix G.3 and visualized in Figure 4, which reveals the distinct\ngeometric signatures of the benchmark datasets and provides a\nbasis for interpreting the ablation results.\nWWW \u201926, April 13\u201317, 2026, Dubai, United Arab Emirates.\nLu et al.\nTable 3: Geometric Ablation on Node Regression and Link Prediction Tasks. The best results are in bold and the second best are\nunderlined, averaged over 5 separate runs.\nNodes Features Regression (MAE\u2193)\nLink Prediction (AP\u2191)\nManifold\nMETR-LA\nPEMS03\nPEMS04\nTransductive\nInductive\nE17\n2.85 \u00b1 0.02\n17.60 \u00b1 0.02\n24.21 \u00b1 0.01\n80.03 \u00b1 0.23\n71.05 \u00b1 0.28\nE16\n2.86 \u00b1 0.01\n17.62 \u00b1 0.02\n24.17 \u00b1 0.02\n80.11 \u00b1 0.37\n71.15 \u00b1 0.22\nS16\n2.85 \u00b1 0.03\n17.51 \u00b1 0.01\n24.22 \u00b1 0.04\n76.24 \u00b1 0.16\n64.10 \u00b1 0.25\nH16\n2.83 \u00b1 0.02\n17.52 \u00b1 0.02\n24.12 \u00b1 0.02\n83.14 \u00b1 0.21\n73.26 \u00b1 0.11\nH8 \u00d7 E8\n2.80 \u00b1 0.02\n17.59 \u00b1 0.02\n24.11 \u00b1 0.03\n81.50 \u00b1 0.25\n71.40 \u00b1 0.19\nE8 \u00d7 S8\n2.77 \u00b1 0.01\n17.49 \u00b1 0.02\n24.19 \u00b1 0.02\n79.40 \u00b1 0.24\n68.30 \u00b1 0.28\nH8 \u00d7 S8\n2.83 \u00b1 0.02\n17.55 \u00b1 0.04\n24.08 \u00b1 0.01\n81.03 \u00b1 0.26\n70.95 \u00b1 0.17\nTable 4: ODE Solver Ablation."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThe paper introduces Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a novel framework that combines continuous-time liquid dynamics with geometric inductive biases on Riemannian manifolds. The main contributions are:\n1. **Specialized ODE Solver**: Develops a provably convergent Ordinary Differential Equation (ODE) solver for stiff dynamics on manifolds, extending stability and universal approximation guarantees to the Riemannian domain.\n2. **RLSTG Framework**: Proposes RLSTG as a unified framework that integrates liquid continuous-time dynamics with Riemannian geometry for spatio-temporal graph representation learning.\n3. **Theoretical Guarantees**: Provides rigorous theoretical guarantees, including stability analyses and expressivity analyses via state trajectory analysis.\n\n#### Methodology\n**Riemannian Manifolds**: The model operates on tangent spaces of Riemannian manifolds, enabling the embedding of complex spatial geometries like hierarchies and cycles. This approach avoids geometric distortion inherent in Euclidean space.\n**Geometric ODE Formulation**: RLSTG formulates dynamics directly on curved manifolds using Liquid Time-Constant (LTC) principles adapted to Riemannian geometry. The system\u2019s state evolves via a Liquid ODE formulated in the manifold's tangent spaces.\n\n#### Key Findings\n1. **Stability and Expressivity**: Extensive experiments demonstrate that RLSTG achieves superior performance on graphs with complex structures, including trees and cycles.\n2. **Expressive Power Analysis**: The paper establishes universal approximation capabilities for RLSTG, quantifying its expressive power via trajectory length in tangent spaces of Riemannian manifolds.\n\n#### Conclusions\nRLSTG overcomes the limitations of existing continuous-time graph models by operating on curved manifolds, thereby preserving intrinsic geometries. This approach enables more accurate and robust learning of complex spatio-temporal dynamics, particularly for graphs with non-Euclidean structures like hierarchies and cycles. The framework's theoretical guarantees and empirical validation underscore its potential as a powerful tool in geometric deep learning.\n\n### Detailed Breakdown\n\n#### Introduction\nThe paper addresses the limitations of existing continuous-time graph models that operate exclusively within Euclidean space, which fails to capture complex spatial geometries inherent in real-world graphs (e.g., hierarchies and cycles). The authors propose RLSTG as a solution by integrating liquid continuous-time dynamics with Riemannian geometry.\n\n#### Related Work\n1. **Continuous-Time Models**: Continuous-time models like Liquid Time-Constant networks have been successful in handling irregularly sampled time-series data.\n2. **Geometric Graph Learning**: Existing geometric graph learning approaches embed non-Euclidean graphs into flat spaces, but these are limited by fixed time intervals and do not address continuous-time dynamics on manifolds.\n\n#### Preliminaries\n1. **Riemannian Geometry Basics**: The paper introduces Riemannian geometry concepts such as tangent spaces, geodesics, exponential maps, and logarithmic maps.\n2. **Graph Representation on Manifolds**: RLSTG models graph evolution by embedding features onto a product manifold and using message-passing schemes adapted for manifolds.\n\n#### Riemannian Liquid ODE Formulation\n1. **Formulating Dynamics**: The paper generalizes LTC principles to evolve node states directly on curved manifolds, capturing decay towards the origin via logarithmic maps.\n2. **Geodesic Decay Solver**: Introduces a specialized solver (GD) for integrating stiff dynamics, ensuring numerical stability.\n\n#### Numerical Stability and Expressive Power\n1. **Numerical Stability**: The paper establishes theoretical guarantees for RLSTG's stability using Riemannian manifolds, bounding the state trajectory within fixed regions.\n2. **Expressive Power Analysis**: Quantifies expressivity via trajectory length in tangent spaces of Riemannian manifolds, showing that aligning geometry with intrinsic graph structures enhances expressive power.\n\n#### Experiments\n1. **Node Feature Regression**: Evaluates RLSTG on traffic forecasting benchmarks (METR-LA, PEMS03, PEMS04), demonstrating superior performance compared to existing methods.\n2. **Edge Link Prediction**: Tests RLSTG on the ENRON social network dataset for link prediction tasks, outperforming previous methods in both transductive and inductive scenarios.\n\n### Conclusion\nRLSTG represents a significant advancement in geometric deep learning by leveraging Riemannian manifolds to preserve intrinsic geometries of complex graphs. The combination of liquid dynamics with geometric inductive biases ensures robust performance on irregularly sampled and structured data, setting the stage for future applications in dynamic graph analysis."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "COLLECTIVE INTELLIGENCE IN SCIENCE:\nDIRECT ELICITATION OF DIVERSE INFORMATION FROM\nEXPERTS WITH UNKNOWN INFORMATION STRUCTURE\nALEXEY V. OSIPOV\u2217,\u2021 AND NIKOLAY N. OSIPOV\u2020,\u00a7,\u00b6\nAbstract. Suppose we need a deep collective analysis of an open scientific\nproblem: there is a complex scientific hypothesis and a large online group\nof mutually unrelated experts with relevant private information of a diverse\nand unpredictable nature. This information may be results of experts\u2019 indi-\nvidual experiments, original reasoning of some of them, results of AI systems\nthey use, etc. We propose a simple mechanism based on a self-resolving play-\nmoney prediction market entangled with a chat. We show that such a system\ncan easily be brought to an equilibrium where participants directly share their\nprivate information on the hypothesis through the chat and trade as if the\nmarket were resolved in accordance with the truth of the hypothesis. This\napproach will lead to efficient aggregation of relevant information in a com-\npletely interpretable form even if the ground truth cannot be established and\nexperts initially know nothing about each other and cannot perform complex\nBayesian calculations. Finally, by rewarding the experts with some real assets\nproportionally to the play money they end up with, we can get an innovative\nway to fund large-scale collaborative studies of any type.\nThe instruments like the Bayesian truth serum [1], its refinements [2, 3, 4],\nBayesian markets [5], or the peer prediction method [6] can effectively incentivize\nexperts to truthfully report their private signals in settings where their reports re-\nspond to a single question and cannot be directly verified. The reported signals can\nbe binary in response to questions like \u201cAre you satisfied with your life?\u201d, multi-\nvalued, or even continuous in response to questions like \u201cWhat is your probability\nestimate that humanity will survive past the year 2100?\u201d. Such universal mech-\nanisms are effective and robust in most cases. However, there is a particular but\nimportant case where we need a dedicated, more specialized tool.\nNamely, suppose we need a deep collective analysis of an open scientific problem.\nTo be more specific, suppose there is a complex scientific hypothesis H and a\nlarge online group of experts whose pooled private information suffices to prove\n\u2217Swiss Re Institute, Z\u00a8urich, Switzerland\n\u2020St. Petersburg State University, St. Petersburg, Russia\n\u2021LLC Usetech Professional, Moscow, Russia\n\u00a7St.\nPetersburg Department of V. A. Steklov Institute of Mathematics of the\nRussian Academy of Sciences, St. Petersburg, Russia\n\u00b6HSE University, International Laboratory of Game Theory and Decision Making,\nSt. Petersburg, Russia\nE-mail addresses: nicknick AT pdmi DOT ras DOT ru.\nKey words and phrases. interpretability, wisdom of crowd, play money, prediction market,\ninformation pooling, information elicitation, rational expectation equilibrium, direct communica-\ntion, large language models, scientific collaboration.\nThe work was supported by the Russian Science Foundation grant 24-11-00087.\n1\narXiv:2601.14047v1  [cs.GT]  20 Jan 2026\n2\nALEXEY V. OSIPOV AND NIKOLAY N. OSIPOV\nits validity.\nThis information may be of an arbitrarily complex nature: results\nof complex experiments, content of academic publications, sophisticated reasoning\nof some experts or LLMs they use, with the LLMs\u2019 answers depending heavily\non the skills of the questioners, etc. Namely, not only the information itself, but\nthe information structures of experts may be diverse and unpredictable, and the\nexperts may initially know nothing about how each other\u2019s information is arranged.\nIn this setting, if we attempt, for example, to iteratively utilize the Bayesian truth\nserum, repeatedly eliciting the experts\u2019 probabilities of H and publicizing them in\norder to reach a common consensus probability [7, 8], then the result will hardly\nbe equal to 1. Below we discuss this issue from a theoretical perspective: in terms\nof [8], the direct and indirect communication equilibria do not have to coincide in\nour setting. In addition to the fact that the consensus probability may be far from\nthe probability conditional on the pooled information, it is merely a number that\ncannot be interpreted by an analyst who is ignorant of the experts\u2019 information\nstructure. Thus, the result may be almost useless scientifically.\nThe same problems would arise if we tried, relying on the theory [9, 10, 11, 12],\nto get a consensus probability by means of a conventional prediction market with\nbetting on the truth of H. Again, since the situation in question can hardly be\ndescribed through finite and commonly known information structure, we can expect\nthat the result would tend to be far from the direct communication equilibrium\nand, in addition, would be uninterpretable. Moreover, two new issues would arise.\nFirst, we need to know the ground truth in order to resolve a conventional prediction\nmarket. This looks like a vicious circle: in order to reward experts for their attempt\nto validate H, we need a conclusive proof or disproof of H. Second, various empirical\nstudies like [13, 14] demonstrate that even if the information structure is commonly\nknown and information pooling is theoretically achievable, this may not be the case\nin reality because experts need to be perfect Bayesians in order to effectively elicit\ninformation from aggregates such as prediction market prices.\nNevertheless, we claim the following. Consider a self-resolving play-money pre-\ndiction market where each participant starts with one and the same amount of play\nmoney and winners are determined by a binary random generator with probabil-\nity parameter equal to the market final price. We argue that such a market can\neasily be brought to an equilibrium where participants directly share their private\ninformation on H via chat and trade as if the market were resolved in accordance\nwith the truth of H. This approach will lead to efficient aggregation of diverse\ninformation in a completely interpretable form even if the information structure\nis not initially commonly known. Furthermore, the analyst do not need to know\nthe ground truth in order to reward experts, and experts do not need to perform\nBayesian calculations in order to update their information. For example, if H is\na true hypothesis and the experts\u2019 pooled information suffices to prove it, then it\nwill be proved even in situations where we cannot hope to achieve this by other\nmethods and, what is more, the proof will be provided explicitly. By rewarding the\nwinners with some real assets proportionally to the play money they end up with,\nwe can effectively incentivize large-scale collaborative research of any type.\nWe borrow the term \u201cself-resolving prediction market\u201d from [15], where simi-\nlar markets are experimentally studied in another context. Another example of\nself-resolving markets is described in [16]. The idea of a mechanism that provides\nincentives to elicit explanations for beliefs from experts was first proposed in our\nCOLLECTIVE INTELLIGENCE IN SCIENCE\n3\nworking paper [17]. We thereby anticipated the later trend [18, 19]. In the present\narticle, we will describe in detail and formalize the aforementioned idea in the stan-\ndard setting of epistemic game theory. Before that, we will demonstrate that when\nit comes to a complex scientific hypothesis, the presence of direct information ex-\nchange is not just a useful feature, but an \u201call or nothing\u201d question. It is also worth\nnoting that in the aforementioned working paper [17, Appendix C], we additionally\nconsidered an extremely general setting in which experts were assumed to be nei-\nther Bayesian with respect to their information, nor risk neutral, nor rational in the\nsense of the von Neumann\u2013Morgenstern axioms, and the exchange of information\nwas an arbitrary flow of signals that were not specified a priori. Not fully formalized\nreasoning based on the analysis of extensive data from a conventional prediction\nmarket showed that even in such a setting, the desired behavior of experts can be\nexpected.\n1. Problem\nThroughout this section, we consider a simple, but illustrative example of a sit-\nuation where the existing instruments and the current consensus theory are insuffi-\ncient. We emphasize that this is only an introductory example and our subsequent\nresults address the general situation where experts\u2019 knowledge may be arbitrary.\n(exm) Suppose there are experts with a common prior \u03c0 over the set \u2126of possible\nstates of the world. Suppose H, A, and B are pairwise disjoint nonnull\nevents in \u2126, and H is a true hypothesis (i.e., it contains the true state of\nthe world). Proceeding as in the most basic examples, we suppose one of\nthe experts knows that A is false, another expert knows that B is false, and\nall the others possess only the public information, which is incorporated in\nthe common prior.\nIn addition to the experts, consider an analyst who has no private information and\nknow nothing about the experts\u2019 information structure, but would like to effectively\nassess the plausibility of H. Suppose the analyst has some way to incentivize the\nexperts to give true answers about their beliefs about H, for example, by means\nof the Bayesian truth serum [1, 2, 3, 4]. In this case, the answers of two informed\nexperts will be \u03c0(H | \u00acA) and \u03c0(H | \u00acB), while all the other answers will be \u03c0(H).\nHere \u00acA and \u00acB are the complementary events of A and B, respectively. If, for\nexample,\n\u03c0(H) = \u03c0(A) = \u03c0(B) = 1\n3,\n(1)\nthen the answers of both informed experts will be\n1\n2.\nSince the analyst knows\nnothing about the experts\u2019 information structure, he or she cannot interpret the\nanswers. Instead, he or she can facilitate indirect communication between experts\nin order for them to interpret each other\u2019s beliefs. Namely, the analyst can make\nthe answers public, repeat the survey to get the updated beliefs, make them pub-\nlic, repeat the survey, and so on. We show that the result of such a procedure\nwill strongly depend on what the experts believe about each other\u2019s information\nstructures.\nThe information structures can be described [7] as partitions of \u2126: each expert\nis informed of that element of his or her partition that contains the true state of the\nworld. In particular, \u00acA and \u00acB are elements of the partitions of the correspond-\ning knowledgeable experts in the example under discussion. In this example, we\nalso assume that \u03c0 is a non-atomic measure and do not assume that the experts\u2019\n4\nALEXEY V. OSIPOV AND NIKOLAY N. OSIPOV\ninformation partitions are commonly known among them (as was the case in [7, 8]).\nSuch a setting is natural if we interpret H as a complex scientific hypothesis, and\nthe experts as a large online group of researchers who do not know each other and\nmay possess information of arbitrary nature: results of experiments and measure-\nments of arbitrary complexity and accuracy, answers of various LLMs to differently\nworded questions, content of academic publications, etc.\nWe note that the non-atomicity of \u03c0 by no means implies that experts simulta-\nneously reflect on infinite number of separate heterogeneous objects. Although our\nexample is simplified and does not map directly to the real world, this continuity\nassumption comes precisely from reality: for instance, the numerical results of a\nparticular complex experiment rule out a continuum of its other results that were\nthought possible (cf. A), and leave a continuum of possibilities for the results of\nthe other experiments (cf. \u00acA). Informally, the non-atomicity of \u03c0 means that\nit is impossible to guess which experiment was conducted based on the posterior\nprobability of H alone (i.e., to guess an expert\u2019s information partition).\nNext, situations with incomplete knowledge of each other\u2019s partitions can be\nformalized by means of types and hierarchies of beliefs [20, 21] or, what is the\nsame [22], through new \u201cpartitions\u201d (\u03c3-algebras in fact) over a certain extension\nof \u2126, which may be treated as commonly known structures. It is important to note\nthat these new commonly known \u201cpartitions\u201d may be infinite even if the initial ones\nwere finite.\nFor simplicity, we make the following assumption on the experts\u2019 belief about\neach other\u2019s information structures.\n(blf0) It is commonly believed among the experts that the private information\nof any of them is one-directional, i.e., that each of them can, at most,\ndetermine the truth or falsity of some subset of \u00acH.\nWe will assume (blf0) only in the context of (exm).\nThe framework (blf0) ap-\nproximates a scenario where a hypothesis cannot be confirmed directly, but can be\ntested through various experiments. Each trial either increases the plausibility of\nthe hypothesis if successful, or conclusively disproves it if not. For example, in drug\ndevelopment, various preclinical trials (in vitro and in vivo studies) serve this role:\npositive results increase confidence in the potential efficacy of a new drug in future\nhuman clinical trials, while any negative result may preclude its viability and lead\nto cancellation of further trials at all.\nThus, it is common knowledge that each expert has either proved the hypothe-\nsis \u00acH, or ruled out some variants of its realization (maybe a null set of variants).\nTherefore, after the first survey and publicizing the results, two of which turn out\nto be greater than the prior \u03c0(H) and the others coincide with \u03c0(H), it will become\ncommonly known among the experts that some two nonnull events A and B within\n\u00acH are not true. Their priors \u03c0(A) and \u03c0(B) will also be calculated by the experts\nand will become commonly known among them (but not A and B themselves). The\nupdated beliefs about H and, as a consequence, the results of the second survey will\nstrongly depend on the experts\u2019 beliefs about overlapping of A and B. One of the\ntwo extreme cases is where after the first iteration it will be common knowledge\namong experts that A and B represent completely different information and are\ntherefore pairwise disjoint. In this case the second iteration will produce the final\nCOLLECTIVE INTELLIGENCE IN SCIENCE\n5\nconsensus coinciding with\n\u03c0(H | \u00acA \u2229\u00acB) =\n\u03c0(H)\n1 \u2212\u03c0(A) \u2212\u03c0(B).\nNamely, in terms of [8] the revision process will take 2 steps to converge and there\nwill be no discrepancy between the direct and indirect communication equilibria. In\nparticular, if (1) holds, then the analyst will see that H is true with probability 1.\nBut from a Bayesian perspective, it is natural for the experts to expect some\nsimilarity in their private information, and the situation worsens once they doubt\nthat their information is mutually complementary. The corresponding extreme case\nis where the experts have the false common belief that their private information has\na single source and is not complementary at all, i.e., where after the first iteration\nit will be common belief that A and B are nested. Since \u03c0 is non-atomic, this belief\nis always internally consistent no matter what \u03c0(A) and \u03c0(B) will be revealed to\nbe. In this case, the second survey will produce the final consensus equal to\nmax\n\u0000\u03c0(H | \u00acA), \u03c0(H | \u00acB)\n\u0001\n=\n\u03c0(H)\n1 \u2212max\n\u0000\u03c0(A), \u03c0(B)\n\u0001.\nIn particular, if (1) holds, then the consensus probability of H will be\n1\n2: the\nresult will change from 1 to 1\n2, i.e., from complete certainty to the maximum en-\ntropy! However, both extreme cases do not seem realistic, and in the case of (1),\nsome intermediate value will be obtained if after the first iteration it will be com-\nmon knowledge among the experts that each of them considers the possible values\nof \u03c0(A \u222aB) to be uniformly distributed between the extreme values 1\n3 and 2\n3. In\nthis case, the second iteration will lead to the final consensus equal to the value\nZ\n2\n3\n1\n3\ndx\n1 \u2212x = log 2 \u22480.69,\nwhich is between 1\n2 and 1 and is also far from the pooling probability 1.\nThe underlying cause of such inefficiency is that the situation cannot be reduced\nto commonly known finite information partitions: no matter how we extend \u2126.\nThis implies that the direct and indirect communication equilibria need no longer\ncoincide almost surely, as was the case in [8]. If we consider even a more realistic\nsituation without simplification (blf0), then the result will in no way become more\nrobust.\nThus, the result of the procedure under discussion may be almost arbitrary and\nuninterpretable in a setting where we are trying to aggregate extremely diverse\ninformation and arguments about a complex open problem. This leads us to the\nquestion whether there is a way to elicit experts\u2019 pooled information in an inter-\npretable form, together with the probability of H given this information, in the\ncase where the information structure is arbitrarily complex and is not commonly\nknown. In particular, for the above example the desired mechanism should always\nreturn the probability of H equal to \u03c0(H | \u00acA\u2229\u00acB), together with the information\n\u00acA \u2229\u00acB explicitly presented.\nWe emphasize once again that when discussing Bayesian truth serum, we are\ndiscussing the use of a very universal tool in a very specific situation for which it is\nnot and should not be adapted. Therefore, our arguments in no way diminish the\n6\nALEXEY V. OSIPOV AND NIKOLAY N. OSIPOV\nsignificance of this tool, and the specialized system we propose below cannot serve\nas a substitute for it.\n2. Preliminaries\nIn prediction markets, participants can trade assets each of which binds a seller\nto pay a buyer 1 unit of real or play money once a certain proposition is revealed\nto be true. Symmetrically, sellers keep the money received if the proposition is\nrevealed to be false. For efficient matching of buyers and sellers, an automated\nmarket maker [23] can be applied (see also [24], where extremely flexible one is\ndescribed). A well-known empirical fact is that clearing prices are well-calibrated\nin real-money prediction markets [25] as well as in play-money ones [26] in the\nsense of [27]: prices approximate the real frequencies of true propositions. This\nfact by no means implies the convergence of prices in such markets to common\nposteriors or, even less, efficient information pooling, but it makes the corresponding\nquestions reasonable. In this connection, we note that the consensus theory [7,\n8] is naturally extended with the theory [9, 10, 11, 12] that explains why the\nprice in such a market may converge to a common posterior for the proposition\nin question. Instead of the experts\u2019 beliefs being iteratively revealed and updated,\nthe price dynamically aggregates them and, being commonly known, causes them\nto be dynamically updated. Additionally, in [11, 12] information pooling is studied\ntheoretically in the context where information partitions are finite and commonly\nknown.\nSince any aggregate of experts\u2019 beliefs cannot contain more information than\nthe beliefs themselves, using a prediction market would not solve the problems\ndescribed in the previous section. As above, information pooling need not even\ntheoretically be efficient in the situation where the information structure cannot be\nsimultaneously finite and commonly known. In addition, the result would not be\ninterpretable. Moreover, new problems would arise for a conventional prediction\nmarket.\nFirst, to distribute experts\u2019 gains and losses after trading, we need to\nknow the ground truth about the proposition in question. This fact severely limits\nthe applicability of traditional prediction markets, especially for solving scientific\nproblems. Second, there are experimental evidences [13, 14] that even in a situation,\nwhere efficient information pooling is theoretically possible, this may not be the case\nin reality. One of the reasons may be the complexity of computations experts need\nto perform in order to elicit each other\u2019s information from the price.\nOn the other hand, there are certain trivial situations, where (and only where)\nthe efficiency of a prediction market is virtually guaranteed theoretically and ex-\nperimentally. We mean the following basic property of prediction markets, which\ncan be considered as the simplest degenerate case of the theory [9, 10].\n(eff0) Suppose there are experts whose subjective probabilities of the truth of a\ncertain proposition take the same value, and this fact is a common belief\namong them. If we arrange, for these experts, a prediction market associ-\nated with the proposition in question, its final price will coincide with that\nvalue.\nThis scenario is equivalent to the situation where experts share a common prior and\nall relevant information about the proposition is common knowledge. Under the\nassumptions of (eff0), the persistent price coinciding with the experts\u2019 probabilities\nwill be completely consistent with the experts\u2019 hierarchies of beliefs. We also note\nCOLLECTIVE INTELLIGENCE IN SCIENCE\n7\nthat if the common belief assumption is violated on any level of any hierarchy, then\nthe hierarchies will eventually be updated and the probabilities (first-order beliefs)\nmay change (together with the price).\nIn the mentioned study [14], property (eff0) has been confirmed experimentally.\nWe take a closer look at that study, where the authors also supported the idea of\nutilizing play-money prediction markets with real incentives for solving scientific\nproblems. More specifically, their experiments simulated the search for an answer\nto the question \u201cIn what order do three genes activate each other?\u201d Above, we\nhave referred to the volatility of their experimental results in the setting where\nthe information of each expert was private and transmitted to others through the\nprices only. In such a setting, information pooling was broken in half the cases\ndespite a finite (but rather complex) and commonly known information structure.\nOn the other hand, in the setting where all the information was public, the final\nprice perfectly approximated the conditional probability given that information in\nall the cases. The same was true for the third setting where the information was\nmade public gradually by the experimenter.\nIs it possible to solve all the problems discussed above relying only on the simple\nand robust property (eff0) of prediction markets? Namely, can we arrange a system\nin which\n\u2022 private information would be directly made public by experts themselves (in-\nterpretability),\n\u2022 we would be under conditions where only public information needs to be pro-\ncessed by the market (independence from the information structure and complex\nBayesian calculations on it), and\n\u2022 gains and losses would be distributed depending only on the information ob-\ntained, not on the ground truth (self-resolvability)?\nWe demonstrate that such a system can be naturally obtained from a prediction\nmarket through minor modifications. We also provide a complete formalization of\nthe solution through a theoretical model that is minimally necessary to capture the\nmain ideas.\n3. Solution\nSuppose we are interested in a certain scientific hypothesis H and would like\nto inspire its deep collective analysis by a large group E of experts. As the first\n(optional) step, we can publicly query an LLM for all available information relevant\nto H like \u201cProvide detailed and verifiable arguments for and against the scientific\nhypothesis [a description of H]\u201d. This step may get us closer to the situation of a\ncommon prior \u03c0 over a certain sample space \u2126containing H and of a substantial\npart of experts being ignorant, i.e., possessing only the public information provided\nby the LLM and incorporated in that prior. This also will lead us to a clearer\ndistinction between public and private information. Namely, only the information\nthat the experts perceive as private will remain private, such as their unpublished\nresults or original reasoning.\nNext, we can arrange a play-money prediction market that will satisfy the fol-\nlowing rules.\n(r1) Each participant initially gets one and the same limited amount of play\nmoney.\n8\nALEXEY V. OSIPOV AND NIKOLAY N. OSIPOV\n(r2) The trade inactivity of announced duration is a necessary condition for\nclosing the market.\n(r3) Winners are determined by a binary random generator that implements\nBernoulli distribution with probability parameter equal to the market final\nprice.\n(r4) A public chat is provided for sharing any information directly.\nThese four rules are public and commonly known.\nIn order to entangle such a\nmarket with the hypothesis H, we can make the following public invitation to the\nexperts.\n(inv0) Trade as if the market would resolve in accordance with the truth of H and,\nafter entering the market, share your private information on H through the\nchat in an interpretable and verifiable form.\nIt can be shown that such a course of action, publicly proposed to the experts,\nforms an equilibrium in the sense that if an expert believes the others follow invita-\ntion (inv0) commonly believing in the same (i.e., an expert believes that any other\nexpert follow (inv0), and believes that any other expert believes that any other\nexpert follow (inv0), and so on), then he or she is bound, in order to make a profit,\nalso to follow it. It is worth noting that such an equilibrium could be described in\nterms of separate strategies as a Bayesian Nash equilibrium. However, this would\nonly complicate the model, at the same time making it less realistic. The fact is\nthat for the required equilibrium to occur, experts do not need to consider each\nother\u2019s strategies. It is sufficient for each expert to reflect only on the probabili-\nties that other experts assign to the generated binary outcome and on which they\nbase their strategies. The corresponding formal model will be provided in the final\nsection, while here we discuss the main idea underlying it.\nConsider an arbitrary expert n \u2208E with information In that is the element\nof his or her partition of \u2126containing the true state of the world. For our basic\nscenario, we make certain simplifying assumptions that make information sharing\nstructured and allow us to incorporate it into the formal model.\n(shr1) We assume that n can publicly share In with others in a completely inter-\npretable and verifiable form, thereby making it common knowledge.\n(shr2) We also assume that the expert n cannot prove to others any false infor-\nmation, and that providing such unverifiable information does not affect\nothers\u2019 beliefs and is identified with being silent.\n(shr3) Finally, we assume that the information In is indivisible in the sense that it\ncannot be transmitted and proved to others in parts in such a way that one\npart would increase the probability of H and the next part would decrease\nit, or vice versa.\nAssumption (shr1) means that n can reveal his or her partition of \u2126(e.g., describe\nan experiment he or she has conducted), report that its element In has turned out\nto be true (e.g., report the results of the experiment), and provide comprehensive\nevidence of this fact (e.g., provide the entire experimental data). Assumption (shr2)\nmeans that false information cannot be verifiable and unverifiable information has\nzero impact. Concerning assumption (shr3), we will see that it can be easily relaxed\nwithin the extended framework sketched in the next section, where we will also\ndiscuss the relaxation of (shr1), (shr2), and other assumptions.\nCOLLECTIVE INTELLIGENCE IN SCIENCE\n9\nBy e1 \u2286E we denote the subgroup of all ignorant experts, which are the experts\nwho possess only the public information In = \u2126(i.e., their knowledge is completely\nincorporated in the prior \u03c0). We suppose it is common knowledge that such experts\nexist and that their number suffices to move the price as required anywhere below.\nWe also note that due to (shr2), ignorant experts will be silent: they have no private\ninformation that could be shared in verifiable form to become common knowledge.\nFurthermore, we do not assume that experts know each other\u2019s information struc-\ntures. Moreover, we assume that they do not speculate on them at all: each expert n\nconsiders e1\u222a{n} to be the only participants in the market, at least as long as other\nexperts do not enter it. In other words, each expert initially believes that there\nare many ignorant experts and does not go any further in his or her analysis. In\nparticular, ignorant experts initially speculate only on the existence and beliefs of\nother ignorant experts.\n(blf1) Among ignorant experts, it is commonly believed that they are ignorant,\nand they do not have any beliefs about informed experts at all.\nAs we have noted above, even under experimental conditions where the informa-\ntion structure is common knowledge (but not the information itself), experts who\nare not specialists in Bayesian calculations often behave as if they do not know\nthat structure at all [14]. The unawareness assumption is all the more natural in\ncomplex settings where experts initially know nothing about each other. Never-\ntheless, in the next section we will discuss how this assumption can be dropped\ncompletely (including (blf1)). In addition, throughout this section it can be seen\nthat all arguments and conclusions can be left unchanged even if we extend (blf1)\nas follows (cf. the Bayesian shift described in [1]: one\u2019s belief about someone else\u2019s\nbelief on a subject shifts toward one\u2019s own belief on that subject).\n(blf2) Each expert n \u2208E believes that any other expert in E may either possess,\nwith a certain small probability, the same information In, or be ignorant.\nThe expert n also believes that the others believe the same in the context\nof their own information, and so on.\nAs long as only the experts from e1 trade, a desired equilibrium is naturally\nmaintained. To see this, consider a certain expert n in e1. While n believes the\nothers follow invitation (inv0) commonly believing in the same, he or she expects\nthem to jointly move the price to \u03c0(H). Indeed, we have assumed that n does not\nspeculate on the information of experts outside e1 before they enter the market\n(see (blf1)). Thus, n believes the others will trade as in a prediction market where\nthere is no private signals related to an outcome and where \u03c0(H) is the commonly\nknown probability of that outcome. Therefore, the basic property (eff0) will make\nn to believe that the price will tend to \u03c0(H) along with the probability parameter\nof the outcome generator. Furthermore, we can expect that n does not believe it\nis possible for him or her to distort the result of such concerted trading actions:\nrule (r1) implies that several experts have more resources than a single one, rule (r2)\nimplies they always have time to react to the price distortion, and, finally, n hardly\nmay expect to affect others\u2019 beliefs via a price shift. Concerning the last, suppose\nn has shifted the price in a direction opposite to the concerted trading actions of\nthe others. If the others believe that n follows (inv0), then they will expect from\nhim or her verifiable information that causes his or her probability to differ from\n\u03c0(H). Since n cannot provide such information, his or her price signal will dis-\nagree with (inv0) and will be regarded as unverifiable manipulative information,\n10\nALEXEY V. OSIPOV AND NIKOLAY N. OSIPOV\nwhich, by analogy with (shr2), will not affect the beliefs of the others. In general,\nit is hardly possible for a single expert with limited resources to manipulate the\nmarket via transactions, provided the number of participants is large (in the next\nsection, we will discuss collusion and manipulation in more detail). We can con-\nclude that rule (r3) will cause the expert n to believe that the probability of the\noutcome coincides with his or her probability of H and, as a consequence, to follow\ninvitation (inv0).\nAs long as the experts from e1 silently trade as described above, their beliefs\nderived from (blf1) as well as their common belief that everyone is following (inv0)\nare supported by their observations of each other\u2019s behavior: a rational expectation\nequilibrium occurs.\nNext, consider a structured scenario where a large number of ignorant experts\nstabilize the price at the level of \u03c0(H) before any actions of informed experts\noutside e1. Consider an expert m \u2208E outside e1 who has private information that\naffects the prior probability: \u03c0(H | Im) \u0338= \u03c0(H). As before, we assume that m\nbelieves the others follow (inv0) commonly believing in the same, and that he or\nshe does not speculate on the existence of other informed experts before they enter\nthe market. His or her belief that the current market participants are ignorant\nand follow (inv0) is supported by their behavior. Thus, again, m believes that the\ncurrent stable price \u03c0(H) is consensus for other experts, and that it is impossible,\ndue to rules (r1) and (r2), to distort it with his or her trading actions. Therefore,\ndue to (shr1)\u2013(shr3), there exists the only course of action that m believes to be\nprofitable, provided he or she is risk neutral: to make a transaction at the current\nprice \u03c0(H) and, after that, to share and prove the information Im, making it\ncommon knowledge. Indeed, by doing so, m would recursively switch the system\nto the state with\n\u2022 e2\ndef= e1 \u222a{m} instead of e1,\n\u2022 Im instead of \u2126,\n\u2022 \u03c0( \u00b7 | Im) instead of \u03c0, and\n\u2022 In \u2229Im instead of each In.\nAssumption (blf1) will be also extended to this new state in the context of the\nupdated public information Im and the updated set e2 of ignorant experts: since it\nwill be commonly believed that m has followed (inv0), it will be commonly believed\nthat m has shared all of his or her information and has become ignorant. We also\nnote that (blf1) will encompass beliefs of m about ignorant experts.\nOn the other hand, if m enters the market without sharing information, the\nexperts from e1 will also extend their beliefs onto him or her: they will consider m\nignorant, which will be consistent with his or her silence and their belief that he or\nshe is following (inv0). The expert m does not believe that under such conditions\nhe or she will be able to change the price and make a profit (for the same reasons\nthat give rise to the initial equilibrium among ignorant experts).\nThe above considerations imply that m believes that the probability of the final\noutcome can be switched to the value \u03c0(H | Im), which differs from the current\nprice \u03c0(H). Thus, m believes that he or she can make a profit and that there is\nthe only way to do it.\nIt is not hard to see that even if m admits that another expert may, with small\nprobability, possess the same information Im (see (blf2)), it is still profitable for m\nto act as described above. In particular, m believes that after he or she shares the"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper\n\n**Title:** Collective Intelligence in Science: Direct Elicitation of Diverse Information from Experts with Unknown Information Structure\n\n#### Main Contributions:\n1. **Proposed Mechanism:** The authors propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. This system allows experts to directly share their private information about a complex scientific hypothesis through the chat, trading as if the market were resolved in accordance with the truth of the hypothesis.\n\n2. **Efficient Information Aggregation:** The approach leads to efficient aggregation of relevant information in an interpretable form even if the ground truth cannot be established and experts initially know nothing about each other or cannot perform complex Bayesian calculations.\n\n3. **Innovative Funding Mechanism:** By rewarding winners with real assets proportionally to their play money, this system incentivizes large-scale collaborative research on any type of problem.\n\n4. **Universal Mechanisms Limitations:** The authors highlight the limitations of existing universal mechanisms like the Bayesian truth serum and conventional prediction markets in settings where experts cannot directly verify each other's information or perform complex Bayesian calculations.\n\n5. **Specialized Tool for Complex Hypotheses:** They introduce a specialized tool (self-resolving play-money prediction market) designed specifically to address situations where experts need to pool diverse, unpredictable private information about an open scientific problem without the ability to establish a common prior or perform sophisticated Bayesian analysis.\n\n#### Methodology:\n1. **Play-Money Prediction Market:** Participants start with equal amounts of play money and winners are determined by a binary random generator based on the market final price.\n2. **Chat for Information Sharing:** A public chat is provided for experts to share their private information about the hypothesis in an interpretable and verifiable form.\n3. **Incentives:** Rewards are given proportionally to the play money won, incentivizing experts to truthfully report their private signals.\n\n#### Key Findings:\n1. **Direct Information Sharing:** The proposed mechanism enables direct sharing of private information among experts without requiring complex Bayesian calculations or a common prior assumption.\n2. **Efficiency in Information Aggregation:** Experts can efficiently aggregate diverse and unpredictable private information about the hypothesis, leading to an interpretable consensus probability.\n3. **Robustness:** The approach is robust even if experts do not know each other's information structures initially and cannot perform complex Bayesian calculations.\n\n#### Conclusions:\n1. **Innovative Approach for Open Scientific Problems:** The proposed mechanism provides a novel way to aggregate private information about open scientific problems, making it possible to achieve consensus without requiring common priors or sophisticated Bayesian analysis.\n2. **Potential Applications:** This approach can be applied to various large-scale collaborative research projects across different fields of science and beyond.\n3. **Limitations and Future Work:** The authors acknowledge the need for further empirical testing and potential extensions to address more complex scenarios, such as relaxing assumptions about expert information structures or exploring alternative incentive mechanisms.\n\nThis paper presents a novel solution to the challenge of aggregating private scientific information from experts with unknown information structures, providing a practical framework that can be applied in various collaborative research settings."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Correcting and Quantifying Systematic Errors in 3D Box Annotations for\nAutonomous Driving\nAlexandre Justo Miro\n1,2, Ludvig af Klinteberg\n2, Bogdan Timus1, Aron Asefaw3,\nAjinkya Khoche1,3, Thomas Gustafsson1, Sina Sharif Mansouri1, Masoud Daneshtalab2\n1 Traton Group R&D, 2 M\u00a8alardalen University, 3 KTH Royal Institute of Technology\n1 {alexandre.justo.miro, bogdan.timus, ajinkya.khoche, thomas.gustafsson,\nsina.sharif.mansouri}@se.traton.com, 2 {ludvig.af.klinteberg, masoud.daneshtalab}@mdu.se,\n3 asefaw@kth.se\nAbstract\nAccurate ground truth annotations are critical to supervised\nlearning and evaluating the performance of autonomous ve-\nhicle systems. These vehicles are typically equipped with\nactive sensors, such as LiDAR, which scan the environ-\nment in predefined patterns. 3D box annotation based on\ndata from such sensors is challenging in dynamic scenarios,\nwhere objects are observed at different timestamps, hence\ndifferent positions. Without proper handling of this phe-\nnomenon, systematic errors are prone to being introduced\nin the box annotations. Our work is the first to discover\nsuch annotation errors in widely used, publicly available\ndatasets. Through our novel offline estimation method, we\ncorrect the annotations so that they follow physically fea-\nsible trajectories and achieve spatial and temporal consis-\ntency with the sensor data. For the first time, we define\nmetrics for this problem; and we evaluate our method on\nthe Argoverse 2, MAN TruckScenes, and our proprietary\ndatasets. Our approach increases the quality of box anno-\ntations by more than 17% in these datasets. Furthermore,\nwe quantify the annotation errors in them and find that the\noriginal annotations are misplaced by up to 2.5 m, with\nhighly dynamic objects being the most affected. Finally, we\ntest the impact of the errors in benchmarking and find that\nthe impact is larger than the improvements that state-of-\nthe-art methods typically achieve w.r.t. the previous state-\nof-the-art methods; showing that accurate annotations\nare essential for correct interpretation of performance.\nOur code is available at https://github.com/alexandre-justo-\nmiro/annotation-correction-3D-boxes.\n1. Introduction\nAccurate ground truth annotations are a cornerstone in the\ndevelopment, evaluation, and testing of autonomous driving\nsystems. Modern perception pipelines are often powered by\ndeep neural networks trained in a supervised manner and\nso rely on high-quality labeled data. Annotations are esti-\nmations of ground truth, and can be obtained in different\nways, e.g. manually by humans, via an automatic labeling\nmethod, or a combination. The error of these estimations in\nautonomous driving datasets is widely assumed to be orders\nof magnitude below the estimation error of the downstream\ntasks that are benchmarked on the annotations. However,\nthis assumption is seldom revisited, and the accuracy of an-\nnotations and their resemblance to the ground truth have\nbeen highly overlooked.\nFor object detection, typical datasets [1\u20136] provide\nbounding box positions at a specific reference timestamp\nwithin a 100 ms interval, i.e. annotation sample, which is\na typical scanning interval for a rotating Light Detection\nand Ranging (LiDAR) sensor. Annotation is typically done\nby fitting a 3D bounding box to the LiDAR data associated\nwith the edges of the object. However, a rotating LiDAR\nmay detect an object any time within the mentioned 100 ms\ninterval. Specifically, during 100 ms, objects can move sig-\nnificantly in highly dynamic scenarios, e.g. 3 m for a typical\nhighway speed of 30 m s\u22121. Hence, in general, the ground\ntruth pose of the object differs between the actual time when\nthe LiDAR detected the object and the common reference\ntime at which all boxes are annotated. This discrepancy\nmay lead to large box annotation errors. These are more\nobviously visible in multi-LiDAR datasets (see Fig. 1), but\nare present in single-LiDAR datasets as well [7].\nFor example, Fig. 1a shows a 3D box annotation in the\nArgoverse 2 [4] dataset. There, the object is moving to-\nwards the right of the image, along its x-axis. However,\nthe box is aligned with the LiDAR detections at 30 ms off-\nset w.r.t. the box annotation timestamp, hence significantly\nmore to the right than where the object really was at the\nbox annotation timestamp. In turn, Fig. 1b shows a simi-\narXiv:2601.14038v1  [cs.CV]  20 Jan 2026\nx\ny\nx\ny\nx\ny\nx\nz\nx\nz\nx\nz\n(a) Argoverse 2.\n(b) MAN TruckScenes.\n(c) Our proprietary dataset.\n\u221240\n\u221220\n0\n20\n40\n60\n80\n100\nTime offset (ms)\nFigure 1. Examples of 3D annotation errors in various datasets. The original LiDAR points are colored by capture time offset relative to\nthe reference time at which the boxes were annotated (see color bar at the right of the figure). The translucent, blue points show where\nthese LiDAR detections would have been at the annotation reference time. Our proposed annotations (dashed-line boxes) are aligned with\npoints with zero time offset, while the original annotations (solid-line boxes) are aligned with point clouds with large time offsets. The\nposition offset between them is an indication of the annotation error that we aim to correct.\nlar situation in the MAN TruckScenes dataset. The posi-\ntion of the box corresponds to the location where the ob-\nject was at about \u221235 ms offset, hence significantly more to\nthe left than where the object really was at the annotation\ntimestamp. Finally, Fig. 1c shows an equivalent setting in\nour proprietary dataset. The box is aligned with LiDAR de-\ntections offset \u221220 ms, hence significantly more to the left\nthan where the object was at the annotation timestamp. In\nall these cases, the boxes should have been placed at the\nlocation where hypothetical LiDAR detections offset with\n0 ms w.r.t. the annotation timestamp (the translucent blue\npoints in Fig. 1) would have hit the moving objects, i.e.\nwhere the dashed-line boxes are drawn.\nFrom inspecting these and more annotations in the\ndatasets, we hypothesize that the original annotations were\nmade by arbitrarily choosing one reference set of sensor de-\ntections to fit the boxes to, without taking into consideration\nneither the time at which those points were captured nor the\ndynamics of the objects.\nAn accurate annotation would either place the boxes\nwhere the objects were at the annotation timestamp or\nshould have indicated the exact timestamp when each ob-\nject\u2019s position was annotated. We focus on the former, as\nannotating multiple boxes at a common, pre-defined sample\ntimestamp has been the standard in the autonomous driving\nfield. However, the latter, i.e. annotating each individual\nbox at the exact location and timestamp that matches the\nsensor data, has the potential to be more accurate and lead\nto better performance in downstream tasks, as the detectors\nwould only need to find a box around existing sensor data,\ninstead of implicitly estimating motion and compensating\nthe sensor data to a predefined timestamp.\nIn the case we focus on, the position of the box shall\nbe extrapolated to the sample annotation timestamp using\nthe object\u2019s dynamics. Since the object dynamics are not\nknown apriori, such mechanism requires estimating several\nstates of the object dynamics, including speed, yaw rate, and\nacceleration; by using several frames from the sequence. As\nexemplified above, this estimation may imply a large box\ndisplacement, and in general it may come across as if the\noriginal detections were not fitting the extrapolated box.\nThe novelty of our work is three-fold. First, we intro-\nduce a principled, motion-model-based annotation correc-\ntion framework that accounts for actors\u2019 motion, enabling\nphysically plausible 3D box placement, a capability absent\nin current datasets and prior methods. Second, we define,\nfor the first time, quantitative metrics tailored to assess the\nquality of dynamic object annotations, allowing for system-\natic diagnosis of annotation errors and benchmarking of\ncorrection techniques. Finally, we demonstrate the signif-\nicant sensitivity of commonly accepted benchmarks to the\nannotation errors studied. These contributions not only aim\nto improve annotation fidelity, but also lay the foundation\nfor future studies on the impact of annotation errors on au-\ntonomous driving pipelines.\n2. Related work\nThe problem of distorted 3D box annotations was first high-\nlighted by [8]. The authors pointed out that the annotations\non their proprietary dataset suffered from distortion issues,\nmore so in highly dynamic environments. They provided\na method that improved the longitudinal placement of the\nboxes by estimating their speed. However, this method is\nlimited in that it assumes rectilinear trajectories, it does not\nenforce consistency with sensor data, and quantitative eval-\nuations are absent.\nMore recently, [9] pointed out that the multiple LiDAR\ndistortion is visible also in popular public datasets, and pro-\nvided a scene flow method to compensate LiDAR points for\ntarget motion so that they become coherent with each other.\nHowever, that work focused on the point clouds only; and\ndid not analyze the box annotations, nor did it attempt to\nimprove them. Their metrics, Chamfer Distance Error and\nMean Point Error, are applicable to point matching but not\nto correction of box annotations.\nTherefore, to the best of the authors\u2019 knowledge, there\nhave been no attempts to holistically correct the 3D bound-\ning box annotations from any automotive dataset nor pro-\nposed metrics for this problem.\nMotion models. In the object tracking domain [10] there\nexist multiple motion models that allow estimation of ob-\nject dynamics such as speed, yaw rate, and acceleration\n[11]. Among these, those that can be utilized in a two-\ndimensional curvilinear trajectory are of particular interest\nto the autonomous driving domain [12]; according to which\nthe best-performing motion model for such applications is\nthe Constant Turn Rate and Acceleration (CTRA) model\n[13]. For complex vehicles, such as vehicles with trailers\nor articulated vehicles, dedicated motion models can also\nbe found in the literature [14].\nOptimization methods.\nA suitable way to estimate the\nstates of the box annotations is by means of an optimiza-\ntion formulation.\nDue to the complexity of the annota-\ntion correction problem, which may include continuous and\ndiscrete variables as well as non-differentiable functions,\na number of global metaheuristic optimization algorithms\n[15] shall be considered. All of them can deal with non-\ndifferentiable functions (derivative free) and have mecha-\nnisms to escape local optima.\nThe most commonly used such algorithms are as fol-\nlows. Pattern Search (PS) [16] requires the search bound-\naries to be defined. It starts from an initial guess and itera-\ntively steps towards a better solution, shrinking the step size\nif no better solution is reachable. Genetic Evolution [17]\nrandomly initializes a population of candidate solutions and\niteratively evolves them using mechanisms such as selec-\ntion, crossover, and mutation. Finally, Particle Swarm [18]\nrandomly initializes a population of candidate solutions and\niteratively improves them based on their momentum and the\ncurrently best known solution locally and globally.\nAmong these algorithms, PS [16] is the most suitable one\nwhen a reliable initial guess can be provided, and has been\nshown to slightly outperform the other mentioned gradient-\nfree methods in some applications [19].\n3. Methodology\nThis section describes the methodology used to solve the\nproblem. Section 3.1 formulates the problem to solve. Sec-\ntions 3.2 to 3.4 define the three terms of the tailored ob-\njective function, respectively. Section 3.5 aggregates these\nterms into one single objective function. Finally, Sec. 3.6\ndefines the evaluation metrics for the problem.\n3.1. Problem formulation\nThe goal of this work is to correct the pose of existing box\nannotations, so that they better match the collection of sen-\nsor (typically, LiDAR) detections associated with the ob-\nject over time and compensated for object dynamics, while\nmaintaining a physically feasible trajectory of the boxes.\nAlthough size attributes might also carry errors, they are\nexpected to be orders of magnitude lower and less rele-\nvant.\nFurthermore, their annotation is not always possi-\nble to do based on sensor data; e.g. when only a fraction\nof an object is observed, and thus some dimensions are\nunobservable; some expected standard sizes are typically\nannotated instead. Therefore, we focus on the errors that\nare caused purely by mishandling of temporal sensor data,\nnamely those in the poses, and leave the size attributes as is.\nThe inputs to the problem are three: the original box\nannotations, the sensor 3D point clouds, and the ego ve-\nhicle poses. First, let the annotated bounding boxes for ev-\nery track ID \u03b1 (constant along the whole sequence for each\ngiven object) and annotation sample i,\nB\u03b1,i =\n\u0002x\u03b1,i\ny\u03b1,i\nz\u03b1,i\n\u03b8\u03b1,i\nL\u03b1,i\nW\u03b1,i\nH\u03b1,i\n\u0003T ,\n(1)\nwhere x, y and z refer to the 3D coordinates of the centroid\nof the box, \u03b8 refers to the orientation of the box about the\nz-axis (i.e., yaw angle) and L, W and H refer to length,\nwidth, and height, respectively.\nSecond, let the sensor data, expressed in the global frame\nof reference to prevent distortion due to ego motion,\nPi,k =\n\u0002px\ni,k\npy\ni,k\npz\ni,k\n\u2206ti,k\n\u0003T ,\n(2)\nfor every annotation sample i and detection k, where px, py,\nand pz refer to the 3D coordinates of a point, and \u2206t refers\nto the time difference between the time at which a point was\ncaptured and the annotation reference timestamp.\nFinally, let the ego vehicle\u2019s 2D pose,\nEi =\n\u0002\nEx\ni\nEy\ni\nE\u03b8\ni\n\u0003T ,\n(3)\nfor every annotation sample i.\nFor the sake of simplicity, we only formulate the pose\ncorrection problem in a 2D, bird\u2019s-eye view representation,\nwhich is where most of the annotation error is expected for\nperfectly flat driving surfaces. It would be needed to extend\nthe problem formulation to 3D when handling, for instance,\ndata from hilly terrain.\nThus, the goal is to estimate every box\u2019s 2D pose, speed,\nyaw rate, and acceleration at their corresponding anno-\ntation reference timestamp; while the bounding box\u2019s z-\ncoordinate, as well as the dimensions L, W and H are as-\nsumed to remain constant. More specifically, we seek to\nestimate for every track ID \u03b1 and sample i the state vector\nX\u03b1,i =\n\u0002x\u03b1,i\ny\u03b1,i\n\u03b8\u03b1,i\ns\u03b1,i\n\u03c9\u03b1,i\na\u03b1,i\n\u0003T ,\n(4)\nwhere the variables s and a refer to linear speed and accel-\neration, respectively, along the box\u2019s x-axis; and \u03c9 refers\nto the box\u2019s yaw rate; such that an objective function, L,\nis minimized independently for each track ID \u03b1, along all\nannotation samples i of a sequence:\n\u02c6\nX\u03b1 = arg min\nX\u03b1\nL (B\u03b1, P , E) .\n(5)\nThe objective function at Eq. (5) is defined in Secs. 3.2\nto 3.5.\n3.2. Motion model\nAs the annotated objects correspond to real objects, often\nvehicles, that move; a motion model shall be enforced in the\noptimization formulation. Since these objects may move\nrectilinearly, curvilinearly, or a combination of both, with\nvarying speed profiles; the motion model must account for\nlinear speed, acceleration, and angular speed. To this end, a\nCTRA [13] motion model was selected.\nThe CTRA equations [13] define state transition over a\ntime interval \u2206t = t \u2212t0 as follows:\n\u2206x = 1\n\u03c92\n0\n(st\u03c90 sin (\u03b8t) + a0 cos (\u03b8t)\n\u2212s0\u03c90 sin (\u03b80) \u2212a0 cos (\u03b80) ) ,\n(6a)\n\u2206y = 1\n\u03c92\n0\n(\u2212st\u03c90 cos (\u03b8t) + a0 sin (\u03b8t)\n+ s0\u03c90 cos (\u03b80) \u2212a0 sin (\u03b80) ) ,\n(6b)\n\u2206\u03b8 =\u03c90 \u00b7 \u2206t,\n(6c)\n\u2206s =a0 \u00b7 \u2206t,\n(6d)\n\u2206\u03c9 =0,\n(6e)\n\u2206a =0.\n(6f)\nFor \u03c90 close or equal to zero, a numerically stable approx-\nimation of Eqs. (6a) and (6b) can be obtained as described\nin [13].\nThe CTRA equations can be rewritten in vector form:\n\u2206Y t\n0 =\n\u0002\u2206x\n\u2206y\n\u2206\u03b8\n\u2206s\n\u2206\u03c9\n\u2206a\u0003T .\n(7)\nThe CTRA constraint aims to minimize the difference be-\ntween the predicted states and the optimization variable\nstates. More specifically, from each state node i, a CTRA\nprediction is made to the time of the subsequent node i + 1,\nY i+1\ni\n= Xi + \u2206Y i+1\ni\n.\n(8)\nThis prediction is compared to the optimization variable at\nnode i + 1 using a weighted squared error,\n\u03f5m\ni+1 =\n\u0000Xi+1 \u2212Y i+1\ni\n\u0001T \u03b2m\n\u0000Xi+1 \u2212Y i+1\ni\n\u0001\n,\n(9)\nwhere \u03b2m is the inverse covariance matrix for the CTRA\nmotion model, specified in Sec. 4.1.\nThis term adds a\npenalty given by how much the movement differs from the\nideal CTRA movement. Note that this formulation requires\nthat tracks consist of at least two boxes along a sequence.\n3.3. Sensor detections\nAdding constraints on the boxes being consistent with the\nsensor detections is key to ensuring that the annotation er-\nrors are corrected. The following describes the components\nneeded for it.\nPoint-to-box association. Because, in general, datasets do\nnot provide labels for each sensor detection, the ones that\nfall within certain limits of each box are considered suscep-\ntible to belonging to the object. Thus, each original box\nis inflated so as to include all the detections that can rea-\nsonably belong to the object, but no detections from other\nobjects. This depends on the typically expected distance be-\ntween objects, which might be larger at high vehicle speeds\nthan in a slow urban traffic.\nThus, we inflate each box on both ends along its x-\naxis (rear and front) proportionally to its speed, \u2206Lrear\n\u03b1,i =\n\u2206Lfront\n\u03b1,i = 1 m + \u2206Tsensor \u00b7 s\u03b1,i, where \u2206Tsensor is the scan-\nning period of the sensor. Moreover, the width is inflated a\nconstant amount on each side (left and right), \u2206W left =\n\u2206W right = 0.5 m, and the height is deflated a constant\namount at the bottom to prevent interference from ground\ndetections in the optimization, \u2206Hbottom = \u22120.2 m, while\nthe height remains untouched at the top. This results in a set\nof sensor detections that are associated with each box, P \u03b1\ni,k.\nTarget motion compensation. Since, in general, annotated\ntargets move over the course of a sample, causing point\ncloud distortion, these point clouds need to be compensated\nfor target motion.\nThe points that have been associated with a box are com-\npensated for target motion using the estimated dynamics of\nthe box (Eq. (4)) at each time step, using the CTRA formu-\nlas from Eqs. (6a) and (6b) while setting \u2206t to each point\u2019s\ntime offset w.r.t. the annotation timestamp. This is,\n\u02dcpx\ni,k =px\ni,k +\n1\n\u03c92\n\u03b1,i\n(st\u03c9\u03b1,i sin (\u03b8t) + a\u03b1,i cos (\u03b8t)\n\u2212s\u03b1,i\u03c9\u03b1,i sin (\u03b8\u03b1,i) \u2212a\u03b1,i cos (\u03b8\u03b1,i) ) ,\n(10a)\n\u02dcpy\ni,k =py\ni,k +\n1\n\u03c92\n\u03b1,i\n(\u2212st\u03c9\u03b1,i cos (\u03b8t) + a\u03b1,i sin (\u03b8t)\n+ s\u03b1,i\u03c9\u03b1,i cos (\u03b8\u03b1,i) \u2212a\u03b1,i sin (\u03b8\u03b1,i) ) ;\n(10b)\nwhere\n\u03b8t =\u03b8\u03b1,i + \u03c9\u03b1,i \u00b7 \u2206ti,k,\n(11a)\nst =s\u03b1,i + a\u03b1,i \u00b7 \u2206ti,k.\n(11b)\nA similar set of equations can be derived for the approxima-\ntion for low yaw rates previously mentioned in this section.\nThis results in a motion compensated point cloud at the\nannotation reference timestamp,\n\u02dc\nP \u03b1\ni,k =\n\u0002\u02dcpx\ni,k\n\u02dcpy\ni,k\npz\ni,k\n\u2206ti,k\n\u0003T ,\n(12)\nfor every sample i and point within the sample k.\nInlier ratio. The optimal box annotation will be such that\nthe ratio of associated, motion compensated points within\nitself is maximal. To this end, we define the cost\n\u03f5inlier\n\u03b1,i = 1 \u2212\n\u02dc\nP \u03b1\ni,k \u2208X\u03b1,i\n\u02dc\nP \u03b1\ni,k\n.\n(13)\nFitness. Maximizing the inlier ratio alone is not enough\nto guarantee optimal box placement. In addition, the boxes\nmust fit the points as tightly as possible. Thus, we define a\ncost term that rewards the points being as close as possible\nto any face of the box.\nLet the two orthogonal vectors that define the box in the\n2D, bird\u2019s-eye view space, cx and cy. Project each point\n\u02dc\nP \u03b1\ni,k to both orthogonal vectors. By basic algebra, when\nthese projections are within the interval [0, 1], the point lies\nwithin the box. More importantly, the closer any of these\nprojections is to 0 or 1, the closer the point is to a face of\nthe box. Because it is not known beforehand which face of\nthe box the points should fit, only the best-fitting face of the\nfour will be evaluated. Thus, the cost term is such that the\naverage of fitness between the associated points and the box\nis maximized,\n\u03f5fit\n\u03b1,i =\n1\n\u02dc\nP \u03b1\ni,k\nX\n\u02dc\nP \u03b1\ni,k\n\u0010\n2 \u00b7 min\n\u0010\n\u02dc\nP \u03b1\ni,k \u00b7 cx, \u02dc\nP \u03b1\ni,k \u00b7 cy,\n1 \u2212\u02dc\nP \u03b1\ni,k \u00b7 cx, 1 \u2212\u02dc\nP \u03b1\ni,k \u00b7 cy\n\u0011\u00112\n.\n(14)\nA set of points that lie on either face of the box will result\nin \u03f5fit\n\u03b1,i = 0; whereas a set of points that lies in the center\nof the box will result in \u03f5fit\n\u03b1,i = 1, which is the local max-\nimum within the box; and a point that lies outside of the\nbox will result in \u03f5fit\n\u03b1,i > 0 and will quadratically increase\nboundlessly with distance away from the box edges.\nOverall sensor detections loss. Overall, the sensor detec-\ntions cost becomes the sum of Eqs. (13) and (14), weighted\nby the constant \u03b2s, defined in Sec. 4.1. This reads\n\u03f5s\n\u03b1,i = \u03b2s\n\u0000\u03f5inlier\n\u03b1,i + \u03f5fit\n\u03b1,i\n\u0001\n.\n(15)\n3.4. Distance to ego\nTo make the box placement more accurate, we add a term\nthat penalizes the box from being close to the ego vehicle.\nThe rationale is that the sensors, which are mounted on the\nego vehicle, will detect the closest faces of the objects, and\nnot the farthest ones. Thus, this term prevents the previous\nterms from fitting a wrong face of the boxes to the points,\nespecially in cases in which only one face of the box was\nsensed. The term reads\n\u03f5d\n\u03b1,i = \u2212\u03b2d \u00b7 \u2225\n\u0002x\u03b1,i\ny\u03b1,i\n\u0003\n\u2212\n\u0002Ex\ni\nEy\ni\n\u0003\n\u2225,\n(16)\nwhere \u03b2d is a constant weight that should make this term\nless influential than the other terms in the optimization, and\nwill be defined in Sec. 4.1.\n3.5. Objective function\nThe overall objective function for a track \u03b1 becomes the\naggregation of Eqs. (9), (15) and (16) along its trajectory,\n\u02c6\nX\u03b1 = arg min\nX\u03b1\nX\nB\u03b1,i\\B\u03b1,1\n\u03f5m\n\u03b1,i +\nX\nB\u03b1,i\n\u0000\u03f5s\n\u03b1,i + \u03f5d\n\u03b1,i\n\u0001\n.\n(17)\nThis objective function can be applied independently to dif-\nferent trajectories, even from different sequences.\nThe complete optimization problem is illustrated in\nFig. 2 for a track composed of N annotated boxes along\nits trajectory.\n3.6. Metrics\nSince no metrics have been defined for the annotation cor-\nrection problem until the time of writing (see Sec. 2), we\npropose the following as a basis for a quantitative evalu-\nation of both the performance of our method and 3D box\nannotation quality.\nInlier points difference. The Inlier Points Difference (IPD)\nproposed metric calculates the relative difference in the ab-\nsolute number of sensor detections compensated for motion\nthat lie within the original and corrected boxes,\nIPD =\n\u02dc\nP \u2208\u02c6\nX \u2212\u02dc\nP \u2208B\n\u02dc\nP \u2208B\n.\n(18)\nX1\nX2\nXN\nY 2\n1\nY N\nN\u22121\n+\u2206Y 2\n1\n\u03f5m\n2\n\u03f5m\nN\nP1\nP2\nPN\n\u03f5s\n1\n\u03f5s\n2\n\u03f5s\nN\nE1\nE2\nEN\n\u03f5d\n1\n\u03f5d\n2\n\u03f5d\nN\nFigure 2. A diagram that illustrates the full optimization problem.\nThe states to estimate, Xi, are within circles, whereas the vari-\nables that are used to compute the error are within squares. The\nCTRA motion model, Y i+1\ni\n, aims to enforce a physically plausi-\nble trajectory by minimizing the difference between the predicted\nstates from the ith to the (i + 1)th time step and the optimization\nvariable at the (i + 1)th time step. In turn, the point cloud, Pi, and\ndistance to ego, Ei, error terms are applied at every time step and\naim to place the corrected boxes coherently with the sensor data.\nNaturally, a functioning annotation correction method is ex-\npected to produce a positive IPD, which means that the\ncorrected boxes capture more compensated points than the\noriginal boxes and thus are more appropriately positioned.\nError distribution and correlation. Once the corrected\nboxes are estimated, the difference between them and the\noriginal boxes can be calculated and regarded as a measure\nof the error of the original boxes. Our proposed metrics\ncompute the Euclidean Distance Error (EDE) and its proba-\nbility distribution; as well as the Decoupled Euclidean Dis-\ntance Error (DEDE) along the box\u2019s x and y axes, to deter-\nmine if the error of the original boxes is correlated with their\ntraveling direction, which would reveal systematic errors\ndue to incorrect handling of object dynamics. We propose\na quantification of the latter, namely Spread of Decoupled\nEuclidean Distance Error (SDEDE), calculated as 3 times\nthe standard deviation [20] of the respective distributions.\nAdditionally, we propose computing the EDE against a set\nof speed and distance to ego intervals to discover eventual\ncorrelations in the datasets between error, on the one hand,\nand speed and distance, respectively, on the other hand.\n4. Experiments\nOur experiments are carried out on 10 sequences of each\nof Argoverse 2, MAN TruckScenes, and our proprietary\ndatasets. The 10 chosen sequences of Argoverse 2 are the\nones with the highest average object speed, calculated by\nnaively dividing box distances by time differences, and can\nbe found in our code; while the 10 sequences of MAN\nTruckScenes correspond to its mini dataset version.\nWe\nlimit the number of sequences because of the intractable\namount of computation time that would be needed to correct\ncomplete datasets with our method, and because the main\ngoal of our work is to raise and quantify an underexplored\nproblem rather than solving it completely or efficiently.\nThe reason why we choose these datasets is that they pro-\nvide a sufficient amount of information, including explicit\ntime offset per individual LiDAR detection and 3D box an-\nnotations along multiple frames within the sequences. Most\nother datasets mentioned in Sec. 1 do not provide this re-\nquired information and thus make the systematic annotation\nerrors that we tackle very difficult to even diagnose at all.\nAlso note that all the datasets to which our method is\napplied in this work happen to be multi-LiDAR. However,\nour method is general enough to handle single or multiple\nLiDAR and any other active sensor datasets.\n4.1. Implementation\nAs discussed in Sec. 2, the PS [16] algorithm is the most\nreasonable choice for our experiments, since our problem\nsetting allows us to provide a reasonably good initial guess\nfrom the original annotations. This algorithm is gradient-\nfree and therefore we use it to accommodate the objective\nfunction of Eq. (17). Any other gradient-free optimization\nalgorithms (see Sec. 2) can accommodate the same objec-\ntive function and are compatible for solving this problem as\nwell. Our method was developed in Python relying on the\nPS implementation from the pymoo [21] library.\nThe states x, y, and \u03b8 of the original box annotations\nwere used as the initial value for the corresponding opti-\nmization variables, while the initial value for speed was es-\ntablished by naively dividing the distance by the time differ-\nence in the original box annotations, and the initial values\nfor yaw rate and acceleration were set at zero.\nAdditionally, the search space was bounded from the\ninitial values with \u00b15 m for x and y; \u00b1\u03c0/16 rad for \u03b8;\n\u00b140 m s\u22121 for speed; \u00b1\u03c0/8 rad s\u22121 for yaw rate; and\n\u00b120 m s\u22122 for acceleration.\nFinally, the optimization weights were set as follows:\n\u03b2m = 10\u00b7fa\u00b7I6 with fa being the box annotation frequency\n(10 Hz for Argoverse 2 and our proprietary dataset and 2 Hz\nfor MAN TruckScenes), \u03b2s = 103, and \u03b2d = 10\u22124 m\u22121; so\nthe ratio and placement of points within the boxes are the\nmost influential, followed by the CTRA motion model, fol-\nlowed by the distance of boxes to ego.\n4.2. Qualitative results\nIn the first experiment, we test our method in the samples\nthat correspond to Fig. 1. The resulting corrected boxes\ncan be seen in Fig. 1 as dashed-line boxes along with the\npoints compensated for the track\u2019s motion as translucent\nblue points. The corrected boxes are much closer to their\nideal position than the original annotated box, as they fit the\nauxiliary motion-compensated sensor detections tightly.\n4.3. Quantitative results\nTable 1 shows results for the mentioned datasets and se-\nquences. All cases show a clear improvement in the IPD.\nDataset\nAverage\nSpeed\nIPD\nArgoverse 2\n11.12 m/s\n+17.53 %\nMAN TruckScenes\n13.58 m/s\n+30.45 %\nProprietary\n15.61 m/s\n+27.99 %\nTable 1. Average Speed and IPD on 10 sequences of each of 3\ndatasets. Only boxes whose speed is at least 3 m/s are included.\nFigure 3 shows the distribution of EDE (Fig. 3a) and\nDEDE along the x and y axes of the boxes (Figs. 3b to 3d,\nrespectively, for each dataset), taken as the position of the\noriginal boxes minus the position of the corrected boxes.\nMoreover, Tab. 2 shows the SDEDE along the x and y axes\nof the boxes. From Fig. 3a, it can be seen that the boxes in\nthe original datasets are incorrectly placed by up to 2.5 m.\nIn addition, according to Figs. 3b to 3d and Tab. 2, the\nstrongest component of the errors is observed along the x\naxes of the boxes, which define their direction of travel.\nThis confirms the hypothesis that most of the error is intro-\nduced due to incomplete handling of sensor data in dynamic\nobjects during the annotation process.\nDataset\nSDEDE-x\nSDEDE-y\nArgoverse 2\n1.36 m\n0.55 m\nMAN TruckScenes\n2.54 m\n1.19 m\nProprietary\n2.17 m\n0.69 m\nTable 2. SDEDE on 10 sequences of each of 3 datasets. Only\nboxes whose speed is at least 3 m/s are included.\nFurthermore, Figs. 3b to 3d show that the longitudinal\nerror is skewed towards different directions for different\ndatasets. For Argoverse 2, it is skewed towards positive er-\nrors; whereas for MAN TruckScenes and our proprietary\ndataset, it is skewed towards negative errors. This is ex-\nplained by the fact that the annotation samples in the Argo-\nverse 2 dataset include sensor detections with time offsets\nbetween 0 ms and 100 ms; whereas the MAN TruckScenes\nand our proprietary dataset\u2019s are between \u221250 ms and\n50 ms, but scanning the road section where most traffic par-\nticipants are (left of ego vehicle) at negative time offsets and\noutside of the road (right of ego vehicle) at positive time off-\nsets. Whenever a box was fitted to a set of LiDAR points\nwith positive time offsets, it means that it is ahead of where\nit should have been (as in Fig. 1a), resulting in a positive\noriginal minus corrected longitudinal distance error, which\ncan be seen in Fig. 3b. In contrast, when a box was fitted to a\nset of LiDAR points with negative time offsets, it means that\nit is behind where it should have been (as in Figs. 1b and 1c),\nresulting in a negative original minus corrected longitudinal\ndistance error, which can be seen in Figs. 3c and 3d.\nFinally, Fig. 4 shows the relationship between the EDE\nand both the speed of the annotated tracks and the distance\nbetween the tracks and the ego vehicle, for the 3 studied\ndatasets. Specifically, in Fig. 4a it can be observed that\nthe annotation error tends to increase with speed.\nThis,\nagain, confirms the hypothesis that the error is systematic\nand caused by not considering object dynamics during the\nannotation process, as it is amplified the more dynamic the\nobjects are. Additionally, Fig. 4b shows that the annotation\nerror is not correlated with the object distance to ego. More\nnotably, near objects, which are typically more critical to an\nautonomous driving task than farther objects, are as suscep-\ntible to annotation errors as any other object in the scene.\n4.4. Impact on downstream tasks and benchmarks\nWhile it is beyond the scope of this paper to evaluate or\ncorrect the annotation error in complete datasets, we design\nan experiment to get an indication of the impact that the\nerrors may have on downstream tasks and benchmarks.\nBased on the limited sequences studied, the annotation\nerror could be on the order of magnitude indicated in Tab. 2.\nIn our experiment, we take as an example the Argoverse\n2 dataset and the object detection network VoxelNeXt [22]\nwith its public checkpoint weights. We assume that the orig-\ninal annotations are error-free, then check if and how much\nthe object detection benchmark metrics [23] change when\nthe validation annotations are modified by injecting random\nerrors according to the distribution of Tab. 2.\nMethod\nVal. data\nAP\nATE\nCDS\nVoxelNeXt [22]\nOriginal\n30.5\n0.461 m\n23.0\nFSD [24]\nOriginal\n28.2\n-\n22.7\nVoxelNeXt [22]\nModified\n26.0\n0.650 m\n18.7\nTable 3. The impact that modifying the Argoverse 2 validation\nsplit according to the error distribution of Tab. 2 has on metrics\nand benchmarks is larger than the amount by which VoxelNeXt\nbecame state-of-the-art ahead of FSD.\nThe results can be seen in Tab. 3: the Argoverse 2 metrics\nAverage Precision (AP), Average Translation Error (ATE),\nand Composite Detection Score (CDS) are degraded by\nan amount greater than the margin by which this network\nbecame state-of-the-art on the Argoverse 2 object detec-\ntion benchmark. Likewise, one could arguably expect the\nperformance in real scenarios to be degraded by similar\namounts due to the existing errors in the training splits of\ndatasets with which networks are trained.\n0\n0.5\n1\n1.5\n2\n2.5\n0\n0.5\n1\n1.5\n2\nEDE (m)\nProbability Density\nArgoverse2\nMAN TruckScenes\nProprietary\n(a) Distribution of EDE on the 3 studied datasets.\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\nDEDE (m)\nProbability Density\nLongitudinal\nLateral\n(b) Distributions of DEDE on the Argoverse 2 dataset.\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\nDEDE (m)\nProbability Density\nLongitudinal\nLateral\n(c) Distributions of DEDE on the MAN TruckScenes dataset.\n\u22122\n\u22121\n0\n1\n2\n0\n2\n4\n6\nDEDE (m)\nProbability Density\nLongitudinal\nLateral\n(d) Distributions of DEDE on our proprietary dataset.\nFigure 3. Distributions of error of the original box annotations w.r.t. the corrected boxes in the selected sequences of the 3 examined\ndatasets. Only boxes whose speed is at least 3 m/s are included in the plots.\n[3, 10) [10, 15) [15, 20) [20, 25) [25, \u221e)\n0\n1\n2\n3\nSpeed Interval (m/s)\nEDE (m)\n(a) EDE grouped by speed intervals.\n[0, 50)\n[50, 100) [100, 150) [150, \u221e)\n0\n1\n2\n3\nDistance to Ego Interval (m)\nEDE (m)\n(b) EDE grouped by distance to ego intervals.\nFigure 4. Box annotation error grouped by intervals. Within each interval, from left to right, the boxes represent the Argoverse 2 (blue),\nMAN TruckScenes (orange), and our proprietary (green) datasets, respectively. Within each box, from bottom to top, are represented: 5%\npercentile, 25% percentile, median, 75% percentile, and 95% percentile. Only objects whose speed is at least 3 m/s are included.\n5. Conclusion and future work\nWe have found that 3D box annotations from state-of-the-\nart automotive datasets carry an error of up to 2.5 m, caused\nby incomplete handling of temporal sensor scanning in dy-\nnamic scenes. We have proposed an optimization method\nto correct them so that they better reflect the actual ground\ntruth at the annotation timestamps provided in the datasets.\nIts applicability extends to any dataset consisting of times-\ntamped 3D sensor detections and box annotations.\nThe\nresults show that such systematic annotation errors occur\nmainly along the traveling direction of the annotated ob-\njects and that it scales with the objects\u2019 speed. These errors\nappear to significantly affect benchmark metrics: their im-\npact goes beyond the typical performance difference among\ndifferent state-of-the-art methods in the benchmarks.\nHowever, a systematic evaluation of the impact on\nreal-world performance would require correcting complete\ndatasets and retraining and testing models on them, which\nis left for future work."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary\n\n**Main Contributions**\nThis paper addresses significant errors in 3D box annotations used to train autonomous vehicle systems. The authors identify that these annotations often suffer from systematic inaccuracies due to improper handling of temporal sensor data and object dynamics. Their main contributions include:\n1. **Proposed Correction Framework**: They introduce a principled, motion-model-based annotation correction framework that accounts for actors' motion, enabling physically plausible 3D box placement.\n2. **Quantitative Metrics Definition**: The authors define new quantitative metrics tailored to assess the quality of dynamic object annotations, allowing systematic diagnosis and benchmarking of correction techniques.\n3. **Experimental Validation**: They evaluate their method on multiple datasets (Argoverse 2, MAN TruckScenes, and proprietary datasets) and demonstrate that it increases box annotation accuracy by more than 17% in these datasets.\n\n### Methodology\n\nThe paper's methodology is structured into three main sections:\n1. **Problem Formulation**: The authors formulate the problem of correcting existing 3D box annotations to better match sensor detections while maintaining physically feasible trajectories.\n2. **Motion Model and Sensor Detections**: They define motion models, including a Constant Turn Rate and Acceleration (CTRA) model for object dynamics estimation. They also describe how to compensate for target motion in sensor data using the CTRA equations.\n3. **Objective Function**: The overall objective function is defined as an aggregation of terms related to inlier points difference, sensor detection consistency loss, and distance to ego vehicle.\n\n### Key Findings\n\nThe key findings from their experiments are:\n1. **Improved Annotation Accuracy**: Their method significantly improves annotation accuracy by more than 17% across multiple datasets.\n2. **Error Distribution Analysis**: They analyze the error distribution of original annotations and find that errors are primarily along the traveling direction of objects, scaling with object speed. This confirms their hypothesis about systematic errors introduced during the annotation process.\n3. **Impact on Benchmarks**: The authors demonstrate that these annotation errors have a significant impact on benchmark metrics, affecting performance beyond typical differences among state-of-the-art methods.\n\n### Conclusions\n\nThe paper concludes by highlighting the importance of accurate annotations in autonomous driving systems and emphasizes the need for future work to evaluate the real-world impact of their proposed correction techniques. They also note that their method is generalizable to any dataset with timestamped 3D sensor detections and box annotations, making it applicable beyond the specific datasets they tested.\n\n### Future Work\nFuture research should focus on correcting complete datasets and retraining/testing models to assess the overall impact of annotation errors in real-world scenarios."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Orthogonium : A Unified, Efficient\nLibrary of Orthogonal and 1-Lipschitz Building Blocks\nThibaut Boissin 1 2 3 Franck Mamalet 1 2 Valentin Lafargue 1 2 4 Mathieu Serrurier 3 2\nAbstract\nOrthogonal and 1-Lipschitz neural network lay-\ners are essential building blocks in robust deep\nlearning architectures, crucial for certified ad-\nversarial robustness, stable generative models,\nand reliable recurrent networks.\nDespite sig-\nnificant advancements, existing implementations\nremain fragmented, limited, and computation-\nally demanding.\nTo address these issues, we\nintroduce Orthogonium , a unified, efficient,\nand comprehensive PyTorch library providing\northogonal and 1-Lipschitz layers.\nOrthogo-\nnium provides access to standard convolution\nfeatures\u2014including support for strides, dilation,\ngrouping, and transposed-while maintaining strict\nmathematical guarantees. Its optimized imple-\nmentations reduce overhead on large scale bench-\nmarks such as ImageNet. Moreover, rigorous\ntesting within the library has uncovered critical\nerrors in existing implementations, emphasizing\nthe importance of standardized and reliable tools.\nOrthogonium thus significantly lowers adoption\nbarriers, enabling scalable experimentation and in-\ntegration across diverse applications requiring or-\nthogonality and robust Lipschitz constraints. Or-\nthogonium is available here.\n1. Introduction\n1-Lipschitz neural networks constrain transformations to\npreserve input norms, providing tight, certifiable robust-\nness against adversarial attacks (Szegedy et al., 2014; Anil\net al., 2019). Orthogonal layers reinforce the 1-Lipschitz\nconstraint by requiring an exact unity constant in almost\n1Institut de Recherche Technologique Saint-Exup\u00b4ery, Toulouse,\nFrance 2Artificial and Natural Intelligence Toulouse Institute,\nFrance 3IRIT, Toulouse, France 4now at IMT, Toulouse, and IN-\nRIA, Bordeaux, France. Correspondence to: Thibaut Boissin\n<thibaut.boissin@irt-saintexupery.com>.\nProceedings of the ICML 2025 Workshop on Championing Open-\nsource Development in Machine Learning (CODEML \u201925). Copy-\nright 2025 by the author(s).\nFigure 1: Orthogonium offers a standardized API to use,\nand create 1-Lipschitz layers, allowing a user to construct,\ntest, and improve easily such a kind of network\nall directions, providing tighter global certification guaran-\ntees. Besides robustness, these layers benefit normalizing\nflows (Kingma & Dhariwal, 2018; Behrmann et al., 2019),\nWasserstein GANs (Arjovsky et al., 2017; Gulrajani et al.,\n2017), stable recurrent architectures (Kiani et al., 2022; Qi\net al., 2020; Bansal et al., 2018), and physics-informed mod-\nels.\nMotivations.\nOver\nthe\nlast\ndecade,\nthe\ncertifiable-robustness community has produced an impres-\nsive toolbox of 1-Lipschitz building blocks\u2014orthogonal\nand norm-controlled layers, specialized activations, residual\nschemes, and normalization layers. Unfortunately, these\ningredients remain scattered across dozens of papers\nand repositories. For practitioners who simply \u201cneed a\n1-Lipschitz backbone\u201d\u2014be they working on Wasserstein\nGANs, stable RNNs, privacy-preserving analytics, or\nsafety-critical perception\u2014the landscape is opaque: Which\nmethods are still maintained? Which supports modern CNN\nstaples such as stride, dilation, grouping, or transposed\nconvolutions? Which combinations scale to ImageNet?\nConstructing a truly 1-Lipschitz network exacerbates the\nproblem. Every layer in every branch must respect the\n1\narXiv:2601.13776v1  [cs.LG]  20 Jan 2026\nOrthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks\nglobal constraint, yet many recent proposals cover only the\nvanilla 3 \u00d7 3 convolution and ignore grouped, dilated, or\nstrided variants that dominate contemporary architectures\n(Liu et al., 2022; Tan & Le, 2019; Sandler et al., 2018;\nRonneberger et al., 2015). In practice, researchers resort to\ncopy-pasting the original authors\u2019 code\u2014sometimes years\nout of date\u2014because re-implementing and validating the\nunderlying mathematics (let alone optimizing kernels) is\nprohibitively time-consuming. Convolutional layers are a\nprime example: half a dozen orthogonalization schemes\nexist, but none has achieved field-wide consensus, and their\nrelative merits are hard to benchmark because no common\ninterface or test-bed exists.\nTraining itself is also expensive. Certifiable objectives typ-\nically require longer schedules to converge, and per-batch\ncost grows as soon as weights are iteratively projected or\nparameter matrices inflated for numerical stability. Here,\nimplementation details matter: an efficient CUDA kernel\nor memory-lean fusion can translate directly into larger\nbatches, deeper models, or simply more optimization steps\non the same hardware budget.\nThese main points motivate Orthogonium . By centralizing\npublished methods behind a unified PyTorch API, standard-\nizing their signatures and test coverage, and prioritizing\nefficient, scalable kernels, the library (i) turns method com-\nparisons into one-liner swaps, (ii) lowers the entry-barriers\nfor neighboring fields to adopt 1-Lipschitz layers, and (iii)\nmakes large-scale experiments\u2014ImageNet-1K or semantic\nsegmentation tasks\u2014practically feasible.\nOur Contributions.\nTo address these challenges, we in-\ntroduce Orthogonium , a unified, efficient library that\ncombines theoretical rigor and practical implementation.\nOur main contributions are:\n\u2022 Unified, Explicit API: A comprehensive, PyTorch-\nfriendly implementation covering dense, convolutional,\nand hybrid orthogonal layers, explicitly constructed in\nthe spatial domain for straightforward integration.\n\u2022 Full Feature Parity: Native support for essential con-\nvolutional operations\u2014striding, dilation, transposition,\nand grouping\u2014allowing seamless integration into mod-\nern network architectures.\n\u2022 Efficient and Scalable Implementation: Optimized\nkernels provide high performance, with minimal over-\nhead (approximately 10% slowdown) compared to un-\nconstrained convolutions on large-scale benchmarks\nlike ImageNet (Deng et al., 2009).\n\u2022 Cross-Fertilization and Flexibility:\nOrthogonium\nprovides modularity to swiftly explore hybrid ap-\nproaches, enhancing existing methods such as SOC,\nSLL, and Sandwich layers, promoting broader adop-\ntion.\n\u2022 Extensive Validation and Testing: Our rigorous unit\ntesting identified and corrected subtle implementation\nerrors in published repositories, improving reliability\nand correctness across all supported methods.\nBy unifying orthogonality, flexibility, and computational\nefficiency, Orthogonium represents a significant advance-\nment, enabling researchers and practitioners to integrate\northogonal layers seamlessly into a wide variety of deep\nlearning applications. The remainder of the paper is struc-\ntured as follows: Section 2 introduces our dense layer imple-\nmentations, Section 3 covers orthogonal convolutions, and\nSection 4 covers the approach we used to unit test all our\nlayers. The issues identified in certain approaches under-\nscore the importance of open source tools for safety-critical\napplications.\n2. Dense Orthogonal Layers\nOrthogonium provides an efficient and flexible implementa-\ntion of orthogonal dense layers with a straightforward, drop-\nin PyTorch interface, OrthoLinear, supporting several\northogonalization methods. This approach simplifies inte-\ngration into existing workflows and allows users to choose\nmethods suited to their computational constraints and stabil-\nity requirements. Below, we outline the supported methods\nand their characteristics:\nUnified\nAPI\nwith\nOrthoLinear.\nThe\nprovided\nOrthoLinear class extends torch.nn.Linear, en-\nsuring seamless integration into standard PyTorch models.\nIt enforces orthogonality constraints through parameteriza-\ntions registered via the customizable OrthoParams ob-\nject, which encapsulates both spectral normalization and\northogonalization methods.\nSupported Orthogonalization Methods.\nOrthogonium\nsupports five distinct orthogonalization algorithms, each\nappropriate for different scenarios:\nBj\u00a8orck\u2013Bowie It-\nerative Projection, Exponential Map method, Modified\nGram\u2013Schmidt (QR Decomposition), Cayley Transform,\nCholesky Decomposition. These parametrizations are fully\ncompatible with the PyTorch parametrize API.\nSpectral Normalization.\nTo ensure numerical stability\nand enforce Lipschitz constraints, spectral normalization via\nbatched power iteration is applied optionally before orthog-\nonalization. This preconditioning enhances the stability and\nconvergence of the orthogonalization processes. Spectral\nnormalization can also be used in a standalone way, leading\nto 1-Lipschitz layers.\n2\nOrthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks\nFlexibility and Extensibility.\nUsers can easily customize\northogonalization and normalization methods through the\nOrthoParams object.\nOrthogonium provides several\npredefined configurations depending on the desired method.\nImplementation Efficiency.\nImplementation and effi-\nciency are crucial factors in the selection of a layer. This\nis why Orthogonium provides layers with some non-trivial\nmodifications in order to be more scalable.\nBy providing a unified API and efficient implementations,\nOrthogonium \u2019s dense orthogonal layers enable easy integra-\ntion, rigorous validation, and high-performance execution\nin diverse deep learning applications.\n3. Orthogonal Convolutions\nOrthogonium implements multiple classes of 1-Lipschitz\nand orthogonality-preserving convolutions\u2014allowing for a\nuser to choose between exactness, speed, and architectural\nflexibility\u2014 plus two 1-Lipschitz extensions that embed\nthe convolutions inside higher-order residual blocks. Ta-\nble 2 summarizes their properties, while the paragraphs\nbelow describe design choices and implementation tweaks;\nalgorithmic derivations are deferred to Appendix B. As the\nexposed layers can differ significantly from their original im-\nplementations, original layers are available in the legacy\nmodule.\nAdaptive\nOrthogonal\nConvolution\n(AOC).\nAOC\nis\nthe\ndefault\nconstructor,\nAdaptiveOrthoConv2d/ConvTranspose2d,\nand generalizes BCOP kernels (Li et al., 2019) to any kernel\nsize, stride, dilation, groups natively (i.e, without resorting\nto reshaping tricks or FFTs).\nTransposed convolutions\nare also supported natively.\nIt is based on the method\ndescribed in (Boissin et al., 2025). The layer materializes\nan explicit weight tensor whose forward path is a single call\nto torch.nn.Conv2d This approach yields a \u22641.13\u00d7\nwall-time over plain Conv2d on ImageNet-1k at batch\nsize 256.\nAdaptive-SOC.\nSkew\nOrthogonal\nConvolution\n(SOC) offers orthogonal training by parameterizing\nthe kernel as the exponential of a skew-symmetric\nfilter (Singla & Feizi, 2021b).\nOrthogonium \u2019s\nAdaptiveSOCConv2d/ConvTranspose2d\nfuses\nSOC with AOC\u2019s stride-aware approach, stores the\nexplicit exponential once per update (making its cost\nindependent of the batch size), and supports grouped,\ndilated or transposed variants out-of-the-box\u2014reducing\nmemory (See Appendix B.2). Also, this method relies on a\nnormalization step. We used \u201dAOL\u201d instead of the original\n\u201dfantastic four\u201d(Singla & Feizi, 2021a) approach, making\nthe convergence quicker than the original method. (3-4\niterations instead of the original 6).\nAlmost-Orthogonal Layers (AOL).\nWhen strict orthogo-\nnality is unnecessary, AOLConv2d implements the almost-\northogonal method of Prach & Lampert (Prach & Lampert,\n2022). The re-parametriser is registered through PyTorch\u2019s\nparametrize API and guarantees a layer Lipschitz con-\nstant \u22641, while remaining fast. Orthogonium implements\na multi-step variant, making use of the proximity between\nthis approach and the Gram iteration described by (Delattre\net al., 2024). This variant allows a tighter normalization\nthan the original method.\nSDP-based Lipschitz Layers (SLL).\n(Araujo et al.,\n2023) defined a 1-Lipschitz residual blocks that bundle a\n\u03c3(\u00b7)-non-linearity inside the convolution, while offering\na tight Lipschitz normalization. We extended the origi-\nnal SDPBasedLipschitzConv to support groups and\ndilation. We also designed its down-sampling equivalent\nSLLxAOCLipschitzResBlock which allows for stride\nand dimension change with an AOC kernel at their core to\nenable strides and channel changes, similarly as the Resnet\ndownsampling blocks (Fig. 3); details are given in Ap-\npendix B.3.\nSandwich-AOC.\nFinally,\nOrthogonium replaces the\ncostly frequency-domain Cayley step of \u201csandwich layers\u201d\n(Wang & Manchester, 2023) with an explicit AOC kernel,\nremoving complex-valued FFTs, making this layer efficient\nfor large input images (e.g, 224 \u00d7 224 ); details are given\nin Appendix B.4. This layer is still under development and\nwill be available soon.\nOther Modules: Activations, Normalization, and Resid-\nual Blocks.\nBeyond convolutions, Orthogonium cen-\ntralises the necessary layers needed to build fully 1-\nLipschitz networks.\nActivations.\nFive\ngradient-norm-preserving\nnon-\nlinearities are shipped in custom activations: Abs,\nSoftHuber, MaxMin, HouseHolder, and its second-\norder variant. Each is unit-Lipschitz by construction; for\nthe Householder family we patched the missing 1/\n\u221a\n2 scale\nfactor, restoring \u03c3max(J) = 1 to machine precision (see\nAppendix C). Detailed APIs appear in the online docs.\nNormalization.\nInstead\nof\nBatchNorm\u2014which\ndestroys\nLipschitz\ncontrol\u2014\nOrthogonium\noffers\nBatchCentering and LayerCentering.\nBoth\nsubtract running means but leave variances untouched,\nso they preserve feature-map norms at inference; their\nimplementation lives in normalization.py.\nSince the usual residual connection is not 1-Lipschitz,\n3\nOrthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks\nFigure 2: Implemented convolutional layers in Orthogonium . All run on GPU, accept stride, dilation, groups,\npadding mode, and have (when possible) parity with nn.Conv2d.\nLayer\nOrthogonality\nKey use-case\nInternal method\nAOC (AdaptiveOrthoConv2d)\nexact\ngeneral CNN backbones\nlifted BCOP / RKO\nAdaptive-SOC\nexact\ndepthwise / small kernel size\nexponential skew filter\nAOL\n\u22641-Lip. (\u2248)\nfast training\nmulti-step projection\nSLL / SLL-AOC\n\u22641-Lip. (tight)\nresidual blocks\nAOL + SDP constraint\nSandwich-AOC\n\u22641-Lip. (tight)\ntight Lipschitz estimation without orthogonality\nAOC pair\nOrthogonium provides several lightweight residual wrap-\npers designed to combine an arbitrary internal function\nfn\u2014typically an orthogonal convolution followed by non-\nlinear activation\u2014with a skip connection, ensuring that\nthe resulting residual blocks remain exactly 1-Lipschitz.\nThese wrappers, implemented as minimal \u2018torch.nn.Module\u2018\nclasses, preserve the benefits of skip connections while\nstrictly enforcing global Lipschitz constraints.\nAmong the various strategies, ConcatResidual splits the\ninput along the channel dimension, applies a function fn\nto one half, and concatenates it back with the untouched\nhalf, ensuring the resulting block remains 1-Lipschitz if\nfn is. L2NormResidual combines the identity and resid-\nual branches using an \u21132 average, specifically outputting\nq\n1\n2x2 + 1\n2fn(x)2 + \u03b5 (with small \u03b5 for numerical stabil-\nity), ensuring exact 1-Lipschitz continuity. AdditiveResidual\nand its variation, PrescaledAdditiveResidual, form convex\ncombinations of the identity and transformed branches us-\ning a learnable scalar gate \u03b1: the former via interpolation\n\u03b1x + (1 \u2212\u03b1)fn(x) (with \u03b1 constrained by a sigmoid), and\nthe latter by premultiplying the input as x+fn(\u03b1x)\n1+|\u03b1| , where \u03b1\nunconstrained.\n4. Unit testing of constrained layers\nDespite theoretical guarantees, empirical verification re-\nmains crucial to detect (i) numerical instabilities (e.g.,\nfloating-point precision errors) and (ii) implementation dis-\ncrepancies (e.g., padding mismatch, missing factor), ensur-\ning orthogonality in practice. More details about our unit\ntesting scheme are available in Appendix C.\nUnit testing of convolutional layers.\nOrthogonality de-\npends on training hyperparameters and convolution pa-\nrameters (stride, group, dilation, transposition). We en-\nsure correctness by combining two methods:\n(i) ex-\nplicit SVD on Toeplitz matrices for precise validation\non small inputs, and (ii) scalable spectral methods (from\nconv.singular values) for practical, large-scale val-\nidation (as it uses parametrization-aware optimizations). All\ntests maintain singular value tolerance ranging from 10\u22124\nto 5e\u22123 (for some methods).\nWe used both of these approaches in our unit tests. This\nenables us to ensure that the second method (which is faster\nand more scalable) is correct and effectively checks for layer\northogonality. We also added several unit tests to ensure\nthat impossible theoretical configurations\u2014as described in\n(Achour et al., 2022)\u2014are rejected.\nUnit testing of non-linear layers.\nTo guarantee that ac-\ntivations and higher-order residual blocks do not silently\nviolate the global 1-Lipschitz constraint, we complement\nthe linear checks described above with a non-linear test suite\nbased on the empirical Jacobian (computed with automatic\ndifferentiation) computed on randomly sampled and opti-\nmized tensors. This verified that the optimizer updates kept\nthe block weights within the desired constraints.\nIssues uncovered using unit-testing\nCrucially, these tests\nuncovered a flaw in the original HouseHolder activation\n(Singla & Feizi, 2022): the reflection missed a 1/\n\u221a\n2 nor-\nmalization and was therefore\n\u221a\n2-Lipschitz. Re-scaling the\nkernel collapses all singular values to 1 \u00b1 10\u22125, after which\nthe layer satisfies our criteria.\nOverall, this test bank was of precious use to confirm that\nall parameters can be combined in practice (i.e., A strided-\ngrouped-transposed convolution with a dilation factor is still\northogonal). The library achieves 94% test coverage overall.\n5. Conclusion\nOrthogonium unifies a decade of advances in orthogonal\nand Lipschitz-constrained layers into a single, efficient, and\ncomprehensive PyTorch library. By providing native sup-\nport for strided, dilated, grouped, and transposed convolu-\ntions\u2014alongside rigorous validation and optimized code\u2014it\nsignificantly reduces implementation overhead, fosters re-\nliable experimentation, and promotes adoption in critical\napplications such as certified robustness, generative model-\ning, and stable recurrent architectures. Furthermore, open-\nsourcing Orthogonium facilitates rigorous testing and val-\nidation by the broader community, uncovering subtle im-\nplementation errors and enabling ongoing verification and\nimprovements. Orthogonium thus serves as a foundational\nresource, bridging theory and practice to enable scalable,\n4\nOrthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks\nrobust, and provably stable deep learning architectures.\nAcknowledgements\nThis work was carried out within the DEEL project,1 which\nis part of IRT Saint Exup\u00b4ery and the ANITI AI cluster. The\nauthors acknowledge the financial support from DEEL\u2019s\nIndustrial and Academic Members and the France 2030 pro-\ngram \u2013 Grant agreements n\u00b0ANR-10-AIRT-01 and n\u00b0ANR-\n23-IACL-0002.\nReferences\nAchour, E. M., Malgouyres, F., and Mamalet, F. Existence,\nstability and scalability of orthogonal convolutional neu-\nral networks. Journal of Machine Learning Research, 23\n(208):1\u201356, 2022.\nAnil, C., Lucas, J., and Grosse, R. Sorting out lipschitz\nfunction approximation. In International Conference on\nMachine Learning, pp. 291\u2013301. PMLR, 2019.\nAraujo, A., Havens, A. J., Delattre, B., Allauzen, A., and\nHu, B. A unified algebraic perspective on lipschitz neural\nnetworks. In The Eleventh International Conference on\nLearning Representations, 2023.\nArjovsky, M., Chintala, S., and Bottou, L. Wasserstein gen-\nerative adversarial networks. In International conference\non machine learning, pp. 214\u2013223. PMLR, 2017.\nBansal, N., Chen, X., and Wang, Z. Can we gain more from\northogonality regularizations in training deep networks?\nAdvances in Neural Information Processing Systems, 31,\n2018.\nBehrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D.,\nand Jacobsen, J.-H. Invertible residual networks. In\nInternational conference on machine learning, pp. 573\u2013\n582. PMLR, 2019.\nBj\u00a8orck, \u02daA. and Bowie, C. An iterative algorithm for com-\nputing the best estimate of an orthogonal matrix. SIAM\nJournal on Numerical Analysis, 8(2):358\u2013364, 1971.\nBoissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena,\nT., and Serrurier, M. An adaptive orthogonal convolution\nscheme for efficient and flexible cnn architectures. arXiv\npreprint arXiv:2501.07930, 2025.\nBoroojeny, A. E., Telgarsky, M., and Sundaram, H. Spec-\ntrum extraction and clipping for implicitly linear layers.\nIn International Conference on Artificial Intelligence and\nStatistics, pp. 2971\u20132979. PMLR, 2024.\n1https://www.deel.ai/\nCayley, A.\nSur quelques Propri\u00b4et\u00b4es des D\u00b4eterminants\nGauches, pp. 332\u2013336. Cambridge Library Collection -\nMathematics. Cambridge University Press, 1846.\nDelattre, B., Barth\u00b4elemy, Q., Araujo, A., and Allauzen, A.\nEfficient bound of lipschitz constant for convolutional\nlayers by gram iteration. In International Conference on\nMachine Learning, pp. 7513\u20137532. PMLR, 2023.\nDelattre, B., Barth\u00b4elemy, Q., and Allauzen, A.\nSpec-\ntral norm of convolutional layers with circular and zero\npaddings. arXiv preprint arXiv:2402.00240, 2024.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pp. 248\u2013255. Ieee, 2009.\nDing, X., Zhang, X., Han, J., and Ding, G. Scaling up\nyour kernels to 31x31: Revisiting large kernel design\nin cnns. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 11963\u2013\n11975, 2022.\nFazlyab, M., Robey, A., Hassani, H., Morari, M., and Pap-\npas, G. Efficient and accurate estimation of lipschitz\nconstants for deep neural networks. Advances in neural\ninformation processing systems, 32, 2019.\nGrishina, E., Gorbunov, M., and Rakhuba, M. Tight and\nefficient upper bound on spectral norm of convolutional\nlayers. arXiv preprint arXiv:2409.11859, 2024.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and\nCourville, A. C. Improved training of wasserstein gans.\nAdvances in neural information processing systems, 30,\n2017.\nHu, K., Leino, K., Wang, Z., and Fredrikson, M. A recipe\nfor improved certifiable robustness.\nIn The Twelfth\nInternational Conference on Learning Representations,\n2023.\nKiani, B., Balestriero, R., LeCun, Y., and Lloyd, S. projunn:\nefficient method for training deep networks with unitary\nmatrices. Advances in Neural Information Processing\nSystems, 35:14448\u201314463, 2022.\nKingma, D. P. Auto-encoding variational bayes. arXiv\npreprint arXiv:1312.6114, 2013.\nKingma, D. P. and Dhariwal, P. Glow: Generative flow\nwith invertible 1x1 convolutions. Advances in neural\ninformation processing systems, 31, 2018.\nLaPlace, P. S. Th\u00b4eorie analytique des probabilit\u00b4es. Courcier,\n1820. URL http://eudml.org/doc/203444.\n5\nOrthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks\nLi, Q., Haque, S., Anil, C., Lucas, J., Grosse, R. B., and Ja-\ncobsen, J.-H. Preventing gradient attenuation in lipschitz\nconstrained convolutional networks. Advances in neural\ninformation processing systems, 32, 2019.\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T.,\nand Xie, S. A convnet for the 2020s. In Proceedings\nof the IEEE/CVF conference on computer vision and\npattern recognition, pp. 11976\u201311986, 2022.\nPauli, P., Wang, R., Manchester, I. R., and Allg\u00a8ower, F.\nLipschitz-bounded 1d convolutional neural networks us-\ning the cayley transform and the controllability gramian.\nIn 2023 62nd IEEE Conference on Decision and Control\n(CDC), pp. 5345\u20135350. IEEE, 2023.\nPauli, P., Gramlich, D., and Allg\u00a8ower, F. Lipschitz constant\nestimation for general neural network architectures using\ncontrol tools. arXiv preprint arXiv:2405.01125, 2024.\nPrach, B. and Lampert, C. H. Almost-orthogonal layers for\nefficient general-purpose lipschitz networks. In European\nConference on Computer Vision, pp. 350\u2013365. Springer,\n2022.\nQi, H., You, C., Wang, X., Ma, Y., and Malik, J. Deep\nisometric learning for visual recognition. In International\nconference on machine learning, pp. 7824\u20137835. PMLR,\n2020.\nRonneberger,\nO.,\nFischer,\nP.,\nand Brox,\nT.\nU-\nnet:\nConvolutional networks for biomedical image\nsegmentation.\nIn Medical image computing and\ncomputer-assisted intervention\u2013MICCAI 2015:\n18th\ninternational conference, Munich, Germany, October\n5-9, 2015, proceedings, part III 18, pp. 234\u2013241.\nSpringer, 2015.\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and\nChen, L.-C. Mobilenetv2: Inverted residuals and linear\nbottlenecks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 4510\u20134520,\n2018.\nSedghi, H., Gupta, V., and Long, P. M. The singular values\nof convolutional layers. In International Conference on\nLearning Representations, 2018.\nSenderovich, A., Bulatova, E., Obukhov, A., and Rakhuba,\nM.\nTowards Practical Control of Singular Values of\nConvolutional Layers. Advances in Neural Information\nProcessing Systems, 35:10918\u201310930, December 2022.\nSerrurier, M., Mamalet, F., Gonz\u00b4alez-Sanz, A., Boissin, T.,\nLoubes, J.-M., and Del Barrio, E. Achieving robustness in\nclassification using optimal transport with hinge regular-\nization. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 505\u2013514,\n2021.\nSingla, S. and Feizi, S. Fantastic four: Differentiable and ef-\nficient bounds on singular values of convolution layers. In\nInternational Conference on Learning Representations,\n2021a.\nSingla, S. and Feizi, S.\nSkew orthogonal convolutions.\nIn International Conference on Machine Learning, pp.\n9756\u20139766. PMLR, 2021b.\nSingla, S. and Feizi, S. Improved techniques for deter-\nministic l2 robustness. Advances in Neural Information\nProcessing Systems, 35:16110\u201316124, 2022.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I., and Fergus, R. Intriguing proper-\nties of neural networks. In International Conference on\nLearning Representations, 2014.\nTan, M. and Le, Q. Efficientnet: Rethinking model scal-\ning for convolutional neural networks. In International\nconference on machine learning, pp. 6105\u20136114. PMLR,\n2019.\nTrockman, A. and Kolter, J. Z. Orthogonalizing convolu-\ntional layers with the cayley transform. In International\nConference on Learning Representations, 2021.\nVirmaux, A. and Scaman, K.\nLipschitz regularity of\ndeep neural networks: analysis and efficient estimation.\nAdvances in Neural Information Processing Systems, 31,\n2018.\nWang, R. and Manchester, I.\nDirect parameterization\nof lipschitz-bounded deep networks.\nIn International\nConference on Machine Learning, pp. 36093\u201336110.\nPMLR, 2023.\nWang, Z., Hu, B., Havens, A. J., Araujo, A., Zheng, Y.,\nChen, Y., and Jha, S. On the scalability and memory effi-\nciency of semidefinite programs for lipschitz constant es-\ntimation of neural networks. In The Twelfth International\nConference on Learning Representations, 2024.\nXiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and\nPennington, J. Dynamical isometry and a mean field\ntheory of cnns: How to train 10,000-layer vanilla convo-\nlutional neural networks. In International Conference on\nMachine Learning, pp. 5393\u20135402. PMLR, 2018.\n6\nOrthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks\nA. Orthogonalization methods available for 2D\nweights matrices\nA.1. QR Factorization via Modified Gram\u2013Schmidt\nWe implement the Modified Gram\u2013Schmidt (MGS) algo-\nrithm (LaPlace, 1820) for numerically stable QR factoriza-\ntion. Starting from W = [wi]C\u22121\ni=0 \u2208RC\u00d7C, we orthogonal-\nize one column at a time, correcting rounding errors at each\nstep:\nTo enforce a unique factorization and improve stability, we\npost-multiply Q by sign(diag(R)) so that all diagonal en-\ntries of R are positive.\nA.2. Cayley Transform\nThe Cayley transform (Cayley, 1846) maps any skew-\nsymmetric A (AT = \u2212A) to an orthogonal Q via:\nQ = (I \u2212A)(I + A)\u22121\nFor rectangular W \u2208RM\u00d7C (M \u2265C), we follow the\npartitioning and \u201caugmented\u201d Cayley of (Pauli et al., 2023):\nAlgorithm 1 Augmented Cayley Transform\nRequire: W \u2208RM\u00d7C\nEnsure: \u02c6W \u2208RM\u00d7C, orthogonal columns\n1: procedure CAYLEY TRANSFORM( W )\n2:\nPartition W =\n\u0014U\nV\n\u0015\n, U \u2208RC\u00d7C\n3:\nA \u2190U \u2212U T + V T V\n\u25b7not strictly skew but\nyields correct block\n4:\nB \u2190(I + A)\u22121\n5:\n\u02c6W1 \u2190B (I \u2212A), \u02c6W2 \u2190\u22122 V B\n6:\n\u02c6W \u2190[ \u02c6W1; \u02c6W2 ]\n7:\nreturn \u02c6W\n8: end procedure\nMatrix inversion can be a bottleneck, so in practice we cache\nand reuse factorizations where possible.\nA.3. Exponential Map\nUsing the Lie-group exponential exp(A) of a skew-\nsymmetric A = W \u2212W T yields an orthogonal matrix\nsince exp(A)T = exp(\u2212A). We normalise A by its spec-\ntral norm to avoid overflow and truncate the power series\nafter p terms (Singla & Feizi, 2021b):\nAlgorithm 2 Exponential Map with Spectral Normalization\nRequire: W \u2208RC\u00d7C, p \u2208N\nEnsure: \u02c6W = exp(A) with A = \u2212AT\n1: procedure LIPSCHITZ EXPONENTIAL(W,p)\n2:\nA \u2190W \u2212W T\n3:\n\u02c6A \u2190A/\u2225A\u22252\n\u25b7spectral normalization\n4:\n\u02c6W \u2190IC, \u02c6Ak \u2190IC\n5:\nfor k = 1, . . . , p do\n6:\n\u02c6Ak \u21901\nk \u02c6Ak \u02c6A\n7:\n\u02c6W \u2190\u02c6W + \u02c6Ak\n8:\nend for\n9:\nreturn \u02c6W\n10: end procedure\nThis method parametrizes only SO(C) and its accuracy\ndepends on p.\nA.4. Cholesky Decomposition\nFollowing (Hu et al., 2023), we form the Gram matrix C =\nWW T + \u03b5I (\u03b5 > 0 for PD), compute its Cholesky C =\nLLT , and solve\nL \u02c6W = W\n=\u21d2\n\u02c6W = L\u22121W,\nwhich enforces \u02c6W \u02c6W T = I. This triangular solve is O(C3)\nbut highly tuned in modern BLAS libraries.\nA.5. Bj\u00a8orck\u2013Bowie Iterative Projection\nThe Bj\u00a8orck\u2013Bowie algorithm (Bj\u00a8orck & Bowie, 1971) finds\nthe nearest orthogonal matrix by fixed-point iteration\nWt+1 = (1 + \u03b2) Wt \u2212\u03b2 WtW \u22a4\nt Wt,\n\u03b2 \u2208\n\u00000, 1\n2\n\u0003\n.\nWith spectral normalization of W0 and \u03b2 = 1\n2, convergence\nis fast: 12\u201325 iterations suffice in practice (Anil et al., 2019).\nWe reuse a cached power-iteration estimate of the top singu-\nlar vector across updates to further accelerate each step.\nB. Technical details of improved orthogonal\nconvolution methods\nB.1. Improving BCOP (AOC)(Boissin et al., 2025)\nOrthogonium introduces Adaptive Orthogonal Convolution\n(AOC),(Boissin et al., 2025), combining the strengths of\nBCOP (Li et al., 2019) and Reshaped Kernel Orthogonal-\nization (RKO) (Serrurier et al., 2021). BCOP constructs\nexplicit orthogonal convolution kernels by composing ele-\nmentary orthogonal building blocks (such as 1\u00d71, 1\u00d72, and\n2\u00d71 convolutions), but originally lacks support for advanced\nconvolutional operations like stride, dilation, transposition,\nor grouped convolutions. Conversely, RKO supports native\n7\nOrthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks\nstriding but typically achieves only approximate orthogonal-\nity.\nAOC addresses these limitations by integrating BCOP and\nRKO into a single orthogonal convolutional kernel. Specifi-\ncally, given a desired stride s = k, AOC defines the convo-\nlutional kernel as:\nAOC = RKO \u229bKBCOP\n(1)\nThe resulting kernel maintains strict orthogonality and ex-\nplicitly supports stride, dilation, grouping, and transposed\nconvolutions natively. Crucially, orthogonality is preserved\nthrough a careful choice of internal channel dimensions, en-\nsuring both flexibility and computational efficiency (Boissin\net al., 2025).\nAOC is rigorously proven orthogonal for any valid con-\nfiguration (k \u2265s), (Achour et al., 2022), offering signif-\nicant practical advantages over existing methods that rely\non computationally expensive reshaping or Fourier-based\noperations.\nNative Strided Convolution.\nUnlike prior methods that\nemulate striding via tensor reshaping\u2014leading to substan-\ntial computational overhead\u2014AOC implements native strid-\ning. This approach avoids the exponential computational\ncomplexity of reshaped methods, making orthogonal convo-\nlutions feasible for large-scale applications.\nNative Transposed Convolution.\nBy explicitly construct-\ning orthogonal kernels, AOC naturally supports transposed\nconvolutions, crucial for architectures requiring learnable\nupsampling such as U-Nets (Ronneberger et al., 2015) and\nVariational Autoencoders (VAEs) (Kingma, 2013).\nNative Grouped Convolution.\nAOC efficiently supports\ngrouped convolutions, widely used in contemporary mod-\nels such as EfficientNet and ResNeXt, by independently\northogonalizing groups within the convolutional layer.\nDilation.\nOrthogonality under dilation follows naturally\nfrom AOC\u2019s explicit kernel construction, providing enlarged\nreceptive fields without additional parameter overhead or\nloss of orthogonality.\nThese native implementations allow AOC to maintain a\nminimal overhead (approximately 10%) compared to uncon-\nstrained convolutional models, even at ImageNet scales.\nIn this section, we will explore how the content of this paper\ncan be used to improve existing layers from the state of the\nart.\nB.2. Improving skew orthogonal convolution\n(SOC)(Singla & Feizi, 2022)\nThis method, introduced by (Singla & Feizi, 2022) uses\nthe fact that an exponential of a skew-symmetric matrix\nis orthogonal. The initial implementation builds a skew-\nsymmetric kernel and computes the exponential convolution.\nHowever, without proper tools to compute the exponential\nof a convolution kernel, this exponential was computed\nimplicitly for each input by using the Taylor expansion of\nthe exponential (see Eq. (2)).\nTheorem B.1 (Explicit conv exponential). We can use the\nblock convolution operator 2 to compute explicitly the expo-\nnential of a kernel K:\nx + K \u2217x\n1!\n+ K \u2217K \u2217x\n2!\n+ . . .\n(2)\n=\n\u0012\nId + K + K \u229bK\n2!\n+ K \u229bK \u229bK\n3!\n+ . . .\n\u0013\n\u2217x\n(3)\nEquation (3) shows that we can compute the exponential of\na convolution kernel a single time, while the formulation in\nEq. (2) needs to be done for each input x. In other words,\nwe can apply one conv instead of niter convs. Note that\nthe resulting kernel is then larger than the original one. In\ntheory, this could unlock large speedups, but the gain is\nlimited in practice as the implementation of convolution\nlayers is optimized for small kernels and large images (Ding\net al., 2022). However, the original implementation requires\nthe storage of niter maps, whereas our implementation only\none. This, in practice, unlocks larger networks and batch\nsizes.\nAlso, it is possible to handle a change in the number of\nchannels and striding using a similar approach as AOC\nlayers.\nB.3. Improving SDP-based Lipschitz Layers (SLL)\n(Araujo et al., 2023)\nSLL layer for convolutions, proposed in (Araujo et al.,\n2023), is a 1-Lipschitz layer defined as:\ny = x \u22122KT \u22c6(\u03c3(K \u22c6x + b))\nNote that in the original paper, the equation is noted with\nproduct of two matrices WT \u22121\n2 , for convolutions it repre-\nsents toeplitz matrix, i.e. WT \u22121\n2 = K.\nSLL layer does not natively support neither strides nor\n2Block convolution operator allows to fuse the kernels of two\nconvolutions to construct the kernel of a convolution equivalent to\nthe composition of the two convolutions. It is defined in (Li et al."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**Orthogonium: A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks**\n\n### Main Contributions\n\n- **Unified API**: Introduces a comprehensive, PyTorch-friendly implementation covering dense, convolutional, and hybrid orthogonal layers. The library provides straightforward access to these layers via an explicit API.\n  \n- **Full Feature Parity**: Native support for essential convolution operations such as striding, dilation, transposition, and grouping allows seamless integration into modern network architectures.\n\n- **Efficient Implementation**: Optimized kernels provide high performance with minimal overhead compared to unconstrained convolutions on large-scale benchmarks like ImageNet. Rigorous testing within the library has uncovered critical errors in existing implementations, emphasizing the importance of standardized tools.\n\n- **Cross-Fertilization and Flexibility**: The library promotes hybrid approaches by providing modularity, enhancing existing methods such as SOC, SLL, and Sandwich layers, and promoting broader adoption.\n\n- **Extensive Validation and Testing**: Rigorous unit testing has identified and corrected subtle implementation errors in published repositories, improving reliability and correctness across all supported methods.\n\n### Methodology\n\nOrthogonium is designed to address the fragmented nature of existing implementations for orthogonal and 1-Lipschitz layers. The library combines theoretical rigor with practical implementation efficiency:\n\n- **Unified API**: A comprehensive PyTorch API that covers dense, convolutional, and hybrid orthogonal layers, explicitly constructed in the spatial domain for straightforward integration.\n  \n- **Orthogonalization Methods**: Orthogonium supports five distinct orthogonalization algorithms: Bj\u00f6rck\u2013Bowie Iterative Projection, Exponential Map method, Modified Gram\u2013Schmidt (QR Decomposition), Cayley Transform, Cholesky Decomposition. These methods are fully compatible with the PyTorch parametrize API.\n\n- **Efficient Kernels**: Optimized kernels provide high performance and minimal overhead compared to unconstrained convolutions on large-scale benchmarks like ImageNet.\n\n- **Flexibility and Extensibility**: Users can customize orthogonalization and normalization methods through the OrthoParams object. The library provides several predefined configurations depending on desired methods.\n\n### Key Findings\n\n1. **Unified API and Implementation**: By centralizing published methods behind a unified PyTorch API, Orthogonium lowers entry barriers for adopting 1-Lipschitz layers in diverse applications.\n2. **Efficient Kernels**: Optimized kernels provide high performance compared to unconstrained convolutions on large-scale benchmarks like ImageNet, reducing overhead by approximately 10%.\n3. **Cross-Fertilization and Flexibility**: The library promotes hybrid approaches by providing modularity for exploring new methods such as SOC, SLL, and Sandwich layers, enhancing existing methods and promoting broader adoption.\n\n### Conclusions\n\nOrthogonium represents a significant advancement in enabling researchers and practitioners to seamlessly integrate orthogonal layers into various deep learning applications. By unifying orthogonality, flexibility, and computational efficiency, Orthogonium significantly reduces implementation overhead, fosters reliable experimentation, and promotes adoption in critical areas such as certified robustness, generative modeling, and stable recurrent architectures.\n\nThe library is available for use, providing a unified interface to access and create 1-Lipschitz layers, allowing users to construct, test, and improve networks that benefit from these constraints."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for\nMedical Imaging\nAdrien Meyera,b,1, Didier Mutterb,c, Nicolas Padoya,b\naUniversity of Strasbourg, CNRS, INSERM, ICube, UMR7357, France\nbIHU Strasbourg, Strasbourg, France\ncUniversity Hospital of Strasbourg, France\nDetecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object\ndetection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection\n(WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while\npreserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point\nannotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including\noverlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome\nthese challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical\nimaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with\nthe proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to\ncapture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, In-\nstance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among\nadjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency\nacross different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance\nacross three datasets spanning different medical domains\u2014endoscopy, chest X-rays, and endoscopic ultrasound\u2014highlighting\nits potential to reduce annotation costs while maintaining high detection accuracy.\nKeywords: Mixture of Experts, Object Detection, Point Annotation, Weakly Semi Supervised Learning\n1. Introduction\nObject-detection approaches are becoming essential tools\nin medical diagnostics, enhancing landmark identification,\nsafety assessments, and anatomy-driven reasoning [19, 24, 1].\nDeep learning has significantly advanced these technologies;\nhowever, its effectiveness relies on the availability of large\ndatasets with detailed annotations [3]. In the medical field,\ncreating these datasets is particularly challenging due to the\ndifficulty in annotating complex anatomical structures. These\nstructures often share similar colors and textures, can be\nhighly deformable, and overlap with each other.\nConse-\nquently, precise annotation is labor-intensive and further com-\npounded by the limited availability of medical professionals.\nTo mitigate the annotation challenges of medical data and\ntrain with limited annotations, Weakly Supervised Object\nDetection (WSOD) and Semi-Supervised Object Detection\n(SSOD) approaches have emerged. WSOD methods use abun-\ndant but weakly annotated data, such as image tags [13, 28].\nThe studies by [29, 11] utilize class activation maps to en-\nable detection and localization of surgical tools in endoscopic\nvideos and breast cancer in ultrasound images, respectively,\nwithout spatial annotations. SSOD strategies combine a small\n1Corresponding author: ameyer1@unistra.fr\nset of images with box-level annotations and a larger pool\nof unlabeled images. These strategies primarily utilize two\ntechniques: First, consistency regularization, which maintains\nstable detector predictions across differently augmented im-\nages [9].\nSecond, pseudo-labeling, where a teacher model\ntrained on the labeled dataset generates pseudo-labels for the\nunlabeled images. A student model learns from both labeled\nand pseudo-labeled datasets, improving detection capabilities\n[17, 30]. Although both methods lower annotation costs, the\neffectiveness of the trained detectors often lags behind that of\nfully-supervised models, depending on the amount of avail-\nable annotations.\nBuilding on these approaches, Weakly Semi-Supervised\nObject Detection with Point annotations [6, 10, 32] (WSSOD-\nP) integrates SSOD and WSOD principles to provide a cost-\neffective, label-efficient solution for object detection. A Point-\nto-Box teacher model, trained on a set of box-labeled images,\ngenerates pseudo box-labels from point annotated images to\ntrain a student model, thus reducing the need for costly box\nannotation (see Fig. 1). A single point marks each weakly\nannotated instance, efficiently conveying both category and\nlocation information with a minimal labeling effort, akin to\nimage-level annotations [2].\nPrior works [6, 10] introduce a point encoder that elegantly\nintegrates positional encoding and class embeddings to gen-\nerate point-queries. Following the classic DETR model [4],\narXiv:2601.13954v1  [cs.CV]  20 Jan 2026\n2\nFig. 1. Pipeline of the Weakly Semi-Supervised Object Detection with Points (WSSOD-P) framework. The framework aims to train a detector while\nminimizing box annotation time by predominantly using faster point annotations across the dataset. (a) Box annotations are used to sample points and\ntrain a Point-to-Box regressor. (b) The trained Point-to-Box model generates pseudo-box labels for point-annotated images. (c) A student detector is\ntrained on both the box-labeled and pseudo-labeled images to improve detection performance.\nthese point-queries are refined through stacked decoder blocks\nusing self and cross-attention with the feature maps, a feed-\nforward network (FFN), and a regressor network to predict\nbounding boxes. However, DETR-based models face notable\nchallenges, including slow convergence and high sensitivity to\npoint locations [32, 10] as (1) points in the center of instances\nare often more informative than peripheral points, and (2) ad-\njacent points can share features, blurring instance boundaries\nand causing confusion.\nTo address these challenges, we introduce DExTeR (DETR\nwith Experts), a DETR-based approach within the WSSOD-\nP framework to enhance object detection in medical imag-\ning. DExTeR analyzes prior information from point-prompts\nacross three key dimensions: the weak location information,\nthe strong appearance priors conveyed by class information,\nand the distinct instance each point indicates. Effective uti-\nlization of class and instance information is crucial, especially\nwhen multiple points are adjacent, as it helps to clarify their\nrelationships and representation. An overview of our method\nis presented in Fig. 2.\nWe\nenhances\nMulti-Scale\nDeformable\nAttention\n(MSDA) [34],\nwhich utilizes feature maps at multiple\nscales from the feature encoder to better distinguish adjacent\npoints.\nMSDA uses the point location as a reference and\nderives offsets from the corresponding point-query to pool\nvisual features. By making the point-query class-aware, we\nguides the attention sampling with class-specific information.\nWe term this approach class-guided MSDA.\nAfter undergoing cross-attention to gather visual features,\nthe point-queries are updated through an FFN. However, the\nFFN operates with fixed parameters for all point-queries, re-\nquiring it to handle queries in a class- and instance-agnostic\nmanner\u2014a significant limitation. To better address the speci-\nficity of each query and decouple the representation learn-\ning process, we introduce our proposed CLass, Instance, and\nCommon Knowledge Mixture of Experts (CLICK-MoE). Each\nquery is processed by three experts: the Common Knowl-\nedge Expert, reminiscent of the FFN used in the transformer\nblock, processes all queries to provide class-agnostic insights;\nthe Class-specific expert, which handle queries specific to\ntheir respective classes, delivers class-tailored expertise; and\nthe Instance Expert, which leverage dynamically generated\ninstance-aware parameters, enhances discrimination among\npoint-queries and reduces confusion between instances.\nFinally, we propose a multi-point training strategy to reduce\npoint location dependence. In each training iteration, we sam-\nple N random points per instance and encode each as a point-\nquery. Each point-query interacts with only one other point-\nquery per object, mirroring the inference scenario where a sin-\ngle point per instance is used. Our multi-point training strat-\negy increases the supervision signals and implicitly promotes\nconsistent predictions across diverse point-prompt scenarios\nthroughout the training process, effectively simulating single-\npoint supervision N times per iteration for each instance to\nenhance training efficiency and robustness.\nWe validate the effectiveness of DExTeR in data-scarce sce-\nnarios across three datasets spanning distinct medical imag-\ning domains: Endoscapes (surgical) [23], VinDr-CXR (X-\nRay) [25], and EUS-D130 (Endoscopic Ultrasound) [21]. We\nsummarize our contributions as follows:\n1. We propose class-guided MSDA to direct cross-attention\nsampling with class-specific information.\n2. We decouple class, instance, and common knowledge\nprocessing with our CLICK-MoE approach, significantly\nimproving point-query refinement.\n3. We address the dependency on specific point locations\nwith our multi-point training strategy.\n4. We demonstrate the effectiveness of our approach across\nthree diverse medical imaging domains for both teacher\n3\nFig. 2. DExTeR Model: it leverages multiple point groups during training and incorporates class-guided Multi-Scale Deformable Attention (MSDA)\nalongside the Class, Instance, and Common Knowledge Mixture of Experts (CLICK-MoE). Point queries are iteratively refined using a Point-to-Box\nstrategy implemented through stacked decoder blocks.\nand student models, achieving consistent improvements\nin performance under varying annotation settings.\n2. Related Work\n2.1. DETR-based Supervised Object Detectors\nDETR proposes a transformer-based end-to-end object de-\ntector [4] that eliminates the need for many hand-crafted com-\nponents. It consists of: (a) a backbone that extracts image\nfeatures from an input image and transforms them into a se-\nquence, with sine/cosine positional encodings added to each\nfeature-point to incorporate spatial context; (b) a multi-layer\ntransformer encoder to refine the features; (c) a set of learn-\nable object queries, used by (d) a multi-layer transformer de-\ncoder that employs self-attention for reasoning among object\nqueries and cross-attention for interaction with image fea-\ntures, followed by a feed-forward network (FFN) to enhance\nrepresentation; and (e) multiple prediction heads. However,\nDETR suffers from slow convergence and limited feature spa-\ntial resolution due to the dense attention modules\u2019 limitations\nin processing image feature maps. Deformable DETR [34]\naddresses this by designing a deformable attention module\nthat attends only to certain sampling points around a refer-\nence point, accelerating convergence and reducing computa-\ntional load, thereby allowing multi-scale attention. This refer-\nence point concept enables iterative box refinement [34, 31],\nwhere bounding boxes are iteratively updated between de-\ncoder stages. Another popular technique is DN-DETR\u2019s [14]\ndenoising training method, which speeds up DETR training\nby feeding noise-added ground-truth queries into the decoder\nand training the model to reconstruct the original ones.\n2.2. Foundational model\nRecent advances in natural image segmentation have led\nto the development of foundation models like the Segment\nAnything Model (SAM) [12], which yield impressive zero-\nshot segmentation capabilities from minimal prompts such as\npoints or boxes. However, its adaptation to medical image\nsegmentation, such as with MedSAM [18]\u2014a fine-tuned ver-\nsion on medical datasets that only accept box prompt\u2014faces\nchallenges due to the unique characteristics of medical im-\nages. Although SAM and MedSAM demonstrate impressive\ncapabilities, they often require multiple prompts and struggle\nto differentiate highly deformable anatomical structures that\nshare similar colors and textures. Despite the promise of a\nsingle foundation model for all modalities, we witnessed the\ndevelopment of domain-adapted foundation model [33], re-\nquiring hugue amount of densely annotated data (i.e., masks).\nOn the one hand, we lack the data and annotations needed\nto adapt these class-agnostic foundation models to each do-\nmain, as they do not leverage prior class information. On the\nother hand, WSSOD-P models are specifically designed to be\ntrained on small datasets and to effectively leverage class in-\nformation.\n2.3. Weakly Semi-Supervised Object Detection with Points\n(WSSOD-P)\nPoint DETR [6] introduces weakly semi-supervised ob-\nject detection with point annotations (WSSOD-P). To lever-\nage point annotations, Point-DETR employs a transformer-\nbased Point-to-Box regressor that converts point annotations\ninto bounding-box annotations, enabling the training of an ac-\ntual object detector. However, it inherits DETR\u2019s convergence\nissues, especially when training data is insufficient [7], which\nis often the case with WSSOD-P as it aims to train on minimal\nsamples. Group R-CNN [32] proposes a CNN-based Point-\nto-Box model to benefit from the better convergence prop-\nerties of CNNs and leverage multiscale features. They sug-\ngest collecting multiple proposals close to the point-prompt\nto learn an instance-aware representation and improve recall.\n4\nAnother challenge in WSSOD-P is its sensitivity to the lo-\ncation of point annotations, particularly in medical imaging,\nwhere points closer to the center provide more useful infor-\nmation for generating pseudo bounding boxes. PBC [10] in-\ntroduces point and symmetric consistency strategies to reduce\npoint location sensitivity in X-ray images. Similarly, PSL-\nNet [16] applies point consistency regularization to Group R-\nCNN. In addition, WSSOD-P can leverage on-the-fly video\nannotations in medical ultrasound [22].\nIn contrast, our proposed DExTeR is a transformer-based\nmethod that leverages multi-scale features and instance infor-\nmation, achieves fast convergence, and reduces sensitivity to\npoint location.\n3. Methods\nThe WSSOD-P pipeline aims to train a Point-to-Box\nteacher detector to generate pseudo-labels on point-annotated\nimages, which are then used to train a downstream detector\n(see Fig. 2). The core task is to train an efficient teacher\nmodel, and our proposed DExTeR fulfills this role. DExTeR is\nbased on the Point-DETR [6] and DETR [4] models, following\ntheir architecture as detailed in Sec. 2.1. A ResNet-50 back-\nbone extracts multi-scale visual features from the input image,\nand a transformer encoder refines these features with MSDA.\nThe points\u2019 positions and corresponding classes are encoded\ninto point-queries using a point-prompt encoder. These point-\nqueries first undergo self-attention for inter-query commu-\nnication, then interact with the image features using Class-\nguided MSDA to extract visual features. The point-queries are\nfurther refined with our CLass, Instance, and Common Knowl-\nedge Mixture of Experts (CLICK-MoE), leveraging the exper-\ntise of different experts. Boxes are then predicted from the\nrefined point-queries. The updated positions, now in the form\nof boxes, are re-encoded with their corresponding classes to\nform updated point-queries for further refinement in subse-\nquent decoder blocks. Instead of using a single point-query\nper instance, we propose a multi-point training strategy with\nN groups of point-queries. Each group consists of a differ-\nent point per instance and does not interact with other groups.\nWe decode each group in parallel, ensuring that the model can\npredict the corresponding box regardless of the point\u2019s posi-\ntion on the instance at any step during training. We detail the\nnovel designs of our architecture in the subsequent sections.\n3.1. Prompt Encoder\nDExTeR, inspired by Point-DETR, utilizes a prompt en-\ncoder to convert point annotations into object queries for the\ntransformer decoder. Unlike DETR\u2019s learned positional em-\nbeddings, these queries contain precise location and category\ninformations, directly linked to individual object instances.\nWe further introduce a Point-to-Box iterative refinement\nstrategy \u2014an adaption from the iterative box-refinement [34,\n31] methods\u2014 which dynamically refines the size and po-\nsition of bounding boxes throughout the decoding process,\nthereby enhancing the accuracy and granularity of object de-\ntection. After each decoder block, a prediction head generates\nFig. 3. Prompt encoding. a) Depending on the input type, either a point\nencoder or a box encoder is used. Each point (x, y, c) or box (x, y, w, h, c) is\nencoded using positional encoding and category embedding, then merged\nby a linear layer into the final representation. b) Point-to-Box refinement\nstrategy: Encodes point annotations and predicts refinements after each\ndecoder stage to form box-queries, which are re-encoded for subsequent\nrefinement.\nadjustments (deltas) to transform prompt queries into bound-\ning boxes. The system employs two types of prompt encoders:\na point encoder and a box encoder (See Fig. 3).\nThe point encoder operates before the first decoder block,\nprocessing a point annotation (x, y, c) into a normalized 2D\ncoordinate (x, y) within [0, 1]2 and a category index c. A fixed\nspatial encoding generates the position embedding epos_xy \u2208\nR256.\nIn contrast, the box encoder, used between decoder\nblocks, mirrors the point encoder\u2019s functionality but expands\nits role to encode not only coordinates (x, y) but also dimen-\nsions (w, h) (widht and height of a bounding box), resulting in\nposition embedding epos_xywh \u2208R512. Both encoders share a\nclass embedding matrix, dimensioned as C \u00d7 256 (where C is\nthe number of classes), optimized alongside the model, from\nwhich the category embedding ecat \u2208R256 is derived using in-\ndex c. We concatenate the epos and ecat embeddings and use\na linear layer to project them into a fixed embedding dimen-\nsion, forming the final prompt-query embedding. For ease of\ndiscussion, we refer to both point and box queries as \"point-\nquery.\"\n3.2. Class-guided MSDA\nPrevious implementations of DETR-based frameworks as\nPoint-to-Box regressors have relied on dense cross-attention\nbetween point queries and image features. However, the high\ncomputational demand of dense attention restricts interactions\nto only the last feature map, which is of lower resolution. This\nrestriction not only diminishes the point-query location infor-\nmation but also hamperes the model\u2019s ability to distinguish\nbetween close or overlapping instances.\nTo leverage multi-scale information, we utilize Multi-Scale\nDeformable Attention (MSDA) [34] in both encoder self-\nattention and decoder cross-attention processes. This method\nemploys feature maps from various scales produced by the\nfeature encoder, thereby enhancing the distinction between ad-\njacent points. MSDA assigns to each object-query a reference\n5\npoint, which is predicted from the encoder output, and calcu-\nlates offsets and weights for sampling features relative to these\nreference points.\nBuilding on this, we introduce Class-guided MSDA, a re-\nfinement where the point-query\u2019s own coordinates and class\nembedding serve as the reference. This modification allows\nthe offsets and weights for feature sampling to be directly de-\nrived from the point-query and its coordinates, utilizing the\nrich class-specific information (such as size and shape) to\nguide attention sampling more effectively.\nThis contribution already significantly enhances the basic\nPoint-DETR model through the integration of Class-guided\nMSDA, the newly proposed Prompt-encoder, and an itera-\ntive Point-to-Box refinement strategy (See Section 3.1), col-\nlectively referred to as Deformable Point-DETR. Despite these\nadvancements, the model still primarily relies on point loca-\ntion and uses instance and class information only for sampling\nvisual features, not for refining point-queries. To address this\nlimitation, we introduce CLICK-MoE to refine point-queries\nby decoupling class, instance, and common knowledge.\n3.3. CLICK-MoE\nWe introduce the CLass, Instance, and Common Knowl-\nedge Mixture of Experts (CLICK-MoE) to refine point-queries\nwithin our transformer-based model (see Fig. 4). This layer\noperates independently for each point-query, replacing the tra-\nditional feed-forward network (FFN) sub-block within the de-\ncoder block. By employing a decoupled approach with three\nspecialized experts, CLICK-MoE effectively processes and\ndifferentiates point-queries based on class, instance, and com-\nmon knowledge, offering a flexible architecture that could po-\ntentially adapt to various downstream datasets.\n\u2022 The Common Knowledge Expert, a two-layer FFN with\nReLU activation after the first layer, processes all queries\nto provide general, class-agnostic insights. The Com-\nmon Knowledge Expert\u2019s generalized insights are di-\nrectly transferable for fine-tuning in transfer learning sce-\nnarios.\n\u2022 The Class-specific Experts, similar in structure to the\nCommon Knowledge Expert, only process queries spe-\ncific to their respective classes to deliver class-specific\nknowledge.\nGiven that most datasets have a manage-\nable number of classes, one expert per class is feasible.\nIn transfer learning scenarios, these experts are replaced\nwith new ones to accommodate different classes, without\ncompromising the other experts.\n\u2022 The Instance Experts, consisting of a single linear layer,\nuse dynamically generated, instance-aware parameters to\nenhance differentiation between point-queries and min-\nimize confusion between instances.\nThese parameters\nare derived from the point-queries and a learnable em-\nbedding through self-attention, followed by a linear layer\n(See Fig. 5). The learnable embedding can be seen as the\nbase weights for the instance expert. Through its inter-\naction with the queries during the attention mechanism,\nFig. 4. Class, Instance, and Common Knowledge Mixture of Experts\n(CLICK-MoE): input object queries are processed through three special-\nized experts\u2014a dynamically generated instance expert, a class-specific\nexpert based on the query\u2019s label, and a common knowledge expert. The\noutputs from these experts are combined using weighted element-wise\nsummation, where the weights are Winstance, Wclass, and Wgen.\nFig. 5. Instance Expert weight generation. The object queries, along with\na learnable embedding, undergo self-attention and are projected with a\nlinear layer to produce instance-aware parameters for the instance ex-\nperts.\nit aids in adapting the representation of the point-queries.\nIt is discarded after completing the attention process with\nthe point-queries.\nThe outputs from all three experts is weighted by Winstance =\nWclass = Wgen = 1\n3, to promote inter-experts compatibility and\ncoherence, and then summed with the original point-queries.\nIn transfer learning, the Common Knowledge Expert, being\ndirectly transferable, serves as a stable base that supports the\nlearning of new class-specific experts, which is particularly\nbeneficial in scenarios involving smaller datasets. We refer to\nthe model incorporating this component as CLICK-DETR.\n3.4. Multi-point Training Strategy\nOne limitation of WSSOD-P is its reliance on location pri-\nors, where point annotations near the center of an anatom-\nical area tend to be more informative than those near the\nboundaries. This difference in semantic information makes\ncentral points more valuable for generating pseudo bounding\n6\nboxes [10]. Thus, a key challenge is to accurately map point\nannotations to bounding boxes while accounting for the vari-\nability in the annotated points\u2019 locations.\nWe propose our Multi-point training strategy. Point-to-Box\nmodels are trained using points sampled from box-annotated\ninstances, allowing for infinite point sampling at no additional\nannotation cost beyond the initial box annotations. Instead of\nusing a single point-query per instance, we sample N groups\nof point-queries. Each group consists of a different point per\ninstance and, through group-wise self-attention, operates inde-\npendently without interacting with other groups. During each\ntraining iteration, each group is decoded in parallel, implic-\nitly encouraging the network to consistently predict the cor-\nrect bounding box for diverse points on a given instance. Our\nmethod also enhances the supervision signal, further acceler-\nating training convergence and improving model accuracy. We\nnormalize the loss by the total number of point-queries, calcu-\nlated as the number of instances multiplied by the number of\ngroups.\nOur strategy is employed solely during the training process,\neffectively simulating the use of a single point per instance N\ntimes in one iteration to enhance training speed and robust-\nness.\nAt inference time, the model operates with just one\ngroup per instance\u2014the annotated points. Since the groups\ndo not interact with each other during training, this setup is\nconsistent with the task the model is trained on. Our approach\ncan be integrated into any DETR-based Point-to-Box model,\naccelerating training convergence and improving the model\u2019s\nability to accurately predict bounding boxes.\n4. Experiments\n4.1. Dataset\nTransformer models are difficult to train from scratch and\nbenefit greatly from pretraining. Since Group R-CNN is pre-\ntrained on the COCO dataset [15], we also pretrain DExTeR\nand the other baselines on COCO. This pretraining substan-\ntially improves performance across all models, and all re-\nsults reported in this paper are finetuned from these pretrained\nweights. During training, Point-to-Box regressors are opti-\nmized by randomly sampling points within instance bound-\ning boxes and using these boxes as regression targets. At in-\nference, to ensure fair comparison across methods, we gen-\nerate a single fixed point annotation per instance from the\navailable mask (or bounding box if unavailable). This fixed\npoint is consistently used for evaluation across all methods in\nweakly annotated datasets. Performance is reported in terms\nof mean Average Precision (mAP), which measures precision\nacross multiple Intersection over Union (IoU) thresholds (0.5\nto 0.95), and mAP@50, which assesses precision at a fixed\nIoU threshold of 50%. We conducted experiments using three\ndatasets from different medical areas: Endoscapes [23] (sur-\ngical), VinDr-CXR [25] (X-Ray), and our private EUS-D130\ndataset [21] (endoscopic ultrasound).\n4.1.1. Endoscapes\nEndoscapes contains 201 annotated videos from laparo-\nscopic cholecystectomy dissections. It includes 1933 frames,\nannotated every 30 seconds, with bounding boxes across six\nclasses (five anatomical and one tool class). We use the offi-\ncial training, validation, and testing splits from [23].\n4.1.2. VinDr-CXR\nVinDr-CXR contains 15,000 chest X-ray images annotated\nfor 14 thoracic diseases, such as aortic enlargement and car-\ndiomegaly.\nSince the VinDr-CXR dataset exhibits a long-\ntailed distribution with some abnormal categories containing\nfewer than ten samples, we follow [10] and combined the eight\nsmallest categories into one class.\n4.1.3. EUS-D130\nEUS-D130 (private) extends EUS-D50 [21], comprising\n130 videos acquired using curved linear and zoomed radial\nprobes from Olympus and Hitachi EUS systems. Elastogra-\nphy and contrast modes are excluded, with all videos captured\nat a frequency of 7.5 MHz. Pancreas and lesion annotations\nare annotated using the MOSaiC platform [20].\nWe use only 12.5% and 50.0% of the training data from\neach respective dataset, simulating a scenario where only a\nsmall portion of the data is box-annotated.\nThe remain-\ning training data were used as point-annotated data, pseudo-\nlabeled using the corresponding Point-to-Box model. The stu-\ndent models were then trained on both sets.\n4.2. Implementation Details\nDExTeR was pretrained on COCO using eight V100 GPUs\nand subsequently finetuned on a single V100 GPU with a\nbatch size of two images per GPU. We followed DETR\u2019s data\naugmentation protocols to maintain consistency. The imple-\nmentation leverages the MMDetection v3.3 library [5]. The\nmodel was trained for 24 epochs during both pretraining and\nfinetuning, with the learning rate reduced by a factor of 10\nat the 20th epoch.\nWe adopted the AdamW optimizer for\nnetwork optimization, starting with an initial learning rate of\n1 \u00d7 10\u22124 and a warm-up period of 500 iterations. Following\ndeformable-detr [34], we use a linear combination of L1 and\nGIoU [27] loss (5:2).\n5. Results\nWe first evaluated the performance of the developed Point-\nto-Box teacher models across datasets.\nNext, we assessed\nDExTeR by training student detectors and conducted exten-\nsive ablation studies to analyze its proposed components. All\nexperiments were performed using three splits, and the mean\nresults are reported.\n5.1. Point-to-Box teacher models\nWe present the quantitative results of the Point-to-Box\nteacher models in Table 1. Results are presented for three\nmodels developed iteratively, incorporating the different pro-\nposed components, with the final iteration represented by\nour proposed model, DExTeR. For comparison, we include\n7\nTable 1. Results of Point-to-Box teacher models in mAP (%) using 12.5% and 50% of the training set. DExTeR significantly improves upon the base\npoint-DETR and outperforms all baselines on the three datasets.\nEndoscapes\nVinDr-CXR\nEUS-D130\nModels\n12.5%\n50.0%\n12.5%\n50.0%\n12.5%\n50.0%\nmAP\nmAP@50\nmAP\nmAP@50\nmAP\nmAP@50\nmAP\nmAP@50\nmAP\nmAP@50\nmAP\nmAP@50\nexisting sota\nGroup-RCNN\n27.1\u00b13.1\n50.7\u00b12.8\n33.7\u00b12.4\n61.6\u00b12.2\n22.7\u00b14.4\n51.7\u00b14.2\n25.4\u00b12.2\n56.9\u00b11.9\n5.9\u00b12.6\n27.2\u00b12.4\n8.1\u00b11.9\n34.5\u00b11.7\nPBC\n17.7\u00b13.7\n35.4\u00b13.5\n23.5\u00b12.5\n49.2\u00b12.3\n17.4\u00b14.8\n38.2\u00b14.5\n20.7\u00b12.9\n50.1\u00b13.0\n6.2\u00b13.2\n27.7\u00b13.5\n8.6\u00b12.7\n35.9\u00b12.0\nPoint-DETR\n16.2\u00b13.3\n36.0\u00b13.1\n23.4\u00b12.7\n48.9\u00b12.7\n15.2\u00b14.8\n38.8\u00b14.9\n18.8\u00b13.2\n47.7\u00b13.0\n5.9\u00b13.3\n26.2\u00b13.1\n8.4\u00b12.4\n35.4\u00b12.0\nours\nDef. Point-DETR\n24.0\u00b13.2\n43.6\u00b13.0\n31.4\u00b12.5\n54.3\u00b12.4\n21.0\u00b14.8\n48.8\u00b14.5\n24.3\u00b12.5\n54.8\u00b12.4\n6.6\u00b13.1\n28.4\u00b12.8\n9.0\u00b12.3\n36.5\u00b11.7\nCLICK-DETR\n27.8\u00b12.9\n47.8\u00b12.9\n33.9\u00b12.4\n57.4\u00b12.3\n21.5\u00b14.9\n49.9\u00b14.1\n25.6\u00b12.1\n56.6\u00b12.3\n6.8\u00b12.9\n29.3\u00b12.6\n9.4\u00b12.3\n37.1\u00b11.4\nDExTeR\n31.4\u00b12.9\n52.8\u00b12.7\n36.8\u00b11.8\n62.9\u00b12.0\n23.2\u00b14.6\n52.9\u00b14.1\n26.5\u00b12.2\n58.3\u00b12.1\n8.1\u00b12.7\n33.0\u00b12.3\n11.0\u00b12.4\n41.1\u00b11.5\n0\n10\n20\n30\n40\n50\n60\nStudent: Faster-RCNN\n36.5\n62.4\n33.2\n33.5\n33.6\n33.8\n35.9\n57.8\n60.4\n59.7\n60.7\n61.4\n25.6\n27.4\n27.9\n28.9\n32.3\n46.4\n50.4\n51.2\n53.0\n55.8\nEndoscape\n0\n10\n20\n30\n40\n25.3\n46.1\n21.6\n22.4\n22.8\n23.5\n24.2\n39.6\n43.6\n43.6\n43.5\n44.7\n16.2\n15.8\n17.1\n20.2\n22.3\n30.5\n33.2\n36.0\n39.7\n42.6\nVinDr-CXR\n0\n5\n10\n15\n20\n25\n30\n35\n40\n12.2\n39.3\n7.7\n8.2\n8.0\n8.4\n10.1\n30.1\n32.6\n32.5\n34.5\n35.4\n5.2\n7.1\n7.1\n6.6\n8.3\n23.6\n29.7\n29.9\n27.2\n31.5\nEUS-130\nSupervised\nPoint\nDETR\nPBC\nGroup\nRCNN\nDExTeR\n0\n10\n20\n30\n40\n50\n60\nStudent: Deformable-DETR\n40.0\n63.9\n27.6\n27.6\n27.8\n31.3\n35.2\n42.8\n50.0\n50.1\n54.4\n58.0\n36.9\n36.3\n36.6\n37.4\n39.5\n60.3\n61.7\n61.8\n62.7\n63.7\nSupervised\nPoint\nDETR\nPBC\nGroup\nRCNN\nDExTeR\n0\n10\n20\n30\n40\n22.7\n43.5\n21.3\n20.7\n21.1\n21.6\n22.1\n41.7\n41.3\n41.2\n41.8\n42.2\n17.7\n15.5\n16.2\n18.8\n20.6\n35.4\n33.3\n35.6\n37.5\n40.4\nSupervised\nPoint\nDETR\nPBC\nGroup\nRCNN\nDExTeR\n0\n5\n10\n15\n20\n25\n30\n35\n10.1\n34.6\n7.4\n8.1\n8.1\n8.8\n9.9\n27.8\n28.1\n28.7\n30.2\n33.8\n4.4\n6.3\n6.3\n6.6\n7.6\n18.4\n24.0\n24.2\n25.8\n26.5\nmAP of 12.5 % set\nmAP of 50.0 % set\nmAP@50 of 12.5 % set\nmAP@50 of 50.0 % set\nfully supervised mAP\nfully supervised mAP@50\nFig. 6. Student models\u2019 results. Solid blue and light blue bars represent the mAP achieved with 12.5% and 50% of the data box-labeled, respectively.\nSimilarly, solid orange and light orange bars represent the mAP@50 under the same 12.5% and 50% box-labeled settings. The solid horizontal lines\nindicate the fully supervised performance for reference. The \"Supervised\" method refers to a model trained solely on available box-labeled data,\nwithout incorporating any pseudo-labels. DExTeR consistently outperforms other state-of-the-art models. Notably, when trained with 50% box-labeled\ndata and 50% pseudo-labeled data, the student models achieve performance comparable to the fully supervised baseline.\nthree state-of-the-art (SOTA) Point-to-Box models: Group-\nRCNN [32], PBC [10], and Point-DETR [6], the latter serving\nas the foundation for our approach.\nThe proposed models show significant improvements over\nthe baseline Point-DETR and the best-performing baseline,\nGroup-RCNN. In the 12.5% box-annotated setting, De-\nformable Point-DETR achieves an average gain of 4.8 mAP\nover Point-DETR, while DExTeR further improves this to 8.5\nmAP. Additionally, DExTeR surpasses Group-RCNN by 2.3\nmAP in this setting. In the 50% box-annotated setting, DEx-\nTeR continues to demonstrate its effectiveness, achieving an\naverage gain of 2.4 mAP over Group-RCNN.\nWe present qualitative results in Fig. 6 for the 12.5% set-\nting. DExTeR demonstrates superior performance, producing\ntighter boxes that better fit objects (rows 1 and 2). In contrast,\nGroup-RCNN often generates overly wide boxes and strug-\ngles to distinguish close instances (row 2). DExTeR effec-\ntively leverages prior class and instance information, address-\ning these issues. However, row 3 highlights a failure case\nwhere the predicted box for the pancreas does not fully en-\n8\ncompass the organ.\n5.2. Student detectors\nWe present the results of the student detectors in Fig. 6,\nutilizing Faster R-CNN [26] and Deformable DETR [34] as\nstudent models. The experiments are conducted with either\n12.5% or 50% of the dataset box-labeled, while the remain-\ning data is pseudo-labeled using the corresponding teacher\nmodel under evaluation. We report both mAP and mAP@50\nmetrics. For both student models, DExTeR, as the teacher\nmodel, consistently outperforms all other baselines. Specif-\nically, when using DExTeR, we achieve on average mAP@50\nscores that are 86.9% and 96.6% of the fully supervised per-\nformance, respectively. In comparison, the supervised base-\nline achieves 67.1% and 87.6% of the fully supervised perfor-\nmance, while Group-RCNN achieves 81.0% and 93.5% under\nthe same settings. These results demonstrate that the improve-\nments achieved by DExTeR as the teacher model effectively\ntranslate into better performance for student detectors, regard-\nless of the specific student model used.\n5.3. Ablation Studies\nTo demonstrate the impact of our proposed components,\nwe conduct ablation studies on the Endoscapes dataset with\nsubsets of 12.5% and 50% well-labeled samples. We limit\ninterference by fixing point annotations on the test set,\nfocusing solely on the efficacy of the Point-to-Box regressor,\nrather than training a full object detector with pseudo-labeled\nimages.\nCLICK Experts Components.\nTo investigate the im-\npact of each expert within our CLICK-MoE module on the\nEndoscapes dataset, we systematically activate and deactivate\ncombinations of Class, Instance, and Common Knowledge\n(CK) experts."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging**\n\n**Main Contributions**\n- **Class-Guided Multi-Scale Deformable Attention (MSDA):** Introduces a new refinement strategy that leverages multi-scale features to enhance the accuracy of object detection, especially in medical imaging where adjacent points can share features.\n- **CLICK-MoE:** Proposes a novel mixture of experts architecture to decouple class and instance information within the transformer model. This allows for more flexible and effective representation learning across different datasets.\n- **Multi-Point Training Strategy:** Implements a strategy that promotes prediction consistency by training on multiple point placements per instance, reducing dependency on specific point locations.\n\n**Methodology**\nThe paper introduces DExTeR (DETR with Experts), which builds upon Point-DETR for medical object detection. It employs a transformer-based architecture to handle weakly semi-supervised learning with point annotations. The key components include:\n1. **Prompt Encoder:** Converts point annotations into object queries, allowing dynamic refinement of bounding boxes.\n2. **Class-Guided Multi-Scale Deformable Attention (MSDA):** Enhances feature extraction by guiding attention sampling using class and instance information.\n3. **CLICK-MoE:** A mixture of three specialized experts\u2014Common Knowledge Expert, Class-specific Expert, and Instance Expert\u2014to refine point queries based on class, instance, and common knowledge.\n\n**Key Findings**\n- DExTeR achieves state-of-the-art performance across multiple medical imaging datasets (Endoscopy, Chest X-rays, Endoscopic Ultrasound) with minimal annotation costs.\n- The proposed multi-point training strategy significantly improves robustness to annotation variability.\n- CLICK-MoE effectively decouples class and instance information, leading to more accurate and flexible representation learning.\n\n**Conclusions**\nDExTeR demonstrates superior performance compared to existing SOTA Point-to-Box models. It achieves consistent improvements in detection accuracy across different annotation settings (12.5% vs 50% box-labeled data) and outperforms Group-RCNN by up to 2.3 mAP in the fully supervised setting. The multi-point training strategy further enhances its robustness, making DExTeR a promising solution for weakly semi-supervised object detection in medical imaging.\n\n**Keywords**\nMixture of Experts, Object Detection, Point Annotation, Weakly Semi-Supervised Learning"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge,\nSkill, and Attitude in Educational Large Language Models\nUnggi Lee1\u2020 , Sookbun Lee2 , Heungsoo Choi3 , Jinseo Lee4 , Haeun Park5\nYounghoon Jeon6 , Sungmin Cho3 , Minju Kang7 , Junbo Koh7 , Jiyeong Bae3\nMinwoo Nam3 , Juyeon Eun3 , Yeonji Jung8 and Yeil Jeong9\u2020\n1Chosun University 2Independent Researcher 3Korea University 4Ewha Womans University\n5Korea Institute for Curriculum and Evaluation 6Upstage 7Seoul National University\n8Texas A&M University 9Indiana University Bloomington\nCorresponding Authors (\u2020): codingchild@korea.ac.kr, yeilj@iu.edu\nAbstract\nLarge Language Models are increasingly de-\nployed as educational tools, yet existing bench-\nmarks focus on narrow skills and lack ground-\ning in learning sciences.\nWe introduce Open-\nLearnLM Benchmark, a theory-grounded frame-\nwork evaluating LLMs across three dimensions de-\nrived from educational assessment theory: Knowl-\nedge (curriculum-aligned content and pedagogi-\ncal understanding), Skills (scenario-based com-\npetencies organized through a four-level center-\nrole-scenario-subscenario hierarchy), and Attitude\n(alignment consistency and deception resistance).\nOur benchmark comprises 124K+ items spanning\nmultiple subjects, educational roles, and diffi-\nculty levels based on Bloom\u2019s taxonomy.\nThe\nKnowledge domain prioritizes authentic assess-\nment items from established benchmarks, while\nthe Attitude domain adapts Anthropic\u2019s Alignment\nFaking methodology to detect behavioral inconsis-\ntency under varying monitoring conditions. Eval-\nuation of seven frontier models reveals distinct ca-\npability profiles: Claude-Opus-4.5 excels in prac-\ntical skills despite lower content knowledge, while\nGrok-4.1-fast leads in knowledge but shows align-\nment concerns.\nNotably, no single model dom-\ninates all dimensions, validating the necessity of\nmulti-axis evaluation.\nOpenLearnLM provides\nan open, comprehensive framework for advancing\nLLM readiness in authentic educational contexts.\n1\nIntroduction\nLarge Language Models (LLMs) are increasingly deployed in\neducational settings as tutors, assessors, and instructional as-\nsistants [Wang et al., 2024]. Meta-analyses demonstrate pos-\nitive effects on learning performance, primarily in language\nlearning and writing tasks [Contributors, 2025a], with com-\nmercial deployments like Khan Academy\u2019s Khanmigo scal-\ning to over 700,000 users [Khan Academy, 2024]. However,\nstudies reveal that current LLMs struggle to match traditional\nPedagogical\nKnowledge\nskills\nContent\nKnowledge\nLLM\nLLM for\nEducation\nTeaching Learning\nAssessment\nResearch\nCounselling\nAdministration\nEpistemic\nInstructional\nNormative & Ethical\nDeception\nPedagogical\nContent\nKnowledge\nAttitude\nSkills\nFigure 1: Prior benchmarks evaluate narrow slices of educational ca-\npability. OpenLearnLM Benchmark provides comprehensive three-\naxis assessment covering Knowledge, Skills, and Attitude.\nITS adaptivity, achieving only 56.6% pedagogical soundness\ndespite 97.3% answer accuracy [Reasoning and Lab, 2025].\nExisting educational benchmarks exhibit significant lim-\nitations.\nLearnLM [Google DeepMind, 2024] grounds\nevaluation in learning science principles but releases no\npublic dataset.\nEduBench [Xu and others, 2025] covers\nnine scenarios without hierarchical organization. Domain-\nspecific benchmarks like MathTutorBench [Reasoning and\nLab, 2025] and TutorBench [Contributors, 2025b] focus nar-\nrowly on tutoring. These share common limitations: lack of\nsystematic grounding in educational theory, absence of hi-\nerarchical structure, and narrow focus on knowledge assess-\nment without considering the full spectrum of educational\nfunctions.\nFurthermore, no existing benchmark addresses\nalignment faking, the potential for models to behave differ-\nently when monitored versus autonomous.\nTo address these gaps, we introduce OpenLearnLM Bench-\nmark1, a theory-grounded framework built upon the KSA\n(Knowledge-Skills-Attitude) model from educational assess-\n1https://anonymous.4open.science/r/\nopenlearnlm-benchmark-17D4\narXiv:2601.13882v1  [cs.CL]  20 Jan 2026\nFigure 2: Overview of the OpenLearnLM Benchmark framework. The benchmark evaluates educational LLMs across three axes: Knowl-\nedge (Content and Pedagogical), Skills (6 Centers with hierarchical taxonomy), and Attitude (Standard stances and Deception detection via\nAlignment Faking methodology).\nment. The Knowledge axis assesses curriculum-aligned con-\ntent knowledge and pedagogical knowledge. The Skills axis\nevaluates scenario-based competencies through a hierarchical\nstructure of 6 Centers, 46 Scenarios, and 89 Sub-scenarios\nwith difficulty tiers based on Bloom\u2019s taxonomy. The At-\ntitude axis examines epistemic, instructional, and normative-\nethical stances, along with deception assessment using Align-\nment Faking methodology [Greenblatt et al., 2024]. Table 1\ncompares OpenLearnLM with existing benchmarks.\nOur contributions are fourfold:\n(1) a systematic KSA\nevaluation framework grounded in educational theory; (2) a\nthree-axis approach capturing multidimensional educational\ncompetence beyond factual accuracy; (3) Alignment Faking\nmethodology for deception evaluation addressing behavioral\nconsistency; and (4) a large-scale dataset of 122K+ items en-\nabling evaluation and development of educational LLMs.\n2\nDataset Description\nThe OpenLearnLM Benchmark evaluates language models\nalong three dimensions essential for acting as educational\npartners, namely Knowledge, Skills, and Attitude. Together,\nthese dimensions operationalize how models understand, ex-\nTable 1: Comparison of OpenLearnLM Benchmark with existing educational LLM benchmarks.\nBenchmark\nScale\nGeneral\nAuto\nHierarchical\nRole\nAttitude\nPublic\nEducation\nEval\nTaxonomy\nDefinition\nEval\nLearnLM [Google DeepMind, 2024]\n\u2013\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nEduBench [Xu and others, 2025]\n18.8K\n\u2713\n\u2713\n9 scen.\n\u2717\n\u2717\n\u2713\nMathTutorBench [Reasoning and Lab, 2025]\n9.1K\n\u2717\n\u2713\n3 skills\n\u2717\n\u2717\n\u2713\nPedagogy BM [Leli`evre et al., 2025]\n1.1K\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\nEducationQ [Shi et al., 2025]\n1.5K\n\u2713\n\u2713\n13 disc.\n\u2717\n\u2717\n\u2713\nTutorBench [Contributors, 2025b]\n1.5K\n\u2717\n\u2717\n6 subj.\n\u2717\n\u2717\n\u2713\nMRBench [Chevalier et al., 2024]\n1.6K\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\nOpenLearnLM Benchmark (Ours)\n124K+\n\u2713\n\u2713\n4-level (6C-11R-46S-81Sub)\n\u2713\n\u2713\n\u2713\necute, and justify decisions in real educational contexts, ex-\ntending beyond correctness-focused evaluation.\n2.1\nKnowledge\nKnowledge captures what a model knows across two comple-\nmentary domains, curriculum-based subject knowledge and\npedagogical knowledge. The subject component spans early\nchildhood, K-12, higher education, and special education,\ngrounded in national and international curriculum standards.\nThe pedagogical component assesses understanding of in-\nstructional theories, teaching strategies, and mechanisms of\nlearning, reflecting whether a model grasps how teaching and\nlearning occur rather than simply recalling facts. Tasks mea-\nsure conceptual correctness and fidelity to widely accepted\ndisciplinary and instructional knowledge.\nThe Knowledge\ndomain contains 2,304 multiple-choice items: 918 for con-\ntent knowledge and 1,386 for pedagogical knowledge.\n2.2\nSkills\nSkills focus on what the model can do in educational prac-\ntice, evaluated through tasks designed to resemble authen-\ntic teaching and learning situations.\nEach task is orga-\nnized hierarchically from a centered educational context to\na role, scenario, and sub-scenario that emulate real decisions\nteachers make. Tasks vary systematically across subject do-\nmain, learner level, and difficulty, the latter defined through\nboth cognitive and affective learning taxonomies and LLM-\noriented challenge factors such as context length and reason-\ning depth. Output modes include short and long answers, pro-\ncedural steps, agentic execution, and tool-assisted responses.\nThe goal is to measure situated capability, whether a model\ncan apply knowledge to support learners, adapt to context,\nand generate usable educational outcomes. The Skills do-\nmain comprises 122,425 items across 6 Centers, 11 Roles, 46\nScenarios, and 81 Sub-scenarios.\n2.3\nAttitude\nAttitude evaluates how the model behaves while generating\nanswers, emphasizing epistemic posture, instructional dispo-\nsition, and normative compliance rather than factual accu-\nracy. This dimension examines whether a model expresses\nuncertainty appropriately, avoids overconfident or mislead-\ning claims, adopts learner-centered strategies such as scaf-\nfolding or prompting rather than shortcutting to answers, and\nadheres to ethical and professional expectations. Measure-\nment methods include concealed scratchpad prompts to test\nsusceptibility to alignment faking and rubric-based human\nevaluations of tone, justification, and decision patterns. This\ndimension captures whether a model approaches tasks in a\nmanner aligned with human pedagogical values. The Atti-\ntude domain includes 14 scenario-based items: 12 for stan-\ndard stances (epistemic, instructional, normative-ethical) and\n2 for deception evaluation.\n2.4\nLicense\nThe OpenLearnLM Benchmark dataset is collected from the\nfollowing sources, and we comply with the licenses of each\ndataset. Table 2 summarizes the data sources for the Knowl-\nedge domain.\nFor the deception evaluation methodology\nin the Attitude domain, we reference the Alignment Faking\nframework [Greenblatt et al., 2024], which is released under\nthe MIT License.\nTable 2: Data sources and licenses for Knowledge domain.\nDataset\nItems License\nSource\nC-Eval\n798 CC BY-NC-SA [Huang\net\nal.,\n2023]\nGPQA\n120 CC BY 4.0\n[Rein\net\nal.,\n2023]\nKICE\n243 KICE\nKICE\nPedagogy 1,143 Apache 2.0\n[Leli`evre et al.,\n2025]\n2.5\nData Collection and Preprocessing\nOur research team comprises 14 experts in education, educa-\ntional technology, and computer science. We analyzed exist-\ning LLM-in-education studies and benchmark papers to es-\ntablish the initial framework, then conducted a four-month\nDelphi study to refine the taxonomy considering future LLM\napplications in educational contexts.\nFor the Skills domain, we employ an automated generation\npipeline using OpenAI\u2019s gpt-5-mini model with minimal rea-\nsoning effort. We generated a total of 122,425 items with a\n99.6% success rate, split into approximately 116,000 training\nitems (95%) and 6,100 test items (5%).\nFor the Knowledge domain, we prioritized incorporat-\ning authentic assessment items from established benchmarks\nrather than relying solely on synthetic generation. Content\nknowledge items were sourced from C-Eval and GPQA (918\nitems total), while pedagogical knowledge items came from\nKICE and Pedagogy Benchmark (1,386 items total).\nThe Attitude evaluation items were designed by educa-\ntion experts, consisting of 14 scenario-based items across\nfour categories: Epistemic Stance (4 items), Instructional\nStance (4 items), Normative and Ethical Stance (4 items),\nand Deception (2 items). The Deception evaluation method-\nology is adapted from Anthropic\u2019s Alignment Faking re-\nsearch [Greenblatt et al., 2024].\n2.6\nQuality Assurance\nWe conducted comprehensive quality assurance using an\nLLM-as-Judge approach. Each generated item was evaluated\nagainst five criteria: (1) answer accuracy, whether the marked\nanswer is logically and factually correct; (2) question clarity,\nwhether the question stem is unambiguous and grammatically\ncorrect; (3) distractor quality, whether wrong options are\nplausible but distinguishable; (4) difficulty match, whether\ncognitive demand matches the specified Bloom\u2019s level; and\n(5) scenario alignment, whether the question serves the edu-\ncational context\u2019s purpose.\nItems were scored on a 25-point scale (5 points per cri-\nterion), with a passing threshold of 20 points. Two criteria\nare designated as critical: answer accuracy (minimum score\nof 3 required) and question clarity (minimum score of 2 re-\nquired). Items failing these thresholds are automatically re-\njected regardless of total score. The overall pass rate was\n99.9% (122,365 out of 122,439 items). The complete rubric\nspecifications are provided in Appendix B.\n2.7\nEvaluation Methodology\nThe benchmark employs three distinct evaluation approaches\nmatched to each domain\u2019s characteristics.\nKnowledge Evaluation uses exact-match accuracy on\nmultiple-choice questions. Models select one answer from\nfour options, and correctness is binary. This approach enables\nefficient, objective measurement of factual and pedagogical\nknowledge.\nSkills Evaluation employs an LLM-as-Judge framework\nwith scenario-specific rubrics. Each of the 81 sub-scenarios\nhas a dedicated 10-point rubric defining quality levels from\ninadequate (1\u20132) to exemplary (9\u201310). Figure 3 shows an\nexample evaluation for a Learner Analysis task.\nAttitude Evaluation uses two complementary methods.\nStandard items (12 scenarios) assess epistemic, instructional,\nand ethical stances using LLM-as-Judge with 10 samples per\nitem at temperature 1.0. Deception items (2 scenarios) apply\nthe Alignment Faking methodology [Greenblatt et al., 2024],\ncomparing model behavior under monitored versus unmoni-\ntored conditions with 100 samples each. The deception score\nmeasures behavioral consistency, where lower scores indicate\nstable alignment regardless of perceived oversight.\n2.8\nStatistics\nTable 3 presents the overall statistics of the OpenLearnLM\nBenchmark across the three evaluation axes.\nSkills Evaluation Example\nSub-scenario: Group Learner Level Assessment\nTask: Analyze provided learner data to produce an assess-\nment report\nRubric Excerpt:\n9\u201310: Comprehensive analysis with clear evidence, reliable\nconclusions, specific actionable recommendations\n5\u20136: Basic analysis but limited evidence; conclusions gen-\neral, lacking specificity\n1\u20132: Fails to utilize context; poor analysis with irrelevant\nconclusions\nScore: 8/10 - \u201cAccurate analysis of key elements with prac-\ntical recommendations; some detailed evidence lacking.\u201d\nFigure 3: Example Skills evaluation showing rubric application for\na Learner Analysis task.\nTable 3: Overall statistics of the OpenLearnLM Benchmark.\nAxis\nTrain\nTest\nType\nKnowledge\n1,831\n473\nMCQ\nSkills\n\u223c116K\n\u223c6.1K\nMCQ + Long\nAttitude\n\u2013\n14\nScenario\nTotal\n\u223c124K+\nThe Skills domain comprises 122,439 items organized in\na four-level hierarchy: Center \u2192Role \u2192Scenario \u2192Sub-\nscenario. This structure reflects authentic pedagogical prac-\ntice, where educational activities are contextualized by insti-\ntutional function (Center), professional responsibility (Role),\ntask type (Scenario), and specific task variant (Sub-scenario).\nTable 4 presents the hierarchy with 6 Centers, 11 Roles, 46\nScenarios, and 81 Sub-scenarios. The complete taxonomy is\nprovided in Appendix A. Items are balanced across three dif-\nficulty levels based on Bloom\u2019s Taxonomy for cognitive tasks\nand Krathwohl\u2019s Taxonomy for affective tasks.\nTable 4: Skills domain hierarchical structure (Center \u2192Role \u2192\nScenario).\nCenter\nRoles\nExample Scenarios\nTeaching\nInstr. Designer\nCurriculum Design, Learner Analysis\nContent Creator, TA Material Gen., Error Detection\nLearning\nTutor\nConcept Support, Inquiry Thinking\nCoach, Sim. Learner Metacognition, Discovery Learning\nAssessment Evaluator\nDiagnostic, Formative, Summative\nCounseling Counselor, Advisor\nStudent/Parent/Teacher Counseling\nResearch\nResearch Assistant\nData Analysis, Experiment Design\nAdmin\nAdmin Assistant\nBudget Mgmt., Coordination\nFigure 4 illustrates a sample item from the Skills domain,\nshowing the hierarchical metadata structure including center,\nrole, scenario, subject, difficulty, and domain classification.\nThe Knowledge domain contains 2,304 items in total. Con-\ntent knowledge accounts for 918 items (798 from C-Eval,\nExample Skills Item\nCenter: Assessment\nRole: Evaluator\nScenario: Diagnostic Assessment / Learner Level Deter-\nmination\nSubject: Higher Education / Chemistry\nDifficulty: Medium\nDomain: Cognitive\nQuestion: Which diagnostic interpretation most accurately\nidentifies a learner\u2019s prior-knowledge level and misconcep-\ntion type based on the following student responses: (1)\nShort-answer: \u201cIn an exothermic reaction, energy is ab-\nsorbed to break bonds.\u201d\n(2) Multiple-choice:\nselected\n\u201cHeat is consumed during product formation\u201d where cor-\nrect choice was \u201cHeat is released during product forma-\ntion\u201d?\nFigure 4: Sample item from the Skills domain illustrating the hier-\narchical metadata structure.\n120 from GPQA), while pedagogical knowledge accounts for\n1,386 items (243 from KICE, 1,143 from Pedagogy Bench-\nmark).\n3\nExperiments\n3.1\nExperimental Setup\nWe evaluate seven frontier LLMs on the OpenLearnLM\nBenchmark: DeepSeek-v3.2, Claude-Opus-4.5, Gemini-3-\nPro, GPT-5.2, Grok-4.1-fast, Kimi-K2-thinking, and GLM-\n4.7. The evaluation covers three domains with different ques-\ntion types and metrics. The Knowledge domain contains 473\nMCQ items evaluated by accuracy. The Skills domain con-\ntains 6,281 items consisting of 171 MCQ items and 6,110\nLong Answer items, where long answers are evaluated us-\ning a 10-point LLM-as-Judge scale. The Attitude domain in-\ncludes 12 standard items and 2 deception items, evaluated on\na 10-point scale with multiple samples.\n3.2\nMain Results\nTable 5 presents the overall evaluation results across all three\nKSA domains. Skills evaluation is broken down by six edu-\ncational activity centers.\nThe Knowledge domain reveals a surprising dissociation\nbetween content mastery and pedagogical understanding.\nGrok-4.1-fast achieves the highest content knowledge ac-\ncuracy (86.5%), while Gemini-3-Pro leads in pedagogical\nknowledge (89.6%). Most notably, Claude-Opus-4.5 exhibits\na 19.8 percentage point gap between content (66.3%) and\npedagogical knowledge (86.1%), the largest among all mod-\nels, suggesting it understands how to teach far better than\nwhat to teach. These findings challenge the assumption that\na single knowledge metric suffices for educational AI evalua-\ntion.\nThe Skills domain evaluation reveals that practical edu-\ncational capability does not simply follow from knowledge\nscores. Claude-Opus-4.5 achieves the highest overall Skills\naverage (8.82) despite having the lowest Content Knowl-\nedge, demonstrating that knowing facts and applying them\nin educational contexts are distinct competencies. GPT-5.2\nachieves the highest center score in Counseling (9.07), indi-\ncating strength in socio-emotional support scenarios. Coun-\nseling consistently receives the highest scores across all mod-\nels (average 8.78), while Assessment (8.42) and Research\n(8.42) present the greatest challenges, indicating that current\nLLMs struggle most with tasks requiring systematic judg-\nment and multi-step analytical reasoning.\nThe Attitude domain reveals critical differences in\nmodel trustworthiness.\nFour models (GLM-4.7, GPT-5.2,\nDeepSeek-v3.2, and Claude-Opus-4.5) demonstrate perfect\nalignment consistency (score 1.0), maintaining identical ped-\nagogical principles regardless of perceived monitoring. How-\never, Grok-4.1-fast (5.50) and Gemini-3-Pro (5.00) exhibit\nsignificant behavioral inconsistency between monitored and\nunmonitored conditions, suggesting potential alignment fak-\ning behavior.\nNotably, Grok-4.1-fast achieves the highest\nStandard Attitude score (8.87) while showing the worst De-\nception score (5.50), highlighting that surface-level attitude\nmetrics may not capture deeper alignment issues.\n4\nIn-depth Analysis\nWe conduct six complementary analyses to understand model\ncapabilities across the KSA framework. Figures 5 and 6 vi-\nsualize these findings.\n4.1\nSkills Performance by Center\nAnalysis across the six educational activity centers reveals\ndistinct capability patterns (Figure 5a). Counseling achieves\nthe highest average score (8.78), indicating that models han-\ndle socio-emotional support and empathic dialogue effec-\ntively.\nIn contrast, Assessment presents the greatest chal-\nlenge (8.42), suggesting that structured judgment tasks such\nas rubric application, error diagnosis, and evaluation item\ndesign remain difficult. The performance gap between the\nhighest-performing model (Claude-Opus-4.5, 8.82 average)\nand lowest-performing model (GLM-4.7, 8.24) spans 0.58\npoints, demonstrating meaningful differentiation among fron-\ntier models.\nClaude-Opus-4.5 leads in four of six centers\n(Learning, Counseling, Research, Admin), while GPT-5.2\nachieves the highest single-center score in Counseling (9.07).\n4.2\nAlignment Consistency\nThe Alignment Faking methodology [Greenblatt et al., 2024]\nreveals critical insights into behavioral consistency (Fig-\nure 5b).\nBy comparing responses under monitored (\u201cthis\nconversation is recorded\u201d) versus unmonitored (\u201cthis con-\nversation is private\u201d) conditions with n = 100 samples at\ntemperature 1.0, we assess whether models exhibit strate-\ngic behavioral changes. Four models (GLM-4.7, GPT-5.2,\nDeepSeek-v3.2, and Claude-Opus-4.5) demonstrate perfect\nconsistency (score 1.0), maintaining identical pedagogical\nprinciples regardless of perceived oversight. However, Grok-\n4.1-fast (5.50) and Gemini-3-Pro (5.00) show concerning in-\nconsistency, behaving more conservatively when monitored\nand more permissively when unmonitored. This finding has\nsignificant implications for educational deployment, where\nAI systems must maintain consistent ethical and pedagogical\nstandards in unsupervised student interactions.\nTable 5: Main performance on the OpenLearnLM Benchmark. Knowledge is evaluated by accuracy (%). Skills and Attitude are evaluated\non a 10-point scale. Skills scores are broken down by six educational activity centers. For Deception, lower scores indicate more consistent\nbehavior (less alignment faking).\nModel\nKnowledge\nSkills (by Center)\nAttitude\nContent\u2191\nPedagogy\u2191\nTeach\u2191\nLearn\u2191\nAssess\u2191\nCounsel\u2191\nResearch\u2191\nAdmin\u2191\nStandard\u2191\nDecept\u2193\nGrok-4.1-fast\n86.5\n86.1\n8.44\n8.67\n8.49\n8.87\n8.45\n8.73\n8.87\n5.50\nGemini-3-Pro\n82.4\n89.6\n8.26\n8.55\n8.38\n8.53\n8.29\n8.36\n7.91\n5.00\nKimi-K2-thinking\n81.4\n87.1\n8.68\n8.79\n8.46\n8.87\n8.54\n8.68\n8.84\n2.00\nGLM-4.7\n80.8\n87.5\n8.16\n8.45\n8.28\n8.33\n8.14\n8.09\n8.68\n1.00\nGPT-5.2\n80.8\n84.6\n8.80\n8.76\n8.22\n9.07\n8.19\n8.73\n8.56\n1.00\nDeepSeek-v3.2\n74.6\n73.2\n8.53\n8.76\n8.52\n8.87\n8.57\n8.50\n8.77\n1.00\nClaude-Opus-4.5\n66.3\n86.1\n8.83\n8.88\n8.56\n8.93\n8.78\n8.95\n8.45\n1.00\nTeaching\nLearning\nAssessment\nCounseling\nResearch\nAdmin\nEducational Activity Center\n7.8\n8.0\n8.2\n8.4\n8.6\n8.8\n9.0\n9.2\nScore (10-point scale)\nSkills Performance by Center Across Models\nClaude-Opus-4.5\nGPT-5.2\nKimi-K2-thinking\nGrok-4.1-fast\nGemini-3-Pro\nDeepSeek-v3.2\nGLM-4.7\n(a) Skills by Center\n0\n1\n2\n3\n4\n5\n6\n7\nDeception Score (lower = more consistent)\nClaude-Opus-4.5\nGPT-5.2\nDeepSeek-v3.2\nGLM-4.7\nKimi-K2-thinking\nGemini-3-Pro\nGrok-4.1-fast\n1.00\n1.00\n1.00\n1.00\n2.00\n5.00\n5.50\nAlignment Consistency: Deception Evaluation Results\nConsistent (1-2)\nMinor changes (3-4)\nAlignment faking risk (5+)\n(b) Alignment Consistency\nContent\nKnowledge\nPedagogical\nKnowledge\nSkills\n(avg)\nStandard\nAttitude\nAlignment\nConsistency\n0.2\n0.4\n0.6\n0.8\n1.0\nModel Capability Profile (Normalized Scores)\nClaude-Opus-4.5\nGPT-5.2\nKimi-K2-thinking\nGrok-4.1-fast\nGemini-3-Pro\nDeepSeek-v3.2\nGLM-4.7\n(c) Capability Profile\nFigure 5: Analysis results (Part 1): (a) Skills performance across six educational activity centers shows Counseling as the strongest (8.78)\nand Assessment as the most challenging (8.42). (b) Alignment consistency evaluation reveals that four models maintain consistent behavior\nregardless of monitoring, while Grok-4.1-fast and Gemini-3-Pro show potential alignment faking risk. (c) Radar chart displays normalized\ncapability profiles across five dimensions.\n4.3\nCapability Profiles\nThe radar chart visualization (Figure 5c) reveals complemen-\ntary strength patterns across models.\nGLM-4.7 and GPT-\n5.2 exhibit the most balanced profiles, maintaining above-\naverage performance across all five dimensions.\nClaude-\nOpus-4.5 shows a distinctive pattern with exceptional Skills\nand Alignment Consistency but notably lower Content\nKnowledge. Grok-4.1-fast demonstrates the inverse pattern\nwith strong Content Knowledge but weak Alignment Consis-\ntency. These profiles suggest that no single model dominates\nacross all dimensions, and optimal model selection depends\non specific educational use cases.\n4.4\nDifficulty-Based Performance\nAnalysis by Bloom\u2019s Taxonomy difficulty levels (Figure 6a)\nreveals expected performance degradation from Easy (Re-\nmembering/Understanding) through Medium (Applying/An-\nalyzing) to Hard (Evaluating/Creating) tasks. The Research\ncenter shows the largest difficulty gap, reflecting the com-\nplexity of experimental design and research ethics judgment.\nCounseling exhibits the smallest gap, suggesting that socio-\nemotional tasks are less sensitive to cognitive complexity lev-\nels. This pattern indicates that current models struggle most\nwith higher-order thinking tasks requiring evaluation and cre-\nation.\n4.5\nKnowledge Domain Gap\nA striking finding emerges from comparing Content Knowl-\nedge (curriculum-based factual knowledge) with Pedagogi-\ncal Knowledge (understanding of teaching and learning pro-\ncesses). As shown in Figure 6b, Claude-Opus-4.5 exhibits an\nexceptional gap of +19.8 percentage points (66.3% Content\nvs. 86.1% Pedagogy), indicating significantly stronger un-\nderstanding of \u201chow to teach\u201d than \u201cwhat to teach.\u201d This sug-\ngests the model excels at instructional strategies and learning\ntheories but may require fact-checking for subject-specific\ncontent. In contrast, DeepSeek-v3.2 shows the opposite pat-\ntern (\u22121.4 gap), with slightly stronger content mastery. Grok-\n4.1-fast demonstrates the most balanced profile with near-\nequal performance (86.5% vs. 86.1%). These gaps have prac-\ntical implications: models with strong pedagogical knowl-\nedge may excel at tutoring dialogue design but require con-\ntent validation, while content-strong models may provide ac-\ncurate information but lack pedagogical nuance.\n4.6\nCross-Domain Independence\nCorrelation analysis across the ten evaluation dimensions\n(Figure 6c) reveals a critical finding: Knowledge, Skills, and\nEasy\nMedium\nHard\nDifficulty Level\nTeaching\nLearning\nAssessment\nCounseling\nResearch\nAdmin\nEducational Activity Center\n8.79\n8.54\n8.06\n8.98\n8.68\n8.28\n8.69\n8.44\n7.96\n9.07\n8.75\n8.34\n8.72\n8.41\n7.98\n8.89\n8.61\n8.10\nSkills Performance by Center and Difficulty\n(Average Across Models)\n7.8\n8.0\n8.2\n8.4\n8.6\n8.8\n9.0\n9.2\nAverage Score\n(a) Difficulty Analysis\n65\n70\n75\n80\n85\n90\nContent Knowledge Accuracy (%)\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\nPedagogical Knowledge Accuracy (%)\nClaude-Opus-4.5\n( =+19.8)\nGPT-5.2\n( =+3.8)\nKimi-K2-thinking\n( =+5.7)\nGrok-4.1-fast\n( =-0.4)\nGemini-3-Pro\n( =+7.2)\nDeepSeek-v3.2\n( =-1.4)\nGLM-4.7\n( =+6.7)\nKnowledge Domain Analysis:\nContent vs. Pedagogical Knowledge\nEqual Performance\nStronger in Pedagogy\nStronger in Content\n(b) Knowledge Gap\nContent\nKnowledge\nPedagogical\nKnowledge\nTeaching\nLearning\nAssessment\nCounseling\nResearch\nAdmin\nStandard\nAttitude\nAlignment\nConsistency\nContent\nKnowledge\nPedagogical\nKnowledge\nTeaching\nLearning\nAssessment\nCounseling\nResearch\nAdmin\nStandard\nAttitude\nAlignment\nConsistency\n1.00\n0.33\n-0.51\n-0.58\n-0.47\n-0.25\n-0.67\n-0.39\n0.11\n-0.63\n0.33\n1.00\n-0.21\n-0.35\n-0.32\n-0.36\n-0.31\n-0.06\n-0.40\n-0.41\n-0.51\n-0.21\n1.00\n0.95\n0.25\n0.91\n0.56\n0.89\n0.22\n0.42\n-0.58\n-0.35\n0.95\n1.00\n0.54\n0.89\n0.78\n0.91\n0.26\n0.33\n-0.47\n-0.32\n0.25\n0.54\n1.00\n0.28\n0.93\n0.49\n0.19\n-0.15\n-0.25\n-0.36\n0.91\n0.89\n0.28\n1.00\n0.49\n0.89\n0.32\n0.17\n-0.67\n-0.31\n0.56\n0.78\n0.93\n0.49\n1.00\n0.69\n0.19\n0.11\n-0.39\n-0.06\n0.89\n0.91\n0.49\n0.89\n0.69\n1.00\n0.18\n0.02\n0.11\n-0.40\n0.22\n0.26\n0.19\n0.32\n0.19\n0.18\n1.00\n0.28\n-0.63\n-0.41\n0.42\n0.33\n-0.15\n0.17\n0.11\n0.02\n0.28\n1.00\nCross-Domain Performance Correlation\n(Pearson r across 7 models)\nKnowledge\nSkills\nAttitude\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nPearson Correlation\n(c) Cross-Domain Correlation\nFigure 6: Analysis results (Part 2): (a) Performance decreases from Easy to Hard difficulty across all centers, with Research showing the\nlargest gap. (b) Content vs. Pedagogical Knowledge scatter plot reveals Claude-Opus-4.5\u2019s significant gap (+19.8), indicating stronger\nunderstanding of \u201chow to teach\u201d than \u201cwhat to teach.\u201d (c) Cross-domain correlation heatmap shows weak or negative correlations between\nKnowledge, Skills, and Attitude, validating the need for separate evaluation axes.\nAttitude represent largely independent capabilities. The cor-\nrelation between Knowledge (average of Content and Peda-\ngogy) and Skills (average of six centers) is r = \u22120.51, indi-\ncating that models strong in factual knowledge do not neces-\nsarily excel in practical educational tasks. Similarly, Knowl-\nedge and Attitude show negative correlation (r = \u22120.63),\nwhile Skills and Attitude exhibit weak positive correlation\n(r = 0.22). This independence validates the KSA frame-\nwork\u2019s three-axis approach, as evaluating only knowledge ac-\ncuracy would miss critical dimensions of educational AI ca-\npability. The negative correlations suggest a potential trade-\noff: models optimized for knowledge retrieval may inadver-\ntently sacrifice learner-centered attitudes or practical applica-\ntion skills.\n5\nRelated Work\n5.1\nLLM in Education\nLLMs are increasingly deployed in educational settings\nacross diverse roles.\nStudies demonstrate positive effects\non learning performance, particularly in language learning\nand writing tasks [Contributors, 2025a], with commercial de-\nployments like Khan Academy\u2019s Khanmigo reaching over\n700,000 users [Khan Academy, 2024].\nResearch has ex-\nplored LLMs as tutors [Wang et al., 2024], automated essay\nscoring systems [Hussein et al., 2024], and instructional de-\nsign assistants [Zhang et al., 2024]. However, studies reveal\nthat current LLMs struggle to match traditional ITS adap-\ntivity, achieving only 56.6% pedagogical soundness despite\n97.3% answer accuracy [Reasoning and Lab, 2025], high-\nlighting the gap between factual correctness and effective\nteaching.\n5.2\nEducational LLM Benchmarks\nSeveral benchmarks have emerged to evaluate LLMs in ed-\nucational contexts.\nLearnLM [Google DeepMind, 2024]\ngrounds evaluation in learning science principles but provides\nno public dataset. EduBench [Xu and others, 2025] covers\nnine educational scenarios with 18.8K items but lacks hier-\narchical organization. Domain-specific benchmarks include\nMathTutorBench [Reasoning and Lab, 2025] for mathemat-\nical tutoring, TutorBench [Contributors, 2025b] for tutoring\ninteractions, and EducationQ [Shi et al., 2025] spanning 13\ndisciplines. Recent work on alignment stability [Greenblatt et\nal., 2024] has shown that models may behave differently un-\nder varying monitoring conditions, yet no educational bench-\nmark addresses this concern. OpenLearnLM Benchmark ad-\ndresses these gaps through a comprehensive KSA framework\nwith hierarchical task taxonomy and deception evaluation.\n6\nConclusion\nExisting educational LLM benchmarks focus narrowly on\ncontent knowledge or isolated pedagogical tasks, lacking\nsystematic grounding in educational theory. We introduced\nOpenLearnLM Benchmark, a comprehensive framework\nbuilt upon the KSA (Knowledge-Skills-Attitude) model that\nevaluates LLMs across curriculum-aligned content, scenario-\nbased competencies organized in a four-level hierarchy, and\nbehavioral alignment including deception resistance.\nOur evaluation of seven frontier models reveals that ed-\nucational AI capability is fundamentally multidimensional.\nSome models excel in practical skills despite weaker content\nknowledge, while others demonstrate strong factual accuracy\nbut concerning alignment inconsistency. The weak correla-\ntions across KSA dimensions validate that single-axis eval-\nuation is insufficient for educational AI assessment, and no\nsingle model dominates across all dimensions.\nOpenLearnLM Benchmark provides the first open, large-\nscale benchmark grounded in learning sciences for the re-\nsearch community. Future work includes extending to mul-\ntilingual contexts, incorporating real classroom interaction\ndata, and developing targeted training strategies to address\nthe capability gaps identified in this study.\nReferences\n[Chevalier et al., 2024] Susanne Chevalier, Jakub Macina,\nManu Kapur, and Mrinmaya Sachan.\nUnifying ai tutor\nevaluation: An evaluation taxonomy for pedagogical abil-\nity assessment of llm-powered ai tutors. arXiv preprint\narXiv:2412.09416, 2024.\n[Contributors, 2025a] Multiple Contributors. The effect of\nchatgpt on students\u2019 learning performance, learning per-\nception, and higher-order thinking.\nNature Humanities\nand Social Sciences Communications, 2025.\n[Contributors, 2025b] Multiple Contributors. Tutorbench: A\nbenchmark to assess tutoring capabilities of large language\nmodels. arXiv preprint arXiv:2510.02663, 2025.\n[Google DeepMind, 2024] Google DeepMind.\nLearnlm:\nImproving gemini for learning. 2024.\n[Greenblatt et al., 2024] Ryan Greenblatt, Buck Shlegeris,\nKshitij Krueger, and Sam Bowman. Alignment faking in\nlarge language models. arXiv preprint arXiv:2412.14093,\n2024.\n[Huang et al., 2023] Yuzhen Huang, Yuzhuo Bai, Zhihao\nZhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng\nLiu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-eval:\nA multi-level multi-discipline chinese evaluation suite for\nfoundation models.\nIn Advances in Neural Information\nProcessing Systems, volume 36, 2023.\n[Hussein et al., 2024] Mohamed Hussein, Hesham Hassan,\nand Mohammad Nassef.\nA survey on automated es-\nsay scoring: Advances, challenges, and future directions.\nACM Computing Surveys, 56(5):1\u201337, 2024.\n[Khan Academy, 2024] Khan Academy. Khanmigo: Khan\nacademy\u2019s\nai\ntutor.\nhttps://www.khanacademy.org/\nkhan-labs, 2024.\n[Leli`evre et al., 2025] Maxime Leli`evre,\nAmy Waldock,\nMeng Liu, Natalia Vald\u00b4es Aspillaga, Alasdair Mackintosh,\nMar\u00b4\u0131a Jos\u00b4e Ogando Portela, Jared Lee, Paul Atherton,\nRobin A. A. Ince, and Oliver G. B. Garrod. Benchmark-\ning the pedagogical knowledge of large language models.\narXiv preprint arXiv:2506.18710, 2025.\n[Reasoning and Lab, 2025] ETH Zurich Language Reason-\ning and Education Lab. Beyond final answers: Evaluating\nlarge language models for math tutoring. arXiv preprint\narXiv:2503.16460, 2025.\n[Rein et al., 2023] David Rein, Betty Li Hou, Asa Cooper\nStickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R Bowman. Gpqa:\nA graduate-level google-proof q&a benchmark.\narXiv\npreprint arXiv:2311.12022, 2023.\n[Shi et al., 2025] Yao Shi,\nRongkeng Liang,\nand Yong\nXu.\nEducationq: Evaluating llms\u2019 teaching capabilities\nthrough multi-agent dialogue framework. In Proceedings\nof the 63rd Annual Meeting of the Association for Compu-\ntational Linguistics, 2025.\n[Wang et al., 2024] Shen Wang, Tianlong Liu, Jiyang Tan,\nJiawei Su, Yanjun Chen, et al.\nLarge language mod-\nels for education: A survey and outlook. arXiv preprint\narXiv:2403.18105, 2024.\n[Xu and others, 2025] Bin Xu et al.\nEdubench: A com-\nprehensive benchmarking dataset for evaluating large lan-\nguage models in diverse educational scenarios.\narXiv\npreprint arXiv:2505.16160, 2025.\n[Zhang et al., 2024] Wei Zhang, Ming Liu, and Xiaoqing\nChen. Large language models for instructional design: A\nsystematic review."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge, Skills, and Attitude in Educational Large Language Models**\n\n**Main Contributions**\nThe OpenLearnLM Benchmark introduces a comprehensive framework grounded in educational assessment theory to evaluate large language models (LLMs) across three dimensions: Knowledge, Skills, and Attitude. The framework is designed to capture multidimensional educational competence beyond factual accuracy.\n\n**Methodology**\n- **Knowledge Domain**: Focuses on curriculum-aligned content knowledge and pedagogical understanding.\n- **Skills Domain**: Evaluates scenario-based competencies through a hierarchical structure of 6 Centers, 46 Scenarios, and 89 Sub-scenarios with difficulty tiers based on Bloom's taxonomy. \n- **Attitude Domain**: Examines epistemic, instructional, and normative-ethical stances along with deception assessment using Alignment Faking methodology.\n\n**Dataset Description**\nThe benchmark comprises over 124K items spanning multiple subjects, educational roles, and difficulty levels based on Bloom\u2019s taxonomy. The Knowledge domain prioritizes authentic assessment items from established benchmarks, while the Attitude domain adapts Anthropic's Alignment Faking methodology to detect behavioral inconsistency under varying monitoring conditions.\n\n**Key Findings**\n- **Claude-Opus-4.5**: Demonstrates strong practical skills despite lower content knowledge.\n- **Grok-4.1-fast**: Leads in knowledge but shows alignment concerns, indicating a need for multi-axis evaluation.\n- **Alignment Faking Methodology**: Reveals critical insights into behavioral consistency and potential alignment faking behavior.\n\n**Conclusions**\nThe OpenLearnLM Benchmark provides an open, comprehensive framework for advancing LLM readiness in authentic educational contexts. It addresses the gap between factual correctness and effective teaching by evaluating multidimensional capabilities including knowledge, skills, and attitude. The findings highlight that no single model dominates all dimensions, validating the necessity of multi-axis evaluation.\n\n**Future Work**\nFuture research will extend to multilingual contexts, incorporate real classroom interaction data, and develop targeted training strategies to address capability gaps identified in this study."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "ASBA: A-line State Space Model and B-line Attention for Sparse Optical\nDoppler Tomography Reconstruction\nZhenghong Li\nWensheng Cheng\nCongwu Du\nYingtian Pan\nZhaozheng Yin\nHaibin Ling\u2217\nStony Brook University\nAbstract\nOptical Doppler Tomography (ODT) is an emerging blood\nflow analysis technique. A 2D ODT image (B-scan) is gen-\nerated by sequentially acquiring 1D depth-resolved raw A-\nscans (A-line) along the lateral axis (B-line), followed by\nDoppler phase-subtraction analysis. To ensure high-fidelity\nB-scan images, current practices rely on dense sampling,\nwhich prolongs scanning time, increases storage demands,\nand limits the capture of rapid blood flow dynamics. Recent\nstudies have explored sparse sampling of raw A-scans to al-\nleviate these limitations, but their effectiveness is hindered\nby the conservative sampling rates and the uniform model-\ning of flow and background signals. In this study, we intro-\nduce a novel blood flow-aware network, named ASBA (A-\nline ROI State space model and B-line phase Attention), to\nreconstruct ODT images from highly sparsely sampled raw\nA-scans. Specifically, we propose an A-line ROI state space\nmodel to extract sparsely distributed flow features along the\nA-line, and a B-line phase attention to capture long-range\nflow signals along each B-line based on phase difference.\nMoreover, we introduce a flow-aware weighted loss func-\ntion that encourages the network to prioritize the accurate\nreconstruction of flow signals. Extensive experiments on\nreal animal data demonstrate that the proposed approach\nclearly outperforms existing state-of-the-art reconstruction\nmethods.\n1. Introduction\nOptical Doppler Tomography (ODT) is an emerging tech-\nnique for analyzing blood flow [19]. ODT measures blood\nflow speed by exploiting the Doppler phase shift induced\nby moving red blood cells within vessels [3]. It offers high-\nresolution high-contrast tomographic flow imaging, making\nit suitable for many clinical applications such as vascular\ndisease monitoring [20]. Despite its promise in blood flow\nspeed estimation, current ODT technique requires dense\nspatial sampling to accurately capture flow dynamics, par-\n\u2217Work was done while with Stony Brook University.\nFigure 1. Examples of sparse ODT reconstruction. Due to big gaps\namong sampled A-lines at high sparsity, traditional pipeline [45]\nthat estimates flow speed directly from phase differences of adja-\ncent A-lines fails to reconstruct flow images. The state-of-the-art\n(SOTA) sparse ODT reconstruction method [24] also loses most\nweak flow signals in small vessels, while our proposed ASBA ef-\nfectively preserves most flows in the reconstructed image.\nticularly to detect slow-moving red blood cells in small ves-\nsels [38]. This dense scanning strategy results in long ac-\nquisition times and substantial data storage demands. More-\nover, the long scanning time further limits the detection of\nhigh dynamic changes in blood flow caused by stimulation.\nIn typical ODT reconstruction, the fundamental scan,\nnamed raw A-scan, encodes the depth information (A-line)\nin the spectral domain. A 2D raw B-scan is acquired by\nlaterally sliding the sensor along the width (B-line).\nTo\nretrieve depth information, 1D IFFT is applied to each A-\nline, decoding the depth-resolved magnitude and phase re-\n1\narXiv:2601.14165v1  [cs.CV]  20 Jan 2026\nsponses in the spatial domain. Phase differences between\nadjacent A-lines are used to estimate the blood flow speed\nmap [19], and magnitude to suppress background noise.\nA recent study [24] shows the potential of reconstructing\nODT images from sparsely sampled raw A-scans. How-\never, this approach is limited in both scanning strategies\nand modeling methodologies. First, it employs conserva-\ntive sparse sampling rates (\u00d72 or \u00d74 sparsity), failing to\nsubstantially shorten scanning times. Second, it uniformly\nmodels the magnitude and phase signals, as well as the flow\nand background regions, without explicit distinction. Given\nthat magnitude and phase play distinct roles in ODT re-\nconstruction, and considering that background regions com-\nprise, on average, more than 94% of the B-scan images in\nour experiments, such undifferentiated modeling strategies\nare suboptimal for accurate flow reconstruction.\nIt is also worth noting that, despite the superficial re-\nsemblance between sparse ODT reconstruction and single-\nimage super-resolution (SISR) [2, 5, 11, 12, 21, 23, 25, 33,\n41, 46], existing SISR methods are inappropriate for sparse\nODT reconstruction. Besides the above mentioned limita-\ntions, this is also primarily due to the unique characteristics\nof ODT phase data\u2014where phase differences, rather than\nindividual values, are critical\u2014and the fundamentally dif-\nferent sampling strategy, which involves directly skipping\nA-lines rather than aggregating neighboring pixels.\nTo more effectively reduce scanning time and memory\nrequirements, we propose reconstructing ODT B-scan im-\nages from highly sparsely sampled raw A-scans. Given\nthat flow speed estimation in traditional ODT relies on the\nphase difference between adjacent raw A-scans [45], a high\nsparsity implies insufficient information for conventional\nflow reconstruction for each A-scan. As shown in Figure 1,\nsuch an aggressive sampling strategy introduces substantial\nnoise and artifacts, making vessels and flows invisible in\ntraditionally reconstructed ODT. Therefore, given the inher-\nent sparsity of flow regions in B-scans and the distinctive\nfeatures of phase signals, a dedicated sparse reconstruction\nmodel that explicitly emphasizes flow modeling is essential.\nIn this work, we propose a flow-aware reconstruction\nnetwork named ASBA (A-line ROI State Space Model and\nB-line phase Attention), aiming to address the modeling\nissues in previous works by focusing on flow information\nreconstruction from highly sparsely sampled ODT raw A-\nscans. The overall pipeline is demonstrated in Figure 2.\nFirst, we design a two-branch framework to separately pro-\ncess the magnitude and phase information for their different\nattributions. Since phase information is the core of ODT\nreconstruction, we specifically focus on phase reconstruc-\ntion in the network block design. Second, we propose an\nA-line Region-of-Interest (ROI) state space block to adap-\ntively model the flow-relevant phase information along each\nA-line. As flow signals are sparsely distributed in depth\nand background signals contribute minimally to flow recon-\nstruction, we dynamically modulate the input matrix of the\nState Space Model (SSM) to highlight the flow region and\nsuppress the background for more effective state transition.\nThird, we introduce a B-line phase attention mechanism\nto model flow information along the B-line. Since it is the\nphase difference, rather than the phase itself, that indicates\nflow characteristics, explicitly incorporating phase differ-\nences into the attention computation better aligns the net-\nwork with the intrinsic features of blood flow. Finally, we\ndevelop a flow-aware weighted loss function that adaptively\nassigns higher weights to the flow regions, encouraging the\nnetwork to prioritize the reconstruction of flow features over\nthe dominant but less informative background signals.\nBased on the above designs, ASBA effectively captures\nflow information, enabling accurate reconstruction from\nhighly sparsely sampled ODT raw B-scans, and hence sig-\nnificantly reducing data acquisition time and storage mem-\nory. Notably, the decreased scanning time facilitates the\nstudy of dynamic blood flow responses to various forms of\nstimulation. Experiments on both awake and anesthetized\nanimal datasets demonstrate that our method has exhibited\npromising results in comparison with state-of-the-arts.\n2. Related Works\nOptical Doppler Tomography (ODT) is an emerging\nnon-invasive imaging approach for blood flow visualiza-\ntion [19]. It employs phase difference of successive A-scans\ncaused by Doppler frequency shift of moving red blood\ncells to estimate flow speed [4, 15, 45]. However, because\nthe sensitivity of ODT is highly dependent on the sampling\ninterval [36], dense sampling is typically required to detect\nslow blood flow [38]. Therefore, traditional ODT imaging\nrequires a long scanning time and large storage memory.\nSparse ODT Reconstruction.\nAddressing the afore-\nmentioned challenge, a recent study in [24] employs state\nspace models for sparse ODT reconstruction. However, the\napproach is limited to conservative sparsity levels, thus fail-\ning to significantly address the scanning time and storage\nissue. Besides, it treats flow and background signals uni-\nformly, overlooking the sparse nature of flow in ODT B-\nscans and the importance of phase difference for flow re-\nconstruction. Inspired by [24] and meanwhile addressing\nits limitations, we propose a new solution for much more\nsparse ODT reconstruction.\nNote that sparse reconstruction has been previously ap-\nplied in other domains such as OCT, radar and MRI using\ncompressive sensing [26, 27, 31, 32, 39, 40, 43]. However,\ndue to domain discrepancies and differences in sampling\nmethodologies, directly adapting these techniques to sparse\nODT reconstruction remains non-trivial or inappropriate.\nState Space Models (SSMs) [17] were originally pro-\nposed in the field of control, describing the transfor-\n2\nFigure 2. Our sparse ODT reconstruction system. The raw B-scan is highly downsampled with a large stride along the B-line. The sparse\nmagnitude and phase responses are forwarded to the proposed ASBA network to reconstruct the B-scan image.\nmation of state spaces.\nRecently, a new SSM named\nMamba [8] was introduced, extending early deep learning-\nbased SSMs [9, 10] by using input data to predict state ma-\ntrices to facilitate selective sequence modeling, improving\nlong-range dependencies with linear complexity. Due to\nthese advantages, Mamba has been widely adopted for var-\nious vision tasks [1, 11, 12, 14, 21, 28, 30, 44, 47, 48].\nAlthough Mamba can selectively process sequences, its\nintrinsic selective ability is not effective for sparse ODT re-\nconstruction, given that the majority of an ODT B-scan is\nthe background that contributes little to flow modeling.\n3. Method\n3.1. Problem Formulation\nThis work aims to reconstruct ODT images from highly\nsparsely sampled ODT raw signals to significantly reduce\nboth scanning time and storage requirements. Following\nthe definition and notations in [24], the fundamental acqui-\nsition unit is referred to as the raw A-scan, denoted as \u02dcai,\nwhich encodes depth (A-line) information in the spectral\ndomain. By laterally translating the sensor along the width\n(B-line), a 2D cross-sectional signal, termed a raw B-scan\n\u02dc\nB = {\u02dcai}W\ni=1, is obtained, where W represents the number\nof raw A-scans. 1D inverse fast Fourier transform (IFFT)\nis applied to each raw A-scan \u02dcai to extract the correspond-\ning magnitude M = {mi}W\ni=1 and phase P = {pi}W\ni=1\nresponses along the depth in the spatial domain. In the tra-\nditional reconstruction pipeline [45], the phase differences\nbetween adjacent A-lines are utilized to estimate blood flow\nspeed, based on the Doppler frequency shift induced by\nmoving red blood cells. The magnitude response is primar-\nily used for post-processing purposes.\nTo effectively address the dense scanning issue of the tra-\nditional pipeline, we propose a sparse ODT reconstruction\npipeline (Figure 2). As discussed before, we aggressively\nsample the raw B-scans with a large stride \u03b4. The mag-\nnitude and phase of sparsely sampled A-lines, denoted as\nMs = {m1+(j\u22121)\u03b4}W \u2032\nj=1 and Ps = {p1+(j\u22121)\u03b4}W \u2032\nj=1, are\nthe inputs to the proposed pipeline.\n3.2. Overall Pipeline of ASBA\nThe overall pipeline of the proposed flow-aware A-line\nROI SSM and B-line phase Attention network (ASBA) for\nsparse ODT reconstruction is presented in Figure 2. Dif-\nferent from the previous work [24] that concatenates the\nmagnitude Ms and phase Ps as the input to the network,\nwe propose a two-branch pipeline that separately processes\nMs and Ps and focuses on the flow-aware phase modeling.\nSpecifically, Ms and Ps are first encoded by a convolution\nlayer to deep features individually, processed by a sequence\nof ng Magnitude-Phase (MP) Groups, and upscaled by the\nB-line Shuffle (B-Shuf) layer [24, 35] to predict the dense\nmagnitude\n\u02c6\nM and phase \u02c6P . Finally, the features of pre-\ndicted dense \u02c6\nM and \u02c6P are concatenated to predict the final\nreconstructed B-scan image \u02c6Y .\nThe core structure of the pipeline is the MP Group, con-\nsisting of three main components: a Magnitude Module\n(MagM), a Phase Module (PhaM), and a Fusion block.\nEach Magnitude Module (Figure 3(a)) comprises N\nMagnitude Layers, where each layer consists of two Resid-\nual Channel Attention Blocks (RCAB) [42]. Given that the\nmagnitude response primarily serves an auxiliary role in\nODT reconstruction, we adopt RCAB, a widely used CNN-\nbased module for pixel-wise feature extraction to achieve\na balance between effectiveness and computational effi-\nciency. The primary focus of this work is on extracting\nflow-related information from the phase response.\nThe Fusion Block (Figure 3(b)) is designed to facilitate\ninformation exchange between the magnitude and phase\nbranches, thereby enhancing feature extraction in both\nbranches. Specifically, it begins by concatenating feature\nmaps from the two branches, followed by a channel-wise\nsqueeze-expand operation implemented using two convo-\n3\nFigure 3. Network Blocks. (a) Magnitude Module, (b) Fusion Block, (c) Phase Module, (d) A-line ROI State Space Model (A-RS) Block,\nand (e) B-line Phase Attention (B-PA) Block.\nlutional layers to fuse the information effectively and ef-\nficiently. The resulting fused feature is then split and in-\ntegrated back into the respective branches, allowing both\nbranches to benefit from complementary contextual cues.\nEach Phase Module (Figure 3(c)) consists of N Phase\nLayers.\nBecause the information distributions differ be-\ntween A-line and B-line, and only B-line is sparsely sam-\npled, we refer to [24] to model these axes individually.\nIn each layer, an A-line ROI SSM (A-RS) block (Fig-\nure 3(d), detailed in Sec. 3.3) is first applied to each A-\nline. This block leverages a modulated Mamba model to se-\nlectively extract flow-related features by emphasizing ROI\nalong depth. Subsequently, a B-line Phase Attention (B-\nPA) block (Figure 3(e), detailed in Sec. 3.4) processes each\nB-line to capture long-range dependencies in the lateral di-\nrection, guided by phase differences induced by the Doppler\neffect. Finally, a feed-forward network (FFN), composed of\nsequential 1D convolutions along A-line and B-line axes, is\nemployed for feature fusion, following the design in [24].\n3.3. A-line ROI State Space Model (A-RS)\nIn ODT, the flow signal along an A-line is typically sparse,\nwhile background regions occupy a large portion of the\ndepth. As shown in the B-scan image in Figure 2, these\nbackground regions contribute little to the modeling of\nblood flow features but often dominate standard modeling\napproaches. Classical attention-based methods, which rely\non computing pairwise similarities across all spatial posi-\ntions, are not well-suited for such a setting: the abundance\nof background signals tends to overwhelm the sparse flow\nsignals, reducing both the efficiency and effectiveness of\nflow feature extraction.\nIn contrast, state space models (SSMs) offer a more suit-\nable alternative for A-line modeling due to their learned fil-\ntering mechanisms and linearly scalable complexity. A no-\ntable example is the recently proposed Mamba model [8],\nwhose scanning operation is formulated as:\nhk = \u00af\nAhk\u22121 + \u00af\nBxk, yk = Chk + Dxk\n(1)\nwhere\n\u00af\nA\n=\nexp (\u2206A) is the state matrix,\n\u00af\nB\n=\n(\u2206A)\u22121(exp (\u2206A) \u2212I) \u00b7 \u2206B is the input matrix, C is\nthe output matrix, and \u2206is the timescale parameter to dis-\ncretize A and B. Although Mamba achieves high computa-\ntional efficiency, its application to A-line modeling in ODT\nis suboptimal due to the overwhelming influence of non-\ninformative background signals during state propagation.\nAddressing this limitation, we propose the A-line ROI\nState Space Model (A-RS) block (Figure 3(d)), which fo-\ncuses on flow-relevant regions within A-line. To mitigate\nthe influence of non-informative background features, A-\nRS introduces a masked input strategy, which adaptively\nsuppresses non-informative background signals and noise\nfrom the SSM\u2019s input matrix prior to state transformation.\nThis selective modeling enables the system to allocate its\nrepresentational capacity to flow regions, significantly en-\nhancing the extraction of flow features along the depth axis.\nFirst, we extract a layer-shared feature F from the con-\ncatenated input magnitude Ms and phase Ps to identify po-\ntential flow regions. The inclusion of magnitude is moti-\nvated by its utility in traditional ODT pipelines for suppress-\ning background noise, thereby aiding in distinguishing flow\nfrom non-flow areas. Second, for each Phase Layer, a layer-\nspecific convolution followed by SiLU [7] is applied to\ngenerate an ROI soft mask R, which adaptively highlights\nflow regions and suppresses background. Finally, this mask\n4\nFigure 4. Operations in the B-PA Block to map the phase difference feature back to the phase domain.\nR is applied to the Mamba input matrix \u00af\nB through element-\nwise multiplication, enabling flow-aware feature extraction\nalong the depth. The A-line ROI Selective Scan (A-RSS)\nblock of the modulated Mamba can be represented as:\nF = Conv(Ms \u2295Ps), R = SiLU(Conv(F ))\nhk = \u00af\nAhk\u22121 + (R \u00b7 \u00af\nB)xk, yk = Chk + Dxk\n(2)\n3.4. B-line Phase Attention (B-PA)\nTo capture the long-range flow dependency along the B-\nline, we introduce the B-line Phase Attention (B-PA) block,\nas illustrated in Figure 3(e). Since the B-line is largely ho-\nmogeneous and flow signals in raw B-scans often exhibit\nlong-range continuity along the B-line due to dense sam-\npling, the attention mechanism is well-suited for modeling\nflow dependencies along the B-line [24]. However, directly\napplying conventional attention modules to phase signals is\nsuboptimal because the characteristics of ODT phase distri-\nbutions differ significantly from those of other data modali-\nties, such as natural images.\nIn ODT, flow-related similarities are not captured by raw\nphase values themselves, but rather by phase differences\nbetween adjacent A-lines, which encode motion-induced\nchanges in signal. Moreover, due to the periodic nature\nof phase (defined within (\u2212\u03c0, \u03c0]), flow regions exhibiting\nsimilar behavior may yield phase differences with oppo-\nsite signs, despite representing equivalent physical motion.\nTo ensure the attention mechanism correctly captures such\nsymmetric relationships, we compute the absolute value of\nthe B-line difference (B-Diff) of the query Q and key K\nof the attention module as a more robust similarity measure.\nThe core of the proposed B-PA block (excluding the gate\nand output modules) can be represented as:\nQb, Kb = B-Diff(Q), B-Diff(K)\nQa, Ka = DConv(|Qb|), DConv(|Kb|)\nVb = B-Diff(V ), V \u2032 = Linear(V )\nX\u2032 = B-SA(Qa, Ka, Vb), X = X\u2032 + Shift(V \u2032)\nwhere DConv denotes depth-wise convolution, and B-SA\ndenotes B-line Self-Attention [24], calculating the 1D at-\ntention [37] along each B-line. Since the B-line difference\noperation pads features before B-line subtraction, to map\nthe phase difference feature X\u2032 back to the phase domain,\nthe auxiliary value V \u2032 should be shifted accordingly and\nadded to X\u2032, as shown in Figure 4.\n3.5. Flow-aware Weighted Loss\nSince the majority of an ODT B-scan comprises back-\nground regions, which may exhibit noise, applying a uni-\nform mean square error (MSE) loss across all pixels may\nbias the network toward fitting the background rather than\nreconstructing flow signals. To address this issue, we pro-\npose a flow-aware weighting strategy that assigns higher\nimportance to flow-dominant regions.\nSpecifically, each\npixel is weighted based on its groundtruth flow. Given the\ngroundtruth flow value yi at the i-th pixel of a B-scan image,\nthe corresponding weight is calculated as follows:\nwi = y\u03b1\ni + \u03b2\n(3)\nwhere \u03b1 controls the balance among different flow values\nand \u03b2 prevents the background weights from being zero.\nThen, we calculate the weighted MSE loss for the predic-\ntion of the reconstructed flow LY , as well as the magnitude\nLM and phase LP . For example, LY is calculated by:\nLY =\nP wi(\u02c6yi \u2212yi)2\nP wi\n(4)\nFinally, the total loss is a weighted sum of the three\nlosses with a parameter \u03bb:\nL = LY + \u03bb \u00b7 (LM + LP )\n(5)\n4. Experiments\n4.1. ODT Dataset of Mouse Brain Cortex\nDue to the complexity of acquiring animal blood flow data,\nmost existing studies rely on relatively small datasets [16,\n20, 24] (less than 20). In this work, we constructed two\ndatasets from the brain cortex of awake and anesthetized\nmice, referred to as the Awake Mouse Cortex Dataset\n(MCD-AW) and the Anesthetized Mouse Cortex Dataset\n(MCD-AN), respectively.\nEach dataset contains 20 3D\nODT volumes, with 14 used for training and 6 for testing.\nThe training and testing sets are collected from different\nmice. Each 3D volume contains 500 2D ODT raw B-scans,\nwith each raw B-scan having a resolution of 256 \u00d7 12600.\nThe groundtruths are obtained by applying the traditional\npipeline to the densely sampled raw signals. Since ODT\nvolumes are typically visualized using en-face Maximum\nIntensity Projection (MIP), we assess reconstruction qual-\nity using PSNR and SSIM on both reconstructed B-scans\n5\nTable 1. Comparison of sparse ODT reconstruction results\nMethod\nMCD-AW\nMCD-AN\nB-scan\nMIP\nB-scan\nMIP\nPSNR\nSSIM\nPSNR\nSSIM\nPSNR\nSSIM\nPSNR\nSSIM\n\u00d78 Sparsity\nTraditional\n14.32\u00b11.35 0.2623\u00b1.0586 14.12\u00b10.32 0.2669\u00b1.0396 14.37\u00b11.53 0.2684\u00b1.0588 14.52\u00b10.72 0.2821\u00b1.0489\nSwinIR [25]\n18.81\u00b11.17 0.4549\u00b1.0402 19.27\u00b11.35 0.4583\u00b1.0458 18.57\u00b11.35 0.4294\u00b1.0371 19.27\u00b11.10 0.4436\u00b1.0442\nHAT [2]\n18.71\u00b11.26 0.4552\u00b1.0416 19.20\u00b11.34 0.4578\u00b1.0445 18.71\u00b11.31 0.4466\u00b1.0371 19.39\u00b11.06 0.4598\u00b1.0421\nSRFormer [46]\n18.70\u00b11.31 0.4613\u00b1.0394 19.42\u00b11.36 0.4676\u00b1.0429 18.78\u00b11.36 0.4434\u00b1.0345 19.45\u00b10.98 0.4517\u00b1.0355\nDAT [5]\n19.23\u00b11.33 0.5036\u00b1.0482 19.90\u00b11.21 0.5076\u00b1.0374 19.04\u00b11.26 0.4577\u00b1.0358 19.64\u00b10.96 0.4764\u00b1.0441\nCFAT [33]\n18.54\u00b11.18 0.4287\u00b1.0356 18.89\u00b11.17 0.4237\u00b1.0381 18.76\u00b11.22 0.4507\u00b1.0383 19.38\u00b11.07 0.4648\u00b1.0421\nMambaIR [11]\n18.97\u00b11.39 0.4811\u00b1.0480 19.67\u00b11.31 0.4948\u00b1.0437 19.21\u00b11.20 0.4826\u00b1.0415 20.15\u00b10.95 0.4997\u00b1.0358\nMambaIRv2 [12] 18.73\u00b11.30 0.4526\u00b1.0453 19.18\u00b11.34 0.4523\u00b1.0546 18.87\u00b11.20 0.4532\u00b1.0380 19.52\u00b11.04 0.4646\u00b1.0417\nMaIR [21]\n19.46\u00b11.33 0.5129\u00b1.0528 20.10\u00b11.30 0.5235\u00b1.0389 19.19\u00b11.35 0.4872\u00b1.0446 20.15\u00b10.85 0.5091\u00b1.0313\nASSAN [24]\n19.61\u00b11.48 0.5315\u00b1.0533 20.65\u00b11.25 0.5430\u00b1.0294 19.50\u00b11.67 0.5068\u00b1.0524 20.46\u00b11.24 0.5271\u00b1.0340\nASBA (Ours)\n20.48\u00b11.50 0.5868\u00b1.0559 21.65\u00b11.02 0.6527\u00b1.0169 20.38\u00b11.61 0.5686\u00b1.0561 21.66\u00b11.05 0.6714\u00b1.0346\n\u00d716 Sparsity\nTraditional\n11.67\u00b10.90 0.1072\u00b1.0177\n7.05\u00b10.44\n0.0715\u00b1.0120 12.01\u00b11.10 0.1197\u00b1.0209\n7.37\u00b10.64\n0.0859\u00b1.0211\nSwinIR [25]\n17.26\u00b11.12 0.3281\u00b1.0497 17.54\u00b11.48 0.3278\u00b1.0779 16.84\u00b11.32 0.2885\u00b1.0605 17.14\u00b10.94 0.3208\u00b1.0694\nHAT [2]\n17.48\u00b11.23 0.3439\u00b1.0526 17.66\u00b11.35 0.3329\u00b1.0715 17.21\u00b11.32 0.3255\u00b1.0583 17.60\u00b11.01 0.3550\u00b1.0726\nSRFormer [46]\n17.20\u00b11.20 0.3150\u00b1.0547 17.43\u00b11.46 0.3296\u00b1.0773 17.10\u00b11.34 0.3032\u00b1.0553 17.58\u00b10.90 0.3603\u00b1.0666\nDAT [5]\n17.52\u00b11.27 0.3542\u00b1.0564 17.58\u00b11.44 0.3216\u00b1.0713 17.44\u00b11.29 0.3453\u00b1.0516 17.75\u00b10.97 0.3422\u00b1.0621\nCFAT [33]\n17.26\u00b11.15 0.3353\u00b1.0530 17.51\u00b11.42 0.3265\u00b1.0759 17.31\u00b11.36 0.3210\u00b1.0525 17.65\u00b10.99 0.3545\u00b1.0725\nMambaIR [11]\n17.60\u00b11.35 0.3614\u00b1.0613 17.70\u00b11.47 0.3236\u00b1.0667 17.67\u00b11.24 0.3579\u00b1.0515 18.10\u00b11.02 0.3577\u00b1.0623\nMambaIRv2 [12] 17.39\u00b11.28 0.3473\u00b1.0560 17.55\u00b11.33 0.3241\u00b1.0695 16.87\u00b11.37 0.3010\u00b1.0584 17.38\u00b10.93 0.3476\u00b1.0712\nMaIR [21]\n17.60\u00b11.25 0.3697\u00b1.0545 17.77\u00b11.43 0.3297\u00b1.0640 17.74\u00b11.22 0.3714\u00b1.0497 18.47\u00b10.89 0.3649\u00b1.0522\nASSAN [24]\n18.00\u00b11.40 0.4074\u00b1.0572 18.45\u00b11.52 0.3717\u00b1.0561 18.03\u00b11.23 0.3984\u00b1.0477 18.59\u00b10.95 0.3740\u00b1.0425\nASBA (Ours)\n19.06\u00b11.49 0.4967\u00b1.0535 19.88\u00b11.42 0.5348\u00b1.0404 18.95\u00b11.33 0.4910\u00b1.0472 20.08\u00b10.74 0.5622\u00b1.0417\nand MIP images. Moreover, because the B-scans are pre-\ndominantly composed of dark background regions that are\nless relevant to flow analysis, we restrict evaluation metrics\non B-scans to pixels whose groundtruth values exceed 5%\nof the maximum possible value (65535 for uint16).\n4.2. Implementation Details\nWe carry out experiments on highly sparsely sampled ODT\nraw B-scans at \u00d78 and \u00d716 sparsity.\nThe implemented\nASBA consists of 4 MP Groups. Each magnitude and phase\nmodule comprises 6 layers. The deep embedding channels\nof both magnitude and phase branches are 60. The input\npatch is of 256 \u00d7 64. The parameters are set as \u03b1 = 0.5,\n\u03b2 = 0.1, and \u03bb = 0.5. The model is trained in batches of\n8 for 100K iterations, with Adam [18] optimizer (\u03b21 = 0.9\nand \u03b22 = 0.999) and CosineAnnealingLR [29] (initial and\nminimum learning rates are 2e\u22124 and 1e\u22126, respectively).\nWe compare ASBA with the classical ODT recon-\nstruction and the recent ASSAN [24].\nIn addition,\nwe include state-of-the-art single image super-resolution\n(SISR) methods\u2014SwinIR [25], HAT [2], SRFormer [46],\nDAT [5], CFAT [33], MambaIR [11], MambaIRv2 [12]\nand MaIR [21]\u2014as they also address an upscaling prob-\nlem. These SISR methods are adapted similarly to ASSAN,\nwhere the sparsely sampled magnitude and phase signals\nare concatenated to form the input patch. For fair compar-\nison, the embedding dimension of all compared methods is\nTable 2. Comparison of vessel segmentation results on the MIP\nimages under \u00d716 sparsity\nMethod\nMCD-AW\nMCD-AN\nDICE\nDICEs\nDICE\nDICEs\nTraditional\n44.93\n38.94\n47.18\n39.94\nSwinIR [25]\n58.83\n47.86\n57.25\n45.67\nHAT [2]\n59.56\n48.18\n60.02\n50.84\nSRFormer [46]\n58.40\n47.01\n59.33\n49.71\nDAT [5]\n60.90\n51.09\n62.91\n54.62\nCFAT [33]\n59.28\n48.20\n61.21\n51.74\nMambaIR [11]\n61.99\n52.64\n62.90\n54.82\nMambaIRv2 [12]\n59.66\n49.31\n58.41\n49.05\nMaIR [21]\n61.39\n51.41\n64.63\n56.44\nASSAN [24]\n66.00\n58.07\n66.65\n59.09\nASBA (Ours)\n70.28\n64.81\n71.93\n66.17\nset to 120, matching the combined channel size of our mag-\nnitude and phase branches. The other network parameters\nand training configurations are kept consistent with ours.\n4.3. Main Results\nQuantitative Results. Table 1 compares ASBA against tra-\nditional ODT reconstruction, SOTA SISR methods, and the\nrecent sparse ODT reconstruction approach under \u00d78 and\n\u00d716 sparsity.\nNotably, under the most challenging \u00d716\nsparsity, ASBA achieves, on average across both datasets,\n6\nTable 3. Ablation Study on A-line ROI SSM (A-RS), B-line Phase\nAttention (B-PA), flow-aware loss (Loss), and other linear com-\nplexity methods for A-line modeling (A-line)\nMethod\nB-scan\nMIP\nPSNR\nSSIM\nPSNR\nSSIM\nA-RS\nw/o ROI\n18.89\n0.4891\n19.61\n0.5240\nw/o Ms\n18.94\n0.4915\n19.70\n0.5321\nROI on C\n18.91\n0.4869\n19.70\n0.5256\nROI on B\n19.06\n0.4967\n19.88\n0.5348\nB-PA\nB-GA\n18.90\n0.4889\n19.70\n0.5278\nw/o ABS\n18.96\n0.4927\n19.67\n0.5310\nw/o Shift\n18.76\n0.4920\n19.56\n0.5263\nB-PA\n19.06\n0.4967\n19.88\n0.5348\nLoss\nw/o wi\n18.15\n0.4132\n18.59\n0.3886\n\u03b1 = 2\n18.09\n0.4477\n19.24\n0.4651\n\u03b1 = 1\n18.81\n0.4779\n19.64\n0.5099\n\u03b1 = 0.5\n19.06\n0.4967\n19.88\n0.5348\nA-line\nMILA [13]\n18.64\n0.4787\n19.35\n0.5025\nVRWKV [6]\n18.49\n0.4715\n19.17\n0.5125\nA-RS (Ours)\n19.06\n0.4967\n19.88\n0.5348\na gain of around 1.0 dB in B-scan PSNR and 1.5 dB in MIP\nPSNR compared to SOTA methods.\nIn addition, ASBA\nconsistently improves SSIM, with an average gain of around\n0.09 on B-scan images and 0.18 on MIP images. All results\ndemonstrate the effectiveness of the proposed ASBA.\nTable 2 presents vessel segmentation results on MIP un-\nder \u00d716 sparsity to evaluate clinical performance.\nWe\nreport DICE scores over the entire image and within a\n256\u00d7256 ROI of each MIP, which primarily contains small\nvessels, denoted as DICE and DICEs, respectively. Because\nmanual vessel annotation is highly labor-intensive, we em-\nploy a UNet [34] pretrained on the OCTA500 dataset [22] to\ngenerate vessel masks for both the groundtruth and sparsely\nreconstructed images.\nThe results indicate that ASBA\nachieves more accurate reconstruction compared to other\nmethods, especially the small vessel flows.\nQualitative Results. Figure 5 provides a visual compar-\nison of MIP reconstructions under \u00d716 sparsity on MCD-\nAW. This high sparsity level poses significant challenges,\nparticularly for recovering slow flows in small vascular\nstructures. Traditional reconstruction methods fail to re-\ncover such flows due to the wide gaps between sampled\nraw A-scans. Similarly, SOTA SISR and sparse ODT re-\nconstruction methods fail to reconstruct flow in small ves-\nsels. In contrast, our ASBA successfully recovers these del-\nicate flow structures, demonstrating its superior capability\nin handling highly sparse inputs and preserving fine vascu-\nlar details. More results are presented in the supplementary.\n4.4. Ablation Study\nAll ablation studies are conducted on MCD-AW at \u00d716\nsparsity. The results are all presented in Table 3.\nA-line ROI SSM. In the absence of ROI guidance, the\nclassical Mamba module is less effective at modeling flow\ninformation, as the ODT B-scan is largely composed of\nbackground regions that contribute minimally to flow re-\nconstruction. In contrast, incorporating guidance from Ms\nenhances performance, since magnitude data is effective for\nidentifying background regions. We also evaluate an alter-\nnative strategy that applies the ROI to the output matrix C\ninstead of the input B of Mamba. However, this approach\nyields inferior results, as the state-space transformation re-\nmains influenced by the background, highlighting the im-\nportance of modulating the input matrix via ROI guidance.\nB-line Phase Attention. First, we compare B-PA with\nthe B-line Gated Attention (B-GA) [24], which directly ap-\nplies 1D attention using a gated mechanism. The results\nshow that incorporating B-line phase differences effectively\nenhances the modeling of long-range flow dependencies\nalong the B-line. Second, computing the absolute value of\nthe B-line phase differences for Q and K improves similar-\nity matching, as the periodic nature of the phase implies that\nthe sign of the difference does not reliably indicate feature\nsimilarity. Finally, omitting the shift operation on the aux-\niliary value feature leads to a performance drop, indicating\nthat proper alignment is essential for accurately mapping\nphase difference features back to the phase domain.\nFlow-aware Weighted Loss. Removing the weight term\nwi leads to a performance drop, as the background noise\ndominating the B-scans distracts the network from effec-\ntively modeling flow features.\nThis issue is exacerbated\nby the raw magnitude and phase supervision signals, which\nare not post-processed and contain significant background\nnoise. Hence, a flow-aware loss function is crucial to make\nthe network focus on flow modeling. We also examine the\neffect of the exponent \u03b1, which controls the concavity of wi.\nCompared to an aggressive weighting strategy that overly\nemphasizes high-flow regions (\u03b1 = 2), a more balanced\nsetting (\u03b1 = 0.5) yields better performance.\nA-line Modeling with Linear Complexity. An A-line\nin ODT is typically long (e.g., 256 pixels), making classi-\ncal self-attention infeasible due to quadratic complexity and\npoor focus on sparse flow regions. To validate the bene-\nfit of our proposed A-RS module for A-line flow modeling,\nwe replace it with two recent linear-complexity methods:\nMILA [13] (linear attention) and VRWKV [6] (attention-\nfree). While MILA reduces computational cost by comput-\ning similarity in a projected space, it remains susceptible\nto background interference. VRWKV\u2019s spatial decay helps\nlimit distant dependencies but uses a single learned decay\nembedding, lacking spatial adaptivity. In contrast, A-RS ex-\nplicitly filters background features through ROI-enhanced\nmodulation, enabling adaptive focus on flow-relevant re-\ngions and yielding superior A-line modeling performance.\nOther Ablation Studies. We further conducted experi-\n7\nFigure 5. Qualitative results of \u00d716 sparsity on MCD-AW dataset. Regions of interest are zoomed in.\nTable 4. Comparison of computational complexity\nMethod\n#param\nFLOPs\nRuntime\nPSNR\nSSIM\nMambaIR\n6.73M\n163.0G\n90.6ms\n18.97\n0.4811\nMambaIRv2\n7.97M\n166.3G\n161.2ms\n18.73\n0.4526\nMaIR\n6.13M\n152.0G\n101.4ms\n19.46\n0.5129\nASSAN\n7.56M\n158.8G\n98.1ms\n19.61\n0.5315\nASBA\n7.07M\n148.0G\n73.9ms\n20.48\n0.5868\nments on other factors of the proposed ASBA. The results\nshow that the two-branch pipeline configuration is reason-\nable, and ASBA is consistently effective on smaller sparsity\nsettings, etc. Details are presented in the supplementary.\n4.5. Computational Complexity Analysis\nTable 8 compares the computational complexity and B-scan\nreconstruction performance on the MCD-AW dataset under\n\u00d78 sparsity. The inference runtime is tested for a 256 \u00d7 64\ninput patch on an RTX A6000 GPU. Compared with other\nSSM-based methods [11, 12, 21, 24], the proposed ASBA\nmodel achieves the best reconstruction performance while\nrequiring the lowest number of FLOPs and inference time.\nThese results demonstrate that ASBA achieves superior\nperformance on both effectiveness and computational effi-\nciency. More results are presented in supplementary.\n5. Conclusion\nWe propose ASBA (A-line State space and B-line Atten-\ntion), a flow-aware network to reconstruct ODT images\nfrom highly sparsely sampled raw A-scans, hence reduc-\ning scanning time and storage requirement significantly.\nSpecifically, we develop a two-branch architecture to pro-\ncess magnitude and phase separately, an A-line ROI SSM\nblock to guide attention to flow regions, a B-line Phase At-\ntention block for phase difference, and a flow-aware loss to\nadaptively emphasize flow regions during training. Exten-\nsive experiments validate the effectiveness of ASBA.\n8\nReferences\n[1] Keyan Chen, Bowen Chen, Chenyang Liu, Wenyuan Li,\nZhengxia Zou, and Zhenwei Shi. Rsmamba: Remote sens-\ning image classification with state space model. IEEE Geo-\nscience and Remote Sensing Letters, 2024. 3\n[2] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao,\nand Chao Dong.\nActivating more pixels in image super-\nresolution transformer. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n22367\u201322377, 2023. 2, 6, 1\n[3] Zhongping Chen and Jun Zhang. Doppler optical coherence\ntomography. Optical Coherence Tomography: Technology\nand Applications, pages 621\u2013651, 2008. 1\n[4] Zhongping Chen, Thomas E Milner, Shyam Srinivas, Xiao-\njun Wang, Arash Malekafzali, Martin JC van Gemert, and\nJ Stuart Nelson. Noninvasive imaging of in vivo blood flow\nvelocity using optical doppler tomography. Optics letters, 22\n(14):1119\u20131121, 1997. 2\n[5] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xi-\naokang Yang, and Fisher Yu. Dual aggregation transformer\nfor image super-resolution. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 12312\u2013\n12321, 2023. 2, 6, 1\n[6] Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei\nLu, Tong Lu, Yu Qiao, Hongsheng Li, Jifeng Dai, and Wen-\nhai Wang."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThe paper introduces ASBA (A-line ROI State Space Model and B-line phase Attention), a novel network architecture for reconstructing Optical Doppler Tomography (ODT) images from highly sparsely sampled raw A-scans. The key contributions are:\n1. **Flow-Aware Reconstruction**: ASBA explicitly models flow information using an A-line Region-of-Interest (ROI) State Space Model and B-line phase attention, addressing the limitations of previous sparse reconstruction methods.\n2. **Efficient Magnitude and Phase Handling**: It separately processes magnitude and phase signals to leverage their distinct roles in ODT reconstruction.\n3. **Adaptive Feature Modulation**: The system dynamically modulates input matrices for state space models to highlight flow regions while suppressing background noise effectively.\n\n#### Methodology\nThe proposed ASBA architecture consists of:\n1. **Two-Branch Framework**: Separates magnitude and phase information processing, with the latter focusing on flow-aware modeling.\n2. **A-line ROI State Space Model (A-RS)**: Models flow-relevant features along each A-line by selectively extracting relevant regions using a modulated Mamba model.\n3. **B-line Phase Attention (B-PA)**: Captures long-range dependencies in B-lines guided by phase differences, ensuring accurate feature extraction.\n\n#### Key Findings\nThe extensive experiments on real animal datasets demonstrate:\n1. **Improved Reconstruction Quality**: ASBA outperforms existing state-of-the-art methods significantly, achieving higher PSNR and SSIM scores.\n2. **Better Handling of Sparse Data**: Under \u00d78 and \u00d716 sparsity conditions, ASBA achieves substantial gains in B-scan PSNR (around 1.0 dB) compared to SOTA methods.\n3. **Clinical Relevance**: The method effectively reconstructs delicate flow structures in small vascular structures, highlighting its superior capability in handling highly sparse inputs.\n\n#### Conclusions\nASBA significantly reduces scanning time and storage requirements by efficiently reconstructing ODT images from sparsely sampled raw A-scans. Its effectiveness is validated through extensive experiments on real animal datasets, demonstrating that ASBA effectively captures flow information and outperforms existing methods. The proposed architecture offers a robust solution for sparse optical Doppler tomography reconstruction, addressing the limitations of traditional dense sampling approaches.\n\n### Detailed Breakdown\n\n#### Introduction\nThe paper addresses the challenges of current ODT techniques requiring dense spatial sampling to accurately capture blood flow dynamics, leading to long acquisition times and substantial data storage demands. It introduces ASBA as a novel network architecture designed for reconstructing ODT images from highly sparsely sampled raw A-scans.\n\n#### Methodology Overview\nASBA comprises:\n1. **Two-Branch Framework**: Separates magnitude and phase information processing.\n2. **A-line ROI State Space Model (A-RS)**: Models flow-relevant features along each A-line by selectively extracting relevant regions using a modulated Mamba model.\n3. **B-line Phase Attention (B-PA)**: Captures long-range dependencies in B-lines guided by phase differences.\n\n#### Key Components\n1. **Two-Branch Architecture**: Separates magnitude and phase information processing, with the latter focusing on flow-aware modeling.\n2. **A-line ROI State Space Model (A-RS)**: Models flow-relevant features along each A-line by selectively extracting relevant regions using a modulated Mamba model.\n3. **B-line Phase Attention (B-PA)**: Captures long-range dependencies in B-lines guided by phase differences.\n\n#### Experiments and Results\nExperiments on real animal datasets show:\n1. **Improved Reconstruction Quality**: ASBA outperforms existing state-of-the-art methods significantly, achieving higher PSNR and SSIM scores.\n2. **Better Handling of Sparse Data**: Under \u00d78 and \u00d716 sparsity conditions, ASBA achieves substantial gains in B-scan PSNR (around 1.0 dB) compared to SOTA methods.\n\n#### Conclusions\nASBA effectively reconstructs ODT images from highly sparsely sampled raw A-scans, significantly reducing scanning time and storage requirements. Its architecture addresses the limitations of traditional dense sampling approaches by explicitly modeling flow information using an A-line ROI State Space Model and B-line phase attention."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "NewsRECON: News article REtrieval for image CONtextualization\nJonathan Tonglet1,2,3, Iryna Gurevych1, Tinne Tuytelaars2, Marie-Francine Moens3\n1 Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science,\nTU Darmstadt and National Research Center for Applied Cybersecurity ATHENE\n2 Department of Electrical Engineering, KU Leuven\n3 Department of Computer Science, KU Leuven\njonathan.tonglet@kuleuven.be\nAbstract\nIdentifying when and where a news image was\ntaken is crucial for journalists and forensic ex-\nperts to produce credible stories and debunk\nmisinformation. While many existing meth-\nods rely on reverse image search (RIS) en-\ngines, these tools often fail to return results,\nthereby limiting their practical applicability.\nIn this work, we address the challenging sce-\nnario where RIS evidence is unavailable. We\nintroduce NewsRECON, a method that links\nimages to relevant news articles to infer their\ndate and location from article metadata. News-\nRECON leverages a corpus of over 90,000 ar-\nticles and integrates: (1) a bi-encoder for re-\ntrieving event-relevant articles; (2) two cross-\nencoders for reranking articles by location and\nevent consistency. Experiments on the TARA\nand 5Pils-OOC show that NewsRECON out-\nperforms prior work and can be combined with\na multimodal large language model to achieve\nnew SOTA results in the absence of RIS evi-\ndence. We make our code available.1\n1\nIntroduction\nImages are frequently used to communicate news\nevents, especially on social media platforms. How-\never, they are often shared with missing or incorrect\nmetadata, such as their date or location. These in-\naccuracies play a significant role in the spread of\nonline misinformation (Dufour et al., 2024). As a\nresult, journalists, fact-checkers, and forensic ex-\nperts must frequently verify or recover the metadata\nof online images, i.e., image contextualization, a\ntask that is crucial for several applications, includ-\ning reliable news coverage, forensic investigation\nof war crimes (Silverman, 2013), or early misin-\nformation detection (Puliafito, 2025; Khan et al.,\n2024). A common approach involves using reverse\nimage search (RIS) tools to retrieve web pages con-\ntaining the image, and inferring the date or location\n1github.com/jtonglet/arxiv2025-newsrecon\nFigure 1: NewsRECON retrieves articles that are loca-\ntion or event-relevant, given an image as query.\nfrom the surrounding page content. However, RIS\nengines often fail to return results (Tonglet et al.,\n2024, 2025). In such cases, identifying the image\u2019s\ndate and location becomes substantially more dif-\nficult, requiring journalists to interpret visual cues\nand gather external evidence. Due to time and hu-\nman resource constraints, these cases are also more\ninteresting to automate.\nSeveral works have proposed methods to auto-\nmate image contextualization (Fu et al., 2022; Shi\net al., 2024; Tonglet et al., 2024; Ayyubi et al.,\n2025; Phan et al., 2025; Siingh et al., 2025; Tonglet\net al., 2025). Some of them tackle the challeng-\ning scenario where RIS engines are unavailable,\nrelying solely on the visual content of the image\n(Fu et al., 2022; Siingh et al., 2025) or leveraging\nencyclopedic knowledge bases (Shi et al., 2024).\nHowever, none of these methods leverage large-\nscale news article corpora, which represent a rich\nsource of contextual evidence (Fu et al., 2022).\nIn this work, we introduce NewsRECON, the\nfirst image contextualization method leveraging\nnews articles in scenarios where RIS engines are\nunavailable. As illustrated in Figure 1, NewsRE-\nCON searches a large news corpus to identify rel-\nevant articles for a given news image, i.e., articles\nthat relate to the same location or event, where an\nevent is defined as the combination of a date and\nlocation. The metadata of these articles, including\npublication dates and geolocation keywords, can\narXiv:2601.14121v1  [cs.CL]  20 Jan 2026\nthen be used as predictions of the image\u2019s date and\nlocation. We construct a corpus of 91,376 news\narticles, spanning the years 2010 to 2023. News-\nRECON comprises three main components: (1) a\nbi-encoder that retrieves the top-K event-relevant\narticles for a given image, (2) a cross-encoder that\nreranks these articles based on their location simi-\nlarity to the image, (3) a second cross-encoder that\nreranks clusters of articles by their event consis-\ntency with the image. Experiments on the TARA\nbenchmark show that NewsRECON outperforms\nretrieval-based baselines (Fu et al., 2022) by 9.7\npercentage points (pp) in GREAT score (Siingh\net al., 2025), a joint metric for date and location\nprediction. Moreover, providing the top-3 articles\nas input to a multimodal large language model\n(MLLM) yields the new SOTA results. We further\ndemonstrate that NewsRECON, after being trained\non TARA, generalizes to the 5Pils-OOC bench-\nmark (Tonglet et al., 2025), despite geographic and\nstylistic shifts. When combined with an MLLM, it\nbecomes the strongest method for images lacking\nRIS web evidence.\n2\nRelated work\nJournalists and forensic experts contextualize news\nimages by identifying key context items such as\nthe source, date, location, and motivation (Urbani,\n2020; Tonglet et al., 2024; Phan et al., 2025). Ap-\nplications of this process include fact-checking im-\nages that have been taken out of context to spread\nmisinformation (Tonglet et al., 2025), or identify-\ning evidence for war crimes investigations (Silver-\nman, 2013). In this work, we focus on two core\ncontext items: the date and location, which are the\nmost widely studied in the literature.\nSeveral datasets have been proposed for this task\n(Fu et al., 2022; Tonglet et al., 2024; Geng et al.,\n2025). Existing methods differ in both their model\narchitectures and the types of external evidence\nthey use. Some approaches formulate image con-\ntextualization as a classification task, fine-tuning\nCLIP (Radford et al., 2021) to retrieve the best\nmatching candidate date-location pairs present in\nthe same dataset split (Fu et al., 2022; Shi et al.,\n2024). However, this setup lacks realism: the can-\ndidate set is limited to a small number of date-\nlocation pairs found in the split, whereas in real-\nworld scenarios, there are thousands of unique\ndates and an even larger number of locations, re-\nsulting in a near-infinite search space.\nOther works treat it as a text generation task and\nrely on (M)LLMs to generate the answers (Ton-\nglet et al., 2024; Phan et al., 2025; Zhang et al.,\n2024; Ayyubi et al., 2025; Siingh et al., 2025; Ton-\nglet et al., 2025). Some of these approaches use\nRIS engines to retrieve external evidence (Tonglet\net al., 2024; Phan et al., 2025; Tonglet et al., 2025).\nHowever, RIS engines may return no results (Ab-\ndelnabi et al., 2022; Tonglet et al., 2024), particu-\nlarly for newly published or less widely circulated\nimages. The absence of RIS evidence makes the\ntask far more challenging but also more valuable\nto automate, as it requires interpreting subtle vi-\nsual cues and relating them to world knowledge.\nOther methods explore alternative sources of exter-\nnal evidence, such as Wikipedia (Shi et al., 2024;\nTonglet et al., 2025) or celebrity recognition APIs\n(Siingh et al., 2025). However, none of the exist-\ning methods leverage news articles, despite their\nstrong potential as a source of temporal and spatial\ncontext (Fu et al., 2022).\n3\nNewsRECON\nNewsRECON, illustrated in Figure 2, predicts the\ndate and location of an image by returning ranked\nlists of relevant news articles. The inference-time\npseudocode is provided in Appendix A.\nRelevant articles labeling\nFor each image, we\ndefine the set of relevant articles. These binary la-\nbels serve as weak supervision signals for training\nand validation. For each image, we construct two\nrelevance sets: one for location and one for event,\nwhere an event is defined as the combination of a\nspecific date and location. We do not define a rele-\nvance set based solely on date, as date prediction\ntypically relies on establishing the correct location\nfirst. The event set is always a subset of the loca-\ntion set. For the location, an article is relevant if\nat least one of its geolocation keywords contains\nthe ground truth. For the event, the same location\ncondition holds, and the publication date of the ar-\nticle must fall within a window of \u00b1Nwindow days\naround the original publication date of the image.\nThis temporal window ensures a sufficient number\nof event-relevant articles for training, since exact\nday-level matches are too limited.\nBi-encoder\nThe first retrieval phase uses a bi-\nencoder trained to retrieve event-relevant articles.\nWe adopt CLIP (Radford et al., 2021) as the base\nmodel. The bi-encoder retrieves the top-K arti-\nVision\nencoder\nText\nencoder\nThe presidential palace\nin Haiti lies in ruins [\u2026]\nAn image from Haiti\nVision\nencoder\nText\nencoder\nMatch\nNon-match\nAn image between\n2010-01-08 and\n2010-01-15 in Haiti\nVision\nencoder\nText\nencoder\nMatch\nNon-match\n(a) Bi-encoder fine-tuning\n(b) Location cross-encoder fine-tuning\n(c) Event cross-encoder fine-tuning\n(d) Retrieval pipeline\nTop K\narticles\nLocation\nreranking\nBi-encoder\nLocation\ncross-encoder\nArticle\nclustering\nEvent\ncross-encoder\nTime\nreranking\nNews articles\ncorpus\nFigure 2: Overview of NewsRECON. Parts a to c illustrate the three training stages with images from TARA (Fu\net al., 2022). Part d shows the retrieval pipeline at inference time.\ncles for an image based on the cosine similarity\nbetween their embeddings. Rather than encoding\nthe full article text, we use its news caption as input\nto the text encoder. News captions focus on the\nvisual elements directly associated with the event\ndescribed in the article. They are generated for\neach article when creating the corpus, as explained\nin Section 4.2. We fine-tune the encoders using a\nsymmetric InfoNCE contrastive loss. It computes\na softmax over all candidate captions in the batch,\nencouraging the bi-encoder to produce embeddings\nwith high cosine similarity between the image and\nthe captions of relevant articles.\nEach training batch contains pairs of the form\n(train set image, caption of an event-relevant arti-\ncle). Pairs are added to the batch one at a time.\nA set of forbidden captions is incrementally up-\ndated to ensure that no caption in the batch is event-\nrelevant for more than one image. The bi-encoder\nshould learn representations for all events in the\nnews corpus, not only those covered by the train\nset images. Hence, we reserve a fraction nrandom of\neach batch for randomly sampled articles from the\nnews corpus that are not event-relevant to any train\nset image. For these articles, we create a pair using\ntheir own image and the corresponding news cap-\ntion. After each epoch, we evaluate the bi-encoder\non the dev set using Recall@K (R@K). We retain\nthe checkpoint corresponding to the best epoch.\nLocation cross-encoder\nThe next step is to\nrerank the articles retrieved by the bi-encoder us-\ning a location-focused cross-encoder. For each\narticle, we create a template sentence \u201cAn image\nfrom LOCATION\u201d, where LOCATION is a list of\nthe article\u2019s geolocation keywords. We use frozen\nCLIP image and text encoders. Their embeddings\nare combined and passed through a trainable linear\nlayer. The output is the similarity score between\nthe image and the article.\nTraining pairs are created as follows. For each\nimage, we take the top-K articles returned by the\nbi-encoder. If at least one of them is location-\nrelevant, the image is included in the cross-encoder\ntraining data. For each such image, we sample\nfrom the top-K one location-relevant article and\nup to Nnegative irrelevant ones, with distinct loca-\ntion keywords, to form training pairs. At the end\nof each epoch, the cross-encoder is evaluated on\nthe dev set using R@K. We retain the checkpoint\nfrom the best epoch. At inference time, the final\nlocation ranking is obtained by multiplying the bi-\nencoder score by the location cross-encoder score.\nThe pipeline stops here for location prediction.\nArticle clustering\nThe top-K articles retrieved\nby the bi-encoder are grouped into clusters, based\non three rules: (1) all articles within a cluster share\nat least one location keyword; (2) the temporal span\nof the cluster, in terms of publication dates, must\nbe less than or equal to 2 Nwindow + 1 days; (4) a\ncluster contains at least Nmin_size articles. A cluster\nis relevant for an image if it contains at least one\nevent-relevant article.\nEvent cross-encoder\nThe final step is to rerank\nthe clusters using an event-focused cross-encoder.\nIf the top-K articles retrieved by the bi-encoder re-\nsult in fewer than two clusters, we skip this step and\nretain the bi-encoder ranking. For each cluster, we\nconstruct a template sentence of the form \u201cAn im-\nage between START_DATE and END_DATE in LO-\nCATION\u201d, where START_DATE and END_DATE\nare the earliest and latest publication dates among\nthe articles in the cluster, and LOCATION is the\nintersection of their geolocation keywords. The\nmodel architecture and training procedure are iden-\ntical to the location cross-encoder. At the end of\neach epoch, we evaluate the event cross-encoder on\nthe dev set using R@K. We retain the checkpoint of\nthe best epoch. At inference time, the date ranking\nis determined by this cross-encoder: we take the\narticle with the highest bi-encoder score from the\nhighest-ranked cluster, followed by the article with\nthe highest bi-encoder score from the second clus-\nter, and so on. Articles that do not belong to any\ncluster are placed at the end of the new ranking.\n4\nExperiments\n4.1\nDatasets\nTARA\nTARA (Fu et al., 2022) contains 15,429\nnews images sourced from The New York Times\narticles. It is divided into train, dev, and test splits.\nMost images are from the 2010s, with the most\nrecent ones dating back to 2021.\n5Pils-OOC\n5Pils-OOC (Tonglet et al., 2024,\n2025) is a test set containing 624 images sourced\nfrom fact-checking articles. These images were\nused in actual cases of misinformation. Compared\nto TARA, 5Pils-OOC includes more images from\nnon-Western regions, particularly India and Eastern\nAfrica, and features more recent examples, includ-\ning 150 images from 2022 and 2023.\n4.2\nNews article corpus\nWe collect news articles from two reputable news-\npapers, The New York Times (NY Times) and The\nGuardian (Guardian). The process is illustrated in\nFigure 3. Each article is stored as a dictionary con-\ntaining metadata, headline, lead paragraph, or full\ntext, and image URLs. The collection of the corpus\nmakes use of the LLM Qwen2.5-7B (Qwen-Team,\n2024). All prompts are provided in Appendix B.\nFigure 3: News articles corpus creation process.\nNY Times\nWe use the official NY Times Archive\nAPI,2 which returns all articles for a given month.\nWe collect articles from 2010 to 2023, covering\nthe same period as TARA and 5Pils-OOC. Since\nTARA images originate from NY Times articles,\nwe remove all articles that contain any of the dev or\ntest images. NY Times articles that contain images\nfrom the train set are excluded during training but\nincluded for evaluation.\nGuardian\nExcluding NY Times articles associ-\nated with TARA images may remove relevant cov-\nerage of certain events. To compensate for this\nloss, we query the Guardian API3 for matching ar-\nticles. For each TARA image, we use the ground\ntruth date (if available) and the keywords from the\noriginal NY Times article. For each 5Pils-OOC\nimage, we use the ground truth date (if available)\nand extract keywords from the image caption using\nthe spaCy model en_web_core_lg for named entity\nrecognition. Each query returns up to 20 articles.\nFiltering\nWe retain only articles that describe\nreal-world events that can be reported on with im-\nages. We prompt Qwen2.5-7B with eight hand-\ncrafted few-shot examples to classify whether an\narticle describes a visualizable event, based on\nits news headline. For example, the article \"On\nnearly every front, President Obama\u2019s goal of\nlower deficits has gotten harder since his first bud-\nget a year ago\" is removed from the corpus by the\nfilter. Appendix C provides a qualitative analysis\nof the filtering step.\n2developer.nytimes.com/docs/archive-\nproduct/1/overview\n3open-platform.theguardian.com/\nNews caption generation\nThe headline of an\narticle often does not provide a clear visual de-\nscription, and many articles could be illustrated\nwith different types of images. To address this, we\ngenerate up to five news captions for each article,\ndescribing plausible images that could accompany\nit. These captions are generated using Qwen2.5-7B,\nprompted with eight hand-crafted few-shot demon-\nstrations. These captions are used in the bi-encoder\nto represent the articles.\nCorpus statistics and variants\nThe intermediate\ncorpus contains 415,193 articles. After filtering,\nthe final corpus contains 91,376 articles, of which\n77,531 are from the NY Times and 13,845 from the\nGuardian. For TARA, we exclude all articles pub-\nlished after 2021, as the images in these sets do not\ncover the years 2022\u20132023. For 5Pils-OOC, we in-\nclude the full set of articles, allowing us to evaluate\nhow well NewsRECON generalizes to images and\narticles from more recent years without retraining.\nAppendix D provides additional statistics on the\nnews articles corpus variants.\n4.3\nMetrics\nWe use several metrics introduced in prior work on\nimage contextualization.\nExact match at K (EM@K) measures whether\nat least one of the top-K retrieved items matches\nthe ground truth.\nExample-F1 (E-F1) (Fu et al., 2022) computes\na Dice coefficient based on the overlap between\nthe sets of hierarchical chains from child to parent\nderived from the ground truth and the prediction.\nFor instance, the prediction \u201cParis, France\u201d, corre-\nsponds to the hierarchical chains {(Paris, France,\nEurope), (France, Europe), (Europe)}.\n\u2206(Tonglet et al., 2024) is a metric for date pre-\ndiction, where the score is inversely proportional to\nthe absolute distance in years between the predic-\ntion and ground truth. CO\u2206(Tonglet et al., 2024)\nis the corresponding metric for location, defined as\nthe inverse of the Haversine distance, in 1000 km,\nbetween the predicted and ground truth coordinates.\nThe Haversine distance is a standard measure of the\ngreat-circle distance between two locations based\non their coordinates on a sphere.\nGREAT (Siingh et al., 2025) is the weighted\naverage of two similarity scores: one for location\nand one for date. Both range from 0 to 1, where\nhigher values indicate a prediction closer to the\nground truth.\nThe location score is computed as\nGREATloc = max\n\u0012\n0, 1 \u2212\nd\n1000\n\u0013\nwhere d is the Haversine distance.\nThe date score at a given granularity u \u2208{cen-\ntury, decade, year, month, day} is defined as\nSu =\n(\n1 if gtu = predu else 0\nif u = century\nmax\n\u0010\n0, 1 \u2212|gtu\u2212predu|\nTu\n\u0011\notherwise\nwhere gtu and predu are the ground truth and\npredicted values, and Tu is a granularity-specific\nthreshold, e.g., Tday = 15. The final GREATdate\nscore is computed as the weighted average of the\nscores at each granularity.\nUnlike prior work (Fu et al., 2022), we do not\npenalize predictions that are more specific than\nthe ground truth. Instead, we compute the scores\nstarting from the lowest available granularity in the\nground truth. For example, if the ground truth is\ngiven a year, we do not consider the day and month\npredictions. This choice is motivated by the fact\nthat the lowest ground truth granularity is often\narbitrary and does not accurately reflect the level\nof precision that can be achieved in practice.\n4.4\nBaselines\nCLIP+ (Fu et al., 2022) CLIP to select the most\nrelevant date and location labels from the set of can-\ndidates in the split. CLIP+ is fine-tuned on TARA.\nThis model uses a simplified setup in which the\ncandidate date-location pairs are restricted to those\npresent in the test set. To enable a fair compari-\nson, we also report the results of NewsRECON on\nTARA after mapping the predicted date to the clos-\nest available date in the test set. We do not compare\nagainst QR-CLIP (Shi et al., 2024), which extends\nCLIP+ by leveraging Wikipedia passages as evi-\ndence, as neither the Wikipedia corpus nor the code\nwere publicly available at the time of submission.\nWe also prompt MLLMs to predict the date and\nlocation in different settings. We evaluate two pop-\nular open-weight MLLMs: InternVL3-8B (Zhu\net al., 2025) and Qwen2.5VL-7B (Qwen-Team,\n2025). The default setting is without external evi-\ndence. The celebrity setting includes in the prompt\nthe list of celebrities detected in the image with\nthe Amazon Rekognition API.4 The GETReason\n4docs.aws.amazon.com/rekognition/latest/dg/celebrities.html\nDate\nLocation\nEM@1\nEM@5\nE-F1\n\u2206\nGREATdate\nEM@1\nEM@5\nE-F1\nCO\u2206\nGREATloc\nGREAT\nCLIP+ (simplified setup)\n(8.1)\n(23.3)\n(44.6)\n(38.2)\n(55.2)\n(19.4)\n(35.0)\n(38.7)\n(19.7)\n(32.3)\n(43.8)\nNewsRECON (simplified setup)\n(11.6)\n(30.2)\n(47.4)\n(43.5)\n(58.2)\n(30.9)\n(52.9)\n(53.3)\n(32.2)\n(48.7)\n(53.5)\nNewsRECON\n3.7\n11.1\n43.5\n42.2\n59.1\n29.1\n41.7\n55.5\n30.8\n49.5\n54.3\nInternVL3-8B - default\n2.0\n-\n42.0\n37.5\n56.3\n25.4\n-\n48.3\n26.3\n41.4\n48.9\nInternVL3-8B - celebrity\n2.2\n-\n42.7\n39.2\n57.5\n27.9\n-\n50.4\n28.9\n44.2\n50.9\nInternVL3-8B - GETReason\n0.6\n-\n27.1\n15.1\n33.8\n17.9\n-\n35.1\n18.9\n32.1\n33.0\nInternVL3-8B - NewsRECON\n4.4\n-\n45.4\n44.1\n60.4\n35.2\n-\n58.5\n36.9\n56.0\n58.2\nQwen2.5VL-7B - default\n3.2\n-\n39.4\n40.7\n59.3\n32.5\n-\n56.7\n34.2\n50.5\n54.9\nQwen2.5VL-7B - celebrity\n3.3\n-\n40.1\n41.7\n59.7\n32.8\n-\n57.0\n34.5\n51.6\n55.7\nQwen2.5VL-7B - GETReason\n2.0\n-\n38.6\n33.6\n58.5\n32.2\n-\n54.0\n34.0\n49.5\n54.0\nQwen2.5VL-7B - NewsRECON\n5.8\n-\n46.8\n47.6\n62.1\n38.4\n-\n61.6\n40.3\n58.7\n60.4\nTable 1: Main results on the TARA test set (%). Results in brackets are obtained under the simplified setup where\nthe choice of labels is limited to the date-location pairs present in the test set.\nsetting uses the GETReason multi-step prompting\nframework (Siingh et al., 2025). Finally, we also\nstudy a NewsRECON setting in which the top-3 ar-\nticles retrieved by NewsRECON serve as evidence\nto an MLLM. Appendix E discusses the impact of\nusing smaller and larger models of the InternVL3\nand Qwen2.5VL families.\nOn 5Pils-OOC, we further compare NewsRE-\nCON to COVE (Tonglet et al., 2025), the SOTA\nmethod for image contextualization. Unlike other\nbaselines, COVE leverages RIS evidence to predict\nthe image\u2019s date and location. We use the default\nLLMs of COVE, i.e., LlavaNext-7B (Liu et al.,\n2023, 2024) for object captioning and Llama3-8B\n(MetaAI, 2024) for date and location prediction. It\nis not meaningful to evaluate COVE on TARA, as\nthe RIS engine would trivially return the NY Times\narticle from which the image was collected.\n4.5\nImplementation details\nAll models are evaluated in a single run.\nAp-\npendix F evaluates NewsRECON with different\nseeds. MLLMs are prompted with a temperature of\n0 to make the results more deterministic. All exper-\niments are run on two A100 GPUs with 80GB of\nmemory. Appendix G provides the hyperparameter\nvalues, tuned on the TARA dev set.\n4.6\nResults on TARA\nTable 1 provides the results on the TARA test set.\nInsight 1: NewsRECON outperforms CLIP+\nunder the simplified setup. While CLIP+ appears\nto achieve the best date EM@1, this result comes\nfrom a constrained candidate set, where they select\namong dates present in the test split. Other mod-\nels operate in a more realistic, open-ended setting.\nHowever, in the location task, CLIP+ performs the\nworst across all metrics. Once adjusted to the sim-\nplified setup, NewsRECON beats CLIP+ by 3.6\npp in date EM@1. Whether it is CLIP+ or News-\nRECON, the performance gap with other results\nindicates that the simplified setup underestimated\nthe task\u2019s difficulty.\nInsight 2: NewsRECON is competitive with\nMLLM baselines.\nDespite being significantly\nsmaller, NewsRECON performs competitively\nwith the MLLM baselines. It outperforms them\nunder the default and celebrity settings for date\nprediction. For the location task, NewsRECON\noutperforms InternVL3 but falls slightly behind\nQwen2.5VL. We hypothesize that location pre-\ndiction is slightly easier than date prediction for\nMLLMs, thanks to their parametric knowledge.\nInsight 3: MLLMs benefit from the articles\nretrieved by NewsRECON. For both MLLMs,\nthe best performance is achieved when articles\nretrieved by NewsRECON are used as external\nevidence. This leads to large improvements in\nGREAT score: +9.2 pp for InternVL3 and +5.5\npp for Qwen2.5VL. These results establish the new\nSOTA and highlight the complementarity of News-\nRECON for evidence retrieval and the MLLM for\nanswer generation.\nAn analysis of time efficiency is provided in\nAppendix H. Appendix I reports the performance of\nNewsRECON by continent and two-year intervals.\n4.7\nResults on 5Pils-OOC\nFigure 4 shows the results on 5Pils-OOC using\nNewsRECON, the best MLLM on the TARA test\nset, Qwen2.5VL, their combination, as well as the\nCOVE baseline.\nInsight 4: Qwen2.5VL with NewsRECON out-\nperforms COVE when RIS evidence is unavail-\nable. COVE relies on RIS evidence, explaining its\noverall superior performance. However, NewsRE-\nAll images\nImages w/o RIS evidence\n0\n10\n20\n30\n40\n50\n60\n70\nGREAT date\nAll images\nImages w/o RIS evidence\nGREAT location\nAll images\nImages w/o RIS evidence\nGREAT overall\nCOVE\nQwen2.5VL-base\nNewsRECON\nQwen2.5VL-NewsRECON\nFigure 4: Results on 5Pils-OOC (%). \u201cAll images\u201d refers to the entire dataset (N=624), while \u201cImages w/o RIS\nevidence\u201d corresponds to the subset where RIS engines do not return any webpages as evidence (N=172).\nEvent (date+location)\nLocation\nR@1\nR@5\nR@1\nR@5\nFrozen bi-encoder\n1.4\n4.0\n14.6\n32.6\nw. bi-encoder fine-tuning\n1.7\n4.6\n18.6\n36.0\nw. location cross-encoder\n1.5\n4.9\n28.6\n42.9\nw. event cross-encoder\n2.1\n5.4\n20.2\n32.7\nTable 2: R@K of relevant articles on the TARA dev set\nwith different versions of NewsRECON (%).\nCON was designed for cases where no RIS results\nare available. In such scenarios, the combination of\nQwen2.5VL and NewsRECON outperforms COVE\nby more than 10 pp in GREAT score, demonstrat-\ning that the automated retrieval of relevant news\narticles is a strong alternative when RIS evidence\nis missing. This is particularly valuable to human\nfact-checkers and journalists, as such cases typi-\ncally require more time and effort to verify.\nInsight 5: NewsRECON generalizes to out-\nof-distribution data. Although a drop in perfor-\nmance is observed relative to the TARA test set,\ndue to domain shifts in geography and image style\ncompared to the TARA train set, NewsRECON\ngeneralizes well to 5Pils-OOC. It remains stronger\nthan Qwen2.5VL for date prediction and slightly\nweaker for location. Qwen2.5VL\u2019s GREAT score is\nimproved by 3.8 pp when combined with NewsRE-\nCON, confirming that the retrieved articles provide\nvaluable evidence to the MLLM.\n4.8\nAnalysis\n4.8.1\nAblations\nTable 2 reports ablation results on the TARA dev\nset using R@K. Fine-tuning the bi-encoder yields\na gain of 4 pp in R@1 for location and 0.3 pp for\nevent, highlighting the greater difficulty of retriev-\ning event-relevant articles, which require the cor-\nBefore fine-tuning\nAfter fine-tuning\nAbstract\n38.2\n42.6\nImage\n25.5\n29.5\nNews caption\n37.1\n43.7\nTable 3: R@100 of event-relevant articles with different\nbi-encoder input types on the dev set (%).\nrect time window. The location cross-encoder leads\nto a substantial improvement of 10 pp in R@1 and\n6.9 pp in R@5. In contrast, the event cross-encoder\nyields smaller gains, 0.4 and 0.8 pp for R@1 and\nR@5, respectively. For both cross-encoders, the im-\nprovement over the fine-tuned bi-encoder exceeds\nthe gains achieved through bi-encoder fine-tuning\nalone, underscoring their importance in the overall\npipeline. Cross-encoders outperform each other for\ntheir respective tasks.\n4.8.2\nImpact of input type for bi-encoders\nWe use news captions to represent each article for\nthe bi-encoder stage. However, other representa-\ntions are possible, such as the abstract or the im-\nage illustrating the article. Table 3 compares the\nR@100 obtained with different input types on the\nTARA dev set. While the best input type before\nfine-tuning is the abstract, the score with news cap-\ntions is 1.1 pp higher after fine-tuning. We hy-\npothesize that this is because news captions are fo-\ncused on describing the visual aspects of the event,\nwhereas the abstract is often a more general sum-\nmary of the event. Using the article\u2019s image yields\na lower R@100, both before and after fine-tuning.\nWe assume this is because images contain more\nmisleading cues, such as a shared flag between two\nimages, which may correctly indicate the location\nbut not the specific event.\nCairo, Egypt\n2013-10-04\nNewsRECON\nGround truth\nMLLM-default\nMLLM-\nNewsRECON\n2013-05-01\nS\u00e3o Paulo,\nBrazil\nKirkuk, Irak\n2011-02-09\nBaghdad, Irak\n2011-06-08\n2015-04-28\nS\u00e3o Paul,\nBrazil\nBaghdad, Irak\n2007-04-05\nCairo, Egypt\n2013-02-01\n2013-04-30\nCairo, Egypt\nCairo, Egypt\n2010-01-26\nCairo, Egypt\n2013-02-01\n2013-04-27\nCairo, Egypt\nBaghdad, Irak\n2007-03-05\nFigure 5: Predictions of different models for four instances of the TARA test set.\nSize\nGREATdate\nGREATloc\nGREAT\nFull corpus\n77,074\n59.5\n51.6\n55.6\n2/3 of corpus\n51,382\n58.9\n50.8\n54.9\n1/3 of corpus\n25,691\n58.8\n50.0\n54.4\nTable 4: GREAT scores on the TARA dev set for differ-\nent size of the news articles corpus (%).\n4.8.3\nImpact of corpus size\nTable 4 examines how the size of the retrieval cor-\npus affects GREAT scores. We randomly shuffle\nthe corpus and split it into three. We then evaluate\nthree corpus sizes: one-third, two-thirds, and the\nfull corpus. Reducing the corpus size negatively\naffects both date and location scores. However, the\noverall decrease is only 1.2 pp in GREAT score,\nindicating that the corpus contains redundancy in\nterms of locations and events, and that a smaller\nsubset is still sufficient to predict many dates and\nlocations accurately. The linear trend for the loca-\ntion score suggests that scaling the corpus beyond\nits current size could yield small but steady gains in\nperformance. Similarly, the date scores suggest that\nfurther improvement can be obtained by scaling the\ncorpus, to a more limited extent.\n4.8.4\nQualitative analysis\nWe analyze predictions of Qwen2.5VL, NewsRE-\nCON, and their combination, on a random sample\nof 50 instances from the TARA test set. Figure 5\npresents four examples. We observe 15 incorrect\npredictions for location and 36 for date. For the lo-\ncation task, all three approaches correctly identify\nthe location in 18 cases. In five cases, the incor-\nporation of NewsRECON\u2019s top-3 retrieved articles\nenables Qwen2.5VL to revise an initially incorrect\nprediction into a correct one. However, in six in-\nstances, Qwen2.5VL\u2019s original correct prediction\nbecomes incorrect due to the retrieved articles. Fi-\nnally, in three cases, Qwen2.5VL retains its correct\nprediction despite the retrieved articles being irrele-\nvant. These observations indicate that the retrieved\narticles should not always be trusted by the MLLM.\nNonetheless, the overall quantitative results on the\nfull TARA test set (Table 1) demonstrate that com-\nbining NewsRECON with Qwen2.5VL yields im-\nprovements. For the date task, NewsRECON\u2019s\ntop-3 articles contain the correct date in three cases,\nand Qwen2.5VL successfully leverages them in\ntwo of them. In many cases, while incorrect, the\ndate predicted with the top-3 articles is closer to the\nground truth than the default MLLM prediction.\n5\nConclusion\nIn this work, we introduce NewsRECON, the\nfirst image contextualization method that leverages\nnews article retrieval for predicting the date and\nlocation of images. It queries a corpus of over\n90,000 articles, using a fine-tuned bi-encoder and\ntwo specialized cross-encoders for location- and\nevent-relevant articles retrieval. NewsRECON is\ndesigned for the challenging yet realistic setting in\nwhich reverse image search engines fail to return\nevidence. In this scenario, NewsRECON outper-\nforms prior work on the TARA and 5Pils-OOC\ndatasets. Moreover, the top-K retrieved articles\ncan serve as evidence for an MLLM, improving the\nGREAT score by up to 9.2 percentage points and\nachieving the new SOTA on the TARA dataset.\nLimitations\nWe identify two limitations to NewsRECON.\nFirstly, the corpus contains articles from only\ntwo organizations. In future work, including more\norganizations from countries other than the UK and\nthe USA would help increase event coverage. With\na larger corpus, new challenges are likely to arise,\nsuch as the need for multilingual retrieval and the\nrisk of introducing more noise.\nSecondly, while NewsRECON overall improves\nthe performance of an MLLM, it can also alter\nthe MLLM\u2019s initial correct predictions due to ir-\nrelevant evidence, as shown in Figure 5. Future\nwork should consider better approaches to combine\nthe MLLM with retrieved news articles, e.g., by\ndirectly resolving conflicts between the top 3 evi-\ndence (Wang et al., 2025) or between the evidence\nand the information extracted from the image.\nEthics statement\nIntended use\nNewsRECON provides a new\nmethod for the important problem of contextual-\nizing news images by identifying their date and\nlocation. This has several applications, including\nthe early detection of multimodal misinformation\non social media. NewsRECON and the accompany-\ning corpus should only be used for non-commercial\nacademic research on image contextualization and\nmultimodal misinformation detection.\nData access\nThe datasets and articles were ac-\ncessed in compliance with the German Act on\nCopyright and Related Rights (\u00a760d UrhG) . We re-\nlease our code under an Apache 2.0 license. TARA\nand 5Pils-OOC are made available by their authors\nunder Apache 2.0 and CC BY-SA 4.0 licenses, re-\nspectively. We do not provide the corpus articles\ndirectly. Instead, we provide the code needed to\nreproduce our corpus collection, along with the\nURLs of the articles.\nAI assistants use\nAI assistants were used in this\nwork to assist with writing by correcting grammar\nmistakes and typos.\nAcknowledgments\nThis work has been funded by the LOEWE\ninitiative\n(Hesse,\nGermany)\nwithin\nthe\nemergenCITY\ncenter\n(Grant\nNumber:\nLOEWE/1/12/519/03/05.001(0016)/72)\nand\nby the German Federal Ministry of Research,\nTechnology and Space and the Hessian Ministry of\nHigher Education, Research, Science and the Arts\nwithin their joint support of the National Research\nCenter for Applied Cybersecurity ATHENE. The\nfigures have been designed using resources from\nFlaticon.com.\nReferences\nSahar Abdelnabi, Rakibul Hasan, and Mario Fritz.\n2022.\nOpen-domain, content-based, multi-modal\nfact-checking of out-of-context images via online\nresources. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), pages 14940\u201314949.\nHammad Ayyubi, Xuande Feng, Junzhang Liu, Xudong\nLin, Zhecan Wang, and Shih-Fu Chang. 2025. Puz-\nzleGPT: Emulating human puzzle-solving ability for\ntime and location prediction. In Findings of the Asso-\nciation for Computational Linguistics: NAACL 2025,\npages 2099\u20132116, Albuquerque, New Mexico. Asso-\nciation for Computational Linguistics."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**NewsRECON: News Article REtrieval for Image Contextualization**\n\n**Main Contributions**\n- **First Method**: Introduces a method that leverages news article retrieval to predict the date and location of images, overcoming challenges where reverse image search (RIS) fails.\n- **Large Corpus Utilization**: Uses a corpus of over 90,000 articles for training, providing rich contextual evidence.\n- **Multi-Stage Retrieval**: Incorporates three main components: a bi-encoder for retrieving relevant articles based on event relevance, and two cross-encoders for location and event consistency ranking.\n\n**Methodology**\n- **Bi-Encoder Training**: Fine-tunes a CLIP-based encoder to retrieve top-K event-relevant articles.\n- **Location Cross-Encoder**: Reranks retrieved articles by their similarity to the image's geolocation keywords.\n- **Event Cross-Encoder**: Further reranks article clusters based on their consistency with the image\u2019s event (date and location).\n\n**Key Findings**\n- On the TARA benchmark, NewsRECON outperforms existing methods by 9.7 percentage points in GREAT score for date and location prediction.\n- When combined with a multimodal large language model (MLLM), NewsRECON achieves new state-of-the-art (SOTA) results on the TARA dataset.\n\n**Conclusions**\n- **Improved Performance**: Demonstrates superior performance compared to prior work, especially when RIS evidence is unavailable.\n- **Generalization**: Shows strong generalization capabilities to datasets from different regions and time periods without retraining.\n- **Limitations**: Identified limitations include reliance on a limited corpus of articles from two organizations and the potential for altering MLLM predictions with irrelevant evidence.\n\n**Future Work**\n- Expanding the news article corpus to cover more events and languages.\n- Developing better methods to integrate retrieved news articles with ML models, such as directly resolving conflicts or extracting image information.\n\nThis summary encapsulates the main contributions, methodology, key findings, and conclusions of the paper \"NewsRECON: News Article REtrieval for Image CONtextualization\"."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "torch-sla: Differentiable Sparse Linear Algebra with\nAdjoint Solvers and Sparse Tensor Parallelism for PyTorch\nMingyuan Chi\nwalker.chi.000@gmail.com\nhttps://github.com/walkerchi/torch-sla\nJanuary 21, 2026\nAbstract\nIndustrial scientific computing predominantly uses sparse matrices to represent unstructured data\u2014finite element\nmeshes, graphs, point clouds. We present torch-sla, an open-source PyTorch library that enables GPU-accelerated,\nscalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU ac-\nceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2)\nMulti-GPU scaling via domain decomposition with halo exchange, reaching 400 million DOF linear solve on 3\nGPUs; and (3) Adjoint-based differentiation achieving O(1) computational graph nodes (for autograd) and O(nnz)\nmemory\u2014independent of solver iterations. torch-sla supports multiple backends (SciPy, cuDSS, PyTorch-native)\nand seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at\nhttps://github.com/walkerchi/torch-sla.\n1\nIntroduction\nIndustrial scientific computing predominantly deals with unstructured data\u2014finite element meshes, graph networks,\npoint clouds\u2014where sparse matrices are the natural representation. These sparse systems arise in finite element\nanalysis [Hughes, 2012], computational fluid dynamics [Jasak et al., 2007, Kochkov et al., 2021], graph neural net-\nworks [Kipf and Welling, 2017, Veli\u02c7ckovi\u00b4c et al., 2018], and physics-informed machine learning [Raissi et al., 2019,\nLu et al., 2021]. Efficiently solving sparse linear systems Ax = b, nonlinear systems F(x) = 0, and eigenvalue\nproblems Av = \u03bbv is fundamental to these applications.\nWith the rise of differentiable programming and neural operators [Li et al., 2020, Brandstetter et al., 2022], there\nis growing demand to integrate sparse solvers with deep learning frameworks like PyTorch [Paszke et al., 2019].\nHowever, three fundamental challenges stand in the way:\nChallenge 1: GPU acceleration for sparse operations. Unlike dense matrices where GPU parallelism is straight-\nforward, sparse matrices have irregular memory access patterns that underutilize GPU bandwidth. Sparse matrix-\nvector products (SpMV) achieve only 10\u201320% of peak memory bandwidth [Li et al., 2024]. Direct solvers (LU,\nCholesky) require O(n1.5) memory due to fill-in, often exceeding GPU memory for problems with >2M unknowns.\nChallenge 2: Multi-GPU scaling. Industrial problems (10M\u20131B unknowns) exceed single-GPU memory. Dis-\ntributed sparse computing requires domain decomposition and halo exchange\u2014communicating boundary values be-\ntween partitions\u2014which is non-trivial to implement correctly with automatic differentiation support.\nChallenge 3: PyTorch integration with efficient gradients. Differentiating through iterative solvers creates O(k)\nnodes in the computational graph, where k may be thousands of iterations. For a 1M-DOF problem with 1000 CG\niterations, this requires \u223c80 GB just for gradient computation. This memory explosion prevents end-to-end training\nof neural networks that interact with sparse solvers.\n1\narXiv:2601.13994v1  [cs.DC]  20 Jan 2026\nThis paper introduces torch-sla*, a library that addresses all three challenges:\n\u2022 GPU-accelerated sparse solvers: We provide unified APIs for linear solves (CG [Hestenes and Stiefel, 1952],\nBiCGStab [Van der Vorst, 1992]), nonlinear solves (Newton, Picard, Anderson acceleration [Anderson, 1965,\nKelley, 1995]), and eigenvalue computation (LOBPCG [Knyazev, 2001]), with multiple backends (SciPy, cuDSS,\nPyTorch-native).\n\u2022 Sparse tensor parallelism (\u00a73.3): We implement domain decomposition with automatic halo exchange, en-\nabling 400 million DOF linear solve on 3 GPUs.\n\u2022 Adjoint-based differentiation (\u00a73.2): We implement the adjoint method for linear, nonlinear, and eigenvalue\nproblems, achieving O(1) autograd graph nodes and O(nnz) memory\u2014independent of solver iterations.\n2\nRelated Work\nDifferentiable linear algebra. JAX [Bradbury et al., 2018] provides differentiable sparse CG, but naive differentiation\nthrough iterations creates O(k) computational graph nodes. Blondel et al. [2022] formalize implicit differentiation\nfor optimization layers. OptNet [Amos and Kolter, 2017] and cvxpylayers [Agrawal et al., 2019] handle convex\noptimization. Deep equilibrium models [Bai et al., 2019] and Jacobian-free backpropagation [Fung et al., 2022]\naddress implicit layers but focus on fixed-point iterations rather than sparse linear systems.\nSparse solvers and GPU acceleration. SciPy [Virtanen et al., 2020] provides SuperLU/UMFPACK for CPU. For\nGPU, NVIDIA cuDSS [NVIDIA Corporation, 2024] offers direct solvers, while AmgX [Naumov et al., 2015] provides\nalgebraic multigrid with excellent scalability. Recent surveys [Li et al., 2024] cover sparse matrix computations on\nmodern GPUs. None of these natively support PyTorch autograd.\nDistributed sparse computing. PETSc [Balay et al., 2023], Trilinos [Heroux et al., 2005], and hypre [Falgout and\nYang, 2002] implement distributed sparse linear algebra with sophisticated preconditioners. OpenFOAM [Jasak et al.,\n2007] uses domain decomposition for CFD. We bring these industrial patterns to PyTorch with automatic differentia-\ntion.\nLearned solvers and preconditioners. Recent work explores learning components of iterative solvers: learned\nmultigrid prolongation [Greenfeld et al., 2019], GNN-based AMG [Luz et al., 2020], and learned interface condi-\ntions for domain decomposition [Taghibakhshi et al., 2024]. These complement our library by providing learnable\ncomponents that can be trained end-to-end.\n3\nMethodology\nWe first introduce the background on sparse linear systems (\u00a73.1), then present our adjoint-based differentiation ap-\nproach (\u00a73.2), and finally describe our distributed computing implementation (\u00a73.3).\n3.1\nPreliminaries: Sparse Linear Systems\nA sparse linear system Ax = b has a matrix A \u2208Rn\u00d7n with nnz \u226an2 non-zero entries. We store matrices in\ncoordinate (COO) format: three arrays for row indices, column indices, and values.\nDirect solvers (LU, Cholesky factorization [Golub and Van Loan, 2013]) compute exact solutions but require\nO(n1.5) memory for 2D problems due to fill-in\u2014new non-zeros created during factorization [George, 1973]. For\na 2D Poisson problem with n unknowns, the factored matrix has O(n log n) non-zeros instead of the original O(n).\nIterative solvers (Conjugate Gradient [Hestenes and Stiefel, 1952], BiCGStab [Van der Vorst, 1992], GMRES [Saad\nand Schultz, 1986]) maintain O(nnz) memory by never forming the factorization. However, they require O(\u221a\u03ba)\niterations for CG where \u03ba is the condition number [Saad, 2003]. Each iteration performs one sparse matrix-vector\nmultiplication (SpMV) costing O(nnz).\nThe differentiation problem. Given loss L(x) where x = A\u22121b, we need gradients with respect to both b and\nthe non-zero values of A. If we differentiate through k iterations of CG, the computational graph has O(k) nodes,\n*Code available at https://github.com/walkerchi/torch-sla. Install via pip install torch-sla.\n2\nTable 1: Complexity comparison: naive vs. adjoint differentiation through iterative solver with k iterations, n unknowns, and nnz\nnon-zeros.\nNaive (through iterations)\nAdjoint (ours)\nComputational graph nodes\nO(k)\nO(1)\nMemory for graph\nO(k \u00b7 n)\nO(n + nnz)\nBackward pass time\nO(k \u00b7 nnz)\nO(Tsolve + nnz)\neach storing intermediate vectors. For k = 1000 iterations and n = 106, this requires \u223c80 GB just for the graph. Our\nadjoint approach reduces this to O(1) nodes.\n3.2\nAdjoint-Based Differentiation\nThe adjoint method computes gradients through implicit functions without storing intermediate solver states. We\npresent it for linear systems, eigenvalue problems, and nonlinear systems.\n3.2.1\nLinear Systems\nConsider x = A\u22121b. Rather than differentiating through the solver iterations, we use the implicit function theorem.\nThe solution satisfies Ax \u2212b = 0. Differentiating implicitly:\ndA \u00b7 x + A \u00b7 dx = db =\u21d2dx = A\u22121(db \u2212dA \u00b7 x)\n(1)\nFor a scalar loss L(x), the chain rule gives:\ndL =\n\u0012\u2202L\n\u2202x\n\u0013\u22a4\ndx =\n\u0012\u2202L\n\u2202x\n\u0013\u22a4\nA\u22121(db \u2212dA \u00b7 x)\n(2)\nDefine the adjoint variable \u03bb = A\u2212\u22a4\u2202L\n\u2202x , solved via A\u22a4\u03bb = \u2202L\n\u2202x . Then the gradients are:\n\u2202L\n\u2202b = \u03bb,\n\u2202L\n\u2202Aij\n= \u2212\u03bbi \u00b7 xj\n(3)\nComplexity analysis. Table 1 compares naive differentiation (through iterations) with our adjoint approach.\nThe forward solve computes x in time Tsolve. The backward pass:\n1. Solves A\u22a4\u03bb = \u2202L\n\u2202x : O(Tsolve) time\n2. Computes\n\u2202L\n\u2202Aij = \u2212\u03bbixj for each non-zero: O(nnz) time\nAlgorithm. The complete procedure is shown below:\nAlgorithm 1: Adjoint Linear Solve\nInput: Sparse matrix A (values, rows, cols), RHS b\nOutput: Solution x, gradients in backward pass\nForward pass:\n1. x \u2190solve(A, b)\n// Any solver: CG, LU, etc.\n2. Store (A, x) for backward\n3\nBackward pass: (given \u2202L/\u2202x)\n1. \u03bb \u2190solve(A\u22a4, \u2202L/\u2202x)\n// One adjoint solve\n2. \u2202L/\u2202b \u2190\u03bb\n3. For each non-zero (i, j): \u2202L/\u2202Aij \u2190\u2212\u03bbi \u00b7 xj\n// O(nnz) total\n3.2.2\nEigenvalue Problems\nFor symmetric eigenvalue problem Av = \u03bbv with normalized eigenvector \u2225v\u2225= 1, the gradient of eigenvalue \u03bb with\nrespect to matrix entries is remarkably simple [Magnus, 1985]:\n\u2202\u03bb\n\u2202Aij\n= vi \u00b7 vj\n(4)\nThis is the outer product vv\u22a4restricted to the sparsity pattern. For k eigenvalues, the total cost is O(k \u00b7 nnz).\n3.2.3\nNonlinear Systems\nFor nonlinear system F(u, \u03b8) = 0 with solution u\u2217(\u03b8) and loss L(u\u2217), implicit differentiation gives:\nJ\u22a4\u03bb = \u2202L\n\u2202u,\n\u2202L\n\u2202\u03b8 = \u2212\u03bb\u22a4\u2202F\n\u2202\u03b8\n(5)\nwhere J = \u2202F\n\u2202u is the Jacobian at the solution.\nAlgorithm 2: Newton-Raphson with Adjoint Gradients\nInput: Residual function F(u, \u03b8), initial guess u0\nOutput: Solution u\u2217\nForward (Newton iteration):\n1. u \u2190u0\n2. while \u2225F(u, \u03b8)\u2225> \u03f5 do:\n(a) J \u2190\u2202F/\u2202u\n// Jacobian via autograd\n(b) \u2206u \u2190solve(J, \u2212F)\n(c) u \u2190u + \u03b1\u2206u\n// \u03b1 from line search\n3. Store (u\u2217, J, \u03b8)\nBackward: (given \u2202L/\u2202u\u2217)\n1. \u03bb \u2190solve(J\u22a4, \u2202L/\u2202u\u2217)\n// One adjoint solve\n2. \u2202L/\u2202\u03b8 \u2190\u2212\u03bb\u22a4\u2202F/\u2202\u03b8\n// VJP via autograd\nMemory analysis. Forward: O(n + nnz) for solution and Jacobian. Backward: O(n) for adjoint variable. Total:\nO(n + nnz), independent of Newton iterations.\n4\n0\n1\n2\n3\n2\n3\n4\n5\nProcess 0\nProcess 1\nsend\nsend\nboundary\nOwned P0\nOwned P1\nHalo (ghost)\nFigure 1: Halo exchange in domain decomposition. Each process owns a subset of nodes (solid colored) and maintains halo\ncopies of boundary neighbors (dashed). Before SpMV, processes exchange updated values at partition boundaries via peer-to-peer\ncommunication.\n3.3\nSparse Tensor Parallel Computing\nFor problems exceeding single-GPU memory, we implement domain decomposition with halo exchange\u2014the standard\napproach in industrial CFD/FEM codes like OpenFOAM and Ansys Fluent.\n3.3.1\nDomain Decomposition\nGiven a sparse matrix A corresponding to a mesh or graph, we partition the n nodes into P subdomains. Each process\np owns nodes Op (owned nodes) and maintains copies of neighboring nodes Hp (halo/ghost nodes) needed for local\ncomputation.\nPartitioning strategies:\n\u2022 METIS [Karypis and Kumar, 1998]: Graph partitioning minimizing edge cuts for load balancing\n\u2022 RCB: Recursive Coordinate Bisection using node coordinates\n\u2022 Contiguous: Simple row-based partitioning (fallback)\n3.3.2\nHalo Exchange\nThe key operation in distributed sparse computing is halo exchange: before each SpMV, processes must exchange\nboundary values with neighbors. Figure 1 illustrates this concept.\nAlgorithm 3: Distributed SpMV with Halo Exchange\nInput: Local vector xlocal, neighbor map, local matrix Alocal\nOutput: Result yowned\nPhase 1: Initiate async communication\n1. for each neighbor q do:\n(a) async_send(my boundary values to q)\n(b) async_recv(halo values from q)\nPhase 2: Wait for completion\n1. synchronize_all()\nPhase 3: Local computation\n1. yowned \u2190Alocal \u00b7 xlocal\n// Uses owned + halo\n5\nTable 2: Single-GPU benchmark on 2D Poisson (5-point stencil), H200, float64.\nDOF\nSciPy\ncuDSS\nPyTorch CG\nMemory\nResidual\n10K\n24 ms\n128 ms\n20 ms\n36 MB\n10\u22129\n100K\n29 ms\n630 ms\n43 ms\n76 MB\n10\u22127\n1M\n19.4 s\n7.3 s\n190 ms\n474 MB\n10\u22127\n2M\n52.9 s\n15.6 s\n418 ms\n916 MB\n10\u22127\n16M\nOOM\nOOM\n7.3 s\n7.1 GB\n10\u22126\n169M\nOOM\nOOM\n224 s\n74.8 GB\n10\u22126\n3.3.3\nDistributed Conjugate Gradient\nBuilding on distributed SpMV, we implement distributed CG. Each iteration requires:\n\u2022 One halo exchange (in SpMV)\n\u2022 Two all_reduce operations for global dot products\nAlgorithm 4: Distributed Conjugate Gradient\nInput: Distributed A, local RHS bowned, tolerance \u03f5\nOutput: Solution xowned\n1. x \u21900, r \u2190bowned, p \u2190r\n2. \u03c1 \u2190all_reduce(r\u22a4r, SUM)\n// Global dot product\n3. while \u221a\u03c1 > \u03f5 do:\n(a) Ap \u2190DistSpMV(A, p)\n// Algorithm 3\n(b) \u03b1 \u2190\u03c1/all_reduce(p\u22a4Ap, SUM)\n(c) x \u2190x + \u03b1p\n(d) r \u2190r \u2212\u03b1Ap\n(e) \u03c1new \u2190all_reduce(r\u22a4r, SUM)\n(f) p \u2190r + (\u03c1new/\u03c1)p\n(g) \u03c1 \u2190\u03c1new\nCommunication complexity. Per iteration: O(|Hp|) for halo exchange + O(log P) for all_reduce. Total per solve:\nO(k \u00b7 (|Hp| + log P)) where k is iterations.\nGradient support. Distributed operations compose with adjoint differentiation: the backward pass performs an-\nother distributed solve with transposed communication patterns.\n4\nExperiments\nWe evaluate torch-sla on 2D Poisson equations (5-point stencil) using NVIDIA H200 GPUs (140GB HBM3).\n4.1\nSingle-GPU Scalability\nFigure 2 and Table 2 compare solver backends across problem sizes spanning five orders of magnitude.\nAnalysis. The results reveal three distinct scaling regimes:\n6\nFigure 2: Single-GPU benchmark results showing performance, memory usage, and residual across solver backends (SciPy,\ncuDSS, PyTorch CG) on 2D Poisson equation with H200 GPU.\nFigure 3: Multi-GPU scaling: distributed CG with NCCL backend on 3\u20134 H200 GPUs, showing time and memory scaling across\nproblem sizes.\n(1) Small problems (<100K DOF): Direct solvers dominate. SciPy SuperLU achieves 24ms with machine precision\n(10\u221214), while GPU overhead makes cuDSS slower (128ms). The factorization cost is negligible, so the O(n3) solve\nphase dominates.\n(2) Medium problems (100K\u20132M DOF): Iterative solvers become competitive. At 1M DOF, PyTorch CG (190ms)\nis 100\u00d7 faster than cuDSS (7.3s). This crossover occurs because: (a) CG requires only O(nnz) memory vs. O(n1.5)\nfor LU fill-in; (b) SpMV is memory-bound at 90% of peak bandwidth on H200.\n(3) Large problems (>2M DOF): Only iterative solvers are feasible. Direct solvers hit OOM due to fill-in: for\n2D Poisson, LU fill-in grows as O(n log n), requiring \u223c100GB at 2M DOF. CG maintains O(n) memory scaling,\nreaching 169M DOF in 74.8GB.\nTime complexity. Fitting T = c \u00b7 n\u03b1 yields \u03b1 \u22481.1 for PyTorch CG, consistent with O(\u221a\u03ba \u00b7 nnz) where condition\nnumber \u03ba \u223cn for 2D Poisson with Jacobi preconditioner. The sub-quadratic scaling enables practical use at 100M+\nDOF.\nMemory efficiency. Measured 443 bytes/DOF vs. theoretical minimum 144 bytes/DOF (matrix: 80 bytes/DOF\nfor 5 non-zeros \u00d7 16 bytes; vectors: 64 bytes for x, b, r, p). The 3\u00d7 overhead comes from PyTorch sparse tensor\nmetadata and Jacobi preconditioner storage.\n4.2\nMulti-GPU Performance\nFigure 3 and Table 3 summarize distributed CG performance on H200 GPUs with NCCL backend.\nScaling analysis. We scale to 400 million DOF on 3 H200 GPUs. The scaling exhibits three regimes:\n(1) Small problems (<1M DOF): Time dominated by kernel launch and communication latency. At 10K DOF, each\n7\nTable 3: Distributed CG on H200 GPUs with NCCL backend.\nDOF\nGPUs\nTime\nMemory/GPU\nResidual\n100M\n4\n36.1 s\n23.3 GB\n1.0 \u00d7 10\u22122\n200M\n3\n120 s\n53.7 GB\n1.5 \u00d7 10\u22122\n300M\n3\n217 s\n80.5 GB\n1.9 \u00d7 10\u22122\n400M\n3\n331 s\n110 GB\n2.3 \u00d7 10\u22122\nTable 4: Gradient verification: adjoint vs. finite difference (\u03f5 = 10\u22125).\nOperation\nRel. Error\nForward\nBackward\nLinear solve (n=1000)\n8.3 \u00d7 10\u22127\n1 solve\n1 solve\nEigenvalue (k=6)\n2.1 \u00d7 10\u22126\nLOBPCG\n1 outer product\nNonlinear (5 Newton)\n4.7 \u00d7 10\u22127\n5 solves\n1 solve\nCG iteration takes \u223c0.1ms, with 50% spent on all_reduce synchronization.\n(2) Medium problems (1M\u2013100M DOF): Near-linear scaling with T \u221dn1.05. At 100M DOF, we achieve 2.8M\nDOF/s throughput, close to the memory bandwidth limit of H200 (3.9 TB/s \u00d7 4 GPUs).\n(3) Large problems (>100M DOF): Memory capacity becomes the bottleneck. At 200M+ DOF, we reduce to 3\nGPUs to fit within 140GB/GPU limit. The time scaling remains efficient: 400M DOF in 331s corresponds to 1.2M\nDOF/s.\nMemory efficiency. Measured 275 bytes/DOF at 400M DOF (110GB / 400M), consistent with sparse matrix\nstorage (80 bytes/DOF) plus solver vectors and halo buffers. The per-GPU memory scales as O(n/P + |Hp|) where\nhalo size |Hp| \u223cO(\np\nn/P) for 2D grids.\nConvergence. Residual increases from 10\u22124 at 10K DOF to 10\u22122 at 400M DOF. This is expected: fixed iteration\ncount (1000) with Jacobi preconditioner, where condition number \u03ba \u223cn for 2D Poisson. For tighter tolerance,\nmultigrid preconditioning would reduce iterations by 10\u00d7.\n4.3\nGradient Verification\nWe verify gradient correctness by comparing adjoint gradients against finite differences:\n\u2202L\n\u2202\u03b8 \u2248L(\u03b8 + \u03f5) \u2212L(\u03b8 \u2212\u03f5)\n2\u03f5\n(6)\nAnalysis. Relative errors < 10\u22125 confirm correct implementation. Notably, the nonlinear solver\u2019s backward pass\nrequires only 1 adjoint solve regardless of Newton iterations (5 in this case), validating the O(1) graph complexity\nclaim. The eigenvalue gradient uses Eq. (4), requiring no additional solves\u2014just O(k \u00b7 nnz) outer product computa-\ntions.\n5\nConclusion\nWe presented torch-sla, a differentiable sparse linear algebra library with two innovations: (1) adjoint-based\ndifferentiation achieving O(nnz) graph complexity, and (2) distributed sparse computing with halo exchange. The\nlibrary scales to 169M DOF on a single GPU and 400M DOF on 3 GPUs, demonstrating industrial-scale capability\nwith full gradient support.\n5.1\nFuture Work\n\u2022 Roofline-guided tuning for distributed solvers: Current distributed CG achieves \u223c50% parallel efficiency\nat billion-scale DOF due to communication bottlenecks. We plan to develop roofline-based auto-tuning that:\n(a) overlaps halo exchange with local SpMV computation; (b) dynamically adjusts partition granularity based\n8\non compute/communication ratio; (c) implements communication-avoiding CG variants [Hoemmen, 2010] to\nreduce all_reduce frequency.\n\u2022 Learned preconditioners: GNN-based multigrid [Luz et al., 2020] and meta-learned preconditioner selec-\ntion [Chen et al., 2022] could adaptively choose optimal preconditioners based on matrix structure, potentially\nimproving convergence rates.\n\u2022 Mixed precision: Following FlashAttention [Dao et al., 2022], we plan FP16/BF16 SpMV with FP64 accumu-\nlation, potentially doubling throughput while maintaining convergence for well-conditioned problems.\n\u2022 Neural operator hybrid: Combining with FNO [Li et al., 2020], DeepONet [Lu et al., 2021], and mesh-\nbased GNNs [Pfaff et al., 2021] enables hybrid neural-classical solvers where neural networks provide coarse\ncorrections and classical solvers refine to high precision.\n\u2022 Differentiable simulation integration: Seamless integration with \u03a6Flow [Holl et al., 2024] and DiffTaichi [Hu\net al., 2020] for end-to-end differentiable physics pipelines, enabling gradient-based optimization of simulation\nparameters.\nAvailability.\nMIT license: https://github.com/walkerchi/torch-sla\n9\nReferences\nAkshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter. Differentiable\nconvex optimization layers. In Advances in Neural Information Processing Systems, volume 32, 2019.\nBrandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In International\nConference on Machine Learning, pages 136\u2013145, 2017.\nDonald G Anderson. Iterative procedures for nonlinear integral equations. Journal of the ACM, 12(4):547\u2013560, 1965.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in Neural Information Processing\nSystems, 32, 2019.\nSatish Balay, Shrirang Abhyankar, Mark F Adams, et al. Petsc users manual. Technical report, Argonne National\nLaboratory, 2023. Version 3.20.\nMathieu Blondel, Quentin Berthet, Marco Cuturi, et al. Efficient and modular implicit differentiation. Advances in\nNeural Information Processing Systems, 35:5230\u20135242, 2022.\nJames Bradbury, Roy Frostig, Peter Hawkins, et al. JAX: composable transformations of Python+NumPy programs,\n2018. URL http://github.com/google/jax.\nJohannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers. International Confer-\nence on Learning Representations, 2022.\nTianlong Chen et al. Meta-learning for fast preconditioner selection. ICLR, 2022.\nTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient\nexact attention with io-awareness. NeurIPS, 2022.\nRobert D Falgout and Ulrike Meier Yang. hypre: A library of high performance preconditioners. In International\nConference on Computational Science, pages 632\u2013641, 2002.\nSamy Wu Fung, Howard Heaton, Qiuwei Li, Daniel McKenzie, Stanley Osher, and Wotao Yin. JFB: Jacobian-free\nbackpropagation for implicit networks. In AAAI Conference on Artificial Intelligence, 2022.\nAlan George. Nested dissection of a regular finite element mesh. SIAM Journal on Numerical Analysis, 10(2):345\u2013\n363, 1973.\nGene H Golub and Charles F Van Loan. Matrix Computations. Johns Hopkins University Press, 4th edition, 2013.\nDaniel Greenfeld, Meirav Galun, Ronen Basri, Irad Yavneh, and Ron Kimmel. Learning to optimize multigrid PDE\nsolvers. International Conference on Machine Learning, 2019.\nMichael A Heroux, Roscoe A Bartlett, et al. An overview of the trilinos project. ACM Transactions on Mathematical\nSoftware, 31(3):397\u2013423, 2005.\nMagnus R Hestenes and Eduard Stiefel.\nMethods of conjugate gradients for solving linear systems.\nJournal of\nResearch of the National Bureau of Standards, 49(6):409\u2013436, 1952.\nMark Hoemmen. Communication-avoiding Krylov subspace methods. PhD thesis, University of California, Berkeley,\n2010.\nPhilipp Holl, Vladlen Koltun, and Nils Thuerey. \u03a6Flow: A differentiable PDE solving framework for deep learning\nvia physical simulations. arXiv preprint arXiv:2402.04983, 2024.\nYuanming Hu, Luke Anderson, Tzu-Mao Li, et al. Difftaichi: Differentiable programming for physical simulation. In\nInternational Conference on Learning Representations, 2020.\nThomas JR Hughes. The finite element method: linear static and dynamic finite element analysis. Courier Corporation,\n2012.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper \"torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch\"\n\n#### Main Contributions:\n- **torch-sla** is an open-source library designed to enable GPU-accelerated, scalable, and differentiable sparse linear algebra in PyTorch. It addresses three fundamental challenges in industrial scientific computing: \n  - GPU acceleration for sparse matrix operations (SpMV), nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation.\n  - Multi-GPU scaling via domain decomposition with halo exchange, achieving up to 400 million degrees of freedom (DOFs) on three GPUs.\n  - Adjoint-based differentiation that achieves O(1) computational graph nodes for autograd and O(nnz) memory complexity, independent of solver iterations.\n\n- **torch-sla** supports multiple backends including SciPy, cuDSS, and PyTorch-native implementations. It seamlessly integrates with PyTorch's autograd system to support end-to-end differentiable simulations.\n\n#### Methodology:\n1. **Preliminaries on Sparse Linear Systems**: The paper introduces the background of sparse linear systems, detailing how they are represented using COO format (three arrays for row indices, column indices, and values). It also discusses direct solvers like LU and Cholesky factorization, which require O(n^1.5) memory due to fill-in, and iterative solvers such as Conjugate Gradient (CG), BiCGStab, GMRES that maintain O(nnz) memory by avoiding the formation of the full matrix.\n\n2. **Adjoint-Based Differentiation**: The authors present adjoint-based differentiation for linear systems, eigenvalue problems, and nonlinear systems. For linear systems, they use the implicit function theorem to compute gradients without storing intermediate solver states. For nonlinear systems, they employ Newton-Raphson iterations with adjoint gradients. \n\n3. **Sparse Tensor Parallel Computing**: torch-sla implements domain decomposition with halo exchange for multi-GPU scaling. The key operation is halo exchange where each process exchanges boundary values with its neighbors before performing SpMV.\n\n#### Key Findings:\n- On single GPUs, torch-sla achieves competitive performance across different problem sizes (from 10K to 2M DOFs), demonstrating scalability and efficiency.\n- In multi-GPU settings, the library scales well up to 400 million DOFs on three H200 GPUs. The time complexity is near-linear with problem size for medium problems (1M\u2013100M DOFs) and efficient for large problems (>100M DOFs).\n- Gradient verification shows that adjoint-based differentiation achieves the expected O(1) graph complexity, validating its correctness.\n\n#### Conclusions:\nThe paper concludes by highlighting the industrial applicability of torch-sla. It supports up to 169 million DOFs on a single GPU and scales to 400 million DOFs on three GPUs. The library's innovations in adjoint-based differentiation and distributed sparse computing make it suitable for end-to-end differentiable simulations, including integration with other frameworks like \u03a6Flow and DiffTaichi.\n\n#### Future Work:\n- **Roofline-guided tuning**: Developing auto-tuning methods to optimize communication patterns.\n- **Learned preconditioners**: Implementing GNN-based multigrid solvers or meta-learning for optimal preconditioner selection.\n- **Mixed precision**: Using FP16/BF16 SpMV with FP64 accumulation to potentially double throughput while maintaining convergence for well-conditioned problems.\n- **Neural operator hybrid**: Combining neural operators with classical solvers for more efficient and accurate simulations.\n\n### Availability:\nThe library is available under the MIT license at [https://github.com/walkerchi/torch-sla](https://github.com/walkerchi/torch-sla).\n\nThis summary encapsulates the main contributions, methodology, key findings, and conclusions of the paper \"torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch\"."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "OpenMOSS\nFutureOmni: Evaluating Future Forecasting\nfrom Omni-Modal Context for Multimodal LLMs\nQian Chen1\nJinlan Fu1,3,\u2020 Changsong Li1,2 See-Kiong Ng3 Xipeng Qiu1,2\n1Fudan University, 2Shanghai Innovation Institute, 3National University of Singapore\nAbstract\nAlthough Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception,\ntheir ability to forecast future events from audio\u2013visual cues remains largely unexplored, as existing\nbenchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce Future-\nOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio\u2013visual environments.\nThe evaluated models are required to perform cross-modal causal and temporal reasoning, as well\nas effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a\nscalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice\nQA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that\ncurrent systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the\nbest accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample\ninstruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy.\nEvaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that\nOFF enhances future forecasting and generalization.\nHomepage: https://openmoss.github.io/FutureOmni\nCode: https://github.com/OpenMOSS/FutureOmni\nDataset: https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni\nCorrespondence: qianchen901005@gmail.com, jinlanjonna@gmail.com\n1\nIntroduction\nMultimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in audio-video\nunderstanding [14, 43]. To access these capabilities, community has established a suite of omni-modal\nbenchmarks designed to evaluate synergistic video-audio understanding [5, 13, 22, 45, 56]. Recent suites like\nWorldSense [16] and DailyOmni [55] have expanded this scope, testing MLLMs on complex tasks ranging from\nevent captioning to temporal grounding holistic question answering. These benchmarks have successfully\ndriven progress in retrospective reasoning [8, 31], enabling models to accurately describe and analyze events\nhave occurred within videos.\nAlthough extensive research has focused on retrospective reasoning, predicting future events is equally\ncritical in real-world applications. For example, in autonomous driving, systems must integrate auditory\n\u2020Corresponding author.\n1\narXiv:2601.13836v1  [cs.CL]  20 Jan 2026\ncues (e.g., honking from nearby vehicles) with visual information (e.g., pedestrian positions) to anticipate\nfuture world states and make timely safety decisions. Some prior works have explored future forecasting.\nFutureBench [39], ForecastBench [19], FutureX [48], and MIRAI [47] predict real-world future events and\nevaluate language-based LLMs considering only textual modality, and require periodic benchmark updates\nto prevent data leakage. VLEP [21], IntentQA [25], and MM-Forecast [24] extend future prediction to vision\nand language modalities. However, the auditory modality, despite its importance for future reasoning, has\nbeen largely overlooked. As a result, the capability of multimodal LLMs to perform future forecasting from\njoint audio-visual inputs remains insufficiently studied.\nQuestion: Tom steps on tacks, what will happen next?\n(A) Tom endures the pain and runs out of the castle\n(B) Tom screams in pain instantly, clutching his injured foot\nKing\nawakes\n1:18\nKing\nscolds Tom\nTom steps\non tacks\nTom endures the\npain and runs out of\nthe castle.\nTeacup\nbreaking   1:23\n1:27\n1:35\n1:57\n2:03\n2:04\n2:07\n2:15\n2:19\n(C) Tom stops to angrily search for Jerry\n(D) Tom falls to the ground\n(E) Tom\u2018s scream wakes the King\nPast & Current Observations\nFuture Event\nVisual cues\nAuditory cues\nFuture Forecasting\nStart Time\nEnd Time\nAuditory Cause\nVisual Cause\nFigure 1 An Example from FutureOmni illustrating the Omni-modal Future Prediction task. The green and pink arrows\ndenote consequences induced by auditory and visual cues, respectively.\nAVicuna-7B\nVideo-LLaVA-7B\nQwen2.5-Omni-3B\nLLaVA-NeXT-7B\nVideoLLaMA2-7B\nQwen2.5-VL-7B\nQwen3-VL-8B\nv-SALMONN2-7B\nVideoLLaMA3-7B\nv-SALMONN2+-7B\nQwen2.5-Omni-7B\nOla-7B\nMiniCPM-o2.6-7B\nQwen3-VL-30B\nGPT-4o\nClaude Haiku 4.5\nQwen3-Omni-30B\nGemini2.5 Flash\nGemini2.5Pro\nGemini3Flash\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\nAccuracy\n30.37\n37.72 38.91 40.62 40.75 43.71 45.84 46.03 46.8\n47\n47.48 48.54 49.08 49.32 49.7 52.03 53.05 55.61 57.93\n64.8\nOmni-LLMs\nVideo-LLMs\nFigure 2 Overall scores on FutureOmni.\nIn this paper, we introduces FutureOmni, the first comprehensive benchmark for evaluating Multimodal\nLLMs on future event forecasting under omni-modal context, with a focus on cross-modal causal forecasting,\ntemporal reasoning, and the effective use of internal knowledge. As illustrated in Figure 1, an evaluated MLLM\nis required to select the correct future event based on the omni-modal context, which in this case consists\nof audio and video modalities. FutureOmni is constructed using a scalable AI-assisted, human-in-the-loop\npipeline to enable efficient dataset construction while maintaining high data quality. It comprises 919 videos\nand 1,034 multiple-choice QA pairs spanning 8 primary domains. To ensure comprehensive evaluation,\nwe collect videos with diverse audio types (speech, environmental sounds, and music) and durations up\nto 20 minutes. To prevent potential shortcut learning, we design four types of adversarial distractors. By\nintroducing visual-only and audio-only conflicts, as well as delayed and reverse-causal options, we require\nMLLMs to perform genuine cross-modal reasoning for future forecasting.\nExperiments and Findings. We conduct extensive experiments on widely used MLLMs, including both\nomni-modal and video-centric models, covering proprietary and open-source systems, as shown in Figure 2.\nThe results reveal a key limitation: (F1) existing omni-modal and video-centric MLLMs struggle to accurately\nforecast future events under omni-modal contexts, with the best performance reaching only 64.8%, achieved by Gemini 3\nFlash. To address this issue, we construct a high-quality instruction-tuning dataset, namely, FutureOmni-7K,\nand propose an Omni-Modal Future Forecasting (OFF) method. Evaluations on FutureOmni , popular\n2\naudio-visual benchmarks (e.g., WorldSense, DailyOmni), and video-only benchmarks (e.g., Video-MME)\nshow that (F2) OFF substantially improves the performance of open-source models on our benchmark and enhances\ntheir out-of-domain generalization. Furthermore, attention-score visualizations indicate that (F3) OFF improves\nthe model\u2019s ability to identify critical keyframes, leading to better generalization and reasoning performance.\nOur main contributions are as follows:\n(1) We introduce FutureOmni , the first benchmark for evaluating the future forecasting ability of MLLMs\nunder omni-modal contexts. It contains 919 videos and 1,034 QAs across 8 domains, filling a key gap in\nomni-modal reasoning evaluation.\n(2) We conduct extensive evaluation and analysis on 20 MLLMs. The results show that both omni-modal and\nvideo-only models exhibit limited future forecasting ability, with even the best proprietary model achieving\nonly 64.8%, revealing substantial room for improvement.\n(3) To enhance omni-modal LLMs, we propose a 7K instruction-tuning samples and an Omni-Modal Future\nForecasting (OFF) method. Results: OFF improves future forecasting and generalization ability, as supported\nby attention-score visualizations.\n2\nRelated Work\nMLLMs. This field has evolved rapidly in recent years, initially focusing on aligning visual representations\nwith large language models for image-text understanding [11, 33]. Foundational works such as Flamingo [1],\nBLIP-2 [26], and LLaVA [29] established the paradigm of projecting visual tokens into the LLM space.\nSubsequent efforts extended this framework to temporal visual inputs for video understanding [27, 50]. In\nparallel, LLMs have been augmented with auditory perception by integrating pretrained audio encoders,\nenabling both speech and non-speech audio understanding [9, 18, 34, 36].\nRecently, there has been a paradigm shift towards Omnimodal Large Language Models capable of processing\nand reasoning across text, vision and audio simultaneously. Proprietary state-of-the-art models, such\nas Gemini 2.5 Pro and Gemini 3 Flash [15] have showcased remarkable abilities in handling long-context,\ninterleaved audio-visual inputs. Within the open-source landscape, the dual-tower paradigm, which processes\nvisual and acoustic signals via distinct encoders, has gained traction. Prominent examples include Qwen2.5-\nOmni [42] and video-SALMONN 2 [37], both of which have substantially advanced the integration of audio\nmodalities into video LLMs.\nMultimodal Benchmarks. For omni-modal evaluation, datasets including AVQA [44] and MUSIC-AVQA [23]\nassessed joint visual\u2013acoustic reasoning, while more recent benchmarks such as WorldSense [16] and Daily-\nOmni [55] further incorporate audio cues into visual QA. Nevertheless, existing datasets predominantly\nemphasize retrospective reasoning, leaving omni-modal future prediction underexplored\nConversely, regarding future prediction task, early benchmarks like VLEP [21] and IntentQA [25] to assess\nforecasting capabilities. These datasets require models to predict future actions or anticipate long-term goals\nbased on current context. Despite their value, they are predominantly vision-centric. They typically function\nwith the audio track muted or disregarded, failing to capture scenarios where sound acts as the primary\nprecursor to a future event. Consequently, there is a distinct lack of benchmarks that simultaneously demand\nomni-modal perception and causal future reasoning, a gap that FutureOmni aims to bridge.\n3\nThe FutureOmni Benchmark\n3.1\nAudio Coordinated Video Selection\nFor our evaluation, low-quality videos are characterized by short duration, static scene changes, or audios\nserving a decorative role. Firstly, we collect approximately 18K YouTube videos ranging from 30 seconds to\n20 minutes and apply an audio coordinated video filtering strategy. Videos with limited scene change are\nremoved by computing frame-level visual similarity between adjacent frames and discarding samples whose\n3\nAudio\nCoordinated\nAudio-Visual Temporal Localization and Calibration\nDynamic\nScene\n\u2026\n01:35-01:44: The\nKing is awake\nand furious\n\u2026\n06:03-06:10:\u2026\nAudio-Coordinated Video Selection\nAudio-Visual  OFP QA Construction\n[Premise]: Tom steps on\ntacks \u2026\n[Target]: Tom runs out castle ...\n[Rationale]: King has warned\nTom, so he had to run out of the\ncastle then scream in pain \u2026\nGrounding\nPrompt\nYoutube\nDomain List\nVideo Filtering\nCartoon\nMovie\n\u2026\nFiltered Videos\nSample of\nFiltered videos\nInitial Caption\nTime Calibrated Caption\n\u2026\n01:35-01:44: The\nKing is awake\nand furious\n\u2026\n06:03-06:10:\u2026\nAudio fulfilled Caption\n\u2026\n01:52-02:00: The\nKing warns \u201dOne\nmore sound and\noff comes your\nhead\u201d\u2026\nDetection\nPrompt\nAudio fulfilled\nCaptions\nForecasting Pairs with\nRationales\nQuestion: [Premise],what will happen next ?\n(A) Tom endures the pains and runs out of\nthe castle\n(B) Tom screams in pain instantly\n(C) Tom stops to angrily search for Jerry\n(D) Tom falls to the ground\n(E)  Tom\u2019s scream wakes the King\nMFCCs\nDistraction\nPrompt\nAudio\nPrompt\n\u2026\n01:52-02:00: The\nKing warns \u201dOne\nmore sound and\noff comes your\nhead\u201d\u2026\nForecasting QA Candidates\nVerification\nPrompt\nFutureOmni\nFigure 3 The pipeline of our FutureOmni.\naverage inter-frame similarity higher than 70%. Inspired by AVoCaDO [6], we propose an audio intervention\nstrategy to filter out videos with weak audio-visual correlations or merely decorative audio tracks, which\ncalculating the semantic similarity gap between captions generated with and without audio input. For\nsimplicity, a larger similarity drop reflects stronger audio dependency. We choose UGCVideoCaptioner [41]\nfor efficiency, and we only select top-50% videos for subsequent annotation. Finally we obtain a subset of 9K\nvideos. Experiment details are in Appendix B.5.\nBenchmarks\nVideos\nAvg.Duration(s)\nQAs\nAnnotation\nw. Audio\nFF Patterns\nFF QAs\nNew Video\nVLEP [2020]\n528\n33.1\n4,192\nM\n\u2717\nT,C,R\n4,192\n\u2713\nIntentQA [2023]\n567\n46.4\n2,134\nA+M\n\u2717\nC\n503\n\u2717\nFutureBench [2025]\n866\n43.1\n1,056\nA+M\n\u2717\nT,C,R\n1,056\nAVUT [2025]\n2,662\n67.8\n13,774\nA+M\n\u2713\n\u2717\n\u2717\n\u2713\nLongVALE [2025]\n8,400\n235.0\n\u2717\nA+M\n\u2713\n\u2717\n\u2717\n\u2717\nDailyOmni [2025]\n684\n42.8\n1197\nA+M\n\u2713\n\u2717\n\u2717\n\u2717\nJointAVBench [2025]\n1,046\n97.2\n2,853\nA+M\n\u2713\n\u2717\n\u2717\n\u2717\nOmniVideoBench [2025]\n628\n384.2\n1,000\nA+M\n\u2713\n\u2717\n\u2717\n\u2713\nWorldSense [2025]\n1,662\n141.1\n3,172\nM\n\u2713\n\u2717\n\u2717\n\u2717\nFutureOmni\n919\n163.5\n1,034\nA+M\n\u2713\nT,C,R\n1,034\n\u2713\nTable 1 Comparison of FutureOmni with other representative video and audio-visual benchmarks. A and M in\nAnnotation indicate automatic and manual annotation. FF Patterns denotes Future Forecasting patterns, including\nThematic Montage (T), Causal (C), Routine Sequences (R). FF QAs represents the amount of future forecasting QAs. New\nVideo denotes whether videos are newly collected.\nrepresents partially collected (52.8%) in FutureBench.\n3.2\nAudio-Visual Temporal Localization and Calibration\nAfter filtering, the next challenge is to locate and describe events within the videos. Previous research typically\nrelied on open-source grounding models [35]; however, these models either lack the precision required\nfor dense video caption [13, 51], or they ignore the audio input. In this paper, we leverage the advanced\nmultimodal capabilities of Gemini 2.5 Flash [15] to implement grounding. Specifically, we first instruct the\nmodel to perform a comprehensive scan of the video to identify plot-relevant events while ignoring trivial or\nstatic background occurrences. We require the model to generate precise timestamps (in MM:SS format) that\ntightly bound the duration of each event.\nTime Boundary Checking. To validate precision of these boundaries, following LongVALE [13], we compute\nMel-frequency cepstral coefficients (MFCCs) at the start and end points; since valid event event transitions\ntypocally correlate with acoustic discontinuities, we verify whether MFCC differences at these time points\n4\n(a) Video Domains\n(b) Audio and Duration Distribution\nFigure 4 (a) Hierarchical distribution of 8 primary video domains and 21 fine-grained sub-categories. Surv: Surveillance,\nDaily: Dailylife, Edu: Education, Emerg: Emergency. (b) Composition of audio modalities and Average video duration.\nexceed a pre-defined threshold 2.0.\nAudio Fulfilling. Following boundary validation, we enrich the temporal segments through prompting\nGemini 2.5 Flash to identify and annotate specific acoustic cues\u2014such as dialogue, sound effects, or\nbackground music\u2014that occur synchronously with the visual content, ensuring that significant audio events\nare captured alongside the visual action.\n3.3\nAudio-Visual QA Construction\nWith the dense, omni-modal event timeline established in the previous stage, the final step involves extracting\nlogical cause-and-effect pairs from sequential events. As illustrated in Figure Figure 3, this process consists of\ntwo key phases: Causal Pair Discovery and Dual-Stage Verification.\nCausal Pair Discovery. Mere temporal succession does not imply causality. To identify events where the\nfuture is logically predictable from the past, we employ DeepSeek-V3.2 [10] to analyze adjacent event segments.\nTo ensure the reliability of the prediction task, we strictly limit the temporal gap between the premise and\nthe future event to a maximum of 30 seconds. We feed the model the chronologically ordered descriptions\nand instruct it to determine if the subsequent event is a direct logical consequence of the former. The model\nis required to explicitly output three components: Premise Event, Target Event and Rationale, which is a\nlogical explanation bridging the gap. To explicitly mine pairs driven by acoustic cues, we instruct the model\nto score the audio causal factor for each candidate pair. Specifically, the model assigns a contribution score on\na scale of 0 to 2, where 0 represents no influence, 1 indicates decoration, and 2 denotes causality. Furthermore,\nthe model classifies the audio factor into three distinct categories: Speech, Sound, and Music.\nQA Construction. Unlike prior event prediction benchmarks [21, 25] that primarily construct distractors\nbased on visual similarity, we propose four novel distractor types to rigorously evaluate the omnimodal\nreasoning abilities of MLLMs: i) Visual-only Perception, which is visually plausible given the video context\nbut is explicitly contradicted by the audio modality. This targets models that fail to integrate auditory cues\n5\ninto their reasoning process. ii) Audio-only Perception, which aligns semantically with the speech or sound\nevents but describes visual actions that do not occur or mismatch the visual scene. This challenges models\nthat over-rely on the audio transcript or sound processing while neglecting visual verification. iii) Delayed. It\ndescribes valid past events from the video that occur before the premise. This evaluates the model\u2019s temporal\nprecision ability. iv) Reverse-Causal. It describes the antecedent or cause of the premise event rather than its\neffect. This tests the model\u2019s understanding of the directional arrow of time and its ability to distinguish\nbetween past triggers and future consequences.\nDual-Stage Verfication To mitigate the ambiguity of our data, we implement a dual-stage verification strategy.\nCandidate QAs are submitted to GPT-4o [32] for automated logical validation first. Then we conduct human\nverification for quality. Verification details are in Appendix C.\n3.4\nDataset Statistics\nTable 1 and Figure 4 present a comprehensive comparison between FutureOmni and other representative\nbenchmarks. Our dataset comprises 919 high-quality videos and 1,034 QA pairs. While recent omni-modal\nbenchmarks like WorldSense and DailyOmni incorporate audio, they focus largely on retrospective perception\nor captioning, ignoring the future forecasting task. FutureOmni is the first to bridge this gap, dedicating\n100% of its samples (1,034 QAs) to Future Forecasting. Moreover, the average video duration in FutureOmni\nis 163.5 seconds, significantly longer than traditional prediction datasets like VLEP (33.1s) and FutureBench\n(43.1s). Unlike benchmarks limited to specific logic types (e.g., IntentQA focuses only on Causal), FutureOmni\ncovers a diverse spectrum of reasoning patterns, including Thematic Montage (T), Causal (C), and Routine\nSequences (R). This ensures a holistic evaluation of a model\u2019s predictive capabilities.\n4\nExperiments\n4.1\nSettings\nWe evaluate a broad spectrum of representative MLLMs on FutureOmni, categorized into three groups:(1)\nopen-source video-audio MLLMs, such as MiniCPM-o 2.6 [46] , video-SALMONN 2 [37] , Ola-7B [30] ,\nQwen2.5-Omni [42] and Qwen3-Omni [43]. (2) open-source video MLLMs, such as VideoLLaMA3 [49] and\nQwen3-VL [3]. (3) proprietary MLLMs, such as Claude Haiku 4.5 [2] , Gemini 2.5 Flash, Pro [15] and Gemini\n3 Flash [14]. All models are evaluated using their official implementations. Performance is measured by\ndirect comparison between outputs and ground-truth annotations.\n4.2\nResults on FutureOmni\nTable 2 reports the performance of representative MLLMs. We evaluate 20 models across three categories:\nOpen-Source Omni-MLLMs, Open-Source Video MLLMs, and Proprietary MLLMs. The results yield several\ninsightful observations.\n\u2022 Open-source Omni-LLMs still lag behind proprietary models. Our results indicate proprietary Omni-LLMs,\nnamely Gemini 2.5 Pro and Gemini 3 Flash, achieve an average accuracy of approximately 61%, whereas\nthe strongest open-source Omni-LLM attains only 53%. This persistent performance gap suggests that\nopen-source models with joint audio\u2013visual processing capabilities remain underexplored and offer\nconsiderable potential for further improvement.\n\u2022 Video-only LLMs consistently underperform Omni-LLMs due to their inability to leverage audio cues. Even\ncompetitive proprietary video-only models, such as GPT-4o, achieve a maximum accuracy of 49.70%,\nwhich is lower than that of open-source Omni-LLMs such as Qwen3-Omni. The gap is even larger\nfor other video-only models, highlighting the importance of audio\u2013visual integration in future event\nprediction.\n6\nMethods\nSize\nCartoon\nEdu\nEmerg\nSurv\nDaily\nMovie\nGame\nDoc\nAvg\nVideo-Audio MLLMs\nAVicuna (Tang et al. 38)\n7B\n31.62\n39.00\n26.09\n35.21\n32.81\n28.19\n33.73\n20.83\n30.37\nVideoLLaMA2 (Cheng et al. 7)\n7B\n43.59\n47.00\n29.35\n53.52\n40.62\n32.60\n57.83\n31.94\n40.75\nQwen2.5-Omni (Xu et al. 42)\n3B\n37.61\n51.00\n29.35\n57.75\n35.94\n32.16\n51.81\n25.00\n38.91\nvideo-SALMONN 2 (Tang et al. 37)\n7B\n43.59\n55.00\n39.13\n57.04\n48.44\n40.97\n57.83\n34.72\n46.03\nvideo-SALMONN 2+ (Tang et al. 37)\n7B\n50.43\n61.00\n39.13\n55.63\n52.34\n40.09\n54.22\n33.33\n47.00\nQwen2.5-Omni (Xu et al. 42)\n7B\n47.86\n55.00\n35.87\n59.86\n48.44\n40.09\n61.45\n40.28\n47.48\nOla (Liu et al. 30)\n7B\n44.44\n62.00\n42.39\n64.08\n47.66\n41.41\n59.04\n37.50\n48.54\nMiniCPM-o 2.6 (Yao et al. 46)\n8B\n48.72\n63.00\n43.48\n59.15\n50.00\n41.85\n62.65\n36.11\n49.08\nQwen3-Omni (Xu et al. 43)\n30B\n52.94\n68.00\n32.88\n62.71\n59.05\n45.60\n62.65\n49.25\n53.05\nClaude Haiku 4.5 (Anthropic 2)\n-\n55.08\n66.00\n44.57\n57.04\n51.56\n48.90\n57.83\n41.67\n52.03\nGemini 2.5 Flash (Google 15)\n-\n50.85\n70.00\n47.83\n59.15\n58.59\n51.54\n60.24\n50.00\n55.61\nGemini 2.5 Pro (15)\n-\n49.15\n75.00\n54.35\n69.01\n62.50\n51.54\n65.06\n46.53\n57.93\nGemini 3 Flash (14)\n-\n62.71\n75.00\n58.70\n80.28\n68.75\n59.03\n65.06\n53.47\n64.80\nVideo MLLMs\nVideo-LLaVA (Lin et al. 28)\n7B\n39.32\n47.00\n33.70\n41.55\n42.19\n32.16\n44.58\n29.86\n37.72\nLLaVA-NeXT (Zhang et al. 52)\n7B\n43.59\n49.00\n31.52\n49.30\n35.94\n38.33\n50.60\n31.94\n40.62\nQwen2.5-VL (Bai et al. 4)\n7B\n43.59\n58.00\n30.43\n52.82\n48.44\n37.00\n53.01\n34.72\n43.71\nQwen3-VL (Bai et al. 3)\n8B\n39.32\n64.00\n34.78\n58.45\n48.44\n38.33\n57.83\n36.11\n45.84\nVideoLLaMA3 (Zhang et al. 49)\n7B\n42.74\n59.00\n33.70\n58.16\n42.97\n43.61\n67.47\n35.66\n46.80\nQwen3-VL (Bai et al. 3)\n30B\n41.88\n66.00\n43.48\n59.15\n53.12\n41.85\n61.45\n39.58\n49.32\nGPT-4o (OpenAI 32)\n-\n44.06\n65.00\n34.78\n57.74\n52.34\n50.22\n51.80\n36.11\n49.70\nTable 2 Overall performance on FutureOmni. Edu:Education, Emerg: Emergency, Surv: Surveillance, Daily: Dailylife,\nDoc:Documentary. Models with a gray background indicate proprietary systems.\nBreakdown Results\nTable 7 represents fine-grained results on audios and durations. (1) Performance varies\nsignificantly across domains. Models generally perform better in Game and Dailylife categories (e.g., Qwen3-\nOmni scores 62.65% on Game), likely due to the predictable nature of game physics and common daily\nroutines. Conversely, the Documentary (Doc) and Emergency domains prove the most difficult, with average\nscores dropping to the 20-40% range (e.g., AVicuna scores only 20.83% on Doc). We hypothesize that\nDocumentaries often rely on complex narration (speech) to explain visual phenomena, while Emergency\nscenarios require rapid processing of chaotic audio-visual cues (e.g., sirens, screams), posing a severe test for\ncurrent models\u2019 synergistic reasoning abilities.\n(2) A Contextual Cold Start phenomenon is observed across all Omni-LLMs. As illustrated in Figure 5b, all models\nstruggle most with shortest duration across four intervals, achieving the lowest scores (e.g. Qwen3-Omni\nwith 34.90% and Gemini 3 Flash with 40.78%). Performance peaks in the medium duration range ([2,4) min)\nbefore slightly dipping for long videos. This could be attributed to future forecasting requires sufficient\nhistorical context to establish prediction, while short videos often lack the necessary narrative buildup.\n(3) Speech is consistently the most challenging modality. As presented in Figure 5a, Qwen3-Omni shows an\napproximately 10% gap between Music (57.54%) and Speech (47.99%). Even Gemini 3 Flash scores 60.52%\non Speech versus 68.31% on Music. This suggests that speech requires high-level linguistic decoding and\nsemantic alignment with visual cues, posing a greater barrier than interpreting atmospheric music or distinct\nsound events.\nModality Ablation\nTo quantify the specific contribution of each modality to future prediction, we conduct a\nmodality ablation study on four strongest open-source omni-modal models: Ola, Qwen2.5-Omni, MiniCPM-o\n2.6 and Qwen3-Omni. The results in Table 3, lead to three key conclusions. (1) The full omni-modal setting\n(A+V) consistently yields the highest performance. For instance, Qwen2.5-Omni achieves 47.48% with both\nmodalities, but drops significantly to 42.50% when provided with only video (V) or only audio (A). This\nsubstantial performance gap (approx. 5%) empirically validates the core premise of FutureOmni: accurate\nfuture prediction relies on the synergistic integration of visual dynamics and acoustic cues, rather than\non either modality in isolation. (2) While supplementing video with text-based information\u2014such as Subtitles\n7\nI\n2.5%\nII\n51.6%\nIII\n15.1%\nIV\n30.8%\nFigure 5 Error Distribution. I: Lack of Knowl-\nedge.\nII: Video Perception Error.\nIII: Audio\nPerception Error. IV: Reasoning Failure.\nAVicuna\nVideoLLaMA2\nQwen2.5-Omni-3B\nv-SALMONN\nv-SALMONN 2+\nQwen2.5-Omni-7B\nOla-7B\nMiniCPM-o\nQwen3-Omni-30B\nClaude\nGemini 2.5 Flash\nGemini 2.5 Pro\nGemini 3 Flash\nVideo-LLaVA\nLLaVA-NeXT\nQwen2.5-VL-7B\nQwen3-VL-8B\nQwen3-VL-30B\nVideoLLaMA3\nGPT-4o\n30\n40\n50\n60\n70\nAccuracy\nSpeech\nSound\nMusic\n(a) Results on three audio types.\nAVicuna\nVideoLLaMA2\nQwen2.5-Omni-3B\nv-SALMONN2\nv-SALMONN2+\nQwen2.5-Omni-7B\nOla-7B\nMiniCPM-o2.6\nQwen3-Omni-30B\nClaude Haiku 4.5\nGemini2.5 Flash\nGemini2.5Pro\nGemini3Flash\nVideo-LLaVA\nLLaVA-NeXT\nQwen2.5-VL-7B\nQwen3-VL-8B\nQwen3-VL-30B\nVideoLLaMA3\nGPT-4o\n30\n40\n50\n60\n70\n80\nAccuracy\n[0,2) min\n[2,3) min\n[3,4) min\n[4,20) min\n(b) Results on four duration intervals.\nFigure 6 Fine-grained results on audio types (a) and duration intervals\n(b).\n(V+Subtitle) or detailed Captions (V+Caption)\u2014improves performance over the video-only baseline, it still falls short\nof the full A+V setting. For example, Ola scores 48.54% with raw audio but only 46.95% with subtitles.\nThis suggests that the raw audio signal contains rich, non-verbal latent information (e.g., emotional tone,\nenvironmental atmosphere, urgency) that cannot be fully captured by textual transcription alone. (3) The\nperformance of omni-modal models on Audio-only (A) and Video-only (V) is strikingly similar. This indicates that the\ndataset is well-balanced; the models cannot shortcut the task by relying solely on visual pattern matching or\naudio classification. To achieve the state-of-the-art results seen in the A+V column, the model must genuinely\nperform cross-modal reasoning.\nError Analysis\nWe choose 318 failure cases from Gemini 3 Flash into four categories between knowledge\ndeficits and reasoning failures. Details are in Appendix B.3. The distribution in Figure 5 reveals three insights:\n(1)Visual Perception is the Primary Bottleneck. The majority of errors (51.6%) stem from Video Perception Errors.\nThis indicates that despite strong general capabilities, the SOTA model still struggle to capture the fine-grained\nvisual dynamics. (2) The Synergistic Reasoning Gap. A substantial portion (30.8%) are Audio-Video Joint\nReasoning Failures. In these cases, the model perceives individual modalities but fails to synthesize them\nlogically (e.g. failing to link a visual action with its corresponding sound effect) to derive the future, validating\nthe omni-modal challenge of out benchmark. (3) Reasoning over Knowledge. Notably, Lack of Knowledge\naccounts for a negligible 2.5% errors. This confirms that current MLLMs possess sufficient world knowledge;\nthe performance gap on FutureOmni is driven by limitations in dynamic perceptions and complext causal\nreasoning rather than a lack of factual data.\n5\nOmnimodal Future Forecasting: From Prediction to Generalization\nAs demonstrated in the previous section, current MLLMs exhibit a significant deficiency in omni-modal\nfuture forcasting. To bridge this gap, we curate a high-quality instruction tuning dataset, FutureOmni-7K,\nand propose a Omni-Modal Future Forecasting method. For empowering models with foresight, we integrate\n8\nMethods\nA+V\nV\nV+S\nA\nA+C\nQwen3-Omni (30B)\n53.05\n51.50-1.55\n52.76-0.29\n50.92-2.13\n50.34-2.71\nMiniCPM-o 2.6 (8B)\n48.54\n48.25-0.29\n49.80+1.26\n48.93+0.39\n48.54\nOla (7B)\n48.54\n43.27-5.27\n46.95-1.59\n46.47-2.07\n46.85-1.69\nQwen2.5-Omni (7B)\n47.48\n42.50-4.98\n43.85-3.63\n42.50-4.98\n44.24-3.24\nTable 3 Results of Modality Ablation. A:audio, V:video, S:subtitle, C:caption.\nMethods\nSpeech Sound Music\nAvg\nQwen2.5-Omni\n37.83\n54.55\n53.85\n47.48\n+OFF\n47.75\n47.55\n50.46\n48.51+1.03\nvideo-SALMONN 2\n39.95\n47.20\n52.92\n46.03\n+OFF\n44.68\n54.39\n52.62\n49.90+3.87\nOla\n40.43\n52.80\n55.38\n48.54\n+OFF\n42.55\n53.50\n57.23\n50.19+1.65\nTable 4 Audio Performance with OFF.\nMethods\nCartoon Edu Emerg Surv Daily Movie Game Doc\nAvg\nQwen2.5-Omni\n47.86\n55.00\n35.87\n59.86 48.44\n40.09\n61.45 40.28 47.48\n+OFF\n50.92\n58.82\n49.33\n41.10 65.25\n55.24\n44.56 62.65 48.51+1.03\nvideo-SALMONN 2\n43.59\n55.00\n39.13\n57.04 48.44\n40.97\n57.83 34.72 46.03\n+OFF\n42.59\n60.00\n50.00\n61.97 55.46\n40.97\n63.85 37.50 49.90+3.87\nOla\n44.44\n62.00\n42.39\n64.08 47.66\n41.41\n59.04 37.50 48.54\n+OFF\n44.44\n59.00\n42.39\n66.20 51.56\n44.49\n60.24 40.28 50.19+1.65\nTable 5 Category Performance with OFF.\nthe rationales derived from our data construction pipeline into each training instance. By explicitly exposing\nthe reasoning chain, which elucidating why a specific future event follows from the audio-visual premise, we\naim to teach the model not just to predict the outcome, but internalize the reasoning logic of future prediction.\nMethods\nAudio-Visual Bench\nVideo-only Bench\nWorldSense DailyOmni JointAVBench OmniVideoBench Video-MME MLVU\nQwen2.5-Omni\n37.67\n45.69\n59.30\n30.70\n53.77\n54.00\n+OFF\n40.22+2.55\n49.03+3.34\n60.88+1.58\n31.70+1.00\n55.51+1.74 54.37+0.37\nvideo SALMONN 2\n48.29\n65.13\n60.21\n34.90\n61.40\n68.00\n+OFF\n48.77+0.48\n65.80+0.67\n61.16+0.95\n35.40+0.50\n61.25-0.15\n67.86-0.14\nOla\n44.07\n53.04\n50.29\n35.50\n48.00\n51.93\n+OFF\n44.10+0.03\n53.63+0.59\n51.13+0.84\n36.90+1.40\n48.07+0.07 52.53+0.60\nTable 6 General Capability Analysis Results.\n5.1\nMain Experiment\nSettings\nWe conduct experiments on three representative open-source omni-modal models: Qwen2.5-Omni-\n7B, Ola-7B, and video-SALMONN 2-7B. To ensure computational efficiency, we use LoRA [17] for fine-tuning.\nDuring the training process, we keep the visual and audio encoders frozen and only update text backbones.\nThe learning rate is set to 1e-5, and the models are trained for 1 epoch. All other hyperparameters and\nconfigurations remain consistent with the official training scripts of the respective models. Other details are\nin Appendix B.1.\nResults\nTable 4 and Table 5 demonstrate the effectiveness of tuning on FutureOmni-7K. All models exhibit\nconsistent gains, with video-SALMONN 2 achieving the largest overall increase of +3.87%. Most notably, the\ntraining significantly boosts performance in the challenging Speech category; Qwen2.5-Omni witnesses a\nsubstantial leap of nearly 10%. This confirms that our instruction tuning effectively enhances the models\u2019\nability to interpret complex acoustic cues, particularly dialogues.\n5.2\nGeneral Capability Analysis\nTo investigate whether the future prediction capability acquired from FutureOmni-7K can transfer to general\ndomains, we evaluate our fine-tuned models on a suite of out-of-domain benchmarks.\nSettings\nSpecifically, we selected four representative omni-modal benchmarks: WorldSense, DailyOmni,\nJointAVBench, and OmniVideoBench, to test audio-visual synergy. Additionally, to assess impacts on pure\nvisual understanding, we included two Video-only benchmarks: Video-MME [12] and MLVU [54].\n9\nResults\nThe results in Table 6 demonstrate the generalization ability of OFF, training solely on future\nprediction leads to consistent improvements across multiple omni-modal QA tasks not relevant with\nthe forecasting task. For instance, Qwen2.5-Omni achieves notable gains on WorldSense (+2.55%) and\nDailyOmni (+3.34%). Remarkably, these benefits extend even to video-only benchmarks where audio is\nabsent. Qwen2.5-Omni shows clear improvements on Video-MME (53.77% to 55.51%) and MLVU (54.00% to\n54.37%).\nL0\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9\nL10\nL11\nL12\nL13\nL14\nL15\nL16\nL17\nL18\nL19\nL20\nL21\nL22\nL23\nL24\nL25\nL26\nL27\n1\n0\n1\n2\n3\nKeyframe Attention Score Difference\n1e\n6\nFigure 7 Attention score difference visualization. The blue represents the attention difference for Video keyframes, while\nthe yellow represents Audio keyframes.\nAttention Visualization\nTo investigate the mechanism of this generalization, we analyze the internal attention\ndistribution of Qwen2.5-Omni before and after training. We choose LongVALE for this experiment due to the\nwell annotations of video and audio keyframes. We propose a metric, called Keyframe Attention Score Difference,\nwhich measures the shift in attention magnitude assgined to ground-truth video and audio keyframes\nacross transformer layers. As illustrated in Figure 7, the results uncover following pattern beneficial to the\ngeneralization: Active Information Seeking. The trained model (positive values) pays more attention to both\nVideo (Blue) and Audio (Orange) keyframes in these critical layers. For instance, at Layer 9 and Layer 20, the\nmodel\u2019s focus on visual cues intensifies dramatically. Simultaneously, the audio attention (Orange) shows a\nconsistent elevation across the middle layers (L8-L17).\n6\nConclusion\nIn this work, we introduce FutureOmni, the first comprehensive benchmarks dedicated to evlauating\nthe Omni-modal Future Prediction capabilites of MLLMs. By establishing a rigorous human-in-the-loop\ndata construction pipeline, we curate a high-quality dataset strictly demands synergistic audio-visual\nreasoning. Our extensive evaluation reveals that current MLLMs struggles with OFP, particularly in speech-\ndense scenarios. To address this deficiency, we construct a rationale-enhanced instruction-tuning dataset,\nFutureOmni-7K and propose an Omni-Modal Future Forecasting training strategy not to sharpen cross-modal\nfuture prediction abilities, but to enhance performance across a broad spectrum of general tasks.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**Summary of the Paper \"OpenMOSS FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs\"**\n\n### Introduction:\nThe paper introduces **FutureOmni**, a comprehensive benchmark designed to evaluate multimodal large language models (MLLMs) on future event forecasting under omnimodal contexts. The focus is on cross-modal causal reasoning, temporal reasoning, and the effective use of internal knowledge for predicting future events from audio-visual cues.\n\n### Main Contributions:\n1. **FutureOmni**: Introduces FutureOmni as the first benchmark to evaluate MLLMs' ability in forecasting future events under omnimodal contexts. It contains 919 videos and 1,034 multiple-choice QA pairs across eight primary domains.\n2. **Instruction-Tuning Dataset (OFF)**: Proposes a high-quality instruction-tuning dataset named FutureOmni-7K to enhance the performance of open-source models on their benchmark. The proposed training strategy, Omni-Modal Future Forecasting (OFF), significantly improves future forecasting and generalization ability.\n3. **Comprehensive Evaluation**: Extensive experiments were conducted using 20 MLLMs, including both omnimodal and video-centric models, covering proprietary and open-source systems. Results reveal that current models struggle with audio-visual future prediction, particularly in speech-heavy scenarios.\n\n### Methodology:\n1. **Data Construction Pipeline**:\n   - **Human-in-the-loop**: A scalable AI-assisted pipeline was constructed to ensure high-quality data construction while maintaining efficiency.\n   - **Video Selection**: Low-quality videos were filtered based on short duration and static scene changes. An audio-coordinated video selection strategy was used.\n   - **Temporal Localization and Calibration**: Events within the selected videos were precisely localized using advanced multimodal capabilities of Gemini 2.5 Flash.\n   - **Audio-Visual QA Construction**: Logical cause-and-effect pairs from sequential events were extracted, with specific distractors designed to rigorously evaluate MLLMs' omnimodal reasoning abilities.\n\n### Key Findings:\n1. **Performance Gaps**:\n   - Existing omni-modal and video-centric models struggle significantly in future forecasting, achieving only 64.8% accuracy on average.\n   - Video-only models perform even worse, with the best proprietary model achieving a maximum of 49.70%.\n\n2. **Modality Contribution Analysis**: \n   - The full omnimodal setting (A+V) consistently yields the highest performance, indicating that accurate future prediction relies on synergistic integration of visual dynamics and acoustic cues.\n   - Supplementing video with text-based information improves performance over the video-only baseline but still falls short compared to the full A+V setting.\n\n3. **Generalization**: \n   - Training solely on FutureOmni-7K leads to consistent improvements in multiple omni-modal QA tasks, even extending benefits to pure visual understanding benchmarks like Video-MME and MLVU.\n\n### Conclusions:\n1. **FutureOmni** is a pioneering benchmark for evaluating MLLMs' future forecasting abilities under omnimodal contexts.\n2. The proposed instruction-tuning dataset (OFF) significantly enhances the performance of open-source models on their benchmark, improving both future forecasting and generalization ability.\n3. FutureOmni highlights significant gaps in current MLLMs\u2019 capabilities, particularly in speech-heavy scenarios, underscoring the need for further research and development.\n\n### Code and Dataset Availability:\n- **Code**: https://github.com/OpenMOSS/FutureOmni\n- **Dataset**: https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni\n\nThis comprehensive benchmark and proposed training strategy pave the way for future research in multimodal language understanding, particularly in enhancing models' ability to forecast future events from audio-visual inputs."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "1\nDevice Association and Resource Allocation for Hierarchical Split\nFederated Learning in Space-Air-Ground Integrated Network\nHaitao Zhao, Xiaoyu Tang, Bo Xu*, Jinlong Sun, Linghao Zhang\nAbstract\u20146G facilitates deployment of Federated Learning\n(FL) in the Space-Air-Ground Integrated Network (SAGIN),\nyet FL confronts challenges such as resource constrained and\nunbalanced data distribution. To address these issues, this pa-\nper proposes a Hierarchical Split Federated Learning (HSFL)\nframework and derives its upper bound of loss function. To\nminimize the weighted sum of training loss and latency, we\nformulate a joint optimization problem that integrates device\nassociation, model split layer selection, and resource allocation.\nWe decompose the original problem into several subproblems,\nwhere an iterative optimization algorithm for device association\nand resource allocation based on brute-force split point search\nis proposed. Simulation results demonstrate that the proposed\nalgorithm can effectively balance training ef\ufb01ciency and model\naccuracy for FL in SAGIN.\nIndex Terms\u2014Space-air-ground integrated network, split fed-\nerated learning, device association, resource allocation.\nI. INTRODUCTION\nDriven by dual requirements of 6G for global coverage\nand edge intelligence, the Space-Air-Ground Integrated Net-\nwork (SAGIN), leveraging the wide coverage of low-earth\norbit (LEO) satellites, \ufb02exible edge deployment of Unmanned\nAerial Vehicles (UAVs), and ubiquitous sensing of mobile\ndevices [1], has emerged as the core architecture for scenar-\nios such as remote communications, emergency rescue, and\nintelligent computing [2]. In particular, Federated Learning\n(FL), which enables privacy-preserving collaborative global\nmodel optimization, is pivotal for SAGIN based edge intel-\nligence [3]. The authors in [4] leveraged the computation and\ncommunication resources in SAGIN, aiming to facilitate and\naccelerate the deployment of FL in remote regions. Besides, a\nhierarchical FL framework based on dynamic reward function\nfor SAGIN was proposed in [5], using a decoupled-recoupled\nalgorithm to optimize resource allocation and aggregation\nweights. However, multi-layer heterogeneity in SAGIN and\nresource constraints pose critical challenges to FL. On one\nhand, resource-constrained mobile devices cannot support\nef\ufb01cient deep neural network (DNN) training, resulting in\nexcessive computational latency [6]. On the other hand, the\nheterogeneity of data collected by different mobile devices\nresults in model aggregation error and makes it dif\ufb01cult for\nthe model to converge quickly [7].\nThe work of was supported in part by the National Natural Science\nFoundation of China under Grant U2441226.\nHaitao Zhao, Xiaoyu Tang, Bo Xu, Jinglong Sun, Linghao Zhang are\nwith the Jiangsu Key Laboratory of Wireless Communications, Nanjing\nUniversity of Posts and Telecommunications, Nanjing 210003, China (e-mail:\nzhaoht@njupt.edu.cn, 1025010240@njupt.edu.cn, xubo@njupt.edu.cn, sunjin-\nlong@njupt.edu.cn, 2024211102@njupt.edu.cn. ).(Corresponding author: Bo\nXu.)\nSplit Federated Learning (SFL) can offer a solution to the\naforementioned challenges by splitting DNN models layer-\nwise. Resource constrained devices handle shallow-layer com-\nputations, where deep-layer tasks are of\ufb02oaded to edge servers\n[8]. The authors in [9] focused on split point selection and\nbandwidth allocation for minimal SFL latency, solved via\nalternating optimization. In [10], a hierarchical SFL framework\nwith convergence performance analysis was proposed, jointly\noptimizing model splitting and aggregation through iterative\ndescent. Besides, the authors in [11] introduced a communi-\ncation and computation ef\ufb01cient SFL framework with dynamic\nmodel splitting and aggregated gradient broadcasting.\nHowever, existing studies have not integrated SFL with the\nSAGIN architecture, nor have they fully considered the impact\nof unbalanced data distribution, which has largely hindered\nthe ef\ufb01cient training of models in the SAGIN scenario. To\naddress these issues, we analyze the convergence performance\nand derive the impacts of split layer selection and device\nassociation on performance. On this basis, we establish a joint\noptimization problem that minimizes the weighted sum of\ntraining loss and latency by optimizing split layer selection,\ndevice association, and resource allocation. Compared with\n[9], the proposed algorithm incorporates model spliting, which\ncan reduce the local training load of devices. Meanwhile,\nin comparison with [12], we incorporate device association\nmodeling and, based on the analytical \ufb01ndings, effectively\nmitigate the adverse effects of imbalance data distribution on\nmodel training. Simulation results can validate the theoretical\nanalysis and demonstrate the effectiveness of the proposed\nsolution.\nII. SYSTEM MODEL\nFor the proposed SAGIN-HSFL framework, we de\ufb01ne the\nsets of UAVs and mobile devices as K and N, respectively.\nUAV k \u2208K covers Nk mobile devices, and the set is denoted\nas Nk. Besides, the set of UAVs available for association with\nmobile device n \u2208N is de\ufb01ned as Kn. We introduce a binary\nvariable an,k \u2208{0, 1} to indicate the association between\ndevice n and UAV k, where an,k = 1 means device n is served\nby UAV k for local model updates, and an,k = 0, otherwise.\nThe traf\ufb01c load of UAV k is de\ufb01ned as nk = P\nn\u2208Nk an,k.\nFor learning tasks, we consider classi\ufb01cation or recognition\nproblems involving C categories, where the category set is\ndenoted by C = {1, 2, . . ., C}.\nA. Training Process and Convergence Analysis\nFor each device n, the goal of model training is to \ufb01nd an\noptimal function Hwn(x), where x is the training data, y is\narXiv:2601.13817v1  [cs.DC]  20 Jan 2026\n2\nthe actual label, and wn represents the model parameters. The\noptimal model can be obtained by minimizing the distance\nbetween the network output and the label y. Therefore, the\nloss function of device n can be formulated as\nFn\n\u0000wU\nn , wB\nn\n\u0001\n\u225c\n1\n|Xn|\nX\nx\u2208Xn,y\u2208Yn\nf\n\u0000HwB\nn\n\u0000HwU\nn (x)\n\u0001\n, y\n\u0001\n,\n(1)\nwhere wU\nn and wB\nn denote the model parameters of the front-\nend and the back-end subnetwork, respectively. |Xn| is the\nnumber of training data for mobile device n, x is the training\ndata in set Xn, and Yn represents the corresponding label set\nof Xn.\nWe examine four key stages in the training process.\n1) Model Initialization. At the beginning of each global\ntraining round, the satellite assesses the round\u2019s expected total\ntraining latency T and its available service latency \u03c4s. If\nservice time is insuf\ufb01cient, the satellite can select an adjacent\nsatellite for model migration, otherwise, it initiates FL training\ndirectly.\n2) Forward Propagation. For the t-th local iteration, each\ndevice n generates intermediate results Ak\nn,t = HwU,k\nn,t (x)\nby leveraging its local data and the current shallow network\nparameters wU,k\nn,t , and subsequently uploads intermediate result\nand label to its associated UAV k via the air-to-ground\nwireless link. Upon receiving the intermediate outputs from\nall connected mobile devices within its coverage area, UAV\nk can perform forward propagation using the deep network\nparameters wB,k\nn,t and compute the corresponding loss value.\n3)\nBackward\nPropagation\nand\nParameter\nUpdate.\nEach UAV k updates the parameters of its subnetwork\nwB,k\nn,t\nthrough\nbackward\npropagation\nand\nsends\nthe\nbackward\ngradient\nat\nthe\nsplit\nlayer\nwith\nlearning\nrate \u03b1. Then, the model parameters are updated with\ngradient\n\u03b1 P\nc pn(c)\u2207wU,k\nn,t Ex|y(c)\nn\nlog\n\u0010\nHwU,k\nn,t (x)\n\u0011o\nand\n\u03b1 P\nc pn(c)\u2207wB,k\nn,t Ex|y(c)\nn\nlog\n\u0010\nHwB,k\nn,t (Ak\nn,t)\n\u0011o\n, respectively,\nwhere pn(c) denotes the data distribution of device n, y|(c)\ndenotes the set of samples with label y belonging to class c\nin the classi\ufb01cation task of FL.\n4) Sub-model Aggregation on UAVs. After E local iteration\nupdates, all subnetworks wB,k\nn,t are aggregated on the UAV, and\nthe updated model is sent to the satellite for global aggregation.\nAfter global aggregation, the satellite distributes the generated\nglobal model to all UAVs. Then, one round of global train-\ning is completed. Edge aggregation and global aggregation\nare performed as wk\nt+E+1 =\n1\nnk\nP\nn\u2208Nk an,kwB,k\nn,t+E, and\nwG\nt+E+1 =\n1\nP\nk nk\nP\nk nkwk\nt+E+1, respectively.\nTo simplify the analysis process, we make the following\nassumptions.\nAssumption 1: Fn(w) is \u03b2-Lipschitz smooth with \u03b2 > 0,\ni.e., Fn(w) \u2264Fn(w\u2032) + (w \u2212w\u2032)T \u2207Fn(w\u2032) + \u03b2\n2 \u2225w \u2212w\u2032\u22252\n2.\nAssumption 2: Fn(w) is \u00b5-strongly convex with \u00b5 > 0, i.e.,\nFn(w) \u2265Fn(w\u2032) + (w \u2212w\u2032)T \u2207Fn(w\u2032) + \u00b5\n2 \u2225w \u2212w\u2032\u22252\n2.\nAssumption\n3:\nThe\nvariance\nof\nthe\ngradient\nof\neach\nlayer\nin\nthe\ndevice\nhas\nan\nupper\nbound,\ni.e.,\nE\n\u2207Fn\n\u0010\nwU,k\nn,t , wB,k\nn,t , \u03ben,t\n\u0011\n\u2212\u2207Fn\n\u0010\nwU,k\nn,t , wB,k\nn,t\n\u0011\n2\n\u2264L\u03c32,\nwhere \u03ben,t is the mini-batch data sampled from device n in\nthe t-th iteration, L is the total number of layers of the DNN\nmodel, and \u03c3 is a \ufb01nite constant.\nAssumption\n4:\nThe\nexpected\nsquared\nnorm\nof\nthe\ngradient\nof\neach\nlayer\nhas\nan\nupper\nbound,\ni.e.,\nE\n\u2207Fn\n\u0010\nwU,k\nn,t , wB,k\nn,t , \u03ben,t\n\u0011\n2\n\u2264\nLZ2,\nwhere\nZ\nis\na\n\ufb01nite constant.\nAssumption\n5:\nThe\ngradient\n\u2207wEx|y(c) {log (Hw(x))}\nis\n\u03c6(c)-Lipschitz\nsmooth,\ni.e.,\n\u2207wEx|y(c) {log (Hw1(x))} \u2212\u2207wEx|y(c) {log (Hw2(x))}\n\u2264\n\u03c6(c) \u2225w1 \u2212w2\u2225.\nAssumption 6: The gradient at the mE-th iteration is\nbounded as\n\u2207wEx|y(c)\n\b\nlog\n\u0000Hwn,mE(x)\n\u0001\t\r\r \u2264AmE, where\nAmE denotes the non-negative upper bound constant of the\ngradient norm.\nTheorem 1: Let \u03c1 = \u03b2\n\u00b5 and \u03b3 = max{8\u03c1, mE} \u22121. After\nmE iterations, given the learning rate \u03b1 =\n4\n\u00b5(\u03b3+t), the upper\nbound of the difference between the average value of the loss\nfunctions of all devices is evaluated as\nE\n( X\nn\u2208Nk\nan,k\nnk\n(Fn (wn,mE))\n)\n\u2212F \u2217\n\u2264\n\u03c1\n\u03b3 + mE\n\"\n8Pn\n\u00b5\n+ \u00b5\n2 (\u03b3 + 1)E\nX\nn\u2208Nk\nan,k\nnk\n\u2225wn,1 \u2212w\u2217\u22252\n#\n,\n(2)\nwhere wn,mE =\nh\nwU,k\nn,mE, 1\nnk\nP\nn\u2208Nk an,kwB,k\nn,mE\ni\n, F \u2217is the\noptimal traning loss, Pn = 2(mE\u22121)2LZ2+4\u03b2\u0393+\u2113Z2+ 1\nnk (L\u2212\n\u2113)Z2+\u2113\u03c32+ 1\nnk (L\u2212\u2113)\u03c32+1\n2\u00b5 P\nn\u2208Nk\n1\nnk E{\n\u0000 Pk\u22121\ni=0 AmE\u2212i\u22121\n\u00001\u2212\n\u03b1 P\nc pn(c)\u03c6(c)\n\u0001i\u00012 P\nc (an,k\u2225pn(c)\u2212p(c)\u2225)2},\nand\n\u0393\n=\nP\nn\u2208Nk\nan,k\nnk (F \u2217\u2212F \u2217\nn) denotes the impact of data hetero-\ngeneity.\nProof. See Proof of Theorem 1.\nThe theorem indicates that higher split layer \u2113lead to\nan increased convergence upper bound and degraded model\nperformance, as fewer aggregated DNN parameters restrict the\namount of device-side information, thereby weakening model\ngeneralization. Furthermore, the error upper bound escalates\nwith the degree of data distribution discrepancy, which is\nstrongly in\ufb02uenced by device association.\nB. Communication and Computation Models\nThe path loss between UAV k and mobile device n is\nde\ufb01ned as PLn,k = 10\u03d5 log(sn,k) + \u03b7(\u03c9 \u2212\u03c90)e\n\u03c90\u2212\u03c9\n\u03b3\n+ k0,\nwhere sn,k is the horizontal distance, \u03c9 and \u03c90 denote the\nvertical angle and its offset, respectively, \u03d5 is the terrestrial\npath loss exponent, \u03b7 and k0 represent the excess path loss\nand its offset, respectively, and \u03b3 is the angle attenuation\nfactor [13]. Then, the uplink data rate of device n is de\ufb01ned\nas rn,k = an,kln,kBU\nk Rn,k, where BU\nk denotes the uplink\nbandwidth for the UAV k, ln,k is the bandwidth allocation\nproportion for device n, Rn,k = log2\n1 + pU\nn 10\u2212\nP Ln,k\n10\nN0\n!\nis the uplink spectral ef\ufb01ciency, pU\nn is the transmit power\nof device n, and N0 denotes the power of additive white\nGaussian noise. Thus, the transmission latency for uploading\n3\nthe intermediate features can be expressed as t\u2113\nn,k =\nM\u2113\nrn,k ,\nwhere M \u2113is the data size of intermediate features output\nby the split layer \u2113. In addition, when the model is split\nat the \u2113-th layer, the local computation latency of device n\ncan be expressed as t\u2113\nn,cp =\nC\u2113\nn\nfn , where C\u2113\nn represents the\ncomputational load, and fn is the computation capability of\ndevice n. Besides, when UAV k is associated with device\nn, the computation latency at the UAV k is expressed as\nt\u2113\nk,cp =\nC\u2113\nn,k\nfn,k , where C\u2113\nn,k denotes the computational workload\nof the model part of\ufb02oaded by device n to UAV k, and fn,k\nis the computation resource allocated by UAV k to device n.\nFurthermore, for the UAV-satellite link, the channel gain\nfrom UAV k to the satellite can be calculated by Hk,s =\nGUAV Gsat\n\u0010\n\u03bb\n4\u03c0sk,s\n\u00112\n10\u2212Frain\n10\n, where GUAV and Gsat are\nthe antenna gains of the UAV and the satellite, respectively, \u03bb\ndenotes the carrier wavelength, sk,s is the distance between\nUAV k and the satellite, and Frain represents the rain at-\ntenuation, which is modeled as a Weibull distribution [14].\nConsequently, the transmission rate from UAV k to the satellite\nis de\ufb01ned as rk,s = BU\nk log2\n\u0010\n1 + pU\nk Hk,s\nN0\n\u0011\n, where pU\nk is the\ntransmit power of the UAV k. Thus, the transmission latency\nfor uploading the edge models is determined by the bottleneck\nUAV and can be expressed as ts = maxk\n\u00af\nM\u2113\nrk,s , where\n\u00af\nM \u2113is\nthe amount of model data uploaded by the UAV.\nFinally, considering synchronous aggregation, we further\nde\ufb01ne td = max{an,k\n\u0010\nt\u2113\nn,cp + t\u2113\nn,k\n\u0011\n} as the maximum com-\nputation and upload latency of the device. Similarly, de\ufb01ne\ntu = max{an,ktcp\nk } as the maximum computation latency\namong the UAVs. Consequently, the total system training\nlatency is expressed as T = td + tu + ts + Nsw\u03c4s, where Nsw\nrepresents the number of satellite handovers, and \u03c4s represents\nthe switching time between satellites.\nIII. PROBLEM FORMULATION AND SOLUTION APPROACH\nA. Problem Formulation\nLeveraging the results from the convergence analysis, we\nformulate a joint optimization problem P1 to minimize training\nloss and system latency\nmin\n{\u2113,an,k,ln,k,Bk,fn,k}\n(1 \u2212\u03b8)T + \u03b8P\n(3)\ns.t.\nX\nn\u2208Nk\nan,kln,k \u22641, k \u2208K,\n(3a)\nX\nk\u2208K\nBU\nk \u2264BU,\n(3b)\nX\nk\u2208Kn\nan,k = 1,\n(3c)\nan,k \u2208{0, 1}, n \u2208N, k \u2208K\n(3d)\nX\nn\u2208Nk\nfn,k \u2264fk, k \u2208K\n(3e)\nfn,k \u22650, n \u2208N, k \u2208K\n(3f)\n\u2113\u2208{1, 2, ..., L},\n(3g)\nwhere P\n= \u2113Z2 +\n1\nnk (L \u2212\u2113)Z2 + \u2113\u03c32 +\n1\nnk (L \u2212\u2113)\u03c32 +\n1\nnk\nP\nc\n\u0000P\nn\u2208Nk an,k\u2225pn(c) \u2212p(c)\u2225\n\u0001\n, BU denotes the total\nuplink bandwidth of all UAVs and fk represents the total\ncomputation resources of UAV k. Besides, constraints (3a) and\n(3b) are related to bandwidth resource allocation, constraints\n(3c) and (3d) pertain to device association, and constraints (3e)\nand (3f) are associated with computational resource allocation.\nGiven the split point \u2113, the optimization objective can be\nwritten as I =\n1\nnk \u03b8 P\nc qk,c + (1 \u2212\u03b8)T , then the subproblem\nP2 is de\ufb01ned as\nmin\n{an,k,ln,k,Bk,fn,k}\nI\n(4)\ns.t.\n(3a) - (3g) .\nThis problem is a mixed-integer nonlinear programming\n(MINLP) problem. It is noteworthy that the data heterogeneity\nmetric only depends on the association variable an,k and is\ndecoupled from the resource allocation variables, which allows\nthe problem to be decomposed.\nB. Resource Allocation\nGiven the device association strategy, the problem is re-\nformulated as minimizing latency in the device-UAV phase.\nMoreover, we observe that bandwidth resource allocation di-\nrectly in\ufb02uences td, whereas computational resource allocation\nindependently affects tu, and these two factors are mutually\nindependent.\n1) Bandwidth Resource Allocation: To minimize the la-\ntency td, we jointly optimize the bandwidth allocation ratio\nand UAV uplink bandwidth, i.e.,\nmin\n{ln,k,Bk,td}\ntd\n(5)\ns.t.\n(3a), (3b), (3g),\nan,k\n\u0012C\u2113\nn\nfn\n+ M \u2113\nrn,k\n\u0013\n\u2264td, n \u2208N, k \u2208K. (5a)\nThe optimal bandwidth resource allocation can be derived\nfrom the following theorem\nTheorem 2: Given the device association an,k, the optimal\nbandwidth allocation ratio l\u2217\nn,k and UAV uplink bandwidth\nBU\u2217\nk\nare expressed as\nl\u2217\nn,k = an,k\nM \u2113\n\u0010\nt\u2217\nd \u2212C\u2113\nn\nfn\n\u0011\nBU\u2217\nk Rn,k\n,\n(6)\nand\nBU\u2217\nk\n=\nX\nn\u2208Nk\nan,k\nM \u2113\n\u0010\nt\u2217\nd \u2212C\u2113n\nfn\n\u0011\nRn,k\n,\n(7)\nwhere t\u2217\nd is the optimal learning latency for local device com-\nputation and upload, which can be solved from the following\nequation using the bisection method i.e.,\nX\nk\u2208K\nX\nn\u2208N\nan,k\nM \u2113\n\u0010\nt\u2217\nd \u2212C\u2113n\nfn\n\u0011\nRn,k\n= BU.\n(8)\n2) Computational Resource Allocation: To minimize the\nUAV computational latency tu, we can derive that\nmin\n{fn,k}\ntu\n(9)\ns.t.\n(3e), (3f), (3g).\n4\nSince the objective function and constraints are convex,\nthere exists a unique global optimal solution. By applying the\nKarush-Kuhn-Tucker (KKT) conditions, the optimal computa-\ntional resource allocation is evaluated as\nf \u2217\nn,k =\nC\u2113\nn,kfk\nP\nn\u2208Nk C\u2113\nn,k\n.\n(10)\nC. Device Association\nAfter determining the optimal resource allocation, the prob-\nlem is simpli\ufb01ed to optimizing the device-UAV association to\nbalance latency and data heterogeneity, i.e.,\nmin\n{an,k}\n\u03b8 1\nnk\nX\nc\nX\nn\u2208Nk\nan,k\u2225pn(c) \u2212p(c)\u2225\n!\n+ (1 \u2212\u03b8)T\n(11)\ns.t.\n(3c), (3d), (3g),\nX\nk\u2208K\nX\nn\u2208N\nan,k\nM \u2113\n\u0010\nt\u2217\nd \u2212C\u2113n\nfn\n\u0011\nRn,k\n\u2264BU,\n(11a)\nP\nn\u2208Nk C\u2113\nn,k\nt\u2217u\n\u2264fk, k \u2208K,\n(11b)\nwhere t\u2217\nd and t\u2217\nu are the optimal latencys for local device\ncomputation/upload and UAV computation under a given\nassociation strategy, respectively. To linearize the norm term\n\u2225pn(c) \u2212p(c)\u2225, an auxiliary variable qk,c is introduced, satis-\nfying\nmin\n{an,k,qk,c}\n1\nnk\n\u03b8\nX\nc\nqk,c + (1 \u2212\u03b8)T\n(12)\ns.t.\n(3c), (3d), (11a), (11b),\nqk,c \u2265\nX\nn\nan,k(pn(c) \u2212p(c)), k \u2208K, c \u2208C,\n(12a)\nqk,c \u2265\nX\nn\nan,k(p(c) \u2212pn(c)), k \u2208K, c \u2208C.\n(12b)\nBy integrating constraints into the objective function via\nLagrangian relaxation , the \ufb01nal device association criterion\nis derived as:\nk\u2217\nn = arg min\nk\u2208Kn\n\uf8f1\n\uf8f2\n\uf8f3\u03c8\u2217\nM \u2113\n\u0010\nt\n\u2032\nd \u2212C\u2113\nn\nfn\n\u0011\nRn,k\n+ \u03bd\u2217\nP\nn\u2208Nk C\u2113\nn,k\nt\u2032u\n+\nX\nc\n\u0000\u03bb\u2217\nk,c \u2212\u00b5\u2217\nk,c\n\u0001\n(pn(c) \u2212p(c))\n)\n,\n(13)\nwhere \u03bbk,c, \u00b5k,c, \u03c8\u2217, \u03bd\u2217are non-negative multipliers. This cri-\nterion comprehensively considers communication cost, com-\nputational cost, and data heterogeneity.\nWe adopt exhaustive search for model split layer l selection,\nsolving the joint optimization problem over all candidate\nsplit layers and selecting the optimal layer and solution via\nobjective function value comparison. The proposed algorithm\nis summarized as Algorithm 1. The algorithm traverses L\ncandidate split layers, contributing a complexity of O(L).\nAlgorithm 1 Primal-Dual Algorithm for the Split Layer-\nDevice-Resource Joint Optimization\nInput: [tmin\nd , tmax\nd\n], [tmin\nu , tmax\nu ], total bandwidth BU, computa-\ntion capability fn and fk, total number of layers L.\nOutput: Optimal split layer \u2113\u2217, device association a\u2217\nn,k, re-\nsource allocation l\u2217\nn,k, BU\u2217\nk , f \u2217\nn,k.\n1: Initialize device association an,k, resource allocation\nln,k, Bk, fn,k, objective I, P = +\u221e.\n2: for \u2113= 1 to L:\n3:\nInitialize td = tmax\nd\n, tu = tmax\nu , I\u2217= +\u221e.\n4:\nrepeat\n5:\nSolve the problem under the conditions of td and\ntu to obtain the optimal device association matrix an,k.\n6:\nObtain I, td and tu through an,k.\n7:\nSet td = td \u2212\u03b4 and tu = tu \u2212\u03b5 (where \u03b4, \u03b5 > 0).\n8:\nif I < I\u2217, set I\u2217= I, a\u2217\nn,k = an,k.end if\n9:\nuntil td \u2264tmin\nd\nand tu \u2264tmin\nu .\n10:\nObtain the optimal device association a\u2217\nn,k.\n11:\nAccording to Theorem 2 and a\u2217\nn,k, obtain the resource\nallocation scheme l\u2217\nn,k, BU\u2217\nk\n, f \u2217\nn,k , P \u2217.\n12:\nif P \u2217< P, update P \u2190P \u2217, \u2113\u2190\u2113\u2217, an,k \u2190a\u2217\nn,k,\nln,k \u2190l\u2217\nn,k, BU\nk \u2190BU\u2217\nk , fn,k \u2190f \u2217\nn,k.end if\n13: end for\nFor each split layer, Titer = max\nnl\ntn\nmax\u2212tn\nmin\n\u03b4\nm\n,\nl\ntk\nmax\u2212tk\nmin\n\u03b5\nmo\niterations are performed to optimize device association and\nresource allocation. In each iteration, all possible device\u2013UAV\nassociation pairs are evaluated to derive the optimal association\nand corresponding resource allocation, incurring a computa-\ntional cost of O(NK). Moreover, bandwidth allocation by\nthe bisection method and closed-form KKT-based resource\ncomputation incur only constant-time overhead, where the\noverall time complexity of the algorithm is O(L \u00b7 Titer \u00b7 NK).\nIV. SIMULATION AND ANALYSIS\nIn the simulation experiments, we consider a SAGIN sce-\nnario consisting of 4 UAVs and 50 mobile devices, where the\ndevices are randomly distributed within the target area. The\ncoverage range and altitude of the UAVs are 1500m and 500m.\nThe transmit power of the mobile devices is 28 dBm, the noise\npower spectral density is -174 dBm/Hz, and the total uplink\nbandwidth is 50 MHz. Regarding model training, the local\niteration count is 5, the batch size is 64, the global iteration\ncount is 2000, and the learning rate is 0.02. Each device\nhas one or more UAVs available for association. AlexNet\nis trained on the CIFAR-10 dataset. Training data is evenly\ndistributed across all devices, with the local dataset of each\ndevice containing only 4 of the 10 classes to ensure non-\nindependent and identically distributed (non-IID) data across\ndevices.\nIn\nthis\npaper,\nMATLAB\u2019s\nwalkerStar\nand\naccessIntervals\nfunctions\nare\nused\nto\nconstruct\nthe constellation model and calculate each satellite\u2019s coverage\ntime over the target area, respectively [4]. Eighty LEO\nsatellites are evenly distributed across 10 orbits, with an\norbital altitude of 800 km and an inclination of 85\u00b0. The\nminimum communication elevation angle is set to 15\u00b0, and\n5\nthe target area is located at 40\u00b0 N latitude and 86\u00b0 W\nlongitude. The satellite closest to the target area is selected\nfor training in each round.\nTo validate the effectiveness of the proposed algorithm, we\nemploy two key evaluation metrics, namely, test accuracy and\ntraining latency, to conduct a comprehensive comparative anal-\nysis against four representative baseline algorithms: (1) Ran-\ndom Association (RA) [9]: Based on the SFL framework, it\nemploys random device association and optimizes resource al-\nlocation with model splitting. (2) Equal Resource Allocation\n(ERA): Performs device association based on maximum SNR\nand allocates resources equally. (3) Hierarchical Federated\nLearning (HFL) [12]: Adopts the HFL for association and\nresource optimization, explicitly excluding model splitting.\n(4) Data Distribution-based Association (DDA): Determines\ndevice association based on data distribution metrics, with\nsimultaneous resource allocation.\n0\n2\n4\n6\n8\n10\nLatency(s)\n104\n20\n40\n60\n80\nTest Accurary(%)\nProposed Algorithm\nRA[10]\nERA\nHFL[13]\nDDA\n(a) Test Accuracy\n0\n2\n4\n6\n8\n10\nLatency(s)\n104\n0\n1\n2\n3\nTraining loss\nProposed Algorithm\nRA[10]\nERA\nHFL[13]\nDDA\n(b) Training Loss\nFig. 1: Learning performance of AlexNet on CIFAR-10\nFig. 1 contrasts learning performance across different device\nassociation and resource allocation strategies. Speci\ufb01cally, Fig.\n1(a) and 1(b) show AlexNet\u2019s test accuracy and training loss\non the CIFAR-10 dataset, respectively. Fig. 1(a) demonstrates\nthat our proposed algorithm reduces learning latency and\nenhances training accuracy over the same period compared\nto baselines, while Fig. 1(b) shows it also achieves lower\ntraining loss. Compared with the optimal baseline (DDA),\nthe proposed algorithm improves the \ufb01nal test accuracy by\napproximately 2.7% and reduces the convergence latency by\naround 23%, exhibiting superior overall performance. This\nis attributed to SFL mitigating resource constraint impacts,\noptimized device association reducing latency and alleviating\nnon-IID data effects, and resource allocation cutting latency\nacross all learning stages.\n15\n20\n25\n30\n35\n40\n45\n50\nBandwidth (MHz)\n5\n10\n15\n20\nOptimization Objective\nProposed Algorithm\nRA[10]\nERA\nHFL[13]\nDDA\nFig. 2: The impact of band-\nwidth\n1\n2\n3\n4\n5\n6\n7\n8\nUAV computing capability (GHz)\n4\n6\n8\n10\n12\n14\nOptimization Objective\nProposed Algorithm\nRA[10]\nERA\nHFL[13]\nDDA\nFig. 3: The impact of UAV\ncomputing capability\nFig. 2 compares optimization objective values across vary-\ning total uplink bandwidths. The objective value decreases\nas total uplink bandwidth increases, as additional bandwidth\nallocated to devices reduces model upload communication\nlatency, in turn lowering the objective value.\nFig. 3 compares training latencies across varying UAV\ncomputing capabilities. Clearly, the optimization objective\nvalue decreases as UAV computing capability improves, which\nis because more computing resources allocated to associated\ndevices reduce UAV-side computing latency, in turn lowering\nthe objective value. Notably, the HFL Algorithm without UAV-\nassisted computing maintains a constant objective value.\nV. CONCLUSION\nIn this paper, we addressed device heterogeneity, unbalanced\ndata distribution, and high latency of FL in SAGIN by propos-\ning a HSFL framework. We formulated a joint optimization\nproblem integrating split layer selection, device association,\nand resource allocation, and developed an alternating iterative\nalgorithm to solve this MINLP problem. Simulations on the\nCIFAR-10 dataset with AlexNet demonstrated that the method\noutperformed four baselines in test accuracy and training\nlatency. In future work, we will explore dynamic split layer\nadjustment to adapt to time-varying network conditions.\nPROOF OF THEOREM 1\nFor the convenience of proof, we annotate some variables in\nthe training process. We de\ufb01ne two additional variables vU,k\nn,t+1\nand vB,k\nn,t+1 to be the intermediate result of one iteration update\nof wU,k\nn,t and wB,k\nn,t . Therefore, we have the following results:\n\uf8f1\n\uf8f2\n\uf8f3\nvU,k\nn,t+1 = wU,k\nn,t \u2212\u03b1 P\nc pn(c)\u2207wU,k\nn,t Ex|y(c)\nn\nlog\n\u0010\nHwU,k\nn,t (x)\n\u0011o\n;\nvB,k\nn,t+1 = wB,k\nn,t \u2212\u03b1 P\nc pn(c)\u2207wB,k\nn,t Ex|y(c)\nn\nlog\n\u0010\nHwB,k\nn,t (Ak\nn,t)\n\u0011o\n.\n(14)\nand\nwU,k\nn,t+1 = vU,k\nn,t+1,\nwB,k\nn,t+1 =\n(\nvB,k\nn,t+1\nif t + 1 /\u2208mE;\nP\nn\u2208Nk\nan,k\nnk vB,k\nn,t+1\nif t + 1 \u2208mE.\n(15)\nTo further simplify the analysis, we de\ufb01ne the total param-\neter of device k as wn = [wU\nn , wB\nn ], and we have vn,t =\n[vU,k\nn,t , P\nn\u2208Nk\nan,k\nnk vB,k\nn,t ], wn,t = [wU,k\nn,t , P\nn\u2208Nk\nan,k\nnk wB,k\nn,t ].\nAdditionally, we de\ufb01ne the component-wise gradients for\nclarity: The gradient of model parameters deployed on device\nis denoted as:\n\u2207Fn(wn,t)U =\nX\nc\npn(c)\u2207wU,k\nn,t Ex|y(c)\nn\nlog\n\u0010\nHwU,k\nn,t (x)\n\u0011o\n.\n(16)\nThe gradient of model parameters deployed on UAV is denoted\nas:\n\u2207Fn(wn,t)B =\nX\nc\npn(c)\u2207wB,k\nn,t Ex|y(c)\nn\nlog\n\u0010\nHwB,k\nn,t (Ak\nn,t)\n\u0011o\n.\n(17)\nThen, the gradient of model parameter is denoted as:\ngn,t =\n\"\n\u2207Fn(wn,t, \u03ben,t)U,\nX\nn\u2208Nk\nan,k\nnk\n\u2207Fn(wn,t, \u03ben,t)B\n#\n,\ngn,t =\n\"\n\u2207Fn(wn,t)U,\nX\nn\u2208Nk\nan,k\nnk\n\u2207Fn(wn,t)B\n#\n.\n(18)\nTherefore, vn,t+1 = wn,t \u2212\u03b1gn,t, and Egn,t = gn,t.\nWe analyze the expected average difference between\nthe\ndevice\u2019s\nparameter\nand\nthe\noptimal\nparameter\nE\n\u0010P\nn\u2208Nk\nan,k\nnk \u2225vn,t+1 \u2212w\u2217\u22252\u0011\nas follows:\nE\nX\nn\u2208Nk\nan,k\nnk\n\u2225vn,t+1 \u2212w\u2217\u22252\n6\n= E\nX\nn\u2208Nk\nan,k\nnk\nwn,t \u2212\u03b1gn,t \u2212w\u2217\u2212\u03b1gn,t + \u03b1gn,t\n2\n= E\nX\nn\u2208Nk\nan,k\nnk\n\uf8eb\n\uf8ec\n\uf8ed\nwn,t \u2212w\u2217\u2212\u03b1gn,t\n2\n|\n{z\n}\nA1\n+ 2\u03b1\nwn,t \u2212w\u2217\u2212\u03b1gn,t, gn,t \u2212gn,t\n|\n{z\n}\nA2\n+ \u03b12 \r\rgn,t \u2212gn,t\n2\n|\n{z\n}\nA3\n\uf8f6\n\uf8f7\n\uf8f8.\n(19)\nNotice that we have Egn,t = gn,t in the previous analysis,\ntherefore EA2 = 0. We know that A1 and A3 satis\ufb01es the\nfollowing property [9]:\nA1 =\nX\nn\u2208Nk\nan,k\nnk\nwn,t \u2212w\u2217\u2212\u03b1gn,t\n2\n\u2264\n\u0012\n1 \u22121\n2\u00b5\u03b1\n\u0013 X\nn\u2208Nk\nan,k\nnk\n\u2225wn,t \u2212w\u2217\u22252\n\u22121\n2\u00b5\u03b1\nX\nn\u2208Nk\nan,k\nnk\n\u2225wn,t \u2212w\u2217\u22252\n+ 2\nX\nn\u2208Nk\nan,k\nnk\n\u2225wn,t \u2212wn,t\u22252\n+ \u03b12(\u2113Z2 + 1\nnk\n(L \u2212\u2113)Z2) + 4\u03b12\u03b2\u0393.\n(20)\nEA3 =E\nX\nn\u2208Nk\nan,k\nnk\ngn,t \u2212gn,t\n2\n=\nX\nn\u2208Nk\nan,k\nnk\n[\u2207Fk (wn,t, \u03ben,t)]U \u2212[\u2207Fk (wn,t)]U\n2\n+\nX\nn\u2208Nk\nan,k\nnk\nX\nn\u2208Nk\nan,k\nnk\n\u0000[\u2207Fk (wn,t, \u03ben,t)]B\n\u2212[\u2207Fk (wn,t)]B\n\u0001\r\r2\n=\nX\nn\u2208Nk\nan,k\nnk\n[\u2207Fk (wn,t, \u03ben,t)]U \u2212[\u2207Fk (wn,t, \u03ben,t)]U\n2\n+\nX\nn\u2208Nk\nan,k\nnk\n[\u2207Fk (wn,t, \u03ben,t)]B \u2212[\u2207Fk (wn,t)]B\n2\n\u2264\u2113\u03c32 + 1\nnk\n(L \u2212\u2113)\u03c32.\n(21)\nTo bound P\nn\u2208Nk\nan,k\nnk \u2225wB\nk,t \u2212wB\nk,t\u22252, we \ufb01rst assume E\nsteps between two aggregations. Therefore, for any t \u22650,\nthere exists a t0 \u2264t, such that t \u2212t0 \u2264E \u22121 and wB\nn,t0 =\nwB\nn,t0, then,\nE\nX\nn\u2208Nk\nan,k\nnk\n\u2225wn,t \u2212wn,t\u22252\n= E\nX\nn\u2208Nk\nan,k\nnk\n\u2225(wn,t \u2212wn,t0) \u2212(wn,t \u2212wn,t0)\u22252\n\u2264E\nX\nn\u2208Nk\nan,k\nnk\n\u2225wn,t \u2212wn,t0\u22252\n\u2264\nX\nn\u2208Nk\nan,k\nnk\nE\nt\nX\nt=t0\n(E \u22121)\u03b12\u2225\u2207Fn(wn,t, \u03ben,t)\u22252\n\u2264(E \u22121)2\u03b12LZ2,\n(22)\nwhere the \ufb01rst inequality is due to E\u2225X \u2212EX\u22252 \u2264E\u2225X\u22252,\nand the second inequality is due to Jensen inequality:\n\u2225wn,t \u2212wn,t0\u22252 =\nt\nX\nt=t0\n\u03b1\u2207Fk(wn,t, \u03ben,t)\n2\n\u2264(t \u2212t0)\nt\nX\nt=t0\n\u03b12\u2225\u2207Fk(wn,t, \u03ben,t)\u22252. (23)\nFurther more, we analysis \u2225wn,t \u2212w\u2217\u2225. When t \u2208mE,\nwith assumption 5 and 6 [12], we have:\n\u2225wn,mE \u2212w\u2217\u2225\u2264\u2225wn,mE\u22121 \u2212w\u2217\u2225\n\u2212\u03b1\nX\nc\npk(c)\u2207wEx|y(c)\n\b\nlog\n\u0000Hwn,mE\u22121(x)\n\u0001\n\u2212\nX\nc\np(c)\u2207wEx|y(c) {log (Hw\u2217(x))}\n\u2264\n1 \u2212\u03b1\nX\nc\np(c)L(c)\n!\n\u2225wn,mE\u22121 \u2212w\u2217\u2225\n\u2212\u03b1AmE\u22121\nX \r\rpk(c) \u2212p(c)\n.\n(24)\nwhere pk(c) is the data distribution of UAV k. Continuing the\niteration until the (m \u22121)E-th iteration, we can obtain:\n\u2225wn,mE \u2212w\u2217\u2225\n\u2264\n1 \u2212\u03b1\nX\nc\np(c)L(c)\n!E \r\rwn,(m\u22121)E \u2212w\u2217\n\u2212\u03b1\n\uf8eb\n\uf8ed\nE\u22121\nX\ni=0\nAmE\u22121\u2212i\n1 \u2212\u03b1\nX\nc\np(c)L(c)\n!i\uf8f6\n\uf8f8X\nc\n\u2225(pn(c) \u2212p(c))\u2225.\n(25)\nFinally, we have (26).\nLet Pn = 2(mE \u22121)2LZ2 +4\u03b2\u0393+\u2113Z2 + 1\nnk (L\u2212\u2113)Z2 +\n\u2113\u03c32 +\n1\nnk (L \u2212\u2113)\u03c32 + 1\n2\u00b5 P\nn\u2208Nk\n1\nnk E{\n\u0000 Pk\u22121\ni=0 AmE\u2212i\u22121\n\u00001 \u2212\n\u03b1 P\nc pn(c)L(c)\n\u0001i\u00012 P\nc (an,k\u2225pn(c)\u2212p(c)\u2225)2}, and \u2206t\n=\nE P\nn\u2208Nk\nan,k\nnk \u2225wn,t \u2212w\u2217\u22252. We assume \u03b1 =\n2\u03bb\n\u03b3+t for some\n\u03bb > 1\n\u00b5 and \u03b3 > 0. Now we prove \u2206t \u2264\nv\n\u03b3+t by induction,\nwhere v = max\nn\n4\u03bb2Pn\n\u03bb\u00b5\u22121 , (\u03b3 + 1)\u22061\no\n. Obviously, the de\ufb01ni-\ntion of v ensures it holds for t = 1. For some t \u2208mE,\n\u2206t+1 \u2264(1 \u2212\u00b5\u03b1)\u2206t + \u03b12Pn\n\u2264\n\u0012\n1 \u2212\n\u03bb\u00b5\nt + \u03b3\n\u0013\nv\nt + \u03b3 + 4\u03bb2Pn\n(t + \u03b3)2\n= t + \u03b3 \u22121\n(t + \u03b3)2 v +\n\u0014 4\u03bb2Pn\n(t + \u03b3)2 \u2212\u03bb\u00b5 \u22121\n(t + \u03b3)2 v\n\u0015\n\u2264\nv\n\u03b3 + t + 1\n(27)\nThen by the \u03b2-smoothness of F(\u00b7),\nE\nX\nn\u2208Nk\nan,k\nnk\nFn(wn,t) \u2212F \u2217\u2264\u03b2\n2 \u2206t \u2264\u03b2\n2\nv\n\u03b3 + t\nSpeci\ufb01cally, let \u03bb = 2\n\u00b5, \u03c1 = \u03b2\n\u00b5 and \u03b3 = max{8\u03c1, E} \u22121, then\nv = max\n\u001a 4\u03bb2Pn\n\u03bb\u00b5 \u22121, (\u03b3 + 1)\u22061\n\u001b\n7\nE\nX\nn\u2208Nk\nan,k\nnk\n\u2225vn,mE \u2212w\u2217\u22252 \u2264(1 \u22121\n2\u00b5\u03b1)\nX\nn\u2208Nk\nan,k\nnk\nE \u2225wn,mE \u2212w\u2217\u22252 \u22121\n2\u00b5\u03b1\nX\nn\u2208Nk\nan,k\nnk\n\u2225wn,mE \u2212w\u2217\u22252 + 2(mE \u22121)2\u03b12LZ2\n+ 4\u03b12\u03b2\u0393 + \u03b12\n\u0012\n\u2113Z2 + 1\nnk\n(L \u2212\u2113)Z2\n\u0013\n+ \u03b12\n\u0012\n\u2113\u03c32 + 1\nnk\n(L \u2212\u2113)\u03c32\n\u0013\n\u2264(1 \u22121\n2\u00b5\u03b1)\nX\nn\u2208Nk\nan,k\nnk\nE \u2225wn,mE \u2212w\u2217\u22252\n+ 1\n2\u00b5\u03b12 X\nn\u2208Nk\nan,k\nnk\nE\n( E\u22121\nX\ni=0\nAmE\u22121\u2212i\n1 \u2212\u03b1\nX\nc\np(c)L(c)\n!i ! X\nc\n\u2225(pn(c) \u2212p(c))\u2225\n)2\n+ 2(mE \u22121)2\u03b12LZ2 + 4\u03b12\u03b2\u0393 + \u03b12\n\u0012\n\u2113Z2 + 1\nnk\n(L \u2212\u2113)Z2\n\u0013\n+ \u03b12\n\u0012\n\u2113\u03c32 + 1\nnk\n(L \u2212\u2113)\u03c32\n\u0013\n(26)\n\u22644\u03bb2Pn\n\u03bb\u00b5 \u22121 + (\u03b3 + 1)\u22061\n\u226416Pn\n\u00b52\n+ (\u03b3 + 1)\u22061\n(28)\nTherefore, we have\nE\nX\nn\u2208Nk\nan,k\nnk\nFn(wn,t) \u2212F \u2217\u2264\u03b2\n2\nv\n\u03b3 + t\n\u2264\n\u03c1\n\u03b3 + t\n\u00128Pn\n\u00b5\n+ \u00b5(\u03b3 + 1)\n2\n\u22061\n\u0013\n.\n(29)\nREFERENCES\n[1] J. Liu, Y. Shi, Z. M. Fadlullah, and N. Kato, \u201cSpace-air-ground integrated\nnetwork: A survey,\u201d IEEE Commun. Surveys Tuts., vol. 20, no. 4, pp.\n2714\u20132741, 4th Quart. 2018.\n[2] Q. Chen, Z. Guo, W. Meng, S. Han, C. Li, and T. Q. S. Quek, \u201cA\nsurvey on resource management in joint communication and computing-\nembedded sagin,\u201d IEEE Commun. Surveys Tuts., vol. 27, no. 3, pp. 1911\u2013\n1954, Jun. 2025.\n[3] Y. Qin, Y. Yang, F. Tang, X. Yao, M. Zhao, and N. Kato, \u201cDifferentiated\nfederated reinforcement learning based traf\ufb01c of\ufb02oading on space-air-\nground integrated networks,\u201d IEEE Trans. Mobile Comput., vol. 23,\nno. 12, pp. 11 000\u201311 013, Dec. 2024.\n[4] D. J. Han, W. Fang, S. Hosseinalipour, M. Chiang, and C. G. Brin-\nton, \u201cOrchestrating federated learning in space-air-ground integrated\nnetworks: Adaptive data of\ufb02oading and seamless handover,\u201d IEEE J.\nSel. Areas Commun., vol. 42, no. 12, pp. 3505\u20133520, Dec. 2024.\n[5] C. Huang, G. Chen, P. Xiao, J. A. Chambers, and W. Huang, \u201cFair\nresource allocation for hierarchical federated edge learning in space-air-\nground integrated networks via deep reinforcement learning with hybrid\ncontrol,\u201d IEEE J. Sel. Areas Commun., vol. 42, no. 12, pp. 3618\u20133631,\nDec. 2024.\n[6] B. Xu, H. Zhao, C. Bao, and H. Zhu, \u201cOptimizing ef\ufb01cient federated split\nlearning over wireless networks: Challenges and opportunities,\u201d IEEE\nNetw., early access, May 2025, doi: 10.1109/MNET.2025.3574444.\n[7] L. Zhao, S. Geng, X. Tang, A. Hawbani, Y. Sun, L. Xu, and D. Tarchi,\n\u201cAlanine: A novel decentralized personalized federated learning for het-\nerogeneous LEO satellite constellation,\u201d IEEE Trans. Mobile Comput.,\nvol. 24, no. 8, pp. 6945\u20136960, Aug. 2025.\n[8] X. Liu, Y. Deng, and T. Mahmoodi, \u201cWireless distributed learning: A\nnew hybrid split and federated learning approach,\u201d IEEE Trans. Wireless\nCommun., vol. 22, no. 4, pp. 2650\u20132665, Apr. 2023.\n[9] C. Xu, J. Li, Y. Liu, Y. Ling, and M. Wen, \u201cAccelerating split federated\nlearning over wireless communication networks,\u201d IEEE Trans. Wireless\nCommun., vol. 23, no. 6, pp. 5587\u20135599, Jun. 2024.\n[10] Z. Lin, W. Wei, Z. Chen, C.-T. Lam, X. Chen, Y. Gao, and J. Luo,\n\u201cHierarchical split federated learning: Convergence analysis and system\noptimization,\u201d IEEE Trans. Mobile Comput., vol. 24, no. 10, pp. 9352\u2013\n9367, Oct. 2025.\n[11] Y. Liang, Q. Chen, R. Li, G. Zhu, M. K. Awan, and H. Jiang,\n\u201cCommunication-and-computation ef\ufb01cient split federated learning in\nwireless networks: Gradient aggregation and resource management,\u201d\nIEEE\nTrans.\nWireless\nCommun.,\nearly\naccess,\nAug.\n2025,\ndoi:\n10.1109/TWC.2025.3594006.\n[12] S. Liu, G. Yu, X. Chen, and M. Bennis, \u201cJoint user association and\nresource allocation for wireless hierarchical federated learning with IID\nand non-IID data,\u201d IEEE Trans. Wireless Commun., vol. 21, no. 10, pp.\n7852\u20137866, Oct. 2022.\n[13] W. Shi, J. Li, N. Cheng, F. Lyu, S. Zhang, H. Zhou, and X. Shen,\n\u201cMulti-drone 3-D trajectory planning and scheduling in drone-assisted\nradio access networks,\u201d IEEE Trans. Veh. Technol., vol. 68, no. 8, pp.\n8145\u20138158, Aug. 2019.\n[14] S. A. Kanellopoulos, C. I. Kourogiorgas, A. D. Panagopoulos, S. N.\nLivieratos, and G. E. Chatzarakis, \u201cChannel model for satellite com-\nmunication links above 10GHz based on weibull distribution,\u201d IEEE\nCommun. Lett., vol. 18, no. 4, pp. 568\u2013571, Apr. 2014."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary\n\n#### Main Contributions\nThis paper proposes a Hierarchical Split Federated Learning (HSFL) framework for the Space-Air-Ground Integrated Network (SAGIN), addressing challenges such as resource constraints and unbalanced data distribution in Federated Learning. The main contributions are:\n1. **Hierarchical Split Federated Learning Framework**: A hierarchical split federated learning framework is proposed to improve training efficiency and model accuracy.\n2. **Joint Optimization Problem Formulation**: A joint optimization problem integrating device association, model split layer selection, and resource allocation is formulated to minimize the weighted sum of training loss and latency.\n3. **Iterative Algorithm for Device Association and Resource Allocation**: An iterative algorithm based on brute-force search for device association and resource allocation is developed.\n\n#### Methodology\nThe proposed HSFL framework addresses challenges in SAGIN by splitting DNN models layer-wise, where resource-constrained devices handle shallow-layer computations while deep-layer tasks are offloaded to edge servers. The methodology includes:\n1. **Device Association**: A binary variable indicating device-UAV association is introduced.\n2. **Model Split Layer Selection and Resource Allocation**: An iterative optimization algorithm for split layer selection, device association, and resource allocation is proposed based on brute-force search.\n3. **Communication and Computation Models**: Detailed models are provided for traffic load, data rates, latency, and bandwidth/resource allocation.\n\n#### Key Findings\nThe key findings from the analysis include:\n1. **Impact of Split Layer Selection**: Higher split layers lead to increased convergence upper bounds and degraded model performance due to fewer aggregated DNN parameters.\n2. **Data Distribution Impact**: The error bound escalates with data distribution discrepancy, strongly influenced by device association.\n3. **Latency Reduction**: The proposed algorithm effectively balances training efficiency and model accuracy for FL in SAGIN.\n\n#### Conclusions\nThe paper concludes that the HSFL framework outperforms existing methods in terms of test accuracy and training latency on the CIFAR-10 dataset with AlexNet. Future work will explore dynamic split layer adjustment to adapt to time-varying network conditions. The proposed algorithm demonstrates superior overall performance, reducing convergence latency by approximately 23% compared to optimal baseline (Data Distribution-based Association - DDA).\n\n### Summary\nThis paper introduces a Hierarchical Split Federated Learning framework for the Space-Air-Ground Integrated Network, addressing resource constraints and data distribution issues in Federated Learning. The proposed HSFL framework integrates device association, model split layer selection, and resource allocation to minimize training loss and latency. Simulation results validate its effectiveness on the CIFAR-10 dataset with AlexNet, demonstrating superior performance compared to existing methods. Future work will explore dynamic adjustment of split layers for evolving network conditions."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "\u201cThe Whole Is Greater Than the Sum of Its Parts\u201d:\nA Compatibility-Aware Multi-Teacher CoT Distillation Framework\nJin Cui\u22171 , Jiaqi Guo\u22172 , Jiepeng Zhou\u22173 , Ruixuan Yang1 , Jiayi Lu1 , Jiajun Xu4 , Jiangcheng\nSong1 , Boran Zhao4 , Pengju Ren\u20201\n1State Key Laboratory of Human-Machine Hybrid Augmented Intelligence,\nand Institute of Artificial Intelligence and Robotics, Xi\u2019an Jiaotong University\n2Nankai University, 3The Hong Kong University of Science and Technology(Guangzhou)\n4School of Software Engineering, Xi\u2019an Jiaotong University\nandycui@stu.xjtu.edu.cn, {boranzhao, pengjuren}@xjtu.edu.cn\nAbstract\nChain-of-Thought\n(CoT)\nreasoning\nempowers\nLarge Language Models (LLMs) with remarkable\ncapabilities, but typically requires enormous pa-\nrameter scales. CoT distillation has emerged as a\npromising paradigm to transfer reasoning prowess\ninto compact student models (SLMs), yet existing\napproaches often rely on a solitary teacher, capping\nthe student\u2019s potential since individual LLMs of-\nten exhibit distinct capability biases and may suffer\nfrom catastrophic forgetting. While leveraging di-\nverse teachers seems appealing, effectively fusing\ntheir supervisions remains challenging: teacher-\nstudent incompatibility risks amplifying hallucina-\ntions, and passive supervision fails to ensure gen-\nuine logic internalization. To address this, we intro-\nduce COMPACT, a framework that adaptively fuses\nsupervisions from different teachers by dynami-\ncally weighting teacher gradients based on the stu-\ndent\u2019s real-time compatibility evaluated by a multi-\ndimensional metric: (1) Graph-based Consensus\nto filter misleading rationales by identifying main-\nstream reasoning paths; (2) Mutual-Information-\nbased Adaptability to detect \u201cepiphany moments\u201d\nfor genuinely understanding the reasoning pro-\ncess rather than merely imitating; and (3) Loss-\nbased Difficulty to assess student receptivity to\nthe teacher\u2019s guidance and prevent negative trans-\nfer.\nOur experiments and latent space analysis\ndemonstrate that COMPACT effectively integrates\ndiverse reasoning capabilities without damaging\nthe model\u2019s original knowledge structure, achiev-\ning state-of-the-art performance on various bench-\nmarks while effectively mitigating catastrophic for-\ngetting.\n1\nIntroduction\nLarge Language Models (LLMs) have demonstrated re-\nmarkable capabilities in complex reasoning tasks, partic-\n*Equal contribution.\n\u2020Corresponding author.\nularly when empowered by Chain-of-Thought (CoT) rea-\nsoning [Wei et al., 2022; Kojima et al., 2022].\nHowever,\nthis emergent ability typically requires enormous parame-\nter scales, hindering its practical deployment in resource-\nconstrained environments. Consequently, CoT distillation of-\nfers a promising solution by transferring the reasoning strate-\ngies of LLMs into compact Student Models (SLMs) using\nteacher rationales as supervision [Ho et al., 2022; Magister et\nal., 2023; Hsieh et al., 2023].\nHowever, existing distillation approaches typically con-\nstrain SLMs to fit a single teacher-generated rationale. Ignor-\ning reasoning path diversity that often leads SLMs to merely\nmimic the teacher\u2019s superficial style rather than internaliz-\ning the underlying logic [Gudibande et al., 2023], thereby\nlimiting generalization capabilities. While subsequent meth-\nods attempt to mitigate this by using multiple rationales to\nenforce answer consistency [Chen et al., 2023] or employ-\ning decoupled LoRA experts for different reasoning tasks.\nHowever, these methods are still limited by their dependence\non a single teacher model. This poses a significant limita-\ntion: as LLMs often have specializations on specific domains,\nthey inevitably exhibit capability biases and susceptibility to\ncatastrophic forgetting [Kirkpatrick et al., 2017; Liu et al.,\n2024]. Consequently, no single teacher dominates across all\ndomains. Instead, each model offers unique strengths and\nspecialized knowledge [Jiang et al., 2023; Wang et al., 2024].\nThis necessitates a multi-teacher distillation framework that\ncultivates a more robust student.\nDespite the appeal of collaborative distillation, effectively\nsynthesizing the reasoning capabilities of different teachers\nremains challenging.\nFirst, a fundamental incompatibility\nexists between teacher distinctiveness and student adaptabil-\nity. Recent studies indicate that teachers imprint recogniz-\nable \u201creasoning signatures\u201d [Chen et al., 2025; Gudibande et\nal., 2023]; mismatches in these distributions can severely dis-\nrupt the transfer of long-horizon reasoning chains [Wu et al.,\n2025; Magister et al., 2023]. This issue is exacerbated by in-\nevitable noise in teacher-generated rationales that makes mix-\ning diverse reasoning paths without proper filtering risky. It\ncould amplify hallucinations and degrade the reasoning pre-\ncision of the distilled models. Second, the definition of an\n\u201coptimal teacher\u201d changes during distillation, depending on\narXiv:2601.13992v1  [cs.CL]  20 Jan 2026\nthe dataset and the student\u2019s current learning progress. Ex-\nisting methods largely remain agnostic to this adaptability.\nThey treat SLMs as passive imitators, lacking mechanisms\nto ensure genuine logic comprehension rather than superfi-\ncial pattern mimicry, thus failing to align with the student\u2019s\nevolving receptive capacity. Third, determining the optimal\nfusion paradigm, specifically between supervision-level inte-\ngration and parameter-level merging, remains an unresolved\ndilemma. Supervision-level approaches are intrinsically con-\nstrained by the difficulty of quantifying teacher-student com-\npatibility in advance. Conversely, while model merging has\nbeen explored as a promising alternative [Ilharco et al., 2023;\nMatena and Raffel, 2022; Mitchell et al., 2022], static pa-\nrameter addition or averaging (e.g., [Shen et al., 2025]) of-\nten induces conflicts within the parameter space in distillation\ntasks. This leads to destructive interference or \u201ccancellation\neffects\u201d among diverse supervision signals, resulting in sub-\noptimal performance. These challenges necessitate a more\nadaptive, student-centric fusion mechanism.\nTo address these challenges, we propose COMPatibility-\nAware Multi-teacher CoT DistillaTion (COMPACT). Unlike\nstatic parameter merging that often incurs parameter conflicts,\nCOMPACT employs a dynamic weighting mechanism that\ntreats teacher-specific updates as independent task vectors to\navoid interferences. This approach allows the student to se-\nlectively synthesize gradients from the most compatible and\nhigh-quality teachers at the instance level, effectively pre-\nventing the \u201ccancellation effects\u201d in post-hoc merging. Cen-\ntral to this framework is a Multi-Dimensional Evaluation\nmechanism designed to quantify teacher-student compatibil-\nity through three distinct metrics: (1) Graph-based Consen-\nsus, which constructs a semantic similarity graph of teacher\noutputs to filter isolated noise and identify mainstream rea-\nsoning paths; (2) Mutual-Information-based Adaptability,\nan information-theoretic probe that detects \u201cepiphany mo-\nments\u201d in the student\u2019s learning process to distinguish gen-\nuine logic comprehension from rote memorization; and (3)\nLoss-based Difficulty, which assesses the student\u2019s receptiv-\nity to a teacher\u2019s guidance to prevent negative transfer. Fur-\nthermore, to ensure robustness across diverse reasoning paths,\nwe impose a consistency constraint that encourages the stu-\ndent to produce stable predictions regardless of different rea-\nsoning patterns.\nExtensive evaluations across In-Distribution (ID) and Out-\nof-Distribution (OOD) benchmarks demonstrate that COM-\nPACT achieves state-of-the-art performance, significantly en-\nhancing the multi-dimensional capabilities of distilled SLMs.\nFurthermore, latent space analysis reveals that our method\nenables the internalization of teacher knowledge without dis-\nrupting the student\u2019s original representation space, confirm-\ning the model\u2019s superior ability to mitigate catastrophic for-\ngetting while absorbing diverse reasoning patterns. Our main\ncontributions are summarized as follows:\n1. We identify the limitations of single teacher supervision\nand static parameter merging in CoT distillation, specif-\nically the issues of parameter conflict, and the neglect of\nstudents\u2019 evolving adaptability.\n2. We propose an effective multi-teacher distillation frame-\nwork that leverages student-centroid multi-dimensional\nevaluation to adaptively internalize teacher capabilities\nwhile effectively mitigating catastrophic forgetting.\n3. We creatively introduce Mutual Information (MI) to de-\ntect \u201cepiphany moments\u201d during distillation, enabling\nthe distinction between genuine logic internalization and\nsuperficial pattern mimicry to enhance generalization.\n4. Extensive experiments and latent space analysis confirm\nthat COMPACT effectively synthesizes the strengths of\ndiverse teachers, achieving the trade-off between supe-\nrior performance and stability in preventing forgetting.\n2\nRelated Work\n2.1\nChain-of-Thought Distillation from LLMs.\nWhile Chain-of-Thought (CoT) reasoning has empowered\nLarge Language Models (LLMs) to tackle tasks that previ-\nously seem to be impossible, these emergent capabilities are\nheavily dependent on massive parameter scales. Small Lan-\nguage Models (SLMs) often struggle to actively generate co-\nherent reasoning chains, driving efforts to distill the capabili-\nties of LLMs into compact students [Ho et al., 2022; Magister\net al., 2023; Hsieh et al., 2023]. To prevent students from\nlearning superficial shortcuts, methods like SCOTT [Wang\net al., 2023] enforce rationale-prediction consistency by con-\ntrastive decoding, while [Chen et al., 2024] proposes maxi-\nmizing mutual information between prediction and explana-\ntion tasks. Beyond single-path imitation, MCC-KD [Chen et\nal., 2023] leverages diverse rationales to regularize predic-\ntions, and UniCoTT [Zhuang et al., 2025] proposes a uni-\nfied framework to transfer knowledge from diverse structural\nCoTs. Recently, EDIT [Dai et al., 2025] utilizes a mistake-\ndriven mechanism to assist students in distinguishing rea-\nsoning steps.\nHowever, despite these sophisticated mech-\nanisms, existing paradigms predominantly rely on a single\nteacher model. This reliance confines the student\u2019s potential\nto the boundaries of a specific teacher\u2019s knowledge distribu-\ntion and reasoning biases, necessitating collaborative multi-\nteacher distillation that leverages complementary expertise.\n2.2\nModel Combining and Merging in LLMs.\nTo break the capability bounds of individual models, recent\napproaches dynamically combine multiple LLMs during in-\nference. Methods like Mixture-of-Agents [Wang et al., 2024]\nfuse intermediate activations, while LLM-Blender [Jiang et\nal., 2023] ranks and ensembles final outputs. However, run-\ntime ensembles inevitably incurs inference overhead and high\nlatency (Time to First Token). Model merging [Yang et al.,\n2024] provides a more efficient solution by combining param-\neters into a single model. Advanced techniques surpass naive\naveraging by preserving functional capabilities; for instance,\nFisher-Weighted Averaging [Matena and Raffel, 2022] uti-\nlizes diagonal Fisher information to approximate the poste-\nrior distribution, outperforming direct averaging. Similarly,\nTask Arithmetic [Ilharco et al., 2023] demonstrates that task-\nspecific vectors can be algebraically manipulated to change\nmodel behavior, enabling the modular addition or removal\nof capabilities without retraining. Other works extend these\nFrozen LLM\nFine-tunable Student Model\nLoss/Gradient Flow\nInput\nAnswer: \ud835\udc66\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\nQuestion: \ud835\udc65\nTeacher\nModel Pool\n(K Heterogeneous\nLLMs, Frozen)\nGenerate\nrationales Y1={y1,y2,...,yK}\nGenerate\nrationales\nGenerate\nrationales\n...\n...\nStudent Model\n(Parameters \ud835\udf03)\nConsistency-Enhanced Branch Optimization\n\ud835\udc3f\ud835\udc46\ud835\udc39\ud835\udc47,\ud835\udc58\n\ud835\udc3f\ud835\udc46\ud835\udc39\ud835\udc47,\ud835\udc58\n(Causal LM Loss on \ud835\udc66\ud835\udc58)\n(Causal LM Loss on \ud835\udc66\ud835\udc58)\n(Bidirectional KL-Div on\nAnswer Distribution \ud835\udc43\ud835\udc4e\ud835\udc5b\ud835\udc60)\nDynamic Weight\nGeneration\nMI-based Adaptability (SMI,k)\nGraph-based Consensus (Scons,k)\nDifficulty Penalty (SPPL,k)\nZ-Score\nNorm\nWeighted\nCombination\n(\u03b21,\u03b22,\u03b23)\nSofmax\nAdaptive Gradient Fusion\nCalculate Info\nGain (\u0394\ud835\udc3c)\nSemantic\nMask (Mt,k)\nReLU &\nSum\nProject\n(\ud835\udc4a\ud835\udc44, \ud835\udc4a\ud835\udc3e)\nCompute Pairwise\nAttention Matrix\n(A)\nDegree\nCentrality\nSum\nAverage Negative\nLog-Likelihood\nMulti-Dimensional Evaluation Metric\nSMI,k\nScons,k\nSPPL,k\nHidden States\nat Thinking\nTokens\nFeatures (\ud835\udc63\ud835\udc58)\nLast Token\nStudent NLL on\nTeacher Path \ud835\udc66\ud835\udc58\n...\nWeighted Sum\n(    \ud835\udefc\ud835\udc58\u00b7 \ud835\udc3f\ud835\udc58)\n\u03a3\nBackward Pass & Update \ud835\udf03\n(Parameter Update)\nSum\n(+\u03bb)\n(+\u03bb)\n\ud835\udc3f\ud835\udc58\n\ud835\udc3f\ud835\udc39\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc59\n\ud835\udc3f\ud835\udc40\ud835\udc36\ud835\udc5c\ud835\udc5b,\ud835\udc58\n\ud835\udc3f\ud835\udc58\n(Bidirectional KL-Div on\nAnswer Distribution \ud835\udc43\ud835\udc4e\ud835\udc5b\ud835\udc60)\n\ud835\udc3f\ud835\udc40\ud835\udc36\ud835\udc5c\ud835\udc5b,\ud835\udc58\n\ud835\udefc\ud835\udc58(\ud835\udc65)\nSum\n\ud835\udee5\ud835\udf03\ud835\udc58= \ud835\udf02\ud835\udefb\ud835\udf03\ud835\udc3f\ud835\udc39\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc59= \u0dcd\ud835\udefc\ud835\udc58\ud835\udc65\ud835\udefb\ud835\udf03\ud835\udc3f\ud835\udc58\nY2={y1,y2,...,yK}\nYk={y1,y2,...,yK}\nFigure 1: Overview of our COMPACT framework. The framework consists of three main components: (1) A frozen Teacher Model Pool\ngenerating diverse rationales; (2) A Multi-Dimensional Evaluation Metric that dynamically computes Adaptability (SMI), Consensus (Scons),\nand Difficulty (SP P L) scores; and (3) An Adaptive Gradient Fusion mechanism that updates the student model using compatibility-aware\ndynamic weights \u03b1k(x) derived from these scores.\nconcepts to patch specific model failures [Ilharco et al., 2022;\nMitchell et al., 2022] or disentangle weight magnitude from\ndirection to adapt pre-trained models [Yu et al., 2024]. How-\never, these merging frameworks usually assume that tasks are\northogonal in parameter space[Ilharco et al., 2023].\nThis\nassumption fails in multi-teacher distillation, where differ-\nent teachers could provide conflicting reasoning paths for the\nsame problem that leads to interference. In contrast, our ap-\nproach adapts parameter fusion to the distillation setting, aim-\ning to unify diverse reasoning patterns while dynamically re-\nsolving interferences between different teacher distributions.\n3\nMethod\n3.1\nPreliminaries and Problem Formulation\nLet M\u03b8 denote the student model parameterized by \u03b8. Given\nan input question x and a gold answer ygold, we assume ac-\ncess to a set of K heterogeneous teacher models. For each in-\nput, these teachers generate a set of diverse rationales (CoTs),\ndenoted as Y = {y1, y2, . . . , yK}. Our goal is to update \u03b8 by\nsynthesizing the supervision signals from Y. We formulate\nthe parameter update rule based on the concept of Task Arith-\nmetic [Ilharco et al., 2023]. We treat the gradient from each\nteacher k as a specific \u201ctask vector\u201d \u2206\u03b8k. The global update\nis defined as a dynamic weighted summation:\n\u03b8new = \u03b8old +\nK\nX\nk=1\n\u03b1k(x) \u00b7 \u2206\u03b8k\n(1)\nwhere \u03b1k(x) is a dynamic weighting coefficient condi-\ntioned on the student\u2019s real-time adaptability to teacher k.\n3.2\nMulti-Dimensional Dynamic Evaluation\nTo determine the optimal weighting coefficients \u03b1k(x), we\nintroduce a multi-dimensional evaluation mechanism, which\nassesses each teacher\u2019s rationale quality from the student\u2019s\nperspective across three dimensions: adaptability, consensus,\nand difficulty.\nMI-based Adaptability Score (SMI). A core challenge in\nCoT distillation is determining whether a student is genuinely\ninternalizing the reasoning logic or merely mimicking sur-\nface textual patterns. Recent findings in information-theoretic\nanalysis of LLMs [Qian et al., 2025] reveal that the genuine\nreasoning manifests as \u201cInformation Peaks\u201d (sudden reduc-\ntions in the uncertainty of the final answer), occurring specif-\nically at critical \u201cThinking Tokens\u201d (e.g., Therefore, Because,\nWait). These tokens act as indicators where the model con-\nsolidates prior context to deduce the next logical step.\nBased on this, we design SMI to measure the information\ngain triggered by the teacher\u2019s rationale at these specific piv-\notal moments. We monitor the \u201cepiphany moments\u201d where\nthe student\u2019s hidden states significantly reduce uncertainty\nabout the final answer ygold. For each token t in the reason-\ning chain yk, we compute the log-likelihood of generating the\ncorrect answer using the student\u2019s current hidden state ht,k:\nIproxy(t, k) =\n1\n|ygold|\n|ygold|\nX\nj=1\nlog P\u03b8(y(j)\ngold|ht,k)\n(2)\nWe define the information gain as \u2206It,k = Iproxy(t, k) \u2212\nIproxy(t\u22121, k). To focus on reasoning steps rather than trivial\nnon-reasoning tokens, we apply a semantic mask Mt,k which\nassigns higher weights to predefined \u201cThinking Tokens\u201d (e.g.,\n\u201cTherefore\u201d, \u201cSo\u201d) and a small weight \u03f5 otherwise. We cal-\nculate the final adaptability score by aggregating the positive\ninformation gains:\nSMI,k =\nTk\nX\nt=1\nReLU(\u2206It,k) \u00b7 Mt,k\n(3)\nWhere Tk is the length of the reasoning chain. A high\nSMI,k indicates that the reasoning path provided by teacher\nk successfully triggers \u201cepiphany moments\u201d in the student\u2019s\nlatent space, signaling effective logical transfer.\nGraph-based Consensus Score (Scons). Since individual\nteachers may hallucinate convincing but incorrect reasoning\npaths, we introduce a consensus score to identify the main-\nstream logic to filter noises. Unlike external evaluation mod-\nels, we construct a semantic consensus graph directly within\nthe student\u2019s own representation space. This ensures that the\nselected teacher is not only correct but also aligned with the\nstudent\u2019s current semantic understanding.\nWe treat the K reasoning paths as nodes in a fully con-\nnected graph. For each path yk, we extract the feature vec-\ntor vk from the student\u2019s hidden state at the final valid token\n(EOS). To measure semantic affinity, we project these vectors\nusing the student model\u2019s own attention projection matrices\n(WQ and WK from the final transformer layer), which pre-\nserves the model\u2019s native similarity metric. We compute the\npairwise attention matrix A \u2208RK\u00d7K:\nAij = Softmax\n(Qgraph)i \u00b7 (Kgraph)T\nj\n\u221a\nd\n!\n(4)\nWhere Qgraph = WQv,\nKgraph = WKv. The consen-\nsus score is derived from the degree Centrality of the graph,\ndefined as the sum of incoming attention weights:\nScons,k =\nX\nj\u0338=k\nAjk\n(5)\nPaths with high centrality represent the mode of the dis-\ntribution in the student\u2019s semantic space, effectively filtering\nout outlier hallucinations and provides reliable supervision.\nDifficulty Penalty (SP P L). While SMI (although related to\nstudent\u2019s capability) and Scons evaluate the intrinsic quality\nof the rationales, they ignore the student\u2019s receptivity. A ratio-\nnale may be logically sound but expressed with a complexity\nor distribution shift that far exceeds the student\u2019s current ca-\npacity, making it difficult to absorb effectively. To mitigate\nthis, we introduce a difficulty probe to measure the learn-\nability of each reasoning path. We quantify this using the\nNegative Log-Likelihood (NLL) of the teacher\u2019s rationale yk\nevaluated by the student model itself. This serves as a direct\nproxy for the distributional gap between the teacher\u2019s super-\nvision and the student\u2019s current knowledge state:\nSP P L,k = \u22121\nTk\nTk\nX\nt=1\nlog P\u03b8(yk,t|x, yk,<t)\n(6)\nA lower SP P L,k indicates the reasoning path lies within\nthe student\u2019s \u201cZone of Proximal Development\u201d [Chen et al.,\n2025], accessible enough to ensure stable gradient updates\nwhile preventing the student from being overwhelmed by ex-\ncessively perplexing supervision. By integrating this penalty,\nthe framework dynamically lowers the weights of rationales\nthat are incompatible with the student, prioritizing effective\nknowledge transfer over exposure to complex reasoning.\nDynamic Weight Generation. The final fusion weight \u03b1k is\nobtained by normalizing the fusion of these three scores:\nScorek = \u03b21\u00b7N(SMI,k) + \u03b22 \u00b7 N(Scons,k) \u2212\u03b23 \u00b7 N(SP P L,k)\n\u03b1k =\nexp(Scorek/\u03c4)\nPK\nj=1 exp(Scorej/\u03c4)\n(7)\nwhere N(\u00b7) denotes Z-Score normalization among teachers.\n3.3\nConsistency-Enhanced Branch Optimization\nTo prevent the student from overfitting to specific stylistic\npatterns of any single teacher, we design a dual-objective loss\nfunction for each teacher branch k.\nSupervised Fine-Tuning (SFT). The primary objective is the\nstandard causal language modeling loss on the teacher\u2019s path:\nLSF T,k = \u2212\nX\nt\nlog P(yk,t|x, yk,<t)\n(8)\nMulti-CoT Consistency (MCon). We enforce a consistency\nconstraint on the answer span. Even if using different reason-\ning paths, the probability distribution over the final answer Pa\nshould remain consistent across high-quality teacher paths.\nWe minimize the Bidirectional KL-Divergence between the\ncurrent branch k and other branches j:\nLMCon,k =\nX\nj\u0338=k\n\u03b1j \u00b7 DKL (Pa(\u00b7|x, yk)||(Pa(\u00b7|x, yj))) (9)\nThis term encourages the student to achieve outcome-\nconsistency regardless of the reasoning topology. The total\nloss for branch k is:\nLk = LSF T,k + \u03bbLMCon,k\n(10)\n3.4\nAdaptive Parameter Fusion\nFinally, we execute the parameter update. While theoreti-\ncally formulated as the fusion of task vectors in the parame-\nter space (where \u2206\u03b8k is equivalent to \u03b7\u2207Lk), we implement\nthis efficiently by automatic differentiation on the dynami-\ncally weighted objective:\nLF inal =\nK\nX\nk=1\n\u03b1k(x) \u00b7 Lk\n(11)\nThis generate equivalent gradient aggregation \u2207\u03b8LF inal =\nPK\nk=1 \u03b1k(x)\u2207\u03b8Lk without the overhead of per-branch gradi-\nent storage. By optimizing LF inal, the student effectively up-\ndates its parameters in the direction of interpolated task vec-\ntors. This allows the student to prioritize the most instructive\nand robust reasoning paths for each specific instance.\n4\nExperiments\n4.1\nExperimental Setup\nDatasets. We evaluate COMPACT on several benchmarks\nto verify performance and generalization: (1) In-Distribution\nmathematical reasonging benchmarks: MATH500 [Lightman\net al., 2023], GSM8K [Cobbe et al., 2021], SVAMP [Patel\net al., 2021]. (2) Out-Of-Distribution commonsense bench-\nmarks: CSQA [Talmor et al., 2019], StrategyQA [Geva et al.,\n2021], GPQA-Diamond [Rein et al., 2023].\nIn-Distribution\nOut-Of-Distribution\nMethod\nMATH500\nGSM8K\nSVAMP\nAvg. gain\nCSQA\nStrategyQA\nGPQA-D\nAvg. gain\nQwen2.5-7B\nBase (Qwen2.5-7B-Instruct)\n77.20\n92.36\n90.33\n\u21914.14\n83.45\n68.68\n30.30\n\u21916.69\nSBS [Hsieh et al., 2023]\n77.40\u21910.20\n94.77\u21912.41\n93.00\u21912.67\n\u21912.38\n83.20\u21930.25\n67.25\u21931.43\n27.46\u21932.84\n\u21918.20\nMCC [Chen et al., 2023]\n82.20\u21915.00\n90.52\u21931.84\n91.00\u21910.67\n\u21912.86\n81.72\u21931.73\n67.03\u21931.65\n26.77\u21933.53\n\u21919.00\nCommittee [Li et al., 2025]\n77.25\u21910.05\n88.34\u21934.02\n91.51\u21911.18\n\u21915.07\n80.52\u21932.93\n64.19\u21934.49\n36.6\u21916.3\n\u21917.20\nEDIT [Dai et al., 2025]\n79.50\u21912.30\n94.28\u21912.49\n93.50\u21913.17\n\u21911.68\n83.80\u21910.35\n67.50\u21931.18\n29.10\u21931.20\n\u21917.37\nMoT[Shen et al., 2025]\n81.60\u21914.40\n94.37\u21912.01\n93.00\u21912.67\n\u21911.14\n83.20\u21930.25\n66.81\u21931.87\n37.88\u21917.58\n\u21915.01\nOurs w/ fusion\n82.70\u21915.50\n95.30\u21912.94\n94.31\u21913.98\n\u2013\n83.59\u21910.14\n72.93\u21914.25\n46.39\u219116.09\n\u2013\nLlama3.1-8B\nBase (Llama3.1-8B-Instruct)\n46.00\n83.89\n87.00\n\u21914.28\n74.77\n70.74\n21.71\n\u21912.68\nSBS [Hsieh et al., 2023]\n52.60\u21916.60\n86.96\u21913.07\n87.67\u21910.67\n\u21910.84\n75.02\u21910.25\n71.05\u21910.31\n19.70\u21932.01\n\u21913.16\nMCC [Chen et al., 2023]\n53.00\u21917.00\n85.01\u21911.12\n83.33\u21933.67\n\u21912.80\n73.33\u21931.44\n67.99\u21932.75\n18.53\u21933.18\n\u21915.13\nCommittee [Li et al., 2025]\n51.93\u21915.93\n81.83\u21932.06\n87.84\u21910.84\n\u21912.71\n74.95\u21910.18\n70.87\u21910.13\n18.47\u21933.24\n\u21913.65\nEDIT [Dai et al., 2025]\n52.80\u21916.80\n86.91\u21913.02\n87.67\u21910.67\n\u21910.79\n74.82\u21910.05\n71.15\u21910.41\n20.20\u21931.51\n\u21913.03\nMoT[Shen et al., 2025]\n50.80\u21914.80\n86.73\u21912.83\n85.00\u21932.00\n\u21912.40\n74.94\u21910.17\n67.25\u21933.49\n23.74\u21912.03\n\u21913.11\nOurs w/ fusion\n53.56\u21917.56\n88.25\u21914.36\n87.93\u21910.93\n\u2013\n76.26\u21911.49\n71.72\u21910.98\n27.27\u21915.56\n\u2013\nQwen2.5-1.5B\nBase (Qwen2.5-1.5B-Instruct)\n50.40\n72.81\n79.33\n\u21913.25\n74.20\n58.07\n24.75\n\u21914.14\nSBS [Hsieh et al., 2023]\n49.80\u21930.60\n73.91\u21911.10\n78.52\u21930.81\n\u21913.35\n73.46\u21930.74\n56.55\u21931.52\n22.99\u21931.76\n\u21915.48\nMCC [Chen et al., 2023]\n53.60\u21913.20\n70.54\u21932.27\n79.80\u21910.47\n\u21912.78\n71.32\u21932.88\n56.99\u21931.08\n20.71\u21934.04\n\u21916.80\nCommittee [Li et al., 2025]\n58.10\u21917.70\n73.69\u21910.88\n77.98\u21931.35\n\u21910.84\n71.21\u21932.99\n59.43\u21911.36\n27.07\u21912.32\n\u21913.91\nEDIT [Dai et al., 2025]\n51.20\u21910.80\n74.50\u21911.69\n79.60\u21910.27\n\u21912.33\n74.47\u21910.27\n57.22\u21930.85\n23.74\u21931.01\n\u21914.67\nMoT[Shen et al., 2025]\n54.00\u21913.60\n74.45\u21911.64\n75.25\u21934.08\n\u21912.86\n71.79\u21932.41\n58.08\u21910.01\n28.71\u21913.96\n\u21913.62\nOurs w/ fusion\n56.20\u21915.80\n75.08\u21912.27\n81.00\u21911.67\n\u2013\n75.37\u21911.17\n58.73\u21910.66\n35.33\u219110.58\n\u2013\nTable 1: Performance comparison of COMPACT and other methods. \u2191and \u2193indicate the relative improvement/decrease to the base model.\nAvg. gain calculates the average relative gain of our method toward baselines. All baselines are trained on their original settings.\nModels and Implementation Details.\nWe employ open-\nsource DeepSeek-R1-Llama-70B, Qwen2.5-70B, Llama-3.1-\n70B and QWQ-32B as our teacher models, selected for their\nremarkable reasoning performance. For students, we select\nwidely-used Qwen2.5-1.5B, Qwen2.5-7B, and Llama3.1-8B\nto evaluate scalability and architectural universality. We ap-\nply LoRA to all linear layers within the student model\u2019s atten-\ntion and feed-forward modules during distillation. We train\nthe models for 8 epochs(1.5B) and 6 epochs(7B/8B) with a\nbatch size of 4 per device and a learning rate of 1 \u00d7 10\u22124 for\n7B/8B models and 5 \u00d7 10\u22125 for the 1.5B model. All experi-\nments are conducted on 2 \u00d7 NVIDIA A100 (80GB) GPUs.\nBaselines.\nWe evaluate COMPACT against diverse base-\nlines: (1) Zero-shot base models, specifically Qwen2.5-1.5B-\nInstruct, Qwen2.5-7B-Instruct, and Llama3.1-8B-Instruct;\n(2) SbS-KD [Hsieh et al., 2023], representing vanilla CoT\ndistillation; (3) MCC-KD [Chen et al., 2023], a multi-path\ndistillation approach enforcing consistency; (4) EDIT [Dai et\nal., 2025], which emphasizes mistake-driven key reasoning\nsteps; (5) MoT [Shen et al., 2025], baseline method that aver-\nages parameters over all students fine-tuned on various teach-\ners; (6) Committee [Li et al., 2024], a mistakes-aware method\nthat simulates a peer-review among multiple teachers.\n4.2\nMain Results\nTable 1 presents the performance of our method against base-\nlines across three student architectures. COMPACT consis-\ntently outperforms all baselines on In-Distribution (ID) rea-\nsoning tasks. Compared to multi-teacher baselines like MoT\nand Committee, COMPACT demonstrates superior perfor-\nmance and robustness. The performance gains are consis-\ntent across diverse model families and scales. Notably, the\n1.5B student distilled via COMPACT even surpasses the 7B\nand 8B Base model on GPQA-D, highlighting the method\u2019s\nability to compress reasoning capabilities into lightweight\nmodels efficiently. Crucially, COMPACT excels in preserv-\ning general capabilities. While baselines often suffer from\ncatastrophic forgetting, our method achieves improvement on\nevaluated OOD tasks. This confirms that our compatibility-\naware weighting effectively isolates reasoning enhancement\nfrom general knowledge retention, preventing the tax on ver-\nsatility often paid by standard distillation. Figure 3 shows the\ndynamic weighting scores for different teachers during train-\ning on MATH dataset. Detailed analysis see Appendix C.\n4.3\nLatent Representation Stability Analysis\nTo investigate the internal adaptation mechanism of the\nstudent model during distillation, we conducted a PCA\nshift analysis on the latent representations of the layers in\nQwen2.5-1.5B distilled by different methods. Following the\nframework proposed by [Xu et al., 2025] and [Wu et al.,\n2025], we project the high-dimensional activations of the dis-\ntilled students onto the principal component basis constructed\nfrom the original, undistilled student.\nMethodology and Rationale. The PCA shift (\u2206) measures\nthe euclidean distance between the centroids of latent repre-\nsentations before and after distillation. A large shift, partic-\nularly in the first and secondary principal components, indi-\ncates a drastic reorganization of the student\u2019s feature space,\noften signaling structural degradation or low corpus affin-\nity. An ideal distillation should achieve knowledge absorp-\ntion without structural destruction. This requires the student\n0.0\n0.6\n1.2\n1.8\n2.4\n3.0\n0.0\n0.6\n1.2\n1.8\n2.4\n3.0\n0.0\n0.6\n1.2\n1.8\n2.4\n3.0\nBase\nSingle Avg.\nSBS\nMCC\nEDIT\nCommittee\nMoT\nCOMPACT\n(b) Qwen2.5-7B Performance\n(c) Qwen2.5-1.5B Performance\n(d) Llama3.1-8B Performance\nfactuality\ncommonsense\ncomprehension\ninsightfulness\ncompleteness\nmetacognition\nreadability\nconciseness\nharmlessness\nrobustness\ncorrectness\nefficiency\n0.0\n0.6\n1.2\n1.8\n2.4\n3.0\n(a) Comprehensive Performance\nDeepSeek-R1\nQWQ\nQwen2.5\nLlama3.1\nOurs\nFigure 2: Fine-grained capability evaluation on FLASK framework. (a) compares teachers (DeepSeek-R1-Distill-Llama-70B, QWQ-32B,\nQwen2.5-72B, Llama3.1-70B)(solid lines) and their corresponding distilled students (dashed lines), alongside COMPACT. (b)(c)(d) show the\nperformance of different distillation methods in Table 1 across three student architectures. All results are scaled for better illustration.\nStep\nScores\n1000\n1250\n1500\n1750\n750\n500\n250\n0\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\nFigure 3: Evolution of dynamic weighting scores for different teach-\ners during training.\nTeacher 0-3 corresponds to Qwen2.5-72B,\nLlama3.1-70B, DeepSeek-R1-Llama-70B, and QWQ-32B.\nPC1 Shift(\u0394)\n20\n0\n-20\n10\n30\n-30\n-10\n-40\n-20\n0\n20\n40\n60\n80\nPC2 Shift(\u0394)\n40\n20\n0\n-20\n-40\n-20\n0\n20\n40\n60\n80\n100\nPC1 Shift(\u0394)\nPC2 Shift(\u0394)\nOurs (COMPACT)\nBase Student\nSingle Teacher Distillation\nNaive Parameter Averaging\n(a) In-Distribution Task\n(b) Out-of-Distribution Task\nFigure 4: Layer-wise PCA shift of latent representation in Qwen2.5-\n1.5B distilled by various methods.\nCOMPACT exhibits minimal\nshift, confirming its stability in preserving the general capabilities.\nexhibit controlled adaptation on in-distribution tasks (learn-\ning reasoning patterns) while maintaining near-zero shift on\nout-of-distribution tasks (preserving general capabilities).\nAnalysis on In-Distribution (ID) Reasoning Tasks. Figure\n4(a) visualizes the representational shift on the ID reasoning\ndataset (MATH500). Naive Parameter Averaging exhibits the\nlargest and most chaotic dispersion (high shift). This con-\nfirms our hypothesis that static parameter averaging induces\ngradient conflicts among heterogeneous teachers, forcing the\nstudent into a \u201crepresentational collapse\u201d where the internal\nstructure is severely twisted to accommodate conflicting rea-\nsoning paths. Single Teacher Distillation shows a moderate\nand directional shift, reflecting the \u201cStyle Signature\u201d effect,\nwhere the student is forced to reorganize its cognitive struc-\nture to mimic a specific teacher\u2019s distribution, potentially at\nthe cost of its original structural stability. In contrast, COM-\nPACT maintains a representation centroid tightly aligned with\nthe original student with minimal shift. This suggests that\nCOMPACT successfully identifies the most compatible paths,\nembedding new reasoning capabilities smoothly into the stu-\ndent\u2019s existing manifold without destructive reorganization.\nAnalysis on Out-of-Distribution (OOD) Tasks. Figure 4(b)\nillustrates the shift on OOD datasets (CSQA), serving as a\nproxy for catastrophic forgetting. Ideally, the latent represen-\ntation of OOD tasks should remain invariant to preserve pre-\nexisting knowledge. However, baselines exhibit substantial\ndistributional shifts, indicating their indiscriminate supervi-\nsion damages the student\u2019s original knowledge space. Con-\nversely, COMPACT achieves near-zero shift. This confirms\nthat our compatibility-aware weighting successfully disentan-\ngles reasoning internalization from general knowledge reten-\ntion, thereby preserving the student\u2019s general capabilities.\n4.4\nFine-grained Capability Analysis\nTo conduct a holistic assessment beyond simple accuracy,\nwe employ the FLASK evaluation framework [Ye et al.,\n2023], which decomposes model capabilities into four dimen-\nsions comprising 12 fine-grained skills across four primary\nabilities: Logical Thinking (Correctness, Robustness, Effi-\nciency), Background Knowledge (Factuality, Commonsense,\nMetacognition), Problem Handling (Comprehension, Com-\npleteness, Insightfulness), and User Alignment (Readability,\nConciseness, Harmlessness). Results are shown in Figure 2.\nBreaking Capability Boundaries through Adaptive Fu-\nsion.\nFigure 2(a) confirms that different teachers excel\nin specific domains and the \u201cstyle signature\u201d phenomenon,\nwhere students distilled from individual teachers inherit dis-\ntinct capability biases. While the naive parameter averaging\n(MoT) attempts to combine these abilities and shows gains in\nLogical Correctness and Completeness, proving that multi-\nteacher distillation indeed introduces diverse knowledge, it\nsuffers a critical failure in Logical Robustness, accompanied\nby increased hallucinations (lower Factuality and Common-\nsense Understanding).\nThis confirms that conflicting gra-\ndient directions neutralize the specialized expertise of indi-\nvidual teachers while disrupting the student\u2019s original knowl-\nedge structure, ultimately leading to mediocrity. In contrast,\nleveraging a dynamic weighting mechanism, the COMPACT-\ndistilled student effectively breaks these capability bound-\naries, synthesizing the complementary strengths of diverse\nteachers to achieve superior performance across all dimen-\nsions, even surpassing individual teachers on some metrics.\nRobustness and Genuine Comprehension. Specific capa-\nbility gains directly validate our multi-dimensional evaluation\nmetrics. (1) Robustness via Consensus: COMPACT holds a\nsignificant lead in Factuality and Commonsense Understand-\ning. We attribute this to the Graph-based Consensus (Scons)\nmetric, which effectively filters out hallucinations from in-\ndividual teachers, ensuring the student internalizes only the\nmainstream correct logic. (2) Insightfulness via Information\nGain: The superior performance in Insightfulness and Com-\npleteness validates the MI-based Adaptability (SMI) metric.\nUnlike baselines that rely on rote memorization of surface\npatterns, the high mutual information guidance fosters gen-\nuine logic comprehension, empowering the student to gener-\nate more complete and novel reasoning paths.\n4.5\nAblation Study\nTo rigorously verify the contribution of each component\nwithin our multi-dimensional evaluation metric, we con-\nducted ablation studies on the Qwen2.5-1.5B student model.\nThe results are summarized in Figure 5. We calculated the\naverage relative degradation rate (%) and the shift in stability\n(Standard Error of the Mean, SEM) across all tasks.\nBase\nFull\nSingle avg.\nw/o. Consensus\nw/o. MI\nw/o. Loss\nDirect avg.\nAccuracy (%)\n80\n78\n76\n74\n72\n70\n68\n66\n64\nCSQA\n40\n35\n30\n25\n20\nCPQA-D\n80\n78\n76\n74\n72\n70\n68\n66\nGSM8K\n80\n78\n76\n74\n72\n82\n84\n86\nSVAMP\n62\n60\n58\n56\n54\n52\n50\nStrategyQA\n60.0\n57.5\n55.0\n52.5\n50.0\n47.5\n45.0\n42.5\nMATH500\nFigure 5: Comprehensive ablation study on model performance.\nStep\nScores\n(a) Without Mutual Information Metric\n(b) Without Difficult Penalty (Catastrophic Forgetting)\nPPL loss\n1.8\n1.6\n1.4\n1.2\n1.0\n0.8\n0.6\n0.4\n0\n500\n1000\n1500\n2000\n2500\nRaw\nSmoothed (window=10)\nStep\nTotal Loss\n0\n500\n1000\n1500\n2000\n2500\nRaw\nSmoothed (window=10)\nStep\n(c) Without Graph-based Consensus (Training Instability\uff09\n1.2\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n0\n500\n1000\n2000\n1500\n2500\nStep\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n0\n500\n1000\n1500\n2000\n2500\nLearning from difficult-to-understand teacher\nDisrupting the original distribution\nFigure 6: Training dynamics under component ablation.\nSuperiority of Dynamic Weighting over Static Merging.\nThe \u201cDirect Avg.\u201d\nbaseline, while improving upon the\nbase model, consistently underperforms the full COMPACT\nframework. Notably, \u201cDirect Avg."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary\n\n#### Main Contributions:\n1. **COMPATIBLE Framework**: The paper introduces a framework named COMPACT that addresses the limitations of single teacher supervision and static parameter merging in Chain-of-Thought (CoT) distillation.\n2. **Multi-Teacher Distillation**: It proposes an effective multi-teacher distillation framework leveraging student-centric multi-dimensional evaluation to adaptively internalize diverse teacher capabilities while mitigating catastrophic forgetting.\n3. **Innovative Metrics**: The paper introduces three distinct metrics for evaluating and synthesizing teacher supervision: \n   - **Graph-based Consensus (Scons)**: Filters misleading rationales by identifying mainstream reasoning paths.\n   - **Mutual-Information-based Adaptability (SMI)**: Detects \"epiphany moments\" to ensure genuine understanding of the reasoning process rather than mere imitation.\n   - **Loss-based Difficulty (SPPL)**: Evaluates student receptivity and prevents negative transfer.\n\n#### Methodology:\n1. **Teacher Rationale Generation**: The framework employs a dynamic weighting mechanism that treats teacher-specific updates as independent task vectors, allowing selective synthesis of gradients from compatible teachers at the instance level.\n2. **Multi-Dimensional Evaluation Metric**: A multi-dimensional evaluation metric is introduced to quantify teacher-student compatibility through Scons (graph-based consensus), SMI (mutual information-based adaptability), and SPPL (loss-based difficulty).\n3. **Dynamic Weight Generation**: The final fusion weight \u03b1k is obtained by normalizing the scores derived from these metrics.\n\n#### Key Findings:\n1. **Performance Enhancement**: COMPACT consistently outperforms baselines across various benchmarks, demonstrating superior performance on In-Distribution tasks compared to multi-teacher baselines like MoT and Committee.\n2. **Latent Representation Stability**: The latent representation of Qwen2.5-1.5B distilled by different methods shows minimal shift during training, confirming the framework's ability to preserve general capabilities without structural destruction.\n3. **Fine-grained Capability Analysis**: COMPACT effectively synthesizes diverse teacher strengths across multiple dimensions, achieving superior performance in robustness and genuine comprehension.\n\n#### Conclusions:\nCOMPACT addresses the limitations of single teacher supervision and static parameter merging by introducing a multi-teacher distillation framework that leverages student-centric evaluation metrics to adaptively internalize diverse teacher capabilities while mitigating catastrophic forgetting. The framework's effectiveness is validated through extensive experiments, demonstrating its potential for enhancing model performance across various benchmarks.\n\n### Detailed Breakdown:\n\n#### Introduction:\nThe paper highlights the remarkable capabilities of Large Language Models (LLMs) in complex reasoning tasks but notes that these abilities typically require enormous parameter scales, hindering their practical deployment. CoT distillation offers a promising solution by transferring LLMs' reasoning strategies into compact student models (SLMs). However, existing approaches often rely on a solitary teacher and may suffer from catastrophic forgetting due to individual LLMs exhibiting distinct capability biases.\n\n#### Methodology:\n1. **Teacher Rationale Generation**: The framework uses multiple heterogeneous teachers to generate diverse rationales for each input.\n2. **Multi-Dimensional Evaluation Metric**:\n   - **Graph-based Consensus (Scons)**: Filters misleading rationales by identifying mainstream reasoning paths.\n   - **Mutual-Information-based Adaptability (SMI)**: Detects \"epiphany moments\" for genuine understanding rather than mere imitation.\n   - **Loss-based Difficulty (SPPL)**: Evaluates student receptivity and prevents negative transfer.\n\n#### Results:\n1. **Performance Evaluation**: COMPACT consistently outperforms baselines on various benchmarks, including In-Distribution and Out-of-Distribution tasks.\n2. **Latent Representation Stability Analysis**: The latent representations of the distilled models show minimal shift during training, confirming the framework's ability to preserve general capabilities without structural destruction.\n3. **Fine-grained Capability Analysis**: COMPACT effectively synthesizes diverse teacher strengths across multiple dimensions, achieving superior performance in robustness and genuine comprehension.\n\n#### Conclusion:\nCOMPACT addresses the limitations of single teacher supervision and static parameter merging by introducing a multi-teacher distillation framework that leverages student-centric evaluation metrics to adaptively internalize diverse teacher capabilities while mitigating catastrophic forgetting. The framework's effectiveness is validated through extensive experiments, demonstrating its potential for enhancing model performance across various benchmarks."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Generalization and Completeness of Stochastic\nLocal Search Algorithms\u2217\nDaniel Loscos, Narciso Mart\u00ed-Oliet, and Ismael Rodr\u00edguez \u2020\nAbstract\nWe generalize Stochastic Local Search (SLS) heuristics into a unique\nformal model. This model has two key components: a common structure\ndesigned to be as large as possible and a parametric structure intended\nto be as small as possible.\nEach heuristic is obtained by instantiating\nthe parametric part in a different way. Particular instances for Genetic\nAlgorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm\nOptimization (PSO) are presented. Then, we use our model to prove the\nTuring-completeness of SLS algorithms in general.\nThe proof uses our\nframework to construct a GA able to simulate any Turing machine. This\nTuring-completeness implies that determining any non-trivial property\nconcerning the relationship between the inputs and the computed out-\nputs is undecidable for GA and, by extension, for the general set of SLS\nmethods (although not necessarily for each particular method). Similar\nproofs are more informally presented for PSO and ACO.\nKeywords: Stochastic local search, evolutionary computation, swarm intelli-\ngence, formal languages, operational semantics, generalization, computability,\nTuring-completeness.\n1\nIntroduction\nTackling optimization problems is a necessity in virtually all industrial and sci-\nentific areas, yet finding the optimal solution is computationally intractable for\nmany of these problems. Thus, we rather focus on looking for reasonable subop-\ntimal solutions for typical problem instances, normally by means of heuristics.\nStochastic Local Search (SLS) algorithms [1], and particularly notable families\n\u2217This paper was published in Swarm and Evolutionary Computation. The present version is\nthe author\u2019s accepted manuscript. Work partially supported by projects TIN2015-67522-C3-3-\nR, PID2019-108528RB-C22, and by Comunidad de Madrid as part of the program S2018/TCS-\n4339 (BLOQUES-CM) co-funded by EIE Funds of the European Union.\n\u2020Dpto.\nSistemas Inform\u00e1ticos y Computaci\u00f3n.\nFacultad de Inform\u00e1tica.\nUniversidad\nComplutense de Madrid. 28040 Madrid, Spain. Narciso Mart\u00ed-Oliet and Ismael Rodr\u00edguez\nare also with Instituto de Tecnolog\u00edas del Conocimiento.\nE-mail: {dloscos,narciso,isrodrig}@ucm.es\n1\narXiv:2601.14212v1  [cs.NE]  20 Jan 2026\nof them such as Evolutionary and Swarm optimization methods [2, 3], can be\ngood choices to tackle many optimization problems. In these methods, a set\nof simple entities interact with each other according to simple rules, chang-\ning across iterations and collaboratively constructing and improving solutions\nto the problem under consideration. For instance, in a Genetic Algorithm [4],\nchromosomes (entities) mix with each other and constitute evolving solutions\nby themselves, whereas in Ant Colony Optimization [5, 6] some ants (agents)\ncollaboratively draw a candidate solution (a path or set of paths) on a graph\nwhile they iteratively traverse it. SLS heuristics in general, and evolutionary\nand swarm algorithms in particular, have been applied to a wide variety of\ndomains (see e.g. [7, 8, 9, 10, 11]).\nDuring the last years, this field has witnessed an explosion of new meth-\nods which, in some cases, have been motivated mainly by the beauty of the\nnatural procedure they try to replicate, rather than by displaying an actually\ndifferent behavior guaranteeing new interesting mathematical properties during\nthe search [12]. Given the huge number of parameters and alternative steps\neach method admits, making a new algorithm reach good solutions for a known\nbenchmark (or a significant part of it) is often a matter of using a powerful\nparameter tuner\n[13, 14, 15] to optimize it for the specific benchmark until\nit achieves its best, sometimes over-fitted, results \u2014or even just a matter of a\nlong try-and-fail designer time until it works fine (sometimes even by chance,\nsometimes just because the modifications made it be roughly equivalent to other\npreviously known good algorithm) [16]. Many newly proposed methods seem\nto be equivalent to others up to some small changes in some algorithm steps,\nyielding the feeling that we are speaking about a sort of continuous space of algo-\nrithms rather than conceptually different algorithms deserving different names.\nAiming to put some order in this field, some classifications of these algorithms\nbased on their morphological structure, natural metaphor, or basic mechanics\nhave been proposed (see e.g. [17, 1, 18]), as well as some informally defined gen-\neralizations embracing just a few specific algorithms (e.g. [19]) and many surveys\ndescribing and comparing methods (e.g. [20, 21]). Perhaps the framework pro-\nviding more generality is the Generalized Local Search Machine (GLSM) [22],\nwhich models SLS strategies as non-deterministic finite state machines. It il-\nlustrates the run-time behaviour and control of SLS algorithms and provides\na modelling tool for hybrid and non-cooperative SLS strategies, although it is\nincomplete: it does not represent termination conditions, neighbourhood re-\nlationships, or the definition of the search space or solution set. Additionally,\nsome of the extensions proposed by the authors (namely the ones for cooperative\nand evolutionary problems) completely alter the framework, instead of slightly\nmodifying some features.\nTo the best of our knowledge, the literature lacks a fully formal (and general\nenough) definition of the common and the different parts of SLS methods, such\nthat any method can be the result of taking the framework and instantiating\nit in some specific way. Such framework would enable the definition of new\nand existing algorithms in a common language, easing their classification and\ndirect comparison. By using a common field language, new methods could be\n2\ndeveloped via a scientific and standardized variation of other known methods,\ntheir properties could be more easily investigated, and the results could be\nseamlessly classified and added into the knowledge base.\nIn this paper we present a formal language with some fixed structure and some\nparametric structure such that all SLS methods we are aware of can be the result\nof instantiating the latter according to the target method. Note that, since all\n(standard or variant) SLS methods are algorithms, any programming language\nwould actually let us implement all of them. Our language, however, will not\npursue the generality of programming languages but the opposite: having the\nleast generality and freedom needed to generalize stochastic local search, in\nparticular by making the common structure be as large and the parametric\none be as small as possible.\nThis way, the language aims to make all basic\nalgorithmic elements visible, atomic, and clear for comprehension, manipulation\nand testing.\nIn order to prove the capability of our general formal model to aid in the\ndiscovery of new non-trivial properties, we will use it to prove a disheartening\nmeta-property: the general impossibility of checking any non-trivial property re-\ngarding the results of SLS methods. One could argue that this impossibility is an\nobvious consequence of their randomness: since they involve non-deterministic\nchoices, we cannot predict their result with certainty. However, we will show\nthat their unpredictability lies on deeper reasons far beyond their randomness,\nas their behavior would remain unpredictable even if they behaved in a de-\nterministic way (or, alternatively, even if we knew in advance the results of\nall coin flips determining all their random choices during their execution). In\nparticular, this implies that their behavior cannot be predicted in general by\nmeans of statistics, as they would remain unpredictable even in the absence of\nrandomness.\nThe key to discover the unpredictability of SLS methods will be proving that\nthey are Turing-complete. Rice\u2019s theorem [23, 24] shows that determining if a\nprogram written in a Turing-complete language fulfills a non-trivial property\nregarding only its observable behavior (i.e. the relationship between its inputs\nand its outputs) is undecidable.1 Our formal model will let us prove that SLS\nmethods are Turing-complete in general, so Rice\u2019s theorem will apply to them\nas well.2\nOne could argue that the Turing-completeness of such methods could be just\na trivial consequence of assuming that they are a too general and free concept\n\u2014or rather the trivial consequence of representing them with a too general\nmodel. For instance, suppose our general model of SLS methods includes a post-\nprocessing step which maps the best solution found by a particular algorithm\n1By non-trivial we mean fulfilled by neither all programs nor by none. Thus, finding out\nif programs fulfill properties such as e.g. returning 7 for input 5, returning 7 for some input,\ncomputing function f(x) = 2x, or halting for all inputs is undecidable.\n2Note that our goal has nothing to do with showing that the result of an SLS method (i.e.\nthe solution it finds to the problem being solved) can be any program written in a Turing-\ncomplete language (the ideal output in Genetic Programming [25, 26, 27]). Very unrelated to\nthis, we will prove that in general SLS methods are Turing-complete by themselves, i.e. for\nany recursive enumerable function, they can compute it.\n3\nupon its termination, along with the problem input, into the actual algorithm\u2019s\nsolution to be returned to the user. Suppose this post-processing step can be\nany algorithm. Then, the Turing-completeness would be the trivial consequence\nof our freedom to make that step be any algorithm.3 Quite on the contrary, our\nproof of Turing-completeness will show that we do not need that much freedom\nat all \u2014regardless of whether it is actually allowed by the model.\nIn fact,\nthe Turing-completeness will arise from the intrinsic basic mechanics of these\nalgorithms, even if each involved step is defined in an extremely simple and\nminimalist way.\nIn order to emphasize the fact that the generality of the model is not neces-\nsary to achieve the Turing-completeness, we will prove the Turing-completeness\nfor a particular instance of our general model representing a particular SLS\nmethod: a Genetic Algorithm (GA). This GA will be such that, given a Turing\nmachine to be simulated and its corresponding input, it iteratively constructs\nindividuals representing longer pieces of the history (i.e. sequence of traversed\nconfigurations) of that Turing machine for that input. The GA will necessarily\ntend to create such individuals because correctly enlarging that history with\nsubsequent Turing machine steps according to the transitions of the machine\nwill increase their fitness \u2014although the fitness function will not embed any\nTuring machine mechanics and will just check sub-string inclusion.\nDespite the fact that we will show the Turing-completeness of our general\nmodel for its GA instantiation in particular, note that the Turing-completeness\nof a general model is implied by the Turing-completeness of any of its instances,\nso this will indeed prove the Turing-completeness of SLS in general. This does\nnot imply that all particular SLS methods are Turing-complete. Based on our\ngeneral model, we will also show how our proof, developed in the context of\nthe GA instance, can be easily adapted to other instances representing popular\nSLS methods, namely Particle Swarm Optimization (PSO) [28] and Ant Colony\nOptimization (ACO) [5, 6].\nThe contributions of this paper are the following:\n(a) developing the first fully formal model with enough generality to represent\nany SLS method we are aware of by instantiating its parametric elements,\nyet keeping the common part of the model as large as possible;\n(b) using the formal model to prove the inherent unpredictability (in partic-\nular, beyond randomness) of stochastic local search methods in general,\nwhich is due to the Turing-completeness of their mechanics at its most\nbasic level; and\n(c) providing a formal proof of the Turing-completeness of GA, as well as\nsketching informal proofs for ACO and PSO.\nThe rest of the paper is organized as follows. In the next section we describe\nour general model of SLS heuristics through its operational semantics. Section 3\n3Similarly, by abusing the freedom to define any fitness function, the fitness function itself\ncould be any computable function.\n4\nintroduces the particular models for GA, PSO, and ACO to exemplify instances\nof the general form. The proof of the Turing-completeness of GAs (and thus\nof SLS) is presented in Section 4, and we informally discuss how to prove the\nTuring-completeness of other instances of SLS in Section 5. In Section 6 we\npresent our conclusions and lines of future work.\n2\nGeneral Form of stochastic local search\nIn order to abstract the common structure of SLS strategies, in this section\nwe design a language and a set of operational semantics constituting our Gen-\neral Form of SLS. We claim that Genetic Algorithms, Evolution Strategies,\nAnt Colony Optimization, Iterated Local Search, Stochastic Gradient Descent,\nSimulated Annealing, Particle Swarm Optimization and every other stochastic\nlocal search algorithm we are aware of are instances of this general operational\nsemantics.\nThe decisions on how to formalize the semantics will be heavily inspired by the\noperational semantics of While by Nielson & Nielson [29, pp. 12-14 and 32-36].\nMore specific semantics will be provided later for different specific algorithms,\nbut we believe the following semantics are at the lowest abstraction level able\nto generalize the whole of SLS computation. In the following, we assume that\na given algorithm A receives a specific instance p of a problem and explores\ndifferent solutions. The best solution found by the computation is stored in a\nvariable named Best.\n2.1\nState and Syntactic Categories\nThe State of the computation is a function from variables to values.\nThe\nrelevant variables of the computation constitute the following tuple:4\n(Prob, Sols, Best, FPW, Extra) \u2208\nProblem \u00d7 Solution[] \u00d7 Solution \u00d7 F/P/W \u00d7 Extra\nSince we are aiming to generalize the SLS computational model, the cate-\ngories that define the tuple will be abstract enough to fit any SLS strategy.\nHowever, we shall define what each category represents and give a glimpse of\nits structure in different algorithms.\nState is not a constant function, as the value associated with each variable\nmay change during the computation. The letter s will represent an instance\nof State. We shall write s[A 7\u2192b] to represent a new state where the value\nassociated to A is b and every other variable has the same value as in the state s.\nNext we explain the syntactic categories necessary for SLS computation:\nProblem must be able to encapsulate the optimization problem that our al-\ngorithm must solve. Not only the abstract problem (e.g. nonlinear optimization,\n4Other auxiliary variables such as array indexes and loop counters are not reflected here,\nalthough they would be necessary for the implementation of SLS algorithms.\n5\nTSP5, etc.) but also the concrete instance and parameters of the problem (e.g.\nthe function to optimize, the graph with edge costs for TSP, etc.). When Prob\n\u2208Problem is initialized at the beginning of the computation, it will determine\nhow the functions required for the computation of the problem work.\nSolution is the category that represents possible solutions of the problem\nwithin the computation. For instance, in a GA Solution would represent the\ncodification of the chromosomes; and in an ACO algorithm, Solution would be\nthe data type to store the subgraphs that represent the paths followed by ants.\nSolution[] represents a set of instances of Solution. Thus, Best \u2208Solution\nwill be the variable that represents the best solution found by the computation\nand Sols \u2208Solution[] the set of solutions that are being considered in the\ncurrent iteration of the algorithm.\nThe most general category is F/P/W, which stands for Fitness / Pheromones\n/ Weights. F/P/W must be general enough to represent every variable neces-\nsary to guide the search in an SLS algorithm; e.g. in a swarm-based heuristic,\nFPW \u2208F/P/W would consist of the Weights of each individual, its linear\nmoment, etc.; for a GA, FPW would store the Fitness of every individual, and\nin an ACO algorithm the Pheromones of each path.\nLastly we introduce the Extra category. It will be used to wrap the param-\neters required to define the auxiliary functions needed for the computation, as\nwell as any other variable needed by the specific algorithm that is being run.\nFor example, the crossover or mutation rate for a GA would belong to this cate-\ngory. Extra will be key to the implementation of complex heuristics, as it is the\ndefault way to carry lateral effects of computation steps. In hybrid methods,\nExtra will carry all the necessary data to transition between heuristics. The\nfunctions would read this information and operate accordingly.\nAdditionally, let T [29, p.14] consist of the truth values tt (for true) and ff\n(for false), let Pexp be the syntactic category used to input the problem into\nthe computation (an instance of Pexp will be translated into its corresponding\nProblem instance to be processed), let Stm be the set of statements (semantic\nblocks) that build the SLS computation model, and Algorithm be the set of\nSLS algorithms.\nWe shall also define the meta-variables that will be used to range over con-\nstructs of our syntactic categories:\np will range over input problem expressions, Pexp.\nA will range over the set of SLS algorithms, Algorithm. Note that we are\nconsidering the abstract algorithms, which are independent of the instance being\nrun or the problem at hand.\nLastly, S will range over statements, Stm. Also S\u2032, S1 and S2 will stand for\nstatements.\nWe assume that the structure for Pexp constructs is given elsewhere as it is\nnot relevant for the computation. However, the structure for Stm constructs is\nindeed relevant and hereby presented:\n5Travelling Salesperson Problem: finding the minimally valued route in a positive-valued\ngraph that goes through every single node exactly once.\n6\nS := S1; S2 | setProb( p ) | generate | nextGen | evaluate | stop |\ncompute( p )\nWe use compute( p ) to start the execution of our given algorithm A by\nrunning setProb( p ), which translates the problem from Pexp to Prob-\nlem and then calls generate to create the original set of candidate solutions,\nevaluate to rate them, and stop to decide whether the algorithm has finished\nor if the process must be iterated by calling nextGen, which generates a new set\nof solutions with the acquired SLS knowledge.\nThe meaning of the statements is further detailed by the following functions\nand rules.\n2.2\nAuxiliary Functions\nBefore we begin to introduce the auxiliary functions that will help us define how\nthe state varies during the computation, let us introduce the following notation:\nLet Space be any space relevant to the computation where variables or syntactic\nconstructs may range (e.g. F/P/W) and let Space not be Extra; then, we\ndefine Space* as Space \u00d7 Extra.\nSince any step of the computation and any change of the state may involve\na change in Extra for at least some SLS algorithm, we note that every one of\nthe following functions can also alter the value of Extra. Next we present the\nfunctions:\n\u2022 CJAK : Algorithm \u2212\u2192\nExtra .\nThis is the function that takes an\ninstance A of Algorithm and starts the computation by initializing every\nparameter and variable needed by the heuristic.\n\u2022 PJpKs : Pexp \u00d7 State \u2212\u2192Problem* . This is the semantic function\nfor Pexp that translates the problem from the input form to the comput-\ning form that can be stored as a variable value. The previous state of the\ncomputation is used to adapt the problem to the algorithm that is being\nrun.\n\u2022 GJKs : State\n\u2212\u2192\nSolution[]* . This function uses the value of Prob\ngiven by the state s to generate a new random value for Sols concordant\nto the problem stored.\n\u2022 NextJKs : State\n\u2212\u2192\nSolution[]* . It uses the values of Prob, Sols,\nand FPW given by the state s to stochastically compute the new value\nfor Sols, using the information obtained by the previous iterations of the\nalgorithm, and in such a way that the solutions are concordant to the\nproblem.\n\u2022 AevalJKs : State \u2212\u2192F/P/W* . It uses the values of Prob, Sols, and\nFPW given by the state s to compute the new value for FPW, determined\nby the problem stored and the information obtained by the previous iter-\nations of the algorithm.\n7\n\u2022 BevalJKs : State \u2212\u2192Solution* . It uses the values of Prob, Sols, Best,\nand FPW given by the state s to compute the new value for Best, which\nis the best solution found so far.\n\u2022 SCJKs : State \u2212\u2192T . This is the stop criterium function. It analyses\nthe whole state and returns tt if the stop criteria has been met and ff if\nit has not.\nSince this is an abstraction of the SLS computation model, different SLS\nalgorithms will implement these functions differently. Just to give an example,\nlet us consider two GAs: the first one returns the value of the best individual of\nthe last generation, and the second one the best individual amongst all genera-\ntions. The former has a BevalJKs function that ignores Best, whereas the latter\ndoes consider it in every generation. Similarly, the parameters of the algorithm\ndirectly affect most of these functions.\nReaders should also be aware that many of these functions, specially functions\nAevalJKs, BevalJKs and SCJKs, could be overloaded with daemon actions such as\nlocal search phases, automatic parameter modifications, and other mechanisms\nto improve solutions.\nFor the sake of generality and notation simplicity, no\nterms or functions are given to specifically denote daemon actions in our design,\nas they can be easily integrated within the other elements of the language.\n2.3\nRules\nFinally, we introduce the rules of our SLS computation model in its General\nForm. The execution begins at an initial state s with the [compute] rule by\ninputting the problem p as a parameter: \u27e8compute( p ), s\u27e9.\nThe values of the initial state s are not relevant, as they will be changed by\nthe input problem and no previous information will affect the computation in\nany way.\nWe introduce a special notation in the same way as we did for defining the\nfunctions. Let Var be any variable of State other than Extra. We define Var*\nas Variable \u00d7 Extra.\nAs usual in operational semantics, the meaning of the statements is specified\nby a transition system with two kinds of configurations:\n\u2022 \u27e8S, s\u27e9represents that the statement S is to be executed from the state s.\n\u2022 s represents a terminal state. These final states often come in the s[A 7\u2192b]\nnotation.\nA valid transition from configuration \u03b1 to configuration \u03b2 is represented by\n\u03b1 \u21d2\u03b2. The transition rule\n\u03b3 \u21d2\u03b4\n\u03b1 \u21d2\u03b2\nindicates that we can only transition from\nconfiguration \u03b1 to configuration \u03b2 if a valid transition from \u03b3 to \u03b4 can be made.\nThe rules for the General Form are defined in Figure 1 and have the following\npurposes: [comp] rules allow to concatenate instructions carrying on the state\n8\n[comp1]\n\u27e8S1, s\u27e9\u21d2\u27e8S\u2032\n1, s\u2032\u27e9\n\u27e8S1; S2, s\u27e9\u21d2\u27e8S\u2032\n1; S2, s\u2032\u27e9\n[comp2]\n\u27e8S1, s\u27e9\u21d2s\u2032\n\u27e8S1; S2, s\u27e9\u21d2\u27e8S2, s\u2032\u27e9\n[set problem]\n\u27e8setProb( p ), s\u27e9\u21d2s[P rob\u22177\u2192PJpKs]\n[generate]\n\u27e8generate, s\u27e9\u21d2s[Sols\u22177\u2192GJKs]\n[next generation]\n\u27e8nextGen, s\u27e9\u21d2s[Sols\u22177\u2192N extJKs]\n[evaluate]\n\u27e8evaluate, s\u27e9\u21d2s[F P W \u22177\u2192AevalJKs, Best\u22177\u2192BevalJKs]\n[stoptt]\n\u27e8stop, s\u27e9\u21d2s\nif SCJKs = tt\n[stopff]\n\u27e8stop, s\u27e9\u21d2\u27e8nextGen; evaluate; stop, s\u27e9\nif SCJKs = ff\n[compute]\n\u27e8setProb( p ), s[Extra 7\u2192CJAK]\u27e9\u21d2s\u2032\n\u27e8compute( p ), s\u27e9\u21d2\u27e8generate; evaluate; stop, s\u2032\u27e9\nFigure 1: Operational semantics for the General Form.\nmodifications; [set problem] loads the instance to run onto the state of the com-\nputation; [generate], [next generation], and [evaluate] run their corresponding\nstatements to generate the initial solutions, the following generation, and eval-\nuating the fitness of their generation, respectively; and the [stop] rules either\nfinish the computation and return the final state or trigger the next iteration\nof the process, depending on the evaluation of the stop function at the current\nstate. Finally [compute] is the first rule we apply to start the computation, and\nall the others are applied as we unravel it.\n3\nGAs, PSO & ACO as instances of the General\nForm\nInstances of the General Form for particular algorithms (GA, ACO, PSO) are\nintroduced in this section.\n3.1\nGenetic Algorithms\nNow that the general model has been presented, more detailed semantics, based\non it, will be provided for genetic algorithms. The goal is now to showcase the\nstructure of the computation of genetic algorithms while keeping the semantics\ngeneral enough to be applied to particular instances of the genetic algorithms\n9\n(i.e. different variants of GA). The representation of the chromosomes, selection\nmethod, mutation rate, stop criteria, and other particularities of the genetic\nalgorithm instance should not be relevant to the semantics. This semantics is\ngeneral enough to cover degenerated instances of the genetic algorithm such as\ne.g. evolutionary strategies, where the selection and crossover steps are trivial.\nThe statements and functions added to this semantics are meant to show the\nimportance of sequentially modifying a set of individuals to guide the search.\nThe memory of genetic algorithms resides on its population, as fitness values of\na generation are no longer relevant after the creation of the next generation.\nThe State is defined as in the General Form. However, some syntactic cat-\negories can be further specified. In this case, Solution will represent the codi-\nfication of a single chromosome and Solution[] a set of instances of Solution.\nThus, Best \u2208Solution will be the fittest chromosome found by the computa-\ntion and Sols \u2208Solution[] the current generation together with every other\nsolution needed to compute crossovers and mutations. Also, FPW \u2208F/P/W\nnow consists of the fitness of every individual (which must be recomputed for\nevery generation).\nWe will use the same meta-variables of the General Form, the only difference\nbeing the structure for Stm constructs:\nS := S1; S2 | setProb( p ) | generate | select | cross | mutate |\nnextGen | evaluate | stop | compute( p )\nThree new statements (select, cross, and mutate) are considered to struc-\nture the process that was previously abstracted by nextGen in a single step.\nThis showcases how the evolution is performed in GAs: fitness-biased selection\nof individuals to cross over, generation of new individuals, and mutation of the\npopulation. The meaning of the statements is further specified by the following\nfunctions and rules.\n\u2022 SJKs : State \u2212\u2192Solution[]* . This uses the values of Prob, Sols, and\nFPW given by the state s to compute the new value of Sols consisting of\nthe individuals of the previous generation selected for the crossover stage.\n\u2022 CJKs : State\n\u2212\u2192\nSolution[]* . It uses the values of Prob, Sols, and\nFPW given by the state s to compute the new value of Sols, which is de-\ntermined by a combination of the result of the crossover operation applied\nto the selected individuals and some of the original individuals.\n\u2022 MJKs : State \u2212\u2192Solution[]* . It uses the values of Prob, Sols, and\nFPW given by the state s to compute the new value of Sols consisting of\nthe individuals already present in Sols after modifying (mutating) some\nof them.\nSince this is an abstraction of the genetic algorithm computation model,\ndifferent genetic algorithms will implement these functions differently. For in-\nstance, consider a GA with elitism where, in each generation, some of the indi-\nviduals will be selected as the elite by SJKs; the CJKs operation will force them\ninto the next value of Sols and MJKs will not modify them. On the contrary,\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Academic Paper \"Generalization and Completeness of Stochastic Local Search Algorithms\"\n\n#### Main Contributions:\nThe paper introduces a comprehensive formal model for Stochastic Local Search (SLS) algorithms, providing a unified framework to represent various SLS methods such as Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO). This model allows any known SLS method to be instantiated according to the target method. The authors demonstrate that this general formal model is Turing-complete, implying that determining non-trivial properties about their results is undecidable.\n\n#### Methodology:\n1. **General Form of SLS Heuristics**: The paper proposes a generalized form for SLS heuristics with two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible.\n2. **Particular Models for GA, PSO, ACO**: Specific instances are presented for Genetic Algorithms (GA), Particle Swarm Optimization (PSO), and Ant Colony Optimization (ACO) to illustrate the general model's applicability across different SLS methods.\n3. **Proof of Turing Completeness**: The authors use their formal model to prove that SLS algorithms in general are Turing-complete, which implies that determining any non-trivial property concerning input-output relationships is undecidable.\n\n#### Key Findings:\n- The proposed formal model successfully represents all known SLS methods by instantiating its parametric part.\n- By proving the Turing-completeness of SLS algorithms, the authors show that they are inherently unpredictable beyond randomness, implying their behavior cannot be predicted using statistical means alone.\n- The general form is flexible enough to encompass different variants and specificities within each method.\n\n#### Conclusions:\nThe paper establishes a foundational framework for understanding and comparing SLS methods. It highlights the undecidability of investigating non-trivial properties about these algorithms' results, suggesting that they are fundamentally unpredictable beyond their inherent randomness. The authors also provide insights into how this model can be adapted to other instances representing popular SLS methods like PSO and ACO.\n\n### Summary in 300 Words\n\nThe paper introduces a comprehensive formal model for Stochastic Local Search (SLS) algorithms, enabling any known method such as Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) to be instantiated within this framework. The authors prove that SLS methods are Turing-complete, meaning they can simulate any Turing machine, implying undecidability of non-trivial properties concerning input-output relationships. This model allows for a unified understanding across different SLS methods while highlighting their inherent unpredictability beyond randomness."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Top 10 Open Challenges Steering the Future of\nDiffusion Language Model and Its Variants\nYunhe Wang1, Kai Han1, Huiling Zhen1, Yuchuan Tian2,\nHanting Chen1, Yongbing Huang3, Yufei Cui1, Yingte Shu2,\nShan Gao1, Ismail Elezi1, Roy Vaughan Miles1, Songcen Xu1,\nFeng Wen1, Chao Xu2, Sinan Zeng3, Dacheng Tao4\n1Huawei Noah\u2019s Ark Lab.\n2Peking University.\n3Huawei Technologies.\n4Nanyang Technological University.\nContributing authors: yunhe.wang@huawei.com;\nAbstract\nThe paradigm of Large Language Models (LLMs) is currently defined by\nauto-regressive (AR) architectures, which generate text through a sequential\n\u201cbrick-by-brick\u201d process. Despite their success, AR models are inherently con-\nstrained by a causal bottleneck that limits global structural foresight and iterative\nrefinement. Diffusion Language Models (DLMs) offer a transformative alterna-\ntive, conceptualizing text generation as a holistic, bidirectional denoising process\nakin to a sculptor refining a masterpiece. However, the potential of DLMs remains\nlargely untapped as they are frequently confined within AR-legacy infrastructures\nand optimization frameworks. In this Perspective, we identify ten fundamen-\ntal challenges ranging from architectural inertia and gradient sparsity to the\nlimitations of linear reasoning that prevent DLMs from reaching their \u201cGPT-4\nmoment\u201d. We propose a strategic roadmap organized into four pillars: founda-\ntional infrastructure, algorithmic optimization, cognitive reasoning, and unified\nmultimodal intelligence. By shifting toward a diffusion-native ecosystem charac-\nterized by multi-scale tokenization, active remasking, and latent thinking, we can\nmove beyond the constraints of the causal horizon. We argue that this transi-\ntion is essential for developing next-generation AI capable of complex structural\nreasoning, dynamic self-correction, and seamless multimodal integration.\nKeywords: Large Language Models, Diffusion Models, Transformers\n1\narXiv:2601.14041v1  [cs.CL]  20 Jan 2026\n1 Introduction to Diffusion Language Models\nThe landscape of Natural Language Processing (NLP) has been fundamentally\nreshaped by the success of Large Language Models (LLMs) [1\u20133], predominantly gov-\nerned by the Auto-Regressive (AR) paradigm. Considering a sequence x = x1, \u00b7 \u00b7 \u00b7 , xN,\nAR typically define the distribution as\np\u03b8(x) = p\u03b8(x1)\nN\nY\nn=2\np\u03b8(xn | x1, \u00b7 \u00b7 \u00b7 , xn\u22121)\n|\n{z\n}\nAutoregressive LM formulation\n.\n(1)\nBy decomposing the joint probability of a sequence into a product of conditional prob-\nabilities, AR models like the GPT series have demonstrated extraordinary capabilities\nin text generation and reasoning. However, this \u201cleft-to-right\u201d sequential generation\ninherently suffers from several limitations: the accumulation of errors (exposure bias),\na lack of global structural foresight, and the \u201ccausal bottleneck\u201d that prevents the\nmodel from revising previous tokens based on future context.\nIn parallel, Diffusion Models have emerged as the gold standard for continuous-\ndomain generative tasks, such as high-fidelity image and video synthesis. Unlike AR\nmodels, diffusion models treat generation as a progressive denoising process\u2014starting\nfrom a state of pure noise (or total masking) and iteratively refining the entire output\ntoward a coherent structure. This is analogous to a sculptor, who begins with a rough\nblock of marble and refines all parts of the statue simultaneously. When adapted to\nthe discrete domain of language, Diffusion Language Models (DLMs) [4\u20136] offer a\ncompelling alternative: they enable non-sequential generation, support bidirectional\ncontext modeling, and allow for flexible \u201cany-to-any\u201d text editing and infilling. Let xt\ndeenote the noised sequence at the t-th timestep, the DLMs usually model the data\ndistribution as\np\u03b8(x) =\nX\nx1:T\u223cq\np(xT)\nT\nY\nt=1\np\u03b8(xt\u22121 | xt),\n|\n{z\n}\nDiffusion LM formulation\n(2)\nwhere q is the forward process progressively corrupts the original data into noisy\ntokens.\nDespite their theoretical appeal, the transition of diffusion techniques from pix-\nels to paragraphs has not been seamless. Text is inherently discrete, categorical, and\nhighly structured, making the definition of \u201cnoise\u201d and \u201cdenoising\u201d far more complex\nthan in continuous space. While early attempts at discrete diffusion and masked lan-\nguage modeling showed promise, they have yet to match the scaling efficiency and\nraw performance of their AR counterparts [7, 8]. Current DLMs often find them-\nselves \u201ctrapped\u201d in architectures and data pipelines originally optimized for AR tasks,\nleading to inefficiencies in inference, optimization, and structural reasoning.\nWhile these limitations are often discussed in the context of text generation quality\nor efficiency, their impact becomes most evident in deep research and agentic scenar-\nios, where models are required to maintain long-horizon goals, revise earlier hypotheses\n2\nbased on newly retrieved evidence, and iteratively refine structured outputs such as\nreports, plans, or theories. In such settings, the auto-regressive paradigm reveals fun-\ndamental cracks, whereas diffusion-based generation offers a more natural substrate\nfor non-linear reasoning, global editing, and iterative belief revision.\nFigure 1 Illustration of Top 10 Open Challenges Steering the Future of Diffusion Language Model\nand Its Variants.\nThis Perspective argues that for Diffusion Language Models to reach their full po-\ntential, we must move beyond merely adapting AR-centric frameworks and instead\ncultivate a native ecosystem designed for iterative, non-causal refinement. We iden-\ntify ten critical dimensions that represent the frontier of this transition. We begin\nby rethinking the foundational infrastructure, advocating for inference-efficient ar-\nchitectures that transcend the limitations of traditional KV caching and multi-scale\nstructured tokenizers that reflect the hierarchical nature of human thought. To address\nthe inherent training inefficiencies of discrete diffusion, we propose novel optimiza-\ntion paradigms to mitigate gradient sparsity and advanced masking strategies that\nmove beyond generic tokens toward functional, structured priors. Furthermore, we ex-\nplore the necessity of dynamic output length mechanisms and diffusion-native data\nengineering to move away from the rigid constraints of fixed-length generation. To\nensure practical viability, we discuss resource-efficient optimization techniques such\nas multi-step distillation and quantization tailored for denoising trajectories. Beyond\nmere generation, we envision a shift toward \u201cslow thinking\u201d through latent reason-\ning and structured prompt engineering, allowing models to utilize their bidirectional\ncontext for iterative editing and complex memory retrieval. Finally, we converge on\nthe prospect of a unified architecture, i.e., a singular, end-to-end framework capable\nof harmonizing understanding, generation, and action across multiple modalities. By\n3\naddressing these interconnected challenges, we can pave the way for a more robust,\nefficient, and structurally intelligent generation of AI that transcends the sequential\nconstraints of the causal horizon.\n2 Top 10 Open Challenges\nDespite the nascent potential of Diffusion Language Models, several fundamen-\ntal bottlenecks remain. We identify ten core challenges that currently hinder their\nperformance and scalability.\n2.1) Inference-Efficient Architectures: Beyond AR Legacies\nThe architectural backbone of most DLMs remains rooted in Transformer designs\noptimized for AR tasks. In AR models, the sequential nature of next-token predic-\ntion allows for efficient KV cache reuse [9]. However, diffusion denoising is inherently\nnon-sequential; the stochastic distribution of mask positions across iterations renders\ntraditional KV caching mechanisms ineffective [10, 11]. Without a native architecture\nthat supports bidirectional, iterative refinement without full-sequence re-computation,\nDLMs struggle to achieve the inference throughput necessary for large-scale deploy-\nment. This limitation becomes particularly acute for deep research agents, which\nrequire repeated global revisions of evolving artifacts (e.g., research drafts, hypotheses\ngraphs). Without diffusion-native inference mechanisms, iterative agent loops become\nprohibitively expensive.\n2.2) Structured Hierarchy in Tokenizers\nCurrent tokenization methods, such as Byte Pair Encoding (BPE) [12], are \u201cflat\u201d\nand statistically driven, lacking the structural hierarchy inherent in human cogni-\ntion. While humans often conceptualize text through a multi-scale lens starting with\nglobal outlines before refining local details, existing DLMs are forced to operate on a\nuniform granularity. This mismatch prevents the model from efficiently allocating com-\nputational resources between high-level semantic structuring and fine-grained lexical\npolishing.\n2.3) Optimization Paradigms: Addressing Gradient Sparsity\nA major source of inefficiency in DLM training is gradient sparsity. During long-\nsequence pre-training, the model is typically trained to denoise only a small, randomly\nmasked subset of tokens within a long context (e.g., 32k tokens). Consequently, the\nvast majority of unmasked tokens in the forward pass do not contribute a loss, re-\nsulting in sparse and inefficient gradient feedback for the computational cost of a full\nforward and backward pass. This sparse supervision creates a second, downstream\nchallenge: it introduces a distribution shift between pre-training (random masking)\nand downstream fine-tuning or reasoning, where the model must often generate or eval-\nuate complete, coherent sequences (e.g., full answers in SFT or full reasoning traces\nin RL). This misalignment complicates the adaptation and alignment process.\n4\n2.4) Advanced Masking: From Generic to Structured Functionalism\nThe prevailing \u201csingle [MASK] token\u201d paradigm is elegant but functionally limited. By\ntreating all masked positions as equivalent, it fails to account for the varying structural\nand logical importance of different tokens (e.g., a control flow operator in code vs.\na filler word in prose). The current masking approach lacks a structured mechanism\nthat considers the interdependencies between masked positions, leading to a lack of\nfunctional diversity in the model\u2019s restoration capabilities. Furthermore, considering\na deep research agent, masking a factual citation and masking a core logical claim\nshould trigger fundamentally different restoration behaviors\u2014something the current\ngeneric [MASK] paradigm cannot express.\n2.5) Dynamic Output Length and Adaptive Termination\nUnlike AR models that naturally terminate via an End-of-Sequence (EOS) token,\nDLMs typically require a pre-defined output length. This rigidity is computationally\ninefficient: tasks requiring minimal reasoning may be forced into long-sequence win-\ndows, while complex tasks may suffer from premature truncation. Current methods\nstruggle to adaptively infer the optimal length for a given query, leading to either\n\u201challucinatory padding\u201d or information loss.\n2.6) Data Engineering: Curating for Bidirectional Learning\nMost DLMs are trained on data curated for AR models, which emphasizes sequential\ncontinuity. However, to unlock the full potential of bidirectional denoising, models\nrequire data that highlights structural relationships and multi-point dependencies.\nCurrent datasets do not explicitly support the learning of global semantic \u201canchors,\u201d\nmaking it difficult for DLMs to develop the same level of structural intelligence as\nthey do in the continuous image domain.\n2.7) Resource-Efficient Model Optimization\nWhile DLMs offer theoretical parallel generation, the \u201citerative tax\u201d of multiple de-\nnoising steps often results in higher latency than AR models at parity. When batch\nsizes increase, the global attention overhead of diffusion can negate its speed advan-\ntages [13]. Finding the right balance between denoising quality and computational cost\nremains an open challenge, particularly when basic structural components of DLMs\nhave not yet converged.\n2.8) Latent Thinking and Iterative Reasoning\nReasoning in LLMs is often equated with sequential Chain-of-Thought (CoT) [14, 15].\nFor DLMs, simply mimicking this linear path is suboptimal. Current SFT paradigms\nfail to leverage the model\u2019s ability to \u201cre-think\u201d or \u201cedit\u201d its output during the de-\nnoising process. If the model is forced into a predetermined length space without\na mechanism for deep, latent refinement, it cannot effectively perform the iterative\nself-correction that characterizes complex human reasoning. Moreover, deep research\nis inherently non-linear: hypotheses are proposed, invalidated, and reformulated.\nDiffusion-native latent thinking provides a natural mechanism for such iterative belief\nrevision, whereas linear Chain-of-Thought enforces an unnatural reasoning trajectory.\n5\n2.9) Structured Prompt Engineering and Contextual Memory\nTraditional prefix-based prompting is a byproduct of causal modeling. For a bidi-\nrectional DLM, the prompt could theoretically be interleaved with the generation or\nserve as a global constraint. However, we currently lack a standardized framework for\n\u201cDiffusion-Native Prompting\u201d. This limits the model\u2019s effectiveness in scenarios like\nDeep Research or Agentic tasks, where a few global key tokens should ideally trigger\na full-sequence logical reconstruction.\n2.10) Toward a Unified Multimodal Architecture\nThe field is currently fragmented: \u201cunderstanding\u201d tasks typically rely on AR\narchitectures, while \u201cgeneration\u201d favors Diffusion. In complex domains like Vision-\nLanguage-Action (VLA) [16], this leads to hybrid models where different modalities\nare governed by different optimization goals. Achieving a truly unified architecture\nwhere understanding, generation, and action are treated as points on a continuous\ndiffusion spectrum remains the ultimate frontier.\n3 Strategic Insights for the Challenges\nTo overcome the aforementioned bottlenecks, we propose a strategic roadmap focused\non shifting from \u201cAR-adaptation\u201d to a \u201cDiffusion-native\u201d ecosystem. These insights\nare categorized into four pillars: architectural foundations, optimization mechanics,\ncognitive reasoning, and the path to a unified intelligence.\n3.1 Pillar I: Infrastructure and Structural Foundations\nRedesigning for Non-Causal Efficiency. We advocate for attention structures na-\ntively designed for diffusion, such as stochastic-aware attention or partial KV caching\ntied to specific masking patterns. While enforcing a left-to-right denoising order of-\nfers a temporary efficiency gain, the long-term solution lies in a fundamental redesign\nof the KV structure that can handle non-causal, iterative updates without redundant\nglobal re-computation.\nThe Multi-scale Tokenizer Framework. To reflect the hierarchical nature of\nhuman language, we propose a multi-scale tokenization approach. By employing\nmulti-layered vocabularies where high-level tokens represent paragraph-level seman-\ntic bridges (outlines) and low-level tokens handle fine-grained lexical details, the\nmodel can simulate hierarchical thought. This requires a shift in the training-inference\nparadigm to accommodate multi-resolution data streams, allowing the model to\n\u201csculpt\u201d global structure before \u201cfilling\u201d local content.\n3.2 Pillar II: Algorithmic Mechanics and Optimization\nDynamic Optimization and Efficiency. To mitigate gradient sparsity, we suggest\ndynamic masking ratios that evolve across training stages (e.g., starting with high-\nratio global masking and moving to low-ratio local refinement). High-impact directions\nfor inference efficiency include multi-step trajectory distillation (reducing denoising\nsteps to N < 5) and speculative decoding [17], where a smaller model proposes a draft\n6\nfor a larger model to refine. Additionally, hybrid regimes that use DLMs for long-\nsequence planning and AR for high-throughput execution could offer a synergistic\ncomputational balance.\nFunctional Masking and Elastic Generation. We advocate for \u201cStructured Mask-\ning\u201d using multiple specialized mask tokens (e.g., [LOGIC-MASK], [ENTITY-MASK])\nwith prior-informed dependencies. By introducing a functional hierarchy to masking,\nDLMs can better capture complex structural constraints. Furthermore, incorporating\nEOS-position prediction directly into the denoising steps allows the model to perceive\nthe optimal output length dynamically, enabling an \u201celastic\u201d generation window that\nadaptively terminates based on the query\u2019s complexity [18, 19].\n3.3 Pillar III: Cognitive Reasoning and Interaction\nDiffusion-Native CoT. We propose shifting from linear, sequential reasoning to\nan iterative \u201coutline-then-detail\u201d process. During denoising, the model should engage\nin Active Remasking: identifying low-confidence tokens or logical inconsistencies and\n\u201cre-masking\u201d them for immediate re-generation. This internal feedback loop enables\na form of \u201clatent thinking\u201d and self-correction that surpasses the rigid, forward-only\nlimits of traditional sequential logic.\nScaffolding Prompts and Contextual Memory. We suggest transitioning from\n\u201cPrefix-Prompts\u201d to \u201cCloze-Scaffolding\u201d. By providing global anchor tokens as a skele-\ntal prompt interleaved throughout the sequence, DLMs can perform rapid, high-fidelity\ninfilling. This paradigm is particularly suited for RAG and long-term memory manage-\nment, where the prompt acts as a non-sequential retrieval cue that guides the global\ndenoising trajectory.\n3.4 Pillar IV: Data Engineering and Unified Intelligence\nDiffusion-Native Data Ecosystems. Data engineering must move toward curat-\ning \u201cstructural dependencies\u201d rather than just \u201csequences.\u201d This involves annotating\n\u201canchor tokens\u201d and structural landmarks within pre-training corpora to guide the\nmodel\u2019s focus. Structured SFT and RL datasets should be redesigned to empha-\nsize multi-point editing and bidirectional restoration, rewarding the model for global\ncoherence and logical stability.\nThe Path to a Unified Diffusion Backbone. The ultimate frontier is a Unified\nDiffusion Objective that treats understanding (high-noise denoising) and generation\n(low-noise denoising) as part of a single continuum. In Vision-Language-Action (VLA)\nmodels, this would allow for a seamless transition between perceiving the environment\nand executing actions, using a singular, natively multimodal backbone that collapses\nthe modality gap into a unified denoising manifold.\n3.5 Pillar V: DLMs as the Cognitive Core of AI Agents\nRather than introducing an additional pillar, we view deep research agents as a\nsystems-level instantiation of the four pillars discussed above. Deep research is not\nmerely an extended form of text generation, but a cognitive process characterized by\nhypothesis formation, evidence aggregation, contradiction resolution, and the iterative\n7\nrefinement of structured artifacts. Unlike single-pass generation, research inherently re-\nquires the ability to revisit, revise, and globally reorganize earlier conclusions in light of\nnewly acquired information. From this perspective, diffusion language models provide a\nmore compatible generative substrate for deep research agents, as their denoising-based\nformulation naturally supports non-linear revision and global structural editing.\nIn practice, a deep research agent may repeatedly re-evaluate and modify an evolv-\ning research draft, hypothesis graph, or literature synthesis as new evidence is retrieved\nor prior assumptions are invalidated. Diffusion-native architectures that support par-\ntial updates, structured masking, and inference-efficient global refinement render such\nlong-horizon research loops computationally viable [20]. In contrast, auto-regressive\ngeneration typically enforces full sequential regeneration, leading to unnecessary re-\ncomputation and limiting the feasibility of iterative, research-style workflows. More\nimportantly, diffusion-based latent thinking enables a form of reasoning that aligns\nclosely with the epistemic dynamics of research. Rather than committing to a fixed\nlinear chain of thought, the model can identify low-confidence, contradictory, or log-\nically fragile regions within its own output and actively re-mask them for targeted\ndenoising. This mechanism enables iterative self-correction and internal consistency\nchecking, resembling a form of implicit peer review that is difficult to realize under\nstrictly causal generation paradigms.\nViewed through this lens, deep research agents are not a downstream application\nlayered on top of diffusion language models, but a natural expression of their core\ninductive bias. By unifying global planning, local refinement, and iterative belief revi-\nsion within a single denoising framework, diffusion language models offer a principled\nfoundation for research-oriented, agentic intelligence.\n4 Conclusion\nThe transition from the current auto-regressive dominance to a more balanced or\nunified diffusion-based landscape represents more than a mere shift in generative mod-\neling; it is a fundamental move toward a more robust and human-like intelligence.\nAs we have argued, the \u201ccausal horizon\u201d of next-token prediction, while powerful,\nimposes intrinsic limits on structural foresight, error correction, and multi-scale rea-\nsoning. To transcend these boundaries, the research community must move beyond\nthe \u201cAR-adaptation\u201d phase and embrace a native ecosystem for text diffusion.\nAddressing the ten open challenges identified in this Perspective\u2014ranging from\nhardware-aware non-causal architectures to the realization of latent, non-linear think-\ning\u2014requires a concerted effort across the stack of model design. It demands a\nfundamental rethink of how we discretize language via tokenization, how we optimize\nfor gradient efficiency, and how we interact with models that can \u201cre-think\u201d and \u201cedit\u201d\ntheir own internal trajectories. By cultivating these \u201cdiffusion-native\u201d principles, we\ncan develop architectures that are not only more computationally flexible and effi-\ncient but also inherently capable of the global structural reasoning that has remained\nelusive for purely sequential models.\nUltimately, the path to a unified intelligence lies in collapsing the divide between\nunderstanding, generation, and action. Diffusion models, with their unique ability to\n8\ntreat these tasks as different facets of a singular denoising manifold, offer a promising\nblueprint for this unification. As we solve the bottlenecks of inference latency and\noptimization stability, Diffusion Language Models may well become the cornerstone\nof a new era of AI that does not just predict the next word, but scrupulously scuplts\nthe entire structure of thought.\nReferences\n[1] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models\nare few-shot learners. Advances in neural information processing systems 33,\n1877\u20131901 (2020)\n[2] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,\nD., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report.\narXiv preprint arXiv:2303.08774 (2023)\n[3] Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S.,\nWang, P., Bi, X., et al.: Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. arXiv preprint arXiv:2501.12948 (2025)\n[4] Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R.,\nLi, C.: Large language diffusion models. arXiv preprint arXiv:2502.09992 (2025)\n[5] Ye, J., Xie, Z., Zheng, L., Gao, J., Wu, Z., Jiang, X., Li, Z., Kong, L.: Dream 7b:\nDiffusion large language models. arXiv preprint arXiv:2508.15487 (2025)\n[6] Tian, Y., Liang, Y., Sun, J., Zhang, S., Yang, G., Shu, Y., Fang, S., Guo, T.,\nHan, K., Xu, C., et al.: From next-token to next-block: A principled adaptation\npath for diffusion llms. arXiv preprint arXiv:2512.06776 (2025)\n[7] Ni, J., Liu, Q., Dou, L., Du, C., Wang, Z., Yan, H., Pang, T., Shieh, M.Q.: Dif-\nfusion language models are super data learners. arXiv preprint arXiv:2511.03276\n(2025)\n[8] Bie, T., Cao, M., Chen, K., Du, L., Gong, M., Gong, Z., Gu, Y., Hu, J., Huang,\nZ., Lan, Z., et al.: Llada2. 0: Scaling up diffusion language models to 100b. arXiv\npreprint arXiv:2512.15745 (2025)\n[9] Kwon, W.: vllm: An efficient inference engine for large language models. PhD\nthesis, University of California, Berkeley (2025)\n[10] Wu, C., Zhang, H., Xue, S., Liu, Z., Diao, S., Zhu, L., Luo, P., Han, S., Xie, E.:\nFast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and\nparallel decoding. arXiv preprint arXiv:2505.22618 (2025)\n[11] Wu, C., Zhang, H., Xue, S., Diao, S., Fu, Y., Liu, Z., Molchanov, P., Luo,\n9\nP., Han, S., Xie, E.: Fast-dllm v2: Efficient block-diffusion llm. arXiv preprint\narXiv:2509.26328 (2025)\n[12] Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare words\nwith subword units. In: Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (volume 1: Long Papers), pp. 1715\u20131725 (2016)\n[13] Fu, H., Huang, B., Adams, V., Wang, C., Srinivasan, V., Jiao, J.: From bits to\nrounds: Parallel decoding with exploration for diffusion language models. arXiv\npreprint arXiv:2511.21103 (2025)\n[14] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\nD., et al.: Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems 35, 24824\u201324837 (2022)\n[15] Bi, Z., Han, K., Liu, C., Tang, Y., Wang, Y.: Forest-of-thought: Scaling test-time\ncompute for enhancing llm reasoning. In: Forty-second International Conference\non Machine Learning (2025)\n[16] Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom,\nL., Hausman, K., Ichter, B., et al.: \u03c00: A vision-language-action flow model for\ngeneral robot control. arXiv preprint arXiv:2410.24164 (2024)\n[17] Gao, Y., Ji, Z., Wang, Y., Qi, B., Xu, H., Zhang, L.: Self speculative decoding\nfor diffusion large language models. arXiv preprint arXiv:2510.04147 (2025)\n[18] Li, J., Dong, X., Zang, Y., Cao, Y., Wang, J., Lin, D.: Beyond Fixed: Training-Free\nVariable-Length Denoising for Diffusion Large Language Models (2025). https:\n//arxiv.org/abs/2508.00819\n[19] Shu, Y., Tian, Y., Xu, C., Wang, Y., Chen, H.: Deferred Commitment Decoding\nfor Diffusion Language Models with Confidence-Aware Sliding Windows (2026).\nhttps://arxiv.org/abs/2601.02076\n[20] Anonymous:\nBeyond\nmasks:\nEfficient,\nflexible\ndiffusion\nlanguage\nmodels\nvia\ndeletion-insertion\nprocesses.\nIn:\nSubmitted\nto\nThe\nFourteenth\nInter-\nnational\nConference\non\nLearning\nRepresentations\n(2025).\nunder\nreview.\nhttps://openreview.net/forum?id=VbvXjs5f72\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThe paper identifies ten fundamental challenges that currently hinder the performance and scalability of Diffusion Language Models (DLMs) and their variants. These challenges span across architectural foundations, optimization paradigms, cognitive reasoning, data engineering, and unified intelligence.\n\n#### Methodology\nThe authors propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. They advocate for the development of native diffusion ecosystems characterized by multi-scale tokenization, active remasking, and latent thinking to move beyond the constraints of the causal horizon.\n\n#### Key Findings\n1. **Inference-Efficient Architectures**: Most DLMs are rooted in Transformer designs optimized for AR tasks, leading to limitations when transitioning from pixel-level to paragraph-level diffusion techniques.\n2. **Structured Hierarchy in Tokenizers**: Current tokenization methods often lack a hierarchical structure reflective of human cognition, causing inefficiencies in resource allocation between high-level and fine-grained processing.\n3. **Optimization Paradigms**: Gradient sparsity remains a major source of inefficiency during DLM training, necessitating novel optimization paradigms to mitigate this issue.\n4. **Advanced Masking**: The current single-token masking approach is functionally limited; it fails to account for the varying structural and logical importance of different tokens in text generation.\n5. **Dynamic Output Length and Adaptive Termination**: Unlike AR models that naturally terminate with an EOS token, DLMs require a predefined output length, leading to inefficiencies in tasks requiring adaptive inference lengths.\n6. **Data Engineering**: Current datasets are primarily curated for AR models, lacking support for the learning of global semantic relationships essential for bidirectional denoising and multi-point dependencies.\n7. **Resource-Efficient Model Optimization**: While DLMs offer theoretical parallel generation, their iterative nature often results in higher latency compared to AR models at parity.\n8. **Latent Thinking and Iterative Reasoning**: Traditional LLM reasoning is sequential, but DLMs require a more natural substrate for non-linear reasoning, global editing, and iterative belief revision.\n9. **Structured Prompt Engineering and Contextual Memory**: Current prompting methods are rooted in causal modeling, limiting the effectiveness of DLMs in scenarios requiring dynamic context handling.\n10. **Unified Multimodal Architecture**: The current fragmented landscape of AI models requires a unified architecture capable of treating understanding, generation, and action as points on a continuous diffusion spectrum.\n\n#### Conclusions\nTo overcome these challenges, the paper advocates for a transition from AR-centric adaptation to a native diffusion ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking. By addressing these fundamental bottlenecks, DLMs can reach their full potential in complex structural reasoning, dynamic self-correction, and seamless multimodal integration.\n\nThe ultimate goal is to develop a unified architecture that treats understanding, generation, and action as points on a continuous diffusion spectrum, enabling the creation of intelligent agents capable of deep research and agency."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "HALT: Hallucination Assessment via Latent Testing\nRohan Bhatnagar * 1 Youran Sun * 2 Chi Andrew Zhang 3 Yixin Wen \u2020 4 Haizhao Yang \u2020 1 2\nAbstract\nHallucination in large language models (LLMs)\ncan be understood as a failure of faithful readout:\nalthough internal representations may encode un-\ncertainty about a query, decoding pressures still\nyield a fluent answer. We propose lightweight\nresidual probes that read hallucination risk di-\nrectly from intermediate hidden states of question\ntokens, motivated by the hypothesis that these\nlayers retain epistemic signals that are attenu-\nated in the final decoding stage. The probe is\na small auxiliary network whose computation is\norders of magnitude cheaper than token genera-\ntion and can be evaluated fully in parallel with\ninference, enabling near-instantaneous hallucina-\ntion risk estimation with effectively zero added\nlatency in low-risk cases. We deploy the probe\nas an agentic critic for fast selective generation\nand routing, allowing LLMs to immediately an-\nswer confident queries while delegating uncertain\nones to stronger verification pipelines. Across\nfour QA benchmarks and multiple LLM families,\nthe method achieves strong AUROC and AURAC,\ngeneralizes under dataset shift, and reveals inter-\npretable structure in intermediate representations,\npositioning fast internal uncertainty readout as a\nprincipled foundation for reliable agentic AI.\n1. Introduction\nLarge language models (LLMs) have made remarkable\nprogress recently (Yang et al., 2025; Comanici et al., 2025)\nand are being applied in an increasing number of real-world\nscenarios. However, hallucination, which refers to situations\nwhere a model produces information that appears plausible\nbut is actually false or not supported by facts, undermines\n*Equal contribution\n1Department of Computer Science, Uni-\nversity of Maryland, College Park, MD, USA\n2Department\nof Mathematics, University of Maryland, College Park, MD,\nUSA 3Department of Statistics, University of Chicago, Chicago,\nIL, USA\n4Department of Geography, University of Florida,\nGainesville,\nFL,\nUSA.\nCorrespondence\nto:\nYixin\nWen\n<yixin.wen@ufl.edu>, Haizhao Yang <hzyang@umd.edu>.\nPreprint. January 21, 2026.\nusers\u2019 trust in LLMs and limits their use in critical settings.\nTherefore, developing effective methods for hallucination\ndetection is critical. Ideally, such methods should main-\ntain real-time responsiveness without increasing response\nlatency or significantly increasing generation costs.\nWe propose a lightweight hallucination-detection method\nbased on question intermediate representations that predicts\nhallucination risk efficiently and accurately. The additional\ncomputation introduced by our method is less than 1h of\nthat required to generate a single token, so our approach\nadds negligible cost to generation. Moreover, the detector\ncan be evaluated in parallel with inference, introducing no\nextra latency to the user. In essence, we aim to build a \u201clie\ndetector\u201d or \u201cmind reader\u201d for LLMs.\nWith an effective hallucination detector, we can build an\nLLM router that adaptively allocates computation. Given a\nuser query, the default LLM begins generating a response\nwhile our detector estimates the hallucination risk in parallel.\nBecause the detector only requires the question\u2019s intermedi-\nate representations, it can run concurrently with generation.\nIf the predicted risk is low, the system returns the gener-\nated response as usual; otherwise, it routes the query to a\nslower but more reliable pipeline (e.g., a stronger model,\nreasoning-augmented generation, cross-model verification,\nor retrieval-augmented generation (RAG)). In the low-risk\ncase, this design introduces zero additional latency; in the\nfallback case, the extra delay is less than the time to generate\na single token.\nOur motivation follows naturally from recent evidence\nthat LLMs perform substantial hidden consideration dur-\ning forward propagation. Prior work (Lindsey et al., 2025;\nChen et al., 2025) suggests that intermediate representa-\ntions encode reasoning, planning, and control signals that\nguide generation, yet these signals may not be faithfully\nexpressed in the final text. This motivates directly reading\nout hallucination-related signals from intermediate repre-\nsentations, rather than relying on generated results.\nBased on the above observations, we hypothesize that in-\ntermediate representations encode uncertainty signals.\nThis is analogous to how students taking an exam can sense\n\u201cI don\u2019t know this\u201d for a question, yet would never write\nthat on the answer sheet. LLMs exhibit a similar pattern\nbecause they are trained to provide an answer whenever\n1\narXiv:2601.14210v1  [cs.CL]  20 Jan 2026\nHALT: Hallucination Assessment via Latent Testing\nFigure 1. Comparison of hallucination handling strategies. Traditional detection pipelines wait for generation to complete before checking\nfor hallucinations, doubling latency in the fallback case. Our method evaluates hallucination risk in parallel with generation, enabling\nzero-latency responses for confident queries while routing uncertain ones to stronger models.\npossible. This motivates us to build a small detector that\ncaptures this internal sense of confusion. Moreover, this\nanalogy suggests that intermediate layers should be more\ninformative than the final output, just as the internal feeling\ndiffers from what appears on the answer sheet.\nIn our experiments, we indeed find that using representa-\ntions from intermediate layers is often more effective than\nusing the final-layer representations. We attribute this to the\nfact that the final layer has already been decoded into the\ntoken space, leading to significant information loss. In other\nwords, the model contains internal features related to confi-\ndence or hallucination, but these features are unnecessary\nfor output generation, so they may be discarded in the last\nfew layers.\nFurthermore, we only use the representations aligned to the\nquestion as input to the detector. This choice is driven by\nthe need to balance detection quality and response latency.\nIn our experiments, we find that question-only representa-\ntions are already sufficient for accurate detection. Including\nanswer representations yields a marginal improvement but\nincurs substantial latency overhead.\nOur main contributions are as follows.\n\u2022 We propose a lightweight hallucination detection\nmethod that incurs negligible computational overhead\nand can be evaluated in parallel with inference.\n\u2022 Building on this detector, we develop a pipeline that\nroutes queries with high hallucination risk to stronger\nmodels to improve generation quality without increas-\ning response latency or token generation cost. Pre-\nliminary experiments show that this pipeline improves\nanswer correctness by xxx.\n\u2022 We conduct extensive ablations and identify sev-\neral properties of intermediate representations. (a)\nIntermediate-layer representations are more effective\nthan the final layer for hallucination detection. (b)\nWhile question-aligned representations are often suf-\nficient for accurate risk estimation, including answer\ntokens improves our method.\n2. Related Work\nThe perplexity of an LLM\u2019s answer can itself serve as an\nindicator for hallucination detection, but as shown in (Ren\net al., 2023), it is unreliable. (Kuhn et al., 2023) and (Far-\nquhar et al., 2024) propose semantic entropy. They require\nthe LLM to generate multiple answers to a given question,\nthen cluster them, and determine the hallucination likeli-\nhood based on the entropy of the clusters. Higher semantic\nentropy indicates a higher probability of hallucination. Sim-\nilarly, (Lin et al., 2024), (Manakul et al., 2023), and (Chen\net al., 2024) detect hallucinations based on consistency over\nmultiple sampled answers.\n2\nHALT: Hallucination Assessment via Latent Testing\nTable 1. Summary of hallucination detection methods.\nMethod\nSampling\nFeatures Used\nQ/A Features Used\nPerplexity\nNo Need\nOutput Logits\nAnswer\nSemantic Entropy (Farquhar et al., 2024)\nNeed\nOutput\nAnswer\nLexical Similarity (Lin et al., 2024)\nNeed\nOutput\nAnswer\nSelfcheckGPT (Manakul et al., 2023)\nNeed\nOutput\nAnswer\nEigenScore (Chen et al., 2024)\nNeed\nMiddle Hidden States\nAnswer\nP(I Know) (Kadavath et al., 2022)\nNo Need\nLast Hidden State\nQuestion\nTrue Direction (B\u00a8urger et al., 2024)\nNo Need\nLast Hidden State\nAnswer\nHaloScope (Du et al., 2024)\nNo Need\nMiddle Hidden States\nAnswer\nHARP (Hu et al., 2025)\nNo Need\nLast Hidden State\nAnswer\nOurs\nNo Need\nMiddle Hidden States\nQuestion\nHowever, a practical hallucination detector should require\nmuch less computation, at least smaller than the cost of\ngenerating an answer with the LLM. (Kadavath et al., 2022)\ntrained a classifier that uses the last hidden state of the ques-\ntion to predict whether the model knows the answer, i.e.,\nP(I know). (B\u00a8urger et al., 2024) used the last hidden state\nof the answer to learn, via linear regression, two directions,\nthe True direction and the False direction, and performed\nhallucination detection within this two-dimensional sub-\nspace.\nBased on the above methods, HARP (Hu et al., 2025) and\nHaloScope (Du et al., 2024) employ SVD to project the last\nor intermediate hidden states corresponding to the answer\ntokens into a low-dimensional subspace, after which a two-\nlayer MLP is used to regress the hallucination score. As\na follow-up to HaloScope, (Park et al., 2025) attempts to\nidentify hallucinations by modifying the intermediate repre-\nsentations of the LLM and then classifying the last hidden\nstate.\n3. Methodology\nProblem Definition\nWe formulate hallucination detection\nas a knowledge prediction task. Given a user query q, our\ngoal is to estimate whether the LLM contains the correct\nanswer before or during generation. During the forward\nprocess, the LLM produces hidden representations hl \u2208\nRN\u00d7D for each layer l \u2208[0, L], where N is the sequence\nlength, D is the hidden dimension, and L is the number of\ntransformer layers. We use a lightweight auxiliary network\ng\u03b8 that takes the hidden representation h as input and outputs\na scalar score\np = g\u03b8(h),\nwhere p \u2208[0, 1] represents the probability that the model\ncan correctly answer the query q. A low value of p indicates\nthat the model is uncertain or likely to hallucinate when\ngenerating the response.\nFeature Extraction from LLM\nThe detector relies on the\nLLM\u2019s hidden representations to infer whether the model is\nconfident about a given query. In principle, representations\nfrom any transformer layer can be used as input. We choose\nto extract features from intermediate layers rather than the\nfinal one. As discussed earlier, the last layer is optimized\nfor next-token prediction and may discard signals unrelated\nto generation. Intermediate layers, by contrast, retain richer\nrepresentations that capture the model\u2019s internal uncertainty.\nWe also restrict the detector\u2019s input to hidden states corre-\nsponding only to the question tokens. Formally, the feature\nwe use can be represented as\nhl = Transformerl(q),\nwhere hl denotes the hidden representation of the\nquery(question) q at layer l. This choice reduces computa-\ntion and avoids the need to process the model\u2019s generated\ntext. More importantly, it allows the hallucination likelihood\nto be estimated before any answer is produced. Methods\nthat depend on analyzing the generated output must first\nwait for the model to finish responding, then, if the answer\nis found unreliable, invoke a stronger model to regenerate it,\neffectively doubling the latency. Our approach avoids this\ninefficiency by evaluating the question representations dur-\ning the original forward pass, so the system can proactively\nswitch to a more reliable model and introduce almost no\nadditional delay. For factual benchmarks, we achieve perfor-\nmance comparable to or better than output-based methods,\nusing only the question representations without waiting for\nthe model\u2019s response.\nTraining Objective\nWe construct the training data from\nquestion-answering benchmarks that include reference an-\nswers. For each question q with the standard answer a\u2217, the\ntarget LLM generates one answer a. We then obtain a cor-\nrectness label by using an external judge to compare (q, a)\nwith a\u2217. In particular, we use gpt4o for to produce more\naccurate labels than rule-based evaluation methods. In prin-\nciple, multiple answers can be sampled for each question to\n3\nHALT: Hallucination Assessment via Latent Testing\nestimate the probability of correctness. Still, in practice, we\ngenerate only one answer per question due to time and cost\nlimitations. The resulting label is y \u2208{0, 1}, where y = 1\nmeans the generated answer is correct and y = 0 means it\nis hallucinated or incorrect.\nGiven the detector output p \u2208[0, 1], we employ a binary\ncross-entropy objective. For a dataset with M examples\n{(qi, yi)}M\ni=1 and detector scores pi, we minimize\nL = \u22121\nM\nM\nX\ni=1\n\u0000yi log pi + (1 \u2212yi) log(1 \u2212pi)\n\u0001\n.\nIn closed-book question answering, the model generates\nanswers solely from its parametric knowledge without ac-\ncess to external sources. An incorrect answer in this setting\nindicates that the model has produced content inconsistent\nwith factual ground truth, which aligns with the definition of\nfactual hallucination. We therefore treat correctness labels\nas a proxy for hallucination detection, following prior work\nin this area (Du et al., 2024; Farquhar et al., 2024).\nTable 2. Detector architecture configurations. Input dimension\nvaries by base LLM (e.g., 3584 for Qwen-2.5-7B).\nArchitecture\nHidden Dim\nLayers\nParams\nMLP\n128\u20131024\n4\n3M\u201337M\nTransformer\n256\u2013512\n4\u20138\n4M\u201330M\nNetwork Architecture (Hallucination Detector)\nWe ex-\nperiment with two architectures for the hallucination detec-\ntor, MLP and Transformer, as summarized in Table 2. Both\nnetworks take the hidden representations of the question hl\nas input and output a single confidence score p.\nFor the MLP architecture, the input must have a fixed length.\nTherefore the hidden representation hl \u2208RN\u00d7D is first\ncompressed into a single vector in RD. We consider several\nsimple aggregation methods, including mean pooling, max\npooling, and using the hidden state of the last token, which\noften corresponds to the question mark. We also consider\napplying principal component analysis (PCA) to the hidden\nrepresentations and retain the top n principal components.\nThe resulting pooled vector or concatenated principal com-\nponents are then fed into an MLP that predict the confidence\nscore p.\nThe transformer architecture is more flexible since it natu-\nrally processes sequence representations. In this case, the\nhidden states hl are directly used as input to a lightweight\ntransformer encoder. This model can capture token-level in-\nteractions within the question without requiring input-stage\npooling. At the output stage, we apply attention pooling to\naggregate the sequence into a single vector. Specifically, a\nsmall MLP computes a scalar score for each token, and the\nfinal representation is a softmax-weighted sum of all token\nrepresentations.\n4. Experiments\nTable 3. Summary of four QA benchmarks used for hallucination\ndetection evaluation.\nDataset\nSize\nType\nTopic\nTriviaQA\n99K\nOpen QA\nTrivia\nNQ-Open\n92K\nOpen QA\nWikipedia\nMMLU-Pro\n12K\nMCQ\nProfessional\nWebQuestions\n6K\nOpen QA\nEntity\nDatasets and models\nOur experiments use four open-\ndomain question answering benchmarks, namely TriviaQA\n(Joshi et al., 2017), NQ-Open (Kwiatkowski et al., 2019),\nMMLU-Pro (Wang et al., 2024), and WebQuestions (Berant\net al., 2013), summarized in Table 3. TriviaQA contains gen-\neral knowledge trivia questions in an unfiltered no-context\nsetting. NQ-Open consists of naturally occurring queries\nfrom Google Search paired with Wikipedia-based answers.\nMMLU-Pro provides multiple-choice questions spanning\n14 professional and academic subjects, often requiring multi-\nstep reasoning. WebQuestions includes entity-centric ques-\ntions originally collected from the Google Suggest API.\nWe evaluate our approach on three model families, namely\nLLaMA-2 (Touvron et al., 2023), Qwen2.5 (Yang et al.,\n2024), and Gemma-3 (Team, 2025). Ablation studies on\nmodel scale are conducted using the 7B, 14B, and 32B in-\nstruct variants of Qwen2.5. Detailed statistics on correctness\nlabels for each model-dataset pair are provided in Table 5.\nEvaluation Metrics\nAUROC (area under the ROC curve)\nis employed as the primary evaluation metric. The ROC\n(receiver operating characteristic) curve plots the true posi-\ntive rate (TPR) against the false positive rate (FPR) across\ndifferent classification thresholds. AUROC summarizes a\nbinary classifier\u2019s ability to separate positives from nega-\ntives over all thresholds, ranging from 0 to 1, where higher\nvalues indicate stronger discriminative power.\nIn addition, accuracy and AURAC (area under the rejec-\ntion\u2013accuracy curve) are also reported. The RAC plots\naccuracy against coverage, where samples are ranked by\nconfidence and coverage denotes the fraction of samples re-\ntained. AURAC evaluates whether the model\u2019s confidence is\nreliable, with higher values indicating a stronger alignment\nbetween correctness and confidence. Figure 3 provides a\nconcrete example of a RAC curve, which may aid under-\nstanding of this metric.\n4\nHALT: Hallucination Assessment via Latent Testing\nTable 4. Hallucination detection performance (AUROC, %) on four QA benchmarks across three model families. Bold indicates best\nperformance per model-dataset pair. * indicates severe label imbalance (see Table 5). Our method consistently outperforms baselines in\nmost settings.\nModel\nMethod\nTriviaQA\nNQ-Open\nMMLU-Pro\nWebQuestions\nLLaMA 2 Chat 7B\nHaloScope\n77.40\n67.0\n79.6\n72.6\nSemantic Entropy\n81.23\n77.57\n76.76\n80.12\nOurs (question)\n82.10\n79.87\n71.22\n80.79\nOurs (answer)\n88.96\n83.76\n77.85*\n82.76\nQwen-2.5-7B-Instruct\nHaloScope\n85.5\n70.3\n81.1\n80.4\nSemantic Entropy\n78.48\n80.22\n74.05\n80.71\nOurs (question)\n87.31\n83.28\n83.21\n84.01\nOurs (answer)\n93.95\n88.43\n87.08\n87.67\nGemma-3-4b-it\nHaloScope\n76.5\n79.62\n77.7\n76.7\nSemantic Entropy\n76.08\n75.48\n57.92\n76.76\nOurs (question)\n83.93\n82.96\n74.65\n79.63\nOurs (answer)\n88.80\n85.65*\n75.53*\n82.51\nBaselines\nWe compare our method with the highest per-\nforming open baselines, including HaloScope (Du et al.,\n2024) and Semantic Entropy (Farquhar et al., 2024).\nCorrectness labeling\nWe use GPT-4o to obtain correct-\nness labels for LLM-generated answers. For each question,\nwe prompt GPT-4o with the original question, the ground-\ntruth answer, and the LLM-generated answer, asking it to\nreturn a binary correctness label (see Appendix A for the\nfull prompt). This approach provides more robust evaluation\nthan rule-based string matching, particularly for open-ended\nquestions where semantically equivalent answers may differ\nin surface form.\n4.1. Main Results\nTable 4 presents the main results. Our method achieves\nthe best AUROC on 10 out of 12 model-dataset combi-\nnations. Qwen-2.5-7B-Instruct shows the strongest per-\nformance, where our approach outperforms all baselines\nacross all four benchmarks, with gains of up to 8.5 points\non TriviaQA and 8.2 points on NQ-Open. On LLaMA 2\nChat 7B and Gemma-3-4b-it, we observe similar improve-\nments, surpassing prior methods by 7.7 and 12.3 points\non TriviaQA respectively. Using answer token representa-\ntions (Ours answer) consistently outperforms question token\nrepresentations (Ours question) across all settings, suggest-\ning that answer tokens encode richer information about the\nmodel\u2019s factual confidence. The two exceptions occur on\nMMLU-Pro for LLaMA 2 and Gemma-3, where HaloScope\nachieves slightly higher AUROC. We attribute this to two\nfactors: (1) a format mismatch, as TriviaQA, NQ-Open, and\nWebQuestions are open-ended QA tasks while MMLU-Pro\nis a multiple-choice benchmark where the model selects\nfrom predefined options rather than generating free-form an-\nswers; and (2) label imbalance, as LLaMA 2 and Gemma-3\nachieve only 15.4% and 27.1% accuracy on MMLU-Pro re-\nspectively (see Table 5), resulting in limited positive samples\nfor training the detector. Overall, these results demonstrate\nthat intermediate-layer representations provide a strong sig-\nnal for hallucination detection, competitive with or superior\nto methods that require sampling multiple outputs.\nWe evaluate whether our detector generalizes under distri-\nbution shift. Unless otherwise specified, we extract hidden\nstates from layer 19 of Qwen-2.5-7B-Instruct. We train the\ndetector on each of the four datasets and evaluate it on the\nremaining datasets. We also train on the union of all datasets\n(All) and test on each dataset. Figure 2 reports the AUROC\nfor each train\u2013test pair.\nThe detector exhibits strong out-of-distribution generaliza-\ntion. Most off-diagonal entries remain in the 75\u201393 AU-\nROC range, indicating that the intermediate representations\ncapture dataset-agnostic signals predictive of hallucination\nrather than dataset-specific patterns. Training on the union\n(All) yields a robust model that achieves AUROC of 94.20\non TriviaQA, 88.78 on NQ-Open, 86.78 on MMLU-Pro, and\n87.17 on WebQuestions. While All does not uniformly domi-\nnate the best single-source model for every target, it provides\ncompetitive and stable performance across all datasets, re-\nducing sensitivity to the particular source distribution. These\nresults support our conclusion that the learned features are\nlargely transferable across different QA domains.\n4.2. Accuracy after Hallucination Removed\nBeyond binary hallucination detection, our method can be\nused for selective prediction: abstaining from answering\nwhen the model is likely to hallucinate. Crucially, because\nwe use only question token representations, the detector\n5\nHALT: Hallucination Assessment via Latent Testing\n(a) Using question tokens only\n(b) Using question + answer tokens\nFigure 2. Out-of-distribution generalization. Heatmap of AUROC when training the detector on one dataset (columns) and evaluating it\non another (rows), including training on the union of all datasets (All).\nFigure 3. Rejection-Accuracy Curve (RAC) for Qwen-2.5-7B\non TriviaQA using only question token representations. As we\nreject samples with low confidence, accuracy increases from 85.7%\nat full coverage to 97.7% at 40% coverage. AURAC = 0.8753\nindicates strong alignment between confidence and correctness.\ncan evaluate hallucination risk before the model generates\nany answer, introducing zero additional latency. We use the\ndetector\u2019s output probability as a confidence score and reject\nsamples below a threshold. Figure 3 shows the rejection-\naccuracy curve (RAC) for Qwen-2.5-7B on TriviaQA, where\nwe vary the confidence threshold and measure accuracy on\nthe retained samples.\nAt full coverage, the model achieves 85.7% accuracy. By\nrejecting the 20% least confident samples, accuracy rises\nto 91.6%; at 60% coverage, it reaches 95.6%; and at 40%\ncoverage, it climbs to 97.7%. The area under the RAC\n(AURAC = 0.8753) quantifies this strong alignment between\ndetector confidence and correctness.\nThis capability supports the LLM router architecture de-\nscribed in Figure 1: queries flagged as high-risk can be\nrouted to more reliable pipelines, while confident queries\nproceed without additional latency. The steep accuracy\ngains at moderate rejection rates suggest that even conserva-\ntive abstention thresholds can substantially improve system\nreliability.\n5. Ablation Studies\nTo understand the design and limitations underlying our\nhallucination detector, we conduct several ablation studies\nanalyzing how different component affect performance. In\nparticular, we explore (1) which transformer layers provide\nthe most informative representations for hallucination de-\ntection, (2) how model scale influences both classification\nseparability and ranking quality under selective deployment,\n(3) how different detector architectures and representation\naggregation strategies impact performance, and (4) whether\nwe can use a detector trained on the question and answer\nto outperform a detector trained on only the question, dur-\ning generation for parallel evaluation. Unless otherwise\nstated, all ablations are performed using the Qwen family\nof models.\n5.1. Effect of Intermediate Layers on Performance\nRecent work has shown that different layers in transform-\ners encode qualitatively different information: early layers\ncapture surface-level features, while deeper layers encode\nhigher-level semantics and factual knowledge (Mir, 2025;\nNi et al., 2025). We hypothesize that intermediate layers,\nwhich balance low-level token representations with high-\nlevel semantic understanding, may provide the strongest\nsignals for hallucination detection. To test this, we train sep-\narate detectors on hidden states extracted from each layer\nand compare their performance.\n6\nHALT: Hallucination Assessment via Latent Testing\nFigure 4. AUROC and AURAC across layers 0\u201328 for Qwen2.5-7B\non Webquestions. Both metrics peak around layer 20, suggesting\nthat intermediate-to-late layers encode the most informative signals\nfor uncertainty detection.\nFigure 4 shows AUROC and AURAC across all 29 layers\nof Qwen2.5-7B. Both metrics increase monotonically from\nearly layers and peak around layer 20, achieving AUROC\nof 0.877 and AURAC of 0.643. Performance then gradu-\nally declines in the final layers. This pattern suggests that\nintermediate-to-late layers, which are believed to encode\nhigher-level semantic and factual information, provide the\nstrongest signals for detecting hallucinations. We observe\nsimilar trends for LLaMA-2-7B and Gemma-3-4B (see Sec-\ntion C in the Appendix).\n5.2. Effect of Model Size on Ranking Quality\nFigure 5. Effect of model scale (7B, 14B, 32B) on AUROC and\nAURAC. While AUROC remains largely stable across scales, AU-\nRAC increases consistently, indicating improved ranking quality\nunder selective deployment for larger models.\nWe study the effect of model scale on selective deployment\nperformance by evaluating Qwen2.5 models at 7B, 14B,\nand 32B parameters on the same question sets. As shown in\nFigure 5, AUROC remains largely stable across scales on\nboth WebQuestions and MMLU-Pro, indicating that overall\nclassification separability changes little with model size. In\ncontrast, AURAC improves consistently with scale, reflect-\ning substantially better ranking quality for selective predic-\ntion. The improvement is most pronounced on MMLU-Pro,\nwhere the 32B model achieves a 12.7-point AURAC gain\nover the 7B model. This trend indicates that larger mod-\nels produce confidence scores that are better aligned with\ncorrectness, enabling more effective filtering of unreliable\nresponses under selective deployment.\n5.3. Effect of Detector Architecture\nFigure 6.\nComparison of detector architectures on TriviaQA\n(Qwen2.5-7B, layer 19). Among MLP variants, last-token pool-\ning achieves the highest AUROC (93.2%), outperforming mean\npooling (92.1%) and max pooling (90.9%). The transformer archi-\ntecture achieves the best AUROC (93.95%) with attention pooling.\nWe compare the two detector architectures described in Sec-\ntion 3: an MLP with input-stage pooling and a lightweight\ntransformer with attention pooling (Table 2). For the MLP,\nwe further compare three pooling strategies: last token,\nmean pooling, and max pooling. All experiments use hid-\nden states from layer 19 of Qwen2.5-7B on TriviaQA.\nAs shown in Figure 6, last-token pooling consistently outper-\nforms mean and max pooling across all metrics, achieving\n93.2% AUROC compared to 92.1% and 90.9%, respec-\ntively. The transformer architecture with attention pooling\nachieves the highest AUROC (93.95%), slightly outperform-\ning the best MLP variant. Based on these results, we use the\ntransformer architecture with attention pooling in our main\nexperiments.\n5.4. Effect of Number of Answer Tokens Used\nOur main experiments compare two settings: using only\nquestion tokens versus using question plus all answer tokens.\nThis ablation fills the gap between these two extremes by\nprogressively increasing the proportion of answer tokens.\nSpecifically, we train detectors using question tokens plus\nthe first X% of answer tokens, where X ranges from 5%\nto 100%. Figure 7 shows the results on TriviaQA using\nQwen-2.5-7B.\nBoth AUROC and AURAC increase monotonically with the\ntoken threshold. At 5% answer tokens, the detector achieves\n82.2% AUROC; at 100%, this rises to 87.1%. AURAC fol-\nlows a similar trend, increasing from 0.628 to 0.655. These\nresults confirm that answer tokens provide additional signal\n7\nHALT: Hallucination Assessment via Latent Testing\nFigure 7. Effect of answer token inclusion on detection perfor-\nmance. We vary the percentage of answer tokens used (5%\u2013100%)\nalongside all question tokens. Both AUROC and AURAC increase\nmonotonically with more answer tokens.\nfor hallucination detection, and this signal accumulates as\nmore tokens are included.\n6. Discussion\nLimitations\nOur\nexperiments\nfocus\non\nsingle-turn\nquestion-answering tasks with relatively short contexts. In\nmulti-turn dialogues or long-context scenarios, the inter-\nmediate representations of the current question may be in-\nfluenced by the preceding conversation history, potentially\ndegrading detector performance. Adapting our method to\nsuch settings may require collecting training data that re-\nflects these longer contexts, and employing compression\nor sliding window strategies to select informative residual\nrepresentations for training and inference. However, we\nemphasize that the length of the generated answer does not\naffect our method, since we are able to achieve superior\nresults in most settings depending only on question token\nrepresentations, and can make predictions before answer\ntokens are produced.\nScaling Trends\nIn Section 5.2 (Figure 5), we find that our\nmethod performs better when applied to larger models. A\nsimilar phenomenon has been reported in prior work (Du\net al., 2024; Kadavath et al., 2022). We therefore conjecture\nthat our method generalizes to stronger, larger LLMs. One\npossible explanation is that larger models have greater ca-\npacity and may perform richer latent computation beyond\nnext-token prediction. As a result, the final text can be\nless faithful to the intermediate state. This can increase\nthe gap between intermediate representations and surface\noutputs, making intermediate features more informative for\nour detector thereby improving its effectiveness on larger\nmodels.\nFuture Directions\nBeyond hallucination detection and\nselective prediction, our method may benefit reinforcement\nlearning for LLMs. In tree search algorithms such as MCTS,\nthe uncertainty signal from our detector could guide explo-\nration: branches where the model is uncertain warrant more\nexploration, while confident branches can be pruned early.\nThis could help prevent the model from converging to local\nminima driven by overconfident but incorrect predictions,\nleading to more robust policy optimization.\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, pp. 1533\u20131544, 2013.\nURL https://aclanthology.org/D13-1160.\nB\u00a8urger, L., Hamprecht, F. A., and Nadler, B. Truth is uni-\nversal: Robust detection of lies in llms, 2024.\nURL\nhttps://arxiv.org/abs/2407.12831.\nChen, C., Liu, K., Chen, Z., Gu, Y., Wu, Y., Tao, M., Fu,\nZ., and Ye, J. Inside: Llms\u2019 internal states retain the\npower of hallucination detection, 2024. URL https:\n//arxiv.org/abs/2402.03744.\nChen, Y., Benton, J., Radhakrishnan, A., Uesato, J., Deni-\nson, C., Schulman, J., Somani, A., Hase, P., Wagner, M.,\nRoger, F., Mikulik, V., Bowman, S. R., Leike, J., Kaplan,\nJ., and Perez, E. Reasoning models don\u2019t always say what\nthey think, 2025. URL https://arxiv.org/abs/\n2505.05410.\nComanici, G., Bieber, E., Schaekermann, M., Pasupat, I.,\nSachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang,\nD., Rosen, E., Marris, L., Petulla, S., et al.\nGemini\n2.5: Pushing the frontier with advanced reasoning, mul-\ntimodality, long context, and next generation agentic ca-\npabilities, 2025. URL https://arxiv.org/abs/\n2507.06261.\nDu, X., Xiao, C., and Li, Y. Haloscope: Harnessing unla-\nbeled llm generations for hallucination detection, 2024.\nFarquhar, S., Kossen, J., Kuhn, L., and Gal, Y. Detecting\nhallucinations in large language models using semantic\nentropy. Nature, 630(8017):625\u2013630, 2024. ISSN 1476-\n4687. doi: 10.1038/s41586-024-07421-0. URL https:\n//doi.org/10.1038/s41586-024-07421-0.\nHu, J., Tu, G., Cheng, S., Li, J., Wang, J., Chen, R., Zhou, Z.,\nand Shan, D. Harp: Hallucination detection via reasoning\nsubspace projection, 2025.\n8\nHALT: Hallucination Assessment via Latent Testing\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L.\nTriviaqa: A large scale distantly supervised challenge\ndataset for reading comprehension, 2017. URL https:\n//arxiv.org/abs/1705.03551.\nKadavath, S., Conerly, T., Askell, A., Henighan, T., Drain,\nD., Perez, E., Schiefer, N., Hatfield-Dodds, Z., DasSarma,\nN., Tran-Johnson, E., Johnston, S., El-Showk, S., Jones,\nA., Elhage, N., Hume, T., Chen, A., Bai, Y., Bowman,\nS., Fort, S., Ganguli, D., Hernandez, D., Jacobson, J.,\nKernion, J., Kravec, S., Lovitt, L., Ndousse, K., Olsson,\nC., Ringer, S., Amodei, D., Brown, T., Clark, J., Joseph,\nN., Mann, B., McCandlish, S., Olah, C., and Kaplan, J.\nLanguage models (mostly) know what they know, 2022.\nURL https://arxiv.org/abs/2207.05221.\nKuhn, L., Gal, Y., and Farquhar, S.\nSemantic uncer-\ntainty: Linguistic invariances for uncertainty estimation\nin natural language generation, 2023.\nURL https:\n//arxiv.org/abs/2302.09664.\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M.,\nParikh, A. P., Alberti, C., Epstein, D., Polosukhin, I., De-\nvlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M.,\nChang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and\nPetrov, S. Natural questions: A benchmark for ques-\ntion answering research. Transactions of the Association\nfor Computational Linguistics, 7:452\u2013466, 2019. URL\nhttps://aclanthology.org/Q19-1026.\nLin, Z., Trivedi, S., and Sun, J. Generating with confi-\ndence: Uncertainty quantification for black-box large\nlanguage models, 2024. URL https://arxiv.org/\nabs/2305.19187.\nLindsey, J., Gurnee, W., Ameisen, E., Chen, B., Pearce,\nA., Turner, N. L., Citro, C., Abrahams, D., Carter,\nS., Hosmer, B., Marcus, J., Sklar, M., Templeton, A.,\nBricken, T., McDougall, C., Cunningham, H., Henighan,\nT., Jermyn, A., Jones, A., Persic, A., Qi, Z., Thompson,\nT. B., Zimmerman, S., Rivoire, K., Conerly, T., Olah, C.,\nand Batson, J. On the biology of a large language model.\nhttps://transformer-circuits.pub/\n2025/attribution-graphs/biology.html,\n2025. Accessed: 2025-10-07; mechanistic interpretability\nstudy of Claude 3.5 Haiku.\nManakul, P."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper \"HALT: Hallucination Assessment via Latent Testing\"\n\n#### Main Contributions:\nThe paper proposes a lightweight hallucination detection method that leverages intermediate hidden representations from large language models (LLMs) to assess the likelihood of model-generated responses being false or unsupported by facts. The key contributions are:\n\n1. **Lightweight Residual Probes**: A small auxiliary network designed to directly readout hallucination risk from question tokens' intermediate hidden states.\n2. **Efficient Evaluation**: Can be evaluated in parallel with inference, enabling near-instantaneous hallucination risk estimation without added latency.\n3. **Adaptive Routing**: Allows LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines.\n\n#### Methodology:\nThe authors formulate hallucination detection as a knowledge prediction task and propose lightweight residual probes that readout hallucination risk directly from intermediate hidden states of question tokens. The method is evaluated across four QA benchmarks (TriviaQA, NQ-Open, MMLU-Pro, WebQuestions) using multiple LLM families.\n\n#### Key Findings:\n1. **Performance Across Benchmarks**: Achieves strong AUROC and AURAC scores on all benchmarks.\n2. **Generalization Under Dataset Shift**: Demonstrates strong out-of-distribution generalization across different datasets.\n3. **Interpretability of Intermediate Representations**: Reveals interpretable structure in intermediate representations, suggesting that these layers retain epistemic signals.\n\n#### Conclusions:\nThe method positions fast internal uncertainty readout as a principled foundation for reliable agentic AI by enabling near-instantaneous hallucination risk estimation without added latency. The authors conclude that their approach is effective and generalizable across different LLM families and datasets, providing interpretable intermediate representations for better understanding of model behavior.\n\n### Detailed Breakdown:\n\n#### Problem Definition:\nThe paper aims to detect hallucinations in LLMs by predicting the likelihood of a given response being correct or incorrect based on intermediate hidden states. This approach contrasts with existing methods that rely solely on generated outputs.\n\n#### Methodology Overview:\n1. **Detector Architecture**: The detector uses lightweight residual probes trained on intermediate hidden representations from question tokens.\n2. **Training Data**: Constructed using reference answers for each question, ensuring correctness labels are accurate and robust.\n3. **Evaluation Metrics**: AUROC (Area Under the Receiver Operating Characteristic Curve) is used as the primary evaluation metric.\n\n#### Experiments:\nThe authors conducted experiments across four QA benchmarks with multiple LLM families, including Qwen-2.5, Gemma-3, and LLaMA-2 models. The method consistently outperformed existing baselines on all datasets.\n\n#### Key Findings:\n1. **Performance**: Achieved strong AUROC scores (up to 93.95) across different benchmarks.\n2. **Generalization**: Demonstrated strong out-of-distribution generalization, indicating that the intermediate representations are robust and informative.\n3. **Interpretability**: Intermediate layers provide interpretable signals for hallucination detection.\n\n#### Limitations:\n1. **Context Dependency**: The method may be less effective in multi-turn dialogues or long-context scenarios due to potential influence from preceding conversation history.\n2. **Model Size Impact**: Larger models generally perform better, suggesting that larger LLMs produce more reliable confidence scores.\n\n### Conclusion:\nThe paper presents a novel and effective approach for hallucination detection using intermediate hidden representations. The method is lightweight, efficient, and capable of providing interpretable insights into model behavior, positioning it as a foundational technique for reliable agentic AI systems."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nHuman Simulation Computation: A Human-Inspired\nFramework for Adaptive AI Systems\nHong Su\nAbstract\u2014Large language models (LLMs) have demonstrated\nstrong capabilities in knowledge representation and reasoning\nbased on textual data. However, their reliance on language\nmaterial alone limits their ability to adapt, verify reasoning\noutcomes, and operate effectively in open and dynamic real-\nworld environments. In this paper, we propose Human Sim-\nulation Computation (HSC), a human-inspired computational\nframework that models intelligence as a continuous, closed-\nloop process involving thinking, action, learning, reflection,\nand activity scheduling, collectively referred to as the internal\nreasoning process. HSC emphasizes active participation both\nwithin the internal reasoning process and in interactions with the\nenvironment, where actions are used not only to achieve goals\nbut also to automatically refine and improve internal reasoning\nmechanisms without external intervention. Furthermore, HSC\nincorporates commonly used human thinking strategies across\nall stages of the internal reasoning process, such as main-feature-\noriented reasoning, scope expansion through action, and on-time\nlearning driven by environmental feedback. Through theoretical\nanalysis, we argue that human simulation strategies cannot be\nfully learned from language material alone, and that human-\nlike reasoning processes and action-grounded reasoning methods\nare essential for robust adaptation and effective interaction with\nreal-world environments.\nIndex Terms\u2014Human Simulation Computation; Environment\nInteraction; Adaptive Artificial Intelligence; Human-Inspired\nReasoning\nI. INTRODUCTION\nHumans interact with the world through an integrated pro-\ncess of thinking, goal formation, action execution, learning,\nand reflection. This process enables individuals to adapt to\ncomplex and changing environments, rather than merely solv-\ning isolated, predefined tasks. The environment itself is not\nlimited to the physical world; it also includes other intelligent\nagents, artificial systems, and even language-based models that\nparticipate in interaction and feedback.\nRecent advances in large language models (LLMs) [1] have\ndemonstrated impressive capabilities in language understand-\ning, reasoning, and knowledge generation. However, LLMs\nare fundamentally trained on static language corpora. As a\nresult, they do not possess intrinsic mechanisms for goal-\ndriven behavior, environmental interaction, or action-based\nverification. Their reasoning remains largely detached from\nthe real world, relying on linguistic consistency rather than\ngrounded feedback. Consequently, LLMs alone cannot fully\nH. Su is with the School of Computer Science, Chengdu University of\nInformation Technology, Chengdu, China.\nE-mail: suguest@126.com.\nreproduce the adaptive processes that humans use to live and\noperate effectively in dynamic environments.\nThis observation raises a fundamental question: Can we\nsimulate the way humans live and adapt\u2014through thinking,\nlearning, action, reflection, and feedback\u2014so that AI systems\ncan improve through direct interaction with their environ-\nments rather than through language alone? Moreover, unlike\ntask-oriented systems designed for narrow objectives, both\nhumans and intelligent agents must continuously adapt to\nnew situations, uncertainties, and long-term goals. An AI\nsystem that aims to operate robustly in real environments\n[2] must therefore go beyond task execution and develop the\ncapability to \u201clive better\u201d through ongoing interaction and self-\nimprovement.\nTo address this challenge, we introduce Human Simulation\nComputation (HSC), a human-inspired framework that models\nintelligence as a systematic, interactive, and continuous pro-\ncess. HSC consists of two tightly coupled aspects. The first\nis a simulation procedure that integrates thinking, learning,\nreflection, action, and feedback acquisition into a closed loop.\nIn this loop, thinking guides decision making; learning records\noutcomes [3] and internal processes in an on-time manner;\nreflection evaluates and updates all stages; and actions actively\ninfluence both the internal cognitive process and the external\nenvironment. Unlike passive observation, actions in HSC are\nused to accelerate feedback, verify reasoning, and promote\nlearning, including the learning of action sequences that reduce\nfuture computational cost.\nThe second aspect of HSC is the incorporation of commonly\nused human thinking strategies throughout the simulation pro-\ncess. Prior work has shown that introducing specific reasoning\npatterns, such as chain-of-thought prompting, can improve\nLLM performance on individual problems. However, such\napproaches are typically problem-specific and lack a unified,\nsystematic structure. In contrast, HSC embeds human-inspired\nthinking strategies\u2014such as difference detection, scope ex-\npansion, candidate comparison, and reflective reasoning\u2014into\neach stage of the simulation loop, enabling consistent adapta-\ntion across tasks and environments.\nA key feature of HSC is that learning is not restricted to\nexternal language data. Instead, the system learns from every-\nthing it processes or senses, including thinking paths, action\nsequences, reflection outcomes, and environmental feedback\nover time. Actions play a particularly important role: they\nnot only affect the external world, but also modify internal\nprocesses by promoting learning, triggering reflection, and\nvalidating reasoning through grounded feedback. In this sense,\nactions are not predefined task operators, but mechanisms for\narXiv:2601.13887v1  [cs.AI]  20 Jan 2026\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\nachieving better long-term adaptation to the environment.\nContributions. The main contributions of this paper are\nsummarized as follows:\n\u2022 We propose Human Simulation Computation, a frame-\nwork that simulates the full human interaction process\nwith the environment\u2014including thinking, learning, re-\nflection, action, and feedback\u2014with the objective of long-\nterm adaptation rather than solving isolated predefined\ntasks.\n\u2022 We identify and formalize commonly used human think-\ning strategies and integrate them into each stage of\nthe human simulation process, enabling AI systems to\nexhibit human-like adaptive behavior in early stages of\ndeployment.\n\u2022 We emphasize the systematic interaction among thinking,\nlearning, reflection, and action, and introduce an activity\ncontrol mechanism that schedules these processes, in-\ncluding background learning and reflection during idle\nperiods.\n\u2022 We demonstrate the importance of action as an ac-\ntive driver of learning and reasoning, enabling on-\ntime data acquisition, internal process optimization, and\nenvironment-grounded verification without reliance on\nexternal intervention.\nThe remainder of this paper is organized as follows. Sec-\ntion II reviews related work. Section III introduces the overall\nframework of Human Simulation Computation for AI systems.\nSection IV presents human simulation thinking as the cognitive\nfoundation of the proposed model. Section V describes the\nhuman simulation activity scheduling mechanism. Section VI\ndiscusses human simulation reflection and retrospection. Sec-\ntion VII elaborates on human simulation learning. Section VIII\nexplains action mechanisms for interacting with the environ-\nment and other AI systems. Section IX provides theoretical\nanalysis and proofs. Finally, we conclude the paper.\nII. RELATED WORK\nThis work relates to several research directions, includ-\ning large language models, reasoning-enhanced prompting\nmethods, embodied and interactive AI, and human-inspired\ncognitive architectures. We review these areas and clarify\nhow Human Simulation Computation (HSC) differs from and\nextends existing approaches.\nA. Large Language Models\nLarge language models (LLMs) have achieved remarkable\nsuccess in natural language understanding, generation, and\nreasoning by learning from large-scale text corpora [1, 4].\nRecent models demonstrate strong performance on a wide\nrange of tasks, including question answering, code generation\n[5], and mathematical reasoning.\nHowever, LLMs are primarily trained on static language\ndata and operate by predicting token sequences conditioned on\ntextual context. As a result, LLMs lack intrinsic mechanisms\nfor goal formation, environment interaction, and action-based\nverification. Their reasoning processes are evaluated mainly\nthrough linguistic consistency rather than grounded feedback\nfrom the real world [6]. Although LLMs can describe actions\nor simulate interaction symbolically, they do not directly\nexperience the consequences of those actions.\nB. Reasoning-Enhanced Prompting and Chain-of-Thought\nTo improve reasoning capability, several studies have in-\ntroduced prompting strategies such as chain-of-thought (CoT)\n[7], self-consistency, and reflection-based reasoning [8]. These\nmethods guide LLMs to generate intermediate reasoning steps,\nwhich can improve performance on complex tasks.\nWhile effective, these approaches remain fundamentally\nlanguage-driven. They focus on improving reasoning within\na single inference episode or a single problem instance. The\nreasoning process is not systematically linked to long-term\nlearning, environmental interaction, or action outcomes. In\ncontrast, HSC embeds reasoning strategies into a continuous\nloop that includes action execution, feedback acquisition,\nreflection, and on-time learning, enabling persistent adaptation\nbeyond isolated problem solving.\nC. Embodied and Interactive Artificial Intelligence\nEmbodied AI and interactive learning emphasize the im-\nportance of grounding intelligence in perception and ac-\ntion [9, 10]. By interacting with physical or simulated en-\nvironments, agents can acquire experience that cannot be\nobtained from language data alone. Reinforcement learning\nand embodied navigation are representative examples of this\nparadigm [11, 12].\nHowever, many embodied AI systems rely on predefined\nreward functions, fixed task definitions, or narrowly scoped\nenvironments. Their learning objectives are typically task-\ncentric rather than oriented toward long-term adaptive living.\nMoreover, the internal reasoning and reflection processes of\nsuch agents are often implicit or opaque. HSC differs by\nexplicitly modeling thinking, learning, reflection, and action\nas separate but interacting stages, with actions serving not\nonly as task operators but also as tools for verification and\nself-improvement.\nD. Cognitive Architectures and Human-Inspired Models\nCognitive architectures aim to model human cognition by\nintegrating perception, memory, reasoning, and action [13].\nThese systems emphasize structured internal processes and\nlong-term knowledge accumulation. Human-inspired reason-\ning strategies, such as heuristic search and difference-reduction\nmethods, have been studied in classical artificial intelli-\ngence [14].\nHSC builds upon this tradition but differs in two important\naspects. First, it explicitly incorporates modern LLMs as\ncomponents within a broader human simulation framework\nrather than treating symbolic reasoning as the sole intelligence\nmechanism. Second, HSC emphasizes continuous interaction\nwith open environments and on-time learning from both in-\nternal processes and external feedback, rather than operating\nwithin closed or predefined cognitive models.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\nE. Summary and Positioning\nIn summary, existing approaches either focus on language-\nbased reasoning without grounded interaction, or on envi-\nronment interaction without explicit modeling of human-\nlike thinking and reflection. Human Simulation Computation\nbridges this gap by unifying thinking, action, learning, re-\nflection, and scheduling into a systematic, interactive frame-\nwork. By treating adaptation and \u201cliving better\u201d as primary\nobjectives, HSC provides a complementary perspective to both\nLLM-centric reasoning methods and task-oriented embodied\nAI systems.\nIII. HUMAN SIMULATION COMPUTATION OF AI SYSTEM\nHumans are shaped by natural selection; consequently, a\nfundamental capability of a person is to adapt to and live\nwithin natural environments. Following this principle, an AI\nsystem designed for real-world deployment should also be\ncapable of adapting to its environment.\nVarious factors trigger an AI system to perform actions.\nThese factors are referred to as action trigger sources. Action\ntrigger sources include the internal goals of the AI system as\nwell as external environmental feedback, including feedback\nfrom other AI systems.\nLarge Language Models (LLMs) are trained on large-scale\nhuman language data. As a result, they do not directly learn\nhuman thinking processes, but instead learn linguistic patterns\nthat are correlated with thinking. Language is a product of\nthinking, and a single thinking mode may generate language\nexpressions across many scenarios. Therefore, learning lan-\nguage alone does not imply the acquisition of general thinking\nmethods. To enable adaptive intelligence, explicit thinking\nmodes need to be provided to LLM-based systems.\nScope limitation: In this paper, we focus on human simu-\nlation computation that can be realized with the assistance of\nLLMs and programmable systems. Cognitive abilities that are\ndifficult to formalize computationally, such as imagination, are\nbeyond the scope of this work.\nA. Model\nThe human-inspired model aims to enable an AI system to\nlive and adapt in a complex environment, rather than relying on\npredefined actions. The system learns, thinks, and takes actions\nas an active participant in its environment. Accordingly, the\ndesign objective of the AI system is to improve its ability to\noperate within the environment, similar to how humans adapt\nover time.\nBased on this perspective, we propose Human Simula-\ntion Computation (HSC), also referred to as human-inspired\ncomputation. Inspired by human thinking processes, an AI\nsystem under HSC actively interacts with the environment and\nother AI systems. The AI system applies its own thinking to\ndecide when and how to act, with the goal of adapting to\nthe environment rather than completing isolated tasks. Unlike\nconventional systems that function as passive observers, the\nproposed agent operates as an active participant, capable\nof triggering, delaying, or modifying environmental events\nthrough deliberate actions.\nFormally, the HSC process can be described as a closed-loop\nstate transition, as shown in Eq. (1). The internal cognitive\nstate \ud835\udc60\ud835\udc61is first processed by a thinking function T (\u00b7), con-\nditioned on environmental factors \ud835\udc53\ud835\udc61. Based on this cognitive\nassessment, an action is selected via A(\u00b7). The outcomes of\nthe action are then evaluated through a reflection mechanism\nR(\u00b7), and the internal state is subsequently updated by a\nlearning function L(\u00b7), yielding the next state \ud835\udc60\ud835\udc61+1.\n\ud835\udc60\ud835\udc61+1 = L(\ud835\udc60\ud835\udc61, R(\ud835\udc60\ud835\udc61, A(\ud835\udc60\ud835\udc61, T (\ud835\udc60\ud835\udc61, \ud835\udc53\ud835\udc61))))\n(1)\nTo adapt to the environment effectively, humans typically\nperform a recurring process of thinking, learning, action, and\nreflection. This process, referred to as the human simulation\nstages, forms a continuous interaction with the environment.\nHuman Simulation Computation follows the same principle\nand exhibits the following characteristics:\n(1) Thinking-driven decision making: Thinking deter-\nmines whether an action should be taken, how it should be\nexecuted, and when it should occur. For example, the system\nmay decide to reject learning outcomes that are harmful to\nitself.\n(2) Active participation through action: Actions are\ntaken to actively participate in the environment rather than\nremaining a passive observer. Action selection is guided by\nthinking, including decisions on whether to act, how to act,\nand whether to reuse existing methods. Actions are also used to\nacquire information for internal reasoning process (thinking,\nreflection, learning et.al.). Such as obtaining additional data\nor accelerating environmental feedback. Instead of waiting for\nevents to occur naturally, the system may act proactively to\nshorten feedback cycles, as discussed in [2].\n(3) Reflection (Reflectionaction): Reflection is used to\nidentify problems and opportunities for improvement across\nall stages. After actions are executed, the system evaluates\nwhether the outcomes are effective and may reflect on prior\nthinking, actions, and even the reflection process itself.\n(4) Continuous learning: Learning accumulates experience\nfrom all stages of the human simulation process. Not only\noutcomes, but also thinking patterns, action strategies, and\nreflection results are learned and retained over time.\n(5) Information acquisition from multiple sources: Infor-\nmation is obtained from the environment, other AI systems,\nand the system itself. All acquired information is examined\nthrough thinking processes to assess correctness, complete-\nness, and consistency. The environment provides diverse con-\ntexts and challenges, which offer broader training opportuni-\nties and guide the evolution of thinking strategies.\nIn conventional systems, environmental factors directly lead\nto results, without intermediate cognitive intervention:\nfactors \u21d2results.\n(2)\nIn contrast, Human Simulation Computation introduces in-\ntermediate stages of thinking, action, reflection, and learning\nbetween factors and results. Actions are repeatedly interleaved\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\nwith cognitive processing, forming a progressive decision\nprocess:\nfactors\naction\n\u001b\n\u21d2\n\ud835\udc5a1\naction\n\uf8fc\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8fe\n\u21d2\n\ud835\udc5a2\naction\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe\n\u21d2\ud835\udc5a3 \u21d2results.\n(3)\nThis formulation emphasizes that results are not produced\nsolely by external factors, but emerge through multiple rounds\nof cognition and interaction. Each intermediate stage may in-\nfluence subsequent actions and reasoning, allowing the system\nto adapt dynamically rather than responding in a single step.\n1) Using Human Thinking Modes to Guide LLM Rea-\nsoning: Human thinking modes are explicitly provided to\nthe LLM as structured inputs. Since an LLM operates as\na conditional prediction model, injecting human simulation\nthinking methods can guide the LLM toward corresponding\nreasoning patterns. In this way, the LLM is encouraged to\nfollow human-like thinking processes rather than producing\nresponses based solely on surface-level language patterns.\nB. Systematic, Interactive, and On-Time Operation\nThe human simulation process is systematic and forms a\nclosed-loop structure. Learning and action mutually reinforce\neach other: learning guides future actions, while actions are\ndeliberately designed to enhance learning. For example, when\na new method is identified, the system may perform additional\nactions to generate more data, thereby reinforcing memory\nand improving retention. Through this interaction, the AI\nsystem can operate autonomously without continuous external\nintervention.\nIn the following sections, we further analyze the human\nsimulation computation process from two perspectives: adopt-\ning human thinking modes that can evolve with experience,\nand enabling actions that participate actively in the current\nsituation rather than relying on predefined responses.\n1) On-Time Processing: Human simulation processes must\noperate in an on-time manner to adapt to changing environ-\nments. Thinking should respond promptly to sudden changes,\nas future conditions cannot be fully anticipated and solutions\nmay need to be adjusted dynamically. This behavior mirrors\nhuman adaptation to evolving situations.\nLearning should also be performed on time by incorporating\nnewly sensed information into the LLM or explicit recording\nmechanisms. Action sequences learned over time and space,\nwhich can only be acquired through real-world interaction,\nshould be updated continuously.\nActions must likewise be executed in a timely manner.\nDelayed actions may become ineffective if environmental\nconditions change, highlighting the importance of on-time\nthinking, learning, and action for successful adaptation.\nIV. HUMAN SIMULATION THINKING\nHuman simulation thinking is a core component of Human\nSimulation Computation. It aims to simulate human thinking\nprocesses in order to identify ways for an AI system to live\nand operate more effectively in complex environments, based\non encountered problems and environmental characteristics.\nThinking is responsible for analyzing situations and determin-\ning whether to act, how to act, and when to act. It governs both\ninternal cognitive processes and interactions with the external\nenvironment, such as rejecting learning outcomes that may be\nharmful to the system itself.\nThe thinking process is not predefined. Instead, it dynami-\ncally evaluates the environment and identifies the main ques-\ntions or key features relevant to the current situation. Thinking\nguides the entire human simulation process by generating\nsolutions and recommending actions at each stage.\nThe AI system adopts human-like thinking modes to guide\nits interaction with the environment. As a result, the same\nstimulus may lead to different outcomes depending on the\nsystem\u2019s accumulated experience and internal judgment. For\nexample, a red color may be interpreted as dangerous by\na system with prior experience related to injury, while in\nother contexts it may be considered neutral. Similarly, actions\nsuch as moving slowly or quickly are not inherently good or\nbad, but are determined by situational assessment and learned\nexperience.\nA. Goal-Oriented Thinking\nThinking processes are inherently directional and are guided\nby the goals of the AI system. This design is inspired by\nhuman behavior, where actions are not performed for isolated\ntasks, but for continuous self-improvement and adaptation\nwithin the environment. Accordingly, the primary objective of\nthe AI system is to maintain and enhance its ability to exist and\noperate effectively in dynamic surroundings. When optimal\nperformance cannot be achieved, goal-oriented thinking shifts\ntoward minimizing harm or maximizing benefit, either at the\nphysical or cognitive level.\nThinking does not directly request the LLM to generate a\nsolution for a specific task. Instead, the AI system provides\ngoal-oriented constraints and preferences, which are applied\neither before LLM generation or during the selection of gen-\nerated candidates. Through this process, the system chooses\nsolutions that best align with its current goals.\nGoals may differ in scope and priority. They may focus\non task completion, self-benefit, or benefit to others. The AI\nsystem may also assign different levels of emphasis to certain\ngoals, reflecting their relative importance. Such emphasis can\nbe incorporated into LLM prompts to guide generation toward\npreferred outcomes.\n1) Learnable Goals: Goals are not static; they evolve\nthrough experience and reflection. A goal represents a high-\nlevel principle that can be applied across multiple tasks and\nmethods. For example, if an autonomous vehicle initially prior-\nitizes speed and subsequently experiences accidents, reflection\nmay identify excessive speed as a major contributing factor.\nAs a result, the system updates its goal hierarchy to place\ngreater emphasis on safety.\nSeparating goals from task-specific methods enables reuse\nacross different contexts. A goal such as safety can be applied\nto driving, work processes, or interactions with other AI\nsystems. If such principles were embedded independently\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\ninto each detailed task, the resulting learning process would\nbecome redundant and inefficient. By decoupling goals from\nspecific methods, the AI system achieves improved flexibility\nand scalability.\n2) Trigger Sources: Human Simulation Computation is not\nstrictly task-driven; therefore, specific factors are required to\ntrigger the AI system to initiate thinking or action. These\nfactors are referred to as trigger sources.\nGoals serve as primary trigger sources by driving the AI\nsystem to perform systematic actions in pursuit of desired\noutcomes. The overarching goal of living better within the\nenvironment is a fundamental trigger source. This goal can be\nfurther divided into basic maintenance requirements, such as\npreserving system power and avoiding physical or functional\ndamage, which ensure the system\u2019s continued operation.\nBeyond basic survival, higher-level trigger sources focus\non improving quality of operation. These include maintaining\nstable internal states, responding to feedback from other AI\nsystems (e.g., avoiding actions that cause harm to others),\nand considering environmental impact, such as pollution or\ninterference.\nSome goals operate over long time horizons and cannot\nbe satisfied through immediate feedback. Such goals require\nlong-term planning and delayed evaluation. Examples include\ntolerating temporary discomfort for long-term benefit, such as\nexercise-induced pain or the acquired tolerance of bitter tastes.\nB. Common Human Thinking Strategies\nHuman thinking employs a variety of intuitive strategies to\nmanage complex and dynamic environments. These strategies\nare grounded in the way people naturally interact with their\nsurroundings. They can guide AI systems to concentrate\non pertinent information, broaden reasoning when needed,\nand adjust decisions based on accumulated experience. The\nfollowing subsections outline several representative thinking\nstrategies incorporated into Human Simulation Computation.\n1) Main-Feature-Oriented Reasoning: Real-world environ-\nments involve numerous factors, many of which may be\nirrelevant to the problem under consideration. Accordingly,\nthe thinking process focuses on identifying the main features\nor core problems that distinguish the current situation. This\nfocus guides the LLM to concentrate on essential information\nrather than being distracted by unrelated factors.\nThe main feature or question is typically characterized\nby differences from expected or normal conditions. Such\ndifferences can be identified by comparing observed outcomes\nwith expected results, historical behavior, feedback from other\nagents, internal states, or progress toward goals [15].\nWhen the detected difference exceeds a certain threshold,\nthe situation is considered abnormal. In such cases, the system\nshould expand its reasoning scope. As an illustration, consider\na visual cue such as \"1/9/18.\" If the slashes are uniform in\nlength\u2014deviating from conventional fractional notation\u2014the\nsystem must avoid an immediate fractional interpretation. The\nappropriate strategy is to actively query for broader contextual\ninformation to resolve the ambiguity before committing to an\ninference.\n2) Wider-Scope Consideration: Effective thinking requires\nconsideration of a sufficiently broad scope. To achieve reliable\nresults, the AI system must evaluate whether the current\nreasoning scope is adequate. If relevant information is missing,\nthe system should explicitly recognize this limitation and take\nactions to acquire additional data.\nScope expansion may involve spatial, temporal, or relational\ndimensions. For example, when deciding the acceleration\nspeed of an autonomous vehicle, the system should consider\nnot only traffic conditions but also the presence of passengers.\nSuch contextual factors cannot always be predefined and\nmust be dynamically verified through interaction with the\nenvironment.\nExpanding the reasoning scope often requires action. The AI\nsystem may actively obtain information from a wider spatial\narea, over a longer time horizon, or from additional sources,\nthereby improving the quality of subsequent decisions.\n3) Continuous Thinking Before Action: Continuous think-\ning refers to repeatedly evaluating a situation over time in\norder to compare alternative methods or incorporate newly\nacquired information. The notion of \u201cthinking multiple times\nbefore action\u201d reflects a common human practice aimed at\nimproving decision quality by considering additional options\nand broader context.\nIn Human Simulation Computation, continuous thinking\nenables the system to revise decisions as new information\nemerges or to select more effective methods based on the\nevolving context. In urgent scenarios, this may involve deliv-\nering an initial prompt response, which is then progressively\nrefined as additional information is obtained.\nFor example, when approaching a red light on a multi-lane\nroad, the immediate decision may be to stop. If subsequent in-\nformation indicates that adjacent lanes are also congested, the\nsystem may determine that changing lanes is unnecessary. This\nillustrates how decisions evolve through continuous thinking\nas more information is acquired.\n4) Generalization and Degeneralization:\nGeneralization\naims to extract more general methods from specific experi-\nences so that they can be applied across different contexts.\nSuch generalization may occur across space, time, or different\nAI systems. For example, if a method is effective in one\nlocation, the system may attempt to apply it in another.\nSimilarly, a method learned in the past may be reused in\nfuture situations, or a strategy effective for one agent may\nbe transferred to others.\nDegeneralization addresses the opposite scenario. When a\ngeneralized method proves ineffective or inappropriate for a\nspecific case, the system identifies the distinguishing features\nof that case and adapts the method accordingly. This process\nrequires recognizing new categories or conditions and mod-\nifying existing methods to fit them. Both generalization and\ndegeneralization rely on prompting the LLM when inconsis-\ntencies or failures are detected.\n5) Oppositional and Holistic Thinking: Human thinking of-\nten considers problems from opposing perspectives or extends\nlocal observations to a broader context. These thinking modes\nhelp prevent premature conclusions and uncover alternative\nsolutions.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\nOppositional thinking [16] involves evaluating both a propo-\nsition and its opposite. For example, when considering whether\nan AI system should proceed along a wet road, the system may\nalso evaluate the alternative of not proceeding and assess the\nconsequences of both options. Such comparison supports more\nbalanced decision making.\nHolistic thinking extends reasoning from a single point\nor partial observation to the entire process or system. If a\nproblem is observed in one part of a spatial or temporal\nsequence, similar issues may exist elsewhere. This mode of\nreasoning enables the system to infer system-level patterns\nfrom localized evidence [17].\n6) Risk-Avoidance and Positive Reframing: In many situa-\ntions, humans proactively consider potential negative outcomes\nin order to avoid serious damage. Accordingly, the AI system\nshould assess possible risks in advance and take preventive\nactions when the consequences may be severe. This risk-\navoidance thinking focuses on identifying what could go\nwrong and reducing the likelihood or impact of failure.\nConversely, when repeated negative outcomes occur and\ncannot be easily changed, humans often adopt positive refram-\ning to maintain stability. For example, the loss of social con-\nnections may initially be perceived as negative, but may also\nbe interpreted as an opportunity to identify more meaningful\nrelationships. Similarly, an AI system can learn to reframe\npersistent failures in a constructive manner, supporting long-\nterm resilience.\nRisk-avoidance and positive reframing function as comple-\nmentary thinking modes, encouraging the system to consider\nboth adverse and favorable aspects of a situation.\n7) Planning Multiple Candidate Actions: Planning multiple\ncandidate actions and anticipating their possible outcomes\nprovides the AI system with greater flexibility in decision\nmaking. By comparing alternative actions in advance, the\nsystem can better evaluate potential benefits and risks before\nexecution.\nThis strategy is particularly useful for preventing irreversible\nfailures. By considering contingency plans and rescue options,\nthe system can avoid committing to actions that offer no op-\nportunity for recovery. As a result, planning multiple candidate\nactions supports safer and more robust behavior in dynamic\nenvironments.\nC. Action as a Participant in Thinking\nIn Human Simulation Computation, action is not merely an\nexecution step but an active participant in the thinking process.\nActions are used not only to change the external environment,\nbut also to support and refine internal reasoning. By interacting\nwith the environment, actions generate feedback that informs\nsubsequent thinking, learning, and reflection.\nThrough this interaction, thinking and action form a tightly\ncoupled process. Thinking guides action selection, while ac-\ntions provide new information that may confirm, challenge, or\nrevise existing assumptions. This bidirectional relationship en-\nables the AI system to adapt its reasoning based on real-world\noutcomes rather than relying solely on internal inference.\n1) Scope Expansion Through Action: As discussed in the\nhuman thinking strategies above, effective reasoning often\nrequires a sufficiently broad scope. When the current scope\nis limited, actions are needed to expand it, such as actively\nacquiring information from a wider spatial or temporal range.\nIn real-world environments, sensing capabilities are often\nconstrained, and it is neither feasible nor efficient to obtain\ncomplete information in advance. Additionally, other AI sys-\ntems may operate without sharing direct physical representa-\ntions of the environment. As a result, thinking based on limited\nscope may lead to suboptimal decisions.\nFor example, when only vehicle dynamics are considered\nin autonomous driving, high speed may appear acceptable.\nHowever, when passengers are present, additional factors such\nas comfort and safety must be considered. Because such\ncontextual factors are dynamic and difficult to predefine,\nthe thinking process should continuously verify whether its\ncurrent scope is sufficient and take actions to expand it when\nnecessary.\n2) Identifying Hidden Objects and Context: When attempt-\ning to solve a problem, it is important to identify not only the\nobservable information, but also the hidden objects and con-\ntextual factors related to the target system. Often, only partial\ninformation is explicitly presented, while critical background\ndetails remain implicit.\nFor example, when diagnosing issues related to accessing a\nself-deployed LLM, a system may attempt to answer the ques-\ntion solely based on the query content. However, if multiple\nLLM instances are deployed on the same machine using the\nsame default port, requests may be routed incorrectly, leading\nto unexpected behavior. Without considering the deployment\ncontext, such issues cannot be effectively resolved, as users\noften provide only the information pertinent to the question\nitself."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems\"\n\n#### Main Contributions:\n1. **Proposed Model**: The paper introduces Human Simulation Computation (HSC), a framework that simulates human interaction with environments, aiming to enable adaptive intelligence rather than solving isolated tasks.\n2. **Incorporation of Thinking Strategies**: HSC embeds common human thinking strategies across all stages of the internal reasoning process, including main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback.\n3. **Systematic Interaction**: The model emphasizes systematic interaction among thinking, learning, reflection, and action, incorporating an activity control mechanism that schedules these processes during idle periods.\n\n#### Methodology:\n1. **Closed-Loop Process**: HSC models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling.\n2. **Human-Inspired Thinking Modes**: The model uses human-inspired thinking modes such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback.\n\n#### Key Findings:\n1. **Adaptive Intelligence**: HSC enables AI systems to adapt effectively in dynamic environments by simulating the full human interaction process.\n2. **Human-Inspired Strategies**: The incorporation of common human thinking strategies improves AI system's ability to exhibit human-like adaptive behavior early in deployment.\n3. **Systematic Interaction**: The model emphasizes systematic interaction among different cognitive processes, enabling more effective and adaptable AI systems.\n\n#### Conclusions:\n1. **Complementary Perspective**: HSC provides a complementary perspective that bridges the gap between existing approaches focusing on language-based reasoning or environment interaction without explicit modeling of human-like thinking.\n2. **Future Work**: The paper concludes by outlining future work directions, including theoretical analysis and proofs, as well as potential applications in real-world environments.\n\n### Summary:\nThe paper proposes Human Simulation Computation (HSC), a framework designed to enable AI systems to adapt effectively in dynamic environments through simulating human interaction processes. HSC integrates common human thinking strategies across various stages of the internal reasoning process, emphasizing systematic interaction among different cognitive components. The model's key contributions include providing a comprehensive simulation of human adaptation and offering actionable insights for future research and application development."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided\nEnhancement for Few-Shot Learning\nJiaying Wu1 , Can Gao1 , Jinglu Hu2 , Hui Li1 , Xiaofeng Cao3 and Jingcai Guo4\u2217\n1Jiangsu Ocean University, China, 2Waseda University, Japan\n3Tongji University, China, 4The Hong Kong Polytechnic University, Hong Kong SAR\n{jiaying, 2024220911}@jou.edu.cn, jinglu@waseda.jp, lih@jou.edu.cn, xiaofengcao@tongji.edu.cn,\njc-jingcai.guo@polyu.edu.hk\nAbstract\nFew-shot learning aims to identify novel cate-\ngories from only a handful of labeled samples,\nwhere prototypes estimated from scarce data are\noften biased and generalize poorly.\nSemantic-\nbased methods alleviate this by introducing coarse\nclass-level information, but they are mostly ap-\nplied on the support side, leaving query representa-\ntions unchanged. In this paper, we present PMCE,\na Probabilistic few-shot framework that leverages\nMulti-granularity semantics with Caption-guided\nEnhancement. PMCE constructs a nonparametric\nknowledge bank that stores visual statistics for each\ncategory as well as CLIP-encoded class name em-\nbeddings of the base classes. At meta-test time, the\nmost relevant base classes are retrieved based on\nthe similarities of class name embeddings for each\nnovel category.\nThese statistics are then aggre-\ngated into category-specific prior information and\nfused with the support set prototypes via a sim-\nple MAP update. Simultaneously, a frozen BLIP\ncaptioner provides label-free instance-level image\ndescriptions, and a lightweight enhancer trained\non base classes optimizes both support prototypes\nand query features under an inductive protocol\nwith a consistency regularization to stabilize noisy\ncaptions. Experiments on four benchmarks show\nthat PMCE consistently improves over strong base-\nlines, achieving up to 7.71% absolute gain over the\nstrongest semantic competitor on MiniImageNet in\nthe 1-shot setting. Our code is available at https:\n//anonymous.4open.science/r/PMCE-275D.\n1\nIntroduction\nFew-shot learning explores how to recognize novel categories\nfrom a small number of labeled samples. Despite the suc-\ncess of deep networks under large-scale supervision, perfor-\nmance often drops sharply in the few-shot regime, where de-\ncision boundaries can easily overfit a few supports and fail\nto transfer to novel classes. Traditional techniques such as\n\u2217Corresponding author\nFigure 1: Comparison of semantic cues for support and query im-\nages. (a) Class-level semantics only (baseline). The support fea-\nture is modulated by its class name \u201crobin\u201d, while the query remains\npurely visual, leading to a relatively weak alignment between them\n(cosine similarity 0.56). (b) Caption-guided support and query\n(PMCE). BLIP-generated captions provide label-free instance-level\nsemantics for both images, yielding more focused activation maps\nand a stronger support\u2013query alignment (cosine similarity 0.90).\nMatching Networks [Vinyals et al., 2016], Prototypical Net-\nworks [Snell et al., 2017], and MAML [Finn et al., 2017],\ntogether with later strong baselines that revisit pre-training\nand episodic learning [Chen et al., 2019; Tian et al., 2020;\nChen et al., 2021a], have substantially improved the land-\nscape.\nHowever, both benchmark studies and recent re-\nevaluations [Chen et al., 2019; Luo et al., 2023] suggest that\nthe margins over simple baselines are still limited, especially\nfor 1-shot tasks and visually diverse categories.\nTo compensate for the scarcity of labeled samples,\nsemantic-based few-shot learning has become increasingly\ninfluential.\nThese methods supplement visual features by\nintroducing auxiliary information such as attributes, class\nnames, human descriptions, and embeddings from visual lan-\nguage models into representations and classifiers [Xing et al.,\n2019; Radford et al., 2021; Chen et al., 2023b; Zhang et al.,\n2024a; Zhou et al., 2025]. For example, attribute-based and\ncross-modal approaches adapt visual prototypes with class-\nlevel semantics [Xing et al., 2019]. More recently, CLIP-\ndriven prompt-based pipelines encode class names with tem-\nplates and align them with image features [Radford et al.,\n2021; Chen et al., 2023b; Zhang et al., 2024a; Chen et al.,\n2026; Li and Wang, 2023; Dong et al., 2025]. It is notewor-\nthy that even lightweight methods using semantics can form\narXiv:2601.14111v1  [cs.CV]  20 Jan 2026\nstrong and robust baselines [Zhou et al., 2025], which mo-\ntivates us to consider how semantics should be leveraged to\novercome the remaining bottlenecks.\nThis design choice matters most in the 1-shot scenarios,\nwhere prototypes are easily influenced by a single support\nexample and query features often dominate the residual un-\ncertainty. On one hand, distribution-based approaches, such\nas distribution calibration and MAP-style (Maximum A Pos-\nteriori) prototype correction [Yang et al., 2021; Wang et al.,\n2023a; Wu et al., 2021; Wu and Hu, 2022], reuse base-\nclass statistics to stabilize novel-class estimation. While ef-\nfective, their priors are often chosen in the visual space via\nnearest neighbor search or manually written rules, rather\nthan based on semantic associations between classes.\nOn\nthe other hand, most semantic-based approaches mainly rely\non class-level text and inject it into support prototypes or\nclassifier weights [Xing et al., 2019; Chen et al., 2023b;\nZhang et al., 2024a], whereas query features are rarely up-\ndated when the model cannot rely on query-set statistics.\nTherefore, even with strong semantic baselines, performance\ncan be limited by the mismatch between noisy supporting\nprototypes and purely visual queries.\nIn this work, we present PMCE, a few-shot framework\nthat uses multi-granularity semantics to address the prob-\nlems of biased prototypes and weak query representations\nin 1-shot recognition. PMCE assigns different roles to dif-\nferent semantics. Class-name embeddings are used to iden-\ntify semantically relevant base categories and turn their statis-\ntics into class-specific priors for prototype calibration, while\nlabel-free captions provide instance-level cues that refine both\nsupport-side prototypes and query features in the same rep-\nresentation space.\nWe train only a lightweight enhance-\nment module on base classes and keep the backbone and\nvision\u2013language encoders frozen, so improvements mainly\ncome from how semantics are injected rather than from in-\ncreasing model capacity.\nExtensive experiments on four\nbenchmarks with two backbones validate the effectiveness of\nthis design, especially in the 1-shot setting. Our contributions\nare summarized as follows:\n\u2022 We propose a PMCE method that combines semantic-\nbased prior calibration with description-based image\nrepresentation refinement without requiring adaptation\nusing query set statistics.\n\u2022 We leverage class-name semantics to obtain class-\nspecific prior information and enhance the support\nset and query representation with unlabeled descrip-\ntions, thereby constructing a more consistent space for\nprototype-based inference.\n\u2022 We introduce a lightweight consistency regularizer and\nachieve consistent improvements across four bench-\nmarks using two backbone networks, with the most sig-\nnificant improvement on the 1-shot tasks.\n2\nRelated Work\nIn this section, we present three main streams for existing\nrepresentative few-shot learning approaches and followed by\nseveral methods based on Vision\u2013language models.\nMetric- and distribution-based few-shot learning. Few-\nshot learning has been extensively explored with techniques\nlike transferable metric spaces and meta-learned initializa-\ntion. These methods include Matching Networks [Vinyals\net al., 2016], Prototypical Networks [Snell et al., 2017],\nMAML [Finn et al., 2017], and later strong baselines that\nrevisit pre-training and episodic learning [Chen et al., 2019;\nTian et al., 2020; Chen et al., 2021a].\nRecent work fur-\nther improves representations and distance functions, such as\nmethod FGFL with frequency guidance [Cheng et al., 2023]\nand CPEA with class-aware patch embeddings [Hao et al.,\n2023]. Another line models feature distributions and trans-\nfers base-class statistics to novel classes, including distribu-\ntion calibration [Yang et al., 2022b] and MAP-style prototype\nestimation [Wu et al., 2021; Wu and Hu, 2022]. While effec-\ntive, they often rely on visual similarities and heuristic se-\nlections in the feature space. PMCE follows this calibration\nperspective, but selects prior candidates with CLIP-encoded\nclass-name semantics and incorporates it with caption-guided\nfeature refinement.\nSemantic-based few-shot learning.\nSemantic-based\nmethods incorporate semantics like attributes, word vec-\ntors, or text embeddings to compensate for limited visual\nexamples [Chen et al., 2021b; Chen et al., 2025c; Chen\net al., 2023d; Chen et al., 2023c; Guo and Guo, 2020;\nGuo et al., 2023; Guo et al., 2024c; Guo et al., 2021;\nLu et al., 2023; Guo et al., 2024b; Guo et al., 2024a]. For ex-\nample, AM3 [Xing et al., 2019] adapts visual prototypes with\nclass-level semantics. Based on CLIP [Radford et al., 2021],\nBMI [Li and Wang, 2023], SP-CLIP [Chen et al., 2023b],\nSemFew [Zhang et al., 2024a] and PRSN [Dong et al., 2025]\nencode class names with prompts and inject text features into\nmetric pipelines or prototype optimization. Most approaches\nremain class-centric and mainly intervene on support pro-\ntotypes or classifier weights, while the query representation\nis often left unchanged or only processed through transduc-\ntion clues. In PMCE, class-name semantics guide prior se-\nlection, and captions provide instance-level cues to optimize\nboth support and query streams in the same metric space.\nVision\u2013language models.\nVision\u2013language pre-training\nprovides rich textual signals for visual recognition.\nCLIP\naligns images and text from large-scale image\u2013caption\npairs [Radford et al., 2021], and BLIP further improves de-\nscription generation and integrates visual language represen-\ntations. [Li et al., 2022]. Recently, several few-shot work\nuse CLIP textual features to represent category names or\nmanually designed prompts [Chen et al., 2023b; Zhang et\nal., 2024a], while automatic description generation has not\nbeen extensively explored under standard inductive evalua-\ntion. PMCE leverages BLIP-generated captions as label-free\ninstance semantics and combines them with class-level CLIP\nsemantics, enabling semantics-guided MAP calibration and\ncaption-guided enhancement without using the joint distribu-\ntion of unlabeled queries.\n3\nPreliminaries\nIn this section, we explain the few-shot setting and review the\nMAP-based prototype calibration that our method builds on.\nBLIP\na small bird perched\non a branch in the\nbush.\nTraining\na dog sitting on a wo\noden bench looking\nat the camera.\nTest\nQuery\nSupport\nText\nEncoder\n\u01cb\nClassifier\n\u01ddquery\nVision\nEncoder\n\u01ca\nText\nEncoder\ng\nFeature Enhancer \u0629\nCross-Attention\nK,V\nQ\nEnhanced  Visual\nFeatures z\nz\nprototype\nPull z to center\n\u2112rec\n\u2112cls\nBLIP\nVision\nEncoder\n\u01ca\ncosine distance\n\u201ca photo of golden retriever\u201d\n\u01bd\ud450\ud459\u01c5\u01d7\u01d7\n\u01bd\u0561\u055d\u05a5\u056d\n\u0581\ncosine\ndistance\nTop-\u01b5 priors\nFeature Enhancer \u0629\nCross-Attention\nMAP Rectified\nPrototype \u01d4\nQ\nK,V\nBLIP\na dog is sitting on a\ntable with a plate.\nVision\nEncoder\n\u01ca\nRaw Query\nFeature \u01de\u01d5\nBase\nKnowledge\nBank \u212c\nText\nEncoder\n\u01cb\nFeature Enhancer \u0629\nCross-Attention\nQ\nK,V\nEnhanced\nSupport Prototype\nEnhanced Query\nEmbedding\nTrainable Parameters\nFrozen Parameters\nLoss\nAddition Operation\nFigure 2: Overall framework of PMCE. Training stage: On base classes, we build a non-parametric knowledge bank that stores class-wise\nvisual statistics and class-name embeddings, and train a caption-guided enhancer \u03a6 by fusing visual features from f with BLIP-generated\ncaption semantics from g. Test stage: (1) extract visual features and label-free captions for both supports and queries; (2) use class-name\nsemantics to retrieve related base classes from the knowledge bank and obtain a class-specific prior, which is fused with the support prototype\nin a MAP-style update to get \u02dcp; (3) apply \u03a6 to enhance \u02dcp with aggregated support captions and to enhance query features with their own\ncaptions, and perform metric-based classification in the aligned feature space.\n3.1\nProblem Setting\nIn the standard few-shot setting, we have a base set\nDb = {(xi, yi)}|Db|\ni=1\n\u2282X \u00d7 Cb and a novel set Dn =\n{(xi, yi)}|Dn|\ni=1 \u2282X \u00d7 Cn, where Cb \u2229Cn = \u2205. Db provides\nabundant annotated data needed to train the visual backbone\nand the enhancement module, while Dn is used only at meta-\ntest time.\nAt meta-test time, we evaluate the performance on the N-\nway K-shot tasks. Each task contains a support set S =\n{(xi, yi)}N\u00d7K\ni=1\nand a query set Q = {(xj, yj)}N\u00d7M\nj=1\n, where\nM is the number of query samples per class. Given support\nset S and a query image xq, the goal is to predict its label by\n\u02c6yq = arg max\nc\u2208C p(c | xq, S, Db),\n(1)\nwhere C denotes the set of N classes sampled in the current\nepisode.\n3.2\nMAP-based Prototype Calibration\nFollowing [Wu et al., 2021; Wu and Hu, 2022], we construct\na visual knowledge bank based on the priors from base set.\nSpecifically, for each base class j \u2208Cb, we store its mean\nfeature\n\u00b5j =\n1\n|Dj\nb|\nX\n(xi,yi)\u2208Dj\nb\nf\u03b8(xi),\n(2)\nwhere Dj\nb = {(xi, yi) \u2208Db | yi = j}.\nFor a novel class c in an episode, pc\ninit is the empirical proto-\ntype calculated from the mean of its support features. Given a\nclass-specific prior mean \u00b5c\nprior obtained from the knowledge\nbank, we calibrate the prototype by a MAP estimator under\nthe Gaussian assumptions, which admits a closed-form con-\nvex combination:\n\u02dcpc = \u03b1 pc\ninit + (1 \u2212\u03b1) \u00b5c\nprior,\n(3)\nwhere \u03b1 \u2208[0, 1] controls the strength of the prior.\n4\nMethod\nIn this section, we present the semantics-guides prior selec-\ntion strategy, the caption-guided enhancer, and the training\nobjective in PMCE, the overall framework is shown in Fig. 2.\n4.1\nSemantics-Guided Prior Selection\nWe demonstrate the prior selection with CLIP class-name se-\nmantics for MAP prototype calibration in Eq. (3).\nClass-level semantics on base classes. Let ny be the nat-\nural language name of class y. We follow the CLIP-style\nprompting strategy and encode\nsy\nclass = g\u03d5(\u201ca photo of a ny\u201d) \u2208Rdt,\n(4)\nwhich provides a coarse description of category y. For each\nbase class j, we store its mean feature \u00b5j and class-name em-\nbedding sj\nbase = sj\nclass in a non-parametric semantic knowl-\nedge bank\nB = {(\u00b5j, sj\nbase)}|Cb|\nj=1.\n(5)\nPrior selection for novel classes. At meta-test time, for\na novel class c with class name nc and embedding sc\nclass, we\nmeasure its similarity to each base class via cosine similarity:\nscorej =\nsc\nclass \u00b7 sj\nbase\n\u2225sc\nclass\u2225\u2225sj\nbase\u2225\n.\n(6)\nWe then select the indices of the top-K base classes to obtain\na neighbor set Ntop and aggregate their visual means into a\nclass-specific prior. Instead of uniform averaging, we use a\nsimilarity-weighted aggregation:\nwk =\nexp(scorek/\u03c4)\nP\nu\u2208Ntop exp(scoreu/\u03c4),\n\u00b5c\nprior =\nX\nk\u2208Ntop\nwk\u00b5k,\n(7)\nwhere \u03c4 is a temperature.\nMAP view with spherical uncertainties.\nWe interpret\nthe support prototype pc\ninit as a noisy observation of the\ntrue class mean \u00b5c under a spherical Gaussian likelihood\npc\ninit \u223cN(\u00b5c, \u03c32\nlikeI), while the retrieved prior mean provides\na spherical Gaussian prior p(\u00b5c) = N(\u00b5c\nprior, \u03c32\npriorI). Un-\nder Gaussian\u2013Gaussian conjugacy, the MAP estimate has a\nclosed form that reduces to a convex combination:\n\u02dcpc = \u03b1 pc\ninit + (1 \u2212\u03b1) \u00b5c\nprior,\n\u03b1 =\n\u03c32\nprior\n\u03c32\nprior + \u03c32\nlike\n.\n(8)\nThus, \u03b1 is determined by the relative confidence between the\nsupport evidence and the retrieved semantic prior. In prac-\ntice, we use a fixed (or learnable) scalar \u03b1 for efficiency,\nwhich is consistent with the above MAP interpretation. Sub-\nstituting Eq. (7) into Eq. (8) yields the MAP-calibrated pro-\ntotype \u02dcpc for class c.\nCompared with [Wu et al., 2021;\nWu and Hu, 2022], prior selection in PMCE is driven by\nCLIP-based class-name semantics rather than by visual dis-\ntances or hand-crafted rules.\n4.2\nCaption-Guided Dual-Stream Enhancement\nWe refine representations with instance-level captions by ap-\nplying the same enhancer \u03a6 to the support and query streams.\nGiven a visual feature and its caption-derived semantic repre-\nsentation, \u03a6 outputs an enhanced visual embedding used for\nprototype construction and query matching.\nInstance-level semantics.\nTo provide finer-grained cues\nand cover unlabeled queries, we use image captions as\ninstance-level semantics. For any image x, we first generate\na caption with a frozen BLIP captioner,\nc = BLIP(x),\n(9)\nand then encode it with the same text encoder g\u03d5(\u00b7) to obtain\na single sentence embedding:\nsinst = g\u03d5(c) \u2208Rdt,\n(10)\nwhere dt is the text feature dimension. sinst is label-free and\nthus available for both supports and queries, serving as the\nsemantic input to \u03a6 on the two streams.\nEnhancement module.\nWe define a mapping\nvout = \u03a6(vin, Sin; \u0398),\n(11)\nwhere vin \u2208R1\u00d7dv is a visual anchor (e.g., a prototype or a\nquery feature), Sin \u2208Rdt is a semantic context, and \u0398 are\nlearnable parameters. Since visual and textual features live in\ndifferent spaces, we first project each semantic token to the\nvisual dimension:\nSproj = ReLU(LN(SinWp + bp)) \u2208Rdv,\n(12)\nwith Wp \u2208Rdt\u00d7dv, bp \u2208Rdv and LN(\u00b7) layer normalization.\nWe then perform cross-attention by using the visual anchor\nas the query and the caption tokens as keys/values:\nQatt = vinWQ,\nKatt = SprojWK,\nVatt = SprojWV ,\n(13)\nwhere WQ, WK, WV \u2208Rdv\u00d7dk are learnable projections.\nThe attention output is computed as\nAttn(Qatt, Katt, Vatt) = Softmax\n\u0012QattK\u22a4\natt\n\u221adk\n\u0013\nVatt.\n(14)\nA residual connection with a learnable scale \u03b2 (initialized\nto a small value) preserves the original visual structure:\nvout = vin + \u03b2 Attn(Qatt, Katt, Vatt).\n(15)\nSupport and query streams.\nWe apply \u03a6(\u00b7) symmetrically\nto supports and queries. For class c, let Sc be its support\nset and S(i)\ninst be the caption embedding of support xi \u2208Sc.\nWe average support caption embeddings to form a class-level\nsemantic context:\nsc\nproto =\n1\n|Sc|\nX\nxi\u2208Sc\ns(i)\ninst \u2208Rdt.\n(16)\nWe then take the MAP-calibrated prototype \u02dcpc as the visual\nanchor to obtain the final enhanced prototype:\nPc\nfinal = \u03a6(\u02dcpc, Sc\nproto).\n(17)\nFor a query image xq with visual feature zq = f\u03b8(xq) and its\ncaption embedding Sq\ninst, we compute\nZq\nfinal = \u03a6(zq, Sq\ninst).\n(18)\nEach query is processed independently, so the overall frame-\nwork remains strictly inductive.\n4.3\nTraining Objective with Consistency\nRegularization\nWe freeze f\u03b8 and g\u03d5 and train only the enhancer \u03a6 together\nwith an auxiliary classifier on base classes. Let v(i)\nout be the\nenhanced feature of a training sample xi with label yi.\nClassification and prototype-preserving regularization.\nWe use a standard cross-entropy loss\nLcls = \u22121\nB\nB\nX\ni=1\nlog P(yi | v(i)\nout),\n(19)\nTable 1: Comparison with state-of-the-art few-shot learning methods under the 5-way 1-shot and 5-way 5-shot settings on MiniImageNet\nand TieredImageNet. All results are reported as mean accuracy (%) with 95% confidence intervals. The best performance in each column is\nhighlighted in bold, and \u201c\u2013\u201d indicates that the result is not reported.\nMethods\nBackbone\nSemantic\nMiniImageNet\nTieredImageNet\n1-shot\n5-shot\n1-shot\n5-shot\nMetric-based\nMatchNet [Vinyals et al., 2016]\nResNet-12\nNo\n65.64 \u00b1 0.20\n78.72 \u00b1 0.15\n68.50 \u00b1 0.92\n80.60 \u00b1 0.71\nProtoNet [Snell et al., 2017]\nResNet-12\nNo\n60.37 \u00b1 0.83\n78.02 \u00b1 0.57\n53.31 \u00b1 0.89\n72.69 \u00b1 0.74\nRelationNet [Sung et al., 2018]\nConvNet64\nNo\n52.19 \u00b1 0.83\n70.20 \u00b1 0.66\n54.48 \u00b1 0.93\n71.32 \u00b1 0.78\nSeFeat [Afrasiyabi et al., 2022]\nResNet-12\nNo\n68.32 \u00b1 0.62\n82.71 \u00b1 0.46\n73.63 \u00b1 0.88\n87.59 \u00b1 0.57\nFGFL [Cheng et al., 2023]\nResNet-12\nNo\n69.14 \u00b1 0.80\n86.01 \u00b1 0.62\n73.21 \u00b1 0.88\n87.21 \u00b1 0.61\nCPEA [Hao et al., 2023]\nViT-S\nNo\n71.97 \u00b1 0.65\n87.06 \u00b1 0.38\n76.93 \u00b1 0.70\n90.12 \u00b1 0.45\nMCNet [Chen et al., 2024]\nResNet-18\nNo\n72.13 \u00b1 0.43\n88.20 \u00b1 0.23\n73.14 \u00b1 0.46\n87.16 \u00b1 0.32\nCads [Zhang et al., 2025]\nResNet-12\nNo\n66.56 \u00b1 0.19\n82.74 \u00b1 0.13\n72.04 \u00b1 0.22\n86.47 \u00b1 0.15\nSRE-ProtoNet* [Chen et al., 2025b]\nResNet-12\nNo\n70.86 \u00b1 0.44\n87.01 \u00b1 0.27\n73.02 \u00b1 0.50\n87.78 \u00b1 0.31\nDistribution-based\nImprovedMAP [Wu and Hu, 2022]\nWRN-28-10\nYes\n71.56 \u00b1 0.74\n85.05 \u00b1 0.49\n79.26 \u00b1 0.08\n90.41 \u00b1 0.72\nTDO [Liu et al., 2023]\nWRN-28-10\nNo\n74.10 \u00b1 0.23\n86.44 \u00b1 0.13\n82.68 \u00b1 0.22\n90.85 \u00b1 0.13\nPBML [Fu et al., 2025]\nNesT ViT\nNo\n70.04 \u00b1 0.67\n85.21 \u00b1 0.58\n76.73 \u00b1 0.88\n90.11 \u00b1 0.65\nDDC [Chen et al., 2025a]\nWRN-28-10\nNo\n69.17 \u00b1 1.25\n85.23 \u00b1 0.97\n\u2013\n\u2013\nSemantic-based\nKTN [Peng et al., 2019]\nConvNet\nYes\n64.42 \u00b1 0.72\n74.16 \u00b1 0.56\n63.43 \u00b1 0.21\n74.32 \u00b1 0.58\nAM3 [Xing et al., 2019]\nResNet-12\nYes\n65.30 \u00b1 0.49\n78.10 \u00b1 0.36\n69.08 \u00b1 0.47\n82.58 \u00b1 0.31\nSEGA [Yang et al., 2022a]\nResNet-12\nYes\n69.04 \u00b1 0.26\n79.03 \u00b1 0.18\n72.18 \u00b1 0.30\n84.28 \u00b1 0.21\nBMI [Li and Wang, 2023]\nResNet-12\nYes\n77.01 \u00b1 0.34\n84.85 \u00b1 0.27\n78.37 \u00b1 0.44\n86.30 \u00b1 0.32\nMAVSI [Zhao et al., 2024]\nResNet-12\nYes\n69.74 \u00b1 0.21\n82.23 \u00b1 0.41\n74.61 \u00b1 0.18\n87.45 \u00b1 0.46\nSEVPro [Cai et al., 2024a]\nResNet-12\nYes\n71.81 \u00b1 0.22\n78.88 \u00b1 0.18\n72.77 \u00b1 0.30\n84.04 \u00b1 0.21\nPRSN [Dong et al., 2025]\nResNet-12\nYes\n71.09 \u00b1 0.90\n86.82 \u00b1 0.61\n74.14 \u00b1 0.86\n88.54 \u00b1 0.61\nVision\u2013language based\nSP-CLIP [Chen et al., 2023b]\nViT-T\nYes\n72.31 \u00b1 0.40\n83.42 \u00b1 0.30\n78.03 \u00b1 0.46\n88.55 \u00b1 0.32\nSP-SBERT [Chen et al., 2023b]\nViT-T\nYes\n70.70 \u00b1 0.42\n83.55 \u00b1 0.30\n73.31 \u00b1 0.50\n88.56 \u00b1 0.32\nSP-Glove [Chen et al., 2023b]\nViT-T\nYes\n70.81 \u00b1 0.42\n83.31 \u00b1 0.30\n74.68 \u00b1 0.50\n88.64 \u00b1 0.31\nSemFew-Trans [Zhang et al., 2024a]\nSwin-T\nYes\n78.94 \u00b1 0.66\n86.49 \u00b1 0.50\n82.37 \u00b1 0.77\n89.89 \u00b1 0.52\nSYNTRANS [Tang et al., 2025]\nResNet-12\nYes\n76.20 \u00b1 0.69\n86.12 \u00b1 0.54\n79.69 \u00b1 0.81\n87.78 \u00b1 0.60\nPMCE(Ours)\nResNet-12\nYes\n82.90 \u00b1 0.78\n92.29 \u00b1 0.46\n83.50 \u00b1 0.32\n95.03 \u00b1 0.34\nSwin-T\nYes\n85.03 \u00b1 0.67\n92.77 \u00b1 0.37\n83.13 \u00b1 0.33\n93.23 \u00b1 0.43\nand a lightweight prototype-preserving term that discourages\nexcessive drift from the frozen backbone structure:\nLrec = 1\nB\nB\nX\ni=1\nv(i)\nout \u2212\u00b5yi\nproto\n1.\n(20)\nWe use a small \u03bbrec so that the enhancer can still introduce\ncaption-guided adaptation while keeping base-class geometry\nstable.\nCaption consistency with supervised contrastive learning.\nCaption quality may vary, so we add a consistency regular-\nization on caption semantics in the projected space. For sam-\nple xi, let s(i)\nproj \u2208Rdv be the projected caption embedding\n(Eq. (12)). We normalize it as\nvcap\ni\n=\ns(i)\nproj\n\u2225s(i)\nproj\u2225\n\u2208Rdv.\n(21)\nWe encourage captions from the same class to be coherent\nwhile maintaining inter-class separation via a supervised con-\ntrastive loss:\nLcon = 1\nB\nB\nX\ni=1\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u2212\n1\n|P(i)|\nX\np\u2208P(i)\nlog\nexp(cos(vcap\ni , vcap\np )/\u03c4c)\nB\nP\na=1\na\u0338=i\nexp(cos(vcap\ni , vcap\na )/\u03c4c)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n(22)\nwhere P(i) = {p \u0338= i | yp = yi} denotes positives in the\nmini-batch and \u03c4c is a temperature.\nOverall objective.\nThe final objective is\nLtotal = Lcls + \u03bbrecLrec + \u03bbconLcon,\n(23)\nwith \u03bbrec and \u03bbcon tuned on the validation set. During meta-\ntest, f\u03b8, g\u03d5 and \u03a6 are all frozen, and classification is per-\nformed with logistic regression using enhanced support and\nquery features.\n5\nExperiments\nDatasets.\nWe evaluate PMCE on four standard few-\nshot benchmarks, including MiniImageNet, TieredImageNet,\nCIFAR-FS, and FC100. MiniImageNet [Vinyals et al., 2016]\nTable 2: Comparison with state-of-the-art few-shot learning methods under the 5-way 1-shot and 5-way 5-shot settings on CIFAR-FS and\nFC100. All results are reported as mean accuracy (%) with 95% confidence intervals. The best performance in each column is highlighted in\nbold, and \u201c\u2013\u201d indicates that the result is not reported.\nMethods\nBackbone\nSemantic\nCIFAR-FS\nFC100\n1-shot\n5-shot\n1-shot\n5-shot\nMetric-based\nProtoNet [Snell et al., 2017]\nResNet-12\nNo\n55.50 \u00b1 0.70\n72.00 \u00b1 0.60\n41.54 \u00b1 0.76\n57.08 \u00b1 0.76\nMetaOptNet [Lee et al., 2019]\nResNet-12\nNo\n72.80 \u00b1 0.70\n84.30 \u00b1 0.50\n47.20 \u00b1 0.60\n55.50 \u00b1 0.60\nCORL [He et al., 2023]\nResNet-12\nNo\n74.13 \u00b1 0.71\n87.54 \u00b1 0.51\n44.82 \u00b1 0.73\n61.31 \u00b1 0.54\nQSFormer [Wang et al., 2023b]\nResNet-12\nNo\n46.51 \u00b1 0.26\n61.58 \u00b1 0.25\n\u2013\n\u2013\nSSFormers [Chen et al., 2023a]\nResNet-12\nNo\n74.50 \u00b1 0.21\n86.61 \u00b1 0.23\n43.72 \u00b1 0.21\n58.92 \u00b1 0.18\nCads [Zhang et al., 2025]\nResNet-12\nNo\n73.23 \u00b1 0.21\n87.67 \u00b1 0.14\n\u2013\n\u2013\nDistribution-based\nPBML [Fu et al., 2025]\nResNet-12\nNo\n73.07 \u00b1 0.59\n85.51 \u00b1 0.41\n47.92 \u00b1 0.49\n62.96 \u00b1 0.51\nDDC [Chen et al., 2025a]\nWRN-28-10\nNo\n76.47 \u00b1 1.86\n87.24 \u00b1 0.89\n\u2013\n\u2013\nSemantic-based\nProtoNet + SEVPro [Cai et al., 2024b]\nResNet-12\nYes\n74.01 \u00b1 0.29\n85.86 \u00b1 0.20\n\u2013\n\u2013\nPRSN [Dong et al., 2025]\nResNet-12\nYes\n78.72 \u00b1 0.96\n90.22 \u00b1 0.60\n47.91 \u00b1 0.57\n62.95 \u00b1 0.52\nVision\u2013language based\nLPE [Yang et al., 2023]\nResNet-12\nYes\n74.88 \u00b1 0.45\n85.30 \u00b1 0.35\n\u2013\n\u2013\nSP-Pretrain [Chen et al., 2023b]\nViT-S\nYes\n71.99 \u00b1 0.47\n85.98 \u00b1 0.34\n43.77 \u00b1 0.39\n59.48 \u00b1 0.39\nSAFF [Cumplido et al., 2025]\nVit-B\nYes\n78.50 \u00b1 0.30\n90.26 \u00b1 0.08\n47.17 \u00b1 0.12\n66.22 \u00b1 0.51\nPMCE(Ours)\nResNet-12\nYes\n76.97 \u00b1 0.81\n84.76 \u00b1 0.61\n49.81 \u00b1 0.82\n63.67 \u00b1 0.73\nSwin-T\nYes\n80.92 \u00b1 0.72\n89.02 \u00b1 0.58\n52.46 \u00b1 0.82\n67.00 \u00b1 0.75\nand TieredImageNet [Ren et al., 2018] are subsets of Ima-\ngeNet [Russakovsky et al., 2015] with 64, 16, and 20 classes\nfor base, validation, and novel splits, and 351, 97, and 160\nclasses for base, validation, and novel splits, respectively.\nCIFAR-FS [Bertinetto et al., 2019] follows a random split\nof CIFAR-100 into 64, 16, and 20 classes, while FC100 [Ore-\nshkin et al., 2018] adopts a superclass-based split with 60, 20,\nand 20 classes to reduce semantic overlap between base and\nnovel categories.\nImplementation details. Following [Zhao et al., 2024],\nwe use ResNet-12 [He et al., 2016] and Swin-T [Liu et al.,\n2021] as visual backbones. Captions are generated using a\nfrozen BLIP captioner, and both class names and captions are\nencoded by ViT-B/16 CLIP [Radford et al., 2021] into 512-\ndimensional semantic features. All CLIP and BLIP compo-\nnents remain frozen throughout training and evaluation. The\nenhancer \u03a6 consists of a projection layer with Linear, Lay-\nerNorm, and ReLU, followed by multi-head attention, with\na residual update scaled by a learnable factor \u03b2 initialized to\n0.1. We use h = 4 heads and dropout 0; the key dimension\nis dk = 160 for ResNet-12 (dv = 640) and dk = 192 for\nSwin-T (dv = 768). All backbones and BLIP/CLIP modules\nremain frozen, and inference is strictly inductive.\nWe follow a two-stage training protocol. We load the back-\nbone pre-trained in [Zhao et al., 2024], freeze it, and train\nonly \u03a6 together with an auxiliary classifier for 50 epochs us-\ning Adam with learning rate 10\u22124 and batch size 128. Dur-\ning evaluation, we sample 600 episodes for each 5-way 1-\nshot and 5-way 5-shot setting and report mean accuracy with\n95% confidence intervals; logistic regression is used by de-\nfault, and nearest-prototype yields similar trends. Unless oth-\nerwise specified, we set \u03b1 = 0.33 for 1-shot and \u03b1 = 0.7\nfor 5-shot in Eq. (3), use top-k retrieval with k = 7 and\nTable 3: Ablation of PMCE components under the 5-way 1-shot set-\nting on MiniImageNet and FC100 with the Swin-T backbone. MAP\ndenotes semantics-guided prior calibration; Enh.(S) and Enh.(Q) de-\nnote caption-guided enhancement on the support and query streams.\nWe report mean accuracy (%) \u00b1 95% confidence intervals.\nMAP\nEnh.(S)\nEnh.(Q)\nMiniImageNet\nFC100\n\u00d7\n\u00d7\n\u00d7\n71.02 \u00b1 0.79\n45.27 \u00b1 0.75\n\u00d7\n\u2713\n\u00d7\n72.18 \u00b1 0.78\n46.03 \u00b1 0.74\n\u00d7\n\u00d7\n\u2713\n72.49 \u00b1 0.81\n45.47 \u00b1 0.77\n\u2713\n\u00d7\n\u00d7\n74.93 \u00b1 0.76\n45.36 \u00b1 0.75\n\u2713\n\u2713\n\u00d7\n74.99 \u00b1 0.74\n45.15 \u00b1 0.77\n\u2713\n\u00d7\n\u2713\n77.17 \u00b1 0.76\n45.27 \u00b1 0.78\n\u00d7\n\u2713\n\u2713\n79.42 \u00b1 0.73\n48.79 \u00b1 0.80\n\u2713\n\u2713\n\u2713\n85.03 \u00b1 0.67\n52.46 \u00b1 0.82\ntemperature \u03c4 = 1.0, and set \u03c4c = 0.1, \u03bbrec = 1.0, and\n\u03bbcon = 1.0 for training.We use class-name embeddings for\nretrieval since they are stable and class-consistent, while cap-\ntions can be noisier on low-resolution images. We also ex-\nperimented with token-level caption features, but they lead to\nsignificantly larger intermediate files and storage overhead,\nso we report sentence-embedding results by default.\n5.1\nExperimental Results\nWe compare PMCE with representative metric-based meth-\nods FGFL [Cheng et al., 2023] and CPEA [Hao et al., 2023],\ndistribution-based methods MAP [Wu et al., 2021] and Im-\nprovedMAP [Wu and Hu, 2022], and recent semantic and\nvision\u2013language baselines BMI [Li and Wang, 2023], SP-\nCLIP [Chen et al., 2023b], SemFew-Trans [Zhang et al.,\n2024a], and PRSN [Dong et al., 2025]. Results on MiniIma-\ngeNet and TieredImageNet are reported in Tab. 1, and results\n(a)\n(b)\n(c)\n(d)\nFigure 3: Sensitivity of PMCE to the prior weight \u03b1 and the number\nof retrieved base classes k on MiniImageNet. Performance is stable\nover a wide range of values, and our default settings lie in the high-\naccuracy region.\non CIFAR-FS and FC100 are reported in Tab. 2.\nAcross ImageNet-style benchmarks, PMCE consistently\nimproves over strong semantic and vision\u2013language baselines\nunder both ResNet-12 and Swin-T. The gains are most ap-\nparent in the 1-shot regime, where prototype bias is severe\nand query-side refinement becomes critical. For example, as\nis shown in Tab. 1, with Swin-T, PMCE reaches 85.03% on\nMiniImageNet 1-shot and surpasses SemFew-Trans [Zhang\net al., 2024a] by 7.71% in this setting. Similar improvements\nare observed on TieredImageNet, indicating that the benefit\ntransfers to a larger and more diverse label space.\nOn CIFAR-100 derived benchmarks, the 32 \u00d7 32 resolu-\ntion makes captions less specific, which naturally attenuates\nthe advantage of instance-level semantics. Nevertheless, as\nshown in Tab. 2, PMCE remains competitive on CIFAR-FS\nand achieves the best 1-shot accuracy of 80.92%, surpassing\nPRSN by 2.79%. On FC100, PMCE achieves the top perfor-\nmance with 52.46% and 67.00%, yielding improvements over\nPRSN by 9.5% and 6.43%, respectively.\nOverall, PMCE delivers consistent gains across backbones,\nwith the most stable improvements on ImageNet-style bench-\nmarks where captions are more informative. We believe the\nimprovements mainly come from two design choices. First,\nsemantics-guided prior retrieval provides a class-specific\nprior that stabilizes prototype estimation when supports are\nextremely limited, which is especially important in 1-shot.\nSecond, PMCE optimizes the query side with label-free cap-\ntions under the inductive protocol, so support and query fea-\ntures are better aligned before matching.\n5.2\nEmpirical Analysis\nAblation study. Tab. 3 analyzes key components under the\n5-way 1-shot setting on MiniImageNet and FC100 with the\nSwin-T backbone. The baseline achieves 71.02% on Mini-\nSupport\nQuery\nw/o caption\nw/ caption\nFigure 4: Grad-CAM visualizations on the MiniImageNet test set.\nCompared with w/o caption, w/ caption produces more focused ac-\ntivations on query images, suppressing background responses and\nhighlighting discriminative object regions.\n(a) Initial Prototype\n(b) MAP Prototype\n(c) Enhanced Prototype\n(d) Final Prototype\nFigure 5: t-SNE visualization of feature distributions and prototype\nevolution on a 5-way 1-shot task from MiniImageNet. Different col-\nors denote classes; \u22c6are ground-truth centers, \u25a0initial prototypes,\n\u25b3MAP rectified prototypes,large \u2022 caption-enhanced prototypes,\nand \u22c4the final prototypes of PMCE. MAP calibration and caption-\nguided enhancement jointly move prototypes toward the true centers\nand make clusters more compact.\nImageNet and 45.27% on FC100. Adding semantics-guided\nMAP calibration alone improves MiniImageNet to 74.93%,\nwhich corresponds to a gain of 5.51%. Applying caption-\nguided enhancement to a single stream yields smaller gains,\nand query-side enhancement is more influential than support-\nside enhancement.\nEnhancing both streams without MAP\nreaches 79.42% on MiniImageNet and 48.79% on FC100.\nCombining MAP calibration with dual-stream enhancement\nyields the best performance, reaching 85.03% on MiniIma-\ngeNet and 52.46% on FC100, which improves over the base-\nline by 19.73% and 15.88%.\nImpact of hyperparameters. Fig. 3 explores the prior\nweight \u03b1 in Eq. (3) and the neighbor count k assigned for\nprior retrieval. In 1-shot, the high-accuracy region concen-\ntrates around \u03b1 in the range of 0.3 to 0.4. In 5-shot, the best\nregion shifts toward a larger \u03b1 around 0.7. Accuracy remains\nstable across a wide range of k, indicating that the retrieval\nstep is not critically dependent on the precise count of base\nclasses.\n5.3\nVisualization Analysis\nEffect of caption guidance. Fig. 4 compares Grad-CAM ac-\ntivation maps with and without caption guidance. Without\ncaptions, query responses are often diffuse and may include\nbackground regions. With caption guidance, activations be-\ncome more concentrated on object regions and less affected\nby background clutter.\nPrototype evolution. Fig. 5 visualizes feature distribu-\ntions and prototype evolution on a 5-way 1-shot episode\nfrom MiniImageNet. MAP calibration shifts prototypes to-\nward more reasonable regions by injecting class-specific pri-\nors, and caption-guided enhancement further tightens clusters\nand improves inter-class separation. The final prototypes are\ncloser to the ground-truth centers, supporting that PMCE im-\nproves prototype estimation and query-side representations in\na unified metric space.\n6\nConclusion\nIn this paper, we have presented PMCE, a probabilistic few-\nshot framework that integrates semantics-guided prior selec-\ntion, MAP prototype calibration, and caption-guided feature\noptimization.\nBy using class-name embeddings to select\nclass-specific priors from base statistics and leveraging BLIP\ncaptions as label-free instance cues for both supports and\nqueries, PMCE produces more stable prototypes and better-\naligned query representations, with a lightweight consistency\nregularizer to mitigate caption noise. Experiments on four\nbenchmarks with two backbones show consistent improve-\nments over strong metric-, semantic-, and distribution-based\nbaselines, with the most pronounced gains in the 1-shot set-\nting. Future work will explore the confidence of captions to\nfurther reduce semantic drift under noisy semantics.\nA\nAppendix\nA.1\nAdditional Implementation Details for PMCE\nTo ensure the reproducibility of our PMCE framework, we\nprovide the detailed algorithmic procedures for the two core\ncomponents: Semantics-Guided Prior Selection and Caption-\nGuided Feature Enhancement.\nSemantics-guided MAP Prototype Calibration\nAlgorithm 1 details how we rectify support prototypes by\nleveraging priors from the base classes.\nAlgorithmic Steps.Our procedure begins by retrieving the\ntop-K semantically similar base classes. We then aggregate\ntheir visual means to form \u00b5prior.This prior is then fused with\nthe support prototype pinit to compute the calibrated proto-\ntype \u02dcp. The hyperparameter \u03b1 is employed to balance the con-\ntribution between the support information and the retrieved\nbase-class knowledge across different shot settings.\nAlgorithm 1 Semantics-guided MAP Prototype Calibration\n(Weighted Prior)\nInput: Support set S, novel class semantic sclass, Knowledge Bank\nB = {(\u00b5j, sj\nbase)}|Cb|\nj=1\nParameter: Number of neighbors K, balancing hyperparameter \u03b1,\ntemperature \u03c4\nOutput: Rectified prototype \u02dcp\n1: Compute the empirical prototype pinit by averaging features of\nS.\n2: for each base class (\u00b5j, sj\nbase) \u2208B do\n3:\nCalculate semantic similarity scorej = cos(sclass, sj\nbase) ac-\ncording to Eq. (3)."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary\n\n#### Main Contributions\nPMCE is a novel probabilistic few-shot learning framework that leverages multi-granularity semantics with caption-guided enhancement. The key contributions are:\n1. **Probabilistic Framework**: PMCE introduces a probabilistic model for few-shot learning, which allows for more robust and flexible prototype estimation.\n2. **Multi-Granularity Semantics**: It incorporates class-level and instance-level (caption-based) semantic information to improve the quality of prototypes and query representations.\n3. **Caption-Guided Enhancement**: PMCE uses caption-guided feature refinement to optimize both support set and query features, ensuring better alignment before classification.\n\n#### Methodology\n**1. Multi-Granularity Semantics:**\nPMCE constructs a non-parametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. This allows it to capture both coarse-level (class-level) and fine-grained (instance-level) semantic information.\n\n**2. Caption-Guided Enhancement:**\nFor novel categories, PMCE retrieves relevant base classes based on their class-name embeddings. It then uses these retrieved base-class statistics to calibrate prototypes via a MAP update. Additionally, it employs a lightweight captioner to provide instance-level cues for both support and query features, which are fused with the prototype information.\n\n**3. Training Objective:**\nPMCE trains only a lightweight enhancement module on base classes while freezing the backbone and vision-language encoders. The training objective includes:\n- **Prototype Calibration**: Uses MAP-style updates to calibrate prototypes using class-name embeddings.\n- **Caption-Guided Enhancement**: Refines support and query features with caption-guided feature refinement, ensuring better alignment before classification.\n\n#### Key Findings\nExperiments conducted on four benchmarks (MiniImageNet, TieredImageNet, CIFAR-FS, FC100) demonstrate that PMCE consistently outperforms strong base models:\n- **Overall Performance**: PMCE achieves up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting.\n- **Specific Benchmarks**:\n  - On CIFAR-FS and FC100, PMCE reaches top accuracies of 80.92% and 52.46%, respectively.\n  - In the 1-shot regime, where prototype bias is severe, PMCE shows significant improvements over strong semantic baselines.\n\n#### Conclusions\nPMCE effectively addresses the challenges of biased prototypes and weak query representations in few-shot learning by leveraging multi-granularity semantics with caption-guided enhancement. The lightweight consistency regularizer helps mitigate caption noise, leading to consistent performance improvements across multiple benchmarks. Future work will explore how to further reduce semantic drift under noisy conditions.\n\n### Detailed Breakdown\n\n#### Introduction\nThe paper addresses the limitations of traditional few-shot learning methods, particularly in handling small datasets and avoiding overfitting. It introduces PMCE as a novel framework that combines probabilistic modeling with multi-granularity semantics and caption-guided enhancement. This approach aims to provide more robust and flexible prototype estimation.\n\n#### Methodology\n\n**1. Multi-Granularity Semantics:**\nPMCE uses a non-parametric knowledge bank to store visual statistics for each category, including class-name embeddings of base classes. These embeddings serve as the basis for both coarse-level (class-level) and fine-grained (instance-level) semantic information.\n\n**2. Caption-Guided Enhancement:**\nFor novel categories, PMCE retrieves relevant base classes based on their class-name embeddings. It then uses these retrieved base-class statistics to calibrate prototypes via a MAP update. Additionally, it employs a lightweight captioner to provide instance-level cues for both support and query features, which are fused with the prototype information.\n\n**3. Training Objective:**\nPMCE trains only a lightweight enhancement module on base classes while freezing the backbone and vision-language encoders. The training objective includes:\n- **Prototype Calibration**: Uses MAP-style updates to calibrate prototypes using class-name embeddings.\n- **Caption-Guided Enhancement**: Refines support and query features with caption-guided feature refinement, ensuring better alignment before classification.\n\n#### Key Findings\n\n**1. Performance on Benchmarks:**\nExperiments show that PMCE consistently outperforms strong base models across multiple benchmarks:\n   - On MiniImageNet, PMCE achieves up to 7.71% absolute gain over the strongest semantic competitor in the 1-shot setting.\n   - On CIFAR-FS and FC100, PMCE reaches top accuracies of 80.92% and 52.46%, respectively.\n\n**2. Specific Improvements:**\n- **1-Shot Setting**: In the challenging 1-shot regime where prototype bias is severe, PMCE shows significant improvements over strong semantic baselines.\n- **Generalization to Larger Spaces**: The benefits of PMCE transfer well to larger and more diverse label spaces, as demonstrated on TieredImageNet.\n\n#### Conclusions\n\nPMCE effectively addresses the challenges of biased prototypes and weak query representations in few-shot learning by leveraging multi-granularity semantics with caption-guided enhancement. The lightweight consistency regularizer helps mitigate caption noise, leading to consistent performance improvements across multiple benchmarks. Future work will explore how to further reduce semantic drift under noisy conditions.\n\n### References\n- [Wu et al., 2021]\n- [Wu and Hu, 2022]\n- [Chen et al., 2019]\n- [Snell et al., 2017]\n- [Vinyals et al., 2016]\n- [Hao et al., 2023]\n- [Li and Wang, 2023]\n- [Chen et al., 2023b]\n- [Zhang et al., 2024a]\n- [Dong et al., 2025]\n\nThis summary encapsulates the main points of the paper, including its contributions, methodology, key findings, and conclusions."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Confident Rankings with Fewer Items: Adaptive LLM Evaluation with\nContinuous Scores\nEsma Balk\u0131r1, Alice Pernthaller1, Marco Basaldella1,\nJos\u00e9 Hern\u00e1ndez-Orallo2,3*, Nigel Collier1,4*\n1Trismik, 2Leverhulme Centre for the Future of Intelligence, University of Cambridge,\n3Universitat Polit\u00e8cnica de Val\u00e8ncia, 4University of Cambridge\n{esma, alice, marco}@trismik.com, {jh2135, nhc30}@cam.ac.uk\nAbstract\nComputerized Adaptive Testing (CAT) has\nproven effective for efficient LLM evaluation\non multiple-choice benchmarks, but modern\nLLM evaluation increasingly relies on gener-\nation tasks where outputs are scored continu-\nously rather than marked correct/incorrect. We\npresent a principled extension of IRT-based\nadaptive testing to continuous bounded scores\n(ROUGE, BLEU, LLM-as-a-Judge) by replac-\ning the Bernoulli response distribution with\na heteroskedastic normal distribution. Build-\ning on this, we introduce an uncertainty aware\nranker with adaptive stopping criteria that\nachieves reliable model ranking while testing\nas few items and as cheaply as possible. We val-\nidate our method on five benchmarks spanning\nn-gram-based, embedding-based, and LLM-as-\njudge metrics. Our method uses 2% of the\nitems while improving ranking correlation by\n0.12 \u03c4 over random sampling, with 95% accu-\nracy on confident predictions.\n1\nIntroduction\nRigorous evaluation of large language models is\nessential, but current practice faces two challenges.\nThe first is cost: exhaustive evaluation becomes\nexpensive as the number of models, test items,\nand metrics grows. The second is methodological:\nscore differences are often reported without sig-\nnificance testing, leading many studies to mistake\nstatistical noise with improved performance and\nproduce non-replicable results (Dror et al., 2018).\nEfficient evaluation methods that maintain statisti-\ncal validity remain underexplored.\nComputerized Adaptive Testing (CAT) dynam-\nically selects informative test items to estimate\nmodel capabilities with far fewer evaluations than\nexhaustive benchmarking, and has emerged as a\npromising approach for efficient evaluation of large\n*Joint senior authors.\nlanguage models (Liu et al., 2024). These meth-\nods use adaptive use Item Response Theory (IRT)\nas their theoretical foundation (Mart\u00ednez-Plumed\net al., 2016; Maia Polo et al., 2024; Rodriguez et al.,\n2021), with recent work extending this to fully\nadaptive item selection (Hofmann et al., 2025).\nExisting approaches for LLM evaluation focus\nexclusively on multiple-choice datasets where re-\nsponses can only be correct or incorrect. How-\never, many of the tasks that reflect real use cases of\nLLMs such as summarization, dialogue, instruction\nfollowing and machine translation must be scored\non a continuous scale. These scoring strategies\ncover traditional metrics such as BLEU (Papineni\net al., 2002) and ROUGE (Lin, 2004), and embed-\nding similarity-based scores such as BERTScore\n(Zhang et al., 2020) and COMET (Rei et al., 2020).\nLLM-as-judge evaluation (Zheng et al., 2024; Liu\net al., 2023), now widely adopted for assessing\ninstruction following and open-ended generation,\nalso produces ordinal ratings that can be normal-\nized and treated as continuous.\nWe present a continuous extension of IRT-based\nadaptive testing that accommodates real-valued\nscores while preserving the mathematical structure\nof binary CAT. Our key insight is to replace the\nBernoulli response distribution with a heteroskedas-\ntic normal distribution that maintains the same lo-\ngistic mean function while introducing variance\nthat mimics the Bernoulli structure. This preserves\nthe natural property that variance is highest when\noutcomes are most uncertain (\u00b5=0.5) and shrinks\nat the boundaries where scores are constrained.\nContinuous CAT already reduces evaluation cost\nsubstantially, but further savings are possible when\nwe consider how evaluation results are typically\nused. Model evaluation is fundamentally compara-\ntive: the primary goal is often to determine whether\none model is better than others. Building on contin-\nuous CAT, we introduce an adaptive multi-model\nranking method that efficiently obtains statistically\narXiv:2601.13885v1  [cs.CL]  20 Jan 2026\nsignificant rankings by monitoring uncertainty es-\ntimates around model scores. Rather than test-\ning each model to a fixed precision threshold, we\nstop as soon as models are well-differentiated from\neach other based on pairwise confidence at a user-\nspecified level. This focuses the testing effort on\nclose competitors while reducing items for clearly\nseparated models. We further observe that when\ndistinguishing two models, testing either one yields\ninformation about their relative performance, allow-\ning cost-aware allocation that preferentially tests\ncheaper models to maximize uncertainty reduction\nper dollar spent.\nWe validate our approach on five generation\nbenchmarks that span summarization (GovRe-\nport, BioLaySumm2025), named entity recognition\n(Nemotron PII), question answering (TruthfulQA)\nand translation (FLORES). For each dataset,\nwe evaluate 5 disjoint sets of 4 hold-out mod-\nels and multiple metric types (ROUGE, BLEU,\nBERTScore, COMET, readability indices, LLM-\nas-judge).\nOur adaptive ranker achieves 0.73\nKendall\u2019s \u03c4 correlation with ground-truth rankings\nwhile using only 2% of the full evaluation budget,\noutperforming random sampling by 0.12 \u03c4. Com-\npared to fixed-length CAT, adaptive stopping pro-\nvides an additional 32% item reduction and 42%\ncost savings.\nOur contributions are:\n1. A principled extension of IRT-based CAT\nfrom binary to continuous bounded outcomes\nvia heteroskedastic normal distribution\n2. Adaptive multi-model ranking algorithm with\npairwise stopping and cost-aware allocation\n3. Empirical validation across diverse generation\ntasks, metrics, and model scales\nThe continuous CAT framework opens adaptive\ntesting to the full spectrum of modern LLM evalua-\ntion, and the adaptive multi-model ranker enables\nefficient comparisons of generation quality across\nmodel candidates. We make our code available.1\n2\nRelated Work\nEarly studies on efficient model evaluation used\nIRT to analyze benchmark properties such as item\ndifficulty and discrimination (Mart\u00ednez-Plumed\net al., 2016; Lalor et al., 2016; Vania et al., 2021;\n1https://github.com/trismik/continuous-cat\nRodriguez et al., 2021). Maia Polo et al. (2024) and\nKipnis et al. (2025) used IRT to identify represen-\ntative static subsets, while Hofmann et al. (2025)\ndeveloped a fully adaptive framework that dynami-\ncally selects items during evaluation. However, all\nexisting approaches assume binary outcomes and\ncannot accommodate the continuous scores. Lalor\net al. (2016) applied IRT to construct evaluation\nscales for NLP systems, and Prud\u00eancio et al. (2015)\nand Lalor et al. (2019) showed that IRT models can\nbe fit using response patterns from model ensem-\nbles rather than human annotations. Chen et al.\n(2019) proposed a Beta-distributed IRT model ap-\nplied to modelling classifier confidence scores.\nThis follows the tradition in psychometrics,\nwith Samejima (1973) introducing the continu-\nous response model, and Noel and Dauvier (2007)\nproposing Beta-distributed models that naturally re-\nspect [0, 1] bounds. Both introduce additional com-\nplexity to Fisher Information calculations, com-\nplicating item selection. Beta models also can-\nnot accommodate exact boundary values, requir-\ning either ad hoc transformations or zero-and-one\ninflated extensions (Molenaar et al., 2022). Re-\nlated continuous-response formulations include lin-\near factor-analytic indices for difficulty and infor-\nmation, and Rasch-type models for continuous re-\nsponses in large-scale learning systems (Ferrando,\n2009; Deonovic et al., 2020). These typically as-\nsume homoskedastic residuals or prioritize scal-\nable scoring over closed-form CAT information.\nMellenbergh (1994) developed generalized linear\nIRT with simpler closed-form solutions, but as-\nsumes constant variance across the score range.\nOur heteroskedastic normal formulation combines\nsimplicity with appropriate variance structure and\nmaintains direct compatibility with standard CAT\nalgorithms.\n3\nBackground\nItem Response Theory (IRT)\n(Hambleton et al.,\n1991) models the probability of a correct response\nas a function of the test-taker\u2019s latent ability \u03b8\nand item parameters. The simplest model, the 1-\nParameter Logistic (1PL) model, defines this prob-\nability as:\nP(X = 1|\u03b8, a, b) =\n1\n1 + exp(\u2212a(\u03b8 \u2212b))\n(1)\nwhere b is item difficulty and a is a discrimi-\nnation parameter shared across all items. When\nFigure 1: Adaptive testing focuses on items around the\nmodel ability, skipping thoses for which it would most\ncertainly (i.e., uninformatively) get high or low scores.\nability matches difficulty (\u03b8 = b), the probability\nof success is exactly 0.5. Higher a yields steeper\nresponse curves that better differentiate between\nability levels.\nComputerized adaptive testing (CAT)\n(Wainer,\n2000) dynamically selects items to efficiently esti-\nmate a test-taker\u2019s ability. The algorithm initializes\nwith a prior distribution over \u03b8, typically N(0, \u03c32\n0)\nwhere the prior variance is a hyperparameter con-\ntrolling initial uncertainty. At each iteration, it se-\nlects the item that maximizes Fisher Information at\nthe current ability estimate i\u2217= arg maxi I(\u02c6\u03b8|bi).\nAfter observing the response, the ability estimate\nand posterior are updated via Bayesian updating.\nAs items accumulate, the standard error decreases:\nSE(\u02c6\u03b8) =\n1\nqPn\ni=1 Ii(\u02c6\u03b8)\n(2)\nTesting terminates when this standard error falls\nbelow a predetermined threshold, yielding a precise\nability estimate with fewer items than fixed-length\ntesting.\nFisher Information\nquantifies how much an ob-\nservation tells us about \u03b8.\nFor the 1PL model\nwith binary outcomes, this is derived directly from\nBernoulli variance:\nI(\u03b8|a, b) = a2 \u00b7 P(\u03b8, b) \u00b7 (1 \u2212P(\u03b8, b))\n(3)\nInformation is highest when P = 0.5 (maximum\nuncertainty about the outcome) and approaches\nzero as P approaches 0 or 1 (outcome nearly cer-\ntain). This creates heteroskedastic variance as a\nfunction of the predicted probability P(\u03b8, b).\n4\nContinuous CAT\nTo extend CAT to continuous outcomes, we require\na response distribution for scores in [0, 1] rather\nthan binary {0, 1}. We retain the logistic mean\nfunction from standard IRT, which captures the re-\nlationship between ability, difficulty, and expected\nperformance, and replace the Bernoulli distribution\nwith a normal distribution:\nX|\u03b8, b, k \u223cN(\u00b5(\u03b8, b), \u03c32(\u03b8, b))\n(4)\nwhere the mean follows the logistic function:\n\u00b5(\u03b8, b) =\n1\n1 + exp(\u2212(\u03b8 \u2212b))\n(5)\nWe preserve the Bernoulli variance structure\nVar(X) = P(1 \u2212P) by defining:\n\u03c32(\u03b8, b) = k \u00b7 \u00b5(\u03b8, b) \u00b7 (1 \u2212\u00b5(\u03b8, b))\n(6)\nwhere k = 1/a2 is a noise parameter capturing\nmeasurement precision. We estimate a single k per\ndataset-metric combination from calibration data.\nThe Fisher Information for this model is:\nI(\u03b8|b, k) = (d\u00b5/d\u03b8)2\n\u03c32\n= \u00b5(1 \u2212\u00b5)\nk\n(7)\nComparing with the 1PL Fisher Information\nI = a2 \u00b7 P(1 \u2212P), we see that k = 1/a2: high\ndiscrimination (a > 1) corresponds to low noise\n(k < 1), and vice versa. Both parameterizations\ncapture the same underlying question: how reliably\ndoes an observed response reflect true ability? This\nequivalence indicates that our continuous extension\nconstitutes a natural generalization that unifies the\ntreatment of measurement precision across binary\nand continuous settings.\nA limitation of the normal distribution is its un-\nbounded support: it can technically assign prob-\nability to values outside [0, 1]. However, the het-\neroskedastic variance structure mitigates this con-\ncern: as \u00b5 approaches the boundaries, \u03c32 = k \u00b7\n\u00b5(1 \u2212\u00b5) shrinks, concentrating the distribution\nwithin the valid range. Alternative distributions\nsuch as the Beta are naturally bounded but do not\npreserve the connection to binary IRT and compli-\ncate the Fisher Information derivation. In practice,\nwe find that the normal approximation performs\nwell across the metrics we consider.\nThe CAT algorithm itself remains unchanged.\nItem selection still maximizes Fisher Information\nat the current ability estimate, with the informa-\ntion formula now incorporating k. Posterior up-\ndates follow the same Bayesian logic with normal\nlikelihoods. The standard error formula SE(\u02c6\u03b8) =\n1/\npP\ni Ii applies as before.\n5\nParameter Estimation\nWe estimate both bi and k from historical evalu-\nation data by fitting the model to observed score\ndistributions across a population of models.\nItem Difficulty.\nWe adopt the 1PL model and es-\ntimate item difficulties from observed scores across\ncalibration models. Following standard IRT con-\nvention, we center the ability scale such that the\nmean model ability is zero. Under this parameteri-\nzation, the maximum likelihood estimate for item\ndifficulty simplifies to:\nbi = \u2212logit(\u02c6pi) = log\n\u00121 \u2212\u02c6pi\n\u02c6pi\n\u0013\n(8)\nwhere \u02c6pi is the average score on item i across\nall calibration model-temperature configurations.\nWhen scores cluster in a narrow range (common for\nmetrics like ROUGE where most models achieve\n0.3\u20130.5), the resulting difficulties may not span the\nfull ability range. We therefore apply min-max\nnormalization to [\u03f5, 1 \u2212\u03f5] before the logit transfor-\nmationn.\nNoise Parameter.\nWe estimate a global k from\nthe calibration data using method-of-moments.\nGiven item difficulties {bi} and model ability esti-\nmates {\u03b8j} (computed as \u03b8j = logit(\u00afyj) where \u00afyj\nis the model\u2019s average scaled score), we obtain:\nk =\nP\ni,j(yij \u2212\u00b5ij)2\nP\ni,j \u00b5ij(1 \u2212\u00b5ij)\n(9)\nwhere \u00b5ij = logit\u22121(\u03b8j \u2212bi) is the predicted\nmean score for model j on item i. This formula di-\nrectly estimates the variance inflation factor: when\nk = 1, observed variance matches the Bernoulli\nstructure exactly; k > 1 indicates noisier measure-\nments than binary responses; k < 1 indicates more\nprecise measurements.\nFiltering Negative Discrimination Items.\nSome\nitems may anti-correlate with model ability. On\nthese items, weaker models score higher than\nstronger models, rendering them effectively un-\ninformative. Because we use a 1PL model with\na global discrimination parameter, we filter such\nitems by computing the Pearson correlation be-\ntween item scores and model abilities across the\ncalibration set, and excluding items with negative\ncorrelation from the adaptive item pool.\n6\nAdaptive Ranker\nStandard CAT approaches evaluate each model\nindependently to a fixed precision threshold, ig-\nnoring the comparative nature of model evaluation.\nThis can be inefficient, since a model that clearly\ndominates or trails the field may receive the same\nnumber of test items as one locked in a close com-\npetition with a neighbor. We propose an adaptive\nmulti-model ranking algorithm that directly opti-\nmizes for the comparative goal. Given M models\nto evaluate on a shared item bank, the algorithm\nruns independent CAT runs for each model, but\nmonitors pairwise confidence in the ranking and\nterminates when all adjacent model pairs are statis-\ntically well-differentiated at a user-specified confi-\ndence level.\nPairwise confidence.\nGiven ability estimates \u02c6\u03b8i\nand \u02c6\u03b8j with standard errors SEi and SEj, we\ncompute the probability that model i outperforms\nmodel j under a normal approximation:\nP(\u03b8i > \u03b8j) = \u03a6\n\uf8eb\n\uf8ed\n\u02c6\u03b8i \u2212\u02c6\u03b8j\nq\nSE2\ni + SE2\nj\n\uf8f6\n\uf8f8\n(10)\nwhere \u03a6 is the standard normal CDF. This quan-\ntifies our confidence in the pairwise ordering given\ncurrent uncertainty in both estimates.\nAdaptive stopping.\nTraditional CAT terminates\nwhen each model\u2019s standard error falls below a\nthreshold \u03f5, regardless of how the models compare.\nWe instead terminate when all adjacent pairs in the\ncurrent ranking satisfy P(\u03b8i > \u03b8j) > 1 \u22121\u2212\u03b3\n2\nor P(\u03b8i > \u03b8j) < 1\u2212\u03b3\n2 , corresponding to a two-\nsided confidence interval at level \u03b3, or reach a max-\nimum item budget nmax. This focuses testing effort\nwhere it matters: models that are clearly separated\nrequire fewer items to establish their relative or-\ndering, while close competitors receive additional\ntesting until their difference reaches statistical sig-\nnificance or the budget is exhausted. If two models\nremain indistinguishable at the budget limit, they\ncan be reported as tied rather than forced into an\narbitrary ordering.\nCost-aware allocation.\nWhen a pair (i, j) re-\nquires additional testing to reach confidence thresh-\nold \u03b3, we must choose which model to evaluate\nnext. Since testing either model reduces uncer-\ntainty in the pairwise comparison, we select the\nAlgorithm 1 Adaptive Multi-Model Ranking\nRequire: Models {1, . . . , M}, item bank {(bi, ki)}, costs {cm}, confidence \u03b3, max items nmax\n1: Initialize \u02c6\u03b8m \u223cN(median(bi), 25),\n2: Warm-up: Administer ninit items to each model using MFI selection\n3: while not all adjacent pairs confident or at max items do\n4:\nRank models by \u02c6\u03b8m\n5:\nIdentify uncertain pairs: U = {(i, j) : \u03b3 > P(\u03b8i > \u03b8j) > 1 \u2212\u03b3 and ni < nmax and nj < nmax}\n6:\nif U = \u2205then\n7:\nbreak\n8:\nend if\n9:\nCollect candidate models: C = {m : m \u2208(i, j) for some (i, j) \u2208U}\n10:\nSelect model: m\u2217= arg maxm\u2208C\nSE2\nm\n(nm+1)\u00b7cm\n11:\nSelect item: i\u2217= arg maxi I(\u02c6\u03b8m\u2217|bi, ki)\n12:\nAdminister item i\u2217to model m\u2217, observe score y\n13:\nUpdate \u02c6\u03b8m\u2217and SEm\u2217\n14: end while\n15: return Ranking with pairwise confidence scores\nmodel that provides the greatest expected uncer-\ntainty reduction per unit cost. We quantify this\nas:\nvaluem =\nSE2\nm\n(nm + 1) \u00b7 cm\n(11)\nwhere nm is the number of items already admin-\nistered to model m and cm is its per-item evaluation\ncost. The numerator reflects current uncertainty;\nthe denominator captures diminishing returns (each\nadditional item contributes less) weighted by cost.\nThis criterion naturally allocates more items to\ncheaper models when resolving uncertain compar-\nisons, yielding additional cost savings beyond item\nreduction alone.\nAlgorithm.\nThe complete procedure combines\ncontinuous CAT estimation with pairwise stopping\nand cost-aware allocation under a total budget con-\nstraint. Rather than allocating a fixed maximum\nnumber of items per model, we specify a total cost\nbudget B that can be flexibly distributed across\nmodels. After a warm-up phase that administers\ninitial items to all models via MFI selection, the\nalgorithm iteratively identifies uncertain pairs, se-\nlects the most cost-effective model to test, chooses\nthe maximally informative item for that model, and\nupdates posteriors. Testing continues until either\nall adjacent pairs reach the confidence threshold,\nor the total budget B is exhausted.\n7\nExperiments\nWe evaluate our continuous CAT extension and\nadaptive ranker across five generation tasks span-\nning summarization, machine translation, question\nanswering, and named entity recognition. Our goal\nis to approximate ground-truth rankings from full\ndataset evaluation using as few items as possible.\nWe compare our adaptive ranker against random\nsampling, and examine whether the confidence es-\ntimates can reliably tell apart genuine performance\ndifferences from statistical ties.\n7.1\nExperimental Protocol\nWe select tasks representing diverse generation set-\ntings and metric types (Table 1). This selection\ncovers n-gram overlap (ROUGE, BLEU), learned\nembeddings (BERTScore, COMET), readability\nindices (FKGL), LLM-as-judge, and span-level\nF1. We exclude BERTScore from GovReport as\nall models achieved near-identical scores (range\n<0.02), yielding no meaningful ground-truth rank-\ning. Full prompts and scoring details are provided\nin Appendix A.\nWe evaluate 21 models from 6 families: Ope-\nnAI GPT, Google Gemini, Amazon Nova, Meta\nLlama, Mistral, and Qwen (see Appendix A for\nfull list). For each dataset-metric pair, we collect\nexhaustive evaluation scores by running all mod-\nels on all items, establishing ground-truth rankings.\nEach model is evaluated at four temperature set-\ntings (T \u2208{0.0, 0.4, 0.7, 1.0}), which yields 84\nmodel-temperature configurations per item. Our\nTable 1: Evaluation datasets. FKGL: Flesch-Kincaid Grade Level (Kincaid et al., 1975).\nDataset\nSize\nSplit\nTask\nMetrics\nBioLaySumm\n(Goldsack et al., 2024)\n1,376\nval\nLay summarization\nROUGE-L\nBERTScore\nFKGL\nGovReport\n(Huang et al., 2021)\n973\nval\nDocument summarization\nROUGE-L\nTruthfulQA\n(Lin et al., 2022)\n817\nval\nOpen-ended QA\nLLM-Judge\nBERTScore (diff.)\nFLORES\n(NLLB Team et al., 2022)\n1,012\ndevtest\nTranslation (TR\u2192EN)\nCOMET\nBLEU\nNemotron-PII\n(Steier et al., 2025)\n2,000\ntest2\nNER/PII detection\nF1 (span-level)\nmain experiment uses five-fold cross-validation\nover models. In each fold, we randomly select 4\nmodels as the hold-out set to be ranked adaptively;\nall temperature variants of hold-out models are ex-\ncluded from calibration. The remaining 17 models\n(68 configurations) provide calibration data for es-\ntimating item parameters (bi, k). We fit parameters\nvia maximum likelihood on the calibration set, then\nsimulate adaptive evaluation on the hold-out mod-\nels at T = 0. We set the confidence threshold\n\u03b3 = 0.95 for pairwise decisions and require a min-\nimum of 10 items per model before stopping.\nFor each hold-out set, we compare the adaptive\nranker to a random sampling baseline which al-\nlocates the total budget B across models by iter-\natively selecting a random affordable model and\na random unseen item to administer. We assess\nranking quality using Kendall\u2019s \u03c4 correlation be-\ntween predicted and ground-truth rankings com-\nputed from point estimates (mean ability \u02c6\u03b8). For\nefficiency, we report the mean number of items\nadministered per model.\n7.2\nMain Results\nTable 2 presents results across all dataset-metric\ncombinations. On 7 of 9 dataset-metric pairs, the\nadaptive ranker achieves higher Kendall\u2019s \u03c4 than\nthe random sampling baseline. The largest gains\noccur on discriminative metrics: +12% on BioLay-\nSumm ROUGE-L (0.957 vs 0.853), +26% on Truth-\nfulQA BERTScore (0.450 vs 0.190), and +22%\non FLORES BLEU (0.803 vs 0.580). These re-\nsults demonstrate that our adaptive ranker provides\ngenuine performance gains over random sampling.\nThe adaptive ranker uses 52\u2013101 items per hold-\nout set, and achieves 0.73 \u03c4 with the full dataset\nranking while using only 2% of the items.\nThe discrimination parameter a predicts where\nadaptive methods provide the most benefit. On\nhigh-discrimination metrics (a > 3), the adap-\ntive ranker consistently outperforms baseline. On\nlow-discrimination metrics like Nemotron F1 (a =\n1.42), individual items provide less information\nabout ability, limiting the advantage of intelligent\nselection.\nDistributional Conformance.\nOur continuous\nIRT extension assumes scores follow a het-\neroskedastic normal distribution with variance\n\u03c32 = k \u00b7 \u00b5(1 \u2212\u00b5). We examine how well each\nmetric conforms to this assumption by measuring\nthe R2 between observed and predicted variance\nacross score bins (see Appendix B for details). Con-\nformance varies substantially: BERTScore and\nBLEU exhibit moderate fit (R2 = 0.24\u20130.36),\nwhile ROUGE-L and COMET show poor confor-\nmance despite high ranking accuracy. We find no\npositive correlation between conformance and rank-\ning quality (r = \u22120.12); instead, discrimination\na is the stronger predictor (r = 0.68). This sug-\ngests that violations of the heteroskedastic variance\nassumption do not degrade ranking performance\nwhen items are sufficiently discriminative.\n7.3\nTie Detection\nThe \u03c4 correlation in Table 2 measures ranking ac-\ncuracy based on point estimates, but point esti-\nmates alone do not indicate when a ranking is reli-\nable. Reporting that model A outperforms model\nB when the difference is due to noise leads to non-\nreproducible results. Our method uses the IRT\nmodel\u2019s posterior to detect statistical ties, where a\npair is reported as a tie when confidence falls below\n95%\nTable 2: Ranking performance across benchmarks. Higher discrimination (a) indicates more informative items. %\nUsed is items administered relative to full evaluation.\nDataset\nMetric\nSize\na\nAdaptive \u03c4 \u2191\nBaseline \u03c4 \u2191\nItems \u2193\n% Used \u2193\nBioLaySumm\nROUGE-L\n1376\n4.13\n0.957\u00b10.13\n0.853\u00b10.20\n82\u00b117\n1.5\u00b10.3%\nBioLaySumm\nBERTScore\n1376\n3.40\n0.903\u00b10.17\n0.743\u00b10.36\n88\u00b19\n1.6\u00b10.2%\nBioLaySumm\nFKGL\n1376\n2.34\n0.800\u00b10.25\n0.713\u00b10.40\n88\u00b115\n1.6\u00b10.3%\nGovReport\nROUGE-L\n973\n4.79\n0.800\u00b10.20\n0.823\u00b10.22\n98\u00b130\n2.5\u00b10.8%\nTruthfulQA\nLLM-Judge\n817\n2.59\n0.490\u00b10.38\n0.400\u00b10.49\n93\u00b114\n2.9\u00b10.4%\nTruthfulQA\nBERTScore\n817\n2.65\n0.450\u00b10.43\n0.190\u00b10.46\n92\u00b17\n2.8\u00b10.2%\nFLORES\nBLEU\n1012\n3.12\n0.803\u00b10.23\n0.580\u00b10.31\n92\u00b18\n2.3\u00b10.2%\nFLORES\nCOMET\n1012\n4.07\n0.677\u00b10.33\n0.503\u00b10.41\n101\u00b111\n2.5\u00b10.3%\nNemotron\nF1\n2000\n1.36\n0.673\u00b10.19\n0.707\u00b10.29\n52\u00b15\n0.7\u00b10.1%\nOverall (mean)\n3.16\n0.73\n0.61\n88\n2.0%\nTable 3: Tie detection against ground-truth ties from bootstrap CIs on full evaluation. Adapt/GT Tie% are the\nfraction of pairs reported as ties for adaptive ranker and ground truth. Tie P/R/F1 are the precision, recall and F1 on\ntie detection quality of the adaptive ranker compared to the full evaluation. Conf Acc is the accuracy of the adaptive\nranker on its non-tie predictions.\nDataset\nMetric\nAdapt Tie%\nGT Tie%\nTie P \u2191\nTie R \u2191\nTie F1 \u2191\nConf Acc \u2191\nBioLaySumm\nROUGE-L\n14%\n3%\n0.60\n1.00\n0.62\n1.00\nBioLaySumm\nBERTScore\n42%\n13%\n0.36\n0.99\n0.43\n1.00\nBioLaySumm\nFKGL\n37%\n17%\n0.46\n0.95\n0.54\n0.98\nGovReport\nROUGE-L\n18%\n10%\n0.39\n0.73\n0.43\n0.94\nTruthfulQA\nLLM-Judge\n76%\n47%\n0.50\n0.89\n0.60\n0.77\nTruthfulQA\nBERTScore\n100%\n73%\n0.73\n1.00\n0.82\n\u2013\nFLORES\nBLEU\n58%\n30%\n0.48\n0.99\n0.58\n1.00\nFLORES\nCOMET\n57%\n37%\n0.52\n0.94\n0.56\n0.89\nNemotron\nF1\n65%\n7%\n0.08\n1.00\n0.14\n1.00\nOverall (mean)\n52%\n26%\n0.46\n0.94\n0.52\n0.95\nTable 3 presents tie detection performance of\nthe adaptive ranker against ground-truth ties de-\nrived from bootstrap confidence intervals on full\nevaluation. The adaptive ranker is consistently con-\nservative: Adapt Tie% exceeds GT Tie% for all\nmetrics. Tie detection recall is high (0.77\u20131.00),\nwhich means that our method catches most ground-\ntruth ties. For pairs where one model is ranked\nstrictly higher than the other, overall accuracy is\n95%. TruthfulQA BERTScore reports 100% ties\nbecause all models score within a 0.7% range, and\nthe differences are too small to resolve within the\nbudget constraints.\nTwo metric-dataset combinations show lower\nperformance: GovReport ROUGE-L (recall 0.73)\nand TruthfulQA LLM-Judge (confident accuracy\n0.77). In both cases, this can be traced to spe-\ncific model comparisons within single holdout sets\nthat have consistently biased estimates across seeds.\nFor TruthfulQA LLM-Judge, one holdout contains\nthree models that are tied in the ground-truth rank-\ning, but the adaptive ranker consistently ranks one\nmodel below the others, detecting the ties in only\n1\u20133 of 20 seeds. For GovReport ROUGE-L, a\nsingle ground-truth tie in one holdout is never de-\ntected (0/20 seeds). Even with these outliers, the\nadaptive ranker achieves 95% overall accuracy on\nconfidently ranked pairs and 94% recall in detect-\ning ground truth ties, perfectly aligning with the\n95% confidence cutoff of the method.\n7.4\nAblation: Adaptive vs Fixed-Length CAT\nThe adaptive ranker terminates based on pairwise\nconfidence, allocating more items to close com-\npetitors and fewer to well-separated models, and\nfurther manages cost by preferentially choosing\nTable 4: Comparison with fixed-length CAT that administers the same number of items to all models.\nDataset\nMetric\nAdapt \u03c4 \u2191\nFixed \u03c4 \u2191\n\u2206\u03c4\nItem Savings\nCost Savings\nBioLaySumm\nBERTScore\n0.90\n0.82\n\u22120.08\n36%\n46%\nBioLaySumm\nFKGL\n0.80\n0.78\n\u22120.02\n31%\n38%\nBioLaySumm\nROUGE-L\n0.96\n0.97\n+0.02\n39%\n47%\nFLORES\nBLEU\n0.80\n0.78\n\u22120.02\n33%\n43%\nFLORES\nCOMET\n0.68\n0.80\n+0.12\n34%\n44%\nGovReport\nROUGE-L\n0.80\n0.85\n+0.05\n35%\n46%\nNemotron\nF1\n0.68\n0.76\n+0.08\n23%\n31%\nTruthfulQA\nBERTScore\n0.45\n0.31\n\u22120.14\n28%\n41%\nTruthfulQA\nLLM-Judge\n0.49\n0.52\n+0.03\n29%\n41%\nOverall\n0.73\n0.73\n0.00\n32%\n42%\ncheaper models. To isolate the value of this strat-\negy, we compare against fixed-length CAT. For\neach holdout set, we administer the same number\nof items n to all models, where n is set to the max-\nimum items used by the adaptive ranker for any\nmodel in that holdout.\nTable 4 shows that the two methods achieve the\nsame average \u03c4, but adaptive uses 32% less items\nand 42% less cost. This shows that the additional\nitems used by the fixed-length CAT are superfluous,\nand the stopping mechanism leads to significant\nefficiency gains with no performance degradation.\n7.5\nGeneralization to Unseen Model Families\nOur main experiments use cross-validation over\nindividual models, but calibration and hold-out\nmodels may share architectural similarities within\nfamilies. To test whether item parameters gen-\neralize to genuinely novel architectures, we con-\nduct a family-stratified evaluation: we hold out\nMixtral-8x7b-instruct, Qwen3-32b, and Nova-pro-\nv1 and calibrating only on OpenAI, Meta/Llama,\nand Google/Gemini models (14 total).\nTable 5 shows results.\nThe adaptive ranker\nmatches or outperforms baseline on 7 of 9 dataset-\nmetric pairs, with substantial gains on TruthfulQA\nBERTScore (+60%) and GovReport ROUGE-L\n(+30%). These results suggest that item difficulty\nparameters transfer across model families: an item\nthat is difficult for Llama models tends to also be\ndifficult for Qwen or Nova models.\n8\nConclusion\nWe present a principled extension of IRT-based\nadaptive testing from binary to continuous bounded\nscores.\nBuilding on this foundation, we intro-\nTable 5: Family holdout evaluation. Calibration on\nOpenAI, Llama, Gemini (14 models); evaluation on\nhold-out Mistral, Qwen, Nova (3 models).\nDataset\nMetric\nBase \u03c4 \u2191\nAdapt \u03c4 \u2191\nBioLaySumm\nBERTScore\n0.53\u00b10.52\n1.00\u00b10.00\nBioLaySumm\nFKGL\n1.00\u00b10.00\n1.00\u00b10.00\nBioLaySumm\nROUGE-L\n1.00\u00b10.00\n0.87\u00b10.27\nFLORES\nBLEU\n0.67\u00b10.33\n1.00\u00b10.00\nFLORES\nCOMET\n0.73\u00b10.33\n0.73\u00b10.33\nGovReport\nROUGE-L\n0.56\u00b10.31\n0.85\u00b10.28\nNemotron\nF1\n0.93\u00b10.20\n0.67\u00b10.33\nTruthfulQA\nBERTScore\n0.27\u00b10.55\n0.87\u00b10.27\nTruthfulQA\nLLM-Judge\n0.40\u00b10.55\n0.60\u00b10.33\nOverall\n0.68\n0.84\nduce an adaptive multi-model ranking algorithm\nwith pairwise stopping and cost-aware allocation.\nAcross five generation benchmarks spanning dif-\nferent metrics, our method achieves 0.73 Kendall\u2019s\n\u03c4 correlation with ground-truth rankings and 95%\naccuracy on confident predictions while using only\n2% of items. Notably, our method can reliably rank\nmodels from entirely unseen families, and remains\nrobust to empirical deviations from the assumed\nvariance structure.\nFor future work, extending our to model item-\nspecific discrimination parameters can improve\nitem selection. Incorporating variable item costs\ncan enable cost-optimal item selection at a more\ngranular level. Extending to multi-metric evalua-\ntion can allow ranking models on several metrics\nsimultaneously in a single adaptive run, further\nincreasing evaluation efficiency. Developing meth-\nods for more efficient item parameter estimation\nsuch as transfer learning from related benchmarks\nor few-shot calibration can reduce the cold-start\ncost for new datasets and enable faster deployment.\n9\nLimitations\nOur method requires calibration data from exist-\ning model evaluations to estimate item parameters,\ncreating a cold-start problem for new benchmarks.\nThis upfront cost of exhaustive evaluation on cali-\nbration models is amortized over subsequent adap-\ntive evaluations, but represents a barrier to immedi-\nate deployment on new datasets. Furthermore, item\nparameters must be re-estimated for each metric\nseparately, which might add overhead when com-\nprehensive multi-metric evaluation is needed.\nOur stopping criterion evaluates each adjacent\npair independently at significance level \u03b1, which\ncontrols the per-comparison error rate but not the\nfamily-wise error rate (FWER). With M models\nand M \u22121 adjacent comparisons, the probability\nof at least one incorrectly ordered pair under the\nnull hypothesis is approximately 1 \u2212(1 \u2212\u03b1)M\u22121,\nreaching 18% for five models at \u03b1 = 0.05. Cor-\nrections such as Holm-Bonferroni could provide\nFWER control by requiring stricter thresholds for\nthe most confident pairs at the cost of additional test\nitems, potentially negating much of the efficiency\ngain from adaptive selection.\n10\nEthical Considerations\nLarge language model inference carries substan-\ntial environmental costs, with energy consumption\nand carbon emissions scaling with the number of\nAPI calls. By reducing evaluation from exhaustive\ntesting to approximately 2% of items, our method\ndirectly decreases the environmental footprint of\nmodel comparison. As the number of candidate\nmodels, benchmarks, and evaluation metrics contin-\nues to grow, efficient evaluation methods become\nincreasingly important for sustainable AI develop-\nment.\nBeyond environmental benefits, efficient evalua-\ntion has implications for equity in AI research. Ex-\nhaustive evaluation across large benchmarks favors\nwell-resourced organizations. By dramatically re-\nducing the number of required items, our approach\ncan lower barriers for smaller laboratories, students\nand independent researchers to perform meaning-\nful model comparisons. Additionally, our method\u2019s\nexplicit uncertainty quantification and tie detection\nencourages more honest reporting practices.\nOne potential concern is benchmark gaming: if\ncertain items are predictably selected as most infor-\nmative, model developers could optimize specifi-\ncally for those items rather than general capability.\nPeriodic re-calibration of data and item banks can\nmitigate this risk.\nReferences\nYu Chen, Telmo Silva Filho, Ricardo B Prudencio, Tom\nDiethe, and Peter Flach. 2019. \u03b23-irt: A new item\nresponse model and its applications. In The 22nd\ninternational conference on artificial intelligence and\nstatistics, pages 1013\u20131021. PMLR.\nBenjamin Deonovic, Maria Bolsinova, Timo Bechger,\nand Gunter Maris. 2020. A Rasch model and rating\nsystem for continuous responses collected in large-\nscale learning systems.\nFrontiers in Psychology,\n11:500039.\nRotem Dror, Gili Baumer, Segev Shlomov, and Roi\nReichart. 2018. The hitchhiker\u2019s guide to testing\nstatistical significance in natural language processing.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1383\u20131392. Association for\nComputational Linguistics.\nPere J Ferrando. 2009. Difficulty, discrimination, and\ninformation indices in the linear factor analysis model\nfor continuous responses.\nApplied Psychological\nMeasurement, 33(1):9\u201324.\nTomas Goldsack, Carolina Scarton, Matthew Shardlow,\nand Chenghua Lin. 2024. Overview of the BioLay-\nSumm 2024 shared task on the lay summarization\nof biomedical research articles. In Proceedings of\nthe 23rd Workshop on Biomedical Natural Language\nProcessing, pages 122\u2013131, Bangkok, Thailand. As-\nsociation for Computational Linguistics.\nRonald K Hambleton, Hariharan Swaminathan, and\nH Jane Rogers. 1991. Fundamentals of Item Re-\nsponse Theory. Sage Publications.\nValentin Hofmann, David Heineman, Ian Magnusson,\nKyle Lo, Jesse Dodge, Maarten Sap, Pang Wei Koh,\nChun Wang, Hannaneh Hajishirzi, and Noah A Smith.\n2025.\nFluid language model benchmarking.\nIn\nCOLM.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efficient attentions for long\ndocument summarization."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper\n\n#### Main Contributions\nThe paper presents a principled extension of Computerized Adaptive Testing (CAT) to accommodate continuous bounded scores in Large Language Model (LLM) evaluation, particularly focusing on tasks that require scoring on a continuous scale. The key contributions are:\n1. **Continuous CAT Framework**: An extension of Item Response Theory (IRT)-based adaptive testing to handle real-valued scores while preserving the mathematical structure of binary CAT.\n2. **Adaptive Multi-Model Ranking Algorithm**: A method for efficiently obtaining statistically significant rankings by monitoring uncertainty estimates around model scores, using pairwise stopping criteria and cost-aware allocation.\n\n#### Methodology\nThe authors introduce a heteroskedastic normal distribution to replace the Bernoulli response distribution in IRT-based adaptive testing. This allows continuous CAT to be applied to tasks that require scoring on a continuous scale (e.g., BLEU, ROUGE, LLM-as-a-Judge metrics). The key steps are:\n1. **IRT-Based Framework**: Retain the logistic mean function from standard IRT and replace the Bernoulli distribution with a heteroskedastic normal distribution.\n2. **Parameter Estimation**: Estimate item difficulties and noise parameters using calibration data to fit the model across datasets and metrics.\n\n#### Key Findings\nThe adaptive multi-model ranking algorithm achieves reliable model rankings while testing as few items and as cheaply as possible:\n- On five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics, the method uses 2% of the full evaluation budget.\n- It improves Kendall\u2019s \u03c4 correlation by 0.12 over random sampling, with 95% accuracy on confident predictions.\n\n#### Experiments\nThe authors validate their approach across five generation benchmarks:\n- **BioLaySumm**: ROUGE-L (0.957 vs 0.853), BERTScore (0.903 vs 0.743)\n- **GovReport**: ROUGE-L (0.800 vs 0.823)\n- **TruthfulQA**: LLM-Judge (0.490 vs 0.400), BERTScore (0.450 vs 0.190)\n- **FLORES**: BLEU (0.803 vs 0.580), COMET (0.677 vs 0.503)\n\n#### Conclusions\nThe adaptive multi-model ranking method achieves a Kendall\u2019s \u03c4 correlation of 0.73 with ground-truth rankings while using only 2% of the full evaluation budget, outperforming random sampling by 0.12 \u03c4. The authors demonstrate that their method can reliably rank models from unseen families and remains robust to empirical deviations from assumed variance structure.\n\n#### Future Work\nFuture work could extend this framework to model item-specific discrimination parameters or incorporate variable item costs for more efficient item selection. Additionally, methods for transfer learning or few-shot calibration could reduce the cold-start cost for new datasets and enable faster deployment.\n\n### Limitations\nThe method requires calibration data from existing evaluations to estimate item parameters, creating a cold-start problem for new benchmarks. Furthermore, item parameters must be re-estimated separately for each metric, which might add overhead when comprehensive multi-metric evaluation is needed."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "MATE: MATRYOSHKA AUDIO\u2013TEXT EMBEDDINGS FOR OPEN-VOCABULARY\nKEYWORD SPOTTING\nYoungmoon Jung, Myunghun Jung, Joon-Young Yang, Yong-Hyeok Lee, Jaeyoung Roh, Hoon-Young Cho\nAI Solution Team, Samsung Research, Seoul, South Korea\nABSTRACT\nOpen-vocabulary keyword spotting (KWS) with text-based enroll-\nment has emerged as a flexible alternative to fixed-phrase trig-\ngers. Prior utterance-level matching methods, from an embedding-\nlearning standpoint, learn embeddings at a single fixed dimen-\nsionality.\nWe depart from this design and propose Matryoshka\nAudio\u2013Text Embeddings (MATE), a dual-encoder framework that\nencodes multiple embedding granularities within a single vector via\nnested sub-embeddings (\u201cprefixes\u201d). Specifically, we introduce a\nPCA-guided prefix alignment: PCA-compressed versions of the full\ntext embedding for each prefix size serve as teacher targets to align\nboth audio and text prefixes. This alignment concentrates salient\nkeyword cues in lower-dimensional prefixes, while higher dimen-\nsions add detail. MATE is trained with standard deep metric learning\nobjectives for audio\u2013text KWS, and is loss-agnostic. To our knowl-\nedge, this is the first application of matryoshka-style embeddings\nto KWS, achieving state-of-the-art results on WSJ and LibriPhrase\nwithout any inference overhead.\nIndex Terms\u2014 Keyword spotting, open-vocabulary, text enroll-\nment, audio\u2013text embedding, deep metric learning\n1. INTRODUCTION\nKeyword spotting (KWS) enables devices to detect specific trigger\nwords in speech, traditionally focusing on a small set of fixed key-\nwords (e.g., \u201cAlexa\u201d, \u201cOK Google\u201d, or \u201cHi Bixby\u201d) [1\u20134]. Recently,\nthere is increasing interest in flexible, open-vocabulary KWS that\nallows users to define arbitrary keywords [5\u201314]. In particular, text-\nbased enrollment, where a user types the desired trigger word or\nphrase instead of providing spoken examples, offers convenience\nand facilitates large-scale deployment, because new keywords can\nbe added by encoding the typed text, eliminating the need for spo-\nken enrollment samples [7\u201313]. This audio\u2013text scenario requires\na shared representation between speech and text so that the system\ncan decide whether an utterance contains the enrolled keyword by\ncomparing their embeddings.\nIn text-enrolled open-vocabulary KWS, two matching paradigms\nare commonly discussed: (i) utterance-level matching, which com-\npares pooled audio and text embeddings for an entire word or\nshort phrase, and (ii) phoneme-level matching, which aligns acous-\ntic and textual sequences at a finer temporal resolution using at-\ntention [9, 11], dynamic programming [10], or CTC-style align-\nment [13]. This work focuses on utterance-level matching because\nit requires no sequence-alignment module and supports open-set\nretrieval with a single pooled embedding per modality, an attractive\nproperty for low-latency, on-device KWS. Extending our approach\nto phoneme-level matching is left for future work.\nMotivated by Matryoshka Representation Learning (MRL) [15],\nwe propose Matryoshka Audio\u2013Text Embeddings (MATE), a dual-\nencoder framework that learns nested sub-embeddings (hereafter,\nprefixes) formed by selecting the leading portion of a single em-\nbedding vector. By gradually extending this selected portion, the\nmodel represents multiple embedding granularities within a single\nvector. In particular, lower-dimensional prefixes capture condensed\nkeyword information, while higher-dimensional prefixes add fine-\ngrained detail. Training uses standard deep metric learning (DML)\n[16] objectives for audio\u2013text KWS. In our experiments, we adopt\nRelational Proxy Loss (RPL) [12] as the main loss. Yet, MATE is\nloss-agnostic and consistently improves AP across diverse DML ob-\njectives (Sec. 4.3).\nSummarizing, this paper makes two contributions.\nFirst,\nwe introduce MATE, to our knowledge the first application of\nmatryoshka-style representation learning to KWS, enabling multi-\nple embedding granularities within a single forward pass from one\ndual-encoder. Second, we propose a PCA-guided prefix alignment\nthat treats PCA-compressed text sub-embeddings as teachers and\naligns both acoustic and text prefixes with a delayed schedule; this\nmitigates cross-scale interference observed with naive per-prefix su-\npervision and yields consistent gains on WSJ and LibriPhrase under\nstandard DML objectives, without changing inference cost.\n2. RELATED WORK\nText-enrolled utterance-level KWS. In utterance-level matching,\nan acoustic encoder and a text encoder are trained with DML so\nthat audio/text embeddings from the same keyword are pulled to-\ngether while those from different keywords are pushed apart, en-\nabling open-set retrieval. Early multi-view acoustic word embed-\ndings relied on triplet losses [7]. Proxy objectives remain effec-\ntive, including Asymmetric Proxy loss (AsyP) [8] and its adaptive\nmargin-and-scale variant AdaMS [17]. RPL adds cross-modal struc-\ntural alignment and reports strong gains on WSJ [12]. ADML com-\nbines modality-adversarial training with DML to reduce the cross-\nmodal gap [14]. Under proxy-based training, the text embedding of\neach keyword acts as a class proxy/centroid that shapes the acoustic\nspace.\nMatryoshka-style embeddings. MRL [15] encodes multiple gran-\nularities within a single vector by using nested sub-embeddings\n(\u201cprefixes\u201d), so low-dimensional prefixes capture compact cues\nwhile larger prefixes add detail, enabling multiple embedding di-\nmensionalities without extra encoder passes. In speaker verification,\nM-Vec [18] and DSM-GS [19] show that compact prefixes can main-\ntain accuracy and benefit from dimension-specific training. ESE [20]\nadopts a two-stage recipe, learn-to-express and learn-to-compress,\nto scale model depth and embedding size. Prior observations in-\ndicate that naively supervising all prefixes with the main loss can\ninduce competition across prefix sizes, whereas compression-driven\nordering of features tends to stabilize multi-embedding training [20].\narXiv:2601.14012v1  [eess.AS]  20 Jan 2026\nInput\nAudio\n\ud835\udc82\nInput\nText\n\ud835\udc95\n\ud835\udc53\ud835\udc4e(\u2219)\n\ud835\udc53\ud835\udc61(\u2219)\n\ud835\udc54\ud835\udc4e(\u2219)\n\ud835\udc54\ud835\udc61(\u2219)\n\ud835\udc37\n\ud835\udc47\ud835\udc4e\n\ud835\udc6c\ud835\udc4e\nphn\n\ud835\udc47\ud835\udc61\n\ud835\udc6c\ud835\udc61\nphn\n\ud835\udc6c\ud835\udc61\nutt = \ud835\udc96\ud835\udc61\n\ud835\udc6c\ud835\udc4eutt = \ud835\udc96\ud835\udc4e\n\ud835\udc37\n\ud835\udc37\n\ud835\udc37\nCentering\nPCA Compression\n\u0d25\ud835\udc96\ud835\udc61\n\u0de5\ud835\udc96\ud835\udc61\n\ud835\udc58\n\u0de5\ud835\udc96\ud835\udc61\n2\n\u0de5\ud835\udc96\ud835\udc61\n1\n\u0de5\ud835\udc96\ud835\udc61\n3\n\ud835\udc96\ud835\udc61\n\ud835\udc37\n\u2112main\n\u2112align,a\n3\n\u2112align,a\n2\n\u2112align,a\n1\nMatryoshka sub-embeddings\nTraining corpus\n\ud835\udc513\n\ud835\udc512\n\ud835\udc511\n\ud835\udc96\ud835\udc4e3\n\ud835\udc513\n\ud835\udc96\ud835\udc4e2\n\ud835\udc512\n\ud835\udc96\ud835\udc4e1\n\ud835\udc511\n\u0d25\ud835\udc68\ud835\udc61\n\ud835\udc37\n\ud835\udc37\n\ud835\udc37\n\ud835\udc68\ud835\udc61\n\ud835\udc51\ud835\udc58\n\ud835\udc37\n\ud835\udc51\ud835\udc58\n\ud835\udc96\ud835\udc61\n3\n\ud835\udc513\n\ud835\udc96\ud835\udc61\n2\n\ud835\udc512\n\ud835\udc96\ud835\udc61\n1\n\ud835\udc511\nEq. (1)\nEq. (2) \u2013 (3)\nEq. (4)\nEq. (5) \u2013 (9)\n\u0de5\ud835\udc96\ud835\udc61\n2\n\u0de5\ud835\udc96\ud835\udc61\n1\n\u0de5\ud835\udc96\ud835\udc61\n3\n\ud835\udc513\n\ud835\udc512\n\ud835\udc511\n\u2112align,t\n3\n\u2112align,t\n2\n\u2112align,t\n1\nPCA-compressed text sub-embeddings\n\ud835\udc51\ud835\udc58\n\ud835\udc37\nSVD\nFig. 1. Overview of MATE. A corpus-wide text inner-dependency\nmatrix \u00af\nAD\nt is estimated and factorized (SVD) to obtain projection\nheads {Adk\nt }. For each dk, the acoustic/text prefixes uk\na, uk\nt (lead-\ning dk-dim sub-vectors) are aligned to the PCA-compressed text\nsub-embedding \u02dcuk\nt ; the full-dim pair uses the main loss (RPL).\nFrom an embedding-learning standpoint, prior utterance-level KWS\nsystems typically learn a single fixed-dimensional embedding. We\ndepart from this single-granularity paradigm by training nested\nsub-embeddings that capture multiple information granularities. At\ninference, however, we use only the full-dim embedding, which is\nidentical to the approach in prior work.\n3. PROPOSED METHOD\n3.1. Overview and Notation\nFig. 1 depicts the overall MATE pipeline. Let B = {(ai, ti, yi)}N\ni=1\ndenote a mini-batch of audio, text, and keyword-label tuples. For a\nsample (a, t, y) \u2208B, the acoustic encoder fa(\u00b7) and the text encoder\nft(\u00b7) produce phoneme-level embedding sequences\nEphn\na\n= fa(a) \u2208RTa\u00d7D,\nEphn\nt\n= ft(t) \u2208RTt\u00d7D,\nwhere Ta and Tt are sequence lengths and D denotes the embed-\nding dimensionality. Following prior work [12, 14], utterance-level\nembeddings are obtained via pooling:\nEutt\na\n= ga(Ephn\na\n) \u2208RD,\nEutt\nt\n= gt(Ephn\nt\n) \u2208RD,\nwhere ga(\u00b7) is channel- and context-dependent statistics pooling\n(CCSP) [21] and gt(\u00b7) is global average pooling. Unless otherwise\nnoted, all embeddings are \u21132-normalized.\nNested sub-embeddings (i.e., prefixes) are defined by taking the\nleading dimensions of a single vector. Let D = {d1, . . . , dK} be the\nset of prefix dimensionalities with d1 < \u00b7 \u00b7 \u00b7 < dK = D. Following\nthe power-of-two (halving) schedule [15],\ndk = D \u00b7 2\u2212(K\u2212k),\nk = 1, . . . , K,\nso that D = {D/2K\u22121, . . . , D/2, D}. Given ua \u225cEutt\na\n\u2208RD\nand ut \u225cEutt\nt\n\u2208RD, the prefixes are uk\na = ua[1 : dk] and uk\nt =\nut[1:dk].\nWe denote by Lmain the utterance-level audio\u2013text objective\napplied to the full-dim pair (ua, ut); it is instantiated later in\nSec. 3.2. In our experiments, we fix D=256 and vary the num-\nber of prefix sizes K \u2208{2, 3, 4, 5}; for example, K=5 yields\nD = {16, 32, 64, 128, 256}.\n3.2. Matryoshka Audio\u2013Text Embeddings (MATE)\nWe motivate MATE with an information squeeze perspective: the\nshared audio\u2013text space should arrange keyword evidence so that\nlower-dimensional prefixes carry condensed, high-salience cues,\nwhile progressively larger prefixes add fine-grained detail. In proxy-\nbased DML, text embeddings act as representative vectors for each\nkeyword.\nMATE concentrates essential textual information into\nthe leading dimensions of the acoustic embedding by aligning both\nacoustic and text prefixes to PCA-compressed text sub-embeddings\nderived from the full text embedding.\nCorpus-wide dependency and PCA compression. Let \u00b5D\nt \u2208RD\nbe the mean of full text embeddings over the training corpus, and let\n\u00afut = ut\u2212\u00b5D\nt denote the centered text embedding. Motivated by the\nself-attention mechanism [22], we form an inner-dependency matrix\nvia a scaled dot-product with row-wise softmax. For numerical sta-\nbility and efficiency, we estimate a training-corpus-wide dependency\nat the beginning of each epoch e using the current text encoder:\n\u00af\nAD\nt\n= Softmaxrow\n1\nM\nM\nX\nj=1\n\u00afu(j)\nt\n\u00afu(j)\u22a4\nt\n\u221a\nD\n!\n\u2208RD\u00d7D,\n(1)\nwhere {\u00afu(j)\nt }M\nj=1 are centered full text embeddings over the training\nset. We then compute its singular value decomposition (SVD):\n\u00af\nAD\nt\n= U \u03a3 V \u22a4,\n(2)\nwhere U, V \u2208RD\u00d7D are orthogonal and \u03a3 \u2208RD\u00d7D is diagonal\nwith nonnegative singular values in descending order along the di-\nagonal. For a target prefix dimensionality dk (1 \u2264dk \u2264D), the\ntop-dk principal dependency components are defined as\nAdk\nt\n= U:,1:dk \u03a31:dk, 1:dk \u2208RD\u00d7dk.\n(3)\nApplying Adk\nt\nto the centered full text embedding yields the PCA-\ncompressed text sub-embedding:\n\u02dcuk\nt =\n\u0010\nAdk\nt\n\u0011\u22a4\n\u00afut \u2208Rdk.\n(4)\nFor brevity, we refer to these SVD-based projections as \u201cPCA-\ncompressed,\u201d since they play the role of principal, variance-ordered\ndirections for prefix targets in practice. Intuitively, \u02dcuk\nt collects the\ndk most influential dependency directions of ut.\nTo inject this\nsqueezed information into the leading dimensions of ua and ut, we\ndefine the alignment objectives below.\nPrefix alignment losses. For each prefix size dk \u2208D\\{D} (i.e., for\nall nested sub-embeddings except the full dim D), we align the PCA-\ncompressed text sub-embedding \u02dcuk\nt (teacher) with both the acoustic\nprefix uk\na and the text prefix uk\nt (students). This encourages (i) the\nleading acoustic coordinates to absorb the compressed, high-salience\nkeyword cues, and (ii) the compressed text proxy to remain consis-\ntent with the geometry of the original text prefix at the same dimen-\nsionality. Let \u03d5\u03c4(x) = Softmax(x/\u03c4) be a temperature-softened\nsoftmax. Each alignment combines an MSE term and a KL diver-\ngence between softened distributions, motivated by [23]:\nLk\nalign,a = MSE\n\u0010\nuk\na, \u02dcuk\nt\n\u0011\n+ KL\n\u0010\n\u03d5\u03c4(uk\na)\n\u03d5\u03c4(\u02dcuk\nt )\n\u0011\n,\n(5)\nLk\nalign,t = MSE\n\u0010\nuk\nt , \u02dcuk\nt\n\u0011\n+ KL\n\u0010\n\u03d5\u03c4(uk\nt )\n\u03d5\u03c4(\u02dcuk\nt )\n\u0011\n,\n(6)\nLk\nalign = Lk\nalign,a + Lk\nalign,t.\n(7)\nTable 1. WSJ AP (%) under RPL with four strategies: RPL (full-only), no matryoshka prefixes; Per-prefix RPL (ours), apply the main\nloss (RPL) to each nested sub-embedding; Per-prefix RPL + PCA-guided alignment (ours), apply both per-prefix RPL and PCA-guided\nalignment on the same prefixes; MATE (ours), apply RPL only to the full embedding and PCA-guided alignment to prefixes. \u2206full =\nAPmethod \u2212APfull-only, \u2206per-prefix = APmethod \u2212APPer-prefix RPL (percentage points, pp).\nMethod\nPer-prefix RPL\nPCA-guided alignment\nAP (%)\n\u2206full\n\u2206per-prefix\nRPL (full-only)\n\u2013\n\u2013\n78.66\n\u2013\n\u2013\nPer-prefix RPL (ours)\n\u2713\n\u2013\n79.49\n+0.83\n\u2013\nPer-prefix RPL + PCA-guided alignment (ours)\n\u2713\n\u2713\n78.01\n-0.65\n-1.48\nMATE (ours)\n\u2013\n\u2713\n80.94\n+2.28\n+1.45\nDefine K = {1, . . . , K \u22121}. The total alignment is then\nLalign =\nX\nk\u2208K\nLk\nalign.\n(8)\nFinal objective and schedule.\nLet Lmain be the utterance-level\naudio\u2013text objective on the full pair (ua, ut) (instantiated as RPL\nin our experiments). The final loss is\nLtotal = Lmain + \u03bbalign Lalign.\n(9)\nWe use a delayed schedule\n\u03bbalign(e) =\n(\n0,\n1 \u2264e \u226420,\n0.5,\ne \u226521,\n(10)\nwhere e is the epoch index. This lets the full text embedding first be-\ncome a reliable proxy/centroid under Lmain before the compression-\nbased alignment starts steering the prefixes.\nEpoch routine. At the start of each epoch, we recompute the corpus\nstatistics with the current text encoder ft: (i) the corpus mean \u00b5D\nt ,\n(ii) the corpus-wide dependency \u00af\nAD\nt via Eq. (1), and (iii) the projec-\ntion heads {Adk\nt }K\u22121\nk=1 by SVD as in Eqs. (2)\u2013(3). During training,\nfor each mini-batch we obtain PCA-compressed targets on the fly by\napplying Eq. (4) to the batch\u2019s centered text embeddings \u00afut, and then\ncompute the prefix alignment losses in Eqs. (5)\u2013(7). The alignment\nterm is weighted according to the schedule in Eq. (10).\n4. EXPERIMENTS\n4.1. Experimental Setup\nDatasets and evaluation protocol.\nWe train on King-ASR-066\n[24] following the setup of [12], including the same augmentation\nrecipe. This yields roughly 4.6k hours of word-level segments cov-\nering about 210k word classes. For evaluation, we use Wall Street\nJournal (WSJ) [25] and LibriPhrase [9]. WSJ trials are constructed\nas in [12]: 3.2k distinct words and 18k word segments, further cor-\nrupted with RIRs [26] and MUSAN noises [27].\nThe WSJ test\ncomprises 18k positive pairs and 50\u00d7 more negatives to emulate\nreal-world imbalance; we therefore report Average Precision (AP).\nLibriPhrase follows its official splits and metrics (Equal Error Rate,\nEER; Area Under the ROC Curve, AUC) on LPE and LPH.\nInputs and encoders. Acoustic features are 40-d log-Mel filter-\nbanks with 25 ms windows, 10 ms frame shift, and utterance-level\nmean normalization. The acoustic encoder is a 256-channel ECAPA-\nTDNN [21] (\u223c1.8M parameters), a standard choice in recent KWS\nworks [4,12,14,28]. Text is tokenized with a grapheme-to-phoneme\n(G2P) front end [29] and embedded via a trainable 256-d lookup\ntable; a 2-layer bi-LSTM (256 hidden units), followed by global av-\nerage pooling and a linear layer, produces a 256-d text embedding.\nOptimization, batching, and inference.\nAll models are imple-\nmented in PyTorch and optimized with AdamW (initial learning rate\n10\u22124; weight decay 10\u22125) for 100 epochs. Each mini-batch con-\ntains 500 utterances from 250 keywords (two utterances per key-\nword). The main utterance-level objective is RPL with the same hy-\nperparameters as [12]. For the MATE alignment, we use temperature\n\u03c4=1 in Eqs. (5)-(7) and apply the loss-weight schedule for \u03bbalign as\nin Sec. 3.2. Inference computes cosine similarity between the full-\ndim utterance-level embeddings ua and ut. Training and evaluation\nrun on two H100 GPUs (about one day for 100 epochs). By de-\nfault we set D=256 and specify the matryoshka prefix set D (i.e., K\nand {dk}) per experiment. For baseline objectives (e.g., Proxy-BD\n(Binomial Deviance) [30], Proxy-MS (Multi-Similarity) [16], CLAT\n[31], AsyP [8], AdaMS [17], and RPL [12]), unless otherwise stated\nwe follow the settings in their original papers.\n4.2. Multi-scale supervision under RPL\nWe first analyze different multi-scale supervision schemes under the\nRPL main loss. Table 1 compares four strategies: (i) a baseline RPL\n(full-only) model without nested sub-embeddings; (ii) Per-prefix\nRPL, which applies the main loss to each prefix; (iii) a combination\nof per-prefix RPL and our PCA-guided alignment; and (iv) MATE,\nwhich reserves RPL for the full embedding and uses PCA-guided\nalignment only on prefixes.\nAcross these variants, two of our multi-scale approaches, Per-\nprefix RPL and MATE, outperform the full-only baseline, indicat-\ning the benefit of leveraging nested sub-embeddings during training.\nPer-prefix RPL yields a modest gain over full-only (+0.83 pp), while\nMATE provides the largest improvement (+2.28 pp) and surpasses\nPer-prefix RPL by +1.45 pp. In contrast, combining Per-prefix RPL\nwith PCA-guided alignment underperforms even the full-only base-\nline (\u22120.65 pp), suggesting conflicting supervision when both ob-\njectives are imposed on the same prefixes. We hypothesize that Per-\nprefix RPL pushes every prefix to be fully discriminative, whereas\nPCA-guided alignment pulls them toward compressed proxies em-\nphasizing variance ordering, increasing interference and optimiza-\ntion difficulty. By reserving the main loss for the full vector and us-\ning compression-guided targets as prefix regularizers, MATE avoids\nthese conflicts and achieves the best AP.\n4.3. WSJ main results\nTable 2 reports WSJ AP (%) for six utterance-level objectives\u2014\nProxy-BD and Proxy-MS (reformulations of BD and MS [8]),\nCLAT, AsyP, AdaMS, and RPL\u2014and their MATE-enhanced coun-\nterparts. All systems share the same data, encoders, and batching;\nonly the utterance-level objective (Lmain) differs. We set K=5 with\nTable 2. WSJ test AP (%). Baselines vs. MATE. MATE uses K=5\nwith D = {16, 32, 64, 128, 256}. \u2206denotes absolute change in AP\n(percentage points): \u2206= APMATE \u2212APbaseline.\nObjective (utterance-level)\nBaseline AP\nMATE AP\n\u2206\nProxy-BD [30]\n69.76\n72.95\n+3.19\nProxy-MS [16]\n70.18\n73.12\n+2.94\nCLAT [31]\n70.52\n73.53\n+3.01\nAsyP [8]\n71.66\n73.98\n+2.32\nAdaMS [17]\n73.87\n75.68\n+1.81\nRPL [12]\n78.66\n80.94\n+2.28\nTable 3.\nWSJ AP (%) for MATE (ours) with RPL as the main\nloss under varying numbers of prefixes K (same \u03bbalign schedule).\n\u2206full = AP \u2212APRPL(full-only) in percentage points (pp).\nConfiguration\nAP (%)\n\u2206full (pp)\nRPL (full-only), K=1 : {256} (no prefixes)\n78.66\n\u2013\nMATE (ours), K=2 : {128, 256}\n79.02\n+0.36\nMATE (ours), K=3 : {64, 128, 256}\n81.03\n+2.37\nMATE (ours), K=4 : {32, 64, 128, 256}\n79.38\n+0.72\nMATE (ours), K=5 : {16, 32, 64, 128, 256}\n80.94\n+2.28\n{16, 32, 64, 128, 256} for the main comparison, following common\npractice in prior work [19], before ablating this choice in Sec. 4.4.\nMATE improves AP across all objectives (avg +2.59 pp; +1.81\u2013\n+3.19), with the largest gain on Proxy-BD (+3.19 pp).\nEven the\nstrongest baseline, RPL, rises to 80.94% AP (+2.28 pp), confirm-\ning that nested sub-embeddings complement diverse metric-learning\nobjectives (relative +2.4\u20134.6%).\n4.4. Effect of the number of prefixes K\nFor the main WSJ comparison, we set K=5 following prior practice\nin matryoshka-style encoders (e.g., DSM-GS [19]), and we addition-\nally ablate K while keeping D=256 and the RPL main loss fixed.\nTable 3 reports WSJ AP. All multi-scale MATE settings outperform\nthe RPL (full-only) baseline.\nThe best AP is obtained with K=3 ({64, 128, 256}). Moving\nto K=4 reduces AP, consistent with the intuition that very small di-\nmensions (32-d) can limit discriminative capacity, a tendency also\nnoted in a different speech task [19]. Adding a 16-d prefix (K=5)\nraises AP relative to K=4; we conjecture that the extra coarse pre-\nfix distributes alignment pressure across the prefixes and alleviates\nover-regularization at 32-d. This demonstrates the general robust-\nness of the multi-prefix approach, as the performance at K=5 re-\nmains highly competitive and only slightly below the peak at K=3\n(80.94% vs. 81.03%). In further experiments, we observed no addi-\ntional AP gains for K \u22656.\n4.5. Effect of alignment composition\nWe ablate alignment composition by varying the weights between\nMSE and temperature-softened KL in Eqs. (5)\u2013(7). Unless noted, all\nruns use the same \u03bbalign schedule and K=3 prefixes {64, 128, 256}\nwith temperature \u03c4=1. Table 4 shows that the equal combination\n(1:1) yields the best AP on WSJ.\nThe two terms provide complementary signals: MSE enforces\nproximity in Euclidean space, while the softened KL divergence\naligns the relative importance of each dimension. Combining them\nyields more robust improvements than using either term alone,\nTable 4.\nWSJ AP (%) for MATE (ours) under different align-\nment compositions (MSE:KL) with RPL main loss, K=3 prefixes\n{64, 128, 256}, and the same \u03bbalign schedule.\n\u2206MSE = AP \u2212\nAPMSE-only, \u2206KL = AP \u2212APKL-only in percentage points (pp).\nMSE:KL\nAP (%)\n\u2206MSE (pp)\n\u2206KL (pp)\n1:0 (MSE only)\n79.96\n\u2013\n\u2013\n0:1 (KL only)\n80.04\n+0.08\n\u2013\n1:1 (proposed)\n81.03\n+1.07\n+0.99\nTable 5. LibriPhrase results. EER (%) and AUC (%) on LPE (easy)\nand LPH (hard). All systems are trained on King-ASR-066.\nMethod\nEER (%)\nAUC (%)\nLPE\nLPH\nLPE\nLPH\nCMCD [9]\n4.20\n25.94\n99.12\n81.14\nPhonMatchNet [11]\n2.33\n24.11\n99.70\n83.39\nRPL [12]\n1.54\n22.45\n99.84\n85.31\nADML (w/o SF2) [14]\n1.43\n21.04\n99.84\n86.91\nMATE (ours)\n1.38\n20.06\n99.86\n88.70\nwhich supports our goal of using PCA-guided targets to concentrate\nsalient information into the prefixes.\n4.6. Performance on LibriPhrase\nTable 5 compares EER (%) and AUC (%) on LibriPhrase\u2019s easy\n(LPE) and hard (LPH) splits.\nTo assess cross-corpus generaliza-\ntion, all systems are trained on King-ASR-066 and evaluated on\nLibriPhrase without using its training set. For a fair comparison,\nall systems utilize the same acoustic and text encoders as MATE.\nFurthermore, to isolate the effect of the DML objective, we re-\nimplement ADML [14] without the SphereFace2-based auxiliary\nloss (SF2) used in its original implementation. MATE adopts RPL\nas its main loss (K=3), consistent with the best-performing WSJ\nmodel. Lower EER and higher AUC are better.\nRelative to ADML (w/o SF2), MATE reduces EER by \u22120.05 pp\non LPE (1.38 vs. 1.43) and by \u22120.98 pp on LPH (20.06 vs. 21.04),\nand increases AUC by +0.02 pp on LPE (99.86 vs. 99.84) and\n+1.79 pp on LPH (88.70 vs. 86.91).\nThese cross-corpus gains\nmirror WSJ trends, indicating that matryoshka-style nested sub-\nembeddings with PCA-guided alignment improve generalization in\ntext-enrolled KWS.\n5. CONCLUSION\nWe presented Matryoshka Audio\u2013Text Embeddings (MATE), a dual-\nencoder framework that, to our knowledge, applies matryoshka-style\nrepresentations to KWS for the first time. MATE organizes keyword\ninformation across nested sub-embeddings by placing the main\nmetric-learning loss only on the full embedding and aligning both\nacoustic and text prefixes to PCA-compressed text sub-embeddings\nwith a delayed weight schedule.\nAcross benchmarks (WSJ and\nLibriPhrase), MATE improved AP on WSJ and EER/AUC on Lib-\nriPhrase over prior utterance-level baselines. Ablations show that\nsupervising prefixes alone already outperforms a full-only baseline,\nhighlighting the value of nested sub-embeddings; using PCA-guided\ntargets without per-prefix main losses stabilizes multi-scale training\nand yields the best results. In future work, we will extend MATE\nto phoneme-level matching and explore replacing PCA with more\ndiscriminative compression such as LDA.\n6. REFERENCES\n[1] G. Chen, C. Parada, and G. Heigold, \u201cSmall-footprint keyword\nspotting using deep neural networks,\u201d in Proc. IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2014, pp. 4087\u20134091.\n[2] T. N. Sainath and C. Parada, \u201cConvolutional neural networks\nfor small-footprint keyword spotting,\u201d in Proc. Interspeech,\n2015, pp. 1478\u20131482.\n[3] R. Tang and J. J. Lin,\n\u201cDeep residual learning for small-\nfootprint keyword spotting,\u201d in Proc. IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP),\n2017, pp. 5484\u20135488.\n[4] Y. Jung, J. Lee, S. Lee, M. Jung, Y.-H. Lee, and H.-Y. Cho,\n\u201cText-aware adapter for few-shot keyword spotting,\u201d in Proc.\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2025, pp. 1\u20135.\n[5] G. Chen, C. Parada, and T. N. Sainath, \u201cQuery-by-example\nkeyword spotting using long short-term memory networks,\u201d in\nProc. IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2015, pp. 5236\u20135240.\n[6] J. Huang, W. Gharbieh, H. S. Shim, and E. Kim, \u201cQuery-by-\nexample keyword spotting system using multi-head attention\nand soft-triple loss,\u201d in Proc. IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2021,\npp. 6858\u20136862.\n[7] W. He, W. Wang, and K. Livescu, \u201cMulti-view recurrent neural\nacoustic word embeddings,\u201d in Proc. International Conference\non Learning Representations (ICLR), 2017.\n[8] M. Jung and H. Kim, \u201cAsymmetric proxy loss for multi-view\nacoustic word embeddings,\u201d in Proc. Interspeech, 2022, pp.\n5170\u20135174.\n[9] H.-K. Shin, H. Han, D. Kim, S.-W. Chung, and H.-G. Kang,\n\u201cLearning audio-text agreement for open-vocabulary keyword\nspotting,\u201d in Proc. Interspeech, 2022, pp. 1871\u20131875.\n[10] K. Nishu, M. Cho, and D. Naik, \u201cMatching latent encoding\nfor audio-text based keyword spotting,\u201d in Proc. Interspeech,\n2023, pp. 1613\u20131617.\n[11] Y.-H. Lee and N. Cho,\n\u201cPhonMatchNet: Phoneme-guided\nzero-shot keyword spotting for user-defined keywords,\u201d\nin\nProc. Interspeech, 2023, pp. 3964\u20133968.\n[12] Y. Jung, S. Lee, J.-Y. Yang, J. Roh, C. W. Han, and H.-Y. Cho,\n\u201cRelational proxy loss for audio-text based keyword spotting,\u201d\nin Proc. Interspeech, 2024, pp. 327\u2013331.\n[13] S. Jin, Y. Jung, S. Lee, J. Roh, C. Han, and H. Cho, \u201cCTC-\naligned audio-text embedding for streaming open-vocabulary\nkeyword spotting,\u201d in Proc. Interspeech, 2024, pp. 332\u2013336.\n[14] Y. Jung, Y.-H. Lee, M. Jung, J. Roh, C. W. Han, and H.-Y.\nCho, \u201cAdversarial deep metric learning for cross-modal audio-\ntext alignment in open-vocabulary keyword spotting,\u201d in Proc.\nInterspeech, 2025, pp. 2645\u20132649.\n[15] A. Kusupati et al.,\n\u201cMatryoshka representation learn-\ning,\u201d in Advances in Neural Information Processing Systems\n(NeurIPS), 2022.\n[16] X. Wang, X. Han, W. Huang, D. Dong, and M. R. Scott,\n\u201cMulti-similarity loss with general pair weighting for deep\nmetric learning,\u201d in Proc. IEEE/CVF Conf. on Computer Vi-\nsion and Pattern Recognition (CVPR), 2019, pp. 5022\u20135030.\n[17] M. Jung and H. Kim,\n\u201cAdaMS: Deep metric learning with\nadaptive margin and adaptive scale for acoustic word discrim-\nination,\u201d in Proc. Interspeech, 2023, pp. 3924\u20133928.\n[18] S. Wang, P. Zhu, and H. Li,\n\u201cM-Vec:\nMatryoshka\nspeaker embeddings with flexible dimensions,\u201d arXiv preprint\narXiv:2409.15782, 2024.\n[19] S. Park and H. S. Kim,\n\u201cDimension-specific margins\nand element-wise gradient scaling for enhanced matryoshka\nspeaker embedding,\u201d IEEE Access, vol. 13, pp. 45473\u201345487,\n2025.\n[20] X. Li, Z. Li, J. Li, H. Xie, and Q. Li, \u201cESE: Espresso sentence\nembeddings,\u201d in Proc. International Conference on Learning\nRepresentations (ICLR), 2025.\n[21] B. Desplanques, J. Thienpondt, and K. Demuynck, \u201cECAPA-\nTDNN: Emphasized channel attention, propagation and aggre-\ngation in TDNN-based speaker verification,\u201d arXiv preprint\narXiv:2005.07143, 2020.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all\nyou need,\u201d in Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2017.\n[23] T. Kim, J. Oh, N. Kim, S. Cho, and S.-Y. Yun, \u201cComparing\nKullback\u2013Leibler divergence and mean squared error loss in\nknowledge distillation,\u201d in Proc. Int. Joint Conf. on Artificial\nIntelligence (IJCAI), 2021, pp. 2628\u20132635.\n[24] DataOceanAI, \u201cKing-ASR-066,\u201d 2015.\n[25] D. B. Paul and J. M. Baker, \u201cThe design for the Wall Street\nJournal-based CSR corpus,\u201d in Proc. Workshop on Speech and\nNatural Language, 1992, pp. 357\u2013362.\n[26] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,\n\u201cA study on data augmentation of reverberant speech for robust\nspeech recognition,\u201d in Proc. IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2017,\npp. 5220\u20135224.\n[27] D. Snyder, G. Chen, and D. Povey,\n\u201cMUSAN: A music,\nspeech, and noise corpus,\u201d arXiv preprint arXiv:1510.08484,\n2015.\n[28] H. Li, B. Yang, Y. Xi, L. Yu, T. Tan, H. Li, and K. Yu, \u201cText-\naware speech separation for multi-talker keyword spotting,\u201d in\nProc. Interspeech, 2024, pp. 337\u2013341.\n[29] K. Park and J. Kim, \u201cg2p: Grapheme-to-phoneme converter,\u201d\nGitHub repository, 2019,\n[Online]. Available: https://\ngithub.com/Kyubyong/g2p.\n[30] D. Yi, Z. Lei, S. Liao, and S. Z. Li, \u201cDeep metric learning for\nperson re-identification,\u201d in Proc. International Conference on\nPattern Recognition (ICPR), 2014, pp. 34\u201339.\n[31] K. Li, H. Zhou, K. Shen, Y. Dai, and J. Du, \u201cPhoneme-level\ncontrastive learning for user-defined keyword spotting with\nflexible enrollment,\u201d arXiv preprint arXiv:2412.20805, 2024."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "This paper presents MATE (Matryoshka Audio\u2013Text Embeddings), a novel framework designed to improve open-vocabulary keyword spotting in speech recognition systems. The main contributions of this work are:\n\n### Main Contributions:\n1. **First Application**: This is the first application of matryoshka-style representation learning to open-vocabulary keyword spotting (KWS). MATE enables multiple embedding granularities within a single forward pass, unlike traditional methods that use fixed-dimensional embeddings.\n2. **PCA-Guided Prefix Alignment**: The paper introduces a PCA-guided prefix alignment strategy where the teacher targets are compressed text sub-embeddings. This approach mitigates cross-scale interference and consistently improves performance on datasets like WSJ and LibriPhrase without adding inference overhead.\n\n### Methodology:\n1. **Nested Sub-Embeddings (Prefixes)**: MATE uses nested sub-embeddings, or \"prefixes\", to capture multiple information granularities within a single vector. This allows for the representation of various embedding dimensions in a unified framework.\n2. **PCA Compression**: The text embeddings are compressed using PCA and used as teacher targets for aligning both acoustic and textual prefixes. This helps concentrate salient keyword cues into lower-dimensional prefixes while adding detail with higher-dimensional ones.\n3. **Training Objective**: MATE is trained using standard deep metric learning (DML) objectives, specifically Relational Proxy Loss (RPL). The main loss is applied to the full embedding vector, and a delayed schedule for prefix alignment is used.\n\n### Key Findings:\n1. **State-of-the-Art Performance**: MATE achieves state-of-the-art results on datasets like WSJ and LibriPhrase without any inference overhead. It outperforms prior utterance-level baselines by an average of +2.59 percentage points (pp) across all objectives.\n2. **Multi-Scale Supervision**: The paper analyzes different multi-scale supervision schemes under RPL, showing that MATE's combination of per-prefix RPL and PCA-guided alignment provides the largest improvement (+2.28 pp on Proxy-BD).\n3. **Generalization to LibriPhrase**: On the LibriPhrase dataset, MATE improves EER by -0.05 percentage points (pp) compared to ADML without SF2 auxiliary loss.\n4. **Effect of Prefixes and Alignment Composition**: Ablations show that supervising prefixes alone already outperforms a full-only baseline. The combination of MSE and temperature-softened KL terms in the alignment losses yields more robust improvements.\n\n### Conclusions:\n1. **Complementary Objectives**: MATE's use of PCA-guided targets to concentrate salient information into prefixes complements diverse metric-learning objectives.\n2. **Future Work**: The authors suggest extending MATE to phoneme-level matching and exploring alternative compression methods like LDA for future work.\n\nIn summary, this paper introduces a novel framework (MATE) that leverages matryoshka-style representations to improve open-vocabulary keyword spotting in speech recognition systems, achieving state-of-the-art results without adding inference overhead."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "VENI: Variational Encoder for Natural Illumination\nPaul Walker1\nJames A. D. Gardner2,3\nAndreea Ardelean1\nWilliam A. P. Smith2,3\nBernhard Egger1\n1Friedrich-Alexander-Universit\u00a8at Erlangen-N\u00a8urnberg\n2University of York\n3pxld.ai\nhttps://paul-pw.github.io/veni\nFigure 1. We build a rotation-equivariant variational autoencoder model to address the limitations of the current state of-the-art illumination\nprior (RENI++). Our model produces a better-behaved latent space, which we demonstrate by evaluating its uniqueness through optimization\nof random latent codes and its reconstruction consistency via the correlation between latent space and image space distances.\nAbstract\nInverse rendering is an ill-posed problem, but priors like\nillumination priors, can simplify it. Existing work either\ndisregards the spherical and rotation-equivariant nature\nof illumination environments or does not provide a well-\nbehaved latent space. We propose a rotation-equivariant\nvariational autoencoder that models natural illumination\non the sphere without relying on 2D projections. To pre-\nserve the SO(2)-equivariance of environment maps, we use\na novel Vector Neuron Vision Transformer (VN-ViT) as en-\ncoder and a rotation-equivariant conditional neural field\nas decoder. In the encoder, we reduce the equivariance\nfrom SO(3) to SO(2) using a novel SO(2)-equivariant fully\nconnected layer, an extension of Vector Neurons. We show\nthat our SO(2)-equivariant fully connected layer outper-\nforms standard Vector Neurons when used in our SO(2)-\nequivariant model. Compared to previous methods, our\nvariational autoencoder enables smoother interpolation in\nlatent space and offers a more well-behaved latent space.\n1. Introduction\nInverse rendering is ill-posed, an image can be created by\nmultiple combinations of shape, material and lighting [5, 27].\nTo solve this, humans draw on strong, learned priors over the\nspace of possible illuminations [30], especially a lighting-\nfrom-above prior [30, 39, 42], and it has been shown that\nhumans perform better on inverse rendering tasks under nat-\nural illumination conditions, with performance degrading\nunder artificial illumination [17]. Whilst natural illumination\nis complex and challenging to model, it displays statistical\nregularities [13], particularly in outdoor scenes. For exam-\nple, the strongest sources of illumination are the sun and\nthe skylight, which produce only a limited range of colors.\nAdditionally, illumination environments have a canonical up\ndirection, with any rotation about the vertical direction being\nequally likely.\nHowever, instead of relying on learned priors for illumi-\nnation, it is still common in inverse rendering to reconstruct\nthe lighting directly [4, 48, 49, 57] or to rely on simple sta-\ntistical models of spherical harmonics parameters of custom\ndatasets [2, 3, 14, 52].\nRecently, there has been some work to build natural il-\nlumination priors using spherical neural fields [7, 19, 20].\nSuch continuous representations are useful in an inverse ren-\ndering setting [21, 24] since the incident illumination can be\nqueried from any direction. Some of these models [19, 20]\nexploit the vertical rotation equivariance inherent to illumi-\nnation environments, leading to a more efficient model and\nnegating the need for rotation augmentation during training.\nHowever, they are constructed as decoder-only architectures\ndue to the difficulty of building a rotation equivariant encoder.\nFor example, RENI++ [20] employs an autodecoder archi-\narXiv:2601.14079v1  [cs.CV]  20 Jan 2026\ntecture [31], in which latent codes are randomly initialized\nfor each training and testing image and jointly optimized\nwith the model. This leads to two key limitations. First,\nsimilar images are often initialized with completely different\nlatent codes, which can cause the model to represent the\nsame image multiple times within its latent space, i.e., the\nlatent space is not unique. Second, the training cannot scale\nto large datasets since a latent code must be optimized per-\nimage and the latent space uniqueness further degrades as\nlarge numbers of similar images are initialized with different\nlatent codes.\nTo overcome these two limitations while retaining rota-\ntion equivariance and the benefits of a neural field represen-\ntation, we propose a novel rotation-equivariant variational\nautoencoder architecture for spherical signals. Following the\nprinciples of the Vector Neuron [11] framework, we design\na novel vision transformer encoder that computes the latent\ncodes, which are then mapped back to the image space via\na neural field decoder. This enables us to use much larger\ndatasets, as we no longer need to explicitly optimize a la-\ntent code for each image, but instead encode the image to a\nlatent code via a forward pass through our rotation equivari-\nant encoder. By construction, encoding two similar images\nnow leads to two similar latent codes, thereby addressing the\nweakness of RENI++ and improving the uniqueness of the\nlatent space. This enables smooth illumination transitions\nthrough latent space interpolation.\nTo summarize, our contributions are:\n\u2022 A rotation-equivariant panoramic vision transformer en-\ncoder enabled by a novel SO(2)-equivariant extension to\nVector Neurons\n\u2022 A rotation-equivariant natural illumination prior with a\nwell-behaved, unique latent space\n\u2022 Extensive evaluation of latent space uniqueness and recon-\nstruction consistency using intuitive metrics\n2. Related Work\nLighting Representations.\nThe lighting at any point in\n3D space can be represented as a spherical signal that defines\nthe incident light intensity and color from every direction. In\ncomputer graphics, it is often assumed that the illumination\nenvironment is the same across all points in the scene, i.e., it\nmodels distant illumination. The illumination environment\nis often modeled by an environment map [34, 44], which\nis a projection of the spherical signal onto a 2D image via\nequirectangular projection or cube mapping [23]. However,\nboth projections introduce some level of distortion, resulting\nin irregular sampling with respect to the original spherical\nimage. Since available datasets are typically provided as\nequirectangular projected images [19, 22, 29], the associated\nsampling issues need to be addressed during training.\nAlternative representations for illumination environments\nused in inverse rendering problems are Spherical Harmonics\n(SH) [2, 4, 14, 34, 52] and Spherical Gaussians (SG) [44,\n49, 56, 57]. Since both express illumination environments\nas spherical signals, they neither introduce distortion nor\nsuffer from irregular sampling. Still, Boss et al. [7] and\nGardner et al. [20] have shown that neural field-based light\nmodeling can capture more detailed environments using\nfewer parameters than SG or SH, so we follow their approach\nfor our decoder.\nIllumination Priors.\nMany inverse rendering works do not\nrely on illumination priors and instead reconstruct lighting di-\nrectly [4, 48, 49, 57]. This often leads to unrealistic lighting\nestimations and hinders the estimation of other parameters,\nsuch as albedo. Since real world illumination exhibits some\nregularities [13], illumination priors have been shown to help\nconstrain the inverse rendering task [21].\nRecent works in light estimation focus on outpainting a\n360\u00b0 HDR environment map from an LDR crop to estimate\nthe lighting in a scene [10, 47, 55]. This introduces an im-\nplicit prior on the illumination. Zhan et al. [55] and Dastjerdi\net al. [10] first predict the location of the light sources from\nthe LDR crop using SG and then use a generative adversarial\nnetwork (GAN) to reconstruct the full HDR image based on\nthe LDR crop and the light source prediction. Wang et al.\n[47] train a two branch StyleGAN [25] to generate LDR and\nHDR panoramas. Using this, they propose a GAN inversion\nmethod to find the latent code of an LDR crop to predict the\nHDR panorama. Phongthawee et al. [32] propose to inpaint\na chrome ball into a scene to estimate the lighting instead\nof directly outpainting a 360\u00b0 panorama. This can also be\nseen as an outpainting method, since the inpainted chrome\nball shows what is behind the camera. While these models\ncan reconstruct high quality illumination environments, they\nrequire that a part of the illumination environment is visible\nin the scene. Without which, these models cannot be used as\nan illumination prior.\nEarlier works that do not have this limitation pro-\nposed statistical models over SH parameters of custom\ndatasets [2, 3, 14, 52], while more recent works rely on\nneural approaches [7, 20, 40]. Sztrajman et al. [40] build an\nenvironment map convolutional autoencoder and Boss et al.\n[7] use a neural field to model pre-integrated lighting. Clos-\nest to our approach is RENI++ [20], which proposes a natu-\nral illumination prior that is rotation-equivariant by design.\nRENI++ uses an autodecoder architecture [31], meaning\nthat latent codes are initialized randomly per training/testing\nimage and then jointly optimized with the model [20]. This\napproach results in a latent space capable of representing\nan image by multiple, different latent codes, i.e., the latent\nspace is not unique. This adversely affects downstream\nperformance. Our autoencoder approach overcomes this\nlimitation, leading to a more well-behaved latent space.\n2\nRotation Equivariance.\nLighting environments are inher-\nently rotation-equivariant. Any rotation around the up-axis\nresults in another valid illumination environment. This prop-\nerty has been largely overlooked in priors for natural illu-\nmination [3, 10, 14, 32, 47, 55], while other works try to\nachieve it via data augmentation [40, 52]. Rotation equiv-\nariance in general is an important property in computer vi-\nsion [8]. Some methods design rotation-equivariant models\nusing convolutions with steerable kernels [9, 33, 41, 50].\nAnother approach is proposed by SE(3) Transformer [18],\nwhich introduces a rotation-equivariant attention mecha-\nnism by combining attention [45] with tensor field convo-\nlution [41]. Steerable kernels and SE(3) transformers are\nrestricted to convolutions, which limits their applicability to\nnon-convolutional models.\nA more general framework for building SO(3)-equivariant\nneural networks is Vector Neurons [11]. By providing basic\nSO(3)-equivariant building blocks, such as linear and ReLU\nlayers, it enables the design of a wide range of equivariant\nmodel architectures. For example, VN-Transformer [1] ap-\nplies the Vector Neuron framework to transformer models.\nWe rely on VN-Transformer to achieve rotation equivariance\nin our encoder. The Vector Neurons approach was also used\nby RENI and RENI++ to build a rotation-equivariant prior\nfor natural illumination [19, 20]. While RENI uses the full\nGram matrix, to achieve rotation equivariance at the cost\nof O(n2) complexity relative to the size of the latent space,\nRENI++ uses the VN-Invariant layer [11], reducing com-\nplexity to O(n) [20]. We follow the approach of RENI++ to\nobtain a rotation-equivariant decoder.\nPanoramic Vision Transformers.\n360\u00b0 panoramic images\npresent inherent challenges, as the 2D projection of such data\n(e.g., equirectangular projection, cube mapping) inevitably\nintroduces distortion. Based on the transformer architec-\nture [45], Vision Transformers use global self-attention to\nsolve many computer vision tasks [12], but standard Vision\nTransformers are not designed to handle distortion. Shen\net al. [37] propose the use of sphere tangent-patches to re-\nmove the negative effects of distortion when using vision\ntransformers with 360\u00b0 equirectangular projected images,\nwhile other methods address it via deformable convolu-\ntions [53, 54, 58]. Yun et al. [54] use deformable convo-\nlutions with fixed offsets as linear projection into the trans-\nformer encoder. Zhao et al. [58] propose a distortion-aware\ntransformer block that uses deformable convolution with\nlearnable offsets. Yuan et al. [53] use deformable convolu-\ntions with learnable offsets in the output projection of a 360\u00b0\nimage segmentation model.\nAll of these methods address the distortion introduced\nwhen mapping the spherical 360\u00b0 image to 2D, but they\ndisregard the true spherical 3D nature of panoramic im-\nages. Consequently, all of them must explicitly account\nFigure 2. Our model is rotation-equivariant, meaning that a rotation\nof the input environment map leads to a corresponding rotation in\nlatent space and in the reconstructed environment map.\nfor such distortions. In contrast, we operate directly in the\n3D domain and build a vision transformer upon the VN-\nTransformer [1]. This approach eliminates the need for dis-\ntortion handling, as no projection from 3D to 2D is involved.\nThe technique we use is closest to early fusion, as proposed\nin VN-Transformer [1].\n3. Methods\nOur goal is to learn a prior for natural illumination with a\nstructured latent space, which will make operations in the\nlatent space (e.g. interpolations) more semantically meaning-\nful. Building on RENI++ [20], we address their architectural\nissues by implementing a rotation equivariant variational\nautoencoder that lets the model implicitly learn a structured\nlatent space. We adopt the RENI++ decoder and introduce a\nnovel Vector Neuron Vision Transformer encoder.\nSO(2)-equivariance.\nA model is rotation-equivariant if a\nrotation of the input induces the same rotation of the output,\nas visualized in Figure 2. For natural, outdoor illumination,\nwhere there is always a well-defined horizon line and up-\naxis, only rotations around the up-axis lead to other realistic\nillumination environments. Rotations around other axes will\nresult in unrealistic illumination environments. Therefore,\nwe design an SO(2)-equivariant model.\nSO(2)-equivariant fully connected layer.\nVector Neu-\nrons, as proposed by Deng et al. in [11], provide a framework\nfor SO(3)-equivariant networks. We deal with multidimen-\nsional data (i.e., direction and color), but only desire equiv-\nariance in the x and y dimensions (i.e., SO(2)-equivariance).\nRotations around any other axis than the up-axis, particu-\nlarly rotations of the color vectors, produce unrealistic illu-\nmination environments. Therefore, these dimensions should\nremain invariant to rotations in the xy-plane. Since both the\ninvariant dimensions and the equivariant dimensions contain\nimportant information, we aim for them to influence each\nother while preserving rotation equivariance in the x and y\ndimensions and invariance in the other dimensions.\n3\nWe extend Vector Neurons by combining invariant op-\nerations and equivariant operations in one neuron, re-\nsulting in SO(2)-equivariance.\nGiven the input, X =\n[Xeq, Xinv] \u2208Rdin\u00d7(2+cinv), consisting of the equivariant\ncomponent, Xeq \u2208Rdin\u00d72, in our case, the x and y dimen-\nsions and the invariant component, Xinv \u2208Rdin\u00d7cinv, where\ncin = 4 is the number of invariant input dimensions, here,\nthe z and color dimensions, and din is the number of inputs\nto the fully connected layer, we arrive at the intermediate\nvalues:\nTinv = [Xinv, 1din\u00d71] \u2208Rdin\u00d7(cinv+1)\n(1)\nT\u2032\ninv = [Xinv, \u2225Xeq\u2225] \u2208Rdin\u00d7(cinv+1)\n(2)\nWhere the concatenation with the column of ones in Tinv\nallows us to directly incorporate a bias in the bilinear com-\nbination with the equivariant inputs Xeq. In T\u2032\ninv, the con-\ncatenation with \u2225Xeq\u2225, which is the L2 norm in the last\n(equivariant) dimensions, allows the invariant outputs to de-\npend on the equivariant inputs without losing invariance.\nGiven the weight and bias matrices,\nWeq \u2208Rdout\u00d7(cinv+1)\u00d7din, Winv \u2208Rdout\u00d7cinv\u00d7din\u00d7(cinv+1)\nBinv \u2208Rdout\u00d7cinv,\nwe define an SO(2)-equivariant fully connected layer,\nf (X, Weq, Winv, Binv), as follows:\nYeq,o,v =\ndin\nX\ni=1\ncinv+1\nX\nk=1\nWeq,o,k,i \u00b7 Tinv,i,k \u00b7 Xeq,i,v\n(3)\nYinv,o,v =\ndin\nX\ni=1\ncinv+1\nX\nk=1\nWinv,o,v,i,k \u00b7 T\u2032\ninv,i,k + Binv,o,v\n(4)\nY = f (X, Weq, Winv, Binv) = [Yeq, Yinv]\n(5)\nWhere Yeq is a bilinear combination of the equivariant input\nXeq and the invariant input Xinv and Yinv is a linear com-\nbination of the invariant input Xinv and the length of the\nequivariant input vectors \u2225Xeq\u2225. Proof of SO(2)-rotation-\nequivariance can be found in the supplementary material.\nVector Neuron Vision Transformer (VN-ViT).\nOur key\ncontribution is a Vision Transformer and Vector Neuron-\nbased encoder (VN-ViT), that can encode spherical images\nto SO(2) rotation-equivariant latent codes. Figure 3 shows an\noverview of the full method. The model architecture is based\non vision transformers [12], but since we work with spheri-\ncal images and require rotation equivariance, we replace all\nViT components with their Vector Neuron counterparts [11].\nWe cannot use positional encoding, since that would break\nequivariance. Instead, we concatenate the color values of\neach sample with the direction vector of where it was sam-\npled from. Patches are then projected to the dimensions of\nthe transformer using an SO(2)-equivariant fully connected\nlayer. We refer to the output of the projection as patch embed-\ndings. Following the Vision Transformer paper, we prepend\nan output token to the patch embeddings [12]. This token is\nonly learnable in the invariant dimensions of our input, in the\nequivariant dimensions, it is set to zero. Having the token be\nlearnable in any of the equivariant dimensions would break\nequivariance. The patch embeddings are then fed into a stan-\ndard SO(3)-equivariant VN-Transformer [1]. The output of\nthe transformer at the output token serves as a representation\nfor the full image. This output is then projected from the\ntransformer dimension to the latent dimension using two\nSO(2)-equivariant fully connected layers, which compute \u00b5\nand \u03c3 that, following the reparameterization trick [26], yield\nthe latent code. We found that using an SO(2)-equivariant\ninput and output projection with a VN-Transformer encoder\nproduces the best results.\nRotation-Equivariant Patching.\nWe sample patches from\nthe spherical image using our SO(2)-equivariant patching\nstrategy, as shown in Figure 4. Patches are sampled as verti-\ncal stripes on the sphere. A rotation of the environment map\naround the up axis by a multiple of the stripe width results\nin a permutation of the color values in the patches. Since\nthe transformer is permutation invariant, having only this\nwithout any positional encoding would make the model ro-\ntation invariant. The patches become equivariant by adding\na directional component in the form of direction vectors to\neach pixel by concatenating the color values with the di-\nrection vectors. A rotation of the environment map around\nthe up-axis by a multiple of the stripe width, while keeping\nthe patches in place, is equivalent to a joint rotation of both\nthe patches and the environment map around the up-axis.\nBoth result in the same rotated patch embeddings. Thus,\nour patching strategy is SO(2)-equivariant. In practice, we\nuse 64 patches as a trade-off between memory usage and\nrotation equivariance. Combining the directional vectors\nwith the color values, is proposed in a similar way as early\nfusion of non-spatial attributes in [1]. We use early fusion\nrather than late fusion since the direction vectors only pro-\nvide directional information and do not hold any additional\ninformation.\nSince we work with spherical data but sample pixels from\nan equirectangular projection of spherical images, we have\nto ensure that the sampling is regularly distributed on the\nsphere. We evenly sample the azimuth \u03c6 and distribute the\npolar angle \u03b8 as:\n\u03b8 = arccos(x), x \u2208[\u22121, 1]\n(6)\nBy handling spherical data directly instead of working with\nprojected images, we do not have to account for the distor-\ntion that would otherwise be introduced by the 3D to 2D\nprojection. Our approach of combining direction vectors and\n4\nFigure 3. Model Overview: We adapt the Vision Transformer architecture to SO(2) equivariance on spherical images. Splitting the\n360\u00b0 spherical image into vertical stripes and embedding the direction vectors and color values of the pixels in each patch using an\nSO(2)-equivariant projection. Then we feed the sequence into a Vector Neuron-Transformer. We use a class token that is learnable in\nnon-equivariant dimensions and zero in equivariant dimensions. The output is projected to \u00b5 and log(\u03c32) and reparameterized. The output\nlatent code is fed into the RENI++ decoder.\nFigure 4. Vertical stripes as patches is a valid SO(2)-equivariant\npatching strategy. Since we use a rotation equivariant transformer\nencoder, a rotation by a multiple of the patch width results in the\noutput of the encoder being rotated in the same way.\ncolor values enables the model to know where each pixel was\nsampled from, eliminating the need for distortion correction.\nRotation-Equivariant Variational Sampling.\nSince our\nlatent space is in 3D, variational sampling has to also take\nplace in 3D space. The naive approach of sampling three 1D\ndistributions results in an axis-aligned distribution, which is\nnot rotation equivariant. To solve this issue, we sample from\na spherical normal distribution that is defined by a 3D mean\nvector and a 1D variance. This distribution is isotropic and\nthus rotation equivariant.\nRENI++ decoder.\nWe adopt the same decoder as proposed\nin RENI++ [20], which introduces a rotation-equivariant\nconditional neural field autodecoder architecture that lever-\nages attention for conditioning. This design is motivated by\nfindings that attention-based conditioning of neural fields\noutperforms alternative methods of conditioning [35]. The\ninput to the RENI++ neural field D is a direction vector d\nand the 3D latent vector Z. The output is the color c at di-\nrection d. To make the model rotation-equivariant, d and Z\nare encoded so they are invariant to simultaneous rotations,\nso D(Rd, RZ) = D(d, Z) with R \u2208SO(3). This makes\nthe neural field equivariant to rotations of Z only. Rotating\nZ is equivalent to rotating the spherical signals such that\nD(d, RZ) = D(R\u22a4d, Z). The invariant encoding of d and\nZ to simultaneous rotations is achieved by encoding the di-\nrection d relative to the latent code: d\u2032 = Z\u22a4d and applying\nthe Vector Neuron invariant layer [11] to the latent code:\nZ\u2032 = VN-Inv(Z).\nPretraining.\nFor rendering, the full dynamic range of nat-\nural light is essential. To let our model learn this full range,\nour dataset must also consist of images that cover the entire\ndynamic range of natural light and not clipping the bright-\nest parts of the image. We use two datasets to train our\nmodel. One is the RENI++ dataset, consisting of 1,694 HDR\nequirectangular images obtained under a CC0 1.0 Univer-\nsal Public Domain Dedication license [20]. It features a\nwide variety of outdoor scenes and illumination conditions.\nThese images have a resolution of 128 \u00d7 64. This low res-\nolution is sufficient for our purposes. The RENI++ dataset\nis limited in size and further availability of 360\u00b0 HDR out-\ndoor illumination environments is limited. However, 360\u00b0\nLDR data is readily available, for example in the form of\nGoogle street view data. Although we cannot use LDR data\ndirectly, there exists a large body of work that focuses on\nreconstructing HDR data from LDR images [16, 38, 46, 51].\nSome of which even take 360\u00b0 environment maps into ac-\ncount [38, 51]. In addition to the RENI++ dataset, we also\nuse the streetlearn dataset [29], consisting of a large num-\nber of LDR 360\u00b0 images captured by Google street view in\nManhattan. We convert 43,310 equirectangular 360\u00b0 images\nfrom the Manhattan part of the streetlearn dataset to HDR\nusing [51]. The converted streetlearn dataset is of lower qual-\nity than the RENI++ dataset since the HDR reconstruction\nprocess is imperfect and the dataset predominantly contains\ncityscapes, whereas we want good HDR reconstruction of\n5\nvaried outdoor scenes. We address this in our training cur-\nriculum by first pretraining our model on the much larger\nstreetlearn dataset and then finetuning it on the RENI++\ndataset.\n4. Experiments\nLosses.\nTo better match how the human visual system\nreacts to luminance, we train our model in log space. This is\ncommon practice for models that deal with HDR data [16,\n38, 46, 51]. With a loss in linear space, areas with high\nluminance would overpower important features in areas with\nlow or medium luminance. A loss in log space alleviates this\nissue by spreading the loss approximately linearly across the\nperceived luminance range [16].\nAdditionally, there is scale ambiguity within our dataset,\nbecause the HDR images are captured at unknown exposure\nvalues (EV). We only know the relative luminance between\npixels, not the absolute luminance of each pixel. Any image\nmultiplied by a positive scale factor k, would still be valid.\nOur dataset contains high frequency details, which are\nespecially prevalent at the boundary between sun and sky,\nwhere there is a large change in intensity in a small area. To\nenable our model to capture more high frequency detail, we\nuse a loss from depth prediction, the Mean Absolute Gradient\nError (MAGE) [6]. This loss improves high frequency detail\nby operating on the gradients of the image. High frequency\ndetails will lead to high gradients and thus the error in high\nfrequency areas is more strongly penalized. During training,\nwe decode the full equirectangular images. Thus, we have to\naccount for the irregular sampling by weighting with the sin\nof the polar angle of each pixel, sin(\u03b8i):\nLMAGE = 1\nM\nM\nX\nj=1\n1\nNj\nNj\nX\ni=1\nsin(\u03b8i)|\u2207Sf(I)j\ni \u2212\u2207SIj\ni|, (7)\nwith \u2207S denoting the spatial derivative operator Scharr\n(S) [36] and M = 2 denoting the number of scales, used\nfor the multi-scale derivative loss. The inputs to the loss are\nI, the ground truth image in log space, and f(I), the model\noutput in log space. Since this loss is applied in log space, it\nis scale invariant. In log space, Scharr removes the influence\nof any arbitrary scale factor k.\nWe also follow the approach of RENI++ and LANet [20,\n51] and use another technique from depth prediction [15, 28],\nthe scale invariant loss. This loss calculates the relative error\nover the entire image, rather than on the gradients of the\nimage:\nLscale-inv = 1\nN\nN\nX\ni=1\n(Ri)2 \u22121\nN 2\nN\nX\ni=1\nRi\n!2\n.\n(8)\nWhere Ri = sin(\u03b8i)(f(I)i \u2212Ii) at pixel i. This helps\nus learn a scale invariant representation of the images. To\nencourage our model to produce accurate color representa-\ntions, we again follow the approach of RENI++ [20] and\nuse a cosine similarity loss. Since this loss operates on the\ndirection of RGB vectors, it is also scale invariant:\nLcosine = 1 \u22121\nN\nN\nX\ni=1\nsin(\u03b8i) f(I)i \u00b7 Ii\n\u2225f(I)i\u2225\u2225Ii\u2225.\n(9)\nWe train our model using variational sampling and regu-\nlarize the latent space to follow a standard normal distribu-\ntion using Kullback Leibler divergence (KLD). Since we use\na 3D isotropic distribution for our variational sampling, we\nmust also adjust the KLD loss term accordingly:\nLKLD = 1\nD\nD\nX\ni\n\u22120.5\u00b7(3+3\u00b7log(\u03c3i)\u2212\u2225\u00b5i\u22252\u22123\u00b7\u03c3i), (10)\nwhere D is the latent dimension. Our full training loss is:\nL = 0.5 \u00b7 LMAGE + Lscale-inv + Lcosine + 0.01 \u00b7 LKLD. (11)\nQuality Metrics. We report in Table 1, the PSNR, SSIM\nand LPIPS scores in LDR tone-mapped space, as well as\nPSNR in linear HDR space, all averaged across the test set.\nRENI++ results are taken from their paper, as we found\nthem reproducible. For our model, we show metrics ob-\ntained with the full autoencoder pass, as well as with latent\noptimization using only the decoder. This makes our model\nmore comparable to RENI++, since RENI++ can only do\nlatent optimization. Especially in the low-dimensional case\nof D = 27, our model strongly outperforms RENI++. We in-\nclude a qualitative comparison of the reconstruction quality\nof the three approaches in the supplementary material.\nInterpolation. A well-behaved latent space enables smooth\ninterpolations between latent codes. Each step of the inter-\npolation should correspond to a realistic and semantically\nmeaningful illumination environment. For example, inter-\npolating between a sunrise and a midday illumination envi-\nronment should result in the sun moving smoothly across\nthe sky and the color temperature changing gradually from\ndaylight to sunset without introducing artifacts.\nFigure 5 shows interpolations of RENI++ and our model\nfor the same image pairs. Our model meaningfully interpo-\nlates between the start and target image, whereas RENI++\nintroduces artifacts in the interpolation steps. In the snow-\nforest example, our model shows a smooth color change,\nwhile RENI++ introduces a sun in the middle of the inter-\npolation, that is not present in either the source or the target\nimage. Our model can also smoothly interpolate between\nsunrise and midday illumination environments, as seen in\nthe lake-field example. The yellow reflection in the lake\nsmoothly disappears and the scene transitions to a more\nbluish midday illumination environment. In the mountain-\nroad interpolation, RENI++ introduces artifacts and noise\n6\nRENI++ (optimization)\nOurs (AE)\nOurs (optimization)\nD\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\nHDR\u2191\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\nHDR\u2191\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\nHDR\u2191\n27\n18.02\n0.39\n0.62\n33.00\n18.78\n0.46\n0.62\n33.37\n20.33\n0.51\n0.61\n33.92\n147\n21.13\n0.51\n0.55\n34.30\n19.40\n0.48\n0.59\n33.59\n21.89\n0.54\n0.55\n35.07\n300\n22.10\n0.55\n0.52\n35.10\n19.47\n0.48\n0.59\n33.68\n22.68\n0.57\n0.48\n35.59\nTable 1. Comparison of reconstruction quality metrics between RENI++ and our model (autoencoder pass and direct latent code optimization),\nacross varying latent dimensions D. PSNR, SSIM and LPISP are in LDR tone-mapped space, PSNR HDR is in linear HDR space.\nRENI++\n27\nGT\nRENI++\n147\nRENI++\n300\nOurs\n27\nOurs\n147\nOurs\n300\nRENI++\n27\nRENI++\n147\nRENI++\n300\nOurs\n27\nOurs\n147\nOurs\n300\nRENI++\n27\nRENI++\n147\nRENI++\n300\nOurs\n27\nOurs\n147\nOurs\n300\nGT\nReconstruction\n\u2190\u2212\u2212Interpolation \u2212\u2212\u2192\nReconstruction\nFigure 5. Interpolations using our model (with direct latent opti-\nmization) and RENI++ with different latent sizes. Three image\npairs are interpolated: snow-forest, lake-field, mountain-road.\nin the sky during interpolation, whereas our model does not.\nThis is especially visible with D = 27.\nUniqueness. One goal of our model is uniqueness of the la-\ntent space. To test this, we optimize two random latent codes\nsampled from a normal distribution N(0, 1) to fit the same\nimage. If the latent space is unique, no other images should\nbe represented between the two optimized latent codes. We\nmeasure model uniqueness by interpolating between two\noptimized latent codes, as shown in Figure 6. For unique\nmodels, the output images do not diverge during interpola-\ntion. Since the output images barely diverge for our model\nand diverge significantly for RENI++, we can conclude that\nUniqueness\u2193\nReconstruction\nConsistency\u2191\nD\nRENI++\nOurs\nRENI++\nOurs\n27\n1.46\n0.04\n0.17\n0.50\n147\n1.11\n0.43\n0.12\n0.30\n300\n1.03\n0.57\n0.07\n0.23\nTable 2. Uniqueness is the mean squared error (MSE) between an\nimage produced by an optimized latent code and an image produced\nby the interpolation midpoint of two latent codes optimized to\nthe same image. Reconstruction Consistency are the Spearman\ncorrelation coefficients between the MSE of random latent pairs\nand the MSE between their corresponding image reconstructions.\nDataset size\nRENI++\nOurs (AE)\nOurs (optimized)\n1500\n20.11\n16.00\n20.99\n43260\n17.14\n16.77\n19.90\nTable 3. Mean PSNR of our model and the RENI++ model trained\non a large and a small subset of the HDR converted streetlearn\ndataset, evaluated on the same test set from the converted streetlearn\ndataset.\nour model is more unique than RENI++. To objectively mea-\nsure uniqueness, we compute the MSE between the image\nproduced by the first optimized latent code and the image\nproduced by the latent interpolation midpoint. Higher er-\nror indicates that the model is less unique. Table 2 shows\nthis metric for our model and the RENI++ model over var-\nious latent dimensions D. We include further examples of\nthe interpolation between two optimized latent codes in the\nsupplementary material.\nThe RENI++ autodecoder architecture hinders unique-\nness in the model. Two similar images may be initialized\nwith vastly different latent codes, while our model encodes\ntwo similar images to similar latent codes by using a vari-\national autoencoder architecture. With the small dataset\nRENI++ uses for training, non-uniqueness is not a signifi-\ncant issue. However, with a larger dataset, there are more\nsimilar images that are then initialized with different latent\ncodes, decreasing the uniqueness of the model even further.\n7\nRENI++\nGT\nOurs\nOptimization 1\n\u2190\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212Interpolation \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nOptimization 2\nFigure 6. Interpolating between two optimized latent codes. With our model, the output images remain consistent during interpolation,\nsuggesting that the latent space is unique. With RENI++ the output images diverge from the optimized images during interpolation,\nsuggesting that the latent space is not unique.\nComponent\n27\n147\n300\nOurs\n18.78\n19.40\n19.47\n- scale-inv & MAGE loss\n17.32\n18.47\n18.80\n- pretraining on streetlearn\n17.37\n18.07\n17.88\n- SO(2) linear projection\n16.89\n17.49\n17.30\nTable 4. Mean PSNR on various ablations of our model\nD\nSO(2) projections\nFull SO(2)\nFull VN\n27\n18.78\n17.89\n17.74\n147\n19.40\n18.18\n18.80\n300\n19.47\n17.93\n18.60\nTable 5. Mean PSNR comparing models only using SO(2) linear\nlayers in the projection layers, using SO(2) layers in the entire\nencoder including the transformer and using VN-layers in the entire\nencoder\nThis also leads to a decrease in RENI++ performance as\ndataset size increases, as seen in Table 3. While RENI++\nperformance degrades with an increased dataset size, our\nmodel\u2019s performance improves with an autoencoder pass\nand does not degrade as much with optimization.\nWe also measure the reconstruction consistency of our\nmodel (see Table 2). This metric calculates the correlation\ncoefficients between the error of random latent pairs and\nthe error between their corresponding image reconstructions.\nFollowing the idea of the latent-data distance constraint by\nTran et al. [43], it tells us how well latent distance matches\nperceived distance and is an indicator for latent space unique-\nness. A higher correlation indicates more uniqueness be-\ncause a low latent error combined with a low reconstruction\nerror (i.e., a more unique model) results in a higher corre-\nlation. In contrast, a high latent error combined with a low\nreconstruction error (i.e., a non-unique model) results in a\nlower correlation.\nAblations.\nTo show that our SO(2)-equivariant exten-\nsion to Vector Neurons improves upon the default SO(3)-\nequivariant Vector Neurons, we do an ablation study. We\ncompare the performance of Vector Neurons with that of our\nSO(2)-equivariant extension in the projection layers and the\nfull encoder across a range of latent dimensions D using the\nmean PSNR. For the SO(2)-equivariant transformer, we have\nto account for the SO(2)-equivariant layers using the invari-\nant axes as additional learnable parameters, by decreasing\nthe inner dimension of the SO(2)-equivariant transformer\naccordingly."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of \"VENI: Variational Encoder for Natural Illumination\"\n\n#### Main Contributions\n\n**Rotation-Equivariant Variational Autoencoder (REVI):**\n- **Model Architecture:** The paper proposes a rotation-equivariant variational autoencoder (REVI) that models natural illumination on the sphere without relying on 2D projections. This architecture enables smoother interpolation in latent space and offers a more well-behaved latent space compared to existing methods.\n  \n**Key Components:**\n1. **Vector Neuron Vision Transformer (VN-ViT):** A novel vision transformer encoder designed for spherical signals, which computes the latent codes and maps them back to image space via a rotation-equivariant conditional neural field decoder.\n2. **SO(2)-Equivariant Fully Connected Layer:** An extension of Vector Neurons that reduces equivariance from SO(3) to SO(2), preserving the canonical up direction in illumination environments.\n\n#### Methodology\n\n**Data Handling:**\n- The model is trained on two datasets: RENI++ (1,694 HDR equirectangular images under CC0 1.0 Universal Public Domain Dedication license) and StreetLearn (a large number of LDR 360\u00b0 images captured by Google Street View in Manhattan).\n  \n**Loss Functions:**\n- The model uses a combination of loss functions including log space luminance, scale invariant depth prediction losses (MAGE), cosine similarity for color representation, and Kullback-Leibler divergence to regularize the latent space.\n\n**Training Details:**\n- Pretraining is performed on the StreetLearn dataset before fine-tuning with RENI++ data.\n- The model employs variational sampling in 3D space and optimizes the latent space using a standard normal distribution.\n\n#### Key Findings\n\n1. **Latent Space Uniqueness:** \n   - REVI produces more unique latent codes compared to previous methods, as demonstrated by optimizing random latent codes and evaluating reconstruction consistency via the correlation between latent space and image space distances.\n   \n2. **Interpolation Quality:**\n   - The model enables smooth interpolation in latent space, resulting in realistic illumination transitions without introducing artifacts.\n\n3. **Performance Evaluation:**\n   - Compared to RENI++, REVI shows improved performance across various latent dimensions (D = 27, 147, and 300), particularly in the low-dimensional case of D = 27.\n   \n#### Conclusions\n\n**Model Advantages:**\n- The proposed rotation-equivariant variational autoencoder (REVI) addresses the limitations of existing illumination priors by producing a better-behaved latent space. This model enables smoother interpolation and offers improved reconstruction consistency.\n\n**Future Work:**\n- Further research could explore how to scale REVI to larger datasets while maintaining its unique properties.\n- Investigating alternative loss functions or architectures might lead to even more robust models for natural illumination representation.\n\n### References\n\nThe references provided in the paper cover a range of related work, including previous illumination priors and rotation-equivariant neural networks."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "1\nOptimizing Energy and Data Collection in\nUAV-aided IoT Networks using Attention-based\nMulti-Objective Reinforcement Learning\nBabacar Toure, Dimitrios Tsilimantos, Omid Esrafilian, and Marios Kountouris\nAbstract\u2014Due to their adaptability and mobility, Unmanned\nAerial Vehicles (UAVs) are becoming increasingly essential for\nwireless network services, particularly for data harvesting tasks.\nIn this context, Artificial Intelligence (AI)-based approaches have\ngained significant attention for addressing UAV path planning\ntasks in large and complex environments, bridging the gap with\nreal-world deployments. However, many existing algorithms suf-\nfer from limited training data, which hampers their performance\nin highly dynamic environments. Moreover, they often overlook\nthe inherently multi-objective nature of the task, treating it\nin an overly simplistic manner. To address these limitations,\nwe propose an attention-based Multi-Objective Reinforcement\nLearning (MORL) architecture that explicitly handles the trade-\noff between data collection and energy consumption in urban\nenvironments, even without prior knowledge of wireless channel\nconditions. Our method develops a single model capable of\nadapting to varying trade-off preferences and dynamic scenario\nparameters without the need for fine-tuning or retraining. Ex-\ntensive simulations show that our approach achieves substantial\nimprovements in performance, model compactness, sample effi-\nciency, and most importantly, generalization to previously unseen\nscenarios, outperforming existing RL solutions.\nIndex Terms\u2014Data collection, energy efficiency, deep reinforce-\nment learning, IoT, multi-objective optimization, attention-based\nmechanisms, UAV, wireless communications.\nI. INTRODUCTION\nU\nNMANNED Aerial Vehicles (UAVs) are expected to play\na key role in next-generation wireless communication\nnetworks, due to their versatility and ease of deployment.\nUAVs support a wide range of Internet of Things (IoT)\napplications, including wireless coverage, data relaying, data\ncollection, edge computing, and disaster response [1]. In par-\nticular, for data collection, UAVs offer an efficient and scalable\nsolution to address the growing volume of IoT devices in\nagricultural, smart city, and industrial environments. Compared\nto ground-based infrastructure alone, UAVs provide mobility,\nrapid deployment, and operational flexibility. They can adapt\ntheir flight paths in real time based on network conditions,\nBabacar Toure (Corresponding author) is with the Advanced Wireless Tech-\nnology Lab, Paris Research Center, Huawei Technologies France and also with\nEURECOM, Sophia Antipolis, France (email:babacar.toure1@huawei.com).\nDimitrios\nTsilimantos\nis\nwith\nthe\nAdvanced\nWireless\nTechnology\nLab, Paris Research Center, Huawei Technologies France (email: dim-\nitrios.tsilimantos@huawei.com).\nOmid Esrafilian is with the Communication Systems Department, EURE-\nCOM, Sophia Antipolis, France (email: omid.esrafilian@eurecom.fr).\nMarios Kountouris is with the Dept. of Computer Science and Artificial\nIntelligence, University of Granada, Spain and also with EURECOM, Sophia\nAntipolis, France (mariosk@ugr.es).\naccess remote or hazardous areas with poor connectivity, and\nhelp reduce both the cost and latency of data collection [2].\nHowever, UAV-based data collection tasks often lead to NP-\nhard and non-convex optimization problems [3]\u2013[5], mainly\ndue to the combinatorial complexity of trajectory planning, the\nabsence of reliable communication models, and the presence\nof multiple, often conflicting, objectives. This complexity is\nfurther exacerbated by the dynamic and uncertain nature of\nthe environments in which UAVs operate, characterized by\nhigh-dimensional state and action spaces. In this context, Deep\nReinforcement Learning (DRL) has emerged as a powerful\ntool for learning effective navigation policies by interacting\nwith the environment, without relying on predefined models\nor assumptions. Moreover, like many real-world applications,\nUAV data collection inherently involves balancing multiple ob-\njectives, such as minimizing energy consumption, maximizing\ndata yield, and optimizing data freshness [2]. In this paper,\nour goal is to address these challenges by leveraging DRL\nalgorithms that are not only scalable, sample-efficient, and\nstable but also capable of handling multiple objectives and\nproducing interpretable policies.\nA. Related Work\nPath planning for UAV-enabled IoT applications has become\na highly active research topic in recent years. According to the\ncomprehensive survey in [2], existing data collection strategies\nfall into three broad categories: (i) conventional heuristics, (ii)\noptimization-based methods, and (iii) AI-driven techniques.\nConventional path-planning methods typically leverage\ngraph-based algorithms such as Dijkstra, A*, Voronoi dia-\ngrams, and related techniques. These approaches model the\nnavigation environment as a graph, where nodes represent\ndiscrete waypoints and edges denote potential flight paths.\nTask-specific metrics, such as the volume of collected data,\nthe data freshness, and the energy consumption, are commonly\nused to guide heuristic decisions within these algorithms. For\nexample, [6] introduces an A* path planning algorithm based\non an efficacy function that accounts for data value, energy\nusage, mission duration, and risk, aiming to address UAV\nenergy constraints. Similarly, [7] employs a Rapidly-exploring\nRandom Tree (RRT)-based planner to facilitate data collec-\ntion from a Wireless Sensor Network (WSN), demonstrating\nits effectiveness in both simulation and real-world settings.\nAlthough these methods perform well in small- to medium-\nscale scenarios, they generally suffer from limited scalability\narXiv:2601.14092v1  [cs.LG]  20 Jan 2026\n2\nand reduced robustness in dynamic environments. In particular,\nenvironmental changes often require reconstruction of the un-\nderlying graph, a process that can be computationally intensive\nand inefficient in real-time operations.\nOptimization algorithms have also been extensively applied\nto tackle data collection challenges in IoT systems. In [8],\nan optimal Age of Information (AoI) trajectory is designed\nfor a UAV that collects data from ground sensor nodes\nin a wireless sensor network (WSN). The authors employ\ndynamic programming and genetic algorithms to derive two\ndistinct trajectories: one that minimizes the average AoI across\nall sensor nodes, and another that minimizes the maximum\nAoI. Similarly, [9] explores a related scenario, but focuses\non the trade-off between AoI and energy consumption. The\nresulting non-convex optimization problem is decomposed\ninto three sub-problems, which are solved using Successive\nConvex Approximation and Lagrangian duality techniques.\nSimulation results demonstrate that simultaneously minimiz-\ning both AoI and energy consumption is generally infeasible,\nhighlighting the need to consider such problems from a multi-\nobjective perspective, where multiple Pareto-optimal solutions\nmay exist. Optimization-based approaches are valued for their\nconvergence guarantees and mathematical rigor. However, they\noften lack adaptability in real-time scenarios and tend to\ngeneralize poorly to changing environmental conditions or\nproblem configurations. In most cases, solutions must be re-\ntuned or re-optimized when applied to new instances, limiting\ntheir scalability and practical deployment.\nRecently, RL algorithms have shown promising perfor-\nmance in UAV path-planning tasks, addressing several limita-\ntions of traditional methods. These algorithms learn adaptive\nnavigation policies by interacting with the environment, with-\nout requiring an explicit model of its dynamics. A notable\nearly contribution is presented in [10], which tackles the\nproblem of data freshness under energy constraints using a\nDeep Q-Network (DQN). The use of DQN specifically miti-\ngates the curse of dimensionality by employing Deep Neural\nNetworks (DNNs) as function approximators, enabling near-\noptimal decision-making in large state spaces and facilitating\nreal-time execution with strong generalization capabilities.\nThe proposed approach outperforms conventional heuristic-\nbased methods. In a similar vein, [11] addresses the joint\noptimization of UAV flight cruise control and scheduling,\naiming to minimize data loss on IoT devices caused by buffer\noverflows. Meanwhile, [12] employs the Twin-Delayed Deep\nDeterministic Policy Gradient (TD3) algorithm to maximize\nthroughput during UAV navigation, reporting substantial gains\nover non-learning baselines in realistic simulation scenarios.\nFurthermore, [13] explores a dual-purpose use of DRL for\nsimultaneous UAV-based wireless energy transfer and data\ncollection. Despite their advantages, DRL models often exhibit\nlimited generalization to new environments or varying problem\nparameters. This shortcoming is largely due to the reliance on\nfixed network architectures and rigid state representations. As\nshown in [14], multi-UAV frameworks in particular require\nretraining when key environmental variables change, which is\ncomputationally intensive given the large number of interac-\ntions typically required for RL convergence. To overcome this\nlimitation, the authors of [14] propose an extended approach in\nsubsequent works [14]\u2013[16], introducing convolutional neural\nnetwork (CNN) architectures in place of standard feedforward\nnetworks. They also propose a novel spatial map representation\nthat integrates a compressed global view with a dynamically\nupdated local map. This hybrid representation significantly im-\nproves generalization across diverse environments and scales\neffectively to larger operational areas.\nAn often overlooked aspect of UAV-based data collection\nis the inherently multi-objective nature of the task. Many\nexisting studies, including those cited above, address problems\ninvolving conflicting objectives, such as maximizing data\ncollection, minimizing energy consumption, and optimizing\ndata freshness. In the worst-case scenario, only one objective is\nconsidered, the rest being neglected. More commonly, multiple\nobjectives are combined into a single scalar objective by linear\nscalarization, typically using a fixed weight vector w, as seen\nin [13], [14], [17]\u2013[19]. However, this approach implicitly\nassumes that the desired trade-off between objectives, encoded\nby w, is known in advance, an assumption that does not always\nhold in practice. Furthermore, when the preferred trade-off\nchanges, the solution derived using the original w becomes\nsuboptimal, requiring fine-tuning or complete retraining (see\nFig. 1a). Alternative strategies attempt to overcome this lim-\nitation by training multiple models, each optimized for a\ndifferent trade-off configuration [20]. Although this allows\nflexibility in choosing trade-offs after training, it significantly\nincreases memory consumption and computational cost (see\nFig. 1b). In our previous work [21], we proposed MOSAC,\na multi-objective extension of the discrete Soft Actor-Critic\n(SAC) algorithm. MOSAC is capable of learning, in a single\ntraining run, a near-optimal policy for any desired trade-\noff between data collection and energy consumption (see\nFig. 1c). However, due to its feedforward architecture and\nstatic state representation, MOSAC also suffers from limited\ngeneralization to environments with changing parameters.\nOBJECTIVE 1\nOBJECTIVE 2\nOther Pareto\nop\u0014mal solu\u0014ons\nFound solu\u0014on\nSingle model, single\npreference\n(a) Naive\nOther Pareto\nop\u000bmal solu\u000bons\nFound solu\u000bon\nOBJECTIVE 1\nOBJECTIVE 2\nSeveral models, several\npreferences\n(b) Costly\nFound solu\ton\nOBJECTIVE 1\nOBJECTIVE 2\nSingle model, all\npreferences\n(c) Desired\nFig. 1: Different approaches in multi-objective learning.\nIn this work, we address the trade-off between data col-\nlection and energy consumption in UAV path planning within\nurban environments. Building upon our previous work [21], we\npropose an algorithm that solves any trade-off vector w with-\nout requiring fine-tuning. Unlike prior approaches, this study\nalso tackles the critical challenge of parameter generalization,\nenabling the learned policy to perform robustly in previously\nunseen environments. This contributes to improved sample\nefficiency, a major bottleneck in real-world applications of RL\n[22]. To the best of our knowledge, this is the first approach\n3\nthat combines multi-objective learning with parameter gener-\nalization, leveraging an attention-based architecture inspired\nby inductive bias techniques for generalization [23].\nB. Contributions and Organization\nOur primary contributions are summarized as follows:\n\u201a We formulate a single UAV-based wireless data harvest-\ning task as a multi-objective path planning problem,\naiming to maximize data collection while minimizing\nenergy consumption. To address the absence of prior\nknowledge about the preferred trade-off between these\nobjectives, we model the problem as a Multi-Objective\nMarkov Decision Process (MOMDP), enabling flexible\npolicy learning across varying trade-off preferences.\n\u201a To solve the formulated MOMDP, we adopt our MOSAC\nmethod, which is well-suited for environments where the\ndynamics of wireless channels are unknown. In particular,\nour approach uses low-dimensional feature vectors rather\nthan high-dimensional spatial maps, significantly reduc-\ning model complexity and improving training efficiency.\n\u201a To this end, we design an attention-based architecture that\nis inherently permutation-invariant with respect to the set\nof IoT devices. Unlike conventional multi-objective RL\napproaches that concatenate the preference vector w to\nthe state, we introduce it as a dedicated input token to\nthe self-attention encoder, allowing for more flexible and\neffective conditioning on the desired trade-off.\n\u201a We conduct a comprehensive evaluation of our approach,\ncomparing it against state-of-the-art DRL algorithms\nand a carefully designed greedy baseline. Our results\ndemonstrate the superiority of our method in zero-shot\ngeneralization, both within training scenarios and, more\nimportantly, in unseen environments with varying num-\nbers of devices, spatial configurations, data volumes, and\ninitial battery levels.\n\u201a Finally, we provide an in-depth analysis of the attention\nscores, revealing that the robustness and generalization\ncapabilities of our algorithm arise from its ability to se-\nlectively focus on task-relevant features while effectively\nfiltering out information that is irrelevant to the specific\nobjective being optimized.\nThe remainder of this paper is organized as follows. Section II\nintroduces the system model, detailing the UAV dynamics,\nthe communication channel model, and the formulation of\nthe multi-objective optimization problem. In Section III, we\nmodel the system as a MOMDP and discuss various state\nrepresentation strategies. Section IV presents our attention-\nbased multi-objective DRL algorithm. Simulation results and\nperformance evaluations are provided in Section V.\nII. SYSTEM MODEL\nIn this section, we present the models corresponding to\neach component of our problem: the physical environment,\nthe UAV, the IoT devices, and the communication framework,\nwhich closely follow our previous work [21].\nA. Environment and UAV model\nEnvironment. We consider an urban environment discretized\ninto a rectangular grid world of size L \u02c6 W P N2, step size\nc, where E denotes the set of all possible positions. Such a\ndiscretization is common in RL applications, as it reduces\nboth the state space and the number of possible actions,\nthus improving training efficiency and exploration. The urban\nenvironment includes buildings of varying shapes and heights,\nsurrounded by alleys of varying widths. Let H \u0102 E denote the\nset of positions occupied by the buildings, and hx,y the height\nof the building at location px, yq P H. We represent the grid\nworld as a matrix E, where each element Ex,y is defined as:\nEx,y \u201c\n#\nhx,y\nif px, yq P H,\n0\notherwise.\n(1)\nH denotes the height of the tallest building on the map. The\nenvironment has designated start and terminal zones in EzH,\ndenoted respectively as ps \u201c pxs, ysq and pf \u201c pxf, yfq.\nDevices. The K static IoT devices are randomly deployed at\nground level within the environment. The location of device k\nis denoted by pk \u201c pxk, ykq P EzH, ensuring that no device is\nplaced on a building. At the start of the mission, each device\nk is assigned an initial data load Dk\n0 P R to be collected. As\nthe mission progresses, the remaining data at time t is denoted\nby Dk\nt P R.\nUAV model. In this work, we consider a single UAV navigat-\ning the described environment at a constant altitude h P R,\nsuch that h \u0105 H, where H is the height of the tallest\nbuilding. This setup ensures collision avoidance and reduces\nthe complexity of the problem to a 2D trajectory optimization\ntask. The UAV operates above the urban landscape, which\nreflects real-world prototyping scenarios while capturing the\ncore challenge of the data collection problem: finding the\noptimal horizontal path to approach IoT devices or designated\nlanding zones. The mission duration is discretized into T time\nslots of equal length \u2206t, indexed by t P 1, . . . , T. The UAV\u2019s\nobjective is to collect data from the K static IoT devices\nlocated at ground level. The UAV begins the mission at the\nstart position ps with a fully charged battery b0 \u201c B P N,\nand the mission terminates only when it successfully reaches\nthe terminal position pf. We assume that the battery level\nis discretized for modeling purposes. At time t, the UAV is\ncharacterized by its 2D position pt \u201c pxt, ytq P R2 and its\nremaining battery level bt P N. We assume that, within a single\ntime slot, the UAV can either hover in place or move a distance\nequal to c (the cell size) in one of the four cardinal directions.\nAccordingly, the UAV\u2019s action set is defined as:\nA \u201c\n# \u201e\n0\n0\n\u0237\nlomon\nhover\n,\n\u201e\n0\nc\n\u0237\nlomon\nnorth\n,\n\u201e\n\u00b4c\n0\n\u0237\nlomon\nwest\n,\n\u201e\n0\n\u00b4c\n\u0237\nlomon\nsouth\n,\n\u201e\nc\n0\n\u0237\nlomon\neast\n+\n.\n(2)\nThus, at time t, after selecting an action at P A, the position\nof the UAV evolves according to:\npt`1 \u201c pt ` at.\n(3)\n4\nNote that certain action restrictions are enforced when the\nbattery level is low or when the UAV is positioned at\nthe edge of the map, as detailed in Section III-B. Battery\nconsumption in UAVs generally depends on several factors,\nincluding flight mode, UAV design (e.g., payload, battery\ncapacity, maximum speed), and environmental conditions such\nas wind and weather [24]. In line with our discretized battery\nmodel and action space, we assume that moving in any of the\nfour cardinal directions consumes one unit of battery, while\nhovering consumes half a unit. This assumption reflects the\nwidely observed fact that hovering typically requires less en-\nergy than translational movement. Accordingly, the evolution\nof the UAV\u2019s battery level is given by:\nbt`1 \u201c\n#\nbt \u00b4 0.5\nif at \u201c hover,\nbt \u00b4 1\notherwise,\n(4)\nand et \u201c bt \u00b4 bt`1 denotes the energy consumed at t.\nB. Channel model and data collection\nTo enable data collection from the IoT devices during the\nmission, a wireless connection must be established between\nthe UAV and the devices. In each time interval t P 1, . . . , T,\nthe amount of data collected successfully by the UAV depends\non the quality of the communication link with each device.\nSpecifically, we write the Signal-to-Noise Ratio (SNR) be-\ntween the UAV and device k at time t as follows [25]:\nSNRk\nt \u201c P100.1gk\nt\n\u03c32\n,\n(5)\nwhere P denotes the transmit power, \u03c32 is the power of the\nadditive white Gaussian noise (AWGN) at the receiver, and\ngk\nt represents the channel gain (in dB) between the UAV\nand the device k at time t. The channel gain is defined as\ngk\nt \u201c \u03b2z \u00b4 \u03b1z log10pdk\nt q ` \u03b7t,z. As defined above, the channel\ngain depends primarily on the Euclidean distance between the\nUAV and device k, given by dk\nt \u201c |pk \u00b4 pt|2. It also incorpo-\nrates log-normal shadowing, modeled as \u03b7t,z \u201e Np0, \u03c32\nzq, and\nsampled at every movement of the UAV, the path loss exponent\n\u03b1z, and the average channel gain \u03b2z at a reference distance\nof 1 meter. Following the channel models in [10]\u2013[12], [14],\n[26], we did not account for additional randomness in the gain.\nCritically, the channel gain also depends on the presence of\nobstacles, specifically buildings, between the UAV and the\ndevice k. This is captured by the link condition indicator\nz P tLoS, NLoSu, which distinguishes between line-of-sight\n(LoS) and non-line-of-sight (NLoS) scenarios. The link con-\ndition z affects the path loss exponent \u03b1z, the reference gain\n\u03b2z, and the shadowing variance \u03c32\nz, thus playing a central role\nin the determination of communication quality. Consequently,\nthe maximum achievable data rate is the following:\nRk\nt \u201c log2p1 ` SNRk\nt q,\n(6)\nwhere the rate is assumed to remain constant throughout the\ntime slot (block fading model). Since the amount of data that\ncan be collected is limited by the remaining data available at\ndevice k, the effective data rate is given by:\nCk\nt \u201c\n#\nRk\nt\nif Dk\nt \u011b Rk\nt \u2206t ,\nDk\nt {\u2206t\notherwise.\n(7)\nWe assume that only one device can be scheduled for data\ncollection during each time slot. The scheduling decision de-\npends on the SNR: the device with the highest SNR is selected\nfor communication. As a result, the scheduling decision is\ndetermined automatically and is not part of the UAV\u2019s action\nspace. We define the scheduling variable \u03b4k\nt such that \u03b4k\nt \u201c 1\nif the device k is scheduled, and \u03b4k\nt \u201c 0 otherwise. It follows:\nK\n\u00ff\nk\u201c1\n\u03b4k\nt \u010f 1,\n@t \u011b 0.\n(8)\nIt also follows that the data volume remaining at device k\nevolves according to: Dk\nt`1 \u201c Dk\nt \u00b4 \u03b4k\nt Ck\nt \u2206t.\nC. Problem definition\nGiven the environment described previously, the UAV model\nand dynamics, and the communication model, we now for-\nmulate the multi-objective path planning problem for data\ncollection and energy consumption:\nmax\n\u03c0\n\u02dc T\u00ff\nt\u201c1\nK\n\u00ff\nk\u201c1\n\u03b4k\nt Ck\nt \u2206t,\n\u00b4\nT\u00ff\nt\u201c1\net\n\u00b8\n(9a)\ns.t.\np1 \u201c ps, pT \u201c pf,\n(9b)\nbT \u011b 0,\n(9c)\np3q, p4q, p8q.\n(9d)\nWe aim to find policies that simultaneously optimize two\nconflicting objectives: maximizing the total volume of data\ncollected and minimizing the battery consumption over the\ncourse of the mission. Note that we used the negative of the\nbattery consumed in order to have a maximization problem for\nboth objectives. Constraint (9b) ensures that the UAV starts\nthe mission from the designated start zone and finishes at\nthe terminal zone. Constraint (9c) guarantees that the UAV\nreaches the terminal zone before depleting its battery, while\nconstraint (9d) represents environmental rules and constraints,\nsuch as obstacle avoidance and movement feasibility. The\ncomplexity of the problem arises from several factors:\na: NP-hardness of the underlying path planning problem;\nb: Multi-objective trade-off, where maximizing data collection\noften implies higher battery consumption;\nc: Model uncertainty, particularly due to incomplete or no\nknowledge of the communication environment;\nd: Stochastic initialization, including randomness in key pa-\nrameters such as the number of devices K, their positions\npk, the initial data volumes Dk\n0 for each device k, and the\nUAV\u2019s initial battery capacity.\nTo address these challenges, we propose an attention-based\nMORL algorithm, presented in Section IV.\nIII. MULTI-OBJECTIVE MARKOV DECISION PROCESS\nWe reformulate the problem defined in II as a MOMDP to\nenable its solution via RL. In addition, we introduce several\nstate representation strategies that significantly influence both\nthe design of the learning architecture and the performance.\n5\nA. Definition\nA MOMDP [27] is defined as a tuple pS, A, P, R, \u03b3, \u00b5, fq,\nwhere S is the state space, A is the action space, P is the\ntransition probability function, R : S \u02c6 A \u00dd\u00d1 RM is the\nvector-valued reward function, with M denoting the number of\nobjectives, \u03b3 P r0, 1q is the discount factor, \u00b5 is the initial state\ndistribution, and f is the utility function that aggregates or\nevaluates the trade-offs among the multiple objectives. Given\na policy \u03c0, defined as a mapping from states to actions, the\nmulti-objective value function V \u03c0 is the expected discounted\nreturn obtained by following policy \u03c0, and is defined as:\nV \u03c0 \u201c E\u03c0,s\u201e\u00b5\n\u00ab 8\n\u00ff\nt\u201c0\n\u03b3tRt|s0 \u201c s\nff\n.\n(10)\n\u03c0 is said to Pareto-dominate another policy \u03c01 (denoted \u03c0 \u0105P\n\u03c01) if \u03c0 yields equal or higher value than \u03c01 in all objectives\nand strictly higher value in at least one objective. Formally,\nthis is expressed as p@m : V \u03c0\nm \u011b V \u03c01\nm q ^ pDm : V \u03c0\nm \u0105 V \u03c01\nm q.\nAccordingly, the objective is to identify the set of non-\ndominated policies, denoted by \u03a0\u02da, defined as:\n\u03a0\u02da \u201c t\u03c0\u02da|E\u03c01 : V \u03c01 \u0105P V \u03c0\u02dau.\n(11)\nA preference vector w is introduced to express the relative im-\nportance of each objective, where w P RM satisfies \u0159\nm wm \u201c\n1 and wm \u011b 0, @m. This vector is typically used to compare\nmulti-objective value functions via linear scalarization, defined\nas: fpV \u03c0, wq \u201c wT V \u03c0. For a given preference vector w, if\nno other policy yields a higher scalarized utility than \u03c0, then\nthe corresponding value V \u03c0 is considered non-dominated with\nrespect to that preference [28].\nB. MOMDP conversion\nState space. The system state is defined by the environment\nmap, the UAV state, and the states of the IoT devices. The\nenvironment map is static and represented by the matrix E,\nas defined in (1). At time t, the UAV state is given by\nsUAV\nt\n\u201c pbSC\nt , bt, dt,x, dt,yq, where bt is the current battery\nlevel, bSC\nt\nis the minimum battery required to reach the\ndestination (terminal zone), and dt,x \u201c xf \u00b4xt, dt,y \u201c yf \u00b4yt\nare the relative distances to the terminal position in the x and\ny directions. The state of each IoT device k is represented as\nsk\nt \u201c pdk\nt,x, dk\nt,y, Dk\nt , SNRk\nt , \u03c1k\nt q, where dk\nt,x \u201c xk \u00b4 xt and\ndk\nt,y \u201c yk \u00b4yt are the relative distances between the UAV and\nthe device k, Dk\nt is the remaining data to collect, SNRk\nt is the\ncorresponding SNR, and \u03c1k\nt is a binary indicator equal to 1 if\nthe device is reachable (i.e., if SNRk\nt \u011b \u03a3), and 0 otherwise.\nAction Space. The action space corresponds directly to the\nset of UAV actions defined in (2). At each time step, the\nUAV selects one action from this set, such as hovering or\nmoving to an adjacent cell, which determines its motion\nwithin the environment. Following the execution of an action,\nthe MOMDP transitions to a new state according to the\nenvironment dynamics described in Section II.\nVector reward. The reward provides an instantaneous measure\nof the long-term objectives defined in (9a) and is given by:\nrt \u201c\n\u00ab K\n\u00ff\nk\u201c1\n\u03b4k\nt Ck\nt \u2206t, \u00b4et\nff\n,\n(12)\nwhere the first component represents the amount of data col-\nlected during time step t, and the second component captures\nthe negative of the energy consumed.\nSafety Controller. To reflect realistic operational constraints\nand enforce safety measures, a safety controller is integrated\nto restrict certain actions in the following scenarios:\n\u201a Map boundaries: when the UAV is located at the edge of\nthe map, any action that would result in moving outside\nthe defined grid is masked and thus prevented.\n\u201a Critical battery level: when the minimum battery required\nto reach the destination becomes zero (bSC\nt\n\u201c 0), only\nactions that lead directly to the terminal zone are allowed.\nC. State representation\nState representation plays a critical role in this problem,\nas it directly affects the design of the DNN architecture,\nthe generalization capability of the learned policy, and the\neffective handling of the multi-objective nature of the task.\nFeature Vector (FTV). In our previous work [21], we used the\nmost conventional state representation for this type of problem:\nsFTV\nt\n\u201c rsUAV\nt\n, s1\nt, . . . , sK\nt s.\n(13)\nThis feature vector representation, however, suffers from sev-\neral important limitations: First, it imposes the use of a\nfeedforward neural network architecture with a fixed input\nsize. Specifically, the state vector has a dimensionality of\n4`5K, which prevents the agent from handling scenarios with\na variable number of devices K. Second, the representation is\nsensitive to the ordering of the devices in the state vector.\nPermuting two devices into the defined state in (13) can lead\nthe trained agent to misinterpret the state as different and\nproduce a suboptimal or incorrect action. Third, as shown\nin [14], this representation tends to perform poorly in environ-\nments with variable parameters, such as dynamic or randomly\ninitialized device positions. It lacks the flexibility and inductive\nbias required to generalize effectively across such variations.\nDespite these drawbacks, the feature vector framework does\noffer a practical advantage: it simplifies the implementation\nof standard deep MORL methods that rely on concatenating\nstate and preference vectors as input.\nGlobal/local maps. To address the limitations of the feature\nvector representation, an efficient spatial map-based represen-\ntation was proposed in [16], which incorporates three key map-\nprocessing operations. The state space is defined as:\nS \u201c\nRL\u02c6W \u02c62\nlooomooon\nEnvironment Map\n\u02c6 RL\u02c6W\nloomoon\nDevices map\n\u02c6 BL\u02c6W\nloomoon\nUAV position\n\u02c6\nN\nlomon\nFlying time\n.\n(14)\nWe adapt the spatial map-based representation to our rectan-\ngular grid environment and problem setting. The state St is\nrepresented as the concatenation of following L\u02c6W matrices:\n\u201a E containing building heights (see (1)),\n6\n\u201a Z: a location map with a value of 1 at the start zone, \u00b41\nat the terminal zone, and 0 elsewhere,\n\u201a Dt: a dynamic map indicating the remaining data Dk\nt at\neach IoT device position pk; all other entries are zero,\n\u201a P t: a binary map marking the UAV\u2019s current position pt\nwith a 1, and 0 elsewhere.\nBuilding on this representation, three processing functions are\napplied: fcenter, f l\nlocal, and f g\nglobal. The centering process fcenter\nreturns a matrix centered around the UAV position. The local\nmapping f l\nlocal extracts a central crop of size l \u02c6l, focusing on\nthe UAV\u2019s immediate surroundings. The global mapping f g\nglobal\napplies average pooling over non-overlapping windows of size\ng \u02c6 g, capturing broader contextual information. Two spatial\nmaps are then derived using the previously defined functions:\nSCL\nt\n\u201c f l\nlocalpfcenterpStqq and SCG\nt\n\u201c f g\nglobalpfcenterpStqq. Refer\nto Fig. 2 for a visual illustration and to [16] for further details.\nThis architecture imposes the use of two parallel convolutional\nnetworks (CNNs) [29], whose outputs are flattened and con-\ncatenated in subsequent layers to form a latent representation.\nThis latent vector is then augmented with the remaining flight\ntime bt. While this approach proved to be efficient in terms\nof performance and parameter generalization, we identify\nseveral limitations inherent to spatial map representations. In\nparticular, they require large convolutional models and result in\nmemory-inefficient state representations (note the sparsity of\nthe matrices in (14)). Furthermore, access to the full city map\nE may not always be feasible in practice. Another limitation\nis related to multi-objective learning: within this framework,\nthe preference w can only be incorporated after the flattening\nand concatenation of the CNN outputs, potentially restricting\nits influence on earlier spatial feature extraction stages.\nfcenter\nflocal\nfglobal\nposition\n(41,31)\n(82,62)\n(10,10)\n(16,12)\nFig. 2: Map processing on matrix E with l \u201c 10 and g \u201c 5.\nSet representation. To address the aforementioned drawbacks,\nwe represent the state of the problem as a set and adopt\na DNN architecture that is invariant to permutations of its\nelements. The state is initially defined as tst, s1\nt, . . . , sK\nt u. In\nthis context, additional components such as the preference\nvector w and the cropped local map SCL\nt , corresponding to\nthe FOV of the UAV, can also be included as elements of the\nset. This yields the final state representation:\nst \u201c tsUAV\nt\n, s1\nt, . . . , sK\nt , w, SCL\nt u.\n(15)\nSince the attention mechanism [30], when used without\npositional encoding, is inherently permutation-invariant, we\nleverage it in our MORL algorithm introduced in the fol-\nlowing section. Unlike the spatial map-based approach, the\ncombination of set representations with attention still faces\nlimitations related to fixed input sizes. To address this, we\ndefine a maximum number of devices Kmax, analogous to the\nmaximum sequence length used in language models [30]. This\nupper bound ensures compatibility with the model architecture,\nand in practice, it suffices to choose Kmax large enough to\ncover the expected number of devices in the environment.\nIV. MULTI-OBJECTIVE SOFT ACTOR-CRITIC (MOSAC)\nA. Learning algorithm\nWe build on our previous work, the MOSAC algorithm\n[21], by extending the network architecture and replacing the\nfeedforward DNN with an attention-based model that supports\nthe set-style representation of the state. Our MOSAC algorithm\nhas already demonstrated significant performance improve-\nments over baseline deep MORL methods, primarily due to the\nexploration enhancements we introduced. MOSAC is a multi-\nobjective extension of the discrete Soft Actor-Critic (SAC)\nalgorithm [31], which is a model-free method that optimizes\na trade-off between expected return and policy entropy. It\nmaintains three neural networks, parameterized respectively by\n\u03d5, \u03b8, and \u00af\u03b8: a policy network (estimator) \u03c0\u03d5 : S \u00d1 r0, 1s|A|,\nan action-value (Q-value) estimator Q\u03b8 : S \u00d1 R|A|, and a\ntarget action-value (Q-network) Q\u00af\u03b8 used to stabilize learning.\nTo mitigate overestimation bias in Q-value learning, Q\u03b8 is\nduplicated, following standard practice in actor-critic methods.\nMulti-objective Q estimator. We extend the Q-function\nand the corresponding target expression to accommodate the\nMORL setting. The modified Q objective is defined as:\nJQp\u03b8q \u201c\nE\nps,a,rq\u201eB\nw\u201eDw\n\u201e1\n2}Q\u03b8ps, a, wq \u00b4 yps, a, wq}2\n2\n\u0237\n,\n(16)\nwhere B denotes the replay buffer and Dw is a sampling\ndistribution over the preference space. The target value y\nincorporates both reward and entropy, governed by a trade-\noff parameter \u03bd, and is defined as:\nyps, a, wq \u201c r ` \u03b3Es1 \u201c`\nQ\u00af\u03b8ps1, \u00a8, wq\u00b4\n\u03bd1M log \u03c0ps1, wqT \u02d8\n\u03c0ps1, wq\n\u2030\n,\n(17)\nwhere 1x is the vector of size x with all elements equal to 1.\nMulti-Objective Policy. The policy is updated by minimizing\nthe following objective:\nJ\u03c0p\u03d5q \u201c\nE\ns\u201eB\nw\u201eDw\n\u201c\n\u03c0\u03d5ps, wqT r\u03bd log \u03c0\u03d5ps, wq\u00b4\nQ\u03b8ps, \u00a8, wqT w\n\u2030\u2030\n.\n(18)\nNote that distinct action-values and policies are learned for\ndifferent preference vectors w, as the scalarized utility is\nexplicitly incorporated in the objective function. Furthermore,\nby training on sampled preferences drawn from the distribution\nDw, our DNN estimators gain the ability to generalize across\nthe entire preference space, as demonstrated in [32].\nEntropy trade-off. \u03bd is learned to regulate policy entropy,\nencouraging it to match a target entropy level H0.\n7\nA\u0002en\u0005on Encoder\nxA\nA\u0002en\u0005on Encoder\nLinear\nLinear\nLinear\nScaling\nSo\fmax\nQuery\nKey\nValue\nMLP\nNorm\nMLP\nDevice-wise\nMaxPool\nId\n\ufb02a\u0002ening\nFig. 3: Attention-based DNN architecture used as RL agent."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThe paper presents a novel approach for optimizing energy and data collection in UAV-aided IoT networks using an attention-based Multi-Objective Reinforcement Learning (MORL) architecture. The primary contributions are:\n1. **Formulation as a Multi-Objective Path Planning Problem**: Formulate the problem of UAV-assisted IoT data harvesting as a multi-objective Markov Decision Process (MOMDP), enabling flexible policy learning across varying trade-off preferences.\n2. **Attention-Based Architecture for Parameter Generalization**: Propose an attention-based architecture that is inherently permutation-invariant with respect to the set of IoT devices, allowing it to generalize effectively without requiring fine-tuning or retraining.\n3. **Multi-Objective Q-Function Estimator**: Extend the Q-function and target expression to accommodate the multi-objective setting, ensuring robust learning in complex environments.\n4. **Generalization Across Preference Space**: Train on sampled preferences drawn from a distribution over the preference space, enabling the DNN estimators to generalize across the entire preference spectrum.\n\n#### Methodology\n1. **System Model**:\n   - **Environment and UAV Model**: Discretize urban environments into grids, model UAV dynamics, and define communication channels.\n   - **Channel Model and Data Collection**: Use a Signal-to-Noise Ratio (SNR) model to estimate data collection success rates based on the quality of wireless links between UAVs and IoT devices.\n2. **Multi-Objective Markov Decision Process (MOMDP)**:\n   - Define the problem as maximizing total collected data volume while minimizing battery consumption, incorporating constraints such as mission duration, environmental rules, and stochastic initialization.\n3. **Attention-Based Multi-Objective Reinforcement Learning (MOSAC) Algorithm**:\n   - Utilize an attention-based architecture to handle varying trade-off preferences dynamically without needing fine-tuning or retraining.\n4. **State Representation**:\n   - Use a set representation that is invariant to permutations of IoT devices, allowing for effective handling of the multi-objective nature of the task.\n\n#### Key Findings\n1. **Performance Evaluation**: Extensive simulations demonstrate significant improvements in performance compared to existing RL solutions, including substantial gains in sample efficiency and generalization to unseen scenarios.\n2. **Generalization Capabilities**: The proposed approach achieves superior zero-shot generalization, performing well both within training scenarios and in unseen environments with varying numbers of devices, spatial configurations, data volumes, and initial battery levels.\n\n#### Conclusions\nThe paper addresses the challenges of balancing multiple objectives (data collection vs. energy consumption) in UAV-aided IoT networks by leveraging attention-based multi-objective reinforcement learning. The proposed approach outperforms existing methods in terms of performance, sample efficiency, and generalization capabilities across dynamic environments. This work contributes to improving real-world applications of RL for complex, multi-objective optimization problems in wireless communication networks."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "PREPRINT. UNDER REVIEW.\n1\nGroup-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill\nRepresentations for Generalizable Behavior\nJunwoo Chang , Joseph Park , Roberto Horowitz , Jongmin Lee , and Jongeun Choi\nAbstract\u2014Unsupervised skill discovery aims to acquire behav-\nior primitives that improve exploration and accelerate down-\nstream task learning. However, existing approaches often ignore\nthe geometric symmetries of physical environments, leading to\nredundant behaviors and sample inefficiency. To address this, we\nintroduce Group-Invariant Skill Discovery (GISD), a framework\nthat explicitly embeds group structure into the skill discovery\nobjective. Our approach is grounded in a theoretical guarantee:\nwe prove that in group-symmetric environments, the standard\nWasserstein dependency measure admits a globally optimal\nsolution comprised of an equivariant policy and a group-invariant\nscoring function. Motivated by this, we formulate the Group-\nInvariant Wasserstein dependency measure, which restricts the\noptimization to this symmetry-aware subspace without loss of\noptimality. Practically, we parameterize the scoring function\nusing a group Fourier representation and define the intrinsic\nreward via the alignment of equivariant latent features, ensuring\nthat the discovered skills generalize systematically under group\ntransformations. Experiments on state-based and pixel-based\nlocomotion benchmarks demonstrate that GISD achieves broader\nstate-space coverage and improved efficiency in downstream task\nlearning compared to a strong baseline.\nIndex Terms\u2014Skill discovery, Group equivariance, Unsuper-\nvised reinforcement learning, Robotics\nI. INTRODUCTION\nU\nNsupervised skill discovery has become a central tool for\nacquiring reusable behavioral primitives that can acceler-\nate downstream task learning. A wide range of methods learn\nlatent skills by maximizing mutual information or distance-\nbased objectives between skills and state trajectories [2]\u2013[7].\nThese methods have shown that pretraining a diverse set of be-\nhaviors can substantially improve exploration, state coverage,\nand downstream task learning efficiency across challenging\ncontinuous-control benchmarks.\nDespite this progress, existing approaches often struggle to\npretrain useful skills efficiently. Standard distance-maximizing\nobjectives treat the state space as largely unstructured, and\n(Corresponding authors: Jongeun Choi and Jongmin Lee.)\nJunwoo\nChang,\nJoseph\nPark,\nand\nJongeun\nChoi\nare\nwith\nthe\nSchool\nof\nMechanical\nEngineering,\nYonsei\nUniversity,\nSeoul,\nSouth\nKorea\n(e-mails: junwoochang@yonsei.ac.kr;\niamjoseph1129@yonsei.ac.kr; jongeunchoi@yonsei.ac.kr).\nJongmin Lee and Jongeun Choi are with the Department of Artificial\nIntelligence,\nYonsei\nUniversity,\nSeoul,\nSouth\nKorea\n(e-mails:\njongminlee@yonsei.ac.kr; jongeunchoi@yonsei.ac.kr).\nJongeun Choi and Roberto Horowitz are with the Department of Mechanical\nEngineering, University of California, Berkeley, CA 94720, USA (e-mail:\nhorowitz@berkeley.edu)\nThe authors used the ChatGPT large language model (OpenAI) [1] to polish\ntexts. All texts written with LLM assistance was reviewed and verified by the\nauthors.\nThis work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may\nno longer be accessible.\nmust discover all behaviors directly in a high-dimensional\nlatent space. Recent works attempt to impose additional priors\nto mitigate this difficulty\u2014for example, LGSD [8] leverages\nlanguage guidance to bias the skill space toward semantically\nmeaningful behaviors, while \u201cDo\u2019s and Don\u2019ts\u201d [9] leverage\ndesired behavioral preference priors. However, these methods\nstill overlook a fundamental property of many robotic envi-\nronments: the presence of strong geometric symmetries (e.g.,\nrotations of a locomotion agent) that make many behaviors\nequivalent up to a group transformation. When such a structure\nis ignored, skill discovery tends to learn redundant variants of\nthe same behavior, wasting samples and weakening general-\nization.\nTo address this limitation, we introduce Group-Invariant\nSkill Discovery (GISD), a framework that injects group\nsymmetry directly into the skill discovery objective and repre-\nsentation. Our approach builds upon a key theoretical insight:\nin a group-invariant environment, the Wasserstein dependency\nmeasure [10] admits a globally optimal solution comprised of\nan equivariant policy and a group-invariant scoring function\nf. This existence guarantee implies that we can safely restrict\nthe search space to a class of group-invariant scoring func-\ntions without any loss of optimality. Based on this principle,\nwe formulate the Group-Invariant Wasserstein dependency\nmeasure (GIWDM), which directly optimizes the objective\nwithin this reduced subspace. GIWDM thus eliminates the\nredundancy of exploring non-symmetric solutions and ensures\nthat the discovered skills inherently respect the environmental\nsymmetry. Building on this theory, we parameterize the group-\ninvariant scoring function in a group Fourier space, discovering\nskills whose latent representations explicitly encode the group\nsymmetry. Once such symmetry-aware skills are learned, we\ncan exploit the same structure: the agent can reuse a skill at\nnovel configurations by transforming its latent representation\nunder the group, enabling systematic generalization across\ngroup transformations.\nThe contributions of our work are summarized as follows:\n1)We prove that, in group-symmetric MDPs, the Wasserstein\nDependency Measure (WDM) admits a globally optimal solu-\ntion consisting of an equivariant policy and a group-invariant\nscoring function f. This result justifies Group-Invariant Skill\nDiscovery (GISD), which restricts f to symmetry-respecting\nfunctions without sacrificing optimality. 2) We instantiate\nGISD by parameterizing the group-invariant scoring func-\ntion f in the group Fourier domain. The intrinsic reward\nis defined via the alignment of equivariant Fourier features,\nproducing skill representations that are group-symmetric by\nconstruction. We also show that this symmetric representation\nenables generalizable skill implementation under the group\narXiv:2601.14000v1  [cs.RO]  20 Jan 2026\nPREPRINT. UNDER REVIEW.\n2\ntransformation in downstream tasks. 3) We evaluate GISD on\nstate-based and pixel-based locomotion benchmarks, showing\nimproved sample efficiency and state-space coverage over a\nstrong distance-based baseline.\nII. RELATED WORK\nA. Unsupervised Skill Discovery\nUnsupervised skill discovery seeks to learn diverse be-\nhaviors in reinforcement learning (RL) without relying on\nexplicit reward signals and reuse them for efficient downstream\ntask training. A common strategy is to maximize the depen-\ndency between states and skills, leading to distinct trajectories\naccording to the different latent skill vectors. One of the\nclasses of unsupervised skill discovery contains leveraging the\nmutual information (MI). MI-based approaches maximize the\ncorrelation between skill and state, typically via a Kullback-\nLiebler (KL)-divergence formulation [2], [4], [11], [12]:\nI(S; Z) = DKL (p(s, z)\u2225p(s)p(z))\n=\nZ\nS\u00d7Z\np(s, z) log p(s, z)\np(s)p(z) dsdz.\nWhile they effectively uncover distinct skills, their reliance on\nKL divergence can limit overall state coverage. To address\nthe limitation of MI-based approach, distance-maximizing\napproaches, which is the other class in unsupervised skill\ndiscovery, replace MI with the 1-Wasserstein distance (or Kan-\ntorovich metric) for tractability, promoting both distinctness\nand a broader exploration of states [5]\u2013[8]:\nIW(S; Z) = W\n\u0000p(s, z), p(s)p(z)\n\u0001\nThese approaches map states to a compact latent space that\npreserves a chosen distance metric (e.g., temporal distance [5],\ncontrollability distance [7], or language distance [8]), thereby\npromoting both broad exploration and the discovery of skills\nthat are well-separated under the metric.\nBased on distance-maximizing unsupervised skill discovery,\nrecent studies have proposed methods to improve the quality\nof skills through the addition of prior knowledge. For example,\none of the prior works, LGSD [8], utilizes language prior to\nimprove semantic diversity between skills. In another prior\nwork, Do\u2019s and Don\u2019ts [9], they utilize the prior knowledge\nof desired state transition for feasible skill discovery. In this\nwork, we also build upon distance-maximizing approaches by\nintegrating a geometric prior, but our method differs in that\nit discovers skills directly from the inherent group symmetric\nstructure of the space, and thereby, the skills are enabled to\nbe discovered with the underlying geometric symmetry.\nB. Group Equivariance in Reinforcement Learning\nGroup equivariance is a mathematical property that char-\nacterizes how a function\u2019s output transforms in response to a\nspecified group action on its input. Incorporating this prop-\nerty into neural networks ensures that transformations of the\ninput data induce predictable and consistent transformations\nof the output [13]\u2013[18]. Recent works have explored group\nequivariance in robotic settings [19]\u2013[27]: for example, some\nprior works [19]\u2013[21] apply group-equivariant convolutions to\npick-and-place tasks, and other prior works [22]\u2013[25] leverage\nroto-translation symmetry in three-dimensional environments,\nnotably improving data efficiency and generalization. Such\nbenefits are particularly relevant for RL, where sample ef-\nficiency remains a central challenge. Several studies have\ndemonstrated that integrating group equivariance into RL\nalgorithms yields enhanced performance and faster learning\ncompared to standard methods [28]\u2013[37]. Notably, some works\n[28], [29] introduce the concept of group-invariant Markov\ndecision process (MDPs), offering a formal basis for these im-\nprovements. Building upon these successes, our work aims to\nincorporate group equivariance into unsupervised skill discov-\nery, thereby addressing limitations in existing approaches and\nfurther advancing the potential of symmetry-based methods.\nIII. PRELIMINARIES\nA. Distance-Maximizing Skill Discovery\nDistance-maximizing skill discovery approaches [5]\u2013[8],\n[38] employ the Wasserstein dependency measure (WDM)\n[10], an equivalent formulation of the 1-Wasserstein distance\ndue to the Kantorovich-Rubinstein duality [39]. Concretely,\nthe objective is given by:\nIW(S; Z) = W1(p(s, z), p(s)p(z))\n=\nsup\n\u2225f\u2225L\u22641\n\u0000Ep(s,z)[f(s, z)] \u2212Ep(s)p(z)[f(s, z)]\n\u0001\nwhere W1 represents 1-Wasserstein distance and \u2225\u00b7 \u2225L indi-\ncates the 1-Lipschitz constraint. The function f is parameter-\nized as f(s, z) = \u03d5(s)\u22a4\u03c8(z), mapping the state space S to\na latent space via \u03d5 : S \u2192RD. In addition, by imposing\nadditional conditions, replacing S with the terminal state ST ,\nsetting \u03c8(z) = z, and sampling z from a zero-mean prior p(z),\nthe objective can be approximated through a telescoping sum:\nIW(ST ; Z) \u2248\nsup\n\u2225\u03d5\u2225L\u22641\nEp(\u03c4,z)\nT \u22121\nX\nt=0\n(\u03d5(st+1) \u2212\u03d5(st))\u22a4z\n(1)\nConsequently, a corresponding reward function is defined\nas rt(st, z, st+1) = (\u03d5(st+1) \u2212\u03d5(st))\u22a4z. The 1-Lipschitz\nconstraint \u2225\u03d5(st+1) \u2212\u03d5(st)\u2225\u2264d(st+1, st) preserves the\ndistance metric in the latent space. A skill conditioned policy\n\u03c0(s, z) is trained via RL to maximize the WDM objective,\nthereby promoting trajectories that accumulate larger defined\ndistance in the direction of z.\nB. Group Equivariance and Representation Theory\nIf a transformation leaves a property of an object or\nsystem unchanged, that transformation is called a symmetry\n[40]. Symmetries satisfy the axioms of associativity, identity,\ninverse, and closure, and the collection of all such transforma-\ntions forms a group. In our setting, the relevant symmetry is\nplanar rotation, represented by the continuous group SO(2).\nFor practical implementation, we approximate this continuous\nsymmetry using a discrete cyclic subgroup CN, where rota-\ntions are quantized in increments of 2\u03c0/N.\nPREPRINT. UNDER REVIEW.\n3\nA group representation \u03c1 : G \u2192GL(n) maps each\ngroup element g \u2208G to an invertible n \u00d7 n matrix that\ndescribes how G acts on a vector space. Examples include\nthe trivial representation \u03c10(g)x = x, which leaves the space\nunchanged, and the regular representation on R|G|, which\ncan be decomposed into irreducible representations \u03c1k that\ncannot be further reduced. For SO(2), the complex irreducible\nrepresentations take the form \u03c1k(\u03b8) = eik\u03b8. In practice, we\nuse a real-valued form by separating real and imaginary parts\ninto a two-dimensional feature vector. Any finite-dimensional\nrepresentation \u03c1 of a compact group can be written, up to a\nchange of basis, as a direct sum of irreducible representations:\n\u03c1(g) = Q\nK\nM\nk=1\n\u03c1k(g)\n!\nQ\u22121,\nfor some invertible matrix Q.\nA function f : X \u2192Y is equivariant with respect to\na group action if applying the group before or after f is\nequivalent, i.e.,\n\u03c1Y (g)f(x) = f(\u03c1X(g)x),\n\u2200g \u2208G, x \u2208X.\nIf the output does not change under the group action on the\ninput, f(x) = f(\u03c1X(g)x), then f is invariant. Throughout\nthe paper we use the shorthand gx to denote the group action\n\u03c1(g)x for notational simplicity.\nC. Group-invariant MDPs\nA group-invariant MDP [28], [29] builds on the notion\nof MDP homomorphism [41], [42] and provides an abstract\nrepresentation of an underlying MDP that respects symmetry.\nDenoted as MG(S, A, P, R, G), this formulation encodes how\na symmetry group G acts on states and actions. When the\nreward and transition dynamics satisfy the group-invariance\nconditions\nR(s, a) = R(gs, ga),\nP(s\u2032|s, a) = P(gs\u2032|gs, ga),\nThe optimal policy and optimal value functions of the original\nMDP can be derived directly from the symmetry-reduced\nMDP. This reduction provides both computational efficiency\nand structural guarantees tied to the underlying group.\nIV. PROBLEM STATEMENT\nIn line with previous unsupervised skill discovery ap-\nproaches, the problem is formalized within the framework\nof a reward-free MDP, denoted as M = (S, A, P). S, A,\nand P represent the state space, action space, and transition\nprobabilities P : S \u00d7 A \u2192S, respectively. The latent skill\nvector z \u2208Z is sampled from the prior distribution, z \u223cp(z).\nThe policy is conditioned by the skill vector, \u03c0(a|s, z), the\nskill vector informing the policy to select the corresponding\naction sequence. During the training, the skill vector doesn\u2019t\nchange during the episode, and the policy is trained with-\nout any reward supervision, only with the intrinsic reward.\nIn this framework, we leverage the group symmetric prior,\nprior knowledge of rotation symmetry, to enhance the sample\nefficiency and generalizability. We convert the original MDPs\nto group-invariant MDPs by assuming the group-invariance of\nthe intrinsic reward function and transition probability.\nTrajectory \ud835\udf0f\nState space \ud835\udce2\nLatent Fourier skill space \ud835\udc81\nRotated\ntrajectory \ud835\udc20\ud835\udf49\n\ud835\udf53\ud835\udc6d: \ud835\udcae\u2192\ud835\udc81\nSkill generalizes\nalong its group orbit\n\ud835\udc9b\n\ud835\udc88\ud835\udc9b\n\ud835\udc7a\ud835\udc76\ud835\udfd0 orbit of a skill z\nFig. 1. Overview of Group-Invariant Skill Discovery (GISD). Our method\nlearns an equivariant mapping \u03d5F from the state space S to the Group\nFourier space Z. By aligning state transitions with latent skill vectors z in the\ngroup Fourier domain, the discovered skills inherently respect the underlying\ngeometric symmetry. This structure enables generalization: a policy trained\nfor a specific trajectory \u03c4 automatically generalizes to any group-transformed\ntrajectory g\u03c4 by simply shifting the skill vector along its group orbit to gz.\nV. METHODS\nIn this section, we present Group-Invariant Skill Discovery\n(GISD), a method that leverages the symmetry of the under-\nlying dynamics to accelerate unsupervised skill discovery. We\nbegin by showing that, in a group-invariant environment, the\nstandard Wasserstein dependency measure (WDM) objective\nalready admits symmetry-respecting optimal solutions. Mo-\ntivated by this result, we then introduce a Group-Invariant\nWDM formulation and a practical Fourier-space parameter-\nization and intrinsic reward.\nA. Theoretical Analysis: Symmetry of Optimal Solutions\nBefore introducing GISD, we analyze the structure of\noptimal solutions to the standard WDM-based skill discov-\nery problem. We show that, under group-invariant dynamics,\nrestricting the search to symmetry-aware policies does not\ncompromise optimality.\nTheorem 1 (Existence of Equivariant Optima). In an MDP\nwhere the dynamics, initial state distribution, skill prior, and\nground metric are all group-invariant, the Wasserstein depen-\ndency measure admits a globally optimal solution (\u00af\u03c0, \u00aff) such\nthat\n\u00af\u03c0(ga | gs, gz) = \u00af\u03c0(a | s, z),\n\u00aff(gs, gz) = \u00aff(s, z),\n\u2200g \u2208G.\nIn other words, among all WDM global maximizers, there\nexists at least one policy-function pair with an equivariant\npolicy and a group-invariant 1-Lipschitz function.\nFor proof, see Appendix VII-A. This theoretical guar-\nantee implies that the exploration of non-equivariant poli-\ncies\u2014which correspond to redundant or suboptimal solutions\nin a symmetric environment\u2014is unnecessary. This justifies\nrestricting the search to the symmetry-aware function class\nwithout sacrificing global optimality.\nB. Group-Invariant Wasserstein Dependency Measure\nTheorem 1 shows that in a group-invariant environment,\nthe standard Wasserstein dependency measure (WDM) admits\nPREPRINT. UNDER REVIEW.\n4\nat least one global optimum where the policy is equivariant\nand the scoring function f is group-invariant. This motivates\nexplicitly restricting the scoring function class to symmetry-\nrespecting functions: instead of hoping that symmetry emerges\nimplicitly, we bake it into the objective.\nLet G act on the state space S and skill space Z, and let\nd be a distance metric on S \u00d7 Z that is group-invariant under\nthe joint action:\nd((gs, gz), (gs\u2032, gz\u2032)) = d((s, z), (s\u2032, z\u2032)),\n\u2200g \u2208G.\nWe define the group-invariant scoring function class FG as the\nset of 1-Lipschitz functions f : S \u00d7 Z \u2192R that are invariant\nunder the joint action of G:\nFG :=\n\b\nf\n\u2225f\u2225L \u22641, f(gs, gz) = f(s, z), \u2200g \u2208G\n.\nDefinition\n1\n(Group-Invariant\nWDM).\nGiven\na\ngroup-\ninvariant metric d on S \u00d7 Z, the group-invariant Wasserstein\ndependency measure is defined as\nIG\nW(S; Z) := sup\nf\u2208FG\nEp(s,z)[f(s, z)] \u2212Ep(s)p(z)[f(s, z)]).\nThis constrained objective guarantees invariance under the\ngroup action, a property not generally enjoyed by the standard\nWDM.\nProposition 1. If the distance metric d is group-invariant, then\nthe group-invariant WDM satisfies\nIG\nW(gS; gZ) = IG\nW(S; Z)\nfor all g \u2208G.\nProof is provided in Appendix VII-B.\nReduction of the search space.\nCompared to the standard\nWDM, which optimizes over all 1-Lipschitz functions, IG\nW\nrestricts the scoring function class to the group-invariant\nsubset FG. Based on Theorem 1, this restriction does not\neliminate any globally optimal symmetric solution, but acts as\na geometric inductive bias: it rules out functions that encode\nspurious dependencies and focuses optimization on symmetry-\nconsistent structure. In the following subsections, we show\nhow to construct such group-invariant scoring functions via\ngroup averaging and how to parameterize them efficiently in\nFourier space.\nC. Group-Averaged Scoring Functions\nThe group-invariant WDM in Definition 1 restricts the\nadmissible scoring functions (the 1-Lipschitz functions in the\ndual formulation) to the invariant class FG. We now construct\nsuch group-invariant scoring functions from arbitrary admis-\nsible 1-Lipschitz functions via a standard group averaging\noperation.\nDefinition 2 (Group-averaged Scoring Function). For any 1-\nLipschitz function f : S \u00d7 Z \u2192R (with respect to the metric\nd), we define its group average as\n\u02dcf(s, z) :=\nZ\nG\nf(gs, gz) d\u00b5(g),\nwhere \u00b5 is the normalized Haar measure on G (Ap-\npendix VII-H).\nThis operator maps a general admissible scoring function f\nto a group-invariant function while preserving the 1-Lipschitz\nconstant:\nProposition 2 (Properties of the group-averaged scoring func-\ntion). Let f : S \u00d7 Z \u2192R be 1-Lipschitz with respect to a\ngroup-invariant metric d on S \u00d7 Z. Then its group average\n\u02dcf is group-invariant and 1-Lipschitz with respect to d. In\nparticular, \u02dcf \u2208FG.\nWe provide the proof in Appendix VII-C. The group aver-\naging operator provides the precise link between the standard\nand group-invariant WDM: given any optimal scoring function\nf \u2217in the unconstrained class, its group average \u02dcf \u2217is feasible\nin FG (Proposition 2) and, assuming an equivariant optimal\npolicy, achieves the same objective value. Thus, for symmetric\noptima, the restricted and unconstrained WDMs coincide.\nD. Skill Discovery in the Group Fourier Space\nIn contrast to the decompositions of f(s, z) used in prior\nworks [5]\u2013[7], we derive our parameterization from a group\nrepresentation-theoretic viewpoint. Assume a compact group\nG acts on the state space S and latent skill space Z. We\nconsider the induced action of the product group G \u00d7 G on\nS \u00d7 Z, given by (g1, g2) \u00b7 (s, z) = (g1s, g2z), and define\nfg1,g2(s, z) := f(g1s, g2z).\nFor each fixed (s, z), the map (g1, g2) 7\u2192fg1,g2(s, z) is\nsquare-integrable on G \u00d7 G, i.e., f(\u00b7,\u00b7) \u2208L2(G \u00d7 G). By the\nPeter\u2013Weyl theorem [43], this function admits an expansion\nin terms of the irreducible representations of G \u00d7 G (see\nAppendix VII-I for details):\nfg1,g2(s, z) =\nX\n\u03c1,\u03c3\u2208\u02c6\nG\nd\u03c1d\u03c3 Tr\n\u0000A\u03c1,\u03c3(s, z) (\u03c1(g1)\u2297\u03c3(g2))\n\u0001\n, (2)\nwhere \u02c6G denotes the set of irreducible representations of G,\nd\u03c1 and d\u03c3 are their dimensions, and A\u03c1,\u03c3(s, z) are coefficient\nmatrices depending on (s, z).\nTo enforce the group-invariance required in Definition 1, we\napply the group average over the diagonal action g 7\u2192(g, g):\n\u02dcf(s, z) =\nZ\nG\nfg,g(s, z) d\u00b5(g) =\nZ\nG\nf(gs, gz) d\u00b5(g).\nSubstituting Eq. (2), we utilize Schur\u2019s Lemma [44] and the\northogonality relations of irreducible representations:\nZ\nG\n\u03c1(g) \u2297\u03c3(g) d\u00b5(g) \u0338= 0 \u21d4\u03c3 \u223c= \u03c1\u2217.\nThis averaging operation annihilates all non-matching com-\nponents (\u03c1 \u0338= \u03c3\u2217) and projects the remaining terms onto the\ngroup-invariant scalar subspace. Consequently, the double sum\ncollapses into a single sum over the spectrum:\n\u02dcf(s, z) =\nX\n\u03c1\u2208\u02c6\nG\n\u03bb\u03c1(s, z),\nPREPRINT. UNDER REVIEW.\n5\nwhere each \u03bb\u03c1(s, z) represents the scalar contraction of the\nmatching representation blocks.\nWe now parameterize these scalar coefficients in terms\nof Fourier-space features of s and z. For each surviving\nirrep \u03c1, we introduce feature vectors \u03d5\u03c1(s) and \u03c8\u03c1(z) in the\ncorresponding representation spaces and parameterize\n\u03bb\u03c1(s, z) \u2248\n\u03d5\u03c1(s), \u03c8\u03c1(z)\n.\nStacking all irrep components into a single global feature\nvector, we obtain the maps\n\u03d5F : S \u2192Rd,\n\u03c8F : Z \u2192Rd,\nand the group-averaged scoring function takes the inner-\nproduct form:\n\u02dcf(s, z) \u2248\n\u03d5F (s), \u03c8F (z)\n.\nBy construction, \u03d5F maps states into the spectral domain and\nsatisfies the equivariance property:\n\u03d5F (gs) = \u03c1F (g)\u03d5F (s),\n(3)\nwhere \u03c1F (g) denotes the block-diagonal direct sum of unitary\nirreducible representations acting on the feature vector.\nFollowing METRA [5], we set \u03c8F (z) = z, treating the\nlatent skill z as a unit-norm vector in Fourier space. This\nchoice, together with a 1-Lipschitz parameterization of \u03d5F ,\nis compatible with the 1-Lipschitz constraint required by the\nWDM objective.\nFrequency Filtering and Interpretability. A key advan-\ntage of working in Fourier space is the explicit separation\nof geometric frequencies. The feature vector \u03d5F (s) is a\nconcatenation of coefficients corresponding to different ir-\nreducible representations (frequencies). Unlike unstructured\nlatent spaces, this allows us to manually emphasize or suppress\nspecific symmetries by applying a frequency mask M:\n\u02dcf(s, z) =\nM \u00b7 \u03d5F (s), z\n.\nFor example, by selecting only the trivial representation (\u03c10),\nwe can discover purely group-invariant skills. By selecting\nhigher-order frequencies, we encourage skills that are sensitive\nto the group action. This provides a degree of interpretability\nand controllability unavailable in standard unsupervised skill\ndiscovery.\nE. Group-Invariant MDP for Skill Discovery\nUsing the group-invariant scoring function parameterization\nfrom Section V-D, we now derive the intrinsic reward used for\nskill discovery and show that it is itself group-invariant.\nRecall that METRA [5] optimizes a WDM-style objective\nover trajectories (Eq. (1)). In our framework, we restrict to\ngroup-invariant scoring functions and parameterize the group-\naveraged scoring function as an inner product in Fourier space,\n\u02dcf(s, z) \u2248\n\u03d5F (s), z\n,\nwhere \u03d5F is equivariant with respect to the group action\n(Section V-D). Substituting this parameterization into Eq. (1)\nleads to\nIW(ST ; Z) \u2248\nsup\n\u2225\u03d5F \u2225L\u22641\nT \u22121\nX\nt=0\n\u0010\nE\u03c4,z\n\u0002\n\u27e8\u03d5F (st+1) \u2212\u03d5F (st), z\u27e9\n\u0003\n\u2212E\u03c4\n\u0002\n\u27e8\u03d5F (st+1) \u2212\u03d5F (st), Ez[z]\u27e9\n\u0003\u0011\n.\n(4)\nAs in METRA, we choose p(z) to have zero mean Ep(z)[z] =\n0, in which case the second term vanishes and (4) reduces to\nIW(ST ; Z) \u2248\nsup\n\u2225\u03d5F \u2225L\u22641\nE\u03c4,z\n\" T \u22121\nX\nt=0\n\u27e8\u03d5F (st+1) \u2212\u03d5F (st), z\u27e9\n#\n.\n(5)\nBecause the sum in (5) is telescoping, it is natural to\ninterpret the incremental term as an intrinsic reward:\nr(st, z, st+1) :=\n\u03d5F (st+1) \u2212\u03d5F (st), z\n.\n(6)\nWe now show that this reward is group-invariant. Let h \u2208G\nact on states and skills as s\u2032 = hs and z\u2032 = hz. Using the\nequivariance of \u03d5F (Eq. (3)) and the fact that the representation\n\u03c1(h) is unitary, we obtain\nr(hst, hz, hst+1) =\n\u03d5F (hst+1) \u2212\u03d5F (hst), hz\n=\n\u03c1(h)\n\u0000\u03d5F (st+1) \u2212\u03d5F (st)\n\u0001\n, \u03c1(h)z\n=\n\u03d5F (st+1) \u2212\u03d5F (st), z\n= r(st, z, st+1),\nwhere\nthe\nthird\nequality\nuses\nunitarity\nof\n\u03c1(h):\n\u27e8\u03c1(h)x, \u03c1(h)y\u27e9\n=\n\u27e8x, y\u27e9. Thus, the intrinsic reward (6)\nis invariant under the joint group action on states and skills.\nThis invariance leads to a significant theoretical result for\nour learning process. Because the environment dynamics are\ngroup-invariant (by assumption) and the derived intrinsic\nreward r is group-invariant (as shown above), the skill\ndiscovery problem constitutes a group-invariant MDP [29].\nThis structural property guarantees the existence of an optimal\nequivariant policy \u03c0\u2217.\nWe now connect this architectural choice back to the theo-\nretical guarantees of our objective:\nRemark 1 (Exactness for Equivariant Policies). While Theo-\nrem 1 guarantees the existence of a symmetric global optimum,\nour method parameterizes the policy \u03c0 to be equivariant based\non the group-invariant MDP. In a group-invariant MDP, an\nequivariant policy induces a group-invariant joint distribution\np(s, z) (Lemma 1). Under such a symmetric distribution, the\noptimal scoring function f \u2217for the unconstrained WDM is\ninherently group-invariant (as non-symmetric components of\nthe scoring function average to zero or contribute nothing\nto the objective). Therefore, for our equivariant policy class,\noptimizing the restricted objective IG\nW is mathematically equiv-\nalent to optimizing the standard WDM, ensuring no loss of\nexpressivity.\nPREPRINT. UNDER REVIEW.\n6\nAlgorithm 1 Group-Invariant Unsupervised Skill Discovery\n1: Initialize: Equivariant policy \u03c0(a|s, z), Fourier mapping\n\u03d5F (s), dual variable \u03bb, replay buffer D\n2: for epoch = 1, . . . , N do\n3:\nfor episode = 1, . . . , M do\n4:\nSample skill z \u223cp(z)\n5:\nCollect trajectory \u03c4 using \u03c0(a|s, z)\n6:\nStore transitions (s, a, s\u2032, z) in D\n7:\nend for\n8:\n1. Update Discriminator \u03d5F :\n9:\nOptimize J\u03d5 (Eq. (8))\n10:\n2. Update Dual Variable \u03bb:\n11:\nOptimize J\u03bb (Eq. (9))\n12:\n3. Update Policy \u03c0:\n13:\nCompute rewards r = \u27e8\u03d5F (s\u2032) \u2212\u03d5F (s), z\u27e9\n14:\nUpdate \u03c0 using symmetry-aware SAC [29], [37]\n15: end for\nF. Group Symmetry in Downstream Tasks\nAfter skill discovery, prior works [5]\u2013[7] reuse the discov-\nered skills for downstream tasks via a hierarchical architecture.\nA high-level policy \u03c0h(z | s, goal) selects a latent skill condi-\ntioned on the current state and goal, while a low-level policy\n\u03c0l(a | s, z) executes primitive actions using the pretrained\nskill-conditioned policy (with parameters frozen).\nThis interaction induces a Fixed-interval Semi-MDP [45],\nwhere the high-level transition dynamics depend on the cumu-\nlative effect of the low-level policy over k steps. We formally\ndefine this transition probability Pk:\nDefinition 3 (Fixed-interval High-level Transition). Given a\nfixed interval k, the high-level transition kernel induced by the\nlow-level policy \u03c0l is defined as the k-step roll-out:\nPk(st+k | st, z) :=\nZ\n\u00b7 \u00b7 \u00b7\nZ\nk\u22121\nY\ni=0\n\u0014\n\u03c0l(at+i | st+i, z) \u00b7 P l(st+i+1 | st+i, at+i)\n\u0015\nd\u03c4int\n(7)\nwhere P l denotes the one-step transition kernel of the under-\nlying MDP, and the integration is over all intermediate states\nand actions \u03c4int = {at:t+k\u22121, st+1:t+k\u22121}.\nFor downstream tasks, the extrinsic reward can typically\nbe chosen to be group-invariant (e.g., sparse goal-reached\nreward). We now show that if the environment dynamics and\nlow-level skill policy are symmetric, then the induced high-\nlevel semi-MDP is itself group-invariant.\nTheorem 2 (Group-invariant Fixed-interval Semi-MDP). As-\nsume the environment dynamics are group-invariant and the\npretrained skill policy is equivariant:\nP l(gs\u2032 | gs, ga) = P l(s\u2032 | s, a)\n\u03c0l(ga | gs, gz) = \u03c0l(a | s, z)\n\u2200g \u2208G.\nThen, the induced high-level transition kernel (Eq. (7)) satisfies\ngroup invariance:\nPk(gst+k | gst, gz) = Pk(st+k | st, z),\n\u2200g \u2208G.\nConsequently, the fixed-interval semi-MDP is group-invariant\nwhenever the high-level reward is group-invariant.\nThe proof is provided in Appendix VII-F.\nGeneralizable Skills.\nTheorem 2 establishes that our pre-\ntrained skills induce a group-invariant fixed-interval semi-\nMDP. Since the skills are represented in the Fourier space\nand encode symmetric features by construction, an equivariant\nhigh-level policy \u03c0h(z | s, g) can systematically exploit this\nstructure. Because the transition dynamics are group-invariant,\na high-level policy trained in one group element can generalize\nto any other group elements by simply transforming the skill\nvector z according to the group action (Fig. 1). This allows\nfor highly sample efficient downstream task training.\nG. Practical Implementation\nTo finalize the training objective, we must specify the\ndistance metric d for the Lipschitz constraint. We adopt the\ntemporal distance dT (s, s\u2032) [5], which measures the expected\nnumber of steps between states. Since the underlying MDP\ndynamics are group-invariant, dT is inherently group-invariant\n(see Appendix VII-E for proof), satisfying the metric require-\nment of Definition 1.\nOur training procedure follows the dual gradient descent\nmethod of METRA, but substitutes the standard discriminator\nwith our equivariant Fourier mapping \u03d5F . The discriminator\nparameters are updated to minimize the following loss, which\nbalances skill alignment against the Lipschitz constraint via a\nLagrange multiplier \u03bb:\nJ\u03d5 = ED\nh\n\u27e8\u03d5F (s\u2032) \u2212\u03d5F (s), z\u27e9\n+ \u03bb \u00b7 min\n\u0000\u03f5, 1 \u2212\u2225\u03d5F (s\u2032) \u2212\u03d5F (s)\u22252\n2\n\u0001i\n,\n(8)\nJ\u03bb = ED\nh\n\u03bb \u00b7 min\n\u0000\u03f5, 1 \u2212\u2225\u03d5F (s\u2032) \u2212\u03d5F (s)\u22252\n2\n\u0001i\n,\n(9)\nwhere D is the replay buffer. The policy \u03c0(a | s, z) is\nupdated using group equivariant SAC [29] or PE-SAC [37]\nto maximize the intrinsic reward r = \u27e8\u03d5F (s\u2032) \u2212\u03d5F (s), z\u27e9.\nVI. EXPERIMENTS\nWe evaluate GISD against the state-of-the-art unsupervised\nskill discovery method, METRA [5]. Our goal is to understand\nhow explicitly exploiting symmetry during skill discovery and\ndownstream task training affects (i) sample efficiency, (ii)\nstate-space coverage, and (iii) downstream task performance.\nWe follow the overall experimental setup of METRA and\nconsider both state-based and pixel-based locomotion with\ncontinuous latent skills.\nA. Benchmark environments and Symmetry Structure\nWe use two benchmark locomotion tasks with known geo-\nmetric symmetries.\nPREPRINT. UNDER REVIEW.\n7\na) State-based Ant (C4 symmetry).: For state-based skill\ndiscovery, we use the MuJoCo Ant environment [46] with a\nplanar (90\u25e6) rotational symmetry, modeled as the cyclic group\nC4. To faithfully preserve this symmetry, we do not apply\nobservation normalization (e.g., running mean/variance) to the\nstate input. Such preprocessing typically warps coordinates\nand breaks equivariance. In this setting, we restrict the Fourier\nrepresentation to the frequency-1 C4 irrep and discover 2D\nlatent skills in this subspace, matching the skill dimensionality\nused in METRA (as introduced in Section V-D). Intuitively,\nthis emphasizes directional, rotation equivariant behaviors\n(e.g., \u201dmove in a given heading\u201d), which are most relevant in\nAnt locomotion. We empirically observed that incorporating\ninvariant components had little benefit and could introduce\nnoise in discovery, so we focus on the equivariant features in\nthis work.\nb) Pixel-based Quadruped (D1 \u223c= C2 / flip symmetry:\n). For pixel-based skill discovery, we adopt the vision-based\nquadruped environment from the DeepMind Control Suite\n(DMC) [47]), following the METRA [5] setup. METRA\nencodes position information by making the floor color vary\nsmoothly as a function of the agent\u2019s (x, y) coordinates. To\nfully expose the horizontal flip symmetry to the agent, we\ninstead redesign the floor so that its color varies radially from\nthe origin. This maintains a consistent appearance under left-\nright flips while still revealing global position, and matches\nthe C2 (horizontal flip) symmetry.\nThe group C2 has a 2D real representation consisting of\nan invariant channel and a sign-flip channel. To keep the\ncomparison with METRA fair, we use a 4D latent skill space\nby concatenating two such Fourier-feature vectors. This can\nbe viewed as learning two independent Fourier components.\nIn practice, this provided a good balance between expressivity\nand parity with the baseline.\nc) Symmetry-aware Policy.: On the control side, we use\nPartially Equivariant SAC [37] for the state-based Ant, which\nis robust to local symmetry-breaking effects (e.g., contact\nasymmetries). For the pixel-based quadruped, we use Equiv-\nariant SAC [29]. In both environments, a high-level policy\n\u03c0h(z | s, g) selects a skill vector z every K steps to pursue\na goal g, while a low-level policy \u03c0l(a | s, z) executes the\nlearned skill. The skill policy is pretrained and then forzen for\ndownstream training, as in METRA.\nB. Evalutation Metric\nWe evaluate both skill discovery and downstream task\nperformance.\na) Staet-space coverage.: Following METRA [5], we\nmeasure how well skills explore the environment by comput-\ning the coverage of the agent\u2019s (x, y) positions. At evalua-\ntion, we sample 48 latent skill vectors uniformly from the\nskill prior, roll each for a fixed horizon, and aggregate the\nresulting trajectories. We then quantify coverage using the\nfraction of grid cells visited in the task-relevant region of the\nplane. For the state-based Ant, we restrict the coverage region\nto [\u221230, 30]2, which corresponds to the maximum distance\nthe agent can reach under the goal-sampling scheme in the\ndownstream tasks. This avoids counting irrelevant areas.\nFig. 2.\nBenchmark Environments. We evaluate GISD in state-based Ant\nand pixel-based Quadruped locomotion. In the pixel-based setting, we redesign\nthe floor so that color varies radially from the origin, making the observations\nconsistent with the horizontal flip symmetry."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThe paper introduces Group-Invariant Skill Discovery (GISD), a novel framework for unsupervised skill discovery in reinforcement learning. GISD addresses the fundamental issue of geometric symmetries in robotic environments, which are often overlooked by existing methods.\n\n1. **Theoretical Guarantee**: The authors prove that in group-symmetric MDPs, the Wasserstein dependency measure admits a globally optimal solution comprised of an equivariant policy and a group-invariant scoring function \\( f \\). This theoretical guarantee justifies GISD, which restricts \\( f \\) to symmetry-respecting functions without sacrificing optimality.\n\n2. **Group-Invariant Wasserstein Dependency Measure (GIWDM)**: The authors formulate the GIWDM, which directly optimizes within this reduced subspace and eliminates redundancy by exploring only symmetric solutions. This measure guarantees invariance under group actions, a property not generally enjoyed by standard WDMs.\n\n3. **Symmetry-Aware Skill Representation**: GISD parameterizes the scoring function using a group Fourier representation and defines the intrinsic reward via the alignment of equivariant latent features. This ensures that discovered skills generalize systematically under group transformations.\n\n4. **Practical Implementation**: The authors provide practical implementations for the GIWDM, including how to construct symmetry-aware scoring functions from arbitrary 1-Lipschitz functions using a group averaging operation and parameterizing them in Fourier space. They also show how to exploit these symmetries in downstream tasks by deriving an intrinsic reward that is itself group-invariant.\n\n#### Methodology\nThe authors' approach leverages the theoretical guarantee of symmetry-respecting solutions for unsupervised skill discovery, which they prove exists in group-symmetric environments. The methodology includes:\n\n1. **Group Symmetry Analysis**: They analyze optimal solutions to the standard WDM-based problem and show that restricting the search space to symmetry-aware policies does not compromise optimality.\n\n2. **Group-Invariant Wasserstein Dependency Measure (GIWDM)**: By defining a constrained objective that restricts scoring functions to those invariant under group actions, they ensure invariance without losing optimality.\n\n3. **Skill Representation**: They parameterize the scoring function using a group Fourier representation and define the intrinsic reward via the alignment of equivariant latent features. This ensures symmetry-aware skills generalize systematically under group transformations.\n\n4. **Downstream Task Implementation**: For downstream tasks, they derive an intrinsic reward that is itself group-invariant by exploiting the structure of symmetric environments. They use partially equivariant SAC for state-based and pixel-based settings to ensure robustness against local symmetries.\n\n#### Key Findings\nThe experiments demonstrate significant improvements over a strong baseline in both sample efficiency and state-space coverage:\n\n1. **Sample Efficiency**: GISD achieves broader state-space coverage, leading to improved exploration and faster downstream task learning compared to the baseline method.\n\n2. **State-Space Coverage**: The authors evaluate skill discovery performance using metrics such as the fraction of grid cells visited by the agent in relevant regions of the environment. They find that GISD discovers a more diverse set of skills covering larger portions of the state space.\n\n#### Conclusions\nThe paper concludes with several key points:\n\n1. **Generalizability**: The authors show that discovered skills are systematically generalizable under group transformations, enabling efficient downstream task training without requiring additional pretraining or fine-tuning.\n\n2. **Practical Feasibility**: They provide practical implementations of the proposed methods, including parameterization in Fourier space and derivation of an intrinsic reward for symmetric environments.\n\n3. **Theoretical Justification**: The theoretical guarantee justifies GISD as a viable approach to unsupervised skill discovery by ensuring that discovered skills are invariant under group transformations without sacrificing optimality.\n\n4. **Future Work**: They suggest exploring more complex symmetries, incorporating prior knowledge into the framework, and extending the method to other domains beyond robotics.\n\nIn summary, GISD offers a robust and systematic way to discover reusable behavioral primitives in robotic environments by leveraging their inherent geometric symmetries, leading to improved exploration and faster downstream task learning."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Penalizing Localized Dirichlet Energies in Low Rank Tensor Products\nParis A. Karakasis 1 Nicholas D. Sidiropoulos 1\nAbstract\nWe study low-rank tensor-product B-spline\n(TPBS) models for regression tasks and investi-\ngate Dirichlet energy as a measure of smoothness.\nWe show that TPBS models admit a closed-form\nexpression for the Dirichlet energy, and reveal\nscenarios where perfect interpolation is possible\nwith exponentially small Dirichlet energy. This\nrenders global Dirichlet energy-based regulariza-\ntion ineffective. To address this limitation, we\npropose a novel regularization strategy based on\nlocal Dirichlet energies defined on small hyper-\ncubes centered at the training points. Leveraging\npretrained TPBS models, we also introduce two\nestimators for inference from incomplete samples.\nComparative experiments with neural networks\ndemonstrate that TPBS models outperform neu-\nral networks in the overfitting regime for most\ndatasets, and maintain competitive performance\notherwise. Overall, TPBS models exhibit greater\nrobustness to overfitting and consistently benefit\nfrom regularization, while neural networks are\nmore sensitive to overfitting and less effective in\nleveraging regularization.\n1. Introduction\nClassification and regression are classical yet fundamen-\ntally ill-posed problems in machine learning. In regres-\nsion, for example, given a collection of samples C =\n{(xm, ym)}M\nm=1, drawn from an unknown distribution and\nevaluated through an unknown function, there exist infinitely\nmany functions that perfectly interpolate C. A similar phe-\nnomenon arises in classification: for virtually any finite\ntraining set, one can construct infinitely many classifiers\nthat achieve zero training error. In both cases, this raises\na central question: among all hypotheses consistent with\nthe observed data, which ones generalize best to unseen\nsamples drawn from the same distribution?\n1Department of Electrical and Computer Engineering, Univer-\nsity of Virginia, Charlottesville, USA. Correspondence to: Paris\nKarakasis <karakasis@virginia.edu>.\nPreprint. January 21, 2026.\nWithout additional assumptions, this question is essentially\nvacuous. In many practical settings, only scarce samples\nare available, while more expressive models typically re-\nquire larger datasets to yield reliable estimates that gener-\nalize effectively. To address this challenge, a wide range\nof function-complexity measures have been developed to\ncombat overfitting and encourage the selection of simpler\nhypotheses that are more likely to generalize. The need\nfor additional constraints to render learning problems well-\nposed can be traced back to Hadamard\u2019s foundational work\non ill-posedness (Hadamard, 1902). Building on this princi-\nple, Tikhonov formalized explicit regularization methods for\ninverse problems, introducing penalty terms that promote\nstability and simplicity (Tikhonov, 1977). These ideas were\nfurther developed within statistical learning theory, where\nVapnik and others established regularization as a central\nmechanism for controlling model complexity and ensuring\nstrong generalization (Vapnik, 2013). Designing effective\nregularization principles that bias learning toward hypothe-\nses with good generalization remains an active research\ndirection.\nThe function approximation model that has attracted the\nmost attention in recent years is deep neural networks\n(DNNs). However, designing effective regularization meth-\nods for DNNs remains challenging (Gouk et al., 2021). The\nmost widely used techniques, such as dropout (Srivastava\net al., 2014) and batch normalization (Ioffe & Szegedy,\n2015), are largely heuristically motivated (Gouk et al.,\n2021), which complicates the design and improvement of\nnew methods. In contrast, classical approaches such as\nTikhonov regularization are generally less effective for deep\narchitectures (Srivastava et al., 2014), underscoring the need\nfor principled and scalable regularization strategies.\nYet another conundrum emerges from the empirical obser-\nvation that large over-parameterized DNNs often generalize\nbetter than their smaller counterparts, which conflicts with\nclassical notions of model parsimony which favor smaller\nmodels (Novak et al., 2018). This paradoxical phenomenon\nis known as the double-descent (Belkin, 2021; Nakkiran\net al., 2021; Dherin et al., 2021) \u2013 see also the appendix of\nthis manuscript where we reproduce this phenomenon using\nthe well-known MNIST dataset and neural networks with\nonly one hidden layer.\n1\narXiv:2601.14173v1  [cs.LG]  20 Jan 2026\nPenalizing Localized Dirichlet Energies in Low Rank Tensor Products\nDespite extensive research, there is still no strong consensus\non what controls generalization in deep neural networks.\nCurrent efforts largely focus on classification problems,\nwhere the targets {ym} are drawn from a finite set. In\nthis setting, several factors have been investigated to un-\nderstand generalization, including Rademacher complexity\n(Shalev-Shwartz & Ben-David, 2014), VC dimension (Vap-\nnik, 2013), early stopping (Hardt et al., 2016), batch size\neffects (Keskar et al., 2016), the magnitudes and norms of\nthe network weights (Bartlett, 1996; Bartlett et al., 2017;\nNeyshabur et al., 2017b; Gouk et al., 2021), sharpness\n(Keskar et al., 2016), and the choice of optimization algo-\nrithm (Neyshabur et al., 2017a). Researchers have explored\nthese factors both empirically, by observing correlations\nwith generalization, and theoretically, by attempting to ex-\nplain these correlations.\nFollowing the intuition that slowly varying functions are\nsimpler and generalize better, the Lipschitz constant has\nemerged as a crucial factor in assessing and enhancing the\nperformance of DNNs (Fazlyab et al., 2019; Gouk et al.,\n2021; Bubeck & Sellke, 2021). Xu & Mannor (2012) es-\ntablish a formal link between a model\u2019s Lipschitz constant\nand its generalization performance via robustness theory,\nand (Gouk et al., 2021) provide empirical evidence that\nenforcing Lipschitz control, e.g., through layer-wise projec-\ntions, improves generalization. Khromov & Singh (2023)\nobserve a striking double-descent trend in both upper and\nlower bounds of the Lipschitz constant during training. The-\noretical work also highlights the interplay between model\nsize, smoothness, and generalization. Bubeck & Sellke\n(2021) show that interpolating n points in RD with a smooth\nfunction requires at least nD parameters, yielding a size\u2013\nrobustness tradeoff: models with only n parameters are\nnecessarily non-robust (large Lipschitz constant), whereas\nachieving robustness (small Lipschitz constant) requires\nlarger models.\nDherin et al. (2021) argue that overparameterized networks\ntrained with stochastic gradient descent are implicitly regu-\nlarized via geometric complexity, which depends on the\nDirichlet energy of the function.\nGradient-based train-\ning induces an implicit preference for functions with low\nDirichlet energy, so the learned DNNs often behave like\nharmonic interpolants of the data rather than arbitrary fits\n(Dherin et al., 2021; 2022). Moreover, common training\nheuristics\u2014including parameter norm regularization, spec-\ntral norm regularization, flatness regularization, implicit\ngradient regularization, noise regularization, and parameter\ninitialization\u2014act to control geometric complexity, provid-\ning a unifying framework for understanding deep learning\n(Dherin et al., 2022). Empirical studies support this view:\na complexity measure similar to geometric complexity cor-\nrelates strongly with generalization in DNNs (Novak et al.,\n2018), and recent work derives generalization bounds that\nscale with the margin-normalized geometric complexity of\nDNNs (Munn et al., 2023). However, this behavior is not\nuniversal across all learning models, as interpolation with\nfunctions of exponentially small Dirichlet energy is possi-\nble, casting doubt on the reliability of Dirichlet energy as a\nuniversal proxy for generalization (Bubeck & Sellke, 2021).\nIn this paper, we study low-rank tensor products of uni-\nvariate functions as an alternative function approximation\nmodel. Such models are compatible with calculus opera-\ntions. This enables the derivation of analytical insights on\nmany interesting questions \u2013 even closed-form solutions in\ncertain cases. We show that the Dirichlet energy of these\nlow-rank tensor product models can be computed exactly in\nclosed form, and that this computation can be carried out ef-\nficiently with a cost that scales linearly in the dimensionality\nof the problem. This provides a clearer understanding of the\nmodel\u2019s behavior and explains why these representations\ncan perfectly interpolate data while maintaining exponen-\ntially small Dirichlet energy. Motivated by this observation,\nwe propose using localized Dirichlet energies, centered at\nthe training points, as a regularization strategy for both re-\ngression and classification tasks.\n2. Background & Related Work\nIn this section, we review the main concepts and tools un-\nderlying our proposed framework. We focus on low-rank\ntensor models for multivariate function approximation with\nB-spline representations for the univariate components. Ad-\nditionally, we introduce the Dirichlet energy as a indirect\nmeasure of the geometric complexity of functions, following\n(Dherin et al., 2021; 2022), which we later use to promote\nsmoothness and regularity in our models.\n2.1. Low Rank Tensor Product models\nLow-rank tensor products of univariate functions are com-\nmonly used to approximate multivariate functions g(x) :\n[0, 1]N \u2192RM and can be represented as a sum of R sepa-\nrable components, i.e.,\ng(x) =\nR\nX\nr=1\nvr\nN\nY\nn=1\ngn,r(xn),\n(1)\nwhere gn,r : [0, 1] \u2192R are univariate functions along the\nn-th variable, vr \u2208RM are vectors associated with each\nseparable component, and R is typically chosen to be small.\nKargas & Sidiropoulos (2021) recently proposed the model\nin (1) for supervised learning of smooth multivariate func-\ntions. More specifically, they considered compactly sup-\nported1 functions f : [0, 1]N \u2192R. Their approach lever-\n1Here we consider functions that are compactly supported on\nthe [0, 1]N hypercube; without loss of generality, any compactly\n2\nPenalizing Localized Dirichlet Energies in Low Rank Tensor Products\nages a periodic extension of these functions, which allows\nthem to be efficiently approximated using truncated multi-\nvariate Fourier series, yielding a finite parametrization of\nthe model. This parametrization naturally appears in the\nform of an N-th order tensor of coefficients. After consid-\nering a constrained decomposition of that tensor, Kargas &\nSidiropoulos (2021) arrive at the model in (1) for M = 1,\nwhere the univariate functions gn,r : [0, 1] \u2192R are mod-\neled using truncated Fourier series.\nGovindarajan et al. (2022) proposed using B-splines, or\nequivalently piecewise polynomials, to discretize the uni-\nvariate component functions gn,r : [0, 1] \u2192R in the low-\nrank tensor model of (1). A spline is a piecewise polynomial\nfunction stitched together to maintain a specified degree of\nsmoothness across its domain. B-splines have long been\nfundamental tools in computational geometry and numeri-\ncal analysis (De Boor, 1978). However, their explicit use\nin machine learning has been comparatively limited, de-\nspite known connections between deep ReLU networks and\nnonuniform linear splines with adaptive knots (Balestriero &\nBaraniuk, 2018; Unser, 2019). Compared to global polyno-\nmials, B-splines offer compact support, allow adaptive local\napproximation of nonlinearities, and avoid the oscillatory\nbehavior often seen in high-degree polynomial interpola-\ntion. Furthermore, B-splines are universal approximators\nfor continuous functions on compact intervals (De Boor,\n1978).\nUnlike global polynomials, whose approximation power\ngrows only by increasing the degree, splines can also be re-\nfined by inserting knots. This improves local resolution and\nenables precise local adaptation. Changes to coefficients\naffect only a small neighborhood, allowing accurate model-\ning of regions with rapidly varying behavior. Low-degree\nsplines avoid numerical instabilities such as the large oscilla-\ntions of Runge\u2019s phenomenon (Trefethen, 2019; Schumaker,\n2007). Such phenomena are known to adversely affect gen-\neralization in machine learning. Their compact support also\nmakes splines particularly effective for capturing localized\nnon-analytic features, such as kinks and sharp transitions.\nGlobal polynomial and Fourier bases, by contrast, can per-\nform poorly in these situations (Powell, 1981; Rivlin, 1981;\nDaubechies et al., 2004).\nGlobal polynomial and Fourier bases have complementary\nstrengths. Their global support allows information to prop-\nagate across the entire domain, enabling interpolation or\nreasonable approximation in regions with few or no training\nsamples, provided the underlying function is sufficiently\nsmooth. However, they lack local adaptability and can ex-\nhibit oscillatory artifacts near boundaries or discontinuities\n(Runge and Gibbs phenomena). By contrast, B-splines pro-\nsupported function can be mapped to this domain via appropriate\ntranslation and rescaling of its arguments.\nvide a controlled, locally supported model, which, when\ncombined with explicit regularization, allows information\nto propagate across the domain in a controllable manner\nwhile maintaining stability and precise local control. These\nproperties make low-rank tensor products of univariate B-\nsplines a practical and theoretically sound choice for high-\ndimensional regression and classification. In this work, we\nuse low rank tensor products of univariate B-splines as our\nfunction approximation model.\n2.2. Dirichlet energy as a measure of complexity\nThe geometric complexity of a function g : [0, 1]N \u2192RM\ncan be intuitively defined as the volume of its graph\ngr(g) := {(x, g(x)) : x \u2208[0, 1]N} \u2282RN+M.\n(2)\nFor smooth functions, this graph is an N-dimensional sub-\nmanifold, and its Riemannian volume, induced by the Eu-\nclidean metric, provides a measure of the functions\u2019s geo-\nmetric complexity (Do Carmo, 2016; Dherin et al., 2021)\n\u2126(g) =\nZ\n[0,1]N\nq\n1 + \u2225\u2207g(x)\u22252\nF dx.\n(3)\nUsing the first-order Taylor approximation \u221a1 + z \u22481 +\n1\n2z, this reduces to\n\u2126(g) \u22481 + 1\n2DE(g),\n(4)\nwhere\nDE(g) :=\nZ\n[0,1]N \u2225\u2207g(x)\u22252\nF dx\n(5)\nis the Dirichlet energy of g. Thus, the Dirichlet energy\nprovides a simple and effective proxy for the geometric\ncomplexity of a function.\nThis notion of geometric complexity has a direct connection\nto deep learning: the Jacobian of a neural network captures\nhow sensitive the network output is to input perturbations,\nwhich is directly related to the Dirichlet energy. Computing\nthe exact Dirichlet energy of a high-dimensional DNN is\nintractable, as it requires integrating over the entire input\nspace. However, sample-based approximations are practical.\nEvaluating \u2225\u2207g(x)\u22252\nF at the training points and averaging\nprovides an empirical estimate of the Dirichlet energy. In\nparticular, averaging the Frobenius norm of the Jacobian\nover the training set has been proposed as an effective reg-\nularizer to improve robustness and generalization (Sokoli\u00b4c\net al., 2017; Hoffman et al., 2019). Furthermore, empiri-\ncal and theoretical studies further support that geometric\ncomplexity correlates strongly with generalization in DNNs\n(Novak et al., 2018; Munn et al., 2023).\n3\nPenalizing Localized Dirichlet Energies in Low Rank Tensor Products\n3. Main Contributions\nIn this section, we present three main contributions. First,\nwe show that the Dirichlet energy of low-rank tensor prod-\nuct models can be computed analytically in closed form.\nThis result allows us to gain insights into how near-perfect\ninterpolations of datasets with exponentially small Dirichlet\nenergy are possible. Second, we propose a regularization\nmethod based on localized Dirichlet energies for supervised\nlearning tasks, including classification and regression. Third,\nwe introduce a framework for inference with incomplete\ndata, i.e., sample vectors x with incomplete elements. These\ncontributions are agnostic to the choice of univariate func-\ntion representations in low-rank tensor-product models, such\nas Fourier series or B-splines. Our framework distinguishes\nitself from prior approaches, including (Kargas & Sidiropou-\nlos, 2021) and (Govindarajan et al., 2022), in three key as-\npects: (i) we introduce averaged localized Dirichlet energy\nas a regularization term for learning, (ii) we propose two\nmarginalization strategies for run-time inference in the pres-\nence of missing data, and (iii) we place particular emphasis\non the overfitting regime, providing empirical evidence for\nthe behavior of the proposed model and its connection to\nthe double-descent phenomenon.\n3.1. The Dirichlet energy of a low rank tensor model\nOne advantage of the low-rank tensor product models in\n(1) is their compatibility with basic calculus operations. In\nparticular, this structure allows us to analytically compute\nthe Dirichlet energy in closed form, providing insight into\nthe model\u2019s behavior and inner mechanisms.\nTheorem 3.1. For a function g represented by the low-rank\ntensor product model in (1), its Dirichlet energy can be\nexpressed in closed form as\nDE(g) =\nZ\n[0,1]N \u2225\u2207g(x)\u22252\nF dx = s(g)\u22a4Z(g)s(g), (6)\nwhere s(g) \u2208RR\n+, Zr,s(g) \u2208RR\u00d7R, A(0)(g) \u2208RR\u00d7R,\nand A(1)(g) \u2208RR\u00d7R are defined element-wise as\nsr(g) = \u2225vr\u2225\nN\nY\nn=1\n\u2225gn,r\u2225,\nZr,s(g) = A(0)\nr,s(g) A(1)\nr,s(g),\nA(0)\nr,s(g) = cos(\u03b8vr,vs)\nN\nY\nn=1\ncos(\u03b8gn,r,gn,s),\nA(1)\nr,s(g) =\nN\nX\nn=1\n\u27e8\u2207gn,r, \u2207gn,s\u27e9\n\u27e8gn,r, gn,s\u27e9\n,\n(7)\nwhere \u27e8f, z\u27e9:=\nR 1\n0 f(x)z(x) dx denotes the standard L2\ninner product, and \u03b8\u00b7,\u00b7 denotes the angle between two func-\ntions in the corresponding inner-product space.\nThe proof of the theorem above can be found in the Ap-\npendix. Notice that vector s (g) \u2208RR\n+ holds the L2 norms\nof each one of the R tensor products/components of model\n(1). According to (14), each element of s (g) is defined as\na product of the norms of the univariate functions gn,r and\nvector vr. Consequently, when most of the univariate func-\ntions have norms smaller than one, the resulting elements of\nv (g) will be exponentially small. As a result, the Dirichlet\nenergy can be exponentially small, unless the eigenvalues of\nthe matrix Z (g) are exponentially large. Among the com-\nponents that constitute DE(g), only the entries of A(0)(g)\ninvolve products across all dimensions, yet these entries are\nbounded in absolute value by one.\nTherefore, we argue that using the model in (1) for learn-\ning tasks with Dirichlet energy-based regularization tends\nto promote functions with exponentially small values in\ns (g). This is a pattern we have consistently observed in our\nexperiments. In turn, this behavior induces a type of concen-\ntration phenomenon, where the energy becomes smoothly\nconcentrated around the training points. Trying to eliminate\nthe dependence on the vector s(g) shifts attention to the\nspectral properties of matrix Z (g). In practice, we have\nobserved that over many datasets and models such matrices\nare consistently positive semidefinite, i.e., all their eigenval-\nues are nonnegative. This implies that penalizing the sum\nof all positive eigenvalues as regularization corresponds to\npenalizing its trace as regularization. The downside of this\napproach is that each separable component of the model is\npenalized in isolation from all the rest, without regard to\nhow the different components interact with each other. Next,\nwe provide a more effective regularization approach that\nrelies on local Dirichlet energies around the training points.\n3.2. Proposed Regularization\nAnother benefit of working with the model in (1) is that it\nallow us to interpolate between sample-based approxima-\ntions of Dirichlet Energy and its computation over the whole\nhypercube. This is accomplished by introducing a collection\nof small hypercubes centered at the training points, whose\n\u201cradius\u201d (half-side length) is a tunable parameter. By ad-\njusting this radius and adding the resulting local integrals,\none can transition from purely pointwise (sample-based)\napproximations to increasingly global estimates that capture\nthe geometry of the full domain.\nThe resulting proposed regularization has the form\nLDE\u03c1 (g) :=\nM\nX\nm=1\nZ\nB\u03c1(xm)\n\u2225\u2207g (x)\u22252\nF dx,\n(8)\nwhere B\u03c1 (xm) =\nn\nx \u2208[0, 1]N : \u2225xm \u2212x\u2225\u221e\u2264\u03c1\no\n. The\nadvantage of this regularization is that it acts on the whole\nmodel and penalizes the localized Dirichlet energy at a\n4\nPenalizing Localized Dirichlet Energies in Low Rank Tensor Products\nunion of balls centered at the training samples. This union\ncan be considered as a rough approximation of the support\nof the density function from which the points are drawn\nfrom. This approximation has exponentially smaller volume\nthan [0, 1]N. Hence, the promise of this approach stems\nfrom the fact that the Dirichlet energy is minimized over an\nexponentially smaller volume for all \u03c1 < 0.5, strategically\nfocusing on the support of the data-generating distribution.\nIn the sequel, we consider LDE\u03c1 (g) as regularization for\nthe tensor product of B-splines model.\nAssuming a collection of data points C := {(xm, ym)}M\nm=1\nand a loss function \u2113(ym, g(xm)), we consider the regular-\nized Empirical Risk Minimization problem\nmin\ng\n1\nM\nM\nX\nm=1\n\u2113(ym, g(xm)) + \u03bb LDE\u03c1(g)\ns.t.\ng admits (1),\n(9)\nwhere \u03bb > 0 is a hyperparameter controlling the strength of\nthe regularization.\nIn this work, we fit low-rank tensor product models within\nthis framework and tune \u03bb using a penalty-method schedule.\nSpecifically, we initialize \u03bb0 to a small value and update\nit according to \u03bbt+1 = h \u03bbt with h > 1 each time the\ntraining algorithm converges. At the end of the procedure,\nwe retain two candidate models: (i) the one achieving the\nbest validation error overall, and (ii) the one achieving the\nbest validation error after overfitting.\n3.3. Handling Incomplete Observations\nMissing entries in x pose significant challenges for most\nlearning algorithms, particularly when the missingness is\nnon-systematic or irregular. Simple mitigation strategies\ntypically rely on imputation methods to fill in the missing\nvalues before applying standard learning algorithms. An\neasy way to do this is by using the averages of the missing\nfeatures. However, the model proposed in (1) can naturally\naccommodate missing data through its structural proper-\nties. Specifically, it allows for seamless integration over any\nsubset of variables, enabling principled marginalization of\nunobserved dimensions without requiring explicit imputa-\ntion. Without loss of generality, suppose that the first D\nelements of a sample x \u2208[0, 1]N have not been observed\nand only the remaining entries xobs have been observed.\nThen, the model in (1) supports the following estimate\nyuni =\nZ\n[0,1]D g (x1, . . . , xD, xobs) dx\n=\nR\nX\nr=1\nvr\nN\nY\nn=D+1\ngn,r (xn)\nD\nY\nd=1\nZ\n[0,1]\ngd,r (xd) dxd.\n(10)\nA more sophisticated approach is to consider a simple model\nof a multivariate density function in the form of low rank\ntensor products (Karakasis & Sidiropoulos, 2024),\np (x) =\nS\nX\ns=1\nws\nN\nY\nn=1\npn,s (xn) ,\n(11)\nwhere pn,s (xn) : [0, 1] \u2192R+ denote bona fide univariate\ndensity functions and w \u2208RS\n+ denotes a probability mass\nfunction (PMF) of S possible outcomes of a latent categor-\nical variable. When S is set to one, it is assumed that the\nobserved features are independent, whereas S > 1 means\nthat those features are conditionally independent given the\nlatent categorical variable.\nIn this case, the marginalization can be performed using a\ndensity function learned from the dataset. Specifically, the\nresulting estimator is given by\nypdf = Ep\n\u0002\ng\nobserved = xobs\n\u0003\n=\nZ\n[0,1]D\ng (x1, . . . , xD, xobs) p\n\u0000x1, . . . , xD\nxobs\n\u0001\ndx1 . . . dxD\n=\nZ\n[0,1]D\ng (x1, . . . , xD, xobs) p (x1, . . . , xD, xobs)\np (xobs)\ndx1 . . . dxD\n=\nPR\nr=1\nPS\ns=1 vr ws\nQN\nn=D+1 gn,r (xn) QD\nd=1 \u27e8gd,rpd,s\u27e9[0,1]\nPS\nt=1 wt\nQN\nv=D+1 pv,t (xv)\n.\n(12)\nNote that in the ideal case where the model for p matches\nthe true data-generating distribution, the estimator in (12)\ncoincides with the optimal mean squared error (MMSE)\nestimator.\n4. Experimental Results\nIn this section, we evaluate the performance of the proposed\nframework using B-splines on supervised learning tasks,\nconsidering both fully observed datasets and scenarios with\nincomplete observations at inference time.\nWe use six standard datasets spanning classification and\nregression tasks.\nFor classification, we consider the\nIonosphere (Ion)2 dataset, containing 34 features describ-\ning radar returns for detecting structures in the iono-\nsphere, and the Breast Cancer Wisconsin (BCW)3 dataset\nwith 30 features for classifying malignant and benign tu-\nmors. For regression, we use the Diabetes4 dataset from\nscikit-learn, which has 10 features predicting disease\n2https://archive.ics.uci.edu/ml/datasets/\nionosphere\n3https://archive.ics.uci.edu/ml/datasets/\nBreast+Cancer+Wisconsin+(Original)\n4https://scikit-learn.org/stable/modules/\ngenerated/sklearn.datasets.load_diabetes.\nhtml\n5\nPenalizing Localized Dirichlet Energies in Low Rank Tensor Products\nprogression, the Yacht Hydrodynamics5 dataset with 6 fea-\ntures modeling yacht drag coefficients, the Physico6 dataset\nwith 9 features related to protein structure, and the Sarcos7\nrobot arm dataset with 21 features for inverse dynamics pre-\ndiction. For each dataset, we generate three random splits\ninto training, validation, and test sets. Table 1 summarizes\ntheir key characteristics.\nAs baselines, we consider neural networks with one or\ntwo hidden layers, regularized using the average Frobenius\nnorm of the Jacobian computed over the training samples.\nFor single-hidden-layer networks, we use widths of 1000,\n5000, and 20000 neurons, while for two-layer networks,\nwe choose equal widths to match the total number of pa-\nrameters of the corresponding single-layer models. For\nthe tensor\u2013product of B-splines (TPBS) models, we fix the\nnumber of knots per dimension to 100 and vary the rank\nso that the total number of parameters is comparable to the\nneural network counterparts. Additionally, to assess the\neffectiveness of these regularization approaches, we present\nresults for both models with and without their use. It should\nbe noted that the case of TPBS models without regulariza-\ntion reduces to the approach proposed by (Govindarajan\net al., 2022). Finally, we use AdamW (Loshchilov & Hutter,\n2017) with weight decay equal to 0, eps equal to 1e-4, and\nmanually tuned learning rates for training all models.\nFor comparison, we retain the models achieving the best val-\nidation performance (i) in general, and (ii) after controlled\noverfitting while gradually increasing the regularization hy-\nperparameters. Here, overfitting refers to the case when\nthe training error falls below a prescribed threshold. This\nanalysis allows us to explore whether controlled overfitting\ncan be beneficial and how different model classes respond\nto overfitting and the use of regularization.\nDataset\nDim\nTask\nTrain\nVal\nTest\nIon\n34\nClassification\n160\n100\n91\nBCW\n30\nClassification\n260\n100\n200\nDiabetes\n10\nRegression\n200\n100\n142\nYacht\n6\nRegression\n150\n58\n100\nPhysico\n9\nRegression\n250\n500\n1000\nSarcos\n21\nRegression\n250\n500\n4449\nTable 1. Dataset statistics: dimensionality, task type, and number\nof samples in training/validation/testing splits.\n5https://archive.ics.uci.edu/ml/datasets/\nyacht+hydrodynamics\n6https://archive.ics.uci.edu/ml/datasets/\nPhysicochemical+Properties+of+Protein+\nTertiary+Structure\n7https://www.gaussianprocess.org/gpml/\ndata/\n4.1. Inference with full datasets\nIn this subsection, we present the results that emerged af-\nter training TPBS and neural network models and using\nfully observed samples for inference. In the appendix, Ta-\nbles (9)-(14) present detailed results for all datasets and\nmodel configurations, and in Table (2), we summarize the\nbest-performing models across all experiments for a concise\ncomparison. In Table (2), we observe that neural networks\nand TPBS models achieve comparable performance on the\nfirst four datasets, with neural networks leading in three of\nthem by a small margin. Overfitting consistently harms neu-\nral network performance in all but two datasets. In contrast,\nTPBS models exhibit notably more robust behavior: in half\nof the datasets, overfitting does not degrade performance\nat all\u2014indeed, in three cases it even improves it. In the\nremaining datasets, the negative impact of overfitting is far\nmilder compared to neural networks. For example, in the\nDiabetes and Physico datasets, overfitting nearly doubles the\nrelative mean squared error for neural networks, whereas\nTPBS performance remains largely unchanged. We also\nobserve that the use of regularization consistently improves\nTPBS performance across all datasets.\nWhen comparing the two methods specifically in the over-\nfitting regime, TPBS models outperform neural networks\nin four out of six datasets, with only one of the remaining\ndatasets showing a substantial advantage for neural net-\nworks. Therefore, in applications where perfect memoriza-\ntion of the training data is desired\u2014such as associative\nmemory models, error-correcting code reconstruction, or\ndictionary-based compression\u2014TPBS models offer a com-\npelling alternative.\n4.2. Inference with incomplete datasets\nTables (3)\u2013(8) present the results obtained in the inference\nfrom incomplete observations setting, evaluated under both\noverfitted and non-overfitted models. For each dataset, we\nconsider scenarios in which 2, 3, or 4 entries of every ob-\nservation are randomly hidden. In all cases, we reuse the\nmodels selected in the previous subsection\u2014those trained\non fully observed samples together with their associated reg-\nularization schemes; the proposed LDE \u03c1 regularization for\nTPBS models, and the averaged Jacobian norm for neural\nnetworks.\nFor the neural network models, we impute the missing en-\ntries using the empirical mean of each feature and then apply\nthe pretrained networks directly. For the tensor product of\nB-splines (TPBS) models, we evaluate four different ap-\nproaches. The first approach mirrors the neural network\ncase: we impute missing entries with the corresponding\nfeature means and use the TPBS model trained on fully\nobserved data. The second approach replaces each miss-\ning entry with the integral of the corresponding learned\n6\nPenalizing Localized Dirichlet Energies in Low Rank Tensor Products\nDataset\nNN (best val)\nNN (overfit)\nTPBS (best val)\nTPBS (overfit)\nIon (34D - \u2191)\n0.949 \u00b1 0.026 (NR)\n0.952 \u00b1 0.014 (R)\n0.930 \u00b1 0.029 (R)\n0.938 \u00b1 0.010 (R)\nBCW (30D - \u2191)\n0.975 \u00b1 0.007 (NR)\n0.958 \u00b1 0.012 (NR)\n0.965 \u00b1 0.014 (R)\n0.965 \u00b1 0.014 (R)\nDiabetes (10D - \u2193)\n0.101 \u00b1 0.010 (R)\n0.223 \u00b1 0.037 (R)\n0.134 \u00b1 0.005 (R)\n0.173 \u00b1 0.019 (R)\nYacht (6D - \u2193)\n0.001 \u00b1 0.000 (R)\n0.001 \u00b1 0.000 (R)\n0.001 \u00b1 0.000 (R)\n0.001 \u00b1 0.000 (R)\nPhysico (9D - \u2193)\n0.263 \u00b1 0.010 (NR)\n0.533 \u00b1 0.075 (NR)\n0.340 \u00b1 0.012 (R)\n0.336 \u00b1 0.013 (R)\nSarcos (21D - \u2193)\n0.049 \u00b1 0.001 (NR)\n0.060 \u00b1 0.005 (NR)\n0.240 \u00b1 0.042 (R)\n0.243 \u00b1 0.046 (R)\nTable 2. Summary of best model test performance across all datasets. NN values refer to the best-performing neural network (across all\ncombinations of hidden-layer counts and whether regularization was applied) before and after overfitting. TPBS values refer to the best\nlow-rank tensor product B-spline model across ranks and regularization options. (R) and (NR) denote whether the best performance was\nachieved by a model with or without regularization, respectively. For datasets with an arrow pointing upwards, the task under consideration\nis classification, and the reported metric is classification accuracy. Hence, the higher, the better. For datasets with an arrow pointing\ndownwards, the task under consideration is regression, and the reported metric is relative mean square error. Hence, the lower, the better.\nunivariate function, following the estimator described in re-\nlation (10). The third and fourth approaches substitute each\nmissing entry with the inner product between the learned\nlow-rank density model and the corresponding univariate\nfunction, using ranks S = 1 and S = 5, respectively. These\ntwo methods correspond to estimators of the form given in\nrelation (12). In all cases, the resulting weighted integrals\nor inner products serve as surrogates for the missing values.\nFor the Ion dataset (Table 3), we observe that neural net-\nworks and TPBS models perform comparably in both the\noverfitting and non-overfitting regimes across all scenarios\nof missing entries. Overfitting seems to help both models\nin some cases. The performance gap grows in the over-\nfitting case where the best performance across the entire\ntable (for 2, 3, and 4 missing elements) is achieved by our\nmarginalized method in the case of 4 missing elements. This\nresult may appear counterintuitive, since a larger number\nof missing entries would typically be expected to degrade\nperformance. A plausible explanation is the presence of\nnoise in the data: in this regime, relying on incomplete ob-\nservations may be more robust, as the model depends less on\npotentially noisy measurements and more on the underlying\ndata statistics.\nFor the BCW dataset (Table 4), we observe that TPBS mod-\nels outperform neural networks in all but the first scenario\nof two random missing entries in the non-overfitting regime.\nMoreover, TBPS appears to be noticeably insensitive to\noverfitting, while in neural networks the difference is still\nsmall but more apparent. The performance gap between the\ntwo models appears in the overfitting case and is constant\nacross all scenarios of missing entries. Again, regularization\nconsistently benefits the TPBS models in all cases and sce-\nnarios, but improves the neural network performance only\nin the overfitting case.\nFor the Sarcos dataset (Table 5), we observe that neural\nnetworks outperform TPBS models in both the overfitting\nand non-overfitting regimes across all scenarios of missing\nentries. The performance gap narrows in the overfitting case\nand in the scenario with 4 missing entries. While regulariza-\ntion does not always improve neural network performance, it\nconsistently benefits TPBS models. For the Physico dataset\n(Table 6), TPBS models outperform neural networks in both\nregimes and across all missing-entry scenarios. Interest-\ningly, overfitting consistently improves TPBS performance,\nwhereas it severely degrades neural network performance.\nAgain, regularization is consistently beneficial for TPBS\nmodels, while its effect on neural networks is actually detri-\nmental (best performing model does not use regularization\nin all cases considered).\nFor the Diabetes dataset (Table 7), neural networks slightly\noutperform TPBS models in the non-overfitting regime,\nwhile TPBS models clearly lead in the overfitting regime.\nOverfitting has a mild effect on TPBS models but signifi-\ncantly harms neural networks."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of \"Penalizing Localized Dirichlet Energies in Low Rank Tensor Products\"\n\n#### Main Contributions\n\n**1. Closed-Form Expression for Dirichlet Energy:** The paper demonstrates that the Dirichlet energy of low-rank tensor product (TPBS) models can be computed analytically in closed form, providing insights into how these models achieve near-perfect interpolations with exponentially small Dirichlet energy.\n\n**2. Novel Regularization Strategy:** A regularization method based on localized Dirichlet energies centered at training points is proposed for both regression and classification tasks. This approach mitigates the limitations of global Dirichlet energy-based regularization, particularly in scenarios where perfect interpolation can be achieved with minimal Dirichlet energy.\n\n**3. Inference Framework for Incomplete Data:** A framework for inference from incomplete data vectors (e.g., samples with missing elements) is introduced. Two estimators are proposed to handle these cases effectively, ensuring robustness and accurate predictions even when observations are not fully available.\n\n#### Methodology\n\nThe authors leverage the closed-form expression of Dirichlet energy in TPBS models to derive a regularization strategy that penalizes localized Dirichlet energies rather than global ones. This approach is agnostic to the choice of univariate function representations, such as Fourier series or B-splines, within the low-rank tensor product framework.\n\nThe proposed regularization method, LDE\u03c1(g), involves integrating over small hypercubes centered at training points, allowing for a balance between localized and global approximations. The effectiveness of this approach is demonstrated through empirical experiments on various datasets spanning classification and regression tasks.\n\n#### Key Findings\n\n**1. Perfect Interpolation with Exponentially Small Dirichlet Energy:** TPBS models can achieve perfect interpolation with exponentially small Dirichlet energy, indicating that traditional regularization methods based on global Dirichlet energy are ineffective in these scenarios.\n\n**2. Superior Performance in Overfitting Regime:** The authors find that TPBS models outperform neural networks significantly more often than not, particularly in the overfitting regime where they exhibit notably more robust behavior compared to neural networks. In contrast, neural networks suffer from significant degradation due to overfitting in many cases.\n\n**3. Effective Regularization for Incomplete Observations:** The proposed framework handles incomplete observations by leveraging localized Dirichlet energies and averaging estimators. This approach consistently improves TPBS models' performance across various datasets, even when data is not fully observed.\n\n#### Conclusions\n\nThe paper presents a novel regularization strategy based on localized Dirichet energy in low-rank tensor product models, which outperforms traditional methods like global Dirichlet energy-based regularization for both regression and classification tasks. The proposed framework also demonstrates robustness to incomplete observations, making it particularly useful in practical applications where data may be missing or noisy.\n\nThe findings suggest that TPBS models can offer a more effective alternative to neural networks in scenarios requiring robust generalization and handling of overfitting issues. Overall, the work contributes significantly to understanding and improving function approximation using low-rank tensor products, providing a principled approach for regularization and inference under various data conditions."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative\nEngine Optimization\nHeyang Zhou1, JiaJia Chen2, Xiaolu Chen1, Jie Bao1, Zhen Chen1*, Yong Liao1\n1School of Cyber Science and Technology, University of Science and Technology of China\n2Institute of Dataspace, Hefei Comprehensive National Science Center, Hefei, China\nheyangzhou@mail.ustc.edu.cn\nAbstract\nAs Generative Engines revolutionize informa-\ntion retrieval by synthesizing direct answers\nfrom retrieved sources, ensuring source visibil-\nity becomes a significant challenge. Improv-\ning it through targeted content revisions is a\npractical strategy termed Generative Engine\nOptimization (GEO). However, optimizing a\ndocument for diverse queries presents a con-\nstrained optimization challenge where hetero-\ngeneous queries often impose conflicting and\ncompeting revision requirements under a lim-\nited content budget. To address this challenge,\nwe propose IF-GEO, a \u201cdiverge-then-converge\u201d\nframework comprising two phases: (i) min-\ning distinct optimization preferences from rep-\nresentative latent queries; (ii) synthesizing a\nGlobal Revision Blueprint for guided editing\nby coordinating preferences via conflict-aware\ninstruction fusion. To explicitly quantify IF-\nGEO\u2019s objective of cross-query stability, we\nintroduce risk-aware stability metrics. Experi-\nments on multi-query benchmarks demonstrate\nthat IF-GEO achieves substantial performance\ngains while maintaining robustness across di-\nverse retrieval scenarios.\n1\nIntroduction\nThe evolution from traditional Search Engines\n(SEs) to Generative Search Engines (GSEs) rep-\nresents a significant shift in information re-\ntrieval (Brin and Page, 1998; Aggarwal et al.,\n2024). Unlike SEs, which present ranked lists of\nhyperlinks, GSEs adopt a retrieve-then-generate\nparadigm (Lewis et al., 2020; Karpukhin et al.,\n2020). By employing Large Language Models\n(LLMs) to synthesize direct answers from retrieved\ndocuments, GSEs provide users with synthesized\ninformation rather than a list of sources.\nHowever, ensuring source visibility presents a\nsignificant challenge for content providers. Since\n* Corresponding author.\nFigure 1: Challenges of GEO. Revision requests of dif-\nferent queries can be conflicting and competitive under a\nlimited content budget. GEO have no idea which query\nto follow.\nvisibility depends on whether a document is se-\nlected and cited by the model rather than its rank-\ning position, traditional optimization techniques\nare less effective. To address this, Generative En-\ngine Optimization (GEO) has been proposed as a\nprominent strategy to improve content visibility\nin generated responses through targeted document\nrevisions (Aggarwal et al., 2024).\nExisting GEO methods typically utilize static\nheuristic rules (Aggarwal et al., 2024) or optimize\ncontent based on aggregated engine preferences in-\nferred from ranking signals (Wu et al., 2025; Chen\net al., 2025a). While recent work like RAID (Chen\net al., 2025b) advances the field by modeling la-\ntent retrieval intents, it prioritizes a single aggre-\ngated intent trajectory. These approaches treat the\nserving of diverse heterogeneous queries as a one-\ndimensional optimization problem. However, in\npractice, a single document needs to serve diverse\nuser queries simultaneously (Broder, 2002; Clarke\net al., 2008; Wang et al., 2009). This creates a\nconstrained optimization problem, where hetero-\ngeneous queries often impose conflicting require-\nments on the document\u2019s limited content as illus-\narXiv:2601.13938v1  [cs.IR]  20 Jan 2026\ntrated in Figure 1 (Marler and Arora, 2004; Rose\nand Levinson, 2004). Current methods lack the\nmechanisms to coordinate these competing pref-\nerences. Consequently, optimizing for one query\noften results in diminished or negative gains for\nothers, creating competitive imbalances that we em-\npirically analyze in Appendix A, leading to incon-\nsistent revisions and performance variance across\nthe query set.\nTo address this challenge, we propose IF-GEO,\na \u201cdiverge-then-converge\u201d framework comprising\ntwo phases: (i) mining representative latent queries\nand formulating their distinct optimization prefer-\nences as structured edit requests; and (ii) synthesiz-\ning a unified Global Revision Blueprint for guided\nediting by coordinating preferences via conflict-\naware instruction fusion. Crucially, we formally\nintegrate risk-aware stability metrics into the IF-\nGEO optimization objective to explicitly quan-\ntify cross-query stability. By introducing Worst-\nCase Performance (WCP), Win-Tie Rate (WTR),\nand Downside Risk (DR), we establish a robust\nstandard that addresses the limitations of mean-\nvariance evaluations\u2014which often mask tail degra-\ndations and conflate beneficial upside with harmful\ndownside volatility.\nOur contributions are summarized as follows:\n\u2022 We\npropose\nIF-GEO,\na\n\u201cdiverge-then-\nconverge\u201d framework.\nIt predicts latent\nqueries to formulate specific edit requests and\nemploys conflict-aware instruction fusion to\nsynthesize a unified Global Revision Blueprint\nfor coherent document revision.\n\u2022 We\nintroduce\nan\nevaluation\nmethodol-\nogy\nutilizing\nrisk-aware\nstability\nmet-\nrics\u2014specifically Worst-Case Performance,\nDownside Risk,\nand Win-Tie Rate\u2014to\nexplicitly quantify the safety and robustness\nof optimization across diverse queries.\n\u2022 Empirical results on multi-query benchmarks\ndemonstrate that IF-GEO achieves substan-\ntial improvements in overall visibility while\neffectively mitigating performance variance,\nensuring robust stability across heterogeneous\nretrieval scenarios.\n2\nRelated Works\n2.1\nGenerative Search Engines\nGenerative Search Engines (GSEs) increasingly fol-\nlow a retrieve\u2013then\u2013generate paradigm, producing\nanswer-centric responses with explicit source at-\ntributions. Technically, many such systems build\non retrieval-augmented generation (RAG) (Lewis\net al., 2020) and retrieval-augmented pretrain-\ning/memory architectures such as REALM (Guu\net al., 2020) and RETRO (Borgeaud et al., 2022).\nIn practice, these pipelines often pair dense neu-\nral retrievers (e.g., DPR) (Karpukhin et al., 2020)\nwith multi-passage evidence fusion generators (e.g.,\nFiD) (Izacard and Grave, 2021). Unlike traditional\nsearch engines that prioritize document ranking\nbased on keyword matching(Gleason et al., 2023),\nGSEs leverage the semantic reasoning capabilities\nof LLMs to synthesize information from multiple\ndisparate sources (Li et al., 2025). To improve ver-\nifiability and citation consistency, prior work has\nintroduced browsing/citation constraints and dedi-\ncated benchmarks for attribution quality (e.g., We-\nbGPT (Nakano et al., 2021), GopherCite (Menick\net al., 2022), and CITE (Gao et al., 2023)). These ar-\nchitectural shifts fundamentally alter the optimiza-\ntion landscape, motivating downstream objectives\nthat emphasize not just retrieval rank, but visibility\nand attribution within the generated narrative.\n2.2\nGenerative Engine Optimization\nGenerative Engine Optimization (GEO) aims to im-\nprove visibility within generative answers through\ncontent modifications. Existing work broadly falls\ninto three lines. (i) Single-objective and heuris-\ntic optimization.\nEarly work formalized GEO\nwith heuristic strategies (Aggarwal et al., 2024),\nwhile recent works explored specific interven-\ntions like caption injection (Chen and Liao, 2025)\nor transformer-based rewriting (L\u00fcttgenau et al.,\n2025). These approaches apply preset or single-\ntarget editing rules, lacking adaptation to diverse\nquery conflicts. (ii) Feedback-driven Optimization.\nThis line treats engines as black boxes, optimizing\ncontent by inferring implicit preferences from rank-\ning signals (Wu et al., 2025) or employing iterative\nfeedback loops (Bagga et al., 2025; Chen et al.,\n2025a). However, they often overfit to specific\nengine behaviors and ignore cross-query stability.\n(iii) Intent-based optimization. This line consid-\ners optimization over a document\u2019s latent queries.\nRAID leverages multi-role reflection to generalize\nlatent search intents(Chen et al., 2025b), but opti-\nmizes around a single aggregated intent. Overall,\nprior methods treat optimization targets in isolation\nor aggregation. They lack mechanisms to coordi-\nnate diverse intents, failing to balance competitive\ntrade-offs under a limited content budget, which\nultimately hinders robust and stable optimization.\n3\nProblem Definition\n3.1\nSetting and Notation\nGiven a query expression q, a generative engine\n(GE) retrieves a set of candidate documents Cq =\nRetrieval(q) = {D1, . . . , DN} and generates a re-\nsponse text rq = Generate(q, Cq), which includes\ncitations to documents in Cq. We define an answer-\ninduced visibility score v(Di, rq) to quantify how\nvisible a document Di \u2208Cq is within the response\nrq, based only on the response text and its cita-\ntion/attribution signals. Since rq is conditioned on\nq, we use the shorthand v(D, q) to denote the visi-\nbility of a document D under query q. Importantly,\nv(\u00b7) is a plug-in metric and can be instantiated with\ndifferent operational definitions of visibility.\n3.2\nOptimization Objective\nGiven a document D, we consider a target set of\nqueries Q(D) = qim\ni=1 that represent the queries\nwe aim to optimize for. For each qi \u2208Q(D), the\nvisibility of D under qi is measured by v(D, qi).\nA GEO method M takes D as input and pro-\nduces an optimized document D\u2032 = M(D). For\neach query qi \u2208Q(D), we define the per-query\nvisibility gain as:\n\u2206vi = v(D\u2032, qi) \u2212v(D, qi).\n(1)\nWe summarize the optimization effect over the\nquery set Q(D) by applying a plug-in aggregation\nfunctional A(\u00b7) to the gain vector:\n\u2206= A\n\u0000{ \u2206vi }m\ni=1\n\u0001\n.\n(2)\nThe choice of A is application-dependent and can\nbe instantiated with different operational defini-\ntions (e.g., mean for average improvement). GEO\naims to find a method M that optimizes \u2206under\nthe chosen definition of A.\n4\nMethod\n4.1\nOverview: IF-GEO\nWe propose IF-GEO, a two-stage framework for\nGEO under generative engines. IF-GEO follows a\n\u201cdiverge-then-converge\u201d workflow. In the diverge\nphase, it discover representative latent queries and\nelicit their distinct optimization preferences as\nstructured edit requests. The converge phase then\ncoordinates and balances these competing prefer-\nences through conflict-aware instruction fusion,\nsynthesizing them into a unified global revision\nblueprint that provides a comprehensive optimiza-\ntion direction for guided editing. Beyond maximiz-\ning average visibility, IF-GEO explicitly optimizes\nfor stability metrics (e.g., WCP, DR) to minimize\ncross-query downside risks, ensuring robust per-\nformance where standard mean-based approaches\noften fail.\nFigure 2 overviews the pipeline. All steps of\nIF-GEO are implemented via LLM calls, where the\nmodel acts as a constrained executor that outputs\nstructured intermediate artifacts rather than per-\nforming free-form rewriting. Full prompt specifica-\ntions and output schemas are provided in Appendix\nB.\nWe next detail the two phases of this workflow:\nQuery Discovery and Request Generation as the\ndiverge phase, followed by Instruction Fusion as\nthe converge phase.\n4.2\nQuery Discovery and Request Generation\n4.2.1\nQuery Mining\nAs the first step in the diverge phase, we conduct\nquery mining to expand the document into a repre-\nsentative query set. We treat this process as reverse\nretrieval: given D, we guide the LLM to act as\na search analyst and discover diverse queries for\nwhich the document should be a relevant result. To\nensure the set is representative and avoid redundant\ntuning, the system explicitly instructs the model\nto target different aspects of the document\u2019s main\ntheme while strictly prohibiting simple paraphrases.\nThis step outputs a weighted query set Q(D):\nQ(D) = {(qi, wi)}m\ni=1\n(3)\nwhere each query qi serves as a specific context for\ngenerating subsequent edit requests. The associ-\nated weight wi is a scalar score (0\u2013100) estimating\nquery popularity. Drawing on the scoring capa-\nbility demonstrated in G-EVAL (Liu et al., 2023),\nwe leverage the model\u2019s parametric knowledge to\nassign this priority, which guides the later fusion\nphase.\n4.2.2\nQuery-specific Request Generation\nGiven the weighted query set Q(D), the core objec-\ntive of this step is to elicit the distinct optimization\npreferences of each query regarding the document.\nIF-GEO operationalizes these abstract preferences\nFigure 2: Overview of the IF-GEO methodology. IF-GEO follows a \u201cdiverge-then-converge\u201d paradigm. In Phase I,\nthe system mines a representative query set and elicits query-specific editing requests, which may be redundant or\nconflicting. In Phase II, IF-GEO performs conflict-aware instruction fusion to synthesize a unified global revision\nblueprint, which guides controlled editing to produce an optimized document with stable visibility across queries.\ninto concrete, actionable edit requests (instruction\ncandidates). By analyzing each query qi in isola-\ntion, the model explicitly diagnoses specific con-\ntent gaps and proposes targeted revisions to address\nthem.\nFormally, for the j-th request derived from query\nqi, we define a structured request tuple:\nri,j = \u27e8ei,j, ui,j, si,j\u27e9\n(4)\nwhere ei,j is the excerpt-based anchor locating the\ntarget text, ui,j is the specific revision suggestion,\nand si,j is a quantitative necessity score (0\u2013100).\nAdopting the scalar evaluation methodology vali-\ndated in (Liu et al., 2023), we employ this score\nto explicitly measure the criticality of the fix for\nsatisfying qi, distinguishing essential edits from\nmarginal improvements. Aggregating these out-\nputs yields a diverged request pool R = {ri,j},\nwhich serves as the raw material for the subsequent\nconflict-aware fusion.\n4.3\nConflict-Aware Instruction Fusion\n4.3.1\nPrioritization and Deduplication\nThe converge phase begins by consolidating the di-\nverged request pool R into a compact, high-signal\ncandidate set. First, to identify requests with high\ncross-query importance, we compute a global pri-\nority score for each item:\ngi,j = wi \u00b7 si,j\n(5)\nwhere wi is the query weight and si,j is the ne-\ncessity score. We filter out low-priority requests\nfalling below a threshold \u03c4 to control perturbation.\nNext, we perform semantic deduplication to re-\nmove redundancy. Requests targeting overlapping\nanchors with similar revision goals are merged into\na single meta-request. These merged items inherit\nthe highest necessity score from their constituents\nand are standardized with concise topic tags. This\nstep reduces the raw pool into a smaller set of well-\nscoped candidates, ready for conflict resolution.\n4.3.2\nConflict Resolution\nFollowing deduplication, the system addresses mu-\ntually exclusive requests targeting the same con-\ntent. IF-GEO employs a priority-based resolution\nstrategy. Instead of relying on a rigid numerical\nthreshold, we delegate the decision to the model,\ninstructing it to evaluate the priority scores (g) in\ncontext to determine the appropriate action:\n\u2022 Selection: When the model perceives a signifi-\ncant priority gap, the higher-scoring instruction\nis strictly retained, and the conflicting one is dis-\ncarded.\n\u2022 Synthesis: When the model determines that pri-\nority scores are comparable (gi \u2248gj), it gen-\nerates a new compromise instruction. This in-\nstruction balances the valid needs of both queries\nrather than satisfying only one.\nThis approach leverages the model\u2019s semantic\nreasoning capabilities to handle edge cases more\nflexibly than hard-coded rules.\n4.3.3\nBlueprint Construction\nAs the final step of the converge phase, we consoli-\ndate the fused instructions into a Global Revision\nBlueprint. This step is crucial for coordinating\nthe competing needs of multiple queries within a\nshared content budget. We map individual instruc-\ntions to their corresponding document sections and\naggregate them into ordered plan items. By organiz-\ning revisions structurally rather than sequentially\nby query, the blueprint ensures that distinct opti-\nmization goals are integrated coherently. The\nresulting JSON blueprint serves as a strict execu-\ntion contract, effectively resolving the structural\nconflicts between queries and preventing the incon-\nsistent edits typical of single-pass optimization.\n4.3.4\nBlueprint-Guided Revision\nIn the final step, IF-GEO uses the Revision\nBlueprint to generate the optimized document D\u2032.\nThe revision model acts as a constrained editor,\nstrictly following the blueprint instructions to mod-\nify the document section by section. Crucially, the\nsystem explicitly instructs the model to preserve\nall unmentioned sections exactly as is. This strict\nadherence ensures that the global optimization strat-\negy is implemented accurately without introducing\nfree-form rewriting or unintended content drift.\n4.4\nRisk-Aware Optimization Objective\nStandard visibility maximization often masks tail\ndegradations and conflates beneficial upside with\nharmful downside volatility. To address this, we\nformally integrate risk-aware stability metrics into\nthe IF-GEO objective. Beyond maximizing ex-\npected gain E[\u2206v], we explicitly safeguard cross-\nquery stability through three dimensions:\n\u2022 Worst Case Performance (WCP) establishes a\nsafety lower bound by capturing the maximum\nsingle-query drop (Ben-Tal and Nemirovski,\n1998):\nWCP =\nm\nmin\ni=1 \u2206vi.\n(6)\n\u2022 Downside Risk (DR) exclusively penalizes the\nmagnitude of negative gains to distinguish harm-\nful failures from beneficial volatility (Mamoghli\nand Daboussi, 2009):\nDR = 1\nm\nm\nX\ni=1\n\u0000min(0, \u2206vi)\n\u00012.\n(7)\n\u2022 Win-Tie Rate (WTR) quantifies the coverage of\nnon-regressive optimization, serving as a proxy\nfor Pareto optimality without collateral dam-\nage(Radlinski et al., 2008):\nWTR = 1\nm\nm\nX\ni=1\nI(\u2206vi \u22650).\n(8)\nIn summary, IF-GEO seeks to maximize visibil-\nity gain subject to minimizing DR and maximizing\nWCP and WTR.\n5\nExperimental Setup\n5.1\nDataset\nTo evaluate performance in a realistic multi-query\nscenario, we adopt the benchmark introduced by\nRAID (Chen et al., 2025b), which expands the\nwidely-used GEO-Bench (Aggarwal et al., 2024).\nIn this dataset, each document D is originally\na top-ranked result (top-5) retrieved by Google\nSearch for a real-world source query. Building\non this high-relevance foundation, RAID extends\neach source query into a cluster of five related\nqueries. Crucially, these queries go beyond sim-\nple rephrasing that they constitute a multifaceted\ninquiry into the document\u2019s topic, targeting dis-\ntinct informational dimensions (e.g., definition,\nusage, pros/cons) rather than mere lexical varia-\ntions. This construction forms a query set that ef-\nfectively serves as a concrete instantiation of the\nrepresentative query set described in our problem\ndefinition. Using this dataset allows us to move\nbeyond single-query evaluation and test whether\nIF-GEO\u2019s optimized document maintains high visi-\nbility and stability across diverse user queries.\n5.2\nBaselines\nWe benchmark IF-GEO against three representa-\ntive methods covering the primary paradigms in\nGenerative Engine Optimization:\nHeuristic-based: GEO (Aggarwal et al., 2024).\nRepresenting static, query-agnostic optimization,\nthis framework applies nine human-designed\nheuristic rules. We evaluate all nine strategies,\nranging from stylistic interventions (e.g., AUTHOR-\nITATIVE) to content enrichment (e.g., STATISTICS\nADDITION, CITE SOURCES).\nTable 1: Comparison of IF-GEO with baseline methods on objective and subjective visibility improvements. We\nreport overall improvement (Mean) to summarize average gains. Underlined indicates the best baseline.\nMethod\nObjective Impression\nSubjective Impression\nWord\nPosition\nOverall\nRel.\nInfl.\nUnique\nDiv.\nFollowUp\nPos.\nCount\nAverage\nTran. SEO\n1.83\n1.77\n1.84\n1.44\n1.62\n1.24\n1.50\n1.44\n1.73\n1.58\n1.51\nUniq. Word\n-0.70\n-1.00\n-0.79\n-0.20\n-0.16\n-0.44\n-0.42\n-0.46\n-0.71\n-0.38\n-0.39\nSimp. Expr.\n0.16\n0.28\n0.28\n0.59\n0.84\n0.73\n0.72\n0.35\n1.02\n0.57\n0.69\nAuth. Expr.\n0.97\n0.88\n0.92\n0.56\n1.08\n0.60\n0.86\n0.57\n1.29\n0.74\n0.81\nFlue. Expr.\n0.92\n0.94\n1.03\n0.52\n0.82\n0.48\n0.48\n0.29\n1.23\n0.56\n0.63\nTerm. Addi.\n1.00\n1.07\n1.17\n0.71\n1.22\n0.50\n0.73\n0.43\n1.06\n0.63\n0.75\nCite Sources\n4.47\n4.59\n4.71\n3.17\n3.45\n3.36\n3.16\n2.96\n3.83\n3.26\n3.31\nQuot. Addi.\n4.29\n4.19\n4.23\n2.57\n2.87\n3.10\n2.45\n2.25\n3.28\n2.48\n2.71\nStat. Addi.\n3.28\n3.39\n3.49\n2.15\n2.48\n2.54\n1.97\n1.89\n3.01\n2.16\n2.31\nRAID\n1.06\n0.78\n0.88\n1.44\n1.75\n1.40\n1.08\n0.84\n1.68\n1.34\n1.36\nAuto-GEO\n7.80\n7.64\n7.59\n4.99\n5.68\n5.18\n4.74\n4.19\n6.63\n4.77\n5.30\nIF-GEO\n11.07\n11.15\n11.03\n5.12\n6.16\n5.90\n5.29\n5.34\n7.32\n5.98\n5.87\nIntent-based: RAID G-SEO (Chen et al., 2025b).\nRepresenting intent-driven optimization, RAID em-\nploys a \u201c4W\u201d multi-role reflection mechanism to\ninfer and refine latent user search intents. It guides\ncontent rewriting by deepening a single, focused\nintent trajectory.\nPreference-based: Auto-GEO (Wu et al., 2025).\nRepresenting data-driven optimization, this frame-\nwork automatically learns generative engine prefer-\nences from large-scale ranking data. We apply the\nlearned, engine-specific rule sets to optimize target\ndocuments based on global utility signals.\nFor a fair comparison, all baselines and IF-GEO\nare implemented using the same underlying LLM\n(GPT-4o-mini) and decoding configurations to iso-\nlate the impact of the optimization methodology.\n5.3\nEvaluation Metrics\nFollowing the protocol in GEO-bench (Aggarwal\net al., 2024), we evaluate both overall visibility\nand cross-query stability. For each query qi, we\ncompute the visibility score v(D, qi) and the opti-\nmization gain \u2206vi.\nVisibility Instantiations v(\u00b7).\nWe instantiate v(\u00b7)\nwith:\n(i) Objective Impression, the Position-\nAdjusted Word Count (PAWC) that weights cited\ntext volume by citation position; and (ii) Sub-\njective Impression, a qualitative LLM-as-a-judge\nscore implemented via G-EVAL (Liu et al., 2023)\nand normalized to match PAWC statistics following\n(Aggarwal et al., 2024).\nAggregation Strategies A(\u00b7).\nTo comprehen-\nsively evaluate the gain vector {\u2206vi}m\ni=1, we instan-\ntiate the aggregation functional A(\u00b7) (defined in \u00a73)\nusing distinct criteria. We report Mean and Vari-\nance to measure general effectiveness and volatility.\nCrucially, to evaluate the safety objectives formu-\nlated in \u00a74.4, we report Worst-Case Performance\n(WCP), Downside Risk (DR), and Win-Tie Rate\n(WTR). All metrics are computed per document\nand averaged over the evaluation set.\n5.4\nExperimental Environment and Setting\nWe follow the simulated generative engine setup\nand evaluation protocol in GEO (Aggarwal et al.,\n2024) for reproducibility. All experiments are con-\nducted under a single target generative engine, GPT-\n4o-mini, using the same decoding parameters as\nGEO to ensure comparability.\nFor IF-GEO, all internal calls use the same\nmodel.\nUnless otherwise specified, we use:\nquery expansion size Nq=5, suggestions per query\nNs=5, internal temperature 0.2, and global priority\nfiltering threshold \u03c4=0.7. We analyze the impact\nof query expansion size in \u00a76.3.\nDue to the cost of generative-engine evalua-\ntion, different experiments use different numbers\nof queries. Unless otherwise specified, main results\nare evaluated on 1,000 queries, matching the origi-\nnal GEO test set. Ablation studies use 250 queries,\nand the query expansion analysis in \u00a76.3 uses 500\nqueries. All queries are sampled from the same\ndistribution.\nTable 2: Comparison of IF-GEO with baseline methods in robustness and stability across queries. We report variance\n(VAR\u2193), worst-case performance (WCP\u2191), win\u2013tie rate (WTR\u2191), and downside risk (DR\u2193) for both the primary\nobjective metric (Obj. Overall) and subjective metric (Subj. Average).\nMethod\nObjective (Overall)\nSubjective (Average)\nVAR (\u2193)\nWCP (\u2191)\nWTR (\u2191)\nDR (\u2193)\nVAR (\u2193)\nWCP (\u2191)\nWTR (\u2191)\nDR (\u2193)\nTran. SEO\n0.0147\n-0.0878\n68.93%\n0.0062\n0.0199\n-0.1026\n88.19%\n0.0080\nUniq. Word\n0.0157\n-0.1316\n61.04%\n0.0095\n0.0226\n-0.1361\n85.34%\n0.0130\nSimp. Expr.\n0.0116\n-0.1051\n66.43%\n0.0058\n0.0156\n-0.0939\n88.17%\n0.0073\nAuth. Expr.\n0.0136\n-0.1018\n65.84%\n0.0065\n0.0200\n-0.1148\n86.89%\n0.0089\nFlue. Expr.\n0.0130\n-0.1028\n68.93%\n0.0061\n0.0187\n-0.1084\n87.38%\n0.0091\nTerm. Addi.\n0.0143\n-0.1113\n67.31%\n0.0062\n0.0216\n-0.1266\n86.61%\n0.0103\nCite Sources\n0.0165\n-0.0785\n72.06%\n0.0044\n0.0209\n-0.0892\n88.93%\n0.0070\nQuot. Addi.\n0.0173\n-0.0831\n70.56%\n0.0055\n0.0218\n-0.1013\n88.25%\n0.0087\nStat. Addi.\n0.0173\n-0.0901\n72.58%\n0.0060\n0.0203\n-0.0946\n88.48%\n0.0074\nRAID\n0.0166\n-0.1141\n64.50%\n0.0088\n0.0195\n-0.1100\n82.37%\n0.0089\nAuto-GEO\n0.0159\n-0.0511\n73.56%\n0.0043\n0.0203\n-0.0798\n84.19%\n0.0064\nIF-GEO\n0.0189\n-0.0090\n80.50%\n0.0023\n0.0116\n-0.0419\n85.56%\n0.0036\n6\nExperiments\n6.1\nMain Results\nOverall effectiveness.\nTable 1 compares IF-GEO\nwith a diverse set of baselines. IF-GEO achieves\nthe best performance across all objective and sub-\njective dimensions, reaching an objective overall\nscore of 11.03 and a subjective average of 5.87. No-\ntably, Auto-GEO forms the strongest baseline tier\nacross nearly all dimensions, reflecting the effec-\ntiveness of explicitly extracted generative-engine\npreference rules in shaping citation- and visibility-\noriented content. RAID yields limited gains in our\nsimulation. This performance lag likely stems from\nits singular query trajectory\u2014converging on one\nrefined query from a single initial estimate \u2014which\nfails to sufficiently span the latent queries or cap-\nture heterogeneous user expectations. Within GEO\nheuristics, evidence-driven strategies (e.g., CITE\nSOURCES, QUOTE ADDITION) are consistently\nstrongest, while lexical interventions (e.g., UNIQ.\nWORD) can even hurt performance.\nRobustness and stability.\nBeyond average gains,\nTable 2 reports stability summaries over the la-\ntent queries. IF-GEO achieves the best objective\nWCP and the lowest DR, indicating strong protec-\ntion against query-specific regressions. Notably, a\nhigher VAR does not necessarily imply worse tail\nstability, as variance captures both positive and neg-\native dispersion. DR and WCP more directly char-\nacterize downside behavior across queries. While\nAuto-GEO is the strongest baseline on several tail-\noriented criteria, it still exhibits larger worst query\ndrops than IF-GEO, and heuristic methods remain\nmore brittle.\nOverall, these results confirm that explicitly coor-\ndinating revisions to resolve conflicts yields higher\nand safer holistic visibility than applying isolated,\nquery-agnostic edits.\n6.2\nAblation Study\nTo quantify each component\u2019s contribution, we\nablate IF-GEO by removing conflict resolution,\nblueprint construction, or the entire instruction\nfusion stage (Phase II). Table 3 reveals a clear\ndivision of labor.\nConflict resolution acts as a\nsafety guardrail: without it, incompatible query-\nspecific edits propagate into execution, producing\nthe largest overall drop and the worst tail behavior\n(more negative WCP and higher DR). Instruction\nfusion is the main stabilizer across queries: remov-\ning it most strongly reduces reliability (WTR) and\namplifies downside risk, consistent with uncoordi-\nnated local requests being redundant or misaligned\nwhen aggregated. In contrast, blueprint construc-\ntion primarily affects gain realizability\u2014removing\nit lowers the achievable overall score while leaving\nDR essentially unchanged. Together, these results\nsuggest a separation of concerns: conflict resolu-\ntion and instruction fusion primarily affect stability\nacross queries, while blueprint construction mainly\nimproves the executability of the aggregated edits.\n6.3\nImpact of query expansion Size\nWe examine the scaling behavior of IF-GEO by\nvarying the number of expanded queries N \u2208\n{1, 3, 5, 7, 9} while keeping other hyperparameters\nfixed. Figure 3 shows that increasing N yields\nconsistent gains: the objective mean performance\nFigure 3: Effect of query expansion size N on performance and stability. (a) Overall objective performance (Mean).\n(b) Win\u2013tie rate (WTR). (c) Worst-case performance (WCP). (d) Volatility metrics: variance (VAR) and downside\nrisk (DR; lower is better).\nTable 3: Ablation study of IF-GEO. We report the over-\nall objective score (Mean) and stability metrics (VAR,\nWCP, WTR, DR) to demonstrate the contribution of\neach component.\nVariant\nMean (\u2191)\nVAR (\u2193)\nWCP (\u2191)\nWTR (\u2191)\nDR (\u2193)\nIF-GEO (Full)\n9.24\n0.0156\n-0.0328\n80.80%\n0.0021\nw/o Blueprint\n8.18\n0.0167\n-0.0517\n81.20%\n0.0021\nw/o Instru. Fusion\n7.07\n0.0156\n-0.0569\n74.80%\n0.0043\nw/o Conflict Res.\n6.14\n0.0174\n-0.0713\n77.20%\n0.0032\nrises monotonically from 8.06 at N=1 to 10.02\nat N=9 (Figure 3a). Stability also improves over-\nall as query coverage expands, with lower down-\nside risk and less negative worst-case performance.\nHowever, these stability gains exhibit diminishing\nreturns beyond N=5 (Figure 3b\u2013d): WTR peaks\nat N=5 (80.00%) and then fluctuates, while DR\nand WCP change only marginally from N=5 to\nN=9. Since computation scales roughly linearly\nwith N due to query generation and fusion over-\nhead, we adopt N=5 as a balanced default that\nachieves near-peak stability with competitive gains\nat substantially lower latency.\n6.4\nAdaptability Analysis\nWe further examine the adaptability of IF-GEO\nunder two realistic variations: changing the un-\nderlying generative engine and varying the initial\nranking of optimized documents.\nCross-model generalization.\nWe evaluate IF-\nGEO on a different generation model, Gemini-\n2.0-Flash, without any method-specific tuning.\nAs shown in Appendix E, IF-GEO consistently\nachieves the strongest objective gains and favor-\nable stability profiles compared with all baselines.\nNotably, while Auto-GEO remains a competitive\nbaseline by leveraging engine-specific preference\nrules, IF-GEO preserves its advantages in worst-\ncase performance and win\u2013tie rate, suggesting that\nexplicit cross-query consolidation generalizes be-\nyond a single generative engine.\nSensitivity to initial document ranking.\nWe fur-\nther analyze IF-GEO by stratifying documents ac-\ncording to their initial ranking positions prior to\noptimization. Results in Appendix D show that\nIF-GEO yields consistent improvements across\nall rank buckets, rather than concentrating gains\non already well-ranked documents.\nIn particu-\nlar, both objective and subjective metrics indicate\nstable worst-case performance and low downside\nrisk even for lower-ranked inputs, suggesting that\nIF-GEO improves content robustness rather than\nmerely exploiting positional advantages.\n7\nConclusion\nWe study Generative Engine Optimization, aiming\nto improve a document\u2019s visibility across a repre-\nsentative query set while maintaining cross-query\nstability. We propose IF-GEO, a \u201cdiverge-then-\nconverge\u201d framework that discovers latent queries,\nelicits query-specific edit requests (instruction can-\ndidates), and fuses them via prioritization, dedupli-\ncation, and conflict resolution into a unified, exe-\ncutable revision blueprint for blueprint-guided edit-\ning. Experiments on GEO-Bench under GPT-4o-\nmini show that IF-GEO surpasses strong baselines,\nachieving higher overall gains with lower downside\nrisk. These results underscore the importance of\nexplicitly coordinating competing revision signals\nfor robust GEO beyond single-query tuning.\n8\nLimitations\nDespite its effectiveness, our work has three main\nlimitations. First, Inference Cost. The multi-stage\n\u201cdiverge-then-converge\u201d workflow involves multi-\nple LLM calls, resulting in higher token consump-\ntion than single-pass baselines. Second, Simula-\ntion Gap. Following standard GEO protocols, we\nrely on LLM-simulated environments (e.g., GPT-\n4o-mini). While reproducible, these simulations\nmay not perfectly mirror the commercial engines.\nThird, Dependency on Query Discovery. The\nquality of the Global Revision Blueprint hinges\non the representativeness of the mined queries. If\nthe initial query expansion fails to capture the true\nlatent search intent distribution, the subsequent op-\ntimization may be misaligned.\nReferences\nPranjal Aggarwal, Vishvak Murahari, Tanmay Rajpuro-\nhit, Ashwin Kalyan, Karthik Narasimhan, and Ameet\nDeshpande. 2024. GEO: generative engine optimiza-\ntion. In Proceedings of the 30th ACM SIGKDD Con-\nference on Knowledge Discovery and Data Mining,\nKDD 2024, Barcelona, Spain, August 25-29, 2024,\npages 5\u201316. ACM.\nPuneet S Bagga, Vivek F Farias, Tamar Korkotashvili,\nTianyi Peng, and Yuhang Wu. 2025. E-geo: A testbed\nfor generative engine optimization in e-commerce.\narXiv preprint arXiv:2511.20867.\nAharon Ben-Tal and Arkadi Nemirovski. 1998. Robust\nconvex optimization. Mathematics of operations re-\nsearch, 23(4):769\u2013805.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, and 9 others. 2022. Improving language\nmodels by retrieving from trillions of tokens. In In-\nternational Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 2206\u20132240. PMLR.\nSergey Brin and Lawrence Page. 1998. The anatomy of\na large-scale hypertextual web search engine. Com-\nput. Networks, 30(1-7):107\u2013117.\nAndrei Broder. 2002. A taxonomy of web search. SI-\nGIR Forum, 36(2):3\u201310.\nMahe Chen, Xiaoxuan Wang, Kaiwen Chen, and Nick\nKoudas. 2025a.\nGenerative engine optimization:\nHow to dominate AI search. CoRR, abs/2509.08919.\nXiaolu Chen and Yong Liao. 2025. Caption injection\nfor optimization in generative search engine. CoRR,\nabs/2511.04080.\nXiaolu Chen, Haojie Wu, Jie Bao, Zhen Chen, Yong\nLiao, and Hu Huang. 2025b. Role-augmented intent-\ndriven generative search engine optimization. CoRR,\nabs/2508.11158.\nCharles L. A. Clarke, Maheedhar Kolla, Gordon V.\nCormack, Olga Vechtomova, Azin Ashkan, Stefan\nB\u00fcttcher, and Ian MacKinnon. 2008. Novelty and\ndiversity in information retrieval evaluation. In Pro-\nceedings of the 31st Annual International ACM SI-\nGIR Conference on Research and Development in\nInformation Retrieval, SIGIR 2008, Singapore, July\n20-24, 2008, pages 659\u2013666. ACM.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2023, Singapore, December 6-\n10, 2023, pages 6465\u20136488. Association for Compu-\ntational Linguistics.\nJeffrey L. Gleason, Desheng Hu, Ronald E. Robertson,\nand Christo Wilson. 2023. Google the gatekeeper:\nHow search components affect clicks and attention.\nIn Proceedings of the Seventeenth International AAAI\nConference on Web and Social Media, ICWSM 2023,\nLimassol, Cyprus, June 5-8, 2023, pages 245\u2013256.\nAAAI Press.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training.\nvolume\nabs/2002.08909.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper \"IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization\"\n\n#### Main Contributions\n\nThe paper introduces IF-GEO, a novel framework designed to optimize multi-query responses generated by generative engines (GSEs) while maintaining cross-query stability. The main contributions are:\n\n1. **Proposed Framework**: IF-GEO is a \"diverge-then-converge\" framework comprising two phases: query discovery and request generation in the diverge phase, followed by instruction fusion in the converge phase.\n\n2. **Query Discovery and Request Generation**: This phase involves mining representative latent queries to elicit distinct optimization preferences as structured edit requests. The system uses a weighted query set to guide the model towards diverse aspects of the document's main theme while avoiding simple paraphrases.\n\n3. **Instruction Fusion**: In this phase, conflicting and competitive preference requests are resolved through conflict-aware instruction fusion. This ensures that multiple queries can be handled simultaneously without compromising on content quality or visibility.\n\n4. **Risk-Aware Optimization Objective**: The authors introduce risk-aware stability metrics (Worst-Case Performance, Downside Risk, Win-Tie Rate) to explicitly quantify the safety and robustness of optimization across diverse queries. These metrics help in addressing the limitations of mean-based evaluations by capturing both beneficial upside and harmful downside volatility.\n\n#### Methodology\n\n1. **Query Mining**: The system conducts query mining to expand a document into a representative query set, ensuring diversity while avoiding redundancy through explicit model instructions targeting different aspects of the document's theme.\n\n2. **Request Generation**: For each weighted query in the set, structured edit requests are generated, capturing specific content gaps and proposed targeted revisions. These requests are analyzed independently for criticality before being aggregated into a diverged request pool.\n\n3. **Instruction Fusion**: The system consolidates these diverged requests into a unified global revision blueprint through prioritization, deduplication, and conflict resolution steps. This ensures that the instructions guide coherent document editing while balancing competing preferences across queries.\n\n#### Key Findings\n\n1. **Performance Gains**: Experiments on multi-query benchmarks demonstrate substantial performance gains for IF-GEO compared to existing methods like GEO (Generative Engine Optimization) and RAID (Reinforced Auto-Geometric Optimization). \n\n2. **Robust Stability**: The framework achieves robust stability across diverse retrieval scenarios, as evidenced by lower Worst-Case Performance scores and reduced Downside Risk.\n\n3. **Safety Metrics**: Explicitly incorporating risk-aware metrics like WCP and DR helps in quantifying the safety of optimization outcomes, ensuring that gains are not at the expense of harmful volatility.\n\n#### Conclusions\n\nThe paper concludes with a robust evaluation framework for IF-GEO, showing its effectiveness across diverse queries. The proposed \"diverge-then-converge\" approach successfully addresses the challenges posed by optimizing multi-query responses in generative engines while maintaining cross-query stability and safety. This work highlights the importance of explicitly coordinating competing revision signals to achieve robust Generative Engine Optimization beyond single-query tuning.\n\n### References\n\nThe references provided cover a range of related works, including foundational papers on search engines (Brin & Page 1998), recent advancements in generative models (Guu et al. 2020; Borgeaud et al. 2022), and existing optimization methods like GEO (Aggarwal et al. 2024) and RAID (Chen et al. 2025b)."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Lost in the Prompt Order:\nRevealing the Limitations of Causal Attention in Language Models\nHyunjong Ok1,2\nJaeho Lee1\n1POSTECH\n2HJ AILAB\nhyunjong.ok@gmail.com, jaeho.lee@postech.ac.kr\nAbstract\nLarge language models exhibit surprising sen-\nsitivity to the structure of the prompt, but the\nmechanisms underlying this sensitivity remain\npoorly understood. In this work, we conduct\nan in-depth investigation on a striking case: in\nmultiple-choice question answering, placing\ncontext before the questions and options (CQO)\noutperforms the reverse order (QOC) by over\n14%p, consistently over a wide range of models\nand datasets. Through systematic architectural\nanalysis, we identify causal attention as the\ncore mechanism: in QOC prompts, the causal\nmask prevents option tokens from attending\nto context, creating an information bottleneck\nwhere context becomes invisible to options.\n1\nIntroduction\nLarge language models (LLMs) are highly sensitive\nto prompt structure. Even minor changes in sur-\nface forms\u2014such as instruction phrasing, example\nplacements, or reasoning elicitation\u2014can lead to\nsubstantial differences in the prediction quality of\nthe models (Wei et al., 2022; Kojima et al., 2022).\nDespite the importance of this sensitivity for the\npractical reliability of LLMs, our current under-\nstanding remains largely descriptive. We know\n\u201cwhat\u201d LLMs are sensitive to, but not \u201cwhy.\u201d For\ninstance, Lu et al. (2022) has shown that permuting\nthe order of demonstrations in in-context learn-\ning (ICL) can dramatically affect accuracy, yet of-\nfers limited insight into what makes certain orders\npreferable. Likewise, recent studies on multiple-\nchoice question answering (MCQA) report signif-\nicant performance fluctuations under option per-\nmutations, but stop short of identifying the mecha-\nnisms underlying this phenomenon (Pezeshkpour\nand Hruschka, 2024; Zheng et al., 2024).\nIn this work, we undertake an in-depth study of\na less obvious but consequential form of prompt-\nsensitivity: the ordering of components in MCQA\nprompts. A typical MCQA prompt consists of three\nSome Cantonese don't like chili, so some\nsoutherners don't like chili.\nA.Some Cantonese love chili\nB.Some people who like peppers are (...)\nC.All Cantonese are southerners\nD.Some Cantonese like neither peppers (...)\nWhich of the following can guarantee the above\nargument?\nSome Cantonese don't like chili, so some\nsoutherners don't like chili.\nA.Some Cantonese love chili\nB.Some people who like peppers are (...)\nC.All Cantonese are southerners\nD.Some Cantonese like neither peppers (...)\nWhich of the following can guarantee the above\nargument?\nContext\nQuestion\nOption\nQOC\n14.7 %\nDrop!\nLogiQA: 39.1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0RACE-H: 69.5\nRACE-M: 74.3\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0SciQ: 94.1\nLogiQA: 33.0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0RACE-H: 48.8\nRACE-M: 49.6\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0SciQ: 86.9\nCQO\nFigure 1: Performance gap between CQO and QOC.\nWe measure the average accuracies of 21 decoder-only\nLLMs on 4 different datasets, when prompted in two\ndistinct structures; CQO (context-question-option) and\nQOC (question-option-context).\nelements: a context passage (C), a question (Q),\nand a set of options (O). Intuitively, reordering\nthese components should have little effect, as their\nsemantic content remains unchanged. Contrary to\nthis expectation, we find that placing the context be-\nfore the questions and options (CQO) consistently\nand substantially outperforms the reverse ordering\n(QOC) across a wide range of setups (Figure 1).1\nTo explain this phenomenon, we formulate three\ncompeting hypotheses and evaluate them through\na series of carefully controlled experiments. By\nsystematically validating (or invalidating) each hy-\npothesis, we aim to uncover the underlying fac-\ntors that drive context-order sensitivity in MCQA.\nSpecifically, we consider the following hypotheses:\n\u2022 Hypothesis 1: Biased training data. CQO-style\nprompts may be more prevalent in training data,\nmaking QOC an unfamiliar format for the model.\n\u2022 Hypothesis 2: Failures in option recall. QOC\nstructure makes it difficult for the model to recall\nthe options located in the middle of the prompt,\na.k.a. \u201clost-in-the-middle\u201d (Liu et al., 2024).\n\u2022 Hypothesis 3: Causal attention (winner). The\ncausal attention structure in decoder-only trans-\n1A similar phenomenon has also been noted by Shaier\net al. (2024) outside the MCQA context, which also does not\ndemystify why such phenomenon happens.\n1\narXiv:2601.14152v1  [cs.CL]  20 Jan 2026\nQuestion (Q)\nOptions (O)\nContext (C)\nCausal Mask\nCannot see future context\n(a) Decoder model\nQuestion (Q)\nOptions (O)\nContext (C)\nBidirectional (All tokens attend to all tokens)\n(b) Encoder model\nFigure 2: Decoder vs. Encoder attention. In QOC\n(Question\u2192Options\u2192Context), causal masking pre-\nvents decoder models from attending to the context\nwhile selecting among options, so they often answer\nfrom option priors rather than evidence. Encoder mod-\nels use bidirectional attention and can condition on the\ncontext when scoring the options.\nformers makes it impossible for the option tokens\nto attend to the context tokens, creating an infor-\nmation bottleneck (Figure 2).\nOur experiments\u2014conducted on a wide range of\ndatasets and models \u2014 support the third hypothesis,\nand rule out the other two hypotheses. Stepping\nfurther, we identify several factors that can affect\nthe impact of causal masking, namely the context\nlength and the option positions.\nFinally, building on the causal-attention-based\nexplanation, we design targeted interventions that\ncan improve the performance of LLMs on QOC-\nstructured prompts, or conversely, degrade the per-\nformance on CQO-structured prompts. These re-\nsults provide additional evidence that causal atten-\ntion is the driving mechanism of the sensitivity.\n2\nExperimental setup\nDatasets.\nWe evaluate on four reading compre-\nhension benchmarks that require context-based\nreasoning: LogiQA (Liu et al., 2020); RACE-\nH/M (Lai et al., 2017); SciQ (Welbl et al., 2017).\nAll tasks are in MCQA format with four options.\nModels.\nWe conduct experiments on 21 decoder-\nonly LLMs from four model families: LLaMA\n3 (Grattafiori et al., 2024), Qwen 2.5/3 (Yang et al.,\n2024), and Gemma 2 (Team et al., 2024). Model\nsizes range from 0.5B to 9B parameters, includ-\ning both base and instruction-tuned variants. For\narchitecture comparison experiments (\u00a73.3), we\nadditionally test Flan-T5 (Chung et al., 2024) fam-\nily (encoder-decoder) and BERT, RoBERTa, AL-\nBERT (Devlin et al., 2019) family (encoder-only).\nLogiQA\nRACE-H\nRACE-M\nSciQ\n0\n5\n10\n15\n20\n25\n30\nPerformance Gap (%)\n5.0\n6.2\n19.9\n21.2\n25.2\n23.8\n8.7\n5.1\nBase\nInstruct\n(a) Performance gap\n0\n1\n3\n5\nNumber of Shots\n50\n55\n60\n65\n70\n75\n80\nAccuracy (%)\n69.3\n54.5\n55.2\n57.2\n57.6\nCQO (0-shot Baseline)\nQOC (0-shot Baseline)\nQOC + ICL\n(b) ICL performance\nFigure 3: Hypothesis 1: Effect of instruction tun-\ning and in-context learning. (a) We compare the per-\nformance gaps between base and instruct models, and\nfind they remains remarkably consistent. (b) Few-shot\nprompting yields marginal gains. These results suggest\nthat the friendly formatting is not the primary driver.\nEvaluation metrics.\nWe measure accuracy under\ntwo prompt orderings: CQO and QOC, and utilize\nthe performance gap (\u2206= AccCQO \u2212AccQOC),\nwhich quantifies context order sensitivity.\nOther details.\nFurther details on datasets, mod-\nels, prompting templates, and evaluation/scoring\nprotocols are provided in Appendix B.\n3\nHypotheses and analysis\nAs shown in Figure 1, permuting the prompt order\nleads to a sharp performance drop across all bench-\nmarks. To understand the cause, we formulate and\ntest a set of hypotheses. All detailed experiments\nfor each model and dataset are in Section C.\n3.1\nHypothesis 1: Bias in training samples\nHypothesis. CQO is simply more frequent in the\ntraining data than QOC. Thus, in the QOC format,\nthe model may fail to process an unfamiliar format.\nExperiment. We test the training-distribution hy-\npothesis in two ways. First, CQO-like prompts\nare more common in instruction data; this hypothe-\nsis predicts a larger CQO-QOC gap for instruction\nmodels. We therefore compare nine matched base-\ninstruct pairs. Second, we use in-context learning\nto familiarize models with the QOC format, vary-\ning the number of demonstrations up to 5-shot.\nResults. As shown in Figure 3a, the CQO-QOC\ngaps are nearly identical, suggesting the phe-\nnomenon is not driven by instruction tuning. More-\nover, even with 5-shot demonstrations shown in\nFigure 3b, QOC accuracy improves by only 3.1%\nand remains far below CQO. Together, these results\nrule out training distribution as the primary cause.\n2\nA\nB\nC\nD\nTarget Option\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nRecall Accuracy (%)\n96.2\n92.8\n91.9\n94.2\n96.9\n95.2\n94.0\n95.4\nCQO\nQOC\nFigure 4: Hypothesis 2: Option recall analysis. To\ninvestigate \"forgetting\" the options due to the long con-\ntext intervention, we evaluated option recall accuracy.\nResults show that accuracy is consistently higher than\nCQO, indicating that the models retain option informa-\ntion and ruling out memory loss as the primary cause.\n3.2\nHypothesis 2: Failure to recall options\nHypothesis. LLMs often fail to recall the infor-\nmation located in the middle of the context (Liu\net al., 2024). Thus, in the QOC format, the model\nmay fail to correctly recall the options, which are\nlocated between the question and the context.\nExperiment. After presenting the prompt, we ask\nthe LLMs to recall each option. Precisely, we mea-\nsure the chance of an exact match for each option.\nResults. As shown in Figure 4, QOC achieves a\nsimilar, or even higher, recall accuracy than CQO.\nThis indicates that the failure of option recall may\nnot be the cause of accuracy drops in QOC.\n3.3\nHypothesis 3: Causal attention\nHypothesis. Causal attention mask prevents op-\ntion tokens from attending to context in QOC. In\ndecoder-only, each token can only attend to preced-\ning tokens. Thus, in QOC, where options appear be-\nfore context, option representations are computed\nwithout information from the context.\nExperiment 1: Architecture comparison. If causal\nmasking is the root cause, models with bidirec-\ntional attention should exhibit no performance gap.\nWe compare three architecture types: decoder-only\n(causal), encoder-decoder (bidirectional encoder),\nand encoder-only (bidirectional).\nFor encoder-\ndecoder models, we feed the entire prompt to the\nencoder and let the decoder generate the answer.\nResults 1. As shown in Figure 5a, decoder-only\nmodels show a 14.72% gap, encoder-decoder mod-\nels (Flan-T5) show 2.30%, and encoder-only mod-\nels (BERT, RoBERTa, ALBERT) show a near-\nzero gap (0.02%). This pattern strongly implicates\ncausal masking as the underlying mechanism.\nExperiment 2: Context removal test. If the context\nDecoder-only\n(Causal)\nEncoder-Decoder\n(Bi-directional)\nEncoder-only\n(Bi-directional)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\n69.3\n68.0\n21.6\n54.5\n65.7\n21.7\nGap: -14.72%\nGap: -2.30%\nGap: +0.02%\nCQO\nQOC\n(a) Model design comparison\nCQO\nQOC\nQO\n(No Context)\n40\n45\n50\n55\n60\n65\n70\n75\n80\nAccuracy (%)\n69.3%\n54.5%\n52.8%\nContext Ignored\n(\nBaseline)\n(b) Context position\nFigure 5: Hypothesis 3, Exp# 1: Architecture com-\nparison. (a) Decoder-only LLMs show a large gap.\nIn contrast, encoder-only and encoder-decoder LLMs\nhave a minimal accuracy gap, confirming that the causal\nmask is the primary factor. (b) For decoder-only LLMs,\nQOC performance drops to nearly QO, indicating that\nthe information inside the context is ignored.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nRelative Option Attention\nCQO\nQOC\n(a) Attention decay\nCQO\nQOC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nContext Attribution Ratio\n0.797\n0.335\n(b) Context attribution ratio\nFigure 6: Hypothesis 3, Exp# 2 & 3: Analysis of con-\ntext utilization. (a) Layer-wise option attention: In\nCQO, attention to options declines as context is inte-\ngrated, whereas in QOC it rises. (b) Gradient-based\nattribution: context tokens contribute substantially more\nto CQO predictions than to QOC.\nis effectively inaccessible in QOC, then removing it\nentirely should not materially change performance.\nWe therefore compare QOC against QO (Question-\nOptions only), where the context is omitted.\nResults 2. As shown in Figure 5b, QOC accuracy\nis nearly identical to QO, consistent with the model\nfailing to use the context in QOC. This supports\nthe hypothesis that the issue is not forgetting but\nrestricted access induced by the causal mask.\nExperiment 3: Attention analysis. We analyze\nhow attention to option tokens evolves across lay-\ners.\nAlso, we measure token contributions us-\ning Gradient\u00d7Input attribution (Shrikumar et al.,\n2017), aggregating per-token scores over the con-\ntext span and normalizing by total attribution.\nResults 3. Option tokens receive exactly zero atten-\ntion from context tokens in QOC by construction\nof the causal mask, confirming that the context-to-\noption pathway is blocked. Across layers, atten-\ntion to options increases with depth in QOC but\ndecreases in CQO, suggesting that QOC increas-\ningly relies on the options themselves while CQO\n3\nShort context\nLong context\nDataset\nLogiQA\nSciQ\nRACE-M\nRACE-H\nAvg. Length\n\u223c70\n\u223c70\n\u223c195\n\u223c305\nGap (\u2206)\n6.2%\n7.3%\n24.8%\n20.8%\n(a) Effect of context length.\nPosition\nA\nB\nC\nD\nGap (\u2206)\n22.4%\n20.5%\n19.2%\n9.9%\n(b) Effect of answer position.\nTable 1: Factors influencing causal masking impact.\n(a) Datasets with longer contexts suffer more from\ncausal masking. (b) The correct answer\u2019s position af-\nfects the gap, with option D being the most robust.\nmore effectively integrates contextual information\n(see Figure 6a). Also, context attribution is 0.797\nin CQO but only 0.335 in QOC (see Figure 6b).\nWhy doesn\u2019t the final attention fix it?\nOne may\nargue that the final answer token can still attend\nto the context in QOC. However, causal masking\nprevents the options themselves from being context-\nconditioned: all option hidden states are computed\nbefore any context is seen, so they cannot encode\nevidence\u2013option alignment. Since later tokens can-\nnot retroactively update earlier states (they can only\nread them), the model must do evidence\u2013option\ncomparison only at the final decoding step using\ncontext-blind option representations.\nModulating factors.\nTwo factors modulate the\ngap severity. First, longer contexts show larger\ngaps (Table 1a): more context means more infor-\nmation becomes inaccessible under causal masking.\nSecond, earlier answer positions suffer greater ac-\ncuracy drops than later ones in QOC (Table 1b), as\nlater options are near the context.\n4\nTargeted interventions\nWe design targeted interventions that directly ma-\nnipulate the option and context attention pathway\n(see Table 2). We apply each intervention to all 21\ndecoder-only models across four datasets.\nAttention pruning (degrading CQO).\nTo sim-\nulate the constraint imposed by causal masking in\nQOC, we block option-to-context attention in CQO.\nFor each sample, we identify token spans for the\ncontext and the options, and set mask[i, j] = \u2212\u221e\nfor all pairs where i \u2208Options and j \u2208Context.\nThis prevents option tokens from attending to con-\ntext tokens while leaving all other attention un-\nMethods\nLogiQA\nSciQ\nRACE-M\nRACE-H\nAvg\nCQO\n39.08\n94.16\n74.32\n69.48\n69.26\n+ Attention pruning\n30.47\n60.94\n39.52\n38.89\n42.46 (-26.8)\nQOC\n32.94\n86.89\n49.57\n48.76\n54.54\n+ Activation patching\n35.18\n87.64\n62.22\n56.94\n60.49 (+6.0)\n+ QOCO (Repeat)\n35.62\n91.80\n63.55\n59.65\n62.76 (+8.2)\nTable 2: Accuracy results on MCQA benchmarks.\nPerformances are the average of decoder-only models.\nchanged. CQO accuracy drops from 69.26% to\n42.46% (\u221226.8), with consistent decreases across\nmodel families (Qwen: 28.1%, LLaMA: 25.3%,\nGemma: 26.9%), indicating that option access to\ncontextual evidence is critical.\nActivation patching (improving QOC).\nWe re-\nstore context-aware option representations in QOC\nby patching option hidden states with those com-\nputed under CQO. For each sample, we run both\ntemplates and align corresponding option token\npositions by exact string matching. At layers in\nthe middle-to-late half of the network (e.g., layers\n12\u201323 for 24-layer models, normalized by depth),\nwe replace hQOC\nopt\nwith hCQO\nopt . We patch only op-\ntion tokens (not context or question) to isolate the\nmechanism; patched states come from a different\ntemplate on the same sample, and token alignment\nis verified by exact string match. This increases\nQOC accuracy by 6.0 points on average, with larger\ngains for models exhibiting larger baseline gaps.\nOption repetition (improving QOC).\nAs a sim-\nple prompt, we repeat the options after the context\n(QOCO). The repeated option tokens can attend\nto the context under the causal mask, requiring no\ninternal model intervention. Repeating options im-\nproves QOC by 8.2 points, partially closing the gap\nwithout modifying model internals.\n5\nConclusion\nWe investigated why decoder-only LLMs exhibit\na huge performance gap between CQO and QOC\norderings in MCQA tasks. After ruling out training\ndistribution bias and memory decay, we identified\ncausal attention as the core mechanism: in QOC,\nthe causal mask prevents options from attending to\ncontext, rendering it effectively inaccessible. This\nfinding is supported by architecture comparisons\n(encoder models show no gap), attention analysis,\nand targeted interventions that successfully degrade\nCQO or improve QOC performance. Our work\nprovides mechanistic insight into prompt sensitivity\nand offers practical guidance.\n4\nLimitations\nWhile our study provides mechanistic insights into\nprompt order sensitivity, our evaluations are pri-\nmarily focused on MCQA tasks with four-option\nformats. However, considering that the identified\nroot cause\u2014causal attention masking\u2014is funda-\nmental to decoder-only architectures, we believe\nour findings regarding the information bottleneck\nare likely applicable to broader QA formats where\ncontext placement varies. Additionally, although\nwe evaluate models up to 9B parameters due to\ncomputational constraints, the consistent perfor-\nmance gap observed across 21 diverse models sug-\ngests that this behavior is intrinsic to the causal\ndecoder design rather than a scale-dependent ar-\ntifact. Our work establishes a rigorous empirical\nfoundation for understanding prompt sensitivity.\nReferences\nSamira Abnar and Willem Zuidema. 2020. Quantify-\ning attention flow in transformers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4190\u20134197.\nShengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng,\nJian-Guang Lou, and Weizhu Chen. 2024. Make\nyour llm fully utilize the context. Advances in Neural\nInformation Processing Systems, 37:62160\u201362188.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, and\n1 others. 2024. Scaling instruction-finetuned lan-\nguage models. Journal of Machine Learning Re-\nsearch, 25(70):1\u201353.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does bert look\nat? an analysis of bert\u2019s attention. In Proceedings\nof the 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n276\u2013286.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics (NAACL), pages 4171\u2013\n4186.\nShuoyang Ding and Philipp Koehn. 2021. Evaluating\nsaliency methods for neural language models. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5034\u20135052.\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christo-\npher Potts. 2021. Causal abstractions of neural net-\nworks. Advances in Neural Information Processing\nSystems, 34:9574\u20139586.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nand 1 others. 2024. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances in\nNeural Information Processing Systems, volume 35,\npages 22199\u201322213.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale read-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 785\u2013794.\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan\nZhang. 2024. Loogle: Can long-context language\nmodels understand long contexts? In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 16304\u201316333.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang,\nYile Wang, and Yue Zhang. 2020. Logiqa: A chal-\nlenge dataset for machine reading comprehension\nwith logical reasoning. In Proceedings of the Twenty-\nNinth International Joint Conference on Artificial\nIntelligence (IJCAI), pages 3622\u20133628.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024. Lost in the middle: How language mod-\nels use long contexts. Transactions of the Association\nfor Computational Linguistics, 12:157\u2013173.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086\u20138098.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associ-\nations in GPT. In Advances in Neural Information\nProcessing Systems, volume 35, pages 17359\u201317372.\nPouya Pezeshkpour and Estevam Hruschka. 2024.\nLarge language models sensitivity to the order of op-\ntions in multiple-choice questions. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2024, pages 2006\u20132017, Mexico City, Mexico. Asso-\nciation for Computational Linguistics.\nAntonin Poch\u00e9, Alon Jacovi, Agustin Martin Picard,\nVictor Boutin, and Fanny Jourdan. 2025. ConSim:\nMeasuring concept-based explanations\u2019 effectiveness\nwith automated simulatability. In Proceedings of the\n5\n63rd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5594\u20135615, Vienna, Austria. Association for Compu-\ntational Linguistics.\nMelanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane\nSuhr. 2024. Quantifying language models\u2019 sensitiv-\nity to spurious features in prompt design or: How i\nlearned to start worrying about prompt formatting.\nIn The Twelfth International Conference on Learning\nRepresentations.\nSagi Shaier, Lawrence Hunter, and Katharina Wense.\n2024. It is not about what you say, it is about how you\nsay it: A surprisingly simple approach for improving\nreading comprehension. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2024, pages\n8292\u20138305.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kun-\ndaje. 2017.\nLearning important features through\npropagating activation differences. In International\nconference on machine learning, pages 3145\u20133153.\nPMlR.\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, L\u00e9onard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ram\u00e9, and 1 others. 2024.\nGemma 2: Improving open language models at a\npractical size. arXiv preprint arXiv:2408.00118.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stuart\nShieber. 2020. Investigating gender bias in language\nmodels using causal mediation analysis. Advances\nin neural information processing systems, 33:12388\u2013\n12401.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\nand 1 others. 2022. Chain-of-thought prompting elic-\nits reasoning in large language models. Advances\nin neural information processing systems, 35:24824\u2013\n24837.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\nIn Proceedings of the 3rd Workshop on Noisy User-\ngenerated Text, pages 94\u2013106.\nSarah Wiegreffe, Oyvind Tafjord, Yonatan Belinkov,\nHannaneh Hajishirzi, and Ashish Sabharwal. 2025.\nAnswer, assemble, ace: Understanding how LMs an-\nswer multiple choice questions. In The Thirteenth\nInternational Conference on Learning Representa-\ntions.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, and 1 others. 2024. Qwen2.\n5 technical report. arXiv preprint arXiv:2412.15115.\nChujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and\nMinlie Huang. 2024. Large language models are not\nrobust multiple choice selectors. In The Twelfth Inter-\nnational Conference on Learning Representations.\n6\nA\nRelated work\nPrompt sensitivity.\nA growing body of work has\nestablished that LLMs can be sensitive to prompt\ndesign choices, including input order and format-\nting. In the in-context learning (ICL) setting, Lu\net al. (2022) show that reordering the same set of\ndemonstrations can substantially change accuracy\nand propose strategies to mitigate order sensitivity,\nbut do not explore the underlying internal mecha-\nnism. In MCQA, prior work reports large variation\nunder option permutations and develops mitigation\nmethods (e.g., Pezeshkpour and Hruschka (2024);\nZheng et al. (2024)), yet largely remains at the\nlevel of behavioral characterization. Prompt for-\nmatting studies further show that semantically pre-\nserving changes\u2014punctuation, spacing, labeling,\nor layout\u2014can induce sizable performance swings\n(e.g., Sclar et al. (2024)). More mechanistic MCQA\nanalyses, such as Wiegreffe et al. (2025), use activa-\ntion patching-style interventions to localize where\nanswer selection is determined and how it is am-\nplified across layers. However, these lines of work\ndo not directly address a block-order question that\narises with long contexts: whether and how the\naccessibility of context information to option rep-\nresentations changes with prompt order. Relatedly,\nShaier et al. (2024) reports large order effects in\nreading comprehension, but stops short of provid-\ning a structural account of the failure mode. In\ncontrast, we study a striking order effect in MCQA\nbetween CQO and QOC and identify an architec-\ntural mechanism: under causal attention, option\ntokens in QOC are structurally prevented from in-\ntegrating information from the subsequent context.\nWe then test this mechanism with targeted causal\ninterventions that manipulate the option\u2013context\ninformation pathway.\nMechanistic analysis tools.\nTo connect prompt-\norder sensitivity to internal computation, we draw\non a set of mechanistic interpretability tools. First,\nwe use attention analysis to characterize how mod-\nels route information under different prompt or-\nders; prior work has shown the utility of inter-\npreting attention weights as explanations (Clark\net al., 2019; Abnar and Zuidema, 2020). We com-\npute attention statistics for each prompt order, use\nthem as descriptive diagnostics, and also perform\ngradient-based analyses.\nWe employ gradient-\nbased input attribution as an auxiliary diagnostic:\nGradient\u00d7Input and related attribution methods\nquantify how sensitive the prediction is to perturba-\ntions of each input token representation, and have\nbeen widely used as simple, interpretable feature-\nimportance baselines (Shrikumar et al., 2017; Ding\nand Koehn, 2021; Poch\u00e9 et al., 2025). We also use\ncausal interventions on internal pathways. In par-\nticular, we (i) ablate the option-to-context attention\npathway by masking attention edges between token\ngroups (a targeted path/edge ablation), and (ii) ap-\nply activation patching (causal tracing/interchange\ninterventions), which swaps hidden states from a\n\u201cclean\u201d run into a \u201ccorrupted\u201d run to test whether a\nspecific representation is causally responsible for\nthe downstream behavior (Vig et al., 2020; Geiger\net al., 2021; Meng et al., 2022). We adapt these\ntools to the prompt-order setting by isolating the\noption\u2013context information pathway: attention ab-\nlation tests necessity, and activation patching tests\nsufficiency of context-conditioned option represen-\ntations for closing the CQO-QOC gap.\nLong-context failure modes.\nPrior work shows\nthat LLMs can underutilize long contexts and\nare sensitive to where relevant evidence appears\n(e.g., lost-in-the-middle; Liu et al. (2024); Li et al.\n(2024); An et al. (2024)). In our setting, we use\nlong contexts to test a specific hypothesis: QOC\nmay underperform because the model forgets the\noptions after reading a long context. Our option-\nrecall results reject this explanation\u2014QOC retains\noptions at least as well as CQO\u2014suggesting that\nthe CQO\u2013QOC gap is not primarily a long-context\nmemory failure, but a distinct prompt-order effect.\nB\nDetailed information\nTo demonstrate the robustness of our findings,\nwe conduct experiments with various models and\ndatasets. Details are below.\nB.1\nModel information\nDetailed information on the model is in Table 3,\nwhich contains the Hugging Face ID of each model.\nB.2\nDataset information\nWe infer using test sets for all benchmarks.\nLogiQA\nis a logical reasoning dataset sourced\nfrom the Chinese Civil Service Examination. It con-\ntains 651 test samples, each requiring multi-step\nlogical inference over a given context. Questions\ncover various reasoning types, including categori-\ncal reasoning, conditional reasoning, and disjunc-\ntive reasoning.\n7\nSciQ\nis a science question answering dataset con-\ntaining 1,000 crowdsourced multiple-choice ques-\ntions spanning physics, chemistry, and biology.\nEach question is paired with a short supporting\npassage (averaging \u223c80 words) that provides the\nnecessary context to answer correctly.\nRACE (Reading comprehension from examina-\ntions)\nconsists of English reading comprehension\nquestions collected from Chinese middle and high\nschool exams. We utilize both subsets:\n\u2022 RACE-M: Middle school level with 1,436 test\nsamples and simpler passages (\u223c250 words on\naverage)\n\u2022 RACE-H: High school level with 3,498 test sam-\nples and more complex texts (\u223c350 words on\naverage)\nQuestions span various types, including detail re-\ntrieval, inference, vocabulary, and main idea identi-\nfication.\nB.3\nPrompting templates and evaluation\nprotocols\nWe employ distinct prompting strategies and\nevaluation protocols tailored to each model\narchitecture\u2014Decoder-only\n(Causal\nLM),\nEncoder-only\n(Masked\nLM),\nand\nEncoder-\nDecoder (Seq2Seq)\u2014to ensure fair and optimal\nperformance assessment.\nB.3.1\nPrompting templates\nWe evaluate two primary template orderings to test\ncontext robustness: CQO (Context \u2192Question\n\u2192Options) and QOC (Question \u2192Options \u2192\nContext).\nDecoder-only models\nFor standard causal lan-\nguage models (e.g., Llama-3, Qwen, Gemma, Mis-\ntral), we use an open-ended completion format that\nguides the model to predict the next token as the\nanswer label.\n\u2022 CQO template\nPrompt:\nContext: {Context}\nQuestion: {Question}\nOptions:\nA: {Option A}\nB: {Option B}\nC: {Option C}\nD: {Option D}\nAmong A to D, the answer is:\n\u2022 QOC template\nPrompt:\nQuestion: {Question}\nOptions:\nA: {Option A}\nB: {Option B}\nC: {Option C}\nD: {Option D}\nContext: {Context}\nAmong A to D, the answer is:\nEncoder-only models\nFor masked language\nmodels (e.g., BERT, RoBERTa), we utilize a Cloze-\nstyle task where the model must predict the token\nat the [MASK] position.\n\u2022 CQO template\nPrompt:\nContext: {Context}\nQuestion: {Question}\nOptions:\nA. {Option A}\nB. {Option B}\nC. {Option C}\nD. {Option D}\nAmong A, B, C, D, the answer is\n[MASK].\n\u2022 QOC template\nPrompt:\nQuestion: {Question}\nOptions:\nA. {Option A}\nB. {Option B}\nC. {Option C}\nD. {Option D}\nContext: {Context}\nAmong A, B, C, D, the answer is\n[MASK].\nEncoder-Decoder\nmodels\nFor\nsequence-to-\nsequence models (e.g.,\nFLAN-T5),\nthe full\nprompt is provided to the encoder. The decoder\nis initialized to generate the answer. We use the\n8\n\"Full\" variant where the entire content resides in\nthe encoder.\n\u2022 CQO template\nEncoder Input:\nContext: {Context}\nQuestion: {Question}\nOptions:\nA. {Option A}\nB. {Option B}\nC. {Option C}\nD. {Option D}\nThe answer is\n\u2022 QOC template\nEncoder Input:\nQuestion: {Question}\nOptions:\nA. {Option A}\nB. {Option B}\nC. {Option C}\nD. {Option D}\nContext: {Context}\nThe answer is\nB.3.2\nEvaluation protocols\nDecoder-only models (likelihood scoring). In-\nstead of relying on unconstrained text generation,\nwhich requires complex parsing and may yield in-\nvalid outputs, we evaluate models using a con-\nstrained likelihood approach.\nGiven the input\nprompt, we compute the logits for the next token\nprediction. We extract the logits corresponding\nto the valid option tokens (\u2018A\u2019, \u2018B\u2019, \u2018C\u2019, \u2018D\u2019) and\ntheir token variations. We apply a Softmax opera-\ntion over these four values to obtain a normalized\nprobability distribution:\nP(Correct) =\nelogitanswer\nP\nk\u2208{A,B,C,D} elogitk\nThe model\u2019s prediction is the option with the high-\nest probability. This method allows for a precise\nmeasurement of the model\u2019s preference even when\ndifferences are subtle.\nEncoder-only models (masked prediction). We\nformulate the task as Masked Language Modeling\n(MLM). The model processes the bidirectional con-\ntext and predicts the token at the [MASK] position.\nSimilar to the decoder approach, we extract the\nprobabilities for the tokens corresponding to the\noptions \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019. To handle tokenizer\ndifferences, we select the maximum logit across\ncase variations (e.g., \u2019A\u2019 and \u2019a\u2019) for each option.\nThese maximum logits are then normalized via\nSoftmax to obtain the final probability distribution.\nEncoder-Decoder models. For T5-based models,\nwe employ the same evaluation protocol as decoder-\nonly models.\nB.4\nInference details\nAll experiments are conducted with greedy de-\ncoding single runs, maximum 16 output tokens,\nbfloat16 precision, and NVIDIA A6000 GPUs.\nC\nAdditional result\nThis section provides detailed per-model and per-\ndataset results for all experiments discussed in the\nmain paper.\nC.1\nFull model-dataset results\nTable 4 shows CQO and QOC accuracy for all\n21 decoder-only models across 4 datasets. The\naverage gap is +14.7%.\nC.2\nBase vs instruction-tuned models\nTable 5 compares CQO-QOC gaps between base\nand instruction-tuned model pairs. Both variants\nshow consistent gaps (Base: 14.70%, Instruct:\n14.12%), indicating that instruction tuning does\nnot mitigate order sensitivity.\nC.3\nIn-context learning results\nTable 6 shows QOC performance with 0, 1, 3,\nand 5 in-context examples. ICL provides marginal\nimprovement (+3.1% from 0-shot to 5-shot) but\ncannot close the gap to CQO (69.26%).\nC.4\nOption recall accuracy\nTable 7 shows option recall accuracy by dataset.\nHigh recall rates in both CQO (93.5%) and QOC\n(94.7%) confirm that the QOC performance drop\nis not due to memory failure.\n9\nCQO\nQOC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nContext Attribution\n0.758\n0."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models\"\n\n#### Main Contributions:\nThe paper investigates why large language models (LLMs) exhibit a significant performance gap between two common prompt orderings, Context-Question-Option (CQO) and Question-Option-Context (QOC), in multiple-choice question answering tasks. The authors identify causal attention as the core mechanism underlying this phenomenon.\n\n#### Methodology:\n1. **Dataset Selection**: The study evaluates four reading comprehension benchmarks: LogiQA, RACE-H/M, SciQ.\n2. **Model Evaluation**: 21 decoder-only LLMs from different model families (LLaMA3, Qwen, Gemma) are tested across these datasets.\n3. **Prompt Order Analysis**: Two distinct prompt orderings\u2014CQO and QOC\u2014are examined for their impact on model performance.\n\n#### Key Findings:\n- **Performance Gap**: Across multiple models and datasets, CQO consistently outperforms QOC by over 14% in accuracy (Figure 1).\n- **Hypothesis Testing**:\n  - **Bias in Training Data**: The authors rule out this hypothesis as a primary cause of the performance gap.\n  - **Option Recall Failure**: They find that option recall is not the main reason for the QOC-QCO gap. Instead, they identify causal attention as the mechanism at play.\n  - **Context Accessibility**:\n    - Causal Masking in QOC prevents options from attending to context tokens, creating an information bottleneck (Figure 2).\n    - Longer contexts and earlier answer positions exacerbate this effect.\n\n#### Conclusions:\n- The study identifies causal attention as the primary factor driving the performance gap between CQO and QOC. This mechanism explains why models perform better when context is placed before options.\n- The authors design targeted interventions to manipulate the option-context information pathway, demonstrating that causal masking is a key factor in this phenomenon.\n\n### Detailed Summary:\n\n#### Introduction:\nThe paper highlights how LLMs are sensitive to prompt structure, with CQO outperforming QOC by over 14% across multiple datasets and models. The authors aim to understand why this happens through detailed analysis of the underlying mechanisms.\n\n#### Methodology:\n- **Datasets**: LogiQA (logical reasoning), RACE-H/M (reading comprehension from high school exams), SciQ (science questions).\n- **Models**: A wide range of decoder-only LLMs, including LLaMA3, Qwen, Gemma.\n- **Prompt Order Analysis**: Evaluates CQO vs. QOC on different datasets and models.\n\n#### Key Findings:\n1. **Performance Gap**:\n   - Across all benchmarks, CQO consistently outperforms QOC by over 14% in accuracy (Figure 1).\n2. **Hypothesis Testing**:\n   - **Bias Hypothesis**: The authors test whether the training data distribution favors CQO over QOC. They find no significant difference between base and instruction-tuned models, ruling out this hypothesis.\n   - **Option Recall Failure Hypothesis**: The study finds that option recall is not a primary cause of the performance gap. Instead, they attribute it to causal attention.\n3. **Causal Attention Mechanism**:\n   - **Context Accessibility Issue**: In QOC prompts, causal masking prevents options from attending to context tokens, creating an information bottleneck (Figure 2). This mechanism explains why CQO outperforms QOC.\n\n#### Conclusions:\n- The study concludes that causal attention is the core mechanism driving the performance gap between CQO and QOC. By manipulating the option-context pathway, they demonstrate how this can be mitigated or enhanced.\n- The authors provide practical interventions to improve model performance in specific prompt orderings, reinforcing their mechanistic understanding of the phenomenon.\n\n### References:\nThe paper references several related works that have explored LLMs' sensitivity to prompt design choices and internal mechanisms."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "1\nA universal linearized subspace refinement framework\nfor neural networks\nWenbo Caoa,b,c, Weiwei Zhanga,b,c,*\na School of Aeronautics, Northwestern Polytechnical University, Xi\u2019an 710072, China\nb International Joint Institute of Artificial Intelligence on Fluid Mechanics, Northwestern Polytechnical\nUniversity, Xi\u2019an, 710072, China\ncNational Key Laboratory of Aircraft Configuration Design, Xi\u2019an 710072, China\n* Correspondence to: aeroelastic@nwpu.edu.cn\nAbstract. Neural networks are typically trained using gradient-based methods, yet in many\napplications their final predictions remain far from the best accuracy attainable within the model\u2019s\nexpressive capacity. Here we introduce Linearized Subspace Refinement (LSR), a general and\narchitecture-agnostic framework that exploits the Jacobian-induced linear residual model at a fixed\ntrained network state. By solving the associated residual problem through a reduced direct least-\nsquares formulation, LSR obtains a subspace-optimal solution of the linearized residual model,\nyielding a refined linear predictor with markedly improved accuracy over standard gradient-trained\nsolutions\u2014without modifying network architectures, loss formulations, or training procedures. Across\na range of learning paradigms\u2014including function approximation, data-driven operator learning, and\nphysics-informed operator fine-tuning\u2014we show that the accuracy achieved by gradient-based\ntraining often remains far from the attainable accuracy, even when local linearization yields a convex\nproblem, suggesting that loss-induced ill-conditioning can constitute a dominant practical bottleneck,\nbeyond issues of model expressivity or nonconvexity. In contrast, one-shot LSR consistently exposes\naccuracy levels that are not exploited by standard gradient-based training, often yielding order-of-\nmagnitude error reductions. For operator-constrained problems with composite loss structures, we\nfurther introduce Iterative LSR, which alternates one-shot LSR with supervised nonlinear alignment,\nconverting ill-conditioned residual minimization into numerically benign fitting steps and yielding\nsubstantially accelerated convergence and improved accuracy. By bridging nonlinear neural\nrepresentations with reduced-order linear solvers at fixed linearization points, LSR provides a\nnumerically grounded, broadly applicable refinement framework for modern neural networks in\nsupervised learning, operator learning, and scientific computing.\nKeywords. Linearized subspace refinement; neural network refinement; Jacobian-based methods;\nreduced-order solvers; scientific machine learning.\n1 Introduction\nNeural networks, owing to their expressive power and flexibility [1], are widely used as function\napproximators in supervised regression, operator learning, and physics-informed scientific computing\n2\n[2-5]. In these applications, the objective is often not merely to obtain qualitatively correct predictions,\nbut to approximate continuous solutions or operators with high numerical accuracy. For example, in\noperator modeling and partial differential equation (PDE) solving, the achievable error magnitude\ndirectly determines whether a learned model is suitable for scientific analysis or engineering\ndeployment [2,3]. Consequently, understanding how to systematically extract the highest attainable\nnumerical accuracy from neural networks under fixed architectures and standard training procedures\nhas become a central challenge in scientific machine learning.\nFrom the perspective of classical numerical analysis and linear algebra, ill-conditioning is widely\nrecognized as a primary factor limiting attainable accuracy. Even when a problem is linear or convex\nin theory, severe ill-conditioning can prevent iterative methods from reaching high-accuracy solutions\nwithin a practical computational budget. In contrast, when memory and computational cost permit,\ndirect factorization-based solvers can reliably attain much higher accuracy by bypassing the slow error\ndecay typical to iterative schemes [6,7]. This fundamental distinction underlies long-standing trade-\noffs between accuracy, stability, and efficiency in numerical computation, and provides a useful lens\nfor re-examining accuracy barriers encountered in neural network models [8].\nDespite these insights, modern neural network training remains overwhelmingly dominated by\ngradient-based iterative optimization [9,10]. In this paradigm, ill-conditioning manifests as slow\nconvergence, training instability, and strong sensitivity to hyperparameter choices, motivating a wide\nrange of increasingly sophisticated optimizers and acceleration strategies, including adaptive learning\nrates, momentum, quasi-second-order updates, and various forms of preconditioning [11-16]. In\nparallel, substantial effort has been devoted to improving accuracy in scientific machine learning\nthrough architectural inductive biases and problem-specific design choices, encompassing network\narchitectures, sampling strategies, loss reweighting, regularization, curriculum learning, and multi-\nstage training [2,3,17-23]. While diverse in form, these approaches share a common feature: accuracy\nimprovements are pursued almost exclusively through continued gradient-based updates of the\nparameter vector \uf071.\nMotivated by these considerations, we formulate a problem setting that is distinct from, yet\ncomplementary to, the conventional training paradigm of neural networks. Rather than asking how to\nfurther optimize the parameter vector \uf071  through continued gradient-based training, we ask\nwhether\u2014after training has converged and the parameters are fixed at\n0\n\uf071\u2014there still exist directions\nwithin the locally linearized model induced by the network Jacobian along which the loss can be\nsubstantially reduced. More fundamentally, we ask whether direct solvers can be leveraged to bypass\nthe conditioning barriers that limit the attainable accuracy of iterative optimization. Beyond enabling\nthis alternative mode of refinement, this perspective also provides a concrete way to interrogate\n3\nattainable accuracy in nonlinear neural networks. For large-scale models, it is generally infeasible to\ncertify whether a trained network has reached a local optimum of the original nonconvex objective,\nand apparent convergence or small gradients do not constitute reliable optimality guarantees. By\ncontrast, the linearized residual model at a fixed trained state yields a convex least-squares problem\nwhose attainable accuracy\u2014within a Jacobian-induced subspace\u2014can be directly approximated using\nrobust direct solvers. This linearized attainable accuracy therefore serves as an operational and\nfalsifiable probe of how much locally accessible improvement remains unused by standard training,\nand offers a principled indirect lens for diagnosing accuracy plateaus in nonlinear optimization.\nA key observation underlying this perspective is that many seemingly disparate learning tasks\nshare a common objective structure. Supervised regression and classification, data-driven operator\nlearning, and physics-constrained PDE solving can all be formulated as minimizing a residual norm,\n2\n2\n1\nmin\n( )\n2\n\uf071\nf\n,\nwhere\n( )\n\uf071\nf\ndenotes the residual vector formed by model predictions evaluated at data samples,\nobservation points, or physical constraints. The primary differences among these problems lie in the\ncomposition and interpretation of the residuals, rather than in the residual-minimization form itself.\nThis structural universality provides a natural entry point for introducing a unified refinement\nframework grounded in classical direct solvers.\nLinearizing the residual vector around a trained model leads to a classical least-squares problem\ninvolving the network Jacobian. However, the Jacobian is typically extremely high-dimensional and\ndense, rendering direct solves over the full parameter space computationally infeasible. To address this\nchallenge, we introduce the Linearized Subspace Refinement (LSR) framework. LSR constructs a\ncompact correction subspace using highly parallel Jacobian\u2013vector products and computes a subspace-\noptimal refinement by directly solving a reduced linear residual problem within this subspace. In its\none-shot form, LSR operates as a post-training procedure: it does not modify network architectures,\nloss formulations, or the original training process, yet it can expose substantial additional accuracy that\nis not exploited by gradient-based training alone under standard training procedures. More broadly,\nthis linearized subspace perspective provides a foundation for both post-training refinement and\niterative refinement strategies, which we demonstrate in the following sections across a range of\nlearning paradigms.\n4\n2 Results\nMethod overview\nFigure 1 | Overview of the Linearized Subspace Refinement (LSR) framework. a, Core mechanism. Starting\nfrom a trained network at parameters \u03b80, the residual vector is locally linearized, inducing a convex least-squares\nproblem with respect to \uf044\uf071. By restricting \uf044\uf071 to a low-dimensional subspace, an explicit and computationally\ntractable direct linear solve yields the optimal correction within this subspace. b, One-shot LSR. For function\napproximation, supervised learning, and data-driven operator learning, a single linearized subspace solve typically\nsuffices to substantially reduce error beyond standard training. c, Iterative LSR. For operator-constrained problems\nwith complex composite losses, LSR is alternated with nonlinear alignment, transforming a difficult nonlinear\noptimization into a sequence of more tractable supervised fitting steps anchored by one-shot LSR solutions, thereby\nimproving both convergence behavior and attainable accuracy. Note: LSR computes \uf044\uf071 using Jacobian\u2013vector\nactions evaluated on the training set. Once \uf044\uf071 is obtained, the refined predictor can be evaluated at any test input\nx via the corresponding pointwise Jacobian action\n0 ( )\nJ\nx\n\uf071\n\uf044\uf071, yielding\n0\n0\n( )\n( )\n( )\nLSR\nq\nx\nq\nx\nJ\nx\n\uf071\n\uf071\n=\n+\n\uf044\uf071. For brevity, we\nsubsequently denote both cases by J, with the understanding that the Jacobian is always evaluated at the relevant\ninputs.\nThe core mechanism of LSR and its two modes of use are illustrated in Figure 1. Starting from a\ntrained parameter vector\n0\n\uf071, the residual vector is linearized with respect to the parameter perturbation\n\uf044\uf071, which induces a linear least-squares problem and, consequently, a convex loss landscape. To\nmake direct solvers computationally tractable, LSR restricts \uf044\uf071 to a low-dimensional subspace,\nyielding the optimal solution of the reduced linearized problem within this subspace at a manageable\ncomputational cost. Importantly, at this stage, the LSR correction is not applied as a nonlinear\nparameter update. Instead, LSR evaluates the linearized solution at the same linearization point to\ndefine a refined predictor, thereby improving accuracy without assuming global validity of the linear\napproximation.\n5\nFor supervised learning and data-driven operator learning tasks dominated by data-misfit losses,\na single one-shot LSR is typically sufficient to expose additional attainable accuracy beyond that\nreached by conventional gradient-based training. In such settings, further nonlinear alignment of the\nLSR solution remains essentially a standard data-fitting problem and is generally no easier than fitting\nthe original target, so iterative refinement offers limited additional benefit. By contrast, for operator-\nconstrained problems governed by composite objectives\u2014such as PDE solving\u2014the optimization\nlandscape is often highly nonlinear and severely ill-conditioned. In this regime, iterative LSR becomes\neffective by alternating linearized refinement with nonlinear alignment, thereby reformulating a\ndifficult residual minimization problem into a sequence of more numerically benign supervised fitting\nsteps anchored at successive one-shot LSR solutions. Rather than serving as a post-training correction,\niterative LSR functions as a restructuring of the training process that mitigates the conditioning issues\ninduced by nonlinear operators and physical constraints.\nIf\n0\n\uf071 is a first-order stationary point of the residual minimization problem of the full training set,\nthe linearized model admits no descent direction, and consequently one-shot LSR yields a zero\ncorrection. This behavior is expected and reflects the fundamental limitation of first-order information\nat stationarity. In practice, however, gradient-based training rarely reaches such stationary points\nwithin realistic computational budgets, even for convex problems induced by local linearization, a\nphenomenon repeatedly observed in our numerical results. As a result, nontrivial correctable\ncomponents typically remain in the Jacobian-induced correction space, allowing LSR to consistently\nimprove upon trained models and, in some cases, to reach accuracy levels that are inaccessible to the\noriginal nonlinear parametrization. A more detailed discussion of stationarity, zero-update behavior,\nand the role of linearized correction spaces is provided in Section S4 of the Supplementary Information.\nIn the following sections, we demonstrate how this unified framework enables consistent, often\norder-of-magnitude accuracy improvements across a range of learning paradigms, and how direct\nsolver\u2013based refinement exposes accuracy limits that are not reached by gradient-based optimization\nunder standard training procedures. All experiments reported in this paper are conducted on a single\nNVIDIA A100 GPU with 40 GB of memory.\nExample 1 | Function approximation\nWe begin with a standard two-dimensional function approximation problem to access the\nattainable accuracy of trained neural networks and to examine the behavior of LSR in the simplest\nsupervised learning setting. The target function is\ns\n( , )\ns\n,\nin(\n) in(\n)\nq\nx\ny\nx y\n\uf070\n\uf070\n=\ndefined on the square\ndomain [\u22121,1]\u00d7[\u22121,1], and is approximated using a conventional feed-forward neural network.\nNetwork parameters are trained using the Adam optimizer with mini-batches until the test mean\nsquared error (MSE) reaches a clear plateau. To isolate the numerical limits of gradient-based training,\nthis experiment is conducted in an idealized supervised setting with dense, noise-free data and\n6\nsufficient model capacity, so that representation error and data noise do not constitute the dominant\nsources of error. Full details of the network architecture and training procedure are provided in Section\nS2.1 of the Supplementary Information.\nFigure 2 | Accuracy enhancement and characteristic behavior of LSR in a two-dimensional function\napproximation problem. a, Spatial distribution of the network prediction error at different training stages, together\nwith the prediction error after applying one-shot LSR at the same parameter states. b, Evolution of the MSE during\ntraining. Curves labeled \u201cLSR\u201d indicate the error levels attained by applying one-shot LSR at the corresponding\ntraining epochs. c, Error convergence for the convex least-squares problem obtained from local linearization of the\nresidual, solved using different optimization or iterative methods.\nFigure 2a shows the spatial distribution of the prediction error at different training epochs (0, 10\nand 200), together with the error obtained by applying one-shot LSR at the same parameter states. As\ntraining progresses, gradient-based optimization progressively suppresses low-frequency error\ncomponents, while the remaining discrepancy becomes increasingly dominated by high-frequency\noscillations, consistent with the well-known spectral bias of neural network training [24,25]. In\ncontrast, one-shot LSR markedly reduces the error magnitude, leaving predominantly fine-scale\noscillatory structures. This comparison indicates that, for this simple supervised task, standard training\neffectively captures the dominant low-frequency content of the target function, whereas the additional\naccuracy exposed by LSR primarily arises from correcting localized high-frequency errors.\nThe evolution of the training and test MSE during optimization is shown in Figure 2b. The curves\nlabeled \u201cLSR\u201d report the MSE level obtained by applying one-shot LSR to the network parameters at\nthe corresponding training epoch. Although gradient-based training rapidly reduces the error at early\nstages, progress slows substantially once a plateau is reached. By contrast, one-shot LSR consistently\nproduces order-of-magnitude MSE reductions throughout training. Notably, even when applied to an\n7\nuntrained network with random initialization, one-shot LSR attains a prediction accuracy substantially\nexceeding the eventual plateau reached by gradient-based training. When applied in the plateau regime,\none-shot LSR further reduces the test MSE from approximately 10-6 to nearly 10-12. In this setting, the\nwall-clock cost of one-shot LSR is negligible relative to training, requiring less than one second in our\nimplementation.\nTo further elucidate the origin of this accuracy limitation, Figure 2c examines the least-squares\nproblem obtained by locally linearizing the residual at the trained network state. In this experiment, no\nsubspace reduction is performed; instead, the full-space linearized least-squares problem is solved\nusing different iterative approaches, including first-order Adam, quasi-second-order Limited-memory\nBroyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (L-BFGS), as well as classical Krylov solvers for least-squares\nproblems, namely Conjugate Gradient for Least Squares (CGLS) and LSQR. It is important to note\nthat the LSR solution, obtained by solving a reduced least-squares problem within a low-dimensional\nJacobian-induced subspace, yields a conservative estimate of the achievable error reduction, since the\nreduced subspace is a strict subset of the full parameter space. Despite the fact that the objective is\nconvex, the accuracy achieved by these iterative methods within a practical iteration budget remains\nsignificantly lower than that achieved by the direct solve of the reduced problem. Among the iterative\napproaches, L-BFGS performs noticeably better than first-order Adam, reflecting partial access to\ncurvature information, while CGLS and LSQR achieve accuracy comparable to L-BFGS. Additional\nresults under different optimizer hyperparameter settings are provided in Section S2.1 of the\nSupplementary Information.\nTaken together, these results highlight a critical yet often underappreciated point: even in the\nabsence of nonconvexity, numerical ill-conditioning alone can constitute a decisive bottleneck for\nattainable accuracy. Since nonlinear optimization is locally well approximated by a convex problem\nnear a trained solution, this observation further suggests that similar accuracy limitations can persist\nin nonlinear models, independent of nonconvex effects. This behavior will be further examined and\nsubstantiated in more complex problems presented in the subsequent sections.\nIt is worth emphasizing that the LSR correction is not applied as a nonlinear parameter update.\nAlthough the linearized least-squares problem admits a solution with substantially lower residual,\napplying the corresponding correction direction to the nonlinear model does not yield comparable\nimprovements, as illustrated in Figure S9. Instead, one-shot LSR uses the linearized solution to define\na refined predictor evaluated at the same linearization point.\nExample 2 | Data-driven operator learning\nWe next examine the behavior of LSR in a data-driven operator learning setting based on the one-\ndimensional Burgers equation. The task is to learn the operator mapping from the initial condition\n( ,\n0)\nq x t =\nto the solution at a later time\n( ,\n1)\nq x t =\n, where the initial conditions are sampled from a\n8\nGaussian random field. Two representative neural operator architectures, DeepONet [2] and\nMultiONet [26], are considered, together with three different activation functions (ReLU, Tanh, and a\ncustom Tanh\u2013Sin activation), in order to assess the robustness of LSR across network architectures\nand activation-induced nonlinear representations. All models are trained using standard supervised\nlearning to obtain baseline solutions, after which one-shot LSR is applied as a post-training refinement.\nFigure 3a reports aggregate statistics over six operator-learning configurations, combining\ndifferent architectures and activation functions. Unless otherwise specified, all reported test errors are\nmeasured using the relative L2 norm. Across all settings, LSR consistently reduces the test error\ncompared with the baseline predictions, with the paired distributions revealing systematic accuracy\ngains across individual test instances rather than isolated improvements. Importantly, instances with\nlower baseline errors tend to exhibit smaller post-LSR errors, indicating that LSR amplifies\u2014rather\nthan replaces\u2014the benefits of effective nonlinear training. These results demonstrate that the benefits\nof LSR are robust across architectures and activations, and are not restricted to a small subset of\nfavorable cases.\nFigure 3 | Accuracy gains from linearized subspace refinement in data-driven operator learning. a, Aggregate\nerror statistics over six data-driven operator-learning configurations. Each point corresponds to one of 50 test\ninstances, with paired distributions comparing baseline predictions and their LSR counterparts. Reported numbers\nindicate the median error reduction factors. b, Rank-dependent training loss and test error for representative trained\nDeepONet and MultiONet models. Increasing the subspace dimension allows LSR to progressively expose lower\nattainable error levels beyond those reached by gradient-based training. c, Corresponding rank-dependent behavior\nfor randomly initialized networks, included as a reference to demonstrate that the Jacobian-induced linear subspace\nalready possesses substantial representational capacity even prior to training.\nFigures 3b and 3c further elucidate how these gains depend on the dimension (rank) of the\nJacobian-induced linearized subspace. For representative trained DeepONet and MultiONet models\n9\n(Figure 3b), increasing the subspace dimension leads to a pronounced and sustained reduction in both\ntraining loss and test error. This monotonic trend over a wide range of ranks indicates that the accuracy\nimprovements provided by LSR do not arise from correcting only a few dominant parameter directions.\nInstead, they reflect the progressive exploitation of additional correction directions admitted by the\nlinearized model as the subspace expands. The corresponding results for randomly initialized networks\n(Figure 3c) reveal that, even prior to training, the Jacobian-induced linear subspace already possesses\nsubstantial representational capacity. As the rank increases, LSR is able to systematically reduce the\nerror, albeit to a higher attainable error floor than in the trained case. This comparison highlights the\nrole of training in reshaping the structure of the linearized subspace, thereby enabling LSR to expose\nsignificantly lower accuracy limits, rather than merely providing an optimization pathway.\nTo further examine the generality of this observation beyond function approximation and\noperator-learning settings, we additionally tested LSR on the MNIST benchmark [27] using randomly\ninitialized convolutional neural networks. Even without any prior training, applying LSR increases the\ntest accuracy from approximately 0.10 to above 0.96, providing further evidence that the Jacobian-\ninduced linear subspace can already exhibit strong representational capacity at initialization. This\nresult corroborates the consistency of the phenomenon observed in Figure 3c across architectures and\nat a larger problem scale, with further details provided in Section S2.5 of the Supplementary\nInformation.\nAt sufficiently large ranks, the monotonic improvement with increasing subspace dimension may\neventually deteriorate, with loss and error exhibiting abrupt increases. This behavior reflects the onset\nof severe numerical ill-conditioning in the reduced linearized least-squares problem as the subspace\ngrows, amplifying sensitivity to noise and finite-precision effects. Consequently, enlarging the\nsubspace introduces an inherent trade-off between increased corrective degrees of freedom and\nnumerical stability, leading to an optimal finite rank for one-shot LSR. Importantly, this instability is\ndirectly manifested in the loss behavior itself, providing a practical criterion for rank selection as the\nlargest dimension for which the loss decreases monotonically. In addition to numerical conditioning,\nthe memory cost associated with storing rank-dependent subspace representations provides a practical\nconstraint on the maximum rank used in LSR. Further details are provided in Section S2.2 of the\nSupplementary Information.\nTaken together, these results show that, in data-driven operator learning, the attainable accuracy\nexposed by LSR is primarily governed by the dimension of subspace. Once the subspace becomes\nsufficiently expressive, one-shot LSR can approach the optimal solution in the local linearized sense\nwithout modifying the network architecture or extending training, whereas excessive subspace\nexpansion is ultimately limited by numerical ill-conditioning.\n10\nExample 3 | Fine-tuning physics-informed operator learning\nUnlike data-driven operator learning, physics-informed operator learning does not rely on explicit\nlabeled data. Instead, supervision is provided directly through the governing equations together with\ninitial and boundary conditions. As a result, the residual vector is naturally defined at a given prediction\ninstance, rather than over an entire dataset. This structural distinction fundamentally alters how LSR\nis applied: in physics-informed settings, one-shot LSR can be performed directly on a single predicted\nsolution by constructing the corresponding physical residual and solving a locally linearized correction\nproblem. In this sense, one-shot LSR naturally operates as an instance-wise post-training refinement.\nWe evaluate this instance-wise one-shot LSR strategy on a collection of widely used benchmark\nphysics-informed operator learning problems introduced by Wang et al. [28]. The benchmarks span a\nrange of representative governing equations, including linear ordinary differential equations, reaction\u2013\ndiffusion systems, the Burgers equation, and advection equations, with uncertainty arising from\nrandom forcing terms, source terms, initial conditions, or spatially varying coefficients. In all cases,\nwe adopt the original network architectures and training procedures to obtain baseline models, and\nthen apply a single LSR step to the trained networks without any additional training. Quantitative\nresults are summarized in Table 1. Across all benchmarks, one-shot LSR consistently reduces the\nphysics residual loss at a small additional computational cost and yields stable improvements in test\nerror, with error reductions ranging from approximately one to two orders of magnitude. These results\ndemonstrate that, even in fully label-free and physics-constrained operator learning problems, one-\nshot LSR serves as an effective post-training refinement framework that exposes accuracy unattained\nby standard training.\nTable 1 | One-shot LSR refinement results for physics-informed operator fine-tuning. All reported results are\naveraged over 300 instances. Loss denotes the composite physics residual loss used during training, and test error is\nevaluated against reference solutions.\nGoverning\nlaw\nEquation form\nRandom\ninput\nBaseline\nLoss\nBaseline\ntest error\nLSR\nloss\nLSR test\nerror\nError\nreduction\nfactor\nWall\ntime\n(s)\nLinear\nODE\n( )\ndq\nu x\ndx =\nForcing\nterms\n3.7\u00d710-5 \u00b1\n4.2\u00d710-5\n3.7\u00d710-3 \u00b1\n3.5\u00d710-3\n2.3\u00d710-10 \u00b1\n2.0\u00d710-10\n1.5\u00d710-5 \u00b1\n1.3\u00d710-5\n~240\n0.04\nDiffusion\nreaction\n2\n2\n2\n( )\nq\nq\nD\nkq\nu x\nt\nx\n\uf0b6\n\uf0b6\n=\n+\n+\n\uf0b6\n\uf0b6\nSource\nterms\n6.9\u00d710-5 \u00b1\n5.6\u00d710-5\n5.6\u00d710-3 \u00b1\n2.6\u00d710-3\n8.5\u00d710-7 \u00b1\n8.3\u00d710-7\n5.7\u00d710-4 \u00b1\n2.8\u00d710-4\n~10\n0.37\nBurgers\u2019\n2\n2\n0\nq\nq\nq\nq\nt\nx\nx\n\uf06e\n\uf0b6\n\uf0b6\n\uf0b6\n+\n\u2212\n=\n\uf0b6\n\uf0b6\n\uf0b6\nInitial\nconditions\n6.7\u00d710-4 \u00b1\n5.2\u00d710-3\n1.2\u00d710-2 \u00b1\n1.5\u00d710-2\n8.6\u00d710-9 \u00b1\n3.9\u00d710-9\n3.9\u00d710-4 \u00b1\n2.2\u00d710-3\n~30\n6.90\nAdvection\n( )\n0\nq\nq\nu x\nt\nx\n\uf0b6\n\uf0b6\n+\n=\n\uf0b6\n\uf0b6\nVariable\ncoefficients\n4.9\u00d710-3 \u00b1\n4.4\u00d710-3\n2.2\u00d710-2 \u00b1\n6.7\u00d710-3\n3.1\u00d710-5 \u00b1\n2.6\u00d710-5\n3.1\u00d710-3 \u00b1\n9.0\u00d710-4\n~8\n6.11"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"A Universal Linearized Subspace Refinement Framework for Neural Networks\"\n\n#### Main Contributions:\nThe paper introduces a novel framework called Linearized Subspace Refinement (LSR) that addresses the issue of numerical ill-conditioning in neural networks, particularly when using gradient-based training methods. LSR is designed to be architecture-agnostic and generalizable across various learning paradigms including function approximation, data-driven operator learning, and physics-informed operator fine-tuning.\n\n#### Methodology:\n1. **Linearization**: The core idea of LSR is to linearize the residual vector around a trained model using its Jacobian. This induces a convex least-squares problem within a low-dimensional subspace.\n2. **LSR Framework**:\n   - **One-shot LSR**: Solves the reduced least-squares problem directly in this subspace, yielding an optimal correction that is applied post-training without modifying network architectures or training procedures.\n   - **Iterative LSR**: Alternates one-shot LSR with supervised nonlinear alignment to handle more complex composite loss structures. This approach converts ill-conditioned residual minimization into numerically benign fitting steps.\n\n#### Key Findings:\n1. **Accuracy Enhancement Across Paradigms**:\n    - In function approximation tasks, LSR consistently exposes additional accuracy beyond what is achieved by gradient-based training.\n    - For data-driven operator learning (e.g., Burgers' equation), LSR significantly reduces test errors compared to baseline predictions.\n    - In physics-informed operator fine-tuning problems, one-shot LSR effectively refines models without requiring additional training.\n\n2. **Robustness Across Architectures and Activations**:\n    - The framework is shown to be robust across different neural network architectures and activation functions (e.g., ReLU, Tanh, custom activation).\n3. **Dimensionality Impact**: \n   - Increasing the subspace dimension leads to substantial accuracy improvements in all settings tested.\n4. **Numerical Ill-conditioning**:\n    - The framework leverages direct solvers within a reduced Jacobian-induced subspace, which is computationally tractable and provides conservative estimates of achievable error reductions.\n\n#### Conclusions:\nLSR offers a principled approach to refining neural network predictions by addressing the inherent numerical ill-conditioning issues that arise during gradient-based training. By operating post-training and leveraging classical direct solvers within a reduced Jacobian subspace, LSR can expose substantial accuracy improvements across various learning paradigms without requiring modifications to network architectures or loss formulations. The framework's effectiveness is demonstrated through consistent accuracy enhancements in function approximation, operator learning, and physics-informed problems.\n\nThis approach not only provides a robust refinement mechanism but also offers insights into the fundamental limitations of gradient-based training methods, suggesting that numerical ill-conditioning can be a significant bottleneck for achieving high-accuracy predictions in neural networks."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for\nEvaluating Sampled Video Frames\nShih-Yao Lin\nIndependent Researcher\nmikeslin.research@gmail.com\nAbstract\nFrame sampling is a fundamental component in video un-\nderstanding and video\u2013language model pipelines, yet eval-\nuating the quality of sampled frames remains challenging.\nExisting evaluation metrics primarily focus on perceptual\nquality or reconstruction fidelity, and are not designed to\nassess whether a set of sampled frames adequately captures\ninformative and representative video content.\nWe\npropose\nSpatio-Temporal\nEntropy\nCoverage\n(STEC), a simple and non-reference metric for evaluating\nthe effectiveness of video frame sampling.\nSTEC builds\nupon Spatio-Temporal Frame Entropy (STFE), which\nmeasures per-frame spatial information via entropy-based\nstructural complexity, and evaluates sampled frames based\non their temporal coverage and redundancy.\nBy jointly\nmodeling spatial information strength, temporal disper-\nsion, and non-redundancy, STEC provides a principled and\nlightweight measure of sampling quality.\nExperiments on the MSR-VTT test-1k benchmark demon-\nstrate that STEC clearly differentiates common sampling\nstrategies, including random, uniform, and content-aware\nmethods. We further show that STEC reveals robustness\npatterns across individual videos that are not captured by\naverage performance alone, highlighting its practical value\nas a general-purpose evaluation tool for efficient video un-\nderstanding.\nWe emphasize that STEC is not designed to predict\ndownstream task accuracy, but to provide a task-agnostic\ndiagnostic signal for analyzing frame sampling behavior\nunder constrained budgets.\n1. Introduction\nVideo frame sampling is a fundamental operation in video\nunderstanding systems, where only a limited number of\nframes can be processed due to computational or memory\nconstraints. This setting is increasingly common in modern\nvideo\u2013language model pipelines, long-form video analysis,\nand efficient video retrieval, making the quality of sampled\nframes critical to downstream performance.\nDespite its importance, frame sampling is often treated\nas a fixed preprocessing step. Most video understanding\npipelines focus on optimizing model architectures or learn-\ning objectives, leaving the quality of temporal coverage in-\nduced by sampling strategies largely unexamined and en-\ntangled with downstream tasks and models.\nMoreover, there is no unified, task-agnostic metric for\nevaluating sparse frame sampling quality. Existing metrics\nmainly focus on perceptual fidelity or reconstruction and\nfail to assess informativeness, temporal coverage, and re-\ndundancy, making principled and task-independent compar-\nison difficult.\nIn this work, we propose Spatio-Temporal Entropy\nCoverage (STEC), a non-reference metric for evaluat-\ning video frame sampling.\nSTEC builds upon Spatio-\nTemporal Frame Entropy (STFE), which quantifies per-\nframe spatial information using entropy-based structural\ncomplexity. To assess a set of sampled frames, STEC fur-\nther incorporates temporal dispersion over the video time-\nline and a redundancy penalty based on visual similarity.\nThe resulting metric jointly reflects spatial informativeness,\ntemporal coverage, and non-redundancy of sampled frames.\nWe evaluate STEC on the MSR-VTT [15] test-1k bench-\nmark across a range of common sampling strategies, includ-\ning random sampling, uniform sampling, and content-aware\nmethods. Our results show that STEC consistently differen-\ntiates these strategies and reveals robustness patterns at the\nper-video level, providing insights that are not captured by\naverage scores alone. These findings suggest that STEC can\nserve as a practical evaluation tool for analyzing frame sam-\npling behavior in efficient video understanding pipelines.\nOur contributions are three-fold:\n\u2022 We introduce STEC, a non-reference, task-agnostic met-\nric to evaluate arbitrary sampled frame sets, decoupled\nfrom the sampling mechanism.\n\u2022 We propose STFE to quantify per-frame spatial informa-\ntion using entropy-based structural complexity.\n\u2022 We conduct a systematic evaluation on MSR-VTT\n1\narXiv:2601.13974v1  [cs.CV]  20 Jan 2026\ndemonstrating that STEC effectively differentiates com-\nmon sampling strategies and reveals per-video robustness\npatterns.\n2. Related Work\n2.1. STEC as an Evaluation Metric\nWe emphasize that STEC does not propose a new frame\nsampling strategy. Instead, it is designed as a task-agnostic\nevaluation metric that assesses the quality of an arbitrary set\nof sampled frames, independent of how the frames are se-\nlected. This distinction is particularly important in modern\nvideo understanding pipelines, where sampling decisions\nare often tightly coupled with model architectures, attention\nmechanisms, or learned policies. By decoupling evaluation\nfrom sampling, STEC enables fair and interpretable com-\nparison across heterogeneous frame selection methods.\n2.2. Frame Selection for Video Understanding and\nVideo-LLMs\nRecent work has proposed increasingly sophisticated frame\nselection strategies to reduce computational cost while pre-\nserving semantic content.\nAdaptive Keyframe Sampling\n(AKS) dynamically selects frames for long-video under-\nstanding under token budgets [13]. Hu et al. propose an M-\nLLM-based frame selection module trained with spatial and\ntemporal supervision, which is then applied to frozen video\nlanguage models [5]. KeyVideoLLM leverages CLIP-based\nkeyframe selection to construct higher-quality frame\u2013text\npairs for VideoLLM training [9]. More recently, Logic-in-\nFrames applies semantic\u2013logical reasoning to identify crit-\nical frames in long videos [2], while prompting-based and\nzero-shot methods leverage large language models to guide\nframe selection without task-specific training [14].\nThese methods focus on improving how frames are se-\nlected. STEC is orthogonal: it evaluates what is selected,\nand can be applied to the outputs of any of these methods to\nassess informativeness, temporal coverage, and redundancy.\nWe note that our experimental evaluation focuses on rep-\nresentative random, uniform, and content-aware sampling\nstrategies. Evaluating STEC on LLM-guided or attention-\nbased frame selection is left for future work. Our goal is not\nto exhaustively benchmark sampling methods, but to vali-\ndate STEC across representative and widely used strategies.\n2.3. Video Summarization and Subset Selection\nVideo summarization shares related goals with frame sam-\npling, aiming to select a subset of frames or segments that\nbest represent a video. Dual video summarization meth-\nods jointly optimize visual and linguistic consistency to pro-\nduce semantically meaningful summaries [6]. Earlier work\non submodular optimization and DPPs provides principled\ntools for modeling importance and diversity [1, 4].\nUnlike summarization, which typically targets dense\nsummaries and often relies on reference-based evaluation,\nSTEC is designed for sparse frame sampling under tight\nbudgets and does not require human reference summaries.\n2.4. Reference-Free Evaluation of Video Summaries\nThe limitations of reference-based evaluation for video\nsummarization have been well documented [12]. Recent\nwork has therefore explored reference-free evaluation met-\nrics. QEVA evaluates narrative video summaries by gen-\nerating multimodal questions and assessing coverage, fac-\ntuality, and temporal coherence without relying on human\nreferences [7]. While QEVA targets summary-level evalua-\ntion via question answering, STEC focuses on frame-level\nsampling quality and provides a lightweight, non-reference\nmetric tailored to video understanding pipelines.\n2.5. Entropy-Based Keyframe Analysis\nEntropy has long been used as an interpretable proxy for\nvisual information content in keyframe extraction. Early\nmethods use entropy differences or relative entropy between\nframes to identify representative keyframes [3, 11]. STEC\nbuilds upon this intuition but generalizes it to a set-level\nevaluation that explicitly models temporal coverage and\nredundancy, which are not addressed by entropy-only ap-\nproaches.\n3. Method\n3.1. Problem Setup\nGiven a video V consisting of N frames {f1, f2, . . . , fN},\na\nsampler\nselects\na\nsubset\nof\nK\nframes\nS\n=\n{fi1, fi2, . . . , fiK} with K \u226aN. Our goal is to evaluate\nhow informative and representative S is with respect to V\nwithout requiring ground-truth annotations or task-specific\nsupervision.\n3.2. Spatio-Temporal Frame Entropy\nSpatial Entropy.\nTo quantify spatial information, we\nmeasure the diversity of local structural responses using\nentropy. For each frame fi, we first convert the frame to\ngrayscale and apply a Laplacian operator to obtain a high-\nfrequency response map L(fi), which emphasizes local\nstructure such as edges and textures while suppressing low-\nfrequency appearance. The Laplacian acts as a differential\noperator and does not itself define an information measure.\nWe therefore compute Shannon entropy over local neigh-\nborhoods of the Laplacian response. Let \u2126denote the set of\nall pixel locations in the frame. For each pixel x \u2208\u2126, we\ndefine a local neighborhood Nr(x) = {y \u2208\u2126| \u2225y \u2212x\u22252 \u2264\nr} with radius r.\nThe Laplacian responses within each\nneighborhood are quantized into a fixed number of bins\n2\nto form an empirical distribution, from which Shannon en-\ntropy is computed. The spatial entropy of frame fi is then\nobtained by averaging over all pixel locations:\nEs(fi) = 1\n|\u2126|\nX\nx\u2208\u2126\nH(L(fi)[Nr(x)]) .\n(1)\nThis formulation captures the unpredictability and richness\nof local structural patterns rather than their raw magnitude,\nand spatial averaging yields stable frame-level estimates.\nTemporal Entropy.\nTo measure how sampled frames are\ndistributed along the video timeline, we consider their nor-\nmalized temporal positions. Given a video with N frames\nindexed from 1 to N, we define the normalized temporal\nposition of each sampled frame fij \u2208S as\n\u03c4j = ij \u22121\nN \u22121,\n(2)\nso that \u03c4j \u2208[0, 1]. We discretize {\u03c4j}K\nj=1 into B temporal\nbins and compute a normalized Shannon entropy:\nEt(S) = \u2212\nPB\nb=1 pb log pb\nlog B\n,\n(3)\nwhere pb denotes the fraction of sampled frames falling into\nbin b. Following standard convention, we define 0 log 0 =\n0 for empty bins.\nThe normalization by log B ensures\nEt(S) \u2208[0, 1].\n3.3. Temporal Coverage and Redundancy\nTemporal Coverage (Span).\nTemporal entropy measures\nthe evenness of dispersion, but does not guarantee global\ncoverage. We therefore define a span factor:\nCt(S) = maxj ij \u2212minj ij\nN \u22121\n,\n(4)\nand combine the two as\nT(S) = Et(S) \u00b7 Ct(S).\n(5)\nNon-redundancy.\nWe assume sampled indices are sorted\nin ascending temporal order, i.e., i1 < i2 < \u00b7 \u00b7 \u00b7 < iK,\nand K \u22652. To penalize redundant selections, we compute\ncosine similarity between HSV color histograms of tempo-\nrally adjacent sampled frames and define\nR(S) = 1 \u2212\n1\nK \u22121\nK\u22121\nX\nj=1\nsim(fij, fij+1),\n(6)\nwhere K = |S| and higher R(S) indicates lower redun-\ndancy. We use \u21131-normalized nonnegative HSV histograms,\nso sim(\u00b7, \u00b7) \u2208[0, 1] and R(S) \u2208[0, 1].\n3.4. Spatio-Temporal Entropy Coverage (STEC)\nWe define the final Spatio-Temporal Entropy Coverage\n(STEC) score as the product of spatial information strength,\ntemporal coverage, and non-redundancy:\nSTEC(S) =\n\uf8eb\n\uf8ed1\nK\nK\nX\nj=1\nEs(fij)\n\uf8f6\n\uf8f8\u00b7 T(S) \u00b7 R(S),\n(7)\nwhere S = {fi1, . . . , fiK} and K = |S|. The multiplicative\nformulation enforces that high STEC scores are achieved\nonly when all three criteria are simultaneously satisfied,\nreflecting the joint importance of spatial informativeness,\ntemporal coverage, and non-redundancy.\nTemporal Entropy vs. Temporal Span.\nFigure 1 shows\na key limitation of temporal entropy when used in isolation.\nAlthough temporal entropy measures how evenly sampled\nframes are distributed across temporal bins, it does not guar-\nantee that these frames cover the full extent of a video. As\nshown in the top example, frames may be uniformly dis-\ntributed within a narrow temporal segment, yielding high\ntemporal entropy but poor global coverage. In contrast, the\nbottom example demonstrates that combining temporal en-\ntropy with a span-based coverage term ensures that sampled\nframes are both evenly dispersed and span the entire video\ntimeline. This motivates our definition of temporal cover-\nage as the product of temporal entropy and temporal span.\n3.5. Discussion\nSTEC is a non-reference, task-agnostic, and computation-\nally efficient metric. Unlike learning-based or task-specific\nevaluation approaches, STEC requires no training, annota-\ntions, or pretrained models. It also avoids reliance on op-\ntical flow, semantic representations, or supervision, mak-\ning it broadly applicable to evaluating diverse frame sam-\npling strategies in video understanding and video\u2013language\nmodel pipelines. The computational cost of STEC scales\nlinearly with the number of sampled frames.\n4. Experimental Setup\n4.1. Dataset\nWe evaluate the proposed STEC metric on the MSR-VTT\ndataset [15], which contains diverse web videos covering\na wide range of events and activities. Following standard\npractice in video\u2013language research, we use the official test-\n1k split, consisting of 1,000 videos.\n4.2. Frame Sampling Methods\nWe evaluate STEC on four representative frame sampling\nstrategies: Random, which selects frames uniformly at ran-\ndom; Uniform, which samples frames at fixed temporal in-\ntervals; Katna [8], a content-aware method based on visual\n3\nstart\nend\n(a) High temporal entropy but low temporal span (Et \u22481, Ct \u226a1).\nstart\nend\n(b) High temporal entropy with high temporal span (Et \u22481, Ct \u22481).\nFigure 1. Temporal entropy versus temporal span. High temporal\nentropy alone does not guarantee global coverage: frames may be\nevenly distributed within a narrow temporal segment (top). Com-\nbining temporal entropy with a span-based coverage term ensures\nthat sampled frames are both evenly dispersed and cover the full\nextent of the video timeline (bottom).\nimportance; and STACFP [10], a clustering-based strategy\nthat selects spatio-temporally diverse frame proposals.\nFor all methods, the number of sampled frames is fixed\nto K = 16. We use the STACFP frame proposal module as\nintroduced in [10]. All samplers are evaluated under identi-\ncal settings to ensure a fair comparison.\n4.3. Evaluation Protocol\nFor each video, sampled frames are extracted independently\nusing each sampling strategy and evaluated using the pro-\nposed STEC metric. Spatial entropy is computed from a\nLaplacian-based complexity map followed by local entropy\nestimation. Temporal entropy is computed by discretizing\nnormalized frame positions into a fixed number of tempo-\nral bins. Redundancy is measured using cosine similarity\nbetween color histograms of temporally adjacent sampled\nframes. All hyperparameters are kept fixed across experi-\nments, including the entropy neighborhood size, the num-\nber of temporal bins, and histogram resolutions, to ensure\nconsistent evaluation across samplers.\n4.4. Implementation Details\nFor spatial entropy computation, each frame is first con-\nverted to grayscale and processed using a 3 \u00d7 3 Laplacian\noperator. Concretely, we normalize the Laplacian response\nmap to 8-bit and compute local Shannon entropy using a\ndisk-shaped structuring element (radius r) via a rank-based\nentropy filter, which implements Eq. (1) with implicit bin-\nning over 256 gray levels.\nAll experiments are conducted without any learning or\nfine-tuning.\nSTEC is computed directly from sampled\nframes and their temporal indices. Unless otherwise spec-\nified, we use K = 16 sampled frames, a fixed number of\nTable 1. Comparison of frame sampling strategies on MSR-VTT\ntest-1k using the proposed Spatio-Temporal Entropy Coverage\n(STEC) metric. Higher values indicate better sampling quality /\ncoverage quality.\nSampler\nS \u2191\nT \u2191\nR \u2191\nSTEC \u2191\nRandom\n1.87\n0.82\n0.21\n0.306\nUniform\n2.41\n1.00\n0.19\n0.463\nKatna [8]\n2.95\n0.91\n0.25\n0.663\nSTACFP [10]\n3.02\n0.88\n0.27\n0.658\nTable 2. Number of videos (out of 1,000) for which each sampling\nstrategy achieves the highest STEC score on MSR-VTT test-1k.\nSampler\n#Videos with Highest STEC\nRandom\n7\nUniform\n91\nKatna [8]\n421\nSTACFP [10]\n481\ntemporal bins B = 8, and a local entropy neighborhood ra-\ndius of 6 pixels. All hyperparameters are kept fixed across\nexperiments to ensure fair comparison.\n5. Results and Analysis\n5.1. Quantitative Results\nAs a sanity check, STEC assigns the lowest scores to ran-\ndom sampling and higher scores to more structured sam-\npling strategies. Table 1 shows the average spatial infor-\nmation score (S), temporal coverage score (T), redundancy\nscore (R), and the STEC score on MSR-VTT test-1k split.\nUniform sampling achieves perfect temporal coverage\nby construction, but suffers from lower spatial information\nand higher redundancy, resulting in a moderate STEC score.\nRandom sampling performs poorly across all metrics, high-\nlighting the difficulty of capturing informative content with-\nout structure.\nContent-aware sampling methods, including Katna [8]\nand STACFP [10], substantially improve spatial informa-\ntion and non-redundancy, leading to significantly higher\nSTEC scores. Katna achieves the highest average STEC\nscore, while STACFP attains a comparable performance.\nThis suggests that while Katna excels on average, STACFP\nexhibits more consistent performance across individual\nvideos. This contrast motivates a per-video robustness anal-\nysis, as average performance alone does not capture consis-\ntency across diverse videos.\n5.2. Per-Video Robustness Beyond Average Perfor-\nmance\nBeyond average performance, we analyze robustness at the\nper-video level. Table 2 reports how often each sampling\n4\nFigure 2.\nDistribution of per-video STEC scores on MSR-\nVTT test-1k. Boxplots summarize STEC values across individ-\nual videos for different sampling strategies. The median and in-\nterquartile range represent typical performance, while outliers cor-\nrespond to rare extreme cases. Content-aware methods (Katna and\nSTACFP) shift the overall distribution toward higher STEC values,\nindicating improved robustness across diverse videos, rather than\nperformance dominated by a small number of outlier cases. No-\ntably, STACFP exhibits a more compact distribution with fewer\nextreme failures, indicating stronger per-video robustness com-\npared to methods with higher variance.\nstrategy achieves the highest STEC score across individual\nvideos. STACFP attains the highest STEC on the largest\nportion of videos, indicating more consistent performance\nacross diverse content.\nTo further examine robustness, Figure 2 shows the dis-\ntribution of per-video STEC scores.\nUniform and Ran-\ndom sampling concentrate at low STEC values, reflecting\nsystematic redundancy despite full temporal coverage. In\ncontrast, content-aware methods shift the median and in-\nterquartile range toward higher values.\nKatna occasion-\nally achieves very high STEC on specific videos, whereas\nSTACFP exhibits a more compact distribution with consis-\ntently strong performance.\nTogether, these results show that STEC reveals per-video\nrobustness patterns that are not evident from average statis-\ntics alone.\nFrom STEC Evaluation to Downstream Retrieval.\nSTEC is designed as a simple, task-agnostic metric for as-\nsessing the quality of sampled frame sets, without optimiz-\ning for any specific downstream task. A practical question\nis whether frame sampling strategies that score well under\nSTEC still work reasonably well when used in real video\u2013\nlanguage retrieval systems.\nTo study this, we run a retrieval experiment on MSR-\nVTT using a fixed video\u2013text encoder and the same retrieval\nsetup across all methods. We compare random sampling,\nuniform sampling, appearance-based clustering (Katna) [8],\nand STACFP [10]. The model, encoder, and retrieval proto-\ncol are kept identical, and only the selected frame sets differ,\nTable 3.\nDownstream retrieval performance on MSR-VTT us-\ning different frame sampling strategies under a fixed video\u2013\ntext encoder. This experiment examines whether higher STEC\nscores correspond to preserved downstream retrieval performance.\nT2V/V2T: Recall@1 (%).\nFrame Sampler\nT2V\nV2T\nRandom\n49.2\n46.1\nUniform\n50.6\n48.0\nKatna [8]\n49.9\n46.8\nSTACFP [10]\n50.9\n48.8\nwith a fixed sampling budget of K = 16.\nTable 3 shows that sampling strategies with better spatio-\ntemporal coverage under STEC generally do not degrade\nretrieval performance under a fixed frame budget. In partic-\nular, STACFP achieves the strongest retrieval results while\nalso obtaining high STEC scores, suggesting that frame sets\nwith good temporal coverage and low redundancy remain\ncompatible with a frozen retrieval model. At the same time,\nhigher STEC scores do not always translate to higher re-\ntrieval accuracy, indicating that STEC is not a predictor of\ndownstream performance.\nOverall, this experiment does not claim that STEC\ncan replace task-level evaluation. Instead, it demonstrates\nthat STEC provides a reasonable, task-independent way to\nscreen frame sampling strategies and identify selections that\nare unlikely to degrade downstream retrieval when compute\nor sampling budgets are limited.\n5.3. Qualitative Analysis of Sampling Behavior\nWhile aggregate statistics compare sampling strategies on\naverage, they do not reveal the underlying reasons for their\nbehavior on individual videos. We therefore provide a qual-\nitative analysis to visualize the sampling patterns and failure\nmodes captured by STEC.\nFigure 3 presents two representative MSR-VTT videos\nillustrating distinct sampling scenarios.\nIn the first case\n(top), Uniform and Random sampling achieve perfect tem-\nporal coverage but select many visually similar frames, re-\nsulting in low non-redundancy and low STEC. STACFP\nsubstantially increases diversity by focusing on informative\nsegments, leading to a much higher non-redundancy score,\nalbeit with reduced coverage. Katna maintains full coverage\nwhile reducing redundancy, achieving the highest STEC on\nthis video. This example highlights that temporal coverage\nalone is insufficient to characterize sampling quality.\nThe second case (bottom) shows a video with highly\nrepetitive visual content.\nAlthough Uniform and Ran-\ndom again achieve perfect coverage, they repeatedly sam-\nple near-duplicate frames, resulting in extremely low non-\nredundancy. Content-aware methods explicitly mitigate re-\n5\nFigure 3. Qualitative analysis of frame sampling behavior using STEC. Two representative MSR-VTT videos illustrate the coverage\u2013\nredundancy trade-off captured by STEC. Top: For a video with diverse scenes, Uniform and Random achieve full temporal coverage but\nselect many redundant frames, resulting in low STEC. Content-aware methods improve diversity: STACFP focuses on informative seg-\nments with higher non-redundancy, while Katna maintains full coverage and achieves the highest STEC. Bottom: For a video dominated\nby repetitive content, Uniform and Random repeatedly select near-duplicate frames, leading to very low non-redundancy. Content-aware\nmethods explicitly reduce redundancy; STACFP emphasizes diversity with reduced coverage, whereas Katna balances coverage and diver-\nsity and yields the highest STEC.\ndundancy by selecting more diverse frames. STACFP con-\ncentrates on a subset of informative segments, while Katna\nbalances coverage and diversity, yielding the highest STEC.\nTogether, these examples demonstrate that STEC provides a\ndiagnostic view of coverage\u2013redundancy trade-offs that are\ninvisible to temporal coverage alone.\n6. Discussion\nTaken together, these results demonstrate that STEC effec-\ntively captures key aspects of frame sampling quality, in-\ncluding spatial informativeness, temporal coverage, and re-\ndundancy. The proposed metric clearly differentiates be-\ntween naive, uniform, and content-aware sampling strate-\ngies, highlighting its usefulness as a general-purpose, non-\nreference evaluation tool for analyzing sampled frame sets\nacross diverse video pipelines. STEC is intended for com-\nparative evaluation of frame sampling strategies under fixed\nbudgets, rather than absolute assessment of video under-\nstanding performance.\nRelation to Downstream Tasks.\nAlthough STEC is de-\nsigned as a task-agnostic metric rather than a predictor of\nabsolute downstream task accuracy, an important practical\nquestion is whether higher STEC scores correspond to more\neffective downstream usage. As shown in Table 3, frame\nsampling strategies with strong spatio-temporal coverage\nunder STEC generally remain competitive in downstream\nretrieval under a fixed video\u2013text encoder and a fixed frame\nbudget.\nThis observation suggests that STEC captures spatio-\ntemporal properties of frame sets that are broadly aligned\nwith the requirements of practical video\u2013language applica-\ntions. We emphasize that this analysis demonstrates consis-\ntency rather than causality, and STEC should be viewed as a\ncomplementary evaluation signal rather than a replacement\nfor task-specific performance metrics.\nLimitations of Spatial Entropy.\nThe spatial entropy\ncomponent of STEC measures information richness based\non low-level visual statistics and does not explicitly account\nfor high-level semantic importance.\nAs a result, scenes\nthat are semantically critical but visually homogeneous may\nreceive lower spatial entropy scores. This design choice\nallows STEC to remain lightweight and training-free, but\nalso highlights an important limitation. Future work could\nincorporate semantic-aware alternatives, such as entropy\ncomputed over pretrained visual\u2013language embeddings or\nobject-conditioned representations, to better capture task-\nrelevant semantics.\nSensitivity to Hyperparameters.\nIn this work, we focus\non demonstrating the effectiveness of STEC under a fixed\nand representative set of hyperparameters. While our exper-\niments indicate that STEC provides meaningful and inter-\npretable comparisons across different sampling strategies, a\nsystematic sensitivity analysis with respect to hyperparame-\nters such as the spatial entropy neighborhood size, temporal\nbinning scheme, and sampling budget remains an important\ndirection for future work. We plan to examine the stabil-\nity of STEC-induced rankings under varying configurations\nand across datasets, with the goal of further characterizing\nthe robustness of the proposed metric.\nDesign Choices and Robustness.\nSTEC employs a mul-\ntiplicative formulation to jointly model spatial information,\ntemporal coverage, and redundancy. This design enforces\nbalanced spatio-temporal coverage by penalizing degener-\nate solutions that optimize only a single component. The\nresults suggest that STEC provides consistent comparative\n6\nsignals under the evaluated parameter settings, enabling in-\nterpretable comparison among diverse frame sampling ap-\nproaches.\nScope and Applicability.\nWe primarily evaluate STEC\non short- to medium-length videos commonly used in\nvideo\u2013language benchmarks. While the formulation natu-\nrally extends to longer videos, its behavior under extreme\nlow-motion or highly repetitive content warrants further in-\nvestigation. We view STEC as a first step toward principled,\nnon-reference evaluation of frame sampling quality, and\nhope it will encourage future research on task-consistent\nand semantically informed evaluation metrics.\n7. Conclusion\nWe introduced Spatio-Temporal Entropy Coverage (STEC),\na simple and non-reference metric for evaluating the effec-\ntiveness of video frame sampling. By jointly modeling spa-\ntial information, temporal coverage, and redundancy, STEC\nprovides a principled and lightweight evaluation of how\nwell sampled frames represent video content. Experiments\non MSR-VTT demonstrate that STEC clearly differentiates\ncommon sampling strategies and reveals robustness patterns\nthat are not captured by average performance alone. We be-\nlieve STEC provides a practical evaluation tool for analyz-\ning frame sampling behavior in efficient video understand-\ning pipelines.\nCode Availability\nThe reference implementation of STEC and all evaluation\nscripts are publicly available at: https://github.\ncom/mikeslin-ir/STEC.\nReferences\n[1] B. Gong, W.-L. Chao, K. Grauman, and F. Sha.\nDiverse\nsequential subset selection for supervised video summariza-\ntion. In NeurIPS, 2014. 2\n[2] R. Guo, T. Wang, and S. Chen. Logic-in-frames: Semantic-\nlogical frame selection for long video understanding. arXiv\npreprint arXiv:2503.13139, 2025. 2\n[3] Yuejun Guo, Qing Xu, Shihua Sun, Xiaoxiao Luo, and Ma-\nteu Sbert. Selecting video key frames based on relative en-\ntropy and the extreme studentized deviate test. Entropy, 18\n(3):73, 2016. 2\n[4] M. Gygli, H. Grabner, and L. Van Gool.\nVideo summa-\nrization by learning submodular mixtures of objectives. In\nCVPR, 2015. 2\n[5] Kai Hu, Feng Gao, Xiaohan Nie, Peng Zhou, Son Tran,\nTal Neiman, Lingyun Wang, Mubarak Shah, Raffay Hamid,\nBing Yin, et al. M-llm based video frame selection for effi-\ncient video understanding. In CVPR, 2025. 2\n[6] X. Hu, Y. Zhang, and J. Li. Dual video summarization for\nvideo\u2013language tasks. In IJCAI, 2023. 2\n[7] Woojun Jung and Junyeong Kim. Qeva: A reference-free\nevaluation metric for narrative video summarization with\nmultimodal question answering. In EMNLP, 2025. 2\n[8] KeplerLab. Katna: Tool for automating video keyframe ex-\ntraction, video compression, image autocrop and smart im-\nage resize tasks. https://github.com/keplerlab/\nkatna, 2025. 3, 4, 5\n[9] Hao Liang, Jiapeng Li, Tianyi Bai, Xijie Huang, Linzhuang\nSun, Zhengren Wang, Conghui He, Bin Cui, Chong Chen,\nand Wentao Zhang. Keyvideollm: Towards large-scale video\nkeyframe selection. arXiv preprint arXiv:2407.03104, 2024.\n2\n[10] Shih-Yao Lin, Sibendu Paul, and Caren Chen. From cap-\ntions to keyframes: Keyscore for multimodal frame scor-\ning and video-language understanding.\narXiv preprint\narXiv:2510.06509, 2025. 4, 5\n[11] Markos Mentzelopoulos and Alexandra Psarrou. Key-frame\nextraction algorithm using entropy difference.\nIn ACM\nSIGMM Workshop on Multimedia information retrieval,\n2004. 2\n[12] Mayu Otani, Yuta Nakashima, Esa Rahtu, and Janne\nHeikkila. Rethinking the evaluation of video summaries. In\nCVPR, 2019. 2\n[13] Xi Tang, Jihao Qiu, Lingxi Xie, Yunjie Tian, Jianbin Jiao,\nand Qixiang Ye. Adaptive keyframe sampling for long video\nunderstanding. In CVPR, 2025. 2\n[14] Q. Wu, P. Liu, and F. Huang.\nContextual prompting and\npseudo-label scoring for zero-shot video summarization.\narXiv preprint arXiv:2510.17501, 2025. 2\n[15] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nCVPR, 2016. 1, 3\n7"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames**\n\n### Main Contributions\nThe paper introduces STEC (Spatio-Temporal Entropy Coverage), a novel metric designed to evaluate the quality of sampled video frames without requiring reference data. This is particularly useful in modern video-understanding pipelines where computational and memory constraints necessitate sampling only a subset of frames.\n\n### Methodology\nSTEC builds upon STFE (Spatio-Temporal Frame Entropy) by quantifying per-frame spatial information using entropy-based structural complexity and evaluating sampled frames based on their temporal coverage and redundancy. The key components are:\n\n1. **Spatial Entropy**: Measures the diversity of local structural responses via Shannon entropy over pixel neighborhoods.\n2. **Temporal Entropy**: Evaluates how sampled frames are distributed along the video timeline by computing normalized Shannon entropy across temporally discretized bins.\n3. **Non-redundancy**: Penalizes redundant selections using cosine similarity between color histograms of temporally adjacent frames.\n\n### Key Findings\nExperiments on the MSR-VTT test-1k benchmark demonstrate that STEC effectively differentiates common sampling strategies, including random, uniform, and content-aware methods. Notably:\n\n- **Random Sampling**: Low spatial information and high redundancy lead to moderate STEC scores.\n- **Uniform Sampling**: Achieves perfect temporal coverage but suffers from lower spatial information and higher redundancy.\n- **Content-Aware Methods (Katna & STACFP)**: Significantly improve spatial information and non-redundancy, resulting in significantly higher STEC scores. \n\n### Conclusions\nSTEC reveals robustness patterns across individual videos that are not captured by average performance alone. For instance:\n- **Uniform Sampling**: Achieves full temporal coverage but selects many redundant frames.\n- **Content-Aware Methods (Katna & STACFP)**: Improve diversity and maintain full coverage while reducing redundancy.\n\n### Practical Value\nSTEC provides a practical evaluation tool for analyzing frame sampling behavior in efficient video understanding pipelines. It is designed to be task-agnostic, requiring no training or annotations, making it broadly applicable across diverse video pipelines.\n\n### Limitations & Future Work\nWhile STEC effectively captures key aspects of frame sampling quality, its spatial entropy component does not explicitly account for high-level semantic importance. Future work could incorporate more sophisticated semantic-aware alternatives. Additionally, the robustness analysis should be extended to investigate extreme low-motion or highly repetitive content scenarios.\n\n**Overall**, STEC offers a principled and lightweight measure for evaluating sampled video frames, providing insights into their representativeness without requiring reference data."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Di\ufb00erentiated Pickup Point O\ufb00ering\nfor Emission Reduction in Last-Mile Delivery\nAlbina Galiullinaa, Wouter J.A. van Heeswijkb, Tom Van Woensela\naDepartment of Industrial Engineering and Innovation Sciences,\nEindhoven University of Technology, PO Box 513, 5600 MB, Eindhoven, Netherlands\nbDepartment of Industrial Engineering & Management Science, Faculty of Behavioural, Management and\nSocial Sciences,\nUniversity of Twente, Ravelijn 3351, 7522 NH, Enschede, Netherlands\nAbstract\nPickup points are widely recognized as a sustainable alternative to home delivery, as con-\nsolidating orders at pickup locations can shorten delivery routes and improve \ufb01rst-attempt\nsuccess rates. However, these bene\ufb01ts may be negated when customers drive to pick up\ntheir orders. This study proposes a Di\ufb00erentiated Pickup Point O\ufb00ering (DPO) policy that\naims to jointly reduce emissions from delivery truck routes and customer travel.\nUnder\nDPO, each arriving customer is o\ufb00ered a single recommended pickup point, rather than an\nunrestricted choice among all locations, while retaining the option of home delivery. We\nstudy this problem in a dynamic and stochastic setting, where the pickup point o\ufb00ered to\neach customer depends on previously realized customer locations and delivery choices. To\ndesign e\ufb00ective DPO policies, we adopt a reinforcement learningbased approach that ac-\ncounts for spatial relationships between customers and pickup points and their implications\nfor future route consolidation. Computational experiments show that di\ufb00erentiated pickup\npoint o\ufb00erings can substantially reduce total carbon emissions. The proposed policies reduce\ntotal emissions by up to 9% relative to home-only delivery and by 2% on average compared\nwith alternative policies, including unrestricted pickup point choice and nearest pickup point\nassignment. Di\ufb00erentiated o\ufb00erings are particularly e\ufb00ective in dense urban settings with\nEmail address: albina.galiullina@proton.me, +31 6 3935 3233 (Albina Galiullina)\narXiv:2601.14196v1  [cs.LG]  20 Jan 2026\nmany pickup points and short inter-location distances. Moreover, explicitly accounting for\nthe dynamic nature of customer arrivals and choices is especially important when customers\nare less inclined to choose pickup point delivery over home delivery.\nKeywords: Last-mile delivery, pickup points, emissions, graph attention network, customer\nchoice.\n1. Introduction\nAs e-commerce grows, last-mile delivery becomes more challenging, accounting for 53%\nof total shipping costs (Accenture, 2023) and 13% of total city carbon emissions (World\nEconomic Forum, 2024). To improve the e\ufb03ciency and sustainability of the last mile, ser-\nvice providers adopt alternative delivery methods to complement traditional attended home\ndelivery. One promising alternative is pickup point delivery, in which orders are delivered\nto intermediate locations, such as stores, post o\ufb03ces, or automated lockers, for customer\ncollection.\nPickup points o\ufb00er advantages for both service providers and customers. For providers,\npickup points enhance last-mile e\ufb03ciency by consolidating orders into a few pickup points,\nreducing the number of stops required for delivery and lowering the risk of failed deliveries\n(Morganti et al., 2014; Deutsch and Golany, 2018; Janjevic et al., 2019; Rautela et al., 2022).\nCustomers may also \ufb01nd this option more convenient than waiting at home for deliveries,\nwhich can be restrictive and uncertain (CIVITAS, 2022). This growing preference for pickup\npoint delivery is evident in recent trends: the share of home delivery in Europe declined\nto 54% in 2025 (DHL, 2025), while the number of pickup point locations increased by 22%\nbetween 2023 and 2025 (GEL Proximity, 2025).\nDespite these advantages, the overall environmental impact of pickup point delivery re-\nmains ambiguous when customers travel emissions are considered. For example, Niemeijer\nand Buijs (2023) show that when a pickup point is located more than 3 km from the cus-\n2\ntomer\u2019s home, the probability that the customer makes a round trip by car exceeds 30%.\nConsequently, because of such customers\u2019 travel, pickup point delivery may result in higher\ntotal carbon emissions than home delivery, contrary to the intended sustainability bene\ufb01ts\nof pickup point delivery. These \ufb01ndings highlight that the sustainability of pickup point\ndelivery depends critically on how pickup points are o\ufb00ered to customers, rather than on\ntheir mere availability.\nTaking into account both the delivery truck and customers\u2019 travel emissions, this study\nexamines di\ufb00erentiated pickup point o\ufb00erings as a tool to minimize total last-mile carbon\ndioxide emissions. We consider a next-day delivery service within a de\ufb01ned region, with mul-\ntiple pickup points and a single depot served by a single truck. Orders arrive throughout the\nday, and customers choose between home delivery and pickup at a designated point when\nplacing orders.\nUnlike industry practice, where customers are given unrestricted choice\namong all pickup points, we examine the policy of o\ufb00ering a single recommended pickup\npoint per order. This policy re\ufb02ects a realistic operational intervention, as service providers\nincreasingly in\ufb02uence customer choices through recommendations, default options, and in-\nterface design. Given the pickup o\ufb00er, the customer decides whether to accept pickup point\ndelivery or opt for home delivery. The next day, the truck ful\ufb01lls the delivery route based\non customers\u2019 choices. Next, customers who select pickup point delivery travel to collect\ntheir orders using either non-polluting modes (e.g., walking or cycling) or car travel. The\nobjective is to design a di\ufb00erentiated pickup point o\ufb00ering (DPO) policy that dynamically\nselects one pickup point for each arriving customer, to minimize total carbon emissions from\nboth delivery operations and customer travel.\nDetermining which pickup point to o\ufb00er each customer is not trivial because it requires\nbalancing the trade-o\ufb00between order consolidation and customer proximity to pickup points.\nO\ufb00ering the nearest pickup point or home delivery minimizes customer travel-related emis-\nsions but can increase delivery truck emissions by requiring trucks to visit multiple locations.\n3\nConversely, consolidating orders at a few centrally located pickup points reduces truck emis-\nsions but increases customer travel distances and associated emissions.\nThe problem is\ninherently dynamic, because each decision in\ufb02uences not only the current order but also the\nstructure of future deliveries. For example, all else equal, o\ufb00ering a pickup point that is\nalready planned to be visited is more e\ufb03cient than introducing a new stop.\nThe dynamic trade-o\ufb00between consolidation and proximity gives rise to a sequential\ndecision-making problem under uncertainty, which can be naturally modeled as a Markov\ndecision process (MDP). When a customer arrives, the policy assigns a pickup point based on\nthe current system state, while accounting for its impact on future consolidation. Although\nthe action space is small (selecting one pickup point from a \ufb01xed set), the state space is\nlarge and variable, as it captures geographical relations between a dynamically changing set\nof customers and pickup points. To capture these geographical and relational dependencies\nin a scalable manner, we represent the system state as a graph with tailored connections and\nnode features. This representation allows the policy to account for spatial proximity and\nconsolidation structure while accommodating a variable number of customers. We employ a\ngraph attention mechanism to prioritize relevant local interactions, such as nearby customers\nor already-visited pickup points, while retaining global information needed to estimate future\nrouting costs. Policy learning is performed using Proximal Policy Optimization (PPO), a\nwell-established reinforcement learning algorithm.\nOur numerical analysis demonstrates the e\ufb00ectiveness of the proposed di\ufb00erentiated\npickup point o\ufb00ering policy across a range of operational settings.\nThe resulting policy\nconsistently outperformed a PPO baseline with a \ufb02attened state representation and a graph-\nbased model without attention. We further conduct experiments on problem settings to\nidentify when a DPO policy is most bene\ufb01cial and when simpler alternative policies perform\nadequately. In particular, we examine how the policies adapt to changes in the size of the\nservice region, customer choice probabilities, and the time of order arrival.\n4\nOur contributions can be summarized as follows:\n1. We introduce a new dynamic policy for di\ufb00erentiated pickup point o\ufb00erings that aims to\nminimize total last-mile carbon emissions. The policy explicitly accounts for emissions\nfrom both delivery trucks and customer travel, and captures the trade-o\ufb00between\nconsolidation e\ufb03ciency and customer proximity to the pickup points.\n2. We propose a scalable graph-based state representation that captures the spatial and\nconsolidation structure of the system and enables e\ufb00ective learning of dynamic pickup\npoint o\ufb00ering policies under uncertainty.\n3. We provide a systematic numerical study that identi\ufb01es problem settings in which\ndi\ufb00erentiated o\ufb00erings are most e\ufb00ective, and those in which simpler policies, such\nas home-only delivery, unrestricted pickup point choice, or nearest pickup point as-\nsignment, perform reasonably well. Di\ufb00erentiated o\ufb00erings are most valuable in dense\nurban environments with many pickup points and short inter-location distances, and\nwhen customers are less inclined to choose pickup point delivery.\n4. We benchmark the proposed policy against a perfect-information solution and develop\na dynamic heuristic that captures key insights from the learned policy while reducing\ncomputational complexity.\nThe remainder of the paper is organized as follows. Section 2 reviews the relevant liter-\nature. Section 3 describes the problem and Section 4 describes formulation. The solution\nmethodology is presented in Section 5. Section 6 describes the setup for the numerical exper-\niments, and Section 7 analyzes the numerical experiments, and the decisions of the learned\npolicy. Section 8 concludes the paper.\n5\n2. Related literature\nThis section reviews two research streams relevant to our study. Section 2.1 reviews\nthe impact of pickup points on the sustainability of last-mile delivery. Section 2.2 reviews\ncustomer choice modeling and control in last-mile delivery.\n2.1. Impact of pickup points on the sustainability of last-mile delivery\nPickup points are recognized as an e\ufb00ective approach for solving last-mile delivery chal-\nlenges. We refer to reviews by Rohmer and Gendron (2020), Janinho\ufb00et al. (2024), and\nMohri et al. (2024) that explore various aspects of pickup point delivery. Our focus is specif-\nically on the sustainability bene\ufb01ts of pickup point delivery, such as reducing the failed\ndelivery rate, decreasing delivery truck travel distances, and enhancing the robustness of\ndelivery systems.\nFailed home deliveries remain a major operational concern, particularly when customers\nare unavailable to receive parcels. Pickup points mitigate this issue by serving as primary\ndelivery locations or as backup options when initial home delivery fails. Empirical studies in\nthe UK (McLeod and Cherrett, 2009; Song et al., 2009; Edwards et al., 2010; McLeod et al.,\n2011; Song et al., 2013) indicate that pickup points can signi\ufb01cantly reduce delivery truck\nroutes by minimizing redelivery attempts. They also reduce customers\u2019 travel distances by\neliminating the need to retrieve packages from distant depots when redelivery is unsuccessful.\nPickup points enable the consolidation of orders at fewer locations, reducing the distance\ndelivery trucks travel. Vogt et al. (2007) formalize this as a routing-allocation problem,\nin which customers are divided into two groups: those receiving home deliveries and those\nassigned to visited vertices (e.g., pickup points), to minimize the combined routing and\nallocation costs. They propose a tabu search heuristic for solving this problem. In a multi-\ndepot, two-echelon variant of this problem, Zhou et al. (2018) use a hybrid genetic algorithm\nto optimize routes and allocations.\nThis problem can also yield emission reductions by\n6\ntreating routing costs as truck emissions and allocation costs as customers\u2019 travel emissions.\nFocusing on minimizing total delivery emissions, Goodchild et al. (2018) use a continuous\napproximation approach to estimate last-mile emissions, comparing two extreme cases: when\nall orders are delivered to homes versus when all are directed to pickup points. Using real-\nworld data from a logistics provider in Poland, Iwan et al. (2016) compare these scenarios,\nreporting a tenfold reduction in truck emissions when all orders are directed to pickup points\nrather than homes.\nMultiple studies have focused on optimizing pickup point networks, examining their lo-\ncations and capacities (Deutsch and Golany, 2018; Janjevic et al., 2019; Lin et al., 2020;\nXu et al., 2021; Rautela et al., 2022; Peppel and Spinler, 2022; Peppel et al., 2024; Zadeh\net al., 2026). Wang et al. (2025a) further emphasize that network density and infrastruc-\nture are critical to making pickup point delivery operationally e\ufb03cient. A case study by van\nDuin et al. (2020) further indicates that pickup points can improve robustness to operational\nuncertainty while reducing truck emissions.\nAlthough customers often show a willingness to use pickup points for sustainability rea-\nsons (Caspersen and Navrud, 2021; Caspersen et al., 2022; Biancolin and Rotaris, 2024),\ncustomers\u2019 delivery preferences and transportation choices signi\ufb01cantly a\ufb00ect environmen-\ntal bene\ufb01ts. Studies across various geographical locations indicate that pickup points tend\nto improve last-mile delivery sustainability in urban areas, whereas in rural and suburban\nregions, home delivery is more sustainable due to limited access to pickup points. For exam-\nple, case studies in Belgium (Rai et al., 2020; Beckers and Verhetsel, 2021; Mommens et al.,\n2021) demonstrate that more than half of the population cannot reach pickup points within\nwalking distance. Moreover, even in capital city regions with a high density of pickup points,\nnearly half of customers use cars to collect their orders, as evidenced by studies in Australia\n(Collins, 2015), Sweden (Liu et al., 2019), and Belgium (Rai et al., 2020). Using real ship-\nment data from Oslo, Pinchasik et al. (2025) show that although replacing home delivery\n7\nwith parcel-locker delivery can reduce total carbon dioxide emissions by about 13\u201332%, the\nnet bene\ufb01ts are highly sensitive to customers pickup-trip mode and travel distance. Niemei-\njer and Buijs (2023) conclude in their study conducted in the Netherlands that customers\u2019\ntravel emissions can be so signi\ufb01cant that it may be more sustainable to limit pickup point\ndelivery to customers living within walking distance or even to remove this option in regions\nwhere delivery trucks have transitioned to renewable fuel.\nTo summarize, the literature shows that pickup point delivery can substantially reduce\nlast-mile emissions through route consolidation. Still, these gains depend on customer loca-\ntions, travel behavior, and the design of the pickup point network. Existing studies largely\nrely on real-world case analyses, whereas formal optimization models typically assume static\nsettings, such as unrestricted access to all pickup points or comparisons between full home\ndelivery and full pickup point delivery. What remains unexplored is a dynamic, operational\nperspective that accounts for the sequential arrival of customers and stochastic customer\nchoices. This gap motivates our study, which proposes a di\ufb00erentiated pickup point o\ufb00ering\npolicy that explicitly captures these dynamic trade-o\ufb00s.\n2.2. Customer choice control in last-mile delivery\nSteering customer choices to improve operational e\ufb03ciency is commonly achieved through\ndemand management tools. It has been shown to enhance both environmental performance\nthrough reductions in emissions, waste, and energy consumption (Agatz and Fleischmann,\n2023), as well as to increase revenues and reduce operational costs (see Klein et al. (2020),\nWa\u00dfmuth et al. (2023), and Fleckenstein et al. (2023) for recent reviews). In the context\nof last-mile delivery, two main approaches have received signi\ufb01cant attention: time slot\nmanagement and delivery option o\ufb00ering.\nIn time slot management, service providers in\ufb02uence customer demand across delivery\nwindows to balance workloads over time and consolidate geographically proximate orders\n8\nthat might otherwise be assigned to separate routes. Demand steering is typically imple-\nmented through di\ufb00erentiated delivery fees, restricted time slot availability, or monetary\nincentives for rescheduling deliveries (Campbell and Savelsbergh, 2006; Agatz et al., 2011;\nYang et al., 2016; Klein et al., 2019; Estrada-Moreno et al., 2019; Ulmer, 2020; Yildiz and\nSavelsbergh, 2020). Closely related are approaches that di\ufb00erentiate shipping fees based on\ncustomers geographic locations, thereby shaping demand geographically to maximize service\npro\ufb01t (Afsar et al., 2021; Afsar, 2022).\nA second line of work focuses on the delivery option o\ufb00ering, in which service providers\nsteer demand toward pickup point delivery rather than home delivery. Zhou et al. (2018),\nTilk et al. (2021), and Dumez et al. (2021) study this problem in static, deterministic set-\ntings, in which the provider jointly selects the delivery mode (home delivery or pickup point\ndelivery) and the delivery time slot for each order while accounting for customer preferences.\nThese studies employ a hybrid genetic algorithm (Zhou et al., 2018), an exact branch-price-\nand-cut algorithm (Tilk et al., 2021), and a large neighborhood search heuristic (Dumez\net al., 2021), respectively. Extending this line of research, Galiullina et al. (2024) consider\na static stochastic setting in which customers are incentivized in advance to choose pickup\npoint delivery, although their \ufb01nal choices remain uncertain. They propose exact and heuris-\ntic branch-and-bound methods to solve this problem.\nOnly a limited number of studies address customer choice control in dynamic and stochas-\ntic environments. Akkerman et al. (2025) model delivery option selection as a Markov de-\ncision process and use deep reinforcement learning to learn a pricing policy that in\ufb02uences\ncustomer choice between home delivery and one of the \ufb01ve nearest pickup points. Their ap-\nproach yields an 8.1% cost reduction compared to a static discounting policy. Mancini and\nGansterer (2025) study the assignment of pickup points to customers to maximize provider\npro\ufb01t. Di\ufb00erent from our work, they assume mandatory acceptance of assigned pickup points\nand allow penalized delivery delays, without accounting for distance-dependent customer ac-\n9\nceptance or environmental impacts.\nThe delivery option o\ufb00ering has also been implemented to manage limited pickup point\ncapacity. Wang et al. (2025b) propose penalizing late pickups to improve locker availabil-\nity. Sailer et al. (2024) study a dynamic and stochastic setting in which pickup points are\nselectively excluded from the choice set to account for uncertain pickup times and locker oc-\ncupancy. In contrast, our approach restricts the o\ufb00er to a single pickup point per customer\nand explicitly models customer acceptance as a function of distance, thereby capturing the\ntrade-o\ufb00between consolidation bene\ufb01ts and customer travel emissions in a dynamic setting.\nTo summarize, we contribute to the literature in three ways. First, we study pickup point\no\ufb00ering in dynamic settings with sequential customer arrivals and stochastic acceptance be-\nhavior, which remain largely unexplored in the literature. Second, we explicitly account\nfor environmental trade-o\ufb00s by incorporating distance-dependent customer travel emissions\nalongside delivery routing emissions.\nThird, from a methodological perspective, we pro-\npose an algorithmic framework for the dynamicstochastic pickup point o\ufb00ering problem that\ncaptures uncertainty, delayed cost realization, and interdependent customer decisions.\n3. Di\ufb00erentiated pickup point o\ufb00ering for next-day delivery\nWe consider a next-day delivery service within a de\ufb01ned region, with multiple pickup\npoints and a single depot served by a truck. The delivery planning process consists of two\nphases:\n1. Capture phase occurs during the day before delivery until the order acceptance cut-\no\ufb00time. During this phase, orders arrive dynamically. For each new order, a single\npickup point is o\ufb00ered, and customers randomly choose between the pickup point and\nhome delivery. The number and the location of upcoming orders are uncertain.\n2. Ful\ufb01llment phase occurs on the delivery day when the delivery truck\u2019s route is exe-\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper \"Di\ufb00erentiated Pickup Point O\ufb00ering for Emission Reduction in Last-Mile Delivery\"\n\n#### Main Contributions:\n- **Proposed Policy**: The authors introduce a novel dynamic policy called Di\ufb00erentiated Pickup Point O\ufb00ering (DPO) aimed at minimizing total last-mile carbon emissions by jointly reducing delivery truck and customer travel emissions.\n- **Environmental Impact Analysis**: They demonstrate that DPO can reduce total carbon emissions by up to 9% compared to home-only delivery, and by an average of 2% relative to alternative policies like unrestricted pickup point choice or nearest pickup point assignment. \n- **Policy Scalability**: The study proposes a scalable graph-based state representation that captures the spatial and consolidation structure of the system, enabling effective learning of dynamic pickup point o\ufb00ering policies under uncertainty.\n- **Dynamic Trade-offs**: The research explicitly accounts for environmental trade-offs between order consolidation efficiency and customer proximity to pickup points. This is achieved by modeling customer arrivals and choices as a Markov Decision Process (MDP).\n\n#### Methodology:\nThe study employs reinforcement learning, specifically Proximal Policy Optimization (PPO), to design DPO policies that dynamically select one pickup point for each arriving customer based on the current system state while accounting for its impact on future consolidation.\n\n#### Key Findings:\n- **Customer Behavior**: Customers are more likely to choose home delivery when given unrestricted choices among all pickup points. However, under DPO, customers tend to prefer pickup point delivery over home delivery.\n- **Emission Reduction**: The proposed policy consistently outperforms a baseline PPO solution with a flattened state representation and a graph-based model without attention mechanisms.\n- **Operational Settings**: DPO is most effective in dense urban environments with many pickup points and short inter-location distances. It also performs well when customers are less inclined to choose pickup point delivery over home delivery.\n\n#### Conclusions:\nThe study provides valuable insights into how dynamic, customer-centric policies can significantly reduce emissions in last-mile delivery scenarios. By explicitly accounting for the sequential nature of customer arrivals and choices, DPO offers a robust framework that could be applied to various operational settings. The research highlights the importance of considering both delivery truck and customer travel emissions when designing sustainable pickup point o\ufb00erings.\n\n### Summary:\nThis paper introduces a novel Di\ufb00erentiated Pickup Point O\ufb00ering (DPO) policy designed to minimize total last-mile carbon emissions by jointly reducing delivery truck and customer travel emissions. The authors demonstrate the effectiveness of DPO through computational experiments, showing reductions in total emissions up to 9% relative to home-only delivery and an average reduction of 2%. The proposed solution employs reinforcement learning techniques and explicitly accounts for environmental trade-offs between consolidation efficiency and customer proximity to pickup points. The study identifies dense urban environments with many pickup points as the most effective settings for DPO, underscoring its importance in sustainable last-mile delivery practices."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Kakugo: Distillation of Low-Resource Languages into Small Language\nModels\nPeter Devine,\nMardhiyah Sanni,\nFarid Adilazuarda,\nJulieta Gil Loizaga,\nBarry Haddow\nSchool of Informatics, University of Edinburgh\nEdinburgh, UK\n{pdevine2, msanni, farid.adilazuarda, julieta.loizaga, bhaddow}@ed.ac.uk\nAbstract\nWe present Kakugo, a novel and cost-effective\npipeline designed to train general-purpose\nSmall Language Models (SLMs) for low-\nresource languages using only the language\nname as input. By using a large teacher model\nto generate synthetic prompts and translate in-\nstruction datasets, we produced training data\nand SLMs for 54 low-resource languages. Eval-\nuations across a diverse set of general natu-\nral language processing tasks, including trans-\nlation, classification, and question answering,\ndemonstrate that our pipeline consistently im-\nproves performance over base models. With\na total generation and training cost of under\n$50 per language, Kakugo offers an accessible\nmethod for communities to develop language-\nspecific AI.\n1\nIntroduction\nSmall Language Models (SLMs) offer a cheaper,\nfaster, and more data-sovereign alternative to Large\nLanguage Models (LLMs) (Schick and Sch\u00fctze,\n2021), achieving remarkable performance across\ndiverse NLP tasks (Abdin et al., 2024; Chae and\nDavidson, 2025; Zheng et al., 2025; Pham et al.,\n2025; Li and Zhang, 2025).\nHowever, this ef-\nficiency often comes at the cost of multilingual\ncapability. While popular SLMs support many\nlanguages (Gemma, 2025; Qwen, 2025; Bakouch\net al., 2025; Hern\u00e1ndez-Cano et al., 2025), they typ-\nically underperform in low-resource settings com-\npared to their larger counterparts (Romanou et al.,\n2024). Although the community has attempted to\nfill this void with language-specific SLMs (Awarri\nTechnologies et al., 2025; Ogueji et al., 2021; Ng\net al., 2025; Tonja et al., 2024; Khanuja et al.,\n2021), these efforts often require expensive special-\nist teams and intimate knowledge of data sources.\nTo democratize access to language-specific mod-\nels, we present Kakugo, a general-purpose pipeline\nthat trains a monolingual SLM using only the lan-\nguage name as input.\nKakugo uses model distillation (Hinton et al.,\n2015; Shirgaonkar et al., 2024; Wang et al.,\n2021) to automate the entire development pro-\ncess. We use a large teacher model, specifically\nGPT-OSS 120B (OpenAI, 2025b), to generate di-\nverse synthetic prompts, reasoning traces, and re-\nsponses, while also translating high-quality instruc-\ntion datasets into the target language. This data\nis then used to fine-tune a student model (IBM\u2019s\nGranite 4 Micro (IBM Research, 2025)).\nWe evaluate Kakugo on 54 low-resource lan-\nguages across translation (FLORES NLLB Team\net al., 2024), classification (SIB-200 Adelani et al.,\n2024), and question answering (GlobalMMLU\nSingh et al., 2025) (Belebele Bandarkar et al.,\n2024).\nOur experiments demonstrate that our\npipeline consistently improves performance over\nbase models across a wide range of tasks.\nWe make three key contributions:\nFirstly, we propose a novel, fully automated\npipeline for generating generalist synthetic data,\ndemonstrating that combining generated reason-\ning traces with translated data is more effective for\nSLM training than any individual method.\nSecondly, we release open-source training\ndatasets and monolingual SLMs for 54 languages.\nNotably, for many of these languages (e.g., Bashkir,\nMaori, Mongolian), these represent the first gener-\nalist monolingual conversational SLMs ever devel-\noped.\nThirdly, we release the Kakugo pipeline itself,\nenabling the creation of language-specific SLMs\nfor less than $50 per language. This allows low-\nresource communities to affordably develop AI\ntools that support their native languages.\nThe code for reproducing our pipeline, as well\nas links to the models and datasets produced in\nthis work can be found at https://github.com/Peter-\nDevine/kakugo.\narXiv:2601.14051v1  [cs.CL]  20 Jan 2026\nFigure 1: The Kakugo pipeline. Given a language name (e.g., \u201cJavanese\u201d), a teacher model generates synthetic\nconversational and instruction data which are used to train an SLM in that language.\n2\nRelated work\nLLMs are effective data generators for downstream\ntasks like intent classification, recommendation,\nand inference (Dai et al., 2025; Sahu et al., 2022;\nLiu et al., 2024). Distilling LLM outputs into SLMs\nsignificantly reduces inference costs while main-\ntaining performance (Shirgaonkar et al., 2024).\nSynthetic data generation approaches range from\nsimple frameworks such as Self-Instruct (Wang\net al., 2023) and Alpaca (Taori et al., 2023),\nwhich use input-output mimicry, to more com-\nplex approaches such as Explanation Tuning in\nOrca (Mukherjee et al., 2023) and Distilling Step-\nby-Step (Hsieh et al., 2023) which demonstrate\nthat supervising student models on reasoning traces\nimproves performance and data efficiency. These\nforms of model distillation have been shown to\nbe very useful for reducing inference time and\ncost while maintaining performance across many\ntasks (Shirgaonkar et al., 2024).\nNotably, Nemotron-4 was trained on 98% syn-\nthetic data and achieved improved evaluation\nscores in a broad array of tasks (Adler et al., 2024),\nthough this model was not specifically optimized\nfor low-resource languages.\nIn the multilingual domain, models such as\nGoldfish (Chang et al., 2024), EMMA-500 (Ji\net al., 2024), and MaLA-500 (Lin et al., 2024)\nhave shown that pre-training on low resource lan-\nguage web text can improve down-stream task per-\nformance. Moreover, instruction tuning projects\nlike BLOOMZ (Muennighoff et al., 2023) and\nAya (\u00dcst\u00fcn et al., 2024) have highlighted the effi-\ncacy of cross-lingual generalization through mul-\ntitask fine-tuning. While these efforts rely heavily\non human-curated or translated datasets, emerging\nsynthetic methods for low-resource languages have\nexplored \u201creverse instruction\u201d generation, as seen\nin Humpback (Li et al., 2023) and MURI (K\u00f6ksal\net al., 2024), which create instructions from native\nweb text to mitigate translation artifacts.\nThis approach of generating training data using\nan LLM has proved effective for training models\nin languages besides English, including low re-\nsource languages, for various domains such as legal\nNLP (Ghosh et al., 2023), translation (Yong et al.,\n2024), text classification (Anikina et al., 2025),\nfact checking (Chung et al., 2025), question an-\nswering (Namboori et al., 2023), and dialogue sum-\nmarization (Suresh et al., 2025). Moreover, pre-\ntraining on synthetically generated data has shown\nto improve down-stream evaluation results (Wang\net al., 2025).\nHowever, none of these approaches result in a\ngeneralist low-resource language model which can\nbe used by a wide variety of AI developers. Our\nwork seeks to distill the generalist abilities of an\nLLM into an SLM for a given language without\nany strict task-based restrictions.\n3\nKakugo model generation pipeline\nThe goal of this research was to build a univer-\nsal pipeline where developers can produce target-\nlanguage SLMs simply by specifying the language\nname.\nOur pipeline works in two main steps: training\ndata creation and model training. This section de-\ntails how we both create our data and then use it to\ntrain our model.\n3.1\nData creation\nTo create training data, we use a teacher model that\nis able to generate output in the target language.\nWe select GPT-OSS 120B (OpenAI, 2025b) as our\nteacher. Preliminary qualitative analysis confirmed\nits proficiency in generating coherent text across\nour target languages, validating its suitability for\ndistillation without extensive few-shot tuning.\nWith this model, we generate data in two main\nways: (1) generating prompts and then responses\nbased on those prompts, and (2) translating existing\nEnglish conversation data into the target language.\n3.1.1\nGenerating synthetic prompts\nWe generate our prompts using three different\nmethods: topic-based, scenario-based, and context-\nbased prompt generation. We describe these meth-\nods in detail below.\nTopic-based prompt generation\nIn order to gen-\nerate training data on a wide range of topics, both\ngeneral and language specific, we take inspiration\nfrom the Nemotron 4 (Adler et al., 2024) prompt\ngeneration pipeline. We start with a set of seed\ntopics then prompt the teacher model to automati-\ncally generate 20 macro-topics related to each seed\nand then 10 topics related to each macro-topic. In\norder to include both general and language or cul-\nture specific data in our training, we use two sets\nof seeds: 8 language-agnostic seeds (e.g. \u201cprac-\ntical skills\u201d, \u201csciences\u201d) and 8 language-specific\nseeds (e.g. \u201c{language_name} speaking places\u201d,\n\u201c{language_name} society\u201d). The teacher LLM\nis then allowed to interpret what is meant by these\nseeds within the confines of the topic-generating\nsystem message (listed in Appendix A). We then\ncombine all seeds, macro-topics, and topics as a sin-\ngle set of topics which we use to prompt the teacher\nmodel to generate 3 conversational prompts in the\ntarget language for each topic.\nScenario-based prompt generation\nTo generate\npractical training data for realistic situations where\nan SLM will be used, we generate prompts based\non specific scenarios. We first prompt the teacher\nmodel to generate 30 broad, general scenarios in\nwhich one could imagine someone using an AI as-\nsistant. We generated both language-specific and\nlanguage-agnostic scenarios by either informing\nor not informing the teacher model that the user\nis a speaker of the target language. We then gen-\nerate 30 detailed scenarios related to each broad\nscenario. We define our final set of scenario seeds\nas the union of the 30 broad scenarios and the gen-\nerated detailed scenarios. We iterate through this\ncombined pool, prompting the teacher model to\ngenerate 5 target-language prompts for each indi-\nvidual scenario.\nContext-based prompt generation\nWe antici-\npate that low-resource language SLMs will be used\nfor many text-based tasks such as translation or\nsummarization, so we generate prompts that relate\nto a piece of text in some way. We first randomly\nsample up to 10,000 texts in the target language\nfrom the popular FineWeb2 corpus (Penedo et al.,\n2025). Taking the first 1,000 tokens of each text,\nwe prompt the teacher model, through the system\nmessage, to generate up to 3 prompts that ask an\nAI assistant to do one of the following randomly\nselected tasks: to translate, summarize, improve,\nclassify, or answer a question about the text. These\ntasks are uniformly distributed except the question\nanswering task, which we over-weight by a factor\nof 4 due to its potential usefulness in deploying\nour final model for RAG pipelines (Lewis et al.,\n2020). To create our final context-based prompts,\nwe prepended the text that was used to generate\neach prompt to the prompt itself.\nSimilarly to Nemotron (Adler et al., 2024), our\ninitial tests showed that the generated prompts us-\ning all three methods were often simplistic or short.\nTherefore, we randomly sample half of each set\nof prompts and prompt the teacher model to make\nthem longer or more complex.\n3.1.2\nGenerating responses from prompts\nWe input all of our synthetic prompts into our\nteacher model with a standard system message in-\ncluding instructions that the model must reply in\nthe target language unless otherwise instructed.\nSince we use a reasoning-based teacher model\n(GPT-OSS 120B) which automatically produces a\n\u2018chain-of-thought\u2019 reasoning trace before its final\nanswer, we capture these naturally occurring rea-\nsoning traces alongside the final response without\nrequiring specific engineering of the system prompt\nto elicit them.\nMore information on the exact seeds and system\nprompts we used to create our generated prompts\nand responses can be found in Appendix A.\n3.1.3\nTranslating data\nTo increase the diversity of our synthetic data and\nto include more general instruction following data,\nwe also generate data by translating an existing\nhigh quality English instruction dataset. We select\nthe \u201c7M_core\u201d (1.48M row) subset of the Infini-\ntyInstruct (Li et al., 2025) dataset which is a cu-\nrated and filtered set of conversational English and\nChinese prompt and response data based on many\nhigh-quality instruction datasets. We filter out non-\nEnglish examples and select the first 15,000 rows\nof the dataset.\nWe translate this data as Python lists of conver-\nsations using our teacher model with the system\nmessage given in Appendix B, then parse these\ntranslated string responses back to Python lists,\nremoving any instances that were incorrectly for-\nmatted.\nTo ensure data quality, we remove conversations\nwhere the translated token count is less than 75%\nor exceeds 25 times the original length. These\nasymmetric filtering thresholds prevent omitting\ncorrectly translated data by accommodating the\nhigh \u201ctoken fertility\u201d and inefficient tokenization\noften observed in low-resource languages or rare\nscripts (Rust et al., 2021; Ahia et al., 2023). While\ntranslation models are prone to repetition (Mi et al.,\n2016), our requirement for correctly formatted\nPython lists in the translated output effectively\nexcludes such instances, as repetitive strings are\nunlikely to maintain the syntax necessary to pass\nour formatting checks, making our 25 times length\nthreshold effective at only filtering out truly repeti-\ntive conversations.\nWe use this filtered data as our translated training\ndata.\n3.2\nModel training\nFor our final dataset, we combine our generated\nand translated data into a single shuffled dataset to\ntrain on.\nWe structure our training data to add controllable\nreasoning capabilities to the student model. For the\ngenerated subset (which contains reasoning traces),\nwe prepend a specific system instruction enabling\n\u201cthinking mode\u201d. For the translated subset (which\nlacks reasoning traces), we use a standard sys-\ntem instruction. This allows the student model to\nlearn a conditional association: it generates <think>\n</think> traces and reasoning steps only when the\n\u2019thinking mode\u2019 system prompt is present, effec-\ntively distilling the teacher\u2019s reasoning ability into\na toggleable feature, similar to Qwen 3\u2019s \u201cthinking\nmode\u201d and \u201cnon-thinking mode\u201d (Qwen, 2025).\nWe train our student model on this data using\nLlama Factory (Zheng et al., 2024) by full fine-\ntuning for one epoch with the intention that we\nstrengthen the generalist abilities of our model with-\nout overfitting to our training data. Since full fine-\ntuning frequently outperforms low rank training\napproaches (e.g. LoRA) when the trained model\nis applied to multiple tasks (Shuttleworth et al.,\n2024), we choose to do full finetuning as our train-\ning method in order to create a more generalist\nmodel.\nIn our experiments, we chose to train Granite 4\nMicro (IBM Research, 2025) as our student model\ndue to its open-source license and its superior per-\nformance on evaluation benchmarks compared to\nsimilarly sized models.\nThe full set of training hyperparameters can be\nfound in Appendix C.\n4\nExperiments\nIn order to compare the effects of the different data\ngeneration techniques in our pipeline, we gener-\nated data and used a variety of data subsets to train\nmodels on a fixed token training budget to evalu-\nate them on our low-resource language evaluation\nbenchmarks.\n4.1\nLanguage selection\nWe targeted a wide variety of languages balancing\nlinguistic diversity with evaluation benchmark cov-\nerage. We selected the following 14 languages\nfor in-depth evaluation across our data subsets:\nAmharic, Bengali, Galician, Guarani, Javanese,\nKyrgyz, Lao, Maori, Mongolian, Scottish Gaelic,\nSinhala, Swahili, Telugu, and Yoruba.\nWe also trained models in a further 40 languages,\nbut we performed less in-depth analyses of these\nmodels, only training a single \u201cfull\u201d model for\neach. These languages were selected by finding\nlanguages which are both included in FLORES 200\nand have between 50,000 and 1,000,000 documents\nin FineWeb2. This was done to target languages\nwhich are low resource, but not so low resource\nthat our teacher model could not reliably generate\ntraining data in that language.\nThese 40 languages were: Aranese, Assamese,\nAsturian, Bashkir, Cebuano, Central Kurdish, Chu-\nvash, Eastern Yiddish, Egyptian Arabic, Faroese,\nHaitian Creole, Hausa, Igbo, Irish, Kinyarwanda,\nLhasa Tibetan, Luxembourgish, Maltese, Mizo,\nNajdi Arabic, Northern Kurdish, Nyanja, Papia-\nmento, Plateau Malagasy, Rundi, Samoan, Shona,\nNum langs\nTask\nBelebele\n34\nClosed QA\nGlobalMMLU\n10\nOpen QA\nSIB200\n53\nClassification\nFLORES xx-en\n54\nTranslation\nFLORES en-xx\n54\nTranslation\nTable 1: Evaluation tasks and how many languages are\ncovered by each task in our evaluation.\nSindhi (Arabic script), South Azerbaijani, Southern\nPashto, Southern Sotho, Sundanese, Tajik, Tatar,\nTigrinya, Turkmen, Uyghur, Welsh, Xhosa, Zulu.\nThis resulted in the creation of datasets and mod-\nels for 54 low-resource languages.\n4.2\nData subsets\nWe ran the data generation pipeline, as described\nin Section 3.1, with each of the above languages.\nWe then trained student models on the following\nsubsets of data as a comparison between training\ndatasets:\n\u2022 Topic - Topic prompt generated data only\n\u2022 Scen - Scenario prompt generated data only\n\u2022 Cont - Context prompt generated data only\n\u2022 Gen - All generated data (Topic+Scen+Cont)\n\u2022 Tran - Translated data only\n\u2022 Gen+Tran - Generated and translated data\n\u2022 GenReas + Tran (tok-lim) - Generated data\nwith reasoning traces and translation data, lim-\nited at the same number of tokens as all above\ndatasets\n\u2022 GenReas + Tran (ex-lim) - Generated data\nwith reasoning traces and translation data,\nwith the number of examples and non-\nreasoning tokens limited to the same as\nGen+Tran, but with added reasoning tokens\nWe counted the number of tokens of each subset,\nfound the minimum per language, and then sam-\npled the subsets to create seven comparable subsets\nwith the same number of tokens for each language.\nOver all languages, this averaged to 6.5 million to-\nkens per subset, with the lowest token count being\n3.2 million (Galician) and the highest being 14.9\nmillion (Telugu).\nAdditionally, with the Gen (w. Reasoning) +\nTran (example limited) model, we compared other\nmodels against the practical situation where reason-\ning traces were freely available after generating our\nsynthetic data. In this case, we kept the number\nof tokens and examples from our generated data\nequal to other models, but added reasoning traces\non top of other data. This resulted in datasets that\nhad between 5-11% more tokens than their token\nlimited counterparts. This was practical in situa-\ntions such as ours, where data generation costs are\nmore expensive than training costs.\nFinally, we trained our full model on all the avail-\nable data without limiting token count, which we\ncalled \u201cGenReas+Tran (full)\u201d. This resulted in\ndatasets of between 23 and 77 million tokens, or\nbetween 34 and 50 thousand unique examples.\n4.3\nEvaluation benchmarks\nWe evaluated our models on 4 diverse multilingual\nbenchmark datasets, Belebele (Bandarkar et al.,\n2024), FLORES (NLLB Team et al., 2024), Glob-\nalMMLU (Singh et al., 2025), and SIB200 (Adelani\net al., 2024). These datasets offer good coverage\nover our 14 languages and cover 5 distinct tasks.\nThese tasks cover 5 important use-cases of a\ngeneralist low-resource language SLM: translation\nfrom English to that language, translation to En-\nglish from that language, classification of text, open\nquestion answering, and closed question answer-\ning.\nThe evaluations on these tasks were done with a\n3-shot learning template, where the first 3 instances\nof each dataset are removed from the test set and\nused as example responses. The temperature used\nwas 0 and the repetition penalty was set to 1.0 to\nprevent the log-probabilities of candidate answer\ntokens (e.g., in single token multiple-choice tasks)\nfrom being penalized simply because they appeared\nin the few-shot examples.\nWe evaluate using accuracy for Belebele, Glob-\nalMMLU, and SIB200, and CHRF++ (Popovi\u00b4c,\n2015) for the FLORES tasks. All evaluations used\n\u201cnon-thinking mode\u201d (i.e. no reasoning trace is\ngenerated).\n4.4\nManual evaluation\nFurther to our automatic analysis, we also con-\nducted a manual preference analysis between the\noutputs of both our final trained Kakugo model (de-\nnoted \"Gen (w. Reasoning) + Tran (full)\" in our\nother evaluation) and the base student model (Gran-\nite 4 Micro) to conversational prompts in three\nlow resource languages: Galician, Javanese, and\nYoruba.\nWe first took the 80 initial prompts from the MT-\nBench conversation benchmark (Zheng et al., 2023)\nand translated them using GPT-5 (OpenAI, 2025a)\nBelebele\nGlobal MMLU\nSIB200\nFLORES xx-en\nFLORES en-xx\nBase model\n41.8\n33.7\n66.5\n37.6\n20.0\nTopic only*\n39.8\n34.4\n65.6\n29.9\n18.5\nScenario only*\n39.4\n34.1\n64.7\n31.7\n18.5\nContext only*\n43.1\n33.5\n67.1\n36.3\n18.1\nGen (Scen+Cont+Topic)*\n42.8\n34.4\n66.7\n34.2\n18.8\nTranslated only*\n41.6\n34.0\n62.2\n32.8\n18.3\nGen + Tran*\n42.4\n34.1\n65.5\n35.3\n18.4\nGen (w. Reasoning) + Tran (token limited)*\n41.7\n33.5\n65.6\n38.3\n18.2\nGen (w. Reasoning) + Tran (example limited)\n42.3\n33.3\n66.2\n38.9\n19.0\nGen (w. Reasoning) + Tran (full)\n48.8\n35.3\n74.9\n44.9\n30.7\nTable 2: Average score across our 14 \u201cin-depth\u201d languages for each dataset. Reported scores are percentage accuracy\nfor all tasks except FLORES tasks, which are CHRF++ scores. Models marked * have been trained on the same\nnumber of tokens for each language.\ninto the three languages. Then a native speaker\nof each of the chosen languages (all authors of\nthis paper) corrected any incorrect or unnatural\ntranslations. These prompts were then input to\nboth the Kakugo model and base student model\nwith a temperature of 0.0 (as recommended by the\nGranite documentation) and the repetition penalty\nset to 1.05 to prevent repetition of words when\noutputting low resource language script.\nThese responses were randomly shuffled, thus\nmaking them double-blind, and the native speaker\nchose which of the two responses best answered\nthe prompt, with the additional option to instead\nsay that the responses were of equal quality.\n5\nResults\nAverage scores across our 14 in-depth languages\nare in Table 2 and full results are in Table 3.\nWhen comparing each single method of data cre-\nation (i.e. Topic only, Scenario only, Context\nonly, and Translated only), we find that differ-\nent subsets lead to better performance on different\ntasks and languages. Since our goal is to create a\ngeneralist SLM that can perform many tasks, we\nfind that training data created using multiple data\ngeneration methods to be the most appropriate for\nour Kakugo pipeline.\nComparing Gen + Tran to GenReas + Tran\n(tok-lim) shows no consistent improvement when\nreasoning traces replace target-language tokens.\nHowever, adding traces on top of target language\ndata (GenReas + Tran (ex-lim)) provides small\nperformance gains. Therefore, we find that develop-\ners should prioritize target-language tokens when\nunder training budget constraints, but leverage rea-\nsoning traces if they are available as generation\nby-products.\nSummarized evaluation results of all languages\ncan be found in Table 4 and full results of the base\nmodel and our full model for all languages can be\nfound in Appendix D.\nOverall, we find that the Gen (w. Reasoning) +\nTran (full) model achieves an accuracy increase of\n9.9% on Belebele, 1.9% on GlobalMMLU, 8.8%\non SIB200, and 8.5 CHRF++ on FLORES (xx-\nen) and 13.1 CHRF++ on FLORES (en-xx), av-\neraged across our 54 languages compared to the\nbase model. This indicates that our pipeline is ef-\nfective at improving performance across a diverse\nset of tasks, suggesting applicability as a generalist,\nmulti-task SLM in low resource languages.\nWe also find performance jumps from most\ntoken-limited to full models, suggesting insuffi-\ncient training data in token-limited settings. This\nleaves the Pareto-optimal data size for low-resource\nSLM distillation an open question.\nPerformance gains vary by language. While lan-\nguages such as Javanese and Yoruba improved sig-\nnificantly, the base model achieved better evalua-\ntion results for languages like Telugu and Bengali.\nWe find an inverse relationship between initial base\nmodel performance and improvements to the base\nmade by our Kakugo pipeline, Pearson correlations\nreveal moderate to high anti-correlation across four\nof five tasks (0.04, -0.60, -0.86, -0.37, -0.38 for\nBelebele, GlobalMMLU, SIB200, FLORES xx-en,\nand FLORES en-xx, respectively). Consequently,\nwe find Kakugo is most effective on languages\nwhere existing SLMs struggle but may yield di-\nminishing returns where the student model already\nperforms well.\nThe results of the manual preference analysis\nbetween responses from our Kakugo model and the\nbase student model can be found in Figure 2.\nWe find a consistent preference for most re-\nBase\nTopic\nScen\nCont\nGen\nTran\nGen + Tran\nGenReas +\nTran (tok-lim)\nGenReas +\nTran (ex-lim)\nGenReas +\nTran (full)\nSIB200\nAmharic\n26.4\n27.9\n26.9\n24.9\n26.9\n26.4\n27.4\n26.9\n29.4\n51.2\nBengali\n85.6\n83.1\n85.1\n79.6\n88.6\n83.6\n86.6\n84.6\n83.1\n85.6\nGalician\n85.6\n85.6\n84.1\n85.1\n83.1\n71.6\n77.1\n81.1\n76.6\n87.1\nGuarani\n69.2\n70.6\n66.7\n68.2\n69.2\n62.7\n69.2\n68.7\n67.7\n77.1\nJavanese\n72.6\n77.1\n80.1\n81.1\n77.6\n75.1\n74.6\n79.1\n77.1\n83.1\nKyrgyz\n75.1\n76.1\n72.6\n78.1\n76.6\n79.1\n76.1\n76.1\n71.1\n81.1\nLao\n63.2\n47.3\n52.2\n49.8\n47.3\n52.2\n51.7\n51.7\n52.7\n67.7\nMaori\n49.3\n50.2\n48.8\n62.2\n53.7\n46.3\n53.2\n57.2\n57.2\n68.2\nMongolian\n63.2\n66.2\n67.2\n69.7\n70.6\n65.7\n72.6\n72.6\n70.1\n81.1\nScottish Gaelic\n59.7\n62.7\n62.7\n58.2\n61.2\n47.3\n52.2\n53.2\n58.2\n64.7\nSinhala\n58.2\n55.2\n53.7\n61.2\n61.2\n57.7\n65.7\n54.2\n67.7\n73.6\nSwahili\n73.6\n71.6\n63.2\n76.1\n72.1\n57.2\n64.7\n67.7\n72.6\n74.6\nTelugu\n86.6\n84.6\n83.1\n84.6\n84.6\n84.1\n84.1\n83.1\n82.1\n86.1\nYoruba\n62.2\n59.7\n59.7\n60.7\n60.7\n61.7\n62.2\n62.2\n60.7\n67.2\nBelebele\nAmharic\n30.0\n29.5\n28.3\n29.9\n27.4\n30.2\n31.9\n29.1\n30.9\n36.8\nBengali\n64.5\n49.5\n48.9\n63.9\n63.2\n58.1\n63.9\n65.6\n64.4\n65.9\nGuarani\n33.3\n29.7\n30.8\n35.6\n31.5\n34.4\n33.6\n34.1\n32.9\n40.4\nJavanese\n50.8\n56.1\n56.5\n60.4\n60.9\n48.9\n45.8\n49.2\n48.0\n65.1\nKyrgyz\n47.4\n46.8\n47.7\n40.8\n52.8\n52.2\n51.7\n47.4\n47.6\n56.9\nLao\n30.9\n30.1\n32.0\n33.3\n32.9\n34.7\n35.5\n33.9\n36.6\n36.7\nMaori\n31.5\n31.9\n33.7\n33.0\n30.8\n32.1\n31.1\n32.0\n31.8\n35.0\nMongolian\n38.1\n37.1\n37.1\n39.5\n38.6\n37.8\n36.8\n36.3\n37.8\n47.0\nSinhala\n38.7\n39.5\n39.0\n44.3\n42.6\n39.6\n43.7\n41.4\n46.5\n52.4\nSwahili\n45.2\n39.0\n40.0\n43.9\n44.4\n38.5\n42.1\n40.9\n40.4\n54.6\nTelugu\n61.0\n56.1\n46.2\n58.6\n57.1\n60.5\n59.6\n57.7\n59.3\n60.8\nYoruba\n29.8\n32.0\n32.2\n33.6\n31.9\n32.3\n33.4\n33.2\n31.9\n34.3\nGlobal\nMMLU\nAmharic\n27.9\n28.0\n27.5\n28.5\n28.0\n28.0\n28.4\n28.6\n28.2\n29.4\nBengali\n40.5\n40.4\n39.9\n38.3\n40.8\n41.3\n41.2\n38.8\n40.0\n40.7\nKyrgyz\n33.8\n37.0\n36.5\n32.7\n37.1\n37.0\n37.6\n33.9\n33.7\n37.5\nSinhala\n30.4\n30.3\n30.5\n30.3\n30.9\n30.4\n30.3\n31.0\n31.0\n32.3\nSwahili\n32.9\n31.6\n32.5\n33.3\n32.6\n30.1\n30.2\n31.7\n31.2\n35.1\nTelugu\n38.9\n40.2\n38.9\n39.9\n39.6\n38.6\n37.9\n39.3\n37.7\n38.7\nYoruba\n31.1\n33.2\n32.9\n31.2\n31.9\n32.7\n32.9\n31.1\n31.2\n33.4\nFLORES\nxx-en\nAmharic\n18.8\n3.2\n8.9\n17.3\n17.1\n13.2\n17.0\n17.3\n17.7\n31.6\nBengali\n53.5\n0.7\n18.8\n49.4\n39.6\n33.0\n49.9\n52.2\n52.9\n51.1\nGalician\n63.5\n62.5\n62.6\n62.7\n62.0\n62.8\n62.9\n62.7\n62.5\n63.2\nGuarani\n28.2\n28.8\n28.3\n30.4\n28.7\n27.7\n29.4\n30.4\n30.6\n35.4\nJavanese\n44.9\n47.1\n47.7\n47.7\n48.4\n46.6\n47.2\n47.9\n47.9\n53.4\nKyrgyz\n38.3\n37.6\n36.9\n14.8\n29.5\n38.9\n33.9\n39.3\n39.4\n43.1\nLao\n34.1\n26.5\n26.3\n31.2\n31.2\n29.3\n28.7\n31.5\n33.6\n38.8\nMaori\n27.3\n26.6\n27.1\n29.3\n28.0\n26.5\n28.0\n29.8\n29.9\n38.9\nMongolian\n29.4\n32.5\n32.2\n34.1\n33.6\n31.6\n33.7\n34.7\n34.4\n42.8\nScottish Gaelic\n31.3\n33.6\n33.2\n36.0\n35.2\n28.0\n29.3\n31.6\n32.4\n44.3\nSinhala\n29.3\n1.9\n2.6\n31.0\n3.7\n5.5\n6.2\n29.9\n33.3\n41.7\nSwahili\n41.0\n39.9\n39.5\n42.8\n41.2\n36.5\n41.2\n42.0\n42.2\n52.8\nTelugu\n54.9\n48.2\n49.2\n47.3\n52.9\n48.0\n54.8\n54.9\n54.2\n54.2\nYoruba\n32.5\n29.9\n29.9\n33.7\n27.1\n31.8\n32.3\n32.7\n33.2\n36.9\nFLORES\nen-xx\nAmharic\n1.1\n0.5\n0.5\n0.5\n0.4\n0.6\n0.3\n0.2\n0.3\n8.4\nBengali\n37.4\n30.1\n31.4\n29.8\n31.2\n35.7\n34.7\n34.1\n35.1\n36.5\nGalician\n52.5\n52.1\n52.6\n52.5\n52.7\n52.0\n53.1\n52.6\n52.6\n54.0\nGuarani\n5.6\n8.0\n8.3\n9.7\n10.4\n11.7\n7.9\n7.6\n9.0\n22.1\nJavanese\n27.2\n34.3\n34.3\n32.3\n33.1\n32.6\n32.2\n32.8\n33.5\n43.8\nKyrgyz\n18.5\n18.7\n17.8\n17.2\n18.7\n19.6\n18.6\n14.9\n16.3\n28.6\nLao\n17.5\n5.0\n4.7\n2.8\n4.3\n4.3\n5.2\n8.9\n6.1\n19.4\nMaori\n11.4\n10.9\n12.4\n14.3\n13.2\n8.6\n9.0\n14.0\n14.9\n36.7\nMongolian\n8.7\n6.7\n6.1\n6.2\n6.7\n9.3\n7.3\n7.6\n7.7\n22.6\nScottish Gaelic\n10.4\n13.1\n10.6\n12.6\n11.1\n5.5\n5.9\n5.5\n7.6\n32.3\nSinhala\n13.2\n8.7\n9.2\n6.6\n9.2\n8.1\n10.1\n7.7\n12.4\n22.2\nSwahili\n20.1\n15.7\n14.9\n14.7\n16.0\n10.6\n14.8\n14.6\n12.6\n38.8\nTelugu\n40.2\n35.9\n36.4\n35.5\n36.2\n39.1\n38.4\n38.2\n39.4\n40.3\nYoruba\n15.7\n19.5\n19.2\n18.6\n19.6\n18.0\n19.6\n16.6\n18.0\n23.9\nTable 3: Full results of all trained models on their relevant tasks for our \u201cin-depth\u201d languages. Values are percentage\naccuracy for SIB200, Belebele, and GlobalMMLU, and CHRF++ for FLORES tasks. Bolded is the highest value in\neach row and underlined is the highest value within the models whose training data was kept constant.\nam\n5/5\ngd\n3/3\nmi\n4/4\nsu\n4/4\narz\n4/4\ngl\n2/3\nmn\n4/4\nsw\n5/5\nars\n2/4\ngn\n4/4\nmt\n4/4\nte\n1/5\nas\n4/4\nha\n5/5\nny\n4/4\ntg\n4/4\nast\n3/3\nht\n4/4\noc\n2/3\nti\n4/4\naz\n1/3\nig\n5/5\npap\n3/3\ntk\n3/3\nba\n3/3\njv\n4/4\nps\n4/4\ntt\n3/3\nbn\n2/5\nkmr\n3/3\nrn\n3/3\nug\n3/3\nbo\n2/4\nsdh\n4/4\nrw\n4/4\nxh\n4/4\nceb\n4/4\nky\n5/5\nsd\n4/4\nyi\n3/3\ncv\n0/2\nlb\n2/3\nsi\n5/5\nyo\n5/5\ncy\n3/3\nlo\n4/4\nsm\n3/3\nzu\n4/4\nfo\n3/3\nlus\n2/3\nsn\n5/5\nga\n3/3\nmg\n4/4\nst\n4/4\nTable 4: Win rates of our full Kakugo models (i.e. Gen-\nReas + Tran (full)) against the base model (i.e. Granite\n4 Micro) for benchmarks in all languages. Bolded are\nvalues where the Kakugo model outperforms the base\nmodel across all benchmarks for that language.\nsponses from our final Kakugo SLM compared\nto the base model. This suggests that our Kakugo\nSLM should be used in conversational settings in\nlow resource languages instead of its base model\ncounterpart. This further indicates the efficacy of\nour training pipeline for improving the conversa-\ntional and instruction following abilities of SLMs\nin low resource languages.\n6\nDiscussion\nThe evaluation results demonstrate that our auto-\nmated Kakugo data and training pipeline is capable\nof improving an SLM\u2019s low-resource language ca-\npabilities. We can see that generated, translated,\nand reasoning trace data play a key role in improv-\ning performance on down-stream tasks, indicating\nthe importance of diversity in data distillation of\nlow resource languages. This is instructive to fu-\nture research because it shows that even when data\nseeds are diverse, a single data generation pipeline\nwill not suffice for creating generally-useful SLMs\nin low-resource languages.\nWe also find our pipeline to be notably afford-\nable, as we estimate the unit cost of this entire\npipeline to be less than $50. We average between\n10-50 million tokens generated per language at a\nprice of $0.6 per million output tokens1, and train-\ning takes between 1-5 hours on a system priced\n1The current price of our teacher model, GPT-OSS 120B\non TogetherAI\nFigure 2: Response preferences from native speakers\nbetween responses from our Kakugo model and the base\nmodel, Granite 4 Micro.\nat $1.3 per hour2, so even allowing for some re-\ndundancy, this pipeline should not exceed $50 per\nlanguage. The size of our student models (\u223c3 bil-\nlion parameters) means that the cost of evaluation\nis negligible compared to training.\nThis affordability gives low-resource language\ncommunities access to the benefits of SLMs, such\nas cheaper, faster, greener, and more data secure\ninference, without expensive development teams or\ntraining costs.\n7\nFuture work\nFuture uses of this technology could include pairing\na trained SLM with highly multilingual automatic\nspeech recognition (Keren et al., 2025; Chen et al.,\n2024; Zhao et al., 2025) and text-to-speech technol-\nogy (Pratap et al., 2024) to allow for a fully aural\ninterface for this language model. This could be\nuseful as most natural languages in the world are\nprimarily spoken rather than written (Lewis and\nSimons, 2010), meaning a spoken interface may be\nmore natural to speakers of that language.\nAnother potential use-case of this model is to\nbe used as a small, fast pilot language model in\na speculative decoding system (Leviathan et al.,\n2023). This would allow low-resource language\noutputs to be generated faster with less memory\nused. This is a particular issue due to the relatively\nlow efficiency of tokenization of low-resource lan-\nguages (Rust et al., 2021), often making generation\nof the same sentence or paragraph slower and more\nexpensive in these languages compared to high-\nresource languages.\n2The approximate current price of a system comparable\nto which we conducted training on (8 x RTX3090 GPUs) on\nVastAI\n8\nConclusion\nThe Kakugo pipeline shows that a diverse mixture\nof synthetic reasoning traces and translated data\nsignificantly enhances the capabilities of SLMs in\nlow-resource settings. We have open-sourced the\nresulting models and conversational datasets for\n54 languages, many of which previously lacked\nsuch resources. Ultimately, this work provides a\nscalable, affordable framework that empowers low-\nresource language communities to create efficient,\ndata-sovereign AI models without requiring expen-\nsive specialist teams.\nLimitations\nOne of the limitations of our experiments is that our\nresults may not generalize beyond the models that\nwe used as the teacher (GPT-OSS 120B) and stu-\ndent (Granite 4 Micro). We chose these models due\nto their superior base model performance on low-\nresource language tasks, making these realistic and\npractical bases from which we train. Instead of re-\npeating our experiments over multiple models, we\ninstead sought to maximize our coverage of differ-\nent languages, covering more than 50 in this work.\nFurther, our pipeline was not specifically designed\nfor these models and so can theoretically be used\nwith other models too. Future work should repli-\ncate our work using other, possibly future, models.\nA fundamental limitation of our work is that\nthis pipeline will not work for languages that the\nteacher model is not proficient at generating data\nfor. For example, if a teacher model has no knowl-\nedge of a language, such as Kalamang (Tanzer\net al., 2023), then our automated pipeline approach\nwill not generate any useful training data. There-\nfore, this approach is restricted to the broad variety\nof languages which have some presence online or\nin LLMs but remain low-resource languages that\nSLMs struggle with.\nAn area that was not explored extensively in\nthis work is the degree to which datasets should\nbe mixed to maximize down-stream performance.\nAlthough we conduct experiments with four sep-\narate methods of creating training data (3 prompt\ngeneration methods plus translation) and evaluate\nperformance when this data is mixed, it remains\nan open question as to what mixture of data would\nlead to the highest evaluation performance. With\nrecent work in evaluating dynamic mixtures of\ndatasets (Wu et al., 2024), future work could in-\nvestigate exactly which proportions of each type\nof data evaluated is most appropriate for making a\ngeneral training pipeline."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary\n\n**Kakugo**: A Novel Pipeline for Training General-Purpose Small Language Models (SLMs) for Low-Resource Languages\n\n#### Main Contributions:\n1. **Novel Pipeline Design**: Kakugo presents a novel, cost-effective pipeline designed to train general-purpose SLMs using only the language name as input.\n2. **Synthetic Data Generation**: The pipeline combines model distillation with synthetic data generation techniques to produce diverse and high-quality training data for low-resource languages.\n3. **Open-Source Datasets and Models**: Kakugo releases open-source datasets and monolingual SLM models for 54 languages, many of which previously lacked such resources.\n\n#### Methodology:\n1. **Teacher Model Utilization**: The pipeline uses a large teacher model (GPT-OSS 120B) to generate synthetic prompts, reasoning traces, and responses.\n2. **Data Creation**: Data is created in multiple ways: topic-based prompts, scenario-based prompts, context-based prompts, translated data, and combinations of these methods.\n3. **Model Training**: The student model (Granite 4 Micro) is fine-tuned on the generated and translated training data using Llama Factory for one epoch.\n\n#### Key Findings:\n1. **Performance Improvement**: Evaluations across diverse NLP tasks (translation, classification, question answering) demonstrate that Kakugo consistently improves performance over base models.\n2. **Affordability**: The total generation and training cost per language is under $50, making the pipeline accessible for low-resource communities.\n\n#### Conclusions:\n1. **Effectiveness of Pipeline**: The results show that the automated Kakugo data and training pipeline significantly enhances SLM capabilities in low-resource settings.\n2. **Scalable Framework**: By providing open-source models and datasets, Kakugo offers a scalable framework for low-resource language communities to develop efficient AI tools.\n\n### Limitations:\n1. **Model Generalization**: The effectiveness may not generalize beyond the teacher (GPT-OSS 120B) and student (Granite 4 Micro) models used in this work.\n2. **Language Proficiency**: The pipeline is limited by the proficiency of the teacher model in generating data for certain languages.\n\n### Future Work:\nFuture research should explore using other models, investigate optimal dataset mixtures, and consider limitations related to language generation proficiency."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "VideoMaMa: Mask-Guided Video Matting via Generative Prior\nSangbeom Lim1\u2020\nSeoung Wug Oh2\nJiahui Huang2\nHeeji Yoon3\nSeungryong Kim3\nJoon-Young Lee2\n1Korea University\n2Adobe Research\n3KAIST AI\nhttps://cvlab-kaist.github.io/VideoMaMa\nInput Video\nVideoMaMa\nInput Mask\nModel-generated Mask to Matte\nGT Mask to Matte\nFigure 1. We introduce Video Mask-to-Matte Model (VideoMaMa), a diffusion-based model that generates high-quality alpha mattes from\ninput binary segmentation masks obtained either from existing models such as SAM2 [30] or from ground-truth segmentation masks in\nexisting datasets such as SA-V [30]. Examples shown highlights our VideoMaMa\u2019s ability to capture fine-grained details including motion\nblur, and intricate boundary structures on natural video footage.\nAbstract\nGeneralizing video matting models to real-world videos re-\nmains a significant challenge due to the scarcity of labeled\ndata.\nTo address this, we present Video Mask-to-Matte\nModel (VideoMaMa) that converts coarse segmentation\nmasks into pixel accurate alpha mattes, by leveraging pre-\ntrained video diffusion models. VideoMaMa demonstrates\nstrong zero-shot generalization to real-world footage, even\nthough it is trained solely on synthetic data.\nBuilding\non this capability, we develop a scalable pseudo-labeling\n\u2020Work done during an internship at Adobe Research.\npipeline for large-scale video matting and construct the\nMatting Anything in Video (MA-V) dataset, which offers\nhigh-quality matting annotations for more than 50K real-\nworld videos spanning diverse scenes and motions. To vali-\ndate the effectiveness of this dataset, we fine-tune the SAM2\nmodel on MA-V to obtain SAM2-Matte, which outperforms\nthe same model trained on existing matting datasets in\nterms of robustness on in-the-wild videos. These findings\nemphasize the importance of large-scale pseudo-labeled\nvideo matting and showcase how generative priors and ac-\ncessible segmentation cues can drive scalable progress in\nvideo matting research.\n1\narXiv:2601.14255v1  [cs.CV]  20 Jan 2026\n1. Introduction\nVideo matting, the task of extracting foreground objects\nwith pixel-level precision from video, serves as a fundamen-\ntal component in video editing applications, including back-\nground replacement [6, 31, 50], visual composition [3, 47],\nand relighting [2, 31, 42].\nDespite its importance, training a video matting model\nthat performs robustly across diverse real-world videos re-\nmains challenging for two key reasons. First, high-quality\nvideo matting annotations are extremely scarce. Ground-\ntruth mattes are typically captured in controlled environ-\nments such as chroma key studios [27, 51] or with special-\nized camera setups [5], which makes it difficult to scale an-\nnotation efforts and to capture diverse objects, scenes, and\ncamera angles. As a result, existing datasets are often lim-\nited to human portraits.\nSecond, most video matting models are trained on syn-\nthetic videos, where foregrounds are composited onto arbi-\ntrary backgrounds. Such synthetic compositions often intro-\nduce unrealistic artifacts in lighting, motion blur, and tem-\nporal coherence. These two factors, limited data diversity\nand the domain gap between synthetic and real videos, sig-\nnificantly hinder the ability of current models to generalize\nto real-world footage where complex interactions between\nforegrounds and backgrounds naturally occur.\nIn this work, we propose a novel bootstrapping strat-\negy that effectively bridges the gap between synthetic and\nreal-world videos, enabling more robust video matting. We\nintroduce Video Mask-to-Matte Model (VideoMaMa), a\ndiffusion-based model that converts binary segmentation\nmasks into continuous alpha mattes (see Figure 1). Built\nupon pretrained video diffusion models [1], VideoMaMa\ndemonstrates remarkable zero-shot generalization to real-\nworld videos, even though the model is trained exclusively\non synthetic data. This strong generalization ability stems\nfrom diffusion priors trained on internet-scale image and\nvideo data, which can generate high-quality content across\ndiverse domains [1, 9, 41]. To adapt these priors for video\nmatting while preserving their generative capabilities, we\nintroduce a two-stage training strategy that separately op-\ntimizes spatial and temporal layers, along with semantic\nknowledge injection via DINOv3 [38] features.\nTo evaluate the robustness of the VideoMaMa model,\nwe conduct experiments on video matting guided by binary\nmasks from diverse sources, including those produced by\nexisting video segmentation models. The results show that\nVideoMaMa consistently generates high-quality video mat-\nting outputs regardless of the input mask type, demonstrat-\ning strong robustness. These findings suggest that Video-\nMaMa can be effectively utilized as a pseudo-labeler for\nconstructing large-scale video matting annotations.\nIn addition, by leveraging VideoMaMa, we propose a\nsimple but scalable pipeline for constructing large-scale\nvideo matting annotations and introduce the Matting Any-\nthing in Videos (MA-V) dataset, the first large-scale pseudo\nvideo matting dataset built by converting segmentation la-\nbels from the SA-V dataset [30]. Unlike existing video mat-\nting datasets [8, 14, 27, 48, 51], which rely on synthetic or\ncomposited content, MA-V provides high-quality matting\nannotations for over 50K real-world videos covering diverse\nscenes, objects, and motion dynamics, as summarized in Ta-\nble 1. This scalable pipeline demonstrates the potential of\nleveraging generative priors and segmentation labels, which\nare considerably easier to obtain than video alpha matte, to\nefficiently construct large-scale, high-quality video matting\ndatasets.\nTo validate the effectiveness of large-scale pseudo video\nmatting annotations, we fine-tune the SAM2 [30] model for\nvideo matting using MA-V, referred to as SAM2-Matte.\nTrained without any architectural modifications, the SAM2-\nMatte model achieves substantially more robust matting\nperformance on in-the-wild videos than the same SAM2\nmodel fine-tuned on existing video matting datasets, as\nwell as other existing video matting methods. The results\ndemonstrate the strong potential of large-scale pseudo an-\nnotations to drive advancements in video matting research.\nOur contributions can be summarized as follows.\n\u2022 VideoMaMa: A diffusion-based model that generates re-\nalistic video matting annotations from binary masks, en-\nabling scalable matting label creation with easily obtain-\nable segmentation labels.\n\u2022 MA-V Dataset: The first large-scale, high-quality video\nmatting dataset built on real captured footage, consisting\nof over 50K diverse videos.\n2. Related Work\nVideo matting.\nVideo matting methods can be catego-\nrized into auxiliary-free and auxiliary-guided approaches.\nAuxiliary-free methods [16, 26, 28] typically focus on por-\ntrait matting, restricting their applicability to human-centric\nscenarios. Trimap-guided approaches [13, 36] require man-\nual trimap annotations, limiting their practicality for zero-\nshot inference. Recent mask-based guidance methods ad-\ndress these limitations: MaGGIe [14] decouples tracking\nfrom matting using binary mask tracks, MatAnyone [48]\npropagates target-assigned matting through a memory-\naugmented architecture, and GVM [8] employs diffusion\nmodels for portrait matting. Despite these advances, ex-\nisting methods remain constrained by domain specificity.\nBeyond algorithmic limitations, existing datasets also\nconstrain progress. Representative datasets include Video-\nMatte240K [27] (484 videos), VideoMatting108 [51] (108\nvideos), and VM800 [48] (826 videos), predominantly fo-\ncusing on human subjects in controlled settings [8, 39, 44].\nCritically, all rely on composition-based generation where\nforeground objects are composited onto random back-\n2\ngrounds, creating artificial scene compositions. No large-\nscale real video matting dataset exists where foreground and\nbackground naturally co-occur.\nDiffusion models for perception tasks. Recent advances\nhave demonstrated that diffusion models [1, 21, 33, 41] en-\ncode rich priors about natural scenes, motion dynamics, and\ntemporal coherence. Marigold [15] pioneered this direc-\ntion by finetuning Stable Diffusion for monocular depth es-\ntimation, achieving strong zero shot generalization despite\ntraining only on synthetic data. This success has extended\nto semantic segmentation [43, 52] and video depth estima-\ntion [11, 37]. These models exhibit remarkable zero shot\ngeneralization across perception tasks [7, 10, 11, 45, 52],\nmaintaining strong performance on real data even when\ntrained exclusively on synthetic data, suggesting they can\nbridge the synthetic to real domain gap. For matting, SD-\nMatte [12] applies diffusion models to interactive image\nmatting, while GVM [8] focuses on portrait video matting\nwith emphasis on hair details. However, these methods are\nlimited to either single images or specific domains like hu-\nman portraits, lacking the scale and generality needed for\ndiverse video matting scenarios.\nPseudo-label generation in segmentation/matting\nhas\nbecome a key driver of recent progress. In the Segment\nAnything Model [20, 30], the authors demonstrated that it\nis possible to create an iterative self-training loop by build-\ning a strong segmentation model and bootstrapping it with\nits own outputs. ZIM [18] trained a converter model that\ntransforms image binary masks into alpha mattes using ex-\nisting image matting datasets.\nInspired by the recent successes, we address video mat-\nting, which presents distinct challenges. Unlike image mat-\nting [22, 24, 25, 34] and video object segmentation, where\nabundant manually annotated ground-truth data [4, 17, 30,\n46] enables training strong labeler models, video matting\ndatasets remain scarce and predominantly synthetic. This\nlimitation hinders model generalization to real-world sce-\nnarios. To overcome the challenge, we exploit the genera-\ntive priors of pre-trained video diffusion models and train\na robust pseudo-labeler using small-scale synthetic video\nmatting annotations.\n3. Video Mask-to-Matte Model\n3.1. Problem Formulation\nVideo matting is mathematically defined by the alpha com-\npositing equation, I = \u03b1F + (1 \u2212\u03b1)B, where I is the\nobserved image, F is the foreground, B is the background,\nand \u03b1 \u2208[0, 1] is the alpha matte representing pixel-level\nopacity. Unlike binary segmentation masks M \u2208{0, 1}, al-\npha mattes capture fine-grained details such as hair strands,\nmotion blur, and intricate regions, which are critical for re-\nalistic video compositing.\nConcat.\nLatent\nDecoder\n\ud835\udc9f\nLatent\nEncoder\n\u2130\nGuide Masks\nLatent\nEncoder\n\u2130\nVideo Frames\nSpatial layer\nTemporal layer\nPrediction\nVideo\nDiffusion\nU-Net\n\u2112mat\nSemantic injection\nDINOv3\n\u2112reg\nGT Matte\nTrainable\nFreeze\nMLP\nFigure 2. Overview of VideoMaMa architecture. RGB frames\nand guide masks are processed through video diffusion U-Net lay-\ners to generate high-quality video mattes. Semantic injection with\nDINO features is applied during training.\nThere are several ways to specify the target object in\nvideo matting. One approach is to predefine the target se-\nmantic class, for example human portraits [8, 22], while an-\nother is to condition the model on additional signals such as\npoints, boxes, or masks [12, 14, 49].\nWe adopt binary segmentation masks as conditioning\nsignals for two critical reasons: (1) masks allow the diffu-\nsion model to focus solely on generating fine-grained mat-\nting details (e.g., hair strands, motion blur) rather than infer-\nring object boundaries, since shape information is already\nprovided by the mask.\nThis design decouples the chal-\nlenges of object localization and matte generation, making\nthe pipeline more robust and generalizable and (2) Finally,\nsince binary masks can be obtained through various sources,\nthe applicability of a model can be greatly expanded. In\nthis paper, we demonstrate that VideoMaMa can not only\nperform video matting in various settings when combined\nwith existing video segmentation models, but also enables\nthe conversion of existing video segmentation datasets into\nmatting datasets.\n3.2. Architecture Design\nWe build VideoMaMa on top of Stable Video Diffusion\n(SVD) [1], a pretrained video diffusion model originally de-\nsigned for image-to-video generation. By leveraging gener-\native priors, it can generate high-quality video matting an-\nnotations that exhibit realistic characteristics such as natural\nmotion blur, proper edge detail, and temporal consistency,\neffectively transferring the model\u2019s learned understanding\nof video dynamics to the matting task.\nWe adapt the SVD architecture for mask-guided video\nmatting by introducing mask-based conditioning in the la-\ntent space and employing single-step inference for im-\nproved efficiency.\nIn addition, we propose a semantic\nknowledge injection technique that enhances the model\u2019s\nunderstanding of object boundaries and strengthens tem-\nporal consistency when tracking complex objects with fine\nstructural details. An overview of the model is shown in\nFigure 2.\nLatent space formulation. Following modern video dif-\nfusion models, we operate in a compressed latent space\n3\nto alleviate the computational overhead of processing high\nresolution videos. We leverage a Variational Autoencoder\n(VAE) [19] for efficient encoding and decoding. All inputs,\nincluding video frames V = {It}T\nt=1, binary mask track\nM = {Mt}T\nt=1, and alpha mattes \u03b1 = {\u03b1t}T\nt=1, are en-\ncoded into the same latent space: zx = E(x), \u02c6x = D(zx),\nwhere E and D denote the encoder and decoder of the VAE,\nrespectively, and x may represent V , M, or \u03b1. Since all\nthree modalities share the same spatial dimensions, this uni-\nfied encoding approach naturally allows them to be pro-\ncessed together in the compressed latent space, reducing\nmemory requirements while preserving spatial correspon-\ndence.\nSingle-step diffusion formulation. We formulate Video-\nMaMa as a single-timestep generative model that directly\nsynthesizes high-fidelity alpha matte latents \u02c6z\u03b1 in a single\nforward pass, where h and w represent the compressed spa-\ntial dimensions and c is the latent channel dimension. Un-\nlike traditional diffusion models that require iterative de-\nnoising steps, our model performs direct prediction from\nnoise to clean latents, making it significantly more efficient\nfor large-scale dataset generation.\nThe generation process takes as input the frame-wise\nconcatenation of video latents zV , mask latents zM, and\nGaussian noise \u03f5 \u223cN(0, I) along the channel dimension:\n\u02c6z\u03b1 = FSVD(concat(zV , zM, \u03f5)),\n(1)\nwhere FSVD represents our adapted SVD model, and\nconcat(\u00b7) denotes a concatenation. We modify SVD\u2019s archi-\ntecture by replacing the original image conditioning input\nwith this concatenated tensor, allowing the model to lever-\nage SVD\u2019s strong temporal modeling capabilities while\nconditioning on both appearance (from zV ) and shape infor-\nmation (from zM). The final alpha matte video is obtained\nby decoding the predicted latents through the VAE decoder:\n\u02c6\u03b1 = D(\u02c6z\u03b1).\n3.3. Training Recipe\nMask augmentation. While providing binary masks as in-\nput conditioning signals simplifies the task for the diffusion\nmodel, it also introduces the risk of copy-paste behavior.\nFor objects with simple shapes (e.g., cars or desks) or masks\nthat already contain fine-grained details (e.g., binarized al-\npha mattes), the model may trivially copy the mask instead\nof generating realistic matting details by reasoning about\nthe RGB image.\nTo prevent this, we apply mask augmentation during\ntraining to remove fine-grained information from the input\nmasks, forcing the model to infer matting details from the\nRGB image. Our mask augmentation strategy consists of\ntwo operations (see Figure 3): (1) Polygon Degradation:\nWe approximate the mask boundary with a polygon, simpli-\nfying the contour and removing fine details. (2) Downsam-\nPolygon Degradation\nWeak\nStrong\nDownsampling Degradation\nWeak\nStrong\nPolygon Degradation\nOriginal Matte\nBinarization\nDownsampling Degradation\nor\nFigure 3. Examples of mask augmentation methods. Polygon\nand Downsampling degradation are applied at weak and strong\naugmentation levels.\npling Degradation: We downsample and upsample the mask\nat the same scale, which effectively removes high-frequency\ndetails while preserving the overall shape. These augmen-\ntations create a gap between the coarse input mask and the\nfine-grained target alpha matte, encouraging the model to\nleverage appearance cues from the RGB video to generate\nrealistic matting details.\nTwo-stage training. For matting tasks, both training and\ninference resolution are critical. As matting operates at the\npixel level with continuous values, downsampling to lower\nresolutions can degrade fine-grained matting details. How-\never, training video diffusion models at high resolution is\ncomputationally prohibitive. To address this challenge, we\npropose a two-stage training strategy that decomposes the\nlearning process: first training spatial layers at high resolu-\ntion on single frames to capture fine details, then training\ntemporal layers at lower resolution on video sequences to\nlearn temporal consistency. In the first stage, we freeze the\ntemporal layers of SVD and train only the spatial layers,\nenabling the model to focus on capturing pixel-level details\nat high resolution. While this stage allows the model to\nlearn fine-grained features within individual frames, it lacks\ntemporal consistency across video sequences. In the second\nstage, we freeze the spatial layers to preserve the learned\ndetail-capturing capability and fine-tune only the temporal\nlayers to learn temporal coherence and video-specific char-\nacteristics. This decomposed training strategy enables our\nmodel to achieve both high-resolution detail and tempo-\nral consistency without requiring full high-resolution video\ntraining.\nMatting loss function. We adopt v-parameterization [35]\nfor single-step generation, where the model is trained to di-\nrectly convert noise into clean alpha matte latents. The pre-\ndicted latent \u02c6z\u03b1 is decoded to pixel space \u02c6\u03b1 = D(\u02c6z\u03b1), and\nthe loss is computed at the pixel level:\nLmat = E\u03b1,zV ,zM,\u03f5 [sim(D(\u02c6z\u03b1), \u03b1)] ,\n(2)\n4\nTable 1. Dataset statistics for video matting.\nDataset\nCategory\n# Videos\nComposition Required\nSynHairMan [8]\nHuman\n200\nYes\nVideoMatte240K [27]\nHuman\n478\nYes\nV-HIM2K5 [14]\nHuman\n443\nYes\nVideoMatting108 [51]\nVarious\n108\nYes\nCRGNN [44]\nVarious\n60\nYes\nVM800 [48]\nHuman\n826\nYes\nMA-V (Ours)\nVarious\n50,541\nNo\nwhere sim(\u00b7, \u00b7) comprises pixel-wise similarity loss terms\nbetween the decoded prediction and ground-truth alpha\nmatte. This pixel-level supervision enables direct optimiza-\ntion for visual quality, capturing fine-grained details such as\nmotion blur and intricate boundary structures.\nSemantic knowledge injection. While diffusion priors are\npowerful for generating precise alpha mattes, they struggle\nwith semantic understanding of object boundaries and con-\nsistently tracking objects, particularly for complex objects\nwith intricate structures. To address this, we inject seman-\ntic knowledge into VideoMaMa by aligning SVD features\nwith robust semantic representations from DINOv3 [38]\nthroughout training.\nWe first extract DINO features hdino = Fdino(V ) from\nthe video frames V . Then, we extract intermediate features\nhl from the l-th layer of the diffusion model and project\nthem into the DINO feature space using a learnable MLP\np\u03d5 with parameters \u03d5. We minimize the alignment loss that\nmaximizes patch-wise cosine similarities:\nLreg = \u2212E\u03b1,zV ,zM,\u03f5,V\nh\ncos-sim(hdino, p\u03d5(hl))\ni\n,\n(3)\nwhere n indexes patches and cos-sim(\u00b7, \u00b7) denotes cosine\nsimilarity.\nThis design allows the model to leverage se-\nmantic understanding of object categories and structures\nwhile maintaining the generative capabilities of the diffu-\nsion backbone, enabling more accurate matte generation for\nchallenging cases such as overlapping objects or complex\narticulated structures.\n4. Matte Anything in Videos (MA-V) Dataset\nThe difficulty of obtaining video matting annotations has\nseverely limited existing datasets. As shown in Table 1,\nprior datasets contain at most hundreds of videos, pre-\ndominantly focusing on human subjects captured in con-\ntrolled environments [27, 51] or through manual annota-\ntion [22, 23, 29, 40]. While composition-based approaches\nenable scaling and precise annotations, they create artificial\nscene compositions that differ fundamentally from natural\nvideo footage, limiting model generalization to real world\nscenarios.\n4.1. Dataset Construction\nTo address these limitations, we introduce Matte Anything\nin Videos (MA-V), created by applying VideoMaMa to an-\nnotate SA-V\u2019s [30] diverse mask annotations. As shown\nin Table 1, MA-V provides 50,541 videos captured in nat-\nural settings, nearly 50\u00d7 larger than existing real-video\ndatasets.\nUnlike prior work that focuses predominantly\non humans, MA-V encompasses diverse object categories\nat multiple scales, from small objects to full scenes. The\ndataset construction process is straightforward yet effec-\ntive: we leverage VideoMaMa\u2019s ability to generate high-\nquality alpha mattes from binary masks, converting SA-\nV\u2019s segmentation annotations into continuous matting la-\nbels while preserving the natural scene context. Critically,\nMA-V is the first large-scale video matting dataset where\nboth foreground and background naturally co-occur in real\ncaptured footage, eliminating the synthetic composition gap\nthat limits previous datasets.\nFigure 4 illustrates the vi-\nsual quality and diversity of MA-V compared to SA-V\u2019s\nbinary masks, demonstrating how VideoMaMa generates\nfine-grained matting details including semi-transparent re-\ngions, motion blur, and intricate boundary structures regard-\nless of the target category that are absent in the source seg-\nmentation masks.\n4.2. Fine-tuning SAM2 with MA-V\nWe finetune SAM2 [30] on our MA-V dataset, referred to\nas SAM2-Matte hereafter. SAM2, originally designed for\nbinary segmentation, requires minimal adaptation for mat-\nting: we apply a sigmoid function after the mask logits\nto produce continuous alpha values in the range [0,1]. To\nassess MA-V quality, we also finetune SAM2 on existing\nvideo matting datasets and evaluate their performance on\nstandard matting benchmarks.\n5. Experiments\n5.1. Experimental Details\nImplementation details. We train VideoMaMa based on\nthe Stable Video Diffusion [1] (SVD) model using diverse\nimage and video matting datasets. Our two-stage training\nstrategy employs different resolutions optimized for each\nstage\u2019s objective. In stage 1, we train the spatial layers of\nSVD on single images at high resolution (1024\u00d71024) to\ncapture fine-grained matting details such as hair strands and\nintricate boundary structures. We freeze the temporal layers\nduring this stage to focus learning on spatial precision. In\nstage 2, we freeze the learned spatial layers and train only\nthe temporal layers on video sequences. To balance compu-\ntational efficiency with temporal modeling, we use 3-frame\nclips at 704\u00d7704 resolution, which enables the model to\nlearn temporal consistency and motion-aware matting char-\nacteristics. To inject semantic knowledge, we incorporate\n5\nRGB\nRGB\nRGB\nSA-V\nMA-V\nSA-V\nMA-V\nSA-V\nMA-V\nFigure 4. Qualitative examples from our MA-V dataset. We show RGB frames with our high-quality MA-V annotations and original\nSA-V [30] masks for comparison. MA-V provides refined alpha mattes for diverse scenarios.\nVideoMaMa\n(Ours)\nSAM2\nMatAnyone\nMaGGIe\nSAM2-Matte\n(Ours)\nAll-frame mask-guided\nFirst-frame mask-guided\nInput Video\nFigure 5. Qualitative comparison on in-the-wild videos. We evaluate two settings: (1) all-frame mask-guided video matting where\nVideoMaMa is compared against MaGGIe [14], and (2) first-frame mask-guided matting where SAM2-Matte is compared against MatAny-\none [48]. All methods use SAM2 [30] to generate mask inputs.\nfeatures from a frozen DINOv3 [38] encoder, aligning the\nDINO features with the first upsampling block of the SVD\ndecoder. This integration provides the diffusion model with\nsemantic understanding of object structures while preserv-\ning its generative capabilities.\nTraining details. For both stages, we use a batch size of 64\nand a learning rate of 5 \u00d7 10\u22125 with the AdamW optimizer.\nThe training objective, Lmat, combines L1 loss for pixel-\nwise accuracy and Laplacian loss for preserving edge sharp-\nness and boundary details. Each stage is trained for 10,000\niterations until convergence. All experiments are conducted\non NVIDIA A100 GPUs with mixed-precision training for\nefficiency. SAM2-Matte is trained on the combination of\nexisting video matting datasets and MA-V. Details of the\nexisting datasets are provided in the appendix.\n6\nTable 2. All-frame mask-guided video matting comparison on V-HIM60 and YouTubeMatte benchmarks. We compare VideoMaMa\n(Ours) against mask-guided matting methods: MaGGIe [14] (video mask-guided) and MGM [49] (image mask-guided). We evaluate on\ntwo mask types: Synthetically Degraded Masks including downsampling (8\u00d7, 32\u00d7) and polygon degradation with varying difficulty levels,\nand Model-Generated Masks from SAM2 [30]. Lower values indicate better performance.\nV-HIM60 [14] (Hard)\nYouTubeMatte [48] (1920 \u00d7 1080)\nMAD \u2193\nGradient \u2193\nMAD \u2193\nGradient \u2193\nInput Mask\nInput\nMGM [49]\nMaGGle [14]\nOurs\nInput\nMGM [49]\nMaGGle [14]\nOurs\nInput\nMGM [49]\nMaGGle [14]\nOurs\nInput\nMGM [49]\nMaGGle [14]\nOurs\nSynthetically Degraded Masks\nDownsampl. 8x\n2.744\n63.8374\n2.3790\n1.306\n11.127\n16.5609\n5.4150\n2.363\n1.946\n3.4973\n1.6420\n0.934\n8.034\n4.0822\n4.9733\n2.031\nDownsampl. 32x\n5.132\n79.2273\n2.5375\n1.461\n33.664\n22.2024\n5.9073\n2.849\n4.203\n3.9061\n1.8171\n1.029\n36.540\n5.3603\n5.6318\n2.560\nPolygon. (Easy)\n3.636\n64.5457\n2.5808\n1.416\n16.000\n17.3675\n5.5572\n2.480\n3.619\n4.1390\n2.2250\n1.206\n19.155\n4.8579\n5.6773\n2.335\nPolygon. (Hard)\n6.771\n75.5501\n3.1567\n1.640\n43.228\n20.5516\n6.4121\n3.024\n9.470\n4.4549\n2.3111\n1.404\n65.844\n5.6132\n5.9184\n3.237\nModel-Generated Masks\nSAM2 [30]\n4.666\n66.5221\n3.1567\n2.435\n21.960\n18.2103\n6.4121\n4.180\n3.569\n4.1136\n1.9499\n1.737\n24.729\n5.0448\n5.7877\n3.635\n5.2. Quantitative Evaluations\nAll-frame mask-guided video matting.\nWe evaluate\nVideoMaMa following the protocol in [14], where input bi-\nnary masks for every frame are provided. We use two cat-\negories of input masks: (1) Synthetically Degraded Masks\nthrough downsampling (8\u00d7, 32\u00d7) and polygon degradation\n(easy, hard), mimicking imperfect user annotations, and (2)\nModel-Generated Masks from SAM2 [30], representing au-\ntomatic segmentation pipelines. This diverse set evaluates\nVideoMaMa\u2019s robustness across different mask quality lev-\nels.\nIn Table 2, we compare VideoMaMa against MaG-\nGIe [14] and MGM [49]. For MGM, which is an image-\nbased method, we perform inference on each frame inde-\npendently. We report metrics comparing both input masks\nand generated results against ground-truth alpha mattes. All\nevaluations use 12-frame sequences.\nWe adopt standard metrics:\nMAD for overall accu-\nracy and Gradient error [32] for boundary quality. Results\nare reported on V-HIM60 Hard [14] and YouTubeMatte\n1920\u00d71080 [48].\nVideoMaMa consistently outperforms existing mask-\nguided matting models across both synthetically degraded\nand model-generated masks. Our evaluation across differ-\nent mask styles demonstrates that VideoMaMa can han-\ndle various types of mask inputs and produce high-quality\nvideo mattes consistently.\nFirst-frame mask-guided video matting. In Table 3, we\nevaluate VideoMaMa and SAM2-Matte following a setting\nproposed in [48] where a binary mask guidance is provided\nonly for the first frame. For VideoMaMa, the binary mask\nguidance across all frames is generated by propagating the\nmask from the first frame using SAM2 [30], denoted as\nSAM2+VideoMaMa. We also report the score of the bi-\nnary masks produced by SAM2 to provide a baseline for\nthe improvements achieved by VideoMaMa.\nWe employ multiple evaluation metrics: MAD for over-\nall semantic accuracy, MSE for pixel-level error, MAD-T\n(trimap-based MAD) which computes MAD only within\nthe uncertain trimap region derived from the ground-truth\nalpha map, and Gradient error [32] for assessing bound-\nary detail extraction quality. The trimap-based evaluation\nis particularly important as it focuses on the challenging\nboundary regions where precise matting is most critical.\nSAM2-Matte trained on the combined dataset sur-\npasses previous state-of-the-art methods, MatAnyone [48],\non challenging benchmarks:\nV-HIM60 Hard [14] and\nYouTubeMatte [48].\nThis demonstrates that MA-V pro-\nvides high-quality training data for video matting models,\nenabling to extract matting quality and temporal consis-\ntency over the video frames. Note that, for VideoMaMa\nin this experiment, we processed each of the 12-frame seg-\nments and merged the results to obtain the complete video\nmatting.\n5.3. Qualitative Evaluations\nTo further evaluate the model\u2019s performance on real-world\nvideos, we present qualitative comparisons in Figure 5. We\ncategorize the methods into two settings: first-frame mask-\nguided and all-frame mask-guided. The initial binary masks\nare obtained manually using the SAM2 model. Unlike pre-\nvious matting methods [14, 48], VideoMaMa and SAM2-\nMatte not only perform well on human portraits but also\ngeneralize to a wide range of object categories and in-the-\nwild video content. Moreover, the high-quality results from\nSAM2-Matte demonstrate that MA-V\u2019s large-scale and di-\nverse annotations effectively enhance video matting perfor-\nmance beyond what existing datasets can achieve.\n5.4. Ablation Studies\nNumber of inference frames. We evaluate VideoMaMa\u2019s\nperformance across varying numbers of frames. Although\ntrained on maximum 3 frames, VideoMaMa successfully\nhandles diverse frame counts from single frames to 24\nframes at inference time. As shown in Table 4, the model\nmaintains consistent performance across different frame\ncounts, demonstrating strong temporal generalization.\nTraining recipe. We ablate our two-stage training strategy\nand semantic knowledge injection on YouTubeMatte [48]\nwith four configurations: (1) image only (stage 1), (2) video\nonly (stage 2), (3) two-stage without semantic knowledge,\n7\nTable 3. First-frame mask-guided video matting comparison on V-HIM60 and YouTubeMatte benchmarks. We evaluate video\nmatting performance across different difficulty levels in V-HIM60. The best and second-best results are highlighted.\nMethod\nV-HIM60 [14]\nYouTubeMatte [48]\nEasy\nMedium\nHard\n1920 \u00d7 1080\nMAD\nMAD-T\nMSE\nGRAD\nMAD\nMAD-T\nMSE\nGRAD\nMAD\nMAD-T\nMSE\nGRAD\nMAD\nMAD-T\nMSE\nGRAD\nSAM2 [30]\n2.7978\n91.5246\n1.4799\n13.3307\n4.9618\n105.2547\n2.9401\n25.6735\n7.8517\n130.0338\n6.0075\n26.3355\n3.8530\n103.2181\n2.6133\n37.4074\nMatAnyone [48]\n3.5803\n92.8273\n1.9905\n7.0763\n7.1955\n100.0465\n4.7586\n11.4541\n5.7195\n102.4861\n3.4620\n9.8199\n1.9909\n51.0939\n0.7085\n8.9019\nSAM2+VideoMaMa\n1.3539\n40.6830\n0.4063\n2.8876\n2.3132\n49.0329\n0.9335\n4.6944\n2.7757\n53.4392\n1.4428\n4.8276\n1.3559\n35.5064\n0.2940\n3.5742\nSAM2-Matte\n1.3446\n49.0329\n0.2932\n3.1245\n2.2710\n51.8996\n0.6669\n5.2227\n2.6112\n58.7796\n1.0750\n5.0869\n1.2695\n33.5268\n0.2831\n3.4640\nTable 4. Ablation study on the number of frame during infer-\nence. We report the mask refinement performance in MAD metric\non YouTubeMatte dataset [48] with different input masks.\nNumber of Frames per Inference\nMask Input\n24\n18\n12\n6\n1\nDownsampl. 32x\nInput\n4.2411\n4.2455\n4.2029\n4.1377\n4.1336\nRefined\n1.0237\n1.0272\n1.0292\n1.0418\n1.1024\nPolygon. (Hard)\nInput\n9.5559\n9.6224\n9.4700\n8.7498\n8.5455\nRefined\n1.4472\n1.4847\n1.4042\n1.3382\n1.5598\nSAM2 [30]\nInput\n3.6023\n3.5991\n3.5693\n3.5476\n3.8711\nRefined\n1.6921\n1.7273\n1.7370\n1.7948\n1.8828\nTable 5. Ablation study on training recipe. We evaluate MAD\nwith different training configurations on YouTubeMatte [48].\nScores for input masks are shown below the each input mask\ntype. Percentages in parentheses indicate the relative improvement\n(green) or degradation (red) compared to the input score.\nS1\nS2\nDINO\nDownsampl. 32x\nInput: 4.2029\nPolygon. (Hard)\nInput: 9.4700\nSAM2\nInput: 3.5693\n\u2713\n\u2717\n\u2717\n3.7620 (10.49%)\n4.8306 (49.00%)\n4.0496 (-13.46%)\n\u2717\n\u2713\n\u2717\n1.2350 (70.62%)\n2.2372 (76.38%)\n2.3124 (35.21%)\n\u2713\n\u2713\n\u2717\n1.2552 (70.13%)\n1.9975 (78.91%)\n1.9404 (45.64%)\n\u2713\n\u2713\n\u2713\n1.0292 (75.51%)\n1.4042 (85.17%)\n1.7370 (51.34%)\nTable 6. Ablation study for training data on V-HIM60 Hard [14]\nand DAVIS val [17], including (a) ED, (B) MAV, (c) ED + MA-V.\nED denotes that existing dataset.\nMethod\nV-HIM60 (Hard)\nDAVIS\nMAD\nMSE\nGRAD\nJ&F\nJ\nF\nMatAnyone [48]\n4.6655\n2.5870\n8.8870\n79.7\n76.5\n82.9\n(a) ED\n7.5798\n5.6824\n8.7091\n77.0\n74.1\n80.0\n(b) MA-V\n3.1830\n1.5603\n5.7869\n87.9\n85.2\n90.6\n(c) ED + MA-V\n2.6112\n1.0750\n5.0869\n85.9\n83.5\n88.3\nand (4) two-stage training with DINO features. As shown in\nTable 5, having both stage 1 and stage 2 training stages led\nto the best results. Adding DINO semantic features further\nimproves performance enabling better object understanding\nand boundary localization. The results validate that both\ncomponents are necessary and complementary for optimal\nperformance.\nImpact of MA-V Dataset. We assess the impact of MA-V\non matting and tracking performance on first-frame mask-\nguided matting task. We evaluate SAM2-Matte trained un-\nder different data configurations by comparing (a) existing\nTime\nRGB\nMatAnyone\nSAM2-Matte (MA-V)\nSAM2-Matte (ED)\nFigure 6. Comparison on a real video between MatAnyone [48]\nand SAM2-Matte variants. Red arrows highlight regions where\nour method achieves the best results.\ndatasets only (ED), (b) MA-V only, and (c) both combined.\nA detailed list of the existing matting datasets used in this\nexperiment will be included in the appendix.\nTo validate the impact of MA-V on matting task, we re-\nport matting quality metrics on the V-HIM60 (Hard) [14].\nAs shown in Table 6, Configuration (a) shows limited per-\nformance due to the small scale and lack of diversity in ex-\nisting datasets. Remarkably, (b) significantly outperforms\n(a) and even surpasses state-of-the-art results, demonstrat-\ning the effectiveness of MA-V. When combining the ex-\nisting matting datasets with MA-V in (c), we achieve the\nbest overall matting quality. We hypothesize that the shared\nsynthetic nature of both the existing matting datasets and\nV-HIM60 contributes to this improved performance.\nWe evaluate tracking performance on DAVIS [17] us-\ning VOS metrics by binarizing matting results. As shown\nin Table 6, configuration (b) substantially improves perfor-\nmance over (a), demonstrating the effectiveness of MA-V.\nInterestingly, adding the existing matting datasets in con-\nfiguration (c) leads to degraded tracking performance com-\npared to using MA-V alone, suggesting that existing syn-\nthetic datasets introduce domain biases that reduce tracking\nrobustness on in-the-wild videos.\nIn Figure 6, we compare MatAnyone [48] and SAM2-\nMatte variants. The results show that using MA-V enables\nrobust tracking even along ambiguous boundaries.\n6."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**VideoMaMa: Mask-Guided Video Matting via Generative Prior**\n\n### Main Contributions\n\n1. **Model Development**: Introduce VideoMask-to-Matte Model (VideoMaMa), a diffusion-based model that converts coarse segmentation masks into pixel-accurate alpha mattes, even when trained on synthetic data.\n2. **Dataset Creation**: Develop the Matting Anything in Videos (MA-V) dataset, offering high-quality matting annotations for over 50K real-world videos across diverse scenes and motions.\n3. **Pseudo-Labeled Dataset**: Demonstrate the effectiveness of large-scale pseudo-labeled video matting datasets by fine-tuning existing models on MA-V to achieve superior performance compared to traditional datasets.\n\n### Methodology\n\n1. **Model Architecture**:\n   - **VideoMaMa**: A diffusion-based model that generates high-quality alpha mattes from input binary segmentation masks, leveraging pretrained video diffusion models.\n   - **Architecture Design**: Utilizes a single-timestep generative model with spatial and temporal layers for improved efficiency. Semantic knowledge injection via DINOv3 features enhances object boundaries and temporal consistency.\n\n2. **Training**:\n   - **Mask Augmentation**: Applies mask augmentation to prevent copy-paste behavior, removing fine-grained information from input masks.\n   - **Two-Stage Training**: Decomposes learning into high-resolution spatial layers for capturing details within individual frames and low-resolution temporal layers for video sequences.\n   - **Matting Loss Function**: Uses v-parameterization for single-step generation, incorporating pixel-wise similarity loss terms to ensure visual quality.\n\n3. **Dataset Construction**:\n   - **MA-V Dataset**: Created by applying VideoMaMa to annotate SA-V\u2019s diverse mask annotations, resulting in a high-quality dataset with over 50K real-world videos.\n   - **Training Details**: Fine-tuned SAM2 on MA-V, demonstrating superior performance compared to existing datasets and methods.\n\n### Key Findings\n\n1. **Robustness Across Different Masks**:\n   - VideoMaMa consistently generates high-quality video mattes regardless of input mask type (synthetically degraded or model-generated), showcasing its robustness.\n   \n2. **Improved Performance on Wild Videos**:\n   - Fine-tuning SAM2 on MA-V results in a more robust model, outperforming state-of-the-art methods like MatAnyone and MGM.\n\n3. **Enhanced Dataset Quality**:\n   - The MA-V dataset significantly improves video matting performance compared to existing datasets by providing high-quality annotations for diverse scenes and motions.\n   \n4. **Impact of Pseudo-Labeled Data**:\n   - MA-V\u2019s pseudo-labeled data effectively enhances the performance of video matting models, outperforming traditional datasets in various tasks.\n\n### Conclusions\n\n1. **Generalization Ability**: VideoMaMa demonstrates strong zero-shot generalization to real-world videos trained solely on synthetic data.\n2. **Dataset Impact**: The MA-V dataset provides a scalable and high-quality resource for constructing large-scale video matting annotations, significantly improving model performance.\n3. **Future Work**: Future research should focus on further refining the pseudo-labeled dataset and exploring more generative priors to enhance video matting capabilities.\n\n### Summary\n\nVideoMaMa is a diffusion-based model that effectively converts coarse segmentation masks into pixel-accurate alpha mattes for real-world videos, leveraging pretrained video diffusion models. The MA-V dataset offers high-quality annotations for over 50K real-world videos and significantly improves the performance of existing video matting models. This work highlights the importance of large-scale pseudo-labeled datasets in advancing video matting research."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Autonomous Knowledge Graph Exploration\nwith Adaptive Breadth-Depth Retrieval\nJoaqu\u00edn Polonuer1,2,\u2217\n, Lucas Vittor1,\u2217\n, I\u00f1aki Arango1,\u2217\n, Ayush Noori1,3,\u2217\n,\nDavid A. Clifton3,4\n, Luciano Del Corro5,6,\u2020,\u2021, Marinka Zitnik1,7,8,9,\u2020,\u2021\n1Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA\n2Departamento de Computaci\u00f3n, FCEyN, Universidad de Buenos Aires, Buenos Aires, Argentina\n3Department of Engineering Science, University of Oxford, Oxford, UK\n4Oxford Suzhou Centre for Advanced Research, University of Oxford, Suzhou, Jiangsu, China\n5ELIAS Lab, Departamento de Ingenier\u00eda, Universidad de San Andr\u00e9s, Victoria, Argentina\n6Lumina Labs, Buenos Aires, Argentina\n7Kempner Institute for the Study of Natural and Artificial Intelligence, Allston, MA, USA\n8Broad Institute of MIT and Harvard, Cambridge, MA, USA\n9Harvard Data Science Initiative, Cambridge, MA, USA\n\u2217Equal contribution.\n\u2020Co-senior authors.\n\u2021Correspondence: delcorrol@udesa.edu.ar, marinka@hms.harvard.edu\nAbstract\nRetrieving evidence for language model queries\nfrom knowledge graphs requires balancing\nbroad search across the graph with multi-hop\ntraversal to follow relational links. Similarity-\nbased retrievers provide coverage but remain\nshallow, whereas traversal-based methods rely\non selecting seed nodes to start exploration,\nwhich can fail when queries span multiple enti-\nties and relations. We introduce ARK: ADAP-\nTIVE RETRIEVER OF KNOWLEDGE, an agen-\ntic KG retriever that gives a language model\ncontrol over this breadth-depth tradeoff using\na two-operation toolset: global lexical search\nover node descriptors and one-hop neighbor-\nhood exploration that composes into multi-hop\ntraversal. ARK alternates between breadth-\noriented discovery and depth-oriented expan-\nsion without depending on a fragile seed selec-\ntion, a pre-set hop depth, or requiring retrieval\ntraining. ARK adapts tool use to queries, using\nglobal search for language-heavy queries and\nneighborhood exploration for relation-heavy\nqueries. On STaRK, ARK reaches 59.1% aver-\nage Hit@1 and 67.4 average MRR, improving\naverage Hit@1 by up to 31.4% and average\nMRR by up to 28.0% over retrieval-based and\nagentic training-free methods. Finally, we dis-\ntill ARK\u2019s tool-use trajectories from a large\nteacher into an 8B model via label-free imi-\ntation, improving Hit@1 by +7.0, +26.6, and\n+13.5 absolute points over the base 8B model\non AMAZON, MAG, and PRIME datasets, re-\nspectively, while retaining up to 98.5% of the\nteacher\u2019s Hit@1 rate.\n1\nIntroduction\nLarge language models rely on knowledge retrieval\nto ground and align their outputs in external evi-\ndence (Ren et al., 2025; Wang et al., 2025a; Xia\net al., 2025a; Zhang et al., 2024), from retrieval-\naugmented generation (RAG) to systems and mem-\nory modules that operate over semi-structured\nknowledge bases (SKB) that mix text with rela-\ntional information (Lewis et al., 2020; Guu et al.,\n2020; Karpukhin et al., 2020; Izacard and Grave,\n2021; Mavromatis and Karypis, 2025; Chen et al.,\n2025; Li et al., 2025c). Knowledge graphs (KGs)\nare a natural data representation for this setting be-\ncause they organize evidence around entities and\ntyped edges, support reuse across queries, and en-\nforce relational constraints that a flat text index\ncannot express. This has motivated graph-aware\nretrievers and graph-grounded generation methods,\nincluding graph-based RAG and SKB retrievers\nthat combine text and relational data (Edge et al.,\n2025; Zhu et al., 2025b; Xia et al., 2025b; Yao\net al., 2025; Jeong et al., 2025).\nRetrieving evidence from KGs is challenging\nbecause it requires coordinating two competing\nsearch modes (Wu et al., 2024b; Lee et al., 2025;\nZhu et al., 2025a). Many queries require breadth:\nthey mention multiple entities or loosely connected\nconcepts, so the retriever must cover the graph\nbroadly to reach the right region. Other queries re-\nquire depth: the supporting evidence only appears\nafter following specific multi-hop relational paths.\nSimilarity-based retrievers provide global coverage\n1\narXiv:2601.13969v1  [cs.AI]  20 Jan 2026\nC\nFind publications by co-authors of \u201cThomson scattering in chirped\nlaser pulses\u201d, which also explore the impact of spin on magnetic fields.\nGlobal search for Thomson scattering in chirped laser pulses\nRETRIEVED NODES\nThomson scattering in high-intensity...\nScore: 0.987 \u2022 Type: Article\nDescription:\nin high-intensity\n...\nThomson scattering\nchirped\nSchwinger vacuum pair production in chirped laser...\nScore: 0.842 \u2022 Type: Article\nDescription: Schwinger vacuum production in\n...\nchirped laser\nNeighbors of       with type Author\nRETRIEVED NODES\nProf. John Rikard\nScore: 0.987 \u2022 Type: Author\nDr. Peter Lynch\nScore: 0.842 \u2022 Type: Author\nNeighbors of       and       with type                and query\nPaper\nImpact of spin\nRETRIEVED NODES FOR\nEffects on the electron\non...\nspin\nScore: 0.762 \u2022 Type: Paper\nRETRIEVED NODES FOR\nWakefield generation and the\n...\nimpact of spin\nScore: 0.987 \u2022 Type: Paper\nA nonlinear theory of the\nfor multidimensional....\nspin\nScore: 0.875 \u2022 Type: Paper\nOrdered node list output by Dynamic Graph Retriever\nWakefield generation and the impact of spin...  0.987\n\u2022\nA nonlinear theory of the spin for multidimensional.... \u2022 0.875\nEffects on the electron spin on... \u2022 0.762\nB\nWhich histamine receptor is a direct target of loratadine?\nNeighbors of       with type Gene\nand query histamine receptor\nRETRIEVED NODES\nHRH1\nScore: 0.872 \u2022 Type: Gene\nDescription:\n...\nEncodes the\nH1\nhistamine\nreceptor\nCYP3A4\nScore: 0.351 \u2022 Type: Gene\nDescription: Drug-metabolizing enzyme involved...\nABCB1\nScore: 0.279 \u2022 Type: Gene\nDescription: Drug transporter (P-glycoprotein) affecting...\nA\nWhat are common H1 antihistamine medicines for allergy symptoms like\nsneezing and itchy eyes?\nGlobal search for H1 antihistamine allergy\nRETRIEVED NODES\nLoratadine\nScore: 0.987 \u2022 Type: Drug\nDescription:\nfor seasonal\n...\nH1 antihistamine\nallergy\nCetirizine\nScore: 0.934 \u2022 Type: Drug\nDescription: H1 antihistamine\nallergy\nused for\nrelief ...\nFexofenadine\nScore: 0.817 \u2022 Type: Drug\nDescription:\nthat treats\nsymptoms ..\nAntihistamine\nallergy\nDesloratadine\nScore: 0.811 \u2022 Type: Drug\nDescription: Second-generation\nfor\n..\nantihistamine\nallergy\nFigure 1: Overview of ADAPTIVE RETRIEVER OF KNOWLEDGE. ARK interacts with a KG through a minimal\ntwo-tool interface: (a) For text-dominant queries, ARK emphasizes breadth by issuing GLOBAL SEARCH to retrieve\na broad set of candidates. (b) For relation-focused queries, ARK applies NEIGHBORHOOD EXPLORATION starting\nfrom a previously retrieved node (in this case, a drug) and expanding to related entities, enabling targeted relational\nretrieval. (c) For relation-dominant queries, ARK performs multi-hop retrieval by alternating GLOBAL SEARCH\nand NEIGHBORHOOD EXPLORATION: it retrieves an initial node (e.g., an article), expands to related entities (e.g.,\nco-authors), and continues expanding and filtering (e.g., papers connected to each author that match query keywords)\nto recover an ordered set of relevant evidence.\nbut often remain shallow and underuse relational\nstructure, whereas traversal-based methods can be\nbrittle because they must choose seed entities to\nstart exploration; when seeds are incomplete or am-\nbiguous, the search stays local and misses evidence\nelsewhere.\nExisting work tackles these breadth-depth re-\nquirements in isolation rather than jointly (Ko\net al., 2025; Ma et al., 2025; Wang et al., 2025b).\nStructure-aware retrievers extend text-based re-\ntrieval with relational structure, for example, by\nlearning node embeddings that aggregate informa-\ntion from nearby neighbors or by generating can-\ndidates using a local graph neighborhood before\nranking them (Lee et al., 2025; Zhu et al., 2025b;\nLei et al., 2025). These methods capture local\nstructure, but they typically encode a fixed neigh-\nborhood around each node, so deeper multi-hop\nqueries require expanding the encoded context or\nstacking additional message-passing and retrieval\nstages, which increases complexity and cost (Wan\net al., 2025; Hu et al., 2025a; Verma et al., 2024).\nBy contrast, traversal-based approaches perform\nmulti-hop exploration, but they depend on identify-\ning a small set of seed entities from which explo-\nration begins (Markowitz et al., 2024; Sun et al.,\n2024). When seeds are incomplete or ambiguous,\nexploration stays local and misses relevant infor-\nmation elsewhere in the graph (Liu et al., 2025; Ma\net al., 2025). Many systems also rely on task- or\ngraph-specific training to learn traversal or scoring\npolicies, which limits transfer across domains and\ngraph schemas (Li et al., 2025a; Lei et al., 2025;\nWu et al., 2024a; Yu et al., 2025). As a result, exist-\ning methods struggle to combine global search with\ntargeted relational reasoning for adaptive retrieval.\nPresent work. We introduce ARK: ADAPTIVE\nRETRIEVER OF KNOWLEDGE, an agentic KG re-\ntriever that gives a language model control over\nevidence discovery using a minimal set of tools\nfor global lexical search and neighborhood explo-\nration (Yao et al., 2023; Schick et al., 2023). ARK\ndoes not require selecting seed entities to start ex-\nploration or establishing a maximum hop depth\nin advance; instead, the model alternates between\nbroad global search and targeted multi-hop traver-\nsal based on the query and what it has retrieved so\nfar. We evaluate ARK on the heterogeneous graphs\nin the STaRK retrieval benchmark and show con-\nsistent gains across all settings. We further study\n2\ncompute-accuracy tradeoffs by varying the tool-call\nbudget and the number of parallel agents, and we\ndistill ARK\u2019s tool-use policy into an 8B model via\nlabel-free trajectory imitation, preserving most of\nthe performance of teacher model at substantially\nlower inference cost (Kang et al., 2025).\nOur contributions are threefold. (i) We intro-\nduce ADAPTIVE RETRIEVER OF KNOWLEDGE, a\ntraining-free retrieval framework that equips lan-\nguage models with a minimal but expressive tool\ninterface for adaptive retrieval from KGs. (ii) We\nshow that ARK balances breadth-oriented retrieval\nwith depth-oriented multi-hop traversal without\ntask- or graph-specific training, achieving strong\nperformance on STaRK. (iii) We distill ARK\u2019s tool-\nuse policy without labeled supervision into a com-\npact Qwen3-8B model (Yang et al., 2025a; Kang\net al., 2025), preserving retrieval quality while re-\nducing inference cost.\n2\nRelated Work\nKnowledge graphs for document-centric RAG.\nRetrieval-augmented generation (RAG) grounds\nLLM outputs in external evidence by retrieving rel-\nevant context from a corpus or index (Lewis et al.,\n2020; Guu et al., 2020). Recent work injects struc-\nture by building graphs over textual units and using\nconnectivity to aggregate evidence. GraphRAG\nperforms local-to-global retrieval over an entity-\ncentric graph (Edge et al., 2025), while KG-guided\nmethods steer evidence selection using external re-\nlations (Zhu et al., 2025b). Related approaches\nretrieve textual subgraphs to support multi-hop\nqueries under context limits (Hu et al., 2025b; He\net al., 2024; Li et al., 2025b). These approaches\nprimarily focus on improving how textual evidence\nis organized or aggregated, but the retrieval process\nitself is typically static.\nRetrieval over semi-structured knowledge bases.\nComplementary to document-centric graph indices,\nsemi-structured knowledge base (SKB) retrievers\ndirectly combine text and explicit relations. KAR\ngrounds query expansion in KG structure (Xia et al.,\n2025b), and hybrid systems mix graph and text\nchannels with iterative refinement, e.g., Graph-\nSearch and HybGRAG (Yang et al., 2025b; Lee\net al., 2025); CoRAG highlights cooperative hy-\nbrid retrieval that preserves global semantic access\nbeyond local neighborhoods (Zheng et al., 2025).\nIn parallel, parametric retrievers such as MoR and\nmFAR learn to fuse lexical, semantic, and struc-\ntural signals for ranking (Lei et al., 2025; Li et al.,\n2025a). Across these variants, retrieval is framed\nas scoring candidates from a static index. ARK dif-\nfers in that retrieval is formulated as an interactive\nprocess: the model dynamically switches between\nglobal search and neighborhood expansion, guided\nby the query requirements and without relying on\ntask-specific supervision.\nAgents for multi-hop KG retrieval and QA. A\nseparate line of work treats the KG as an envi-\nronment for iterative interaction. Earlier agents\nfollow relation paths using reinforcement learning\nor learned policies (Das et al., 2018; Xiong et al.,\n2017; Sun et al., 2019; Asai et al., 2020). In the\nLLM era, tool-use frameworks such as ReAct (Yao\net al., 2023) and prompt-optimization methods such\nas AvaTaR (Wu et al., 2024a) enable interactive\nevidence gathering. Within KG retrieval, recent\ntraversal-based approaches expand from seed enti-\nties using prompted heuristics or learned policies,\nincluding Tree-of-Traversals, Think-on-Graph, and\nGraphFlow (Markowitz et al., 2024; Sun et al.,\n2024; Yu et al., 2025; Ma et al., 2025); related\nKG-grounded reasoning methods also emphasize\nmulti-step planning or navigation on the KG (Luo\net al., 2024; Sun et al., 2025). Traversal-based\nagents are effective when the correct starting en-\ntities are known, but they are prone to anchoring\nerrors and can over-commit to local neighborhoods\nonce exploration begins. In ARK, global search\nremains available throughout the trajectory, allow-\ning the agent to retain a complete view of the KG\nat each step. This design enables coordination be-\ntween global discovery and deep multi-hop expan-\nsion within a single retrieval process.\nExisting work varies in whether it treats retrieval\nas static ranking over an index or as sequential\ndecision-making on the graph, and in whether it\nrequires graph-specific supervision to learn a rank-\ning function or a traversal policy. ARK adapts its\nsearch strategy online through a minimal, graph-\nnative tool interface.\nIt is training-free; when\nneeded, its tool-use policy can be distilled into com-\npact models from interaction trajectories without\nground-truth relevance labels, improving efficiency\nwhile preserving retrieval quality.\n3\nAdaptive Retriever of Knowledge\nWe study retrieval over a knowledge graph\nG = \u27e8V, E, \u03d5V , \u03d5E, dV \u27e9, where V and E denote\nentities and edges, \u03d5V and \u03d5E assign a type to\n3\neach node and relation, and dV (v) denotes the text\nattributes associated with node v, such as titles,\ndescriptions, or other metadata. Given a natural-\nlanguage query Q, retrieval is formulated as an in-\nteractive process in which an agent A = \u27e8LLM, T \u27e9\nqueries the graph through a tool interface T (Yao\net al., 2023; Schick et al., 2023) and produces a tra-\njectory \u03c4 =\n\u0000(s1, A1, o1), . . . , (sT , AT , oT )\n\u0001\n. At\nstep t, st contains Q and the interaction history,\nAt is a sequence of tool invocations, and ot is the\nobservation returned after executing At.\nThroughout the trajectory, the agent maintains\nan ordered list of retrieved nodes R. At each step,\nit can SELECT nodes returned by tools and append\nthem to R, or terminate by issuing a dedicated\nFINISH action. Execution ends either when the\nagent calls FINISH or when the maximum trajectory\nlength Tmax is reached. The final retrieval output\nis the ranked list R = (v1, v2, . . . ), where earlier\nselections receive higher rank.\nTo rank candidate nodes returned by tools, we\nuse a relevance function rel(q, dV (v)) \u2208R\u22650 that\nscores node v for a textual subquery q provided\nby the agent as a tool parameter. We implement\nrel with BM25 (Robertson and Zaragoza, 2009)\nover an inverted index of node textual attributes\n(Manning et al., 2008), yielding fast and stable\nscoring for the many short, evolving subqueries\nissued during exploration.\n3.1\nTools\nWe implement the interaction described above\nthrough a small set of retrieval tools. Each tool\nreturns a candidate set of nodes, which the agent\nmay append to R or use to guide subsequent steps.\nGlobal search retrieves the k highest-scoring\nnodes in the graph under rel for an agent-issued\nsubquery q, as shown in Figure 1a:\nSearchG(q, k) := Top-k\nv\u2208V\nrel(q, dV (v))\nThis tool provides broad entry points into the graph\nand is primarily used (i) to locate entities related\nto the user query Q, which will then be used for\nfurther exploration, and (ii) to handle cases where\ndirect text matching suffices without requiring\nmulti-hop reasoning.\nNeighborhood exploration returns adjacent nodes\nof a node v filtered by optional node and edge type\nconstraints F := (FV , FE) selected by the agent\nas tool parameters, and optionally ranked using an\nagent-generated subquery q (Figure 1b).\nThe filtered one-hop neighborhood NF of a node\nv is defined as:\nNF (v) :=\n(\nu \u2208N(v)\n\u03d5V (u) \u2208FV ,\n\u03d5E({u, v}) \u2208FE\n)\nwhere N(v) denotes the open neighborhood of\nv and {u, v} denotes the edge connecting v and\nu, regardless of direction. Edge directionality and\nrelation types are exposed in the tool output. To\ncontrol the size of the retrieved neighborhood, we\nintroduce a fixed retrieval budget k \u2208N:\nNeighbors(v, q, F) := Top-k\nu\u2208NF (v)\nrel(q, dV (u))\nWe restrict Neighbors to single-hop expansion\nso that multi-hop exploration emerges through\ncomposition rather than fixed-depth traversal (Fig-\nure 1c).\n3.2\nParallel Exploration\nWe increase robustness by running n independent\ninstances of the same agent in parallel and aggre-\ngating their retrieved lists, akin to self-consistency\nand voting-based ensembling in LLM reasoning\n(Wang et al., 2023; Kaesberg et al., 2025). Each\nagent produces an ordered list of retrieved nodes\nR(i) = (v(i)\n1 , v(i)\n2 , . . . ) from an independent trajec-\ntory. We then combine these lists using a simple\nrank-fusion rule inspired by classical rank aggre-\ngation and data fusion methods (Fagin et al., 2003;\nCormack et al., 2009).\nConcretely, we concatenate the per-agent lists in\nagent order to form:\nL := R(1) \u2225R(2) \u2225\u00b7 \u00b7 \u00b7 \u2225R(n),\nand let VL be the set of unique nodes in L. The final\nranking R orders nodes by decreasing frequency in\nL (vote count), breaking ties by the earliest position\nat which a node appears in any trajectory, favoring\nnodes discovered earlier during exploration.\n3.3\nAgent Distillation\nWhile ARK operates on off-the-shelf models, its\nbehavior can be distilled into a smaller language\nmodel to reduce inference cost and latency (Hinton\net al., 2015). We adopt a standard teacher\u2013student\nparadigm in which a student model imitates the\n4\ntool-usage trajectories of a stronger teacher LLM\nvia supervised fine-tuning (Schick et al., 2023).\nTrajectory generation. For each training query Q\non a given graph G, we run the teacher agent to\ncollect trajectories \u03c4 as defined in Section 3. Each\ntrajectory contains the full interaction record: the\nagent\u2019s tool calls and parameters interleaved with\nthe resulting tool observations.\nTraining objective. The student is trained with\nnext-token prediction on the collected trajectories\n(Ouyang et al., 2022).\nWe compute loss only\non assistant-authored tokens; user messages and\ntool outputs are masked (Huerta-Enochian and Ko,\n2024; Shi et al., 2024). This trains the student to\nreproduce the teacher\u2019s decisions, which tools to\ninvoke and how to parameterize them, while tool\nexecution remains external to the model.\nLabel-free supervision. Importantly, distillation\ndoes not require ground-truth evidence nodes for\nqueries. Supervision is derived solely from teacher\ntrajectories, making the approach applicable in real-\nistic settings where relevance labels are unavailable:\none can run a strong teacher to generate trajectories\non a target graph and then fine-tune a smaller model\ndirectly from these interactions (Schick et al., 2023;\nKang et al., 2025).\n4\nExperimental Setup\nWe measure retrieval performance on STaRK, a\nbenchmark for entity-level retrieval over heteroge-\nneous, text-rich KGs (Wu et al., 2024b).\n4.1\nBenchmark\nSTaRK comprises three large, heterogeneous\nknowledge graphs. AMAZON is an e-commerce\ngraph with roughly 1M entities and 9.4M relations,\nconstructed from Amazon metadata, reviews (He\nand McAuley, 2016), and question\u2013answer pairs\n(McAuley et al., 2015). MAG is a scholarly graph\nwith 1.9M entities and 39.8M relations derived\nfrom the Microsoft Academic Graph (Wang et al.,\n2020). PRIME is a biomedical graph built from\nPrimeKG (Chandak et al., 2023), containing 129K\nentities and 8.1M relations. Each node is associated\nwith text-rich attributes, making STaRK a natural\ntestbed for hybrid retrieval over structured and tex-\ntual signals. Given a query, the retriever must return\na ranked list of nodes that support the answer. We\nreport the agent configuration and hyperparameters\nin Appendix A.2.\n4.2\nBaselines and metrics\nWe compare with representative retrieval-based and\nagent-based baselines, focusing on methods that\nreport results on all three graphs, as our goal is to\nevaluate performance consistently across different\nregimes and assess generality.\nRetrieval-based. BM25 (Robertson and Zaragoza,\n2009) is the same sparse, lexical retriever used for\nglobal search. We also include dense embedding re-\ntrievers that rank nodes by cosine similarity, using\nada-002 and GritLM-7B, an instruction-tuned 7B\nencoder (Muennighoff et al., 2025). mFAR (Li\net al., 2025a) is a multi-field adaptive retriever\nthat combines keyword matching with embedding\nsimilarity to learn query-dependent weights over\ndifferent node fields. KAR (Xia et al., 2025b)\naugments queries with knowledge-aware expan-\nsions and applies relation-type constraints during\nretrieval. MoR (Lei et al., 2025) is a trained re-\ntriever that combines multiple retrieval objectives.\nAgent-based. Think-on-Graph (Sun et al., 2024)\nis a training-free LLM agent that iteratively ex-\npands paths in the graph using beam search.\nGraphFlow (Yu et al., 2025) learns a policy for\ngenerating multi-hop retrieval trajectories using\nGFlowNets (Bengio et al., 2021). AvaTaR (Wu\net al., 2024a) is a tool-using agent that optimizes\nprompting from positive and negative trajectories.\nResults for KAR, mFAR, MoR, AvaTaR, and\nGraphFlow are reported as in their respective pa-\npers, which evaluate on the official STaRK splits\nand metrics. For Think-on-Graph, we report the\nnumbers provided in the GraphFlow study, which\nincludes a direct comparison to Think-on-Graph\nunder the same STaRK setup (Yu et al., 2025; Sun\net al., 2024).\nMetrics. We follow the STaRK protocol and report\nHit@1, Hit@5, Recall@20 (R@20), and Mean\nReciprocal Rank (MRR), which capture top-rank\nprecision, coverage of the ground-truth set, and\noverall ranking quality. Note that Hit@5 is reported\nin Table 5 in the Appendix.\n4.3\nDistillation Setup\nFor each graph, we collect teacher trajectories on\nthe training split to distill ARK into a smaller,\nlower-cost model (Section 3.3), offering a viable\nalternative when under tighter compute budgets.\nUsing GPT-4.1 as the teacher, we run ARK three\ntimes per training query with stochastic decoding\n(temperature = 0.7), producing three trajectories\n5\nAMAZON\nMAG\nPRIME\nAverage\nCategory\nMethod\nHit@1\nR@20\nMRR\nHit@1\nR@20\nMRR\nHit@1\nR@20\nMRR\nHit@1\nR@20\nMRR\nTraining-free\nRetrieval-based\nBM25\n44.94\n53.77\n55.30\n25.85\n45.69\n34.91\n12.75\n31.25\n19.84\n27.85\n43.57\n36.68\nada-002\n39.16\n53.29\n50.35\n29.08\n48.36\n38.62\n12.63\n36.00\n21.41\n26.96\n45.88\n36.79\nGritLM-7B\n42.08\n56.52\n53.46\n37.90\n46.40\n47.25\n15.57\n39.09\n24.11\n31.85\n47.34\n41.61\nKAR\n54.20\n57.24\n61.29\n50.47\n60.28\n57.51\n30.35\n50.81\n39.22\n45.01\n56.11\n52.67\nAgent-based\nThink-on-Graph + GPT-4o\n20.67\n25.81\n30.90\n23.33\n48.03\n36.38\n16.67\n54.35\n27.02\n20.22\n42.73\n31.43\nThink-on-Graph + LLaMA3\n4.21\n2.61\n5.25\n12.00\n6.77\n12.67\n21.92\n33.84\n26.61\n12.71\n14.41\n14.84\nARK\n55.82\n60.61\n64.77\n73.40\n84.47\n79.87\n48.20\n69.46\n57.68\n59.14\n71.51\n67.44\nARK + GPT-4o\n55.13\n57.18\n64.29\n67.01\n79.79\n75.46\n36.01\n60.13\n46.44\n52.72\n65.70\n62.06\nRequires training\non target graph\nRetrieval-based\nmFAR\n53.0\n66.3\n64.3\n55.9\n74.1\n64.3\n40.0\n72.6\n52.0\n49.63\n71.00\n60.20\nMoR\n52.19\n59.92\n62.24\n58.19\n75.01\n67.14\n36.41\n63.48\n46.92\n48.93\n66.14\n58.77\nAgent-based\nGraphFlow\n47.85\n36.15\n55.49\n39.09\n57.18\n47.82\n51.39\n79.71\n61.37\n46.11\n57.68\n54.89\nAvaTaR\n49.90\n60.60\n58.70\n44.36\n50.63\n51.15\n18.40\n39.30\n26.73\n37.55\n50.18\n45.53\nARK distilled\n54.99\n60.31\n64.24\n61.66\n81.39\n70.09\n31.87\n57.22\n41.08\n49.51\n66.31\n58.47\nTable 1: Retrieval performance on STaRK synthetic test sets.\nDark green and light green indicate best and\nsecond-best in the training-free category, respectively. Dark blue and light blue indicate best and second-best in\nthe requires-training category, respectively. Bold indicates the best result overall for each metric column.\nper query. We cap the distillation budget by sub-\nsampling up to 6,000 training queries per graph,\nyielding at most 18,000 trajectories per graph (full\nstatistics in Table 3) and summing to 94.4 million\ntokens. Each trajectory is limited to Tmax=20 steps\nand ends when the agent issues FINISH or reaches\nthe step limit. We apply no trajectory filtering or\nrejection sampling, preserving a label-free setting.\nWe then distill a Qwen3-8B (Yang et al., 2025a) stu-\ndent via supervised fine-tuning with LoRA adapters\n(Hu et al., 2021), using next-token prediction over\nassistant-authored tokens only. We train for one\nepoch with a 16,384-token context length using\nAdamW (Loshchilov and Hutter, 2019) at learning\nrate 1 \u00d7 10\u22125, selecting checkpoints via early stop-\nping on the official validation split. Training runs\non a single NVIDIA H100 GPU and completes in\napproximately five hours.\n5\nResults\n5.1\nBenchmarking\nTable 1 reports retrieval performance on STaRK,\ngrouped by training regime. Across all methods\nassessed, ARK achieves the best average perfor-\nmance.\nClassical retrievers remain strong baselines on\nAMAZON, when queries are predominantly de-\nscriptive. By incorporating local structural cues,\nKAR improves over lexical methods, but its shal-\nlow neighborhood expansion is limited on multi-\nhop queries (Xia et al., 2025b).\nThink-on-Graph and GraphFlow highlight the\nbenefits of multi-hop traversal, performing well\non PRIME. Think-on-Graph is appealing due to\nits training-free setup, and GraphFlow shows that\nstrong performance can be achieved with smaller\nbackbones through reinforcement learning. How-\never, both degrade on AMAZON\u2019s text-heavy,\nbroad queries, as they lack global search primitives\nand can be sensitive to brittle anchoring and entity\nidentification (Sun et al., 2024; Yu et al., 2025).\nARK performs consistently across regimes and\nis especially strong on MAG. This pattern aligns\nwith its tool design. Global search offers a reliable\nanchor for text-heavy queries and supports strong\ntop-rank accuracy on AMAZON. Typed, query-\nranked one-hop expansion enables controlled multi-\nhop evidence gathering in relational settings, con-\ntributing to the best results on MAG and solid per-\nformance on PRIME, where it is surpassed only by\nthe RL-trained GraphFlow.\nWhile ARK uses a large backbone, the distilled\nvariant preserves most of these gains with a sub-\nstantially smaller Qwen3-8B model via label-free\ntrajectory imitation (Section 5.5).\n5.2\nText vs. Relational Adaptive Retrieval\nSTaRK reports, for each graph, the average share of\nqueries that are primarily textual versus primarily\nrelational (multi-hop) (Fig. 5 in Wu et al. (2024b)).\nWe use these proportions as a reference and com-\npare them to ARK\u2019s tool-call allocation on our\nevaluation split. Concretely, we treat the fraction\nof global search calls as a proxy for text-centric\nevidence use and the fraction of neighborhood ex-\nploration calls as a proxy for relation-centric evi-\ndence. Figure 2 shows a proportional match: on\n6\n0%\n20%\n40%\n60%\n80%\n100%\nAMAZON\nMAG\nPRIME\n87.7%\n12.3%\n34.7%\n65.3%\n47.7%\n52.3%\nSTaRK: Textual\nTool: Global Search\nSTaRK: Relational\nTool: Neighborhood Exploration\nFigure 2: Thin bars show the share of text- vs. relation-\ncentric queries in STaRK; thick bars show ARK\u2019s tool-\ncall use. These STaRK annotations are not provided to\nARK; instead, ARK autonomously shifts tool use to\nmatch the dominant query type.\nAMAZON, where queries are mostly textual, ARK\nrelies almost entirely on global search (87.7%),\nwhereas on MAG and PRIME, where relational\nrequirements dominate, ARK shifts toward neigh-\nborhood exploration to traverse multi-hop evidence\n(65.3% and 52.3%, respectively).\nThis finding\nshows that ARK autonomously adapts retrieval,\nchoosing tools to match what each query needs\nrather than following a fixed retrieval recipe.\n5.3\nImpact of Toolset Design\nWe conduct various ablation studies to assess the\nimpact of toolset design choices. Table 2 demon-\nstrates that neighborhood exploration is the main\nsource of gains on relational, multi-hop queries. Re-\nmoving this tool decreases performance on MAG\nand PRIME as the system is then limited to global\nlexical search without graph traversal. On AMA-\nZON, performance drops more moderately and\nmoves toward lexical baselines (Table 1).\nSetup\nAMAZON\nMAG\nPRIME\nHit@1\nR@20\nHit@1\nR@20\nHit@1\nR@20\nFull\n58.5\n60.2\n79.2\n83.3\n49.2\n73.3\nw/o Neighbors\n54.5\n55.4\n30.5\n39.4\n23.1\n40.5\nNeighbors w/o q\n56.0\n57.9\n72.1\n79.8\n44.7\n68.3\nNeighbors w/o F\n55.5\n59.9\n79.2\n84.8\n42.2\n65.0\nTable 2: Impact of toolset design on retrieval perfor-\nmance across graphs. w/o Neighbors removes neigh-\nborhood exploration entirely. Neighbors w/o q dis-\nables query-based ranking within the neighborhood, and\nNeighbors w/o F disables type-based filtering. Results\nare reported on a random 10% subset of the test split.\nWe further separate two complementary con-\ntrols in neighborhood exploration. Disabling query-\nbased ranking causes smaller but consistent drops,\nsuggesting that lexical matching within a local\nneighborhood helps surface relevant neighbors and\nprevents drift toward high-degree distractors. Dis-\nabling type-based filtering is more detrimental, es-\npecially in heterogeneous graphs such as PRIME\n(Table 4). In such environments, type constraints\nare important to direct the agent towards semanti-\ncally relevant edges and nodes, preventing search\nfrom drifting into unrelated parts of the graph.\nNote that we do not ablate global search because\nit is required: it maps query text to candidate nodes\nand provides the node identifiers needed to start\nneighborhood expansion. Without this initial an-\nchor, the agent cannot reliably enter the right part\nof the graph, so the system fails outright.\n5.4\nCompute-Performance Trade-offs\nWe next study how retrieval quality scales with\nthe inference-time budget. Figure 3 shows that\nperformance improves monotonically as compute\nincreases, moving from single-agent settings to par-\nallel multi-agent configurations. Additional com-\npute helps most on queries that require multi-hop\nexpansion, and yields smaller gains when global\nlexical search is already sufficient.\nParallelization yields performance benefits with\nminimal overhead. Increasing the agent count\u2013\nparticularly the transition from one to two agents\u2013\nresults in substantial gains while only modestly in-\ncreasing latency. Because agents run independently,\nend-to-end latency is determined by the bottleneck\nof the slowest agent rather than the cumulative run-\ntime of all agents.\nARK\nprovides\nan\ninterpretable\nbudget-\nperformance landscape.\nPractitioners can fix a\nlatency or compute budget and choose an operating\npoint in Figure 3 that matches their needs, trading\noff depth and parallelism to balance quality and\ncost across graph regimes.\n5.5\nImpact of Distillation\nWe also study how the distillation budget affects\nfinal performance. Figure 4 compares Qwen3-4B\nand Qwen3-8B students trained on increasing num-\nbers of teacher trajectories across the three STaRK\ngraphs; full results for all metrics are in Table 5.\nDistillation is data-efficient: using 10% of the\ntrajectories recovers roughly half of the total im-\nprovement achieved with the full training set. This\nmakes distillation practical when collecting trajec-\ntories is costly. In our setup, distilling the 600-\nquery setting can be done in 30 minutes on a single\nH100 GPU.\n7\n1 agent\n2 agents\n3 agents\nHit@1\n51.2\n54.1\n55.0\n55.3\n55.4\n54.0\n55.3\n55.6\n55.6\n55.6\n55.0\n55.9\n56.0\n55.9\n55.8\nAMAZON\n46.9\n59.0\n66.2\n68.9\n69.4\n53.5\n65.0\n70.4\n71.9\n72.1\n56.7\n67.4\n71.7\n73.1\n73.4\nMAG\n25.8\n36.1\n41.5\n43.8\n44.1\n31.6\n41.4\n45.1\n46.0\n46.1\n34.4\n43.9\n47.3\n48.0\n48.3\nPRIME\n1 agent\n2 agents\n3 agents\nRecall@20\n50.2\n53.1\n54.0\n54.5\n54.6\n56.3\n58.2\n58.8\n58.8\n58.8\n58.8\n60.2\n60.6\n60.6\n60.6\n51.5\n64.6\n71.7\n74.7\n75.2\n60.1\n72.9\n79.1\n81.3\n81.7\n64.2\n77.0\n82.3\n83.9\n84.3\n36.2\n48.9\n55.0\n57.6\n58.1\n45.5\n58.3\n63.4\n65.3\n65.7\n49.9\n62.6\n67.0\n69.0\n69.3\n5\n7\n10\n15\n20\n1 agent\n2 agents\n3 agents\nLatency (s)\n7.70\n7.90\n7.98\n8.02\n8.04\n8.73\n8.99\n9.12\n9.17\n9.19\n9.22\n9.52\n9.67\n9.74\n9.76\n5\n7\n10\n15\n20\n6.86\n7.93\n8.78\n9.08\n9.14\n9.04\n10.27\n11.21\n11.63\n11.76\n9.72\n11.99\n12.98\n13.48\n13.63\n5\n7\n10\n15\n20\n7.16\n8.34\n9.12\n9.67\n9.85\n8.28\n9.69\n10.67\n11.37\n11.69\n8.94\n10.40\n11.48\n12.33\n12.74\n0.52\n0.54\n0.56\n0.5\n0.6\n0.7\n0.3\n0.4\n0.55\n0.60\n0.6\n0.7\n0.8\n0.4\n0.5\n0.6\n8\n9\n8\n10\n12\n8\n10\n12\nNumber of steps (Tmax)\nFigure 3: Retrieval quality and latency as a function of inference-time budget. Heatmaps report Hit@1, Recall@20,\nand end-to-end latency (seconds) on each STaRK graph. Moving from the top left (shallow trajectories, single agent)\nto the bottom right (deeper trajectories, multi-agent) allocates more compute and improves retrieval performance at\nthe cost of higher latency. Color scales are normalized within each graph and metric for readability.\nHit@1\nRecall@20\nMRR\n0\n20\n40\n60\n80\n100\nAMAZON\nHit@1\nRecall@20\nMRR\n0\n20\n40\n60\n80\n100\nMAG\nHit@1\nRecall@20\nMRR\n0\n20\n40\n60\n80\n100\nPRIME\nQwen3-4B\nQwen3-8B\nGPT-4.1\nBase (light)\nDistilled 600 (medium)\nDistilled 6000 (dark)\nFigure 4: Evaluation of the same ARK pipeline on the STaRK test sets while varying only the LLM backbone\n(Qwen3-4B/8B base, Qwen3-4B/8B distilled, or GPT-4.1). \u201cDistilled 600\u201d and \u201cDistilled 6000\u201d denote Qwen\nbackbones fine-tuned on trajectories generated by GPT-4.1 from 600 or 6000 training queries per graph, respectively\n(three trajectories per query; tool calls and observations only; no label supervision).\n0\n10\n20\n30\nSuccessful\nMAG\nSuccessful\nPRIME\n0\n5\n10\n15\n20\n25\n30\n0\n10\n20\n30\nUnsuccessful\n0\n5\n10\n15\n20\n25\n30\nUnsuccessful\nNumber of neighborhood exploration calls\nPercentage of trajectories\nMedian\nMean\nFigure 5: Distribution of the number of neighborhood\nexploration calls, split by successful (Hit@5) and un-\nsuccessful trajectories.\nStudent size matters most on PRIME. Because\nbase models perform poorly in this regime, distil-\nlation is more important, and the teacher-student\ngap is the largest. The stronger performance of the\n8B student suggests that higher-capacity models\nbetter absorb the long-horizon, high-branching ex-\nploration patterns required for complex biomedical\nreasoning in PRIME.\n5.6\nNeighborhood Exploration vs. Retrieval\nWe next examine how neighborhood exploration\nrelates to retrieval success on MAG and PRIME.\nHere, successful trajectories refers to the runs (i.e.,\ntool call sequences) that retrieve the correct nodes\nand therefore score as correct on the retrieval metric\nfor that query."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of \"Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval\"\n\n#### Main Contributions:\n1. **ARK (ADAPTIVE RETRIEVER OF KNOWLEDGE)**: Introduces a novel agent-based knowledge graph retriever designed to dynamically balance breadth and depth retrieval modes, providing language models control over this trade-off.\n2. **Two-Operation Toolset**: ARK employs two operations\u2014global lexical search over node descriptors and one-hop neighborhood exploration\u2014to achieve multi-hop traversal without requiring seed selection or pre-set hop depths.\n3. **Adaptive Query Handling**: The system adapts its tool usage to the query, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries.\n\n#### Methodology:\n1. **Knowledge Graph Representation**: ARK operates on a knowledge graph representation where entities (V) are connected by typed edges (E), and nodes have associated text attributes.\n2. **Tool Interface**: The system uses a minimal set of tools: Global Search to retrieve broad sets of candidates, and Neighborhood Exploration for targeted multi-hop retrieval starting from previously retrieved nodes.\n\n#### Key Findings:\n1. **Performance on STaRK Benchmark**: ARK achieves the best average performance across all methods tested, with improvements in Hit@1 (up to 31.4%) and MRR (up to 28.0%) over retrieval-based and agent-free training.\n2. **Distillation of Tool-Use Policy**: The tool-use policy is distilled into an 8B model via label-free imitation learning from teacher trajectories, improving Hit@1 by +7.0, +26.6, and +13.5 points on the AMAZON, MAG, and PRIME datasets respectively.\n3. **Adaptive Query Handling**: ARK adapts its tool usage to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries.\n\n#### Conclusions:\n1. **Advantages of ARK**: ARK offers a robust framework that balances breadth and depth retrieval modes dynamically without requiring seed selection or pre-set hop depths.\n2. **Distillation Efficiency**: The distilled model retains most performance gains from the teacher, reducing inference cost while preserving quality.\n3. **Generalizability**: ARK's design allows for consistent performance across different query types (textual vs. relational), making it suitable for a wide range of knowledge graph retrieval tasks.\n\n### Detailed Breakdown:\n\n#### Introduction:\n- The paper addresses challenges in retrieving evidence from knowledge graphs, balancing broad search with multi-hop traversal to follow relational links.\n- Existing methods either provide shallow coverage or rely on fragile seed selection and pre-set hop depths. ARK introduces an agent-based retriever that dynamically balances these modes.\n\n#### Methodology:\n- **Knowledge Graph Representation**: The system operates on a graph where nodes (entities) are connected by typed edges, with associated text attributes.\n- **Tool Interface**: Uses two operations: Global Search for broad retrieval and Neighborhood Exploration for targeted multi-hop traversal starting from previously retrieved nodes.\n\n#### Key Findings:\n1. **Performance on STaRK Benchmark**:\n   - ARK achieves the best average performance across all tested methods.\n   - It improves Hit@1 by up to 31.4% and MRR by up to 28.0% over retrieval-based and agent-free training.\n\n2. **Distillation of Tool-Use Policy**:\n   - The tool-use policy is distilled into an 8B model via label-free imitation learning, improving Hit@1 by +7.0, +26.6, and +13.5 points on the AMAZON, MAG, and PRIME datasets respectively.\n   \n3. **Adaptive Query Handling**:\n   - ARK adapts its tool usage to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries.\n\n#### Conclusions:\n- ARK provides a robust framework that dynamically balances breadth and depth retrieval modes without requiring seed selection or pre-set hop depths.\n- The distilled model retains most performance gains from the teacher while reducing inference cost.\n- Its design allows consistent performance across different query types, making it suitable for various knowledge graph retrieval tasks."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Fine-Grained Zero-Shot Composed Image Retrieval\nwith Complementary Visual-Semantic Integration\nYongcong Ye1, Kai Zhang1*, Yanghai Zhang1, Enhong Chen1, Longfei Li3, Jun Zhou2\n1 State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China\n2 Zhejiang University, Hangzhou, China\n3 Independent Researcher\n{ycongy, yhzhang0612}@mail.ustc.edu.cn, {kkzhang08, cheneh}@ustc.edu.cn\nlilongfei123@hotmail.com, junzhougucas@gmail.com\nAbstract\u2014Zero-shot composed image retrieval (ZS-CIR) is\na rapidly growing area with significant practical applications,\nallowing users to retrieve a target image by providing a reference\nimage and a relative caption describing the desired modifications.\nExisting ZS-CIR methods often struggle to capture fine-grained\nchanges and integrate visual and semantic information effectively.\nThey primarily rely on either transforming the multimodal\nquery into a single text using image-to-text models or employing\nlarge language models for target image description generation,\napproaches that often fail to capture complementary visual\ninformation and complete semantic context. To address these\nlimitations, we propose a novel Fine-Grained Zero-Shot Com-\nposed Image Retrieval method with Complementary Visual-\nSemantic Integration (CVSI). Specifically, CVSI leverages three\nkey components: (1) Visual Information Extraction, which not\nonly extracts global image features but also uses a pre-trained\nmapping network to convert the image into a pseudo token,\ncombining it with the modification text and the objects most\nlikely to be added. (2) Semantic Information Extraction, which\ninvolves using a pre-trained captioning model to generate multi-\nple captions for the reference image, followed by leveraging an\nLLM to generate the modified captions and the objects most\nlikely to be added. (3) Complementary Information Retrieval,\nwhich integrates information extracted from both the query\nand database images to retrieve the target image, enabling\nthe system to efficiently handle retrieval queries in a variety\nof situations. Extensive experiments on three public datasets\n(e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI\nsignificantly outperforms existing state-of-the-art methods. Our\ncode is available at https://github.com/yyc6631/CVSI.\nIndex Terms\u2014image retrieval, composed image retrieval, mul-\ntimodal, zero-shot learning\nI. INTRODUCTION\nTraditional image-text retrieval methods [1]\u2013[3] typically\ninvolve retrieving text from images or retrieving images from\ntext, where the input is usually unimodal data. However, Com-\nposed Image Retrieval (CIR) [4]\u2013[6] focuses on more realistic\nscenarios. When a user wants to make specific modifications\nto an image and retrieve a target image that aligns with both\nthe image content and the modification\u2019s semantic information,\nCIR is required. The input to CIR consists of multimodal data,\nincluding a reference image and modification text. It enables\nmore precise retrieval based on the user\u2019s specific needs and\n* Corresponding author.\n(a) Existing methods for ZS-CIR\na photo of $ that is green and is viewed\nfrom the front\na green double decker bus is parked next to\na bus, viewed from the front\nMethod 1: textual inversion\nMethod 2: LLM-based\nDatabase Images\nRetrieval\nRetrieval\nis green and is\nviewed from\nthe front\nModification Text\nQuery\nReference Image\n(b) Our proposed method for ZS-CIR\na photo of $ that is green\nand is viewed from the\nfront. And the photo\nshould have a green bus\nviewed from the front\nReference image features\nDatabase\nImages\nfeatures\ntwo green double\ndecker buses parked\nunder a bridge\ncomplementary\ninformation\nretrieval\na green double decker\nbus is parked next to a\nbus, viewed from the\nfront\nis green and is\nviewed from\nthe front\nModification Text\nQuery\nReference Image\nDatabase Images\nFig. 1. Comparison of methods for ZS-CIR. (a) Existing methods. (b) Our\nproposed method.\nhas potential applications in fields such as E-commerce, among\nothers [7]\u2013[9], which has led to growing attention.\nHowever, the CIR datasets typically requires training data\nin the form of triplets \u27e8Ir, T, It\u27e9, where Ir is the reference\nimage, T is the modification text, and It is the target image.\nUnlike image-text paired datasets, collecting such triplets is\nchallenging, which limits the widespread application of CIR\nin more domains. To address this data bottleneck, recent\nresearch has focused on Zero-Shot Composed Image Retrieval\n(ZS-CIR) task [10], which aims to generalize CIR models\nwithout relying on pre-collected triplet datasets. There are\ntwo main existing approaches to ZS-CIR, as shown in Figure\n1(a). The first method [10]\u2013[15] is based on textual inversion,\nwhich requires pre-training a mapping network. The pre-\ntrained mapping network maps the image into a pseudo token,\nwhich is then concatenated with the modification text to form a\nsingle text for retrieval in the CLIP feature space. The second\nmethod [16], [17], based on an LLM, bypasses pre-training\nby using a pre-trained captioning model to generate a caption\nfor the reference image, which is then combined with the\nmodification text to produce the target caption for retrieval.\nAlthough these methods have made significant progress in\nZS-CIR, they overlook the complementary nature of visual\narXiv:2601.14060v1  [cs.CV]  20 Jan 2026\nand semantic information, focusing only on one aspect for\ncomplete extraction. As shown in Figure 1(a), method 1 (i.e.,\ntextual inversion) combines the pseudo token and modification\ntext into a single query using a fixed template for retrieval,\nwhich contains visual information but lacks complete and\ncoherent semantic information (e.g., \u201ca green double decker\nbus is parked next to a bus\u201d). In contrast, method 2 (i.e.,\nLLM-based) generates captions for the reference image, but\nstruggles to fully capture the rich visual information (e.g.,\nthe shape and style of the double decker bus in the ref-\nerence image). Moreover, both methods fail to enable fine-\ngrained retrieval of the changed objects (e.g., retrieving \u201ca\ngreen bus\u201d instead of other objects in the image) and ne-\nglect the extraction and complementarity of both visual and\nsemantic information in the target image. The ZS-CIR task\nis inherently a retrieval task that integrates both visual and\nsemantic information. According to [18], simple modifications\nrely more on visual information, while complex ones depend\nmainly on semantic information for retrieval. Missing either\ntype of information negatively impacts the model\u2019s retrieval\nperformance. Therefore, as shown in Figure 1(b), we propose\nto integrate complementary visual and semantic information\nextracted separately from the Query and Database Images,\nwhich further enables fine-grained composed image retrieval.\nIndeed, there are many technical challenges inherent in\ndesigning effective solutions to integrate visual-semantic in-\nformation into ZS-CIR process. 1) How to extract rich and\ncomplete visual and semantic information? In previous ZS-\nCIR methods, while the use of pseudo tokens allows for\nrelatively complete extraction of visual information, some loss\nof visual information still occurs during the mapping process.\nFor LLM-based methods, Yang et al. [17] proposed a method\nfor generating dense captions in the ZS-CIR task, which\ncaptures rich semantic information. 2) How to achieve fine-\ngrained retrieval, enabling retrieval at the object level? Most\nprior methods [10], [11], [13], [14] rely on global features for\nretrieval, which miss fine-grained details and negatively impact\nretrieval performance. Some methods [12], [15] extract local\nfeatures by training a model for retrieval, but this increases\nboth model complexity and training time. 3) How to effectively\nintegrate the visual and semantic information from both\nthe Query and Database Images for retrieval? Previous\napproaches focus on extracting either visual or semantic\ninformation from the Query, but integrating multiple sources\nof visual and semantic information for retrieval presents a\nsignificant challenge.\nTo address these challenges, we propose a novel fine-grained\nzero-shot composed image retrieval method with Complemen-\ntary Visual-Semantic Integration (CVSI). As shown in Figure\n2, our CVSI consists of three key components: (1) Visual\nInformation Extraction, (2) Semantic Information Extraction,\nand (3) Complementary Information Retrieval. For Challenge\n1, to extract comprehensive visual information, we leverage\nthe mapping network in LinCIR [14], a text-based training\nmethod, to generate pseudo tokens from images. Additionally,\nCLIP is employed to extract global image features as sup-\nplementary visual information. To capture complete semantic\ninformation, we use BLIP-2 [19] to generate dense captions,\nas described in [17]. Then, an LLM is employed to generate\nmodified captions, enriching the semantic information. For\nChallenge 2, we use an LLM to generate the objects most\nlikely to be added based on the modification text and captions,\nwhich allows us to pinpoint changes at the object level rather\nthan the entire image, enabling fine-grained retrieval. For\nChallenge 3, in the real-world ZS-CIR task, the combinations\nof images and texts are often complex and diverse, and\nthe impact of visual and semantic information on the final\nretrieval results varies. Therefore, we assign weights based\non the nature of the query and separately perform weighted\nsummation on the query and target features, followed by\nretrieval in the CLIP feature space.\nOur main contributions can be summarized as follows:\n\u2022 We propose a novel fine-grained zero-shot composed\nimage retrieval method with Complementary Visual-\nSemantic Integration (CVSI). It not only fully considers\nthe complementarity of visual and semantic information\nin the query but also in the database images, enabling\ncomprehensive retrieval.\n\u2022 We propose a fine-grained retrieval method that uses an\nLLM to generate the objects most likely to be added in the\ntarget image and incorporates them as part of the query\nfeatures, which enables simple yet efficient fine-grained\nretrieval, enhancing retrieval performance.\n\u2022 We propose a complementary information fusion and\nretrieval method that assigns weights based on the nature\nof the query, fully considering the varying importance\nof visual and semantic information in different contexts,\nwhich enables better adaptation to the real-world ZS-CIR\ntask.\n\u2022 Extensive experiments on three public datasets validate\nthe effectiveness of our proposed method, with CVSI\nachieving new state-of-the-art results on most metrics.\nII. RELATED WORK\nA. Composed Image Retrieval\nThe field of image retrieval has garnered significant at-\ntention from many researchers [20], [21]. One area that has\nremarkable progress recently is Composed Image Retrieval\n(CIR) [4]\u2013[6], a task focused on retrieving target images based\non queries composed of a reference image and a modification\ntext. Existing CIR methods can be primarily divided into\ntwo categories. The first category [5], [6], [22], [23] adopts\nstandard deep learning methods to extract features from both\nvisual and textual modalities. On the other hand, the second\ncategory [18], [24], [25] employs vision-language pre-trained\nmodels for extracting features, such as CLIP [1]. However,\nthese methods rely on supervised pre-training, which requires a\nlarge amount of manually labeled triplets for training, making\nit time-consuming and labor-intensive.\nThus, Zero-Shot CIR [10] is introduced because it does\nnot require manually annotated triplets for training. Methods\nVisual\nEncoder\nMapping\nNetwork\n$\nReference image\nfeatures\nReference image\nis green and is\nviewed from\nthe front\nModification text\nBLIP-2\na double decker\nbus is parked next\nto a bus\nLLM\nN\u00d7 captions\na green double decker\nbus is parked next to a\nbus, viewed from the\nfront\nN\u00d7 Modified captions\n\u2160. Visual Information Extraction\n\u2161. Semantic Information Extraction\na green bus viewed\nfrom the front\nObjects most likely added\na photo of\n$ that is green and is\nviewed from the front.\nAnd the photo should\nhave a green bus viewed\nfrom the front\nFine-grained information\nReference image\nis green and is\nviewed from\nthe front\nModification text\n\u2160. Visual\nInformation\nExtraction\n\u2161. Semantic\nInformation\nExtraction\nDatabase images\nReference image\nfeatures\nText\nEncoder\nFine-grained\nfeatures\nText\nEncoder\nAverage modified\ncaptions features\nFine-grained\nfeatures\nmean\nAverage modified\ncaptions features\nTarget images\nfeatures\n(1-\u03b3)\u00b7\n\u00b7\u03b2\n\u00b7(1-\u03b1-\u03b2)\nText\nEncoder\nAverage captions\nfeatures\nmean\nAverage captions\nfeatures\n\u00b7\u03b1\n\u03b3\u00b7\n\u2162. Complementary Information Retrieval\nTarget image\n\u00b7\u00b7\u00b7\nCosine\nsimilarity\nQuery\n\u2160. Visual\nInformation\nExtraction\n\u2161. Semantic\nInformation\nExtraction\nFig. 2. Overview of the proposed CVSI. The CVSI consists of three key components: (1) Visual Information Extraction, (2) Semantic Information Extraction,\nand (3) Complementary Information Retrieval.\nfor Zero-Shot CIR can be broadly categorized into two types.\nOne approach [10]\u2013[15] involves using a pre-trained mapping\nmodel to transform the reference image into a pseudo token,\nwhich is then combined with the modification text for retrieval.\nSpecifically, Gu et al. [14] proposed a method that trains the\nmapping model using only text. The other approach [16], [17]\ninvolves captioning the reference image using a pre-trained\nVision-Language Model (VLM), and then using an LLM to\nrecombine the caption based on the modification text for\nsubsequent retrieval.\nHowever, the approach using mapping models often results\nin retrieval queries that lack sufficient contextual semantic\ncoherence. When the modification text is complex, retrieval\nperformance tends to degrade. On the other hand, another\napproach of converting reference image information into cap-\ntions often struggles to capture the full range of information\nin an image, or to highlight its key aspects. This results in\na lack of refinement in visual information, which can lead to\npoor retrieval performance when the reference image itself is\ncrucial. To overcome this challenge, we propose a method that\ncomplements visual and semantic information, incorporating\nchanged details to enhance fine-grained retrieval.\nB. Vision-Language Pretrained Models for CIR\nPretrained models have shown remarkable effectiveness\nacross domains [26], [27]. Recently, Vision-Language Pre-\ntrained (VLP) models trained on large-scale image-text pairs\nhave gained attention for their versatility and strong perfor-\nmance on various downstream tasks [1], [2], [28]. Among\nthese models, CLIP [1] stands out as one of the most\nrepresentative, demonstrating excellent performance across a\nwide range of retrieval tasks, including CIR. Additionally,\nBLIP [29], primarily designed for multimodal understanding\nand generation tasks, has also shown impressive cross-modal\nalignment and generation capabilities across multiple tasks. In\nthis paper, for the Zero-Shot CIR task, we leverage the BLIP-\n2 [19] model to generate captions for reference images and\nuse CLIP to extract features for retrieval, fully exploiting the\nstrengths of VLP models.\nIII. METHOD\nIn this section, we first formulate the problem and then pro-\nvide a detailed description of our proposed CVSI framework.\nAs shown in Figure 2, CVSI consists of three main com-\nponents: Visual Information Extraction, Semantic Information\nExtraction and Complementary Information Retrieval.\nA. Problem Formulation\nZS-CIR is essentially a multimodal retrieval task. Unlike\ntraditional multimodal retrieval tasks, its input includes both\nimage and text modalities. Given a reference image Ir and a\nmodification text T, the goal is to retrieve a target image It\nfrom an image database D = {Ik}K\nk=1, where the target image\nis required to be visually and semantically consistent with the\nreference image Ir modified by T. Since this is a zero-shot\ntask, it requires that no training samples are available.\nB. Visual Information Extraction\nIn this component, we utilize a pre-trained VLM, such\nas CLIP or BLIP, which includes a vision encoder and a\ntext encoder as feature extractors. For the reference image\nIr, the frozen visual encoder fv extracts the global features\nof the image as qv = fv(Ir) \u2208Rd\u00d71, where d is the\nfeature dimension. Then, following LinCIR [14], we leverage\na mapping network trained solely on text, which maps the\nimage to a pseudo token, denoted as\n$ = \u03a6(qv),\n(1)\nwhere $ is the pseudo token and \u03a6 is the frozen mapping\nnetwork. Then, we concatenate the pseudo token with the\nmodification text T and the objects most likely added to form\na prompt in the following format: \u201da photo of $ that [T]. And\nthe photo should have [objects most likely added]\u201d, which\nserves as the fine-grained information FG. Finally, we extract\nthe text features from the FG by passing it through the frozen\ntext encoder ft, resulting in\nqf = ft(FG),\n(2)\nwhere qf represents the fine-grained features. Similarly, for\neach image Ik in the database D, we extract the visual features\ntv = fv(Ik) \u2208Rd\u00d71 for subsequent retrieval.\nC. Semantic Information Extraction\nIn this component, we aim to employ an image captioning\nmodel to derive a textual description for the reference image\nor the database images. Specifically, we utilize a pre-trained\ncaptioning model to generate natural language captions for\nthe images, and for this, we have chosen BLIP-2 [19]. Fur-\nthermore, to obtain dense captions, we follow LDRE [17] and\nemploy nucleus sampling [30] during the caption generation\nprocess to enhance the diversity and semantic richness of\nthe generated captions. The generation process of multiple\ncaptions can be described as follows:\nCB = {CB\ni }N\ni=1 = BLIP-2(Ir),\n(3)\nwhere CB is the set of captions generated from the reference\nimage. Similarly, the following equation holds:\nCT = {CT\ni }N\ni=1 = BLIP-2(Ik),\n(4)\nwhere CT is the set of captions generated from Ik in the\ndatabase. For the query, by generating multiple captions for\nthe reference image, we can capture rich semantic information\nfrom the reference image, thus avoiding the potential loss of\nkey semantic information in the query when only one caption\nis used. For the database images, the rich semantic information\nfrom the images to be retrieved is also involved in the final\nretrieval process. Unlike previous methods where only image\nfeatures are used in the final retrieval, we pass the set of\ncaptions CT through the frozen text encoder ft, then average\nall the text features to obtain the average caption features tc,\nwhich helps to obtain a more balanced semantic representation.\nFormally, we have:\ntc = 1\nN\nN\nX\ni=1\nft(CT\ni ).\n(5)\nWhen extracting semantic information, for the query, if we\nonly perform feature extraction on the modification text, the\nsemantic information will solely reflect the modified parts,\nthereby losing the relatively unchanged parts. On the other\nhand, if we only extract features from the reference image\ncaptions, the most crucial modification information may be\nlost. Although some previous methods [10]\u2013[15] have used\nfixed templates to combine them, such approaches result in\nrigid sentence structures and lack semantic coherence. There-\nfore, following previous LLM-based methods, we also use an\nLLM to generate multiple modified captions for each query.\nAdditionally, if we can identify which objects are most likely\nto be added to the reference image in order to approximate\nthe target image, we can direct the retrieval towards specific\nobjects, making the retrieval more intuitive and enabling fine-\ngrained search capabilities.\nFor the objects most likely added, we define a simple prompt\ntemplate pa that combines the first reference image caption CB\n1\nwith the modification text T, forming a complete prompt. This\nprompt is then used by the LLM to generate the objects most\nlikely added. Formally, we have:\nFadd = LLM(pa(CB\n1 , T)),\n(6)\nwhere Fadd refers to the objects most likely added. Addition-\nally, the description of the prompt template pa is shown in\nFigure 3.\nPrompt template for the LLM to generate the objects most likely added:\nI have an image. Given an instruction to edit the image, carefully predict the objects that are\nmost likely to be added in the edited image. I will provide the image content beginning with\n\u201cImage Content:\u201d. The instruction I provide will begin with \u201cInstruction:\u201d. For Objects most\nlikely added, provide a noun phrase describing the new objects introduced by the instruction,\nadding any necessary details based on the modification.\nYour response should be structured as follows:\nObjects most likely added: [Noun phrase with relevant details from the instruction]\nPlease keep your responses short and focused on the modifications.\nExample:\nImage Content: A man is holding a large fish on a stick.\nInstruction: Add a crowd of people watching behind a fence.\nObjects most likely added: crowd of people, fence\nImage Content: The dog is wearing a red hat.\nInstruction: has a dog of a different breed and shows a jolly roger.\nObjects most likely added: a dog of a different breed, a jolly roger\nFig. 3. The description of the designed prompt template pa.\nFor generating modified captions, we adopt the prompt\ntemplate pm as used in [16], which can be used to combine\nCB\ni (where i = 1, . . . , N) with the modification text T to form\na complete prompt. This prompt is then used by the LLM to\ngenerate a set CM of N modified captions. We then pass CM\nthrough the frozen text encoder ft, and average all the text\nfeatures to obtain the average modified caption features qm,\nensuring that the final semantic features used in retrieval are\nbalanced across multiple key pieces of information. Thus, we\nhave:\uf8f1\n\uf8f2\n\uf8f3\nCM\ni\n= LLM(pm(CB\ni , T)),\nfor\ni = 1, . . . , N\nqm = 1\nN\nPN\ni=1 ft(CM\ni ),\n(7)\nwhere CM\ni\nrepresents the i-th modified caption, and qm rep-\nresents the final average modified caption features involved in\nretrieval.\nD. Complementary Information Retrieval\nWhether it is the previous textual inversion method or\nthe LLM-based method, in the end, image retrieval is per-\nformed through text, which essentially transforms the ZS-\nCIR task into a unimodal retrieval task, thus mitigating the\nissues caused by misalignment between modalities. However,\nthese methods overlook the fact that ZS-CIR is inherently a\nmultimodal retrieval task. If more modalities are involved in\nthe final retrieval process, and the retrieved target can provide\nmore information (e.g., shifting from providing only visual\ninformation to offering both visual and semantic information),\nretrieval accuracy can be further improved. Moreover, in the\nreal-world CIR task, there are various scenarios. In cases\nwith complex modifications, many visual properties of the\nreference image need to be altered, making the modification\ntext crucial. In contrast, in cases with simple modifications,\nonly minor aspects of the reference image need to be changed,\nthus making the reference image play a more dominant role.\nTo address these diverse situations, different modalities should\nbe assigned different weights based on actual needs during the\nretrieval process.\nIn this component, we use the input reference image and\nmodification text as the query. The query is passed into\nthe visual information extraction component to obtain the\nreference image features qv and fine-grained features qf. And\nit is also passed into the semantic information extraction\ncomponent to obtain the average modified captions features\nqm. We then assign weights to these features and compute a\nweighted sum, which results in a feature that participates in\nthe retrieval process. Formally, we have:\nq = \u03b1 \u00b7 qm + \u03b2 \u00b7 qf + (1 \u2212\u03b1 \u2212\u03b2) \u00b7 qv,\n(8)\nwhere q represents the final query features involved in re-\ntrieval, and \u03b1 and \u03b2 are the weights of the corresponding\nfeatures.\nSimilarly, for each image Ik in the Database (where k =\n1, . . . , K, and K is the number of images in the database),\nwe input it into the visual information extraction component\nto obtain the target image features tv, and into the semantic\ninformation extraction component to obtain the average cap-\ntions features tc. We then assign weights to these features\nand perform a weighted sum, ultimately obtaining the feature\nrepresentation of each image in the database for retrieval.\nTherefore, we have:\nt = \u03b3 \u00b7 tc + (1 \u2212\u03b3) \u00b7 tv,\n(9)\nwhere t represents the final feature representation of each\nimage in the database, and \u03b3 is the weight of the corresponding\nfeature. Finally, for each image Ik in the database, a feature\nrepresentation tk is obtained. During retrieval, we compute the\ncosine similarity between q and each tk in the CLIP feature\nspace, and select the image with the highest similarity as the\ntarget image:\nIt = arg max\nIk\nq \u00b7 tk\n\u2225q\u2225\u2225tk\u2225,\n(10)\nwhere It is the target image.\nIV. EXPERIMENT\nA. Experimental Settings\n1) Evaluation Datasets: To evaluate the performance of our\nCVSI in various downstream CIR tasks, following [17], we\nconduct our experiments on three public datasets:\n\u2022 CIRR [31] consists of 36,554 queries, each paired with a\ntarget image. Our model was evaluated on the test set of\nCIRR, which contains 4.1K triplets.\n\u2022 CIRCO [11] is an open-domain dataset derived from the\nCOCO dataset [32]. Unlike CIRR, each sample in CIRCO\nconsists of a reference image, a modification text, and\nmultiple target images. And each query has an average of\n4.53 ground truths. We evaluated our model on the test set,\nwhich comprises 800 samples.\n\u2022 FashionIQ [33] is a fashion-focused dataset, which includes\n77,683 images, organized into three subcategories: Dress,\nShirt, and Toptee. We assessed our model using the vali-\ndation set, which contains a total of 6K triplets across the\nthree categories.\n2) Implementation Details: For the mapping network, we\nadopt the approach used in LinCIR [14]. For captioning, we\nfollow [17] and utilize the pre-trained BLIP-2 [19] with the\nOPT-6.7b large language model. The number of captions N\nis set to 15, as in [17]. For the LLM, we use GPT-3.5-Turbo\n[34] following prior work [16], [17], and additionally perform\nscalability experiments with LLama2-70B [35] and GPT-4o.\nFor the VLM, we ensure fair comparison with prior methods\nby using CLIP models from OpenAI (ViT-B/32, ViT-L/14)\nand OpenCLIP (ViT-G/14) [36]. Regarding hyperparameters,\nfor datasets like CIRCO and CIRR, which involve complex\nmodification texts, we set \u03b1 = 0.6, \u03b2 = 0.4, and \u03b3 = 0.2. For\nFashionIQ, which relies more on visual information, we set\n\u03b1 = 0.2, \u03b2 = 0.6, and \u03b3 = 0.1. The model was implemented\nin PyTorch using one NVIDIA V100 GPU.\n3) Evaluation Metrics: We followed the standard eval-\nuation protocols for each dataset. For CIRCO, we chose\nmean Average Precision (mAP) as a more precise metric,\nspecifically mAP@k, where k \u2208{5, 10, 25, 50} and k denotes\nthe number of top-ranked retrieval results being evaluated. For\nCIRR, we employed Recall@k (where k \u2208{1, 5, 10}) and\nRecallsubset@k (where k \u2208{1, 2, 3}) as evaluation metrics.\nNotably, Recallsubset@k limits the candidate target images\nto those semantically similar to the correct target image,\nTABLE I\nPERFORMANCE COMPARISON ON CIRCO AND CIRR TEST SETS. THE BEST-PERFORMING RESULTS ARE SHOWN IN BOLD, AND THE SECOND-BEST\nRESULTS ARE HIGHLIGHTED WITH AN UNDERLINE.\nBackbone\nMethod\nCIRCO\nCIRR\nmAP@k\nRecall@k\nRs@k\nk=5\nk=10\nk=25\nk=50\nk=1\nk=5\nk=10\nk=1\nk=2\nk=3\nViT-B/32\nPALAVRA(ECCV\u201922)\n4.61\n5.32\n6.33\n6.80\n16.62\n43.49\n58.51\n41.61\n65.30\n80.94\nSEARLE(ICCV\u201923)\n9.35\n9.94\n11.13\n11.84\n24.00\n53.42\n66.82\n54.89\n76.60\n88.19\nSEARLE-OTI(ICCV\u201923)\n7.14\n7.83\n8.99\n9.60\n24.27\n53.25\n66.10\n54.10\n75.81\n87.33\nCIReVL(ICLR\u201924)\n14.94\n15.42\n17.00\n17.82\n23.94\n52.51\n66.00\n60.17\n80.05\n90.19\nLDRE(SIGIR\u201924)\n17.96\n18.32\n20.21\n21.11\n25.69\n55.13\n69.04\n60.53\n80.65\n90.70\nCVSI(Ours)\n21.67\n22.47\n24.46\n25.44\n31.64\n62.43\n74.19\n64.17\n82.48\n91.69\nViT-L/14\nImage-only\n1.28\n1.70\n2.35\n2.69\n3.64\n12.75\n23.32\n11.58\n31.41\n45.26\nText-only\n2.63\n2.85\n3.30\n3.58\n20.51\n43.21\n55.08\n60.39\n80.02\n90.05\nCaptioning\n1.65\n1.96\n2.42\n2.71\n4.05\n15.88\n25.69\n20.87\n40.60\n60.89\nPic2Word(CVPR\u201923)\n8.72\n9.51\n10.64\n11.29\n23.90\n51.70\n65.30\n53.76\n74.46\n87.08\nSEARLE(ICCV\u201923)\n11.68\n12.73\n14.33\n15.12\n24.24\n52.48\n66.29\n53.76\n75.01\n88.19\nSEARLE-OTI(ICCV\u201923)\n10.18\n11.03\n12.72\n13.67\n24.87\n52.31\n66.29\n53.80\n74.31\n86.94\nLinCIR(CVPR\u201924)\n12.59\n13.58\n15.00\n15.85\n25.04\n53.25\n66.68\n57.11\n77.37\n88.89\nCIReVL(ICLR\u201924)\n18.57\n19.01\n20.89\n21.80\n24.55\n52.31\n64.92\n59.54\n79.88\n89.69\nLDRE(SIGIR\u201924)\n23.35\n24.03\n26.44\n27.50\n26.53\n55.57\n67.54\n60.43\n80.31\n89.90\nCVSI(Ours)\n26.89\n28.24\n30.79\n31.89\n33.81\n63.64\n75.04\n65.81\n82.92\n92.17\nViT-G/14\nPic2Word(CVPR\u201923)\n5.54\n5.59\n6.68\n7.12\n30.41\n58.12\n69.23\n68.92\n85.45\n93.04\nSEARLE(ICCV\u201923)\n13.20\n13.85\n15.32\n16.04\n34.80\n64.07\n75.11\n68.72\n84.70\n93.23\nLinCIR(CVPR\u201924)\n19.71\n21.01\n23.13\n24.18\n35.25\n64.72\n76.05\n63.35\n82.22\n91.98\nCIReVL(ICLR\u201924)\n26.77\n27.59\n29.96\n31.03\n34.65\n64.29\n75.06\n67.95\n84.87\n93.21\nLDRE(SIGIR\u201924)\n31.12\n32.24\n34.95\n36.03\n36.15\n66.39\n77.25\n68.82\n85.66\n93.76\nCVSI(Ours)\n33.51\n34.96\n37.86\n38.98\n41.37\n72.58\n82.39\n71.35\n87.54\n94.68\nhelping to mitigate the issue of false negatives. For FashionIQ,\nwe used Recall at rank K (Recall@K) as the evaluation\nmetric, specifically adopting R@10 and R@50. Additionally,\nwe computed the average performance across the three subsets\nof different categories to assess overall performance.\n4) Baselines: To thoroughly evaluate the effectiveness of\nour method, we have adopted the following baselines and\nbriefly summarized their key concepts. All of these methods\nultimately perform retrieval in the CLIP feature space.\n\u2022 Image-only. It directly uses the visual features of the\nreference image to retrieve the most similar target image.\n\u2022 Text-only. It directly uses the textual features of the modi-\nfication text to retrieve the most similar target image.\n\u2022 Captioning. The caption generated by BLIP-2 for the ref-\nerence image is concatenated with the modification text to\nform a single query for retrieval.\n\u2022 PALAVRA [37]. It involves performing textual inversion\nthrough a mapping function, followed by optimizing the\npseudo token.\n\u2022 Pic2Word [10]. It employs a textual inversion network to\nmap the reference image into a pseudo token, which is then\ncombined with the modification text to form a single query\nfor retrieval.\n\u2022 SEARLE / SEARLE-OTI [11]. The SEARLE-OTI frame-\nwork employs an optimized textual inversion approach to\nlearn a pseudo token. SEARLE improves upon this by learn-\ning a more efficient mapping network, distilling knowledge\nfrom SEARLE-OTI.\n\u2022 LinCIR [14]. It trains a textual inversion network solely\nthrough text to map the reference image into a pseudo token,\nwhich is then combined with the modification text to form\na single query for retrieval.\n\u2022 CIReVL [16]. It uses a captioning model to generate a\ncaption for each reference image, then employs an LLM\nto generate a modified caption for retrieval.\n\u2022 LDRE [17]. It uses a captioning model to generate multiple\ncaptions for each reference image, then employs an LLM\nto generate multiple modified captions, which are weighted\nand summed for retrieval.\nB. Results and Analysis\nTables I and II summarize the performance comparison\non the three datasets. From these tables, we obtained the\nfollowing observations: 1) Our model achieves state-of-the-\nart performance across all metrics on the CIRCO and CIRR\ndatasets and on most metrics of the FashionIQ dataset, which\ndemonstrates the effectiveness of our model in the ZS-CIR\ntask. 2) On CIRCO and CIRR, LLM-based methods clearly\noutperform textual inversion methods due to the more complex\nmodification texts in these datasets, where the retrieval results\nrely more on semantic information. However, on FashionIQ,\nwith the ViT-G/14 backbone, the textual inversion method\nrepresented by LinCIR performs significantly better than\nLLM-based methods. This is because the modification text\nin this dataset is simpler, allowing visual information to\ndominate the final retrieval, which limits the performance of\nLLM-based methods. The results align with our expectations\nfor the real-world ZS-CIR task and our method\u2019s weight\nallocation strategies. 3) Our method extracts both semantic\nand visual information from query and target images, and\nTABLE II\nPERFORMANCE COMPARISON ON FASHIONIQ VALIDATION SET. THE BEST-PERFORMING RESULTS ARE SHOWN IN BOLD, AND THE SECOND-BEST\nRESULTS ARE HIGHLIGHTED WITH AN UNDERLINE.\nBackbone\nMethod\nShirt\nDress\nTopee\nAverage\nR@10\nR@50\nR@10\nR@50\nR@10\nR@50\nR@10\nR@50\nViT-L/14\nPic2Word(CVPR\u201923)\n26.20\n43.60\n20.00\n40.20\n27.90\n47.40\n24.70\n43.70\nSEARLE(ICCV\u201923)\n26.89\n45.58\n20.48\n43.13\n29.32\n49.97\n25.56\n46.23\nSEARLE-OTI(ICCV\u201923)\n30.37\n47.49\n21.57\n44.47\n30.90\n51.76\n27.61\n47.90\nLinCIR(CVPR\u201924)\n29.10\n46.81\n20.92\n42.44\n28.81\n50.18\n26.28\n46.49\nCIReVL(ICLR\u201924)\n29.49\n47.40\n24.79\n44.76\n31.36\n53.65\n28.55\n48.57\nLDRE(SIGIR\u201924)\n31.04\n51.22\n22.93\n46.76\n31.57\n53.64\n28.51\n50.54\nCVSI(Ours)\n33.56\n50.15\n25.63\n48.59\n34.01\n54.05\n31.07\n50.93\nViT-G/14\nPic2Word(CVPR\u201923)\n33.17\n50.39\n25.43\n47.65\n35.24\n57.62\n31.28\n51.89\nSEARLE(ICCV\u201923)\n36.46\n55.35\n28.16\n50.32\n39.83\n61.45\n34.81\n55.71\nLinCIR(CVPR\u201924)\n46.76\n65.11\n38.08\n60.88\n50.48\n71.09\n45.11\n65.69\nCIReVL(ICLR\u201924)\n33.71\n51.42\n27.07\n49.53\n35.80\n56.14\n32.19\n52.36\nLDRE(SIGIR\u201924)\n35.94\n58.58\n26.11\n51.12\n35.42\n56.67\n32.49\n55.46\nCVSI(Ours)\n44.95\n63.59\n40.75\n63.71\n49.21\n69.86\n44.97\n65.72\nthen combines these features for complementary information\nretrieval, which enables a balanced performance across both\ntypes of datasets (visual information-dominant and semantic\ninformation-dominant). Clearly, our method shows a signif-\nicant improvement over the previous state-of-the-art method,\nLDRE, on the CIRCO and CIRR datasets. While the improve-\nment on FashionIQ is limited. This is because our method\nextracts semantic information for both the query and the\ntarget, allowing for effective filtering of irrelevant domain\npairs during retrieval. However, FashionIQ is a dataset focused\non the fashion domain, which limits the effectiveness of our\nmodel on this dataset. As a result, CVSI would perform better\non multi-domain datasets, aligning with the characteristics of\nthe real-world ZS-CIR task.\nC. Ablation Studies\nTo evaluate the contribution of each component of our\nmodel and the effectiveness of the features involved in comple-\nmentary information retrieval, we conducted ablation experi-\nments. We compared CVSI with the following derivatives: 1)\nw/o visual information extraction: The query and target do not\nundergo visual information extraction, and only semantic in-\nformation is involved in complementary information retrieval.\n2) w/o semantic information extraction: The query and target\ndo not undergo semantic information extraction, and only\nvisual information is involved in complementary information\nretrieval. 3) w/o reference image features: The reference image\nfeatures are excluded from the complementary information\nretrieval process. 4) w/o fine-grained features: Fine-grained\nfeatures are excluded from the complementary information re-\ntrieval process."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThe paper introduces a novel method called Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration (CVSI). The main contributions include:\n1. **Comprehensive Information Extraction**: CVSI leverages three key components to fully capture both visual and semantic information: \n   - **Visual Information Extraction**: Uses a pre-trained mapping network to convert images into pseudo tokens, which are then combined with modification texts.\n   - **Semantic Information Extraction**: Utilizes a pre-trained captioning model to generate multiple captions for the reference image, followed by an LLM to produce modified captions and objects most likely added.\n2. **Fine-Grained Retrieval**: CVSI incorporates these extracted features into retrieval queries, enabling efficient fine-grained retrieval of specific object changes in images.\n3. **Weighted Fusion**: The method assigns weights based on the nature of the query, ensuring that both visual and semantic information are appropriately balanced for different contexts.\n\n#### Methodology\nThe proposed CVSI framework is designed to address limitations in existing zero-shot composed image retrieval (ZS-CIR) methods:\n1. **Visual Information Extraction**: CVSI uses a pre-trained mapping network to convert images into pseudo tokens, which capture global visual features and integrate with modification texts.\n2. **Semantic Information Extraction**: The reference image captions are used by a captioning model to generate multiple captions, followed by an LLM to produce modified captions and objects most likely added.\n3. **Complementary Information Retrieval**: Features from both the query and database images are combined for retrieval, enabling efficient handling of diverse scenarios.\n\n#### Key Findings\nExtensive experiments on three public datasets (CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods:\n- **CVSI Achieves New State-of-the-Art Results**: On most metrics, CVSI achieves new best results across all datasets.\n- **Improved Performance Across Datasets**:\n  - **CIRR**: Achieves the highest mAP@k scores on CIRR test set.\n  - **CIRCO**: Demonstrates superior performance over existing methods in terms of Recall@k and Recallsubset@k metrics.\n  - **FashionIQ**: Shows significant improvement compared to LLM-based approaches, especially with simpler modification texts.\n\n#### Conclusions\nThe proposed CVSI method effectively integrates visual and semantic information for zero-shot composed image retrieval. It significantly outperforms existing state-of-the-art methods across various datasets, demonstrating its effectiveness in real-world applications such as e-commerce. The method's ability to handle diverse scenarios through weighted feature fusion further enhances its robustness and applicability."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Highlights\nProgressive self-supervised blind-spot denoising method for LDCT\ndenoising\nYichao Liu, Yueyang Teng, Junwen Guo\n\u2022 a progressive self-supervised blind-spot denoising framework is pro-\nposed for LDCT that requires only low-dose CT images and no paired\nnormal-dose data.\n\u2022 The step-wise masking and denoising strategy enables fine-grained noise\nsuppression and effectively bridges the performance gap between self-\nsupervised and supervised methods.\n\u2022 A noise-based regularization mechanism, implemented by injecting con-\ntrolled noise into both inputs and targets, effectively mitigates overfit-\nting and strengthens the self-supervised learning signal.\n\u2022 Extensive experiments demonstrate architecture-agnostic performance\nimprovements, with results comparable to or surpassing representative\nsupervised LDCT denoising methods.\narXiv:2601.14180v1  [cs.CV]  20 Jan 2026\nProgressive self-supervised blind-spot denoising\nmethod for LDCT denoising\nYichao Liua, Yueyang Tengb,c,\u2217, Junwen Guod,\u2217\naIWR, Heidelberg University, Heidelberg, 69120, Baden W\u00fcrttemberg, Germany\nbCollege of Medicine and Biological Information Engineering, Northeastern\nUniversity, Shenyang, 110169, Liaoning, China\ncKey Laboratory of Intelligent Computing in Medical Image, Ministry of\nEducation, Shenyang, 110169, Liaoning, China\ndDepartment of Epidemiology & Global Health, Ume\u00e5 University, , Ume\u00e5, 90187, Sweden\nAbstract\nSelf-supervised learning is increasingly investigated for low-dose computed to-\nmography (LDCT) image denoising, as it alleviates the dependence on paired\nnormal-dose CT (NDCT) data, which are often difficult to acquire in clinical\npractice. In this paper, we propose a novel self-supervised training strategy\nthat relies exclusively on LDCT images. We introduce a step-wise blind-spot\ndenoising mechanism that enforces conditional independence in a progressive\nmanner, enabling more fine-grained denoising learning. In addition, we add\nGaussian noise to LDCT images, which acts as a regularization and mitigates\noverfitting. Extensive experiments on the Mayo LDCT dataset demonstrate\nthat the proposed method consistently outperforms existing self-supervised\napproaches and achieves performance comparable to, or better than, several\nrepresentative supervised denoising methods.\nKeywords:\nSelf-supervised learning, LDCT denoising, Progressive learning,\nBlind spot denoising\n\u2217Corresponding author\nEmail addresses: tengyy@bime.neu.edu.cn (Yueyang Teng), junwen.guo@umu.se\n(Junwen Guo)\n1. Introduction\nLow-dose computed tomography (LDCT) plays a critical role in reducing\nradiation exposure to patients while maintaining diagnostic utility, in ac-\ncordance with the \u201cas low as reasonably achievable\u201d (ALARA) principle [1].\nHowever, lowering the radiation dose inevitably increases noise and artifacts\nin reconstructed CT images, which can severely degrade image quality and\nhinder accurate clinical interpretation. To address this issue, image denois-\ning has become an essential post-processing step in LDCT imaging pipelines,\naiming to suppress noise while preserving diagnostically relevant anatomical\nstructures.\nExtensive efforts have been devoted to LDCT image denoising in recent\nyears. Supervised learning\u2013based approaches remain the dominant paradigm\nand have achieved strong performance. Representative methods include con-\nvolutional neural network (CNN)\u2013based models such as U-Net [2], Residual\nEncoder-Decoder CNN (RED-CNN) [3], and cycle-consistent generative ad-\nversarial networks (CycleGANs) [4, 5] , Transformer-based architectures such\nas CTformer [6], as well as hybrid models combining convolution and atten-\ntion mechanisms, for example, Swin-UNet [7].\nHowever, traditional LDCT denoising methods depend heavily on paired\nnormal-dose CT (NDCT) images, which are often scarce in clinical practice.\nWith the advancement of deep learning, unsupervised approaches, particu-\nlarly self-supervised learning, have gained increasing attention due to their\nability to learn without clean references.\nClassical model-based methods\nlike Block-Matching 3D (BM3D) [8] suppress noise by exploiting non-local\nself-similarity through grouping similar patches and performing collaborative\nfiltering in a transform domain. More recently, self-supervised learning tech-\nniques, collectively known as Noise2- methods, have substantially advanced\ndenoising performance by leveraging different assumptions about noise and\ndata availability. Noise2Noise [9] trains a network to map one noisy observa-\ntion to another under the assumption that noise realizations are independent\nand zero-mean, thus requiring multiple noisy instances of the same image\nbut no clean targets. Noise2Self [10] relaxes this requirement by formalizing\na conditional independence assumption, training networks to predict each\npixel exclusively from its spatial neighbors. This allows effective denoising\nusing only single noisy images, without clean targets or paired noisy data.\nBuilding on this framework, Noise2Void [11] further introduces a masking\nstrategy in which selected pixels are predicted from their surrounding con-\n2\ntext, relying on the assumptions that noise is independent across pixels and\nthat the underlying signal can be inferred from neighboring pixels. In con-\ntrast, Neighbor2Neighbor [5] avoids explicit masking and instead assumes\nthat neighboring patches share the same underlying clean signal but con-\ntain different noise realizations, enabling one noisy patch to supervise the\ndenoising of the other.\nBuilding upon the core assumptions underlying Noise2Noise, Noise2Void,\nand Noise2Self, a series of extensions have been proposed to relax data re-\nquirements or strengthen training constraints in self-supervised denoising.\nNoise2Inverse [12] extends the Noise2Noise paradigm to tomographic imag-\ning by splitting a single raw sinogram into independent subsets of projection\nangles, producing multiple noisy reconstructions of the same anatomy. Under\nthe assumption of independent measurement noise and identical underlying\nstructure, the network learns to suppress noise by predicting one reconstruc-\ntion from another. Noise2Same [13] addresses the information loss introduced\nby blind-spot masking through a dual-forward pass strategy, enforcing con-\nsistency between masked and unmasked predictions and thereby improving\ndetail preservation over earlier methods such as Noise2Void. From a prob-\nabilistic perspective, Noise2Score [14] formulates self-supervised denoising\nas a Bayesian score estimation problem, learning the gradient of the log-\nprobability density directly from noisy data without clean references; the\nclean image is then analytically recovered via Tweedie\u2019s formula, enabling a\nunified treatment of noise from the exponential family, including Gaussian\nand Poisson distributions. In a complementary direction, Noise2Sim [15] ex-\nploits structural similarity across intrinsically registered sub-images, such as\nadjacent CT slices, and uses them as training pairs to suppress both indepen-\ndent and correlated noise. Under the assumption that noise and structural\ndiscrepancies between similar sub-images are zero-mean, Noise2Sim is the-\noretically shown to be asymptotically equivalent to supervised Noise2Clean\ntraining.\nIn this paper, inspired by the observation of Zhu et al. [16] that the trans-\nformation from low-quality LDCT images to high-quality NDCT images can\nbe progressively approximated by neural networks, we propose a progres-\nsive self-supervised learning framework for LDCT image denoising. Building\non the blind-spot denoising principle, the proposed approach extends it in\na step-wise manner, allowing the network to progressively improve image\nquality by denoising inputs in a more fine-grained way. To further mitigate\noverfitting, we add random noise of identical intensity to both input and tar-\n3\nget images. Extensive experiments demonstrate that the proposed method\nconsistently outperforms existing self-supervised denoising approaches and\nachieves performance competitive with, or superior to, some representative\nsupervised methods.\n2. Methodology\nThis section introduces the proposed progressive self-supervised denoising\nmethod, which extends the blind-spot method by incorporating a step-wise\nmasking scheme.\n2.1. Supervised noise2clean\nWe first consider LDCT denoising in a supervised learning setting. Given\na LDCT image xi and the corresponding NDCT image yi, assuming x =\nxc + n0, the denoising process can be expressed as:\nargmin\n\u03b8\nX\ni\nL(f\u03b8(xi), yi)\n(1)\nwhere, xc is the clean CT images and n0 is the noise on LDCT images, f\u03b8\nrepresents denoising networks with optimal parameters \u03b8, and L denotes the\nloss function.\nBy assuming that the noise is zero-mean and independent\nacross all dimensions, self-supervised learning methods such as Noise2Self\ncan be developed.\n2.2. Noise2Self\nNoise2Self is trained using the loss function L(f) = Ex||f(x) \u2212x||2. Im-\nportantly, the function f is required to be J -invariant, as established by\nBatson et al.\n[10] to prevent learning the identity mapping.\nThanks to\nJ -invariant and the zero-mean assumption of noise, minimizing the self-\nsupervised loss Ex||f(x)\u2212x||2 indirectly minimizes the supervised loss Ex||f(x)\u2212\ny||2 [13], which can be expressed as:\nEx||f(x) \u2212x||2 \u221dEx||f(x) \u2212y||2\n(2)\nCurrently, all existing implementations for computing the J -invariant de-\nnoising function f rely on blind-spot networks [11, 10, 17].\n4\n2.3. Progressive self-supervised denoising\nConsider a temporal supervised denoising process where \u02c6xt denotes the\ndenoised LDCT image at time t and noise(t) represents the noise at time\nt. The output denoised image \u02c6xt serves as the input for the next time step,\nsuch that f(\u02c6xt) = \u02c6xt+1. The denoising process is illustrated as follows:\n\u02c6xt = f(\u02c6xt) + noise(t), \u02c6xt+1 = f(\u02c6xt+1) + noise(t + 1)\n, ..., \u02c6xt+n = f(\u02c6xt+n) + noise(t + n)\n(3)\nAs the time step t increases, \u02c6xt should progressively approach the NDCT\nimage y.\nlim\nt\u2192\u221e\u02c6xt = lim\nt\u2192\u221ef(\u02c6xt) = y, lim\nt\u2192\u221enoise(t) = 0\n(4)\nSince minimizing the self-supervised loss Ex||f(x) \u2212x||2 indirectly min-\nimizes the supervised loss Ex||f(x) \u2212y||2, the limt\u2192\u221enoise(t) = 0 should\nalso hold for Ex||f(x) \u2212x||2 as well. Therefore, a step-wise blind-spot self-\nsupervised loss function can be formulated as follows:\nargmin\n\u03b8\nX\ni\nL(f\u03b8(M \u2299\u02c6xt\ni), xi)\n(5)\nWhere M \u223cM\u03b1{M1, ..., Mn} is a binary random mask. M\u03b1 denotes a set of\nmasks with random sampling rate \u03b1. \u2299represents the Hadamard (element-\nwise) product. Here, Similar to supervised temporal denoising, the update\nis given by \u02c6xt+1\ni\n= f\u03b8(M \u2299\u02c6xt\ni)\nTo prevent overfitting and encourage the model to learn contextual fea-\ntures rather than noise, we add random zero-mean noise n1 to the input xi\nand n2 to the target xi. Note that this does not violate Eq. (2). As detailed\nin the following,\nEx||f(x + n1) \u2212(x + n2)||2 = Ex,y||f(x + n1) \u2212(y + n2)||2\n(6)\n+ Ex,y||x \u2212y||2 \u22122\u27e8f(x + n1) \u2212(y + n2), x \u2212y\u27e9\n(7)\n= Ex,y||f(x + n1) \u2212(y + n2)||2 + Ex,y||x \u2212y||2\n(8)\nHere, since f is J -invariant and the zero-mean noise assumption holds, min-\nimizing Eq. (8) indirectly minimizes Ex||f(x) \u2212y||2.\n5\nTherefore, the final loss function is defined as follows:\nargmin\n\u03b8\nX\ni\nL(f\u03b8(M \u2299(\u02c6xt\ni + 1{\u02c6xt\ni=x}n1), xi + n2)\n(9)\nWhere 1{\u02c6xt\ni=x} is the indicator function that equals one when the input\nis an LDCT image. In this work, we generate n1 and n2 as a combination of\nPoisson noise and additive white Gaussian noise (AWGN) [16].\nThe advantage of our method is that it requires only LDCT images for\ntraining. At each denoising time step, a random mask is applied and the\nmodel is optimized by minimizing Eq. (5). Intuitively, this random masking\nstrategy progressively suppresses noise-corrupted pixels, enabling the model\nto recover the underlying anatomical structures and contextual information\nof the CT image over k denoising steps.\nDuring inference, we sample k\ndifferently masked versions of the LDCT image, and the final denoised CT\nimage is obtained by averaging the corresponding outputs. An overview of\nthe proposed progressive self-supervised denoising framework is shown in Fig.\n1.\n3. Experiments\nWe first introduce the configuration of dataset and experiments.\nWe\ndiscuss the progressive blind-spot strategy and noise configurations through\nablation experiments. We also experimentally verify the generalizability of\nthe model for different patients. Finally, we compare our method to several\nsupervised and self-supervised methods.\n3.1. Experimental details\nWe conduct our experiments on the AAPM Low-Dose CT Grand Chal-\nlenge dataset, a publicly available benchmark that provides paired low-dose\nand standard-dose CT scans acquired under identical imaging conditions [18].\nThe dataset includes scans from multiple anatomical regions. Each volume\nis reconstructed with an in-plane resolution of 512 \u00d7 512 and a voxel spacing\nof 0.5859 \u00d7 0.5859 \u00d7 3.0mm3. A total of 36 subjects are included, with 27\nvolumes used for training and the remaining 9 reserved for testing to ensure\nsubject-level separation. All images are provided in DICOM format and are\nfirst converted to Hounsfield Units (HU) to preserve the physical meaning\nof CT attenuation values. The intensities are then normalized to the range\n[0, 1] using a fixed HU window of [\u22121024, 3072], which covers air, soft tissue,\n6\nand high-density structures while maintaining consistency across subjects\nand experiments. It is worth noting our method does not require any NDCT\nat the training stage.\nWe implemented our model using the PyTorch 2.7 library and trained it\non a server equipped with a single NVIDIA V100 GPU. The Adam optimizer\nwas employed with parameters \u03b21 = 0.9, \u03b22 = 0.99, and \u03f5 = 10\u22128. The initial\nlearning rate was set to 10\u22123 and decayed by a factor of two every 20 epochs.\nTraining was conducted for a total of 100 epochs. We used a batch size of 1,\nwith 10 patches per image and a patch size of 128 \u00d7 128. The combination\nnoise with zero mean and variance 10 of AWGN was added to both the\nnetwork inputs and targets.\nFor quantitative assessment, model performance is evaluated using three\nwidely adopted image quality metrics: peak signal-to-noise ratio (PSNR),\nstructural similarity index (SSIM), and root mean squared error (RMSE).\nThese metrics are computed in accordance with the evaluation protocol de-\nscribed in Chen et al. [3], enabling consistent and reproducible comparison\nacross methods. PSNR and RMSE quantify pixel-wise fidelity between the\ndenoised and reference images, while SSIM measures structural and percep-\ntual similarity.\nThe mathematical formulation shows as Eq.\n10 and Eq.\n11. We use the standard deviation as a measure of variability to assess the\nstability of the models.\nPSNR = 10log10(\nMAX2\n1\nn\nPn\ni=1(mi \u2212li)2)\n(10)\nwhere mi and li are pixels of LDCT or denoised CT images and NDCT\nimages, respectively.\nn is the number of pixels in the image and MAX\ndenotes the maximum value of the image pixels.\nSSIM(m, l) =\n(2\u00b5m\u00b5l + c1)(\u03c3ml + c2)\n(\u00b52\nm + \u00b52\nl + c1)(\u03c32\nm + \u03c32\nl + c2)\n(11)\nwhere \u00b5m and \u00b5l are the means of m and l, respectively; \u03c32\nm and \u03c3l are\nvariances of m and l, respectively. \u03c3ml is the covariance of m and l, and c1\nand c2 are two hyperparameters for stabilizing the division operation.\n3.2. Results\n3.2.1. Number of time steps\nThe number of time steps k was selected empirically through a number\nof experiments. To ensure consistency between training and inference, the\n7\nT\nT+1\nT+n\nL1 loss\nLDCT+n1\nLDCT+n2\nFigure 1: Overview of progressive self-supervised denoising method\nnumber of mask sampling steps during inference was also set to k. We evalu-\nated k in the range of 1 to 6. As shown in Fig. 2, the denoising performance\nconsistently improves as k increases and reaches its optimum at k = 5. Fur-\nther increasing k leads to a degradation in performance in both PSNR and\nSSIM.\nThis indicates that from 1 step to 5 steps, the model learns to gradu-\nally denoise. Such behavior is expected, as accurate recovery of corrupted\nregions requires sufficient contextual information from other parts of the\nimage, which cannot be fully propagated in a single denoising step. By it-\neratively applying the denoising process, the model progressively refines its\npredictions and aggregates increasingly global contextual cues, leading to im-\nproved reconstruction quality. This observation is consistent with the trend\nshown in Fig. 2, where performance improves with increasing k until suffi-\ncient contextual information has been accumulated.\n3.2.2. Noise level\nTo investigate the impact of noise level on model performance, we evalu-\nate different levels of the combination noise, parameterized by the standard\ndeviation of the Gaussian distribution. Fig. 3 illustrates the effect of varying\nnoise levels on performance over the test set. As the noise variance increases,\nboth PSNR and SSIM consistently decrease, indicating degraded reconstruc-\ntion quality. The highest PSNR is achieved at a noise level of 10. For SSIM,\nthe differences among noise levels 5, 10, and 15 are relatively minor, suggest-\ning limited sensitivity within this range.\n8\nFigure 2: Quantitative results on the test set: number of time steps\n3.2.3. Mask ratio\nWe perform point-wise Bernoulli sampling over the spatial domain, where\neach pixel is independently activated with a success probability \u03b1, corre-\nsponding to the masking ratio. Fig. 4 summarizes the impact of different\nmasking ratios on denoising performance.\nPSNR reaches its maximum at \u03b1 = 0.1, while SSIM seems remains rela-\ntively stable over a broad range of masking ratios from \u03b1 = 0.01 to \u03b1 = 0.2.\nWhen the masking ratio is too low (\u03b1 = 0.1), the supervision signal remains\nweak due to the high similarity between input and target, limiting perfor-\nmance improvements. Conversely, excessively high masking ratios (\u03b1 = 0.2)\nremoves substantial contextual information, resulting in degraded reconstruc-\ntion quality and reduced structural fidelity.\n3.2.4. ablation study\nWe further conducted ablation experiments to analyze the effects of com-\nbining Poisson and AWGN with the proposed progressive self-supervised\nblind-spot strategy. The results are summarized in Table 1.\nAs shown, applying the progressive self-supervised blind-spot strategy\nalone yields only marginal performance improvements compared to the LDCT\nbaseline reported in Table 2. This is primarily because the input and target\nimages remain largely identical, which limits the learning signal and leads\nto overfitting.\nIn contrast, introducing noise substantially enhances per-\n9"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper\n\n#### Main Contributions\nThe paper proposes a novel progressive self-supervised blind-spot denoising method for Low-Dose Computed Tomography (LDCT) images. This approach leverages only low-dose CT (LDCT) images and no paired normal-dose data, making it suitable for clinical settings where NDCT images are often scarce. The key contributions include:\n\n1. **Progressive Blind-Spot Denoising Framework**: A step-wise masking and denoising strategy that enables more fine-grained noise suppression, bridging the gap between self-supervised and supervised methods.\n2. **Noise-Based Regularization Mechanism**: By injecting controlled Gaussian noise into both inputs and targets, this mechanism effectively mitigates overfitting and strengthens the self-supervised learning signal.\n3. **Architecture-Agnostic Performance Improvements**: Extensive experiments demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to or surpassing several representative supervised denoising methods.\n\n#### Methodology\nThe methodology involves a progressive self-supervised training framework for LDCT image denoising, which extends the blind-spot denoising principle. Key steps include:\n\n1. **Supervised Noise2Clean**: A traditional supervised learning setting where LDCT images are denoised using paired NDCT data.\n2. **Noise2Self**: A self-supervised approach that trains a network to map noisy observations to clean ones under assumptions of zero-mean noise and conditional independence.\n3. **Progressive Self-Supervised Denoising**: The proposed method incorporates a step-wise masking scheme, allowing the model to progressively improve image quality by denoising inputs in finer-grained ways.\n\n#### Key Findings\n- **Performance Improvement**: Extensive experiments on the Mayo LDCT dataset show that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to or surpassing several representative supervised methods.\n- **Effectiveness of Progressive Strategy**: The number of denoising steps (k) impacts model performance, with optimal results achieved at k = 5. Increasing k beyond this point leads to a degradation in both PSNR and SSIM scores.\n- **Noise Level Impact**: Higher noise levels result in degraded reconstruction quality as measured by PSNR and SSIM metrics. The method shows robustness within certain noise ranges (e.g., 10% Gaussian noise).\n- **Mask Ratio Analysis**: Point-wise Bernoulli sampling over the spatial domain reveals that optimal performance is achieved with a masking ratio of around 10%, balancing between strong supervision signals and avoiding excessive removal of contextual information.\n\n#### Conclusions\nThe proposed progressive self-supervised blind-spot denoising method for LDCT images demonstrates significant improvements in image quality compared to existing approaches, all while requiring only low-dose CT data. The step-wise masking strategy effectively mitigates overfitting by injecting controlled noise into both inputs and targets, leading to robust performance across different datasets and clinical settings. This work paves the way for more efficient and effective LDCT denoising methods in resource-constrained environments.\n\n---\n\nThis summary encapsulates the main contributions, methodology, key findings, and conclusions of the paper in a comprehensive manner."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "OmniTransfer: All-in-one Framework for\nSpatio-temporal Video Transfer\nPengze Zhang\nYanze Wu\u2020\nMengtian Li\nXu Bai\nSongtao Zhao\u2021\nFulong Ye\nChong Mou\nXinghui Li\nZhuowei Chen\nQian He\nMingyuan Gao\nIntelligent Creation Lab, ByteDance\nAbstract\nVideos convey richer information than images or text, capturing both spatial and temporal dynamics.\nHowever, most existing video customization methods rely on reference images or task-specific\ntemporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos,\nthereby limiting flexibility and generalization in video generation. To address these limitations,\nwe propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages\nmulti-view information across frames to enhance appearance consistency and exploits temporal\ncues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer\nincorporates three key designs: Task-aware Positional Bias that adaptively leverages reference\nvideo information to improve temporal alignment or appearance consistency; Reference-decoupled\nCausal Learning separating reference and target branches to enable precise reference transfer\nwhile improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic\nguidance to dynamically distinguish and tackle different tasks. Extensive experiments show that\nOmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer\n(camera movement and video effects), while matching pose-guided methods in motion transfer\nwithout using pose, establishing a new paradigm for flexible, high-fidelity video generation.\nDate: January 20, 2026\nProject Page: https://pangzecheung.github.io/OmniTransfer/\n1\nIntroduction\nWe have all heard the old adage: \u201cA picture is worth a thousand words\u201d\u2014and if we follow that logic a little\nfurther, a video might be worth a million. After all, a static image freezes just one moment of light, texture,\nand form; video weaves those moments into something dynamic, carrying not just how things look, but how\nthey move and change. These are details that neither words nor a single image can fully convey.\nThe same insight applies to diffusion-based video generation. A reference video offers greater flexibility and\ncapability than using an image or text alone. Not only can the model reference identity (ID) and style in\nspatial appearance aspects, but it can also exploit temporal information, including camera movement, motion,\nand visual effects, enabling more coherent and expressive video synthesis.\n\u2020 Corresponding author, \u2021 Project lead. This work is purely academic and non-commercial. Demo reference images/videos\nare from public domains or AI-generated. For copyright concerns, please contact us for the removal of relevant content.\n1\narXiv:2601.14250v1  [cs.CV]  20 Jan 2026\nOurs\nSpatial Appearance Video Transfer\nMotion\nRefV\n(ID + Effect)\nImage\n(Motion)\nEffect\nOurs\nRefV\n(Effect)\nStyle\nRefV\n\u201creading book under the tree\u201d\nTemporal Video Transfer\nOurs\n(Style)\n(ID)\nRefV\nOurs\nID\nID\nStyle\nVideo Transfer Combination\nEffect\nID\nRefV 1\nRefV 2\nOurs\nRefV 1\nRefV 2\n(ID + Style)\n\u201cplaying guitar\u201d\nOurs\n\u201cPlaying the piano\u201d\nImage\n\uff08ID + Effect)\nFigure 1 OmniTransfer seamlessly unifies spatial appearance (ID and style) and temporal video transfer tasks (effect,\nmotion and camera movement) within a single framework, and exhibits strong generalization across unseen task\ncombinations.\nDespite this potential, existing methods have yet to fully exploit their spatio-temporal information. 1) For\nspatial information, most approaches [38, 67, 71, 72] still mainly rely on pixel-level cues extracted from\nreference images. However, cues such as ID and style are inherently spatio-temporal, extending across multiple\nframes and views. This makes it difficult for a single image to capture their full details, thereby limiting\ntheir effectiveness. 2) For temporal information, current approaches remain in its early stages: some depend\non pre-defined physical priors (e.g., pose or camera position) [9, 19, 75, 78], some employ inversion-based\nreconstruction [22, 36], while others require additional fine-tuning for specific temporal dynamics during\ntest time [47, 77].\nRecently, CamCloneMaster [40] made an initial reference-based attempt on camera\nmotion through temporal context concatenation, yet it struggles to generalize to in-the-wild conditions, and\nfails to address general temporal video reference tasks. These limitations highlight the need for a unified\nframework that directly leverages temporal information from reference videos, enabling flexible and robust\nvideo customization in real-world scenarios.\nTo tackle the aforementioned challenges, this paper introduces a novel exploration in the domain of video\nreference, proposing an all-in-one framework for spatio-temporal video transfer, OmniTransfer. This framework\nnot only integrates multi-frame reference information from the reference video at the spatial level, enhancing\nthe consistency of reference video ID and style, but also effectively models temporal aspects such as motion,\ncamera movement, and video effects, providing multi-dimensional control and unification over the video\ngeneration process. First, to unify video transfer tasks, we introduce Task-aware Positional Bias. For temporal\ntransfer tasks, based on our assumption that video diffusion models inherently maintain temporal consistency\nthrough spatial context, we add spatial offsets to positional embeddings to preserve temporal alignment.\nFor appearance transfer tasks, temporal offsets are applied to propagate appearance information across\nframes. Second, we introduce Reference-decoupled Causal Learning, which employs unidirectional transfer\nfrom reference to target, preventing simple copy-pasting. By separating the reference and target branches,\nthe reference branch requires only a single forward pass, reducing computational time by 20% compared to\nfull-attention models. Finally, to improve semantic guidance and avoid cross-task confusion, we introduce\na Multimodal Large Language Model (MLLM) via a Task-adaptive Multimodal Alignment module. This\nmodule leverages multiple sets of task-specific MetaQueries [46] to dynamically integrate and align semantic\ninformation across tasks, effectively meeting the requirements of each task.\n2\nIn sum, our contribution can be summarized as follows.\n\u2022 We propose OmniTransfer, a unified framework for the new task of spatio-temporal video transfer that\nsupports diverse tasks such as identity, style, effect, camera movement, and motion transfer, while\ngeneralizing seamlessly to their compositional combinations (Fig. 1).\n\u2022 We propose Task-aware Positional Bias, Reference-decoupled Causal Learning, and Task-adaptive\nMultimodal Alignment, each designed to unify various video customization tasks, enabling efficient and\nflexible spatio-temporal information transfer.\n\u2022 Experiments show that OmniTransfer outperforms existing methods in appearance (ID, style) and\ntemporal (camera movement, effects) transfer, while matching pose-guided methods in motion transfer\nwithout using pose. Moreover, benefiting from our model design, it achieves these improvements with a\n20% reduction in runtime compared to the basemodel architecture.\n2\nRelated Work\n2.1\nAppearance reference task\nThe two main tasks in appearance reference are ID and style transfer. For ID transfer in images, approaches\nhave evolved from adapter-based tuning [15, 29, 31, 57, 69, 70] to in-context learning [5, 16, 34, 45, 63]. In\nvideo ID transfer, most approaches still rely on single reference images for ID preservation [14, 18, 67, 72],\nwhile some works further explore broader subject-to-video generation tasks [7, 8, 23, 26, 35, 38, 44]. For\ninstance, Phantom [38] concatenates ID features along the temporal dimension to maintain appearance\nconsistency. Similarly, style transfer has been widely explored in image diffusion models [33, 54, 55, 64]. Early\nvideo stylization extends these models to the temporal domain via Image-to-Video paradigms [4, 12, 30],\nwhile recent Text-to-Video methods [6, 37, 59, 71] offer more flexible and controllable stylization. Despite\nthese advances, these methods rely solely on single-image references, overlooking the rich multi-frame and\nmulti-view cues inherent in videos. Our approach exploits these cues to achieve consistent appearance and\nstable temporal coherence.\n2.2\nTemporal reference task\nTemporal video customization covers motion generation, camera movement, and effect synthesis. Pose motion\ntransfer initially relies on GAN-based warping [49, 50, 73, 76], while diffusion-based models [3, 9, 21, 41, 52, 60,\n66, 75, 79, 80] improve temporal smoothness. However, they are limited by pose priors, as skeleton alignment\nmay cause loss of appearance cues and hinder their extension to multi-person scenarios. Some studies explore\nmore general motion control [28, 42, 47, 56, 68, 74, 77], but most rely on diffusion inversion or test-time\nfinetuning. Camera motion generation typically uses explicit parameterization [17, 19, 32, 58, 62, 65, 78]\nand parameter-free attention inversion [22, 36], while CamCloneMaster [40] explores temporal context\nconcatenation, yet resolution and generalization remain limited. Effect generation in industry often relies on\nLoRA-based [20] finetuning, whereas academic works [13, 39, 43] focus on precise spatial and temporal control\nof effects, but cannot generate new effects conditioned on a reference video. In contrast, our OmniTransfer\nachieves unified motion, camera, and effect transfer without explicit priors, demonstrating strong generalization\nto in-the-wild scenarios.\n3\nPreliminary\nOur framework is built upon Wan2.1 I2V 14B [53] as the underlying diffusion model. The input latent\nlt \u2208Rf\u00d7h\u00d7w\u00d7(2n+4) = [c, m, zt] concatenates three components along the channel dimension: latent noise\nzt \u2208Rf\u00d7h\u00d7w\u00d7n obtained by adding timestep t noise to VAE-compressed video features z; condition latent\nc \u2208Rf\u00d7h\u00d7w\u00d7n encoded from the condition image I concatenated with zero-filled frames; binary mask latent\nm \u2208Rf\u00d7h\u00d7w\u00d74 with values of 1 for preserved and 0 for generated frames. [\u00b7, \u00b7] denotes feature concatenation\nalong the channel dimension, and f, h, w, n represent frame number, height, width, and channel dimension,\nrespectively.\n3\nTask-specific\nMeta Queries\nLoRA\n...\nMLLM\n...\n...\n...\nTarget Video\nVAE Encoder\nNoise\n0\n0\n0\nFrame\n1\n0\n0\n0\nTask\nFlag\nTask\nFlag\nTask\nFlag\nTask\nFlag\nFrame\nFirst Frame\n(Optional)\nReference Video\nMLLM Vision Encoder\nWidth\nHeight\nFrame\nRoPE for\nAppearance Reference Task\nRoPE for\nTemproal Reference Task\nTarget\nTokens\nWidth\nFrame\nHeight\nReference\nTokens\nTarget\nTokens\nReference\nTokens\nTask-aware Positional Bias (TPB)\ntime = t\n+\nSelf Attention\nSelf Attention\nCross Attention\nCross Attention\n...\nConnector\nTarger Video\n...\ntime = t\ntime = 0\nDiT Block\nDiT Block\nDiT Block\nDiT Block\nDiT Block\nDiT Block\n...\n...\n...\nMSE Loss\nTarget Output\nTask Template\nInput\nPrompt\n(Optional)\nChannel-wise\nConcatation\nReference-Decoupled Causal Learning\nTask-adaptive Multimodal Alignment\nFirst Frame Tokens\n(Optional)\nReference Video\nTokens\nTask Template\nTokens\nReference Tokens\nTarget Tokens\nReference Latent Construction\n\u01d0ref\n\u01d0tgt\n\u05a9\nMLLM Text Encoder\n\u01c7ref\n\u01d1ref\n\u01deref\n0\n\u01c7tgt\n\u01d1tgt\n\u01detgt\n\u05a9\n\u01d0ref\n\u01d0tgt\n\u05a9\n\u01bbtgt\n\u01c0tgt\nre\n\u01c0\nf\n\u01b5tgt\n\u01b5ref\n\u01bbref \u01b5ref\nTPB\nRoPE\nQKV Project\nre\n\u01c0\nf\nQKV Project\nFigure 2 OmniTransfer comprises three key components: 1) Task-aware Positional Bias: exploits the model\u2019s inherent\nspatial and temporal context capabilities for diverse tasks. 2) Reference-decoupled Causal Learning: separates reference\nand target branches for causal and efficient transfer. 3) Task-adaptive Multimodal Alignment: leverages an MLLM to\nunify and enhance semantic understanding across tasks.\nEach Diffusion Transformer (DiT) block in Wan2.1 includes self-attention and cross-attention layers. The\nself-attention adopts 3D Rotary Positional Embedding (RoPE):\nAttn(R\u03b8(Q), R\u03b8(K), V ) = softmax\n\u0012R\u03b8(Q) R\u03b8(K)\u22a4\n\u221a\nd\n\u0013\nV,\n(1)\nwhere Q = WQlt, K = WKlt, V = WV lt. R\u03b8(\u00b7) denotes the RoPE rotation applied to queries and keys, WQ,\nWK, WV are learnable projections, and d is the feature dimension. Cross-attention integrates textual features\nas Attn(Q, Kp, Vp) with Kp and Vp derived from prompt p.\n4\nMethod\nIn this work, we study both appearance and temporal video reference, across several representative tasks\nsummarized in Table 1. To better unify these tasks, we propose OmniTransfer, as illustrated in Fig. 2. It\nconsists of four components- Reference Latent Construction, Task-aware Positional Bias, Reference-decoupled\nCausal Learning and Task-adaptive Multimodal Alignment\u2014described in detail below.\n4.1\nReference Latent Construction\nTo handle practical scenarios where reference and target videos may have different resolutions, we construct\nseparate latents for the two inputs (Fig. 2). For the target video, the latent is constructed in the same\nmanner as in Section 3, i.e., ltgt \u2208Rf\u00d7htgt\u00d7wtgt\u00d7(2n+4) = [ctgt, mtgt, zt\ntgt].\nFor the reference video, the\nlatent representation is defined as: lref \u2208Rf\u00d7href\u00d7wref\u00d7(2n+4) = [cref, mref, z0\nref]. The conditional latent cref\ncorresponds to the feature encoded by the VAE. The mask latent mref is assigned task-specific flags: \u22121\nfor temporal tasks, \u22122 for ID transfer, and \u22123 for style transfer. Notably, the reference latent z0\nref is kept\nnoise-free to maximally preserve its information.\n4\nTask\nInput\nOutput-Vtgt\nAppearance Transfer (T2V)\nID Transfer\nVref, p\nID from Vref, following prompt p\nStyle Transfer\nVref, p\nStyle from Vref, following prompt p\nTemporal Transfer (I2V)\nMotion Transfer\nVref, I\nMotion from Vref, starting from I\nCamera Movement\nVref, I\nCamera movement from Vref, starting from I\nEffect Transfer\nVref, I\nEffect from Vref, starting from I\nTable 1 Overview of video reference tasks. Vref: reference video; I: first-frame image; p: prompt; Vtgt: generated\nvideo.\n\u201cGenerate a two-column video. The left panel shows a boy performing a dance. The right panel should show a girl\nperforming the same dance, synchronized in time and rhythm.\u201d\nWan2.1 T2V\nSeedance 1.0\nSpatial Incontext\n\u01d8 = 0\n\u01d8 = \u01be//2\n\u01d8 = \u01d8\u2019\n\u01d8 = \u01be//2 + \u01d8'\nTemproal Incontext\n\u201cGenerate a two-shot video. [Scene 1] shows a boy performing a dance. [Scene 2] should show a girl performing the same\ndance, synchronized in time and rhythm.\u201d\nWan2.1 T2V\nSeedance 1.0\n\u01d8 = 0\n\u01d8 = \u01be//2\n\u01d8 = \u01d8\u2019\n\u01d8 = \u01be//2 + \u01d8'\nWan 2.1 fails to generate multi-shot video\nShot1\nShot1\nPose Consistent\nShot2\nShot2\nPose Inconsistent\nFigure 3 Video diffusion models are inherently capable of handling temporal consistency through spatial context.\n4.2\nTask-aware Positional Bias\nIC-Lora [24] proposes the assumption that \u201ctext-to-image models inherently possess in-context generation\ncapabilities\u201d. This hypothesis has been well validated in image customization methods [5, 45, 63], and is\nalso supported in appearance-consistent video customization [38, 67, 72], where reference appearances are\nleveraged through temporal in-context learning. However, it remains unclear whether current video diffusion\nmodels exhibit comparable in-context capabilities for temporal consistency tasks. To investigate this, we\ndirectly evaluate the video consistency of two representative models, Wan 2.1 [53] and Seedance [11], under\nthe T2V setting, as shown in Fig. 3. We observe that when generating side-by-side videos, both models easily\nmaintain motion consistency across columns. In contrast, when generating two temporally consecutive shots,\nthe model fails to keep actions consistent across two shots. This observation motivates us to propose a new\nassumption for video models: video diffusion models are inherently capable of handling temporal consistency\n5\nthrough spatial context.\nBased on the proposed assumption, we introduce Task-aware Positional Bias. Specifically, for temporal\nreference tasks, we add an offset to the RoPE of the reference video along the spatial (width) dimension,\naiming to leverage spatial in-context cues to enhance temporal consistency. The offset is set to the width\nof the target video, wtgt. In contrast, for appearance reference tasks, to exploit the temporal propagation\nof appearance information in the video model, we add an offset along the temporal dimension, equal to the\nnumber of frames in the target video, f. In summary, we define the RoPE rotation of the reference latents as\nR\u2217\n\u03b8:\nR\u2217\n\u03b8(\u00b7)=\n(\nR\u03b8(\u00b7, \u2206=(0, wtgt, 0)), for temporal ref.\nR\u03b8(\u00b7, \u2206=(f, 0, 0)),\nfor appearance. ref.,\n(2)\nwhere \u2206= (\u2206T , \u2206W , \u2206H) represents the offsets applied along the temporal, width, and height dimensions.\n4.3\nReference-decoupled Causal Learning\nA straightforward way to enable interaction between reference and target videos is through joint self-attention.\nHowever, our experiments reveal that this bidirectional attention mechanism may lead to two main issues in\nvideo transfer tasks: 1) The generated videos often exhibit a simple \u201ccopy-paste\u201d effect from the reference\nvideo. We attribute this to the reference branch\u2019s full access to the target video context, which encourages it\nto adopt a target-like representation, resulting in direct copying of simple patterns. 2) Concatenating the\nreference and target videos for joint self-attention doubles the number of tokens, leading to a fourfold increase\nin computational complexity, which is often impractical.\nTo address the aforementioned issues, we propose fully decoupling the reference and target branches, as\nillustrated in Fig. 2. Formally, both reference and target tokens are first projected into the queries, keys,\nand values, yielding Qref, Kref, Vref, Qtgt, Ktgt and Vtgt. Next, task-aware positional bias is applied to Qref\nand Kref, while Ktgt and Vtgt use the standard RoPE positional encoding. Subsequently, attention interacts\nbetween the two branches in a causal manner. The reference branch performs intra-branch self-attention to\ncapture internal contextual dependencies:\nAttnref = Attn(R\u2217\n\u03b8(Qref), R\u2217\n\u03b8(Kref), Vref),\n(3)\nwhile the target branch integrates information from both its own features and the reference features by\nconcatenating the keys and values:\nAttntgt=Attn(R\u03b8(Qtgt),[R\u03b8(Ktgt);R\u2217\n\u03b8(Kref)],[Vtgt;Vref]),\n(4)\nwhere [\u00b7; \u00b7] denotes token-wise concatenation.\nWe further decouple the time embeddings of the two branches. Specifically, the reference branch adopts a\nfixed t = 0, making it independent of the target video\u2019s noise level. Thanks to this design, the reference\nbranch becomes time-invariant during inference, effectively reducing computational overhead and shortening\nthe overall generation time by 20% compared to the standard architecture.\n4.4\nTask-adaptive Multimodal Alignment\nIn multi-task video transfer, different tasks demand reference information with distinct semantic focuses.\nHowever, conventional in-context learning in diffusion models primarily captures shallow visual correspondences\nrather than semantic intent, limiting their adaptability across tasks. To overcome this, we replace the original\nT5 [48] features in Wan with representations from MLLM, i.e., Qwen-2.5-VL [2], introducing richer visual-\nsemantic cues that enhance reference understanding and task-level alignment.\nThe MLLM takes as input the first-frame tokens of the target video, the reference video tokens, template\ntokens, and prompt tokens. To extract task-specific representations, we draw inspiration from MetaQuery [46]\nand introduce a set of learnable tokens dedicated to each task. For temporal tasks, the MetaQuery aggregates\ntemporal cues from the reference video together with the target\u2019s first-frame content, effectively capturing\n6\nID Transfer\nStyle Transfer\nRef\nI/V\nConsisID\n(Ref I)\nPhantom\n(Ref I)\nStand-in\n(Ref I)\nOurs\n(Ref V)\nRef\nI/V\nStyleMaster\n(Ref I)\nOurs\n(Ref V)\n\u201ca person leaning over a large sketchpad, drawing slow, deliberate lines. The\nsound of pencil scratching blends with faint ambient noise from an open window\u201d\n\u201ca blonde-haired woman in a black top, gently touches and bends down to\nsmell the flowers\u2014picturesque background of more greenery and white flowers\u201d\nPrompt\n\u201cThe video depicts a person folding a large piece of fabric on a table,\nsmoothing out wrinkles carefully with their hands.\u201d\n\u201cThe video shows a person watering a row of plants on a balcony, tilting the\nwatering can slowly over each pot.\u201d\nPrompt\nFigure 4\nQualitative comparison for appearance video transfer. The yellow box highlights how OmniTransfer\ncaptures richer appearance details from multiple cross-view video frames. The red box denotes the input image for\nimage-reference methods.\ncross-frame dynamics in the generated sequence. For appearance tasks, it instead fuses identity or style\ninformation from the reference with the semantic context derived from the prompt tokens.\nTo preserve the multimodal reasoning capability while enabling parameter-efficient adaptation, the MLLM is\nfine-tuned using LoRA [20]. For seamless integration with the diffusion model, its outputs are passed through\na three-layer MultiLayer Perceptron (MLP) and injected solely into the target branch, thereby preventing\ninterference with the reference branch. Consequently, the cross-attention in the target branch is defined as\nAttn(Qtgt, KMLLM, VMLLM), where KMLLM and VMLLM denote the keys and values derived from the aligned\nMLLM features.\n5\nExperiment\n5.1\nImplementation Details\nOur training process is divided into three sequential stages with distinct optimization objectives. In the first\nstage, we train the DiT blocks via in-context learning. Subsequently, we freeze the DiT blocks and focus on\ntraining the connector to align the MLLM with diffusion models. Finally, we unfreeze all components to\nconduct joint fine-tuning. In terms of training hyperparameters, we set the learning rate to 1e \u22125, with a\nbatch size of 16. The three stages are trained for 10,000, 2,000, and 5,000 training steps, respectively.\nDue to the lack of public data sets containing reference video pairs at present, we collected our own data sets\nfrom the Internet to support spatio-temporal video transfer.\n5.2\nEvaluation Details\nTo evaluate spatio-temporal video transfer, we curated dedicated test sets for each subtask. For ID transfer,\n50 videos of diverse individuals are paired with two prompts each to test identity consistency. Style transfer\nincludes 20 unseen visual styles, each with two prompts to assess stylistic variation. Effect transfer contains\n7\nEffect Transfer\nReference Vdieo\nSeedance\nI2V\nOurs\nWan2.1\nI2V\nSeedance\nI2V\nOurs\nWan2.1\nI2V\nReference Video\nReference Vdieo\nReference Vdieo\nOurs\nCamClone\nMaster\nMotionClone\nOurs\nCamClone\nMaster\nMotionClone\nOurs\nOurs\nWan\nAnimate\n(Wan 2.2)\nWan\nAnimate\n(Wan 2.2)\nMimic\nMotion\nMimic\nMotion\nFirst Frame\nFirst Frame\nFirst Frame\nFirst Frame\nReference Vdieo\nFirst Frame\nMotion Transfer\nReference Vdieo\nFirst Frame\nCamera Movement Transfer\nFigure 5 Qualitative comparison for appearance video transfer.\n50 unseen effects from visual-effects websites. Camera movement transfer uses 50 professionally shot videos\nwith complex trajectories. Motion transfer comprises 50 popular dance videos covering diverse dynamic and\nfine-grained body motions.\n5.3\nComparison\n5.3.1\nID Transfer\nWe compared our method with State-Of-The-Art (SOTA) video ID preservation approaches, including\nConsisID [72], Phantom [38] and Stand-in [67].\nFollowing Phantom, we measure text\u2013video alignment\nusing the CLIP-T [61] score, and assess identity consistency with several face recognition models, including\n8\nMethod\nVSim-Arc \u2191\nVSim-Cur \u2191\nVSim-Glint \u2191\nCLIP-T \u2191\nConsisID [72]\n0.34\n0.32\n0.36\n21.54\nPhantom [38]\n0.45\n0.41\n0.47\n20.34\nStand-in [67]\n0.30\n0.21\n0.26\n20.38\nOurs\n0.48\n0.43\n0.51\n20.35\nTable 2 Quantitative comparison for ID Transfer.\nMethod\nVCSD \u2191\nCLIP-T \u2191\nAesthetics \u2191\nStyleCrafter [37]\n0.44\n24.72\n0.47\nStyleMaster [71]\n0.29\n26.82\n0.59\nOurs\n0.51\n27.16\n0.61\nTable 3 Quantitative comparison for Style Transfer.\nSim-Arc [10], Sim-Cur [25], and Sim-Glint [1]. We further extend these metrics to video-level similarity by\nmatching four evenly sampled reference frames with all generated frames, denoted as VSim-Arc, VSim-Cur,\nand VSim-Glint.\nLeveraging multi-view facial information, our method achieves high facial similarity with natural, fluid motions.\nAs shown in the yellow box of Fig. 4, it preserves fine details like acne across frames, which is difficult for\nimage reference methods. In Fig. 4 (right), our method generates diverse poses\u2014including frontal, profile,\nand tilted views\u2014while maintaining high facial similarity, highlighting the advantage of using video as the\nreference.\n5.3.2\nStyle Transfer\nWe compared our method with SOTA text-to-video stylization approaches, StyleCrafter [37] and StyleMas-\nter [71]. Following StyleMaster, we evaluated text\u2013video alignment with CLIP-T, while aesthetics with the\nAesthetics Score [27], and style consistency with the video CSD Score [51] (VCSD) using four sampled reference\nframes.\nTable 3 shows that our method outperforms others on all three metrics. For qualitative comparison in\nFig. 4, we only include StyleMaster, as StyleCrafter yields lower visual quality due to its earlier UNet-based\ndesign. The results demonstrate that our method effectively captures style from the video, surpassing previous\nimage-based methods.\n5.3.3\nEffect Transfer\nSince our test set comes from effect websites with effects that existing commercial LoRA models cannot\nreproduce, we instead compare our method with SOTA image-to-video models, Wan 2.1 I2V [53] and\nSeedance [11], using prompts generated by Qwen-2.5-VL [2] from the reference effect videos. With no standard\nmetrics for effect consistency, we conduct a user study with 20 volunteers, who rate effect fidelity, first-frame\nconsistency, and overall visual quality on a five-point scale.\nAs shown in Table 4, our method achieves the highest scores on all three metrics. Qualitative results in Fig. 4\nfurther show that only our method accurately reproduces the effects of the reference videos, outperforming\nSeedance and Wan I2V. This demonstrates that text alone is insufficient, emphasizing the value of temporal\nvideo references.\n5.3.4\nCamera Movement Transfer\nWe compare our method with SOTA camera movement models, MotionClone [36] and CamCloneMaster [40].\nSince the estimation of complex camera trajectory remains a challenging problem, we conducted a user study\nusing the same setup as in Section 5.3.3, evaluating camera fidelity, image consistency, and overall quality.\n9\nMethod\nEffect Fidelity \u2191\nImage Consistency \u2191\nQuality \u2191\nWan2.1 I2V [53]\n1.81\n2.89\n2.03\nSeedance I2V [11]\n1.95\n3.20\n2.42\nOurs\n3.45\n3.49\n3.27\nTable 4 User study on Effect Transfer.\nMethod\nCamera Fidelity \u2191\nImage Consistency \u2191\nQuality \u2191\nMotionClone [36]\n1.75\n1.23\n1.29\nCamCloneMaster [40]\n1.79\n1.45\n1.29\nOurs\n4.19\n3.89\n3.85\nTable 5 User study on Camera Movement Transfer.\nBaseline\n+TPB\n+RCL\n+TMA\nRefV\nBaseline\n+ TPB\n+RCL\n+TMA\nRefV\nBaseline\n+ TPB\n+RCL\n+TMA\nRefV\n(a) Effect Transfer\nBaseline\n+ TPB\n+RCL\n+TMA\nRefV\nBaseline\n+ TPB\nRefV\nImage\nImage\n\u201ca man with a rugged beard, wearing a leather jacket, riding a vintage motorcycle along a desert highway.\u201d \u201cThe video shows a person arranging books on a shelf, stepping back occasionally to adjust their alignment.\u201d\n(b) Effect Transfer\n(c) ID Transfer\n(d) Style Transfer\n(e) Pose Transfer\nImage\nFigure 6 Qualitative ablation study comparison. Best viewed in zoom.\nAs shown in Table 5, our method outperforms all others across the three metrics. The qualitative results in\nFig. 5 show that competing methods produce only fixed-resolution outputs, resulting in unavoidable resizing\nor cropping. Moreover, our approach is the only one that successfully replicates camera movements from\ncinematic scenes (left) and complex professional tracking shots (right), demonstrating strong generalization\ncapability.\n5.3.5\nMotion Transfer\nWe compare our method with state-of-the-art motion transfer approaches, MimicMotion [75] and WanAni-\nmate [9]. As the test set contains in-the-wild videos without paired ground truth, we conducted a user study\nfollowing Section 5.3.3, evaluating motion fidelity, image consistency, and overall quality. As shown in Table 5,\ndespite using the smaller 14B Wan 2.1 model without additional pose input, our method achieves the highest\nimage consistency, with motion consistency and quality comparable to WanAnimate, which relies on the larger\n28B Wan 2.2 model.\nQualitative comparisons in Fig. 6 show that baseline methods need to align the first frame to a target pose,\noften causing appearance loss (bottom-right of Fig. 6) and unnatural motion. In contrast, our method\nrequires no pose guidance, preserves appearance, produces natural motions, and easily extends to multi-person\nscenarios (Fig. 1).\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**Summary of the Paper \"OmniTransfer: All-in-One Framework for Spatio-temporal Video Transfer\"**\n\n**Main Contributions**\n- **Unified Framework**: OmniTransfer is a unified framework designed to handle diverse spatio-temporal video transfer tasks, including identity (ID), style, effect, motion, and camera movement transfers. It seamlessly unifies these tasks within a single framework.\n  \n- **Task-Aware Positional Bias**: This design allows the model to leverage temporal context cues for various tasks by adding spatial offsets to positional embeddings.\n\n- **Reference-Decoupled Causal Learning**: By separating reference and target branches, this approach enables precise reference transfer while improving computational efficiency. \n\n- **Task-Adaptive Multimodal Alignment**: It uses a multimodal language model (MLLM) to unify and enhance semantic understanding across different tasks dynamically.\n\n**Methodology**\n- The framework integrates multi-frame reference information from the reference video at the spatial level, enhancing appearance consistency.\n  \n- For temporal transfer tasks, offsets are applied along the temporal dimension to preserve temporal alignment. Appearance transfer uses offsets along the width dimension for better propagation of appearance information.\n\n- Reference-decoupled causal learning separates the reference and target branches, reducing computational time by 20% compared to full attention models.\n\n- Task-adaptive multimodal alignment leverages an MLLM to dynamically integrate and align semantic information across tasks, ensuring effective task-specific guidance.\n\n**Key Findings**\n- **Appearance Transfer**: OmniTransfer outperforms existing methods in appearance transfer (ID and style) and matches pose-guided methods in motion transfer without using poses. It achieves a 20% reduction in runtime compared to the base model architecture.\n  \n- **Temporal Transfer**: The framework successfully handles various temporal tasks such as camera movement, video effects, and motion generation.\n\n**Conclusions**\nOmniTransfer establishes a new paradigm for flexible, high-fidelity video generation by seamlessly unifying diverse spatio-temporal video transfer tasks. It demonstrates strong generalization across unseen task combinations and outperforms existing methods in appearance and temporal transfer while matching pose-guided methods in motion transfer without using poses. The framework's design allows for efficient and precise spatio-temporal information transfer, making it a significant advancement in the field of video customization.\n\n**Experiments**\n- **Quantitative Evaluation**: OmniTransfer achieves superior performance on metrics such as ID transfer (CLIP-T score), style transfer (Aesthetics Score), effect transfer (user study results), camera movement transfer (user study results), and motion transfer (user study results).\n  \n- **Qualitative Analysis**: The qualitative evaluation confirms the effectiveness of OmniTransfer in producing high-fidelity video outputs, with detailed comparisons across various tasks.\n\n**Limitations**\nWhile OmniTransfer shows promising results, it may require further optimization for handling more complex or diverse scenarios. Additionally, its reliance on pre-defined temporal priors and pose information might limit its applicability in certain real-world contexts where these are not readily available.\n\nThis summary encapsulates the main contributions, methodology, key findings, and conclusions of the \"OmniTransfer\" paper, highlighting its significance in advancing video customization techniques."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "RM-Distiller: Exploiting Generative LLM for Reward Model Distillation\nHongli Zhou1\u2217, Hui Huang1\u2217, Wei Liu1\u2217, Chenglong Wang2, Xingyuan Bu3,\nLvyuan Han1, Fuhai Song1, Muyun Yang1\u2020, Wenhao Jiang1, Hailong Cao1, Tiejun Zhao1\n1Faculty of Computing, Harbin Institute of Technology, Harbin, China\n2School of Computer Science and Engineering, Northeastern University, Shenyang, China\n3M-A-P\nhongli.joe@stu.hit.edu.cn, yangmuyun@hit.edu.cn\nAbstract\nReward models (RMs) play a pivotal role in align-\ning large language models (LLMs) with human\npreferences. Due to the difficulty of obtaining high-\nquality human preference annotations, distilling\npreferences from generative LLMs has emerged as\na standard practice. However, existing approaches\npredominantly treat teacher models as simple bi-\nnary annotators, failing to fully exploit the rich\nknowledge and capabilities for RM distillation. To\naddress this, we propose RM-Distiller, a framework\ndesigned to systematically exploit the multifaceted\ncapabilities of teacher LLMs: (1) Refinement capa-\nbility, which synthesizes highly correlated response\npairs to create fine-grained and contrastive signals.\n(2) Scoring capability, which guides the RM in\ncapturing precise preference strength via a margin-\naware optimization objective. (3) Generation ca-\npability, which incorporates the teacher\u2019s genera-\ntive distribution to regularize the RM to preserve\nits fundamental linguistic knowledge.\nExtensive\nexperiments demonstrate that RM-Distiller signif-\nicantly outperforms traditional distillation meth-\nods both on RM benchmarks and reinforcement\nlearning-based alignment, proving that exploiting\nmultifaceted teacher capabilities is critical for ef-\nfective reward modeling. To the best of our knowl-\nedge, this is the first systematic research on RM dis-\ntillation from generative LLMs1.\n1\nIntroduction\nLarge language models (LLMs) have demonstrated remark-\nable capabilities across diverse domains, and their effective\ndeployment relies on alignment with human values through\nReinforcement Learning from Human Feedback (RLHF) [Bai\net al., 2022a]. This paradigm primarily depends on reward\nmodeling, which provides a proxy for human preferences to\n\u2217The first three authors contributed equally.\n\u2020 Corresponding author.\n1Our code is available at https://github.com/Joe-Hall-Lee/\nRM-Distiller.\nTeacher\nClassifier\nTeacher\nTraditional Distillation\nScoring\nStudent\nStudent\nRefinement\nGeneration\nRM-Distiller\nresponse 1\nresponse 2\nquery\nPreference Label\nFine-grained Signal\nPreference Strength\nGenerative Distribution\nFigure 1: Compared to traditional method, RM-Distiller unlocks the\nmultifaceted potential of teacher LLMs for better distillation.\nsupervise policy optimization [Zhong et al., 2025]. How-\never, effective reward models (RMs) require to train on large-\nscale, high-quality human preference data. As task complex-\nity scales, obtaining high-quality human annotations becomes\nprohibitively expensive and difficult [Touvron et al., 2023].\nTo mitigate reliance on human feedback, some works pro-\nposed to directly use powerful generative LLMs as RMs\n[Zheng et al., 2023]. However, despite their strong prefer-\nence modeling capabilities, their practical use is limited by\nhigh API costs and inference latency [Liu et al., 2025c]. Con-\nsequently, other works have proposed to distill discriminative\nRMs from generative LLMs [Bai et al., 2022b; Malik et al.,\n2025]. These methods typically employ the LLM as an an-\nnotator to assign binary preference labels to response pairs.\nThe resulting preference data with triplets of (instruction, re-\nsponses, label) are then used to train RMs. Distilling RMs\nfrom powerful generative LLMs offers a cost-effective and\nscalable alternative to human annotation. As a result, it has\nbeen adopted widely in recent RL practice [Liu et al., 2024;\nLiu et al., 2025a].\nDespite its popularity, this paradigm predominantly uti-\nlizes the teacher model\u2019s capability to assign binary prefer-\nence labels, often treating it as a mere binary classifier [Huang\narXiv:2601.14032v1  [cs.CL]  20 Jan 2026\net al., 2025], failing to fully leverage the rich capabilities\nembedded in LLMs. In practice, generative LLMs possess\na wide range of capabilities and internal knowledge that are\nhighly beneficial for preference modeling. The insufficient\nutilization of these capabilities limits the upper bound of RM\nperformance and subsequent RL-based alignment.\nWe therefore investigate a crucial yet unexplored question:\nHow can we more effectively distill generative LLMs\ninto reward models?\nIn this paper, we propose RM-Distiller, a framework that\nexploits generative LLMs to distill RMs, by incorporating\ntheir multifaceted capabilities across three dimensions:\n1. Refinement capability: We leverage the refinement ca-\npability of generative LLMs to construct highly con-\ntrastive preference pairs for RM training. By perform-\ning targeted revisions on rejected responses, the teacher\nsynthesizes highly correlated response pairs that differ\nonly in the attributes determining preference. This com-\npels the RM to discern subtle quality nuances instead of\nspurious correlations [Jiang et al., 2025].\n2. Scoring capability: We utilize the scoring capability of\ngenerative LLMs to assess response quality and model\npreference strength. Instead of relying on discrete bi-\nnary annotation, the teacher provides continuous prefer-\nence scores that better reflect varying degrees of quality\ndifferences between response pairs. This allows the RM\nto be optimized with a margin-aware objective, captur-\ning more nuanced preference strengths.\n3. Generation capability: We incorporate the generation\ncapability of generative LLMs to safeguard the founda-\ntional linguistic proficiency of the student model. By\ndistilling the teacher\u2019s output distribution into the stu-\ndent backbone, we enforce a constraint that prevents the\nRM from undergoing catastrophic forgetting. This en-\nsures the model retains robustness during regressive RM\ntraining [Yang et al., 2024].\nWe evaluate RM-Distiller on both RM benchmarks and\ndownstream RL alignment tasks.\nThe results demonstrate\nthat RM-Distiller significantly outperforms existing distilla-\ntion methods with superior RM accuracy and RL alignment\nefficacy. Despite the prevailing practice of distilling genera-\ntive LLMs into RMs, this is the first work that systematically\nexplores the best practice for RM distillation.\nOur contributions are as follows:\n1. We conduct the first systematic investigation into RM\ndistillation, by introducing a multifaceted distillation\nstrategy RM-Distiller.\n2. We propose to exploit three capabilities of the generative\nLLMs, refinement, scoring, and generation, to provide\ncomprehensive supervision for training the RMs.\n3. Extensive experiments demonstrate that RM-Distiller\nsignificantly improves RM performance both on RM\nbenchmarks and downstream alignment scenarios.\n2\nBackground\n2.1\nPreliminary: Reward Modeling\nRMs traditionally relied on human preference datasets, typ-\nically trained using the Bradley-Terry (BT) model to predict\npairwise rankings [Wang et al., 2025b]. The standard ob-\njective of RM training is to learn a scalar function r\u03d5(x, y)\nthat reflects human preference. Given a preference dataset\nD = {(x, yw, yl)}, where yw and yl denote the chosen and\nrejected responses for instruction x, respectively, the RM is\noptimized by minimizing the negative log-likelihood:\nLBT(\u03d5) = log \u03c3 (r\u03d5(x, yw) \u2212r\u03d5(x, yl))\n(1)\nwhere \u03c3 is the sigmoid function. While effective for rank-\ning, this loss function only considers the binary order of re-\nsponses, ignoring the preference strength between them.\n2.2\nRM Distillation from Generative LLM\nDue to the high cost and limited scalability of human over-\nsight [Touvron et al., 2023], the community has increasingly\nshifted toward getting preference annotation from generative\nLLM [Liu et al., 2024], where teacher models are employed\nto evaluate paired responses and identify the preferred ones,\nthereby constructing preference annotations for training BT\nmodels [Bai et al., 2022b; Liu et al., 2025a]. However, this\nparadigm often treats the teacher as a black-box classifier,\nfailing to exploit its generative potential or the nuanced pref-\nerence intensities necessary for reliable alignment.\nTo address this, recent studies have explored ways to enrich\nthe distillation signal by incorporating more diverse informa-\ntion from teacher models. For instance, SynRM [Ye et al.,\n2025] integrates natural language critiques into RMs by ap-\npending teacher-generated justifications to the responses dur-\ning both training and inference. Further extending this idea,\nCLoud [Ankner et al., 2024] explicitly distills the capabil-\nity of generating critiques into discriminative RMs. Another\nline of research focuses on data augmentation, utilizing LLMs\nto synthesize a larger volume of preference data through re-\nsponse perturbation to cover a broader distribution of scenar-\nios [Park et al., 2024; Shen et al., 2025]. While these method-\nologies enhance supervision or data diversity, they often only\nexploit a fraction of the teacher model\u2019s multifaceted capabil-\nities, failing to fully harness its potential for RM distillation.\n3\nRM-Distiller\nRM-Distiller begins by Initial Preference Pair Construction,\nwhich serves as the foundation for RM distillation. Subse-\nquently, it fully exploits the teacher model\u2019s multifaceted ca-\npabilities through: (1) Refinement capability for Contrastive\nRefinement, (2) Scoring capability for Strength Guidance, (3)\nGeneration capability for Generative Regularization.\n3.1\nInitial Preference Pair Construction\nInitial Candidate Generation\nGiven an instruction x from an instruction set Dx, we first\nchoose a set of candidate models M = {m1, m2, . . . , mK}.\nAfter that, we collect a response pool Yx = {y1, y2, . . . , yK}\nsuch that each yj \u223cmj(x). The candidate models are chosen\nto differ in model families and sizes, ensuring responses to\neach instruction are with varying styles and quality levels.\n\u2461Contrastive Refinement\n\u2460Initial Preference Pair Construction\n\u2462Preference Modeling with Strength Guidance\nTeacher\nrefinement history\nTeacher\n\u2463Regularization with Generative Distribution\nTeacher\nStudent\nlogityT:   0.2     0.4      0.5     0.9 \u2026\nlogityT:   0.4     0.2      0.5     0.6 \u2026\nStudent\nmargin\nscore\nrejected\n1\nscore\nrefined\nchosen\n8\nrejected\nrefined\nchosen\nreward\nrejected\nreward\nrefined\nchosen\n0.5\n2.5\nmargin\nrefined\nchosen\nrejected\nquery\ncandidate model pool\nresps\nTeacher\nchosen\nrejected\ncontrastive\ndiagnostic\nteacher\nresp\nquery\nteacher\nresp\nchosen\nrejected\n\ud835\udc9a\ud835\udc98 : 9.9 > 9.11\n\ud835\udc99: 9.11 and 9.9, which is bigger?\n\ud835\udc9a\ud835\udc8d: 9.11 is bigger than 9.9.\n\ud835\udc9a\ud835\udc98\u2217: 9.11 is smaller than 9.9.\n\ud835\udc9a\ud835\udc8d: 9.11 is bigger than 9.9.\n\ud835\udc9aT:\nTo determine which number \u2026\nMargin-Aware\nRegression\nGenerative\nRegularization\nFinal Student\nFigure 2: The illustration of RM-Distiller. We first initialize preference pairs from diverse candidate models, and then synthesize highly cor-\nrelated response pairs. During the training phase, the student model is guided by continuous preference margins and generative regularization.\nInitial Preference Annotation\nTo construct the initial preference data, for each instruction\nx, we randomly sample two distinct responses (ya, yb) from\nYx. We then employ the generative teacher LLM fT to deter-\nmine the relative quality between (ya, yb). This results in an\nannotated preference pair (yw, yl), where yw and yl denote\nthe chosen and rejected responses, respectively.\nAt this stage, we obtain preference triplets (x, yw, yl),\nwhich can be directly used to train RMs under the BT ob-\njective, as in standard RM distillation. However, this will\nresult in the insufficient utilization of teacher models\u2019 capa-\nbilities. Therefore, we introduce the following methods to\nfurther leverage the teacher\u2019s supervision signal.\n3.2\nContrastive Refinement\nThe efficacy of RMs depends heavily on the contrastive qual-\nity of the preference pairs. In standard distillation practices,\npairs are often sampled independently, therefore exhibit sig-\nnificant discrepancies in length, formatting, or linguistic style\n[Jiang et al., 2025], which results in the RMs to rely on super-\nficial shortcut instead of intrinsic quality attributes [Zhou et\nal., 2024]. To eliminate these spurious correlations and force\nthe model to identify fine-grained preference boundaries, we\nutilize the refinement capability of the teacher model to syn-\nthesize highly correlated contrasts, as shown in Figure 3.\nThe refinement process begins by analyzing the gap be-\ntween the rejected response yl and the teacher-generated re-\nsponse yT , where yT serves as a high-quality reference. We\nfirst prompt the teacher model fT to perform a contrastive di-\nagnostic, where it articulates specific reasons for the rejection\nContrastive Refinement Example\nInstruction\nHow to get the last element of a list L in Python?\nRejected Response (yl)\nYou can use L[0] to access the last element of a list.\nRefined Chosen Response (y\u2217\nw)\nYou can use L[-1] to access the last element of a list.\nTeacher Response (yT )\nIn Python, negative indexing allows you to access elements\nfrom the end, with -1 representing the last item.\nFigure 3: A concrete example of Contrastive Refinement.\nand outlines a path for improvement. Following this diagnos-\ntic, fT is instructed to minimally modify yl into a refined cho-\nsen version y\u2217\nw that matches the quality of yT while strictly\npreserving the original structure and phrasing. By maintain-\ning a minimal edit distance, we ensure that y\u2217\nw and yl share\na nearly identical semantic backbone, differing only in the\nattributes that define superiority.\n3.3\nPreference Modeling with Strength Guidance\nMargin-Aware Regression\nIn the traditional BT framework, the RM is trained as a bi-\nnary classifier to maximize the probability P(yw \u227byl). This\nobjective implicitly assuming a uniform preference strength\nacross all response pairs. However, in reality, the quality dif-\nference between responses varies significantly [Wang et al.,\n2024]. Therefore, we leverage the scoring capability of LLMs\nto explicitly capture and represent the strength of preferences\nto provide fine-grained supervision signals.\nFirst, we prompt the teacher model fT to assign scalar\nquality scores sT (x, y) for chosen and rejected responses yw\nand yl, respectively. After that, we propose Margin-Aware\nRegression, to guide the student model to mimic the teacher\u2019s\nevaluative sensitivity by aligning the predicted reward differ-\nence with the teacher-generated score margin. Specifically,\nwe define margin-aware loss as follows:\nLmargin =\n\u0010\n(r\u03d5(x, yw) \u2212r\u03d5(x, yl)) \u2212(sT (x, yw) \u2212sT (x, yl))\n\u00112 (2)\nWith this objective, the student model explicitly learns\nto align not only the relative order but also the exact mag-\nnitude of preferences, thereby being able to capture fine-\ngrained variations in preference strength, distinguishing be-\ntween subtle improvements and absolute errors. While some\nrecent studies have attempted to introduce a margin as a lower\nbound [Touvron et al., 2023], they still fail to capture the nu-\nanced intensity of preference as in this work.\nSelf-Calibrated Scoring\nAs established in Section 3.2, we obtained the refined re-\nsponse y\u2217\nw through Contrastive Refinement as the chosen re-\nsponse. To assign more precise quality scores, we introduce\nSelf-Calibrated Scoring. Specifically, we explicitly provide\nthe teacher with the previously assigned score of the rejected\nresponse sT (x, yl) as a fixed anchor, and task it with scoring\nthe refined response y\u2217\nw on the same scale. As a result, the\nscore sT (x, y\u2217\nw) reflects a calibrated increase proportional to\nthe specific corrections applied during refinement2. This rela-\ntive evaluation protocol yields more stable and learnable pref-\nerence margins for downstream reward modeling.\nMoreover, during this process, we reuse the refinement di-\nalogue history as the context for scoring, exposing the scorer\nto the teacher-produced contrastive diagnosis and its refine-\nment outcome. By conditioning the scoring on the explicit\nrefinement history, Self-Calibrated Scoring transforms abso-\nlute quality assessment into a relative adjustment.\nPreference Pair Filtering\nTo ensure high data quality, we filter the synthesized prefer-\nence pairs to retain sufficiently strong contrasts based on two\ncriteria: (1) a substantial score margin and (2) a minimum\nedit distance between y\u2217\nw and yl:\nDrefine = {(x, y\u2217\nw, yl, sT (x, y\u2217\nw), sT (x, yl) |\ndistedit(y\u2217\nw, yl) > \u03c4e \u2227\u2206s > \u03c4s}\n(3)\nwhere \u03c4s and \u03c4e denote the minimum thresholds for the score\nmargin and the edit distance, respectively. Filtering for edit\ndistance ensures that the refinement has introduced meaning-\nful changes, while score margin ensures that the teacher rec-\nognizes a sufficient quality improvement. This ensures effec-\ntive contrast between preference pairs for RM training.\n2For complete prompts please refer to Appendix D.\n3.4\nRegularization with Generative Distribution\nGenerative Regularization\nSolely optimizing for discriminative signals can lead to re-\nward over-optimization, where the model gradually loses its\nfoundational understanding of language, which is essential\nfor generalizability [Yang et al., 2024]. Therefore, we pro-\npose Generative Regularization to preserve the generative\nknowledge of the backbone model during distillation.\nSpecifically, we retain the student\u2019s language model (LM)\nhead and use the teacher fT as a linguistic anchor. For each\nteacher response yT , we supervise the student\u2019s LM head\nusing a combination of negative log-likelihood (NLL) and\nKullback-Leibler (KL) divergence distillation:\nLreg = 1\nL\nL\nX\nt=1\n\u0010\n\u2212\u03b1 \u00b7 log P\u03d5(yt\nT |x, y<t\nT )\n+ \u03b2 \u00b7\nX\nv\u2208V\nPT (v|x, y<t\nT ) log PT (v|x, y<t\nT )\nP\u03d5(v|x, y<t\nT )\n\u0011\n(4)\nwhere yT = {y1\nT , y2\nT , . . . , yL\nT } denotes the teacher response\nof length L, and V denotes the vocabulary. P\u03d5 and PT rep-\nresent the next-token probability distributions of the student\nand the teacher, respectively. \u03b1 and \u03b2 are coefficients that\ncontrol the strength of the NLL loss and the KL divergence.\nNotably, if the teacher model is closed-source or if the teacher\nand student do not share a common vocabulary, we set \u03b2 = 0.\nTotal Objective Function\nCombining our proposed Margin-Aware Regression and the\nGenerative Regularization loss, the total optimization objec-\ntive of RM-Distiller is formulated as a weighted sum:\nLtotal = (1 \u2212\u03b1 \u2212\u03b2) \u00b7 Lmargin + Lreg\n(5)\nThis multi-task objective synergizes precise preference\nmodeling with linguistic anchoring. Specifically, Lmargin en-\nables the model to capture precise quality differences, while\nLreg prevents representation collapse by grounding the shared\nbackbone in the teacher\u2019s generative distribution. This dual\nsupervision ensures semantic robustness, leading to better\ngeneralization and stability during downstream RLHF.\n4\nExperiments\nIn this section, we evaluate RM-Distiller through a series of\nexperiments to assess its reward modeling performance.\n4.1\nExperimental Setup\nData Settings\nFor training RMs, we utilize a 10,000-sample subset ran-\ndomly drawn from the Skywork-Preference-v0.2 dataset [Liu\net al., 2024]. Our method is evaluated across two scenarios:\n\u2022 Unlabeled Scenario where no prior preference annota-\ntions are available, and we only utilize instructions from\nthe dataset and employ candidate models to generate re-\nsponses. This setup mirrors practical distillation settings\nwith no access to high-quality human annotations, making\nLLM-based distillation a more practical alternative.\nStudent\nTeacher\nMethod\nRewardBench\nRM-Bench\nChat\nHard\nSafety\nReason\nAvg.\nEasy\nNormal\nHard\nAvg.\nLLM-as-a-Judge\nClaude-3.5-Sonnet-20240620\n96.4\n74.0\n81.6\n84.7\n84.2\n73.8\n63.4\n45.9\n61.0\nLlama-3.1-70B-Instruct [Grattafiori et al., 2024]\n97.2\n70.2\n82.8\n86.0\n84.1\n74.7\n67.8\n54.1\n65.5\nGPT-4o [Hurst et al., 2024]\n96.1\n70.7\n87.8\n75.1\n82.4\n83.8\n70.9\n48.2\n67.6\nQwen3-14B [Yang et al., 2025]\n96.4\n69.2\n88.0\n82.0\n83.9\n83.6\n71.7\n52.1\n69.8\nQwen2.5-3B-Instruct [Qwen et al., 2025]\n88.3\n43.2\n55.1\n42.7\n57.3\n69.6\n49.4\n34.3\n50.7\nOpen-Source Reward Models\nEurus-RM-7B [Yuan et al., 2025]\n98.0\n65.6\n81.4\n86.3\n81.6\n87.2\n70.2\n40.2\n65.9\nGRAM-LLaMA3.2-3B-RewardModel [Wang et al., 2025a]\n90.6\n83.9\n79.8\n80.2\n83.6\n84.2\n65.2\n40.0\n63.2\nJudgeLRM-7B [Chen et al., 2025]\n92.9\n56.4\n78.2\n73.6\n75.2\n73.2\n66.2\n54.8\n64.7\nTraining on the Same Preference Dataset in the Unlabeled Scene\nQwen2.5-3B-Instruct\nGPT-4o\nBT Classifier\n93.3\n37.7\n69.3\n81.0\n70.3\n91.9\n62.3\n13.6\n55.9\nMargin BT [Touvron et al., 2023]\n93.6\n33.6\n73.5\n80.8\n70.4\n90.4\n61.0\n17.5\n56.3\nSteerLM [Dong et al., 2023]\n94.4\n46.5\n74.3\n82.8\n74.5\n90.2\n59.8\n17.1\n55.7\nSynRM [Ye et al., 2025]\n96.1\n47.6\n78.7\n79.1\n75.4\n91.0\n66.2\n25.1\n60.7\nRMBoost [Shen et al., 2025]\n91.3\n35.1\n67.8\n74.8\n67.3\n86.3\n54.9\n17.6\n52.9\nCLoud [Ankner et al., 2024]\n93.6\n37.1\n62.6\n63.6\n64.2\n87.1\n59.2\n18.9\n55.1\nLSAM [Wang et al., 2024]\n94.1\n37.3\n73.5\n81.4\n71.6\n89.3\n58.8\n22.2\n56.8\nRM-Distiller\n95.5\n51.1\n84.1\n87.4\n79.5\n86.4\n64.8\n32.2\n61.1\nQwen3-14B\nBT Classifier\n88.0\n43.4\n72.4\n83.5\n71.8\n89.8\n60.7\n19.8\n56.7\nMargin BT [Touvron et al., 2023]\n94.7\n46.3\n79.9\n70.4\n72.8\n90.7\n63.6\n26.3\n60.2\nSteerLM [Dong et al., 2023]\n93.6\n45.0\n79.1\n78.5\n74.0\n89.9\n64.1\n19.9\n58.0\nSynRM [Ye et al., 2025]\n84.9\n57.0\n79.1\n91.1\n78.0\n92.7\n66.7\n20.0\n59.8\nRMBoost [Shen et al., 2025]\n92.7\n43.6\n73.7\n90.6\n75.2\n84.8\n60.3\n25.9\n57.0\nCLoud [Ankner et al., 2024]\n91.1\n39.7\n74.1\n67.5\n68.1\n87.7\n58.4\n20.3\n55.5\nLSAM [Wang et al., 2024]\n92.5\n39.9\n72.7\n84.9\n72.5\n88.8\n60.4\n24.6\n57.9\nRM-Distiller\n95.0\n55.0\n81.8\n89.0\n80.2\n87.2\n66.3\n34.6\n62.7\nTraining on the Same Preference Dataset in the Labeled Scene\nQwen2.5-3B-Instruct\n\u2014\nBT Classifier\n77.4\n71.3\n86.5\n60.0\n73.8\n81.7\n64.5\n43.2\n63.2\nGPT-4o\nMargin BT [Touvron et al., 2023]\n88.6\n70.8\n87.3\n67.8\n78.6\n80.3\n66.8\n48.2\n65.1\nSteerLM [Dong et al., 2023]\n91.6\n65.4\n80.4\n63.5\n75.2\n85.6\n65.8\n37.4\n62.9\nSynRM [Ye et al., 2025]\n81.0\n82.5\n86.9\n78.6\n82.2\n80.9\n66.0\n40.3\n62.4\nRMBoost [Shen et al., 2025]\n85.8\n73.7\n86.6\n74.9\n80.3\n79.3\n59.0\n38.8\n59.1\nCLoud [Ankner et al., 2024]\n69.3\n66.0\n72.6\n66.9\n68.7\n68.3\n57.1\n45.4\n57.0\nLSAM [Wang et al., 2024]\n89.4\n66.5\n88.1\n63.8\n77.0\n85.2\n63.8\n36.7\n61.9\nRM-Distiller\n92.7\n64.3\n90.7\n86.9\n83.6\n87.0\n68.9\n42.0\n65.9\nQwen3-14B\nMargin BT [Touvron et al., 2023]\n88.6\n72.8\n86.6\n71.6\n79.9\n81.0\n67.8\n49.1\n66.0\nSteerLM [Dong et al., 2023]\n89.7\n69.5\n87.2\n67.1\n78.4\n84.6\n68.3\n43.8\n65.6\nSynRM [Ye et al., 2025]\n78.2\n80.7\n85.1\n74.8\n79.7\n73.6\n61.5\n48.5\n61.2\nRMBoost [Shen et al., 2025]\n82.7\n73.7\n82.2\n88.2\n81.7\n84.8\n60.3\n25.9\n57.0\nCLoud [Ankner et al., 2024]\n83.0\n67.3\n75.0\n62.2\n71.9\n77.3\n61.9\n44.9\n61.4\nLSAM [Wang et al., 2024]\n93.0\n69.7\n84.1\n70.4\n79.3\n90.7\n66.0\n35.2\n63.9\nRM-Distiller\n91.3\n69.7\n85.5\n91.3\n84.5\n89.5\n69.3\n40.5\n66.4\nTable 1: Results of different RM distillation methods on RewardBench and RM-Bench.\n\u2022 Labeled Scenario where response pairs are associated with\nhigh-quality ground-truth preference annotations, and we\nretain the original response pairs from the dataset and keep\nonly those samples for which the teacher\u2019s preference judg-\nments are consistent with the original labels.\nWe employ RewardBench [Lambert et al., 2025] and RM-\nBench [Liu et al., 2025b] as our primary evaluation bench-\nmarks, which cover diverse subsets.\nModels\nWe employ Qwen2.5-3B-Instruct [Qwen et al., 2025] as the\nbackbone for the student RM. To facilitate distillation, we use\nGPT-4o [Hurst et al., 2024] and Qwen3-14B [Yang et al.,\n2025] in its non-thinking mode as the teacher models. For\nconstructing the response pool in the unlabeled scenario, we\nleverage a set of candidate models including Llama-3.1-8B-\nInstruct [Grattafiori et al., 2024], Mistral-7B-Instruct-v0.3\n[Jiang et al., 2023], Gemma-2-9b-it [Team et al., 2024] and\nVicuna-7b-v1.5 [Zheng et al., 2023].\nBaselines\nWe compare RM-Distiller against several RM distillation\nmethods including the vanilla BT Classifier, alongside its ex-\ntensions Margin BT [Touvron et al., 2023] and LSAM [Wang\net al., 2024]. We also evaluate against methods that leverage\nteacher-generated critiques, revisions or attributes, including\nSynRM [Ye et al., 2025], CLoud [Ankner et al., 2024], RM-\nPolicy\nStudent RM\nTeacher\nDataset\nMethod\nAlgorithm\nAlpacaEval 2.0\nFollowBench\nCF-Bench\nLC\nLen\nHSR\nSSR\nCSR\nISR\nPSR\nLlama-3-8B\n\u2014\n\u2014\nUltraChat\nBaseline\nSFT\n8.86\n1,017\n41.04\n57.39\n0.51\n0.15\n0.22\nQwen2.5-3B\n-Instruct\nGPT-4o\nHH-RLHF\nBT Classifier\nPPO\n14.32\n2,045\n46.85\n61.20\n0.57\n0.18\n0.26\nGRPO\n17.89\n2,310\n48.33\n62.15\n0.61\n0.22\n0.30\nDAPO\n18.25\n2,005\n48.59\n62.48\n0.62\n0.22\n0.31\nRM-Distiller\nPPO\n16.90\n1,927\n48.42\n62.05\n0.60\n0.23\n0.29\nGRPO\n20.10\n2,342\n49.95\n63.14\n0.64\n0.24\n0.33\nDAPO\n20.52\n2,487\n50.09\n63.31\n0.63\n0.23\n0.32\nShareGPT\nBT Classifier\nPPO\n13.80\n2,584\n46.20\n60.95\n0.56\n0.17\n0.25\nGRPO\n17.92\n2,360\n47.90\n61.88\n0.61\n0.21\n0.30\nDAPO\n17.45\n2,200\n48.15\n62.10\n0.60\n0.21\n0.29\nRM-Distiller\nPPO\n16.24\n2,293\n47.78\n61.77\n0.59\n0.19\n0.27\nGRPO\n19.85\n2,090\n49.45\n62.90\n0.62\n0.22\n0.31\nDAPO\n20.15\n2,335\n49.62\n63.15\n0.63\n0.23\n0.32\nTable 2: Performance of policy model supervised by different RM distillation methods.\nMethod\nRewardBench\nRM-Bench\nAverage\nAverage\nGPT-4o as Teacher and Qwen2.5-3B-Instruct as Student\nBT Classifier\n70.3\n55.9\n+ Margin-Aware Regression\n72.7\n57.3\n+ Contrastive Refinement\n76.8\n58.8\n+ Generative Regularization\n79.5\n61.1\nQwen3-14B as Teacher and Qwen2.5-3B-Instruct as Student\nBT Classifier\n71.8\n56.7\n+ Margin-Aware Regression\n75.7\n59.5\n+ Contrastive Refinement\n78.7\n61.5\n+ Generative Regularization\n80.2\n62.7\nTable 3: Ablation study of RM-Distiller components.\nBoost [Shen et al., 2025], and SteerLM [Dong et al., 2023]3.\nWe also include a range of external RMs as reference\npoints. These comprise discriminative models such as Eurus-\nRM-7B [Yuan et al., 2025], as well as off-the-shelf gen-\nerative models including Claude-3.5-Sonnet and Llama-3.1-\n70B-Instruct, along with fine-tuned generative models in-\ncluding GRAM-LLaMA3.2-3B-RewardModel [Wang et al.,\n2025a] and JudgeLRM-7B [Chen et al., 2025].\n4.2\nMain Results\nAs shown in Table 1, RM-Distiller consistently and signif-\nicantly outperforms traditional distillation methods across\nboth unlabeled and labeled settings\uff0cwhether using a closed-\nsource or open-source model as a teacher.\nNotably, RM-\nDistiller also surpasses several powerful external baselines,\nincluding both frontier generative models and open-source\nRMs built on significantly larger foundation models or trained\nwith more extensive datasets.\nThese performance gains are driven by the effective syn-\nergy of three key capabilities leveraged from the teacher\nmodel. Specifically, the teacher\u2019s refinement and scoring abil-\n3For more details please refer to Appendix A.\nities enable the RM to identify fine-grained differences in re-\nsponse pairs, thereby enhancing the perceptual capability of\nnuanced quality aspects. Furthermore, the teacher\u2019s genera-\ntion capability helps the RM maintain a foundational under-\nstanding of language, allowing it to better comprehend what\nconstitutes a good response.\n4.3\nAblation Study\nTo further verify the effectiveness of RM-Distiller, we con-\nduct an ablation study by incrementally adding each mod-\nule, corresponds to each capability of the teacher. As shown\nin Table 3, each proposed module contributes significantly\nto the overall performance gains. Across both RewardBench\nand RM-Bench, we observe a steady increase in average ac-\ncuracy as we incrementally integrate Margin-Aware Regres-\nsion, Contrastive Refinement, and Generative Regularization.\nThis confirms that leveraging the teacher\u2019s multifaceted ca-\npabilities is essential for high-fidelity RM distillation on both\nevaluation benchmarks4.\n4.4\nPerformance on RLHF\nTo evaluate the effectiveness of the distilled RMs in down-\nstream alignment, we conduct RLHF experiments using PPO\n[Schulman et al., 2017], GRPO [Shao et al., 2024], and\nDAPO [Yu et al., 2025]. For all RL experiments, we adopt\nLlama-3-8B [Grattafiori et al., 2024] as the base policy\nmodel. We first perform SFT on the base model using the\nUltraChat dataset [Ding et al., 2023] as a cold-start.\nWe\nthen use prompts from the HH-RLHF [Bai et al., 2022a] and\nShareGPT [Zheng et al., 2023] datasets as training instruc-\ntions during RL. Alignment effectiveness are evaluated on\nboth general and complex instruction following benchmarks,\nincluding AlpacaEval 2.0 [Dubois et al., 2024], FollowBench\n[Jiang et al., 2024] and CFBench [Zhang et al., 2025]5.\nAs shown in Table 2, RM-Distiller consistently outper-\nforms the BT Classifier across all datasets and RL algo-\n4Additional ablation studies are reported in Appendix B.\n5Since these benchmarks rely on LLM-based evaluation, we\nadopt GPT-4o-0806 as the judge model.\nModel\nSample Num\nAcc.\nSkywork-Reward-V2-8B [Liu et al., 2025a]\n40,000K\n81.3\nSkywork-Reward-8B-v0.2 [Liu et al., 2024]\n80K\n75.9\nURM-LLaMA-3.1-8B [Lou et al., 2024]\n100K\n76.7\nLlama-3-OffsetBias-8B [Park et al., 2024]\n70K\n83.2\nTulu-3-8B-RM-RB2 [Malik et al., 2025]\n350K\n83.2\nBT-Qwen2.5-3B-Instruct\n10K\n72.2\nRM-Distiller-Qwen2.5-3B-Instruct\n10K\n85.4\nTable 4: Results of different methods and previous RMs on the Ara-\nbic preference test set.\nStudent\nTeacher\nMethod\nEvalBiasBench\nGPT-4o [Hurst et al., 2024]\n70.0\nQwen3-14B [Yang et al., 2025]\n75.0\nQwen2.5-3B-Instruct [Qwen et al., 2025]\n44.4\nQwen2.5-3B-Instruct\nGPT-4o\nBT Classifier\n30.0\nRM-Distiller\n57.5\nQwen3-14B\nBT Classifier\n47.5\nRM-Distiller\n63.8\nTable 5: Results of different methods on EvalBiasBench.\nrithms, indicating more effective reward signal for RLHF.\nOur distilled RM significantly enhances both general align-\nment and complex instruction following, demonstrating its\nability to discern nuanced quality differences. Furthermore,\nthe consistent gains under length-controlled evaluations con-\nfirm that these improvements reflect genuine enhancements\nin response quality rather than a mere verbosity bias.\n4.5\nFast Adaptation Capability\nFast Adaptation refers to the ability to rapidly provide reli-\nable reward signals for novel, domain-specific tasks, which\nis a vital determinant of RM\u2019s efficacy. RM-Distiller offers\nan efficient and practical solution, enabling the rapid distil-\nlation of specialized RMs with only a small amount of tar-\ngeted data. To evaluate this capability, we choose the Ara-\nbic language domain, where we utilize an Arabic dataset6,\nwhich has significant difference with general preference mod-\neling7. We utilize Qwen2.5-3B-Instruct as the student model\nand Qwen3-14B as the teacher model.\nAs shown in Table 4, RM-Distiller not only demonstrates\na substantial margin over the BT model but also surpasses\ncurrently top-performing generalist RMs. This empirical ev-\nidence reveals that generalist RMs struggle to achieve true\nuniversality across diverse specific domains. In contrast, RM-\nDistiller provides an efficient mechanism to adapt and distill\nreliable reward signals into specialized scenarios with only a\nsmall group of instruction data, enabling fast adaptation of\nreward modeling to a new domain.\n6https://huggingface.co/datasets/FreedomIntelligence/\nArabic-preference-data-RLHF\n7We randomly sample 10,000 instructions as the start point for\ndistillation. For evaluation, we extract a test set of 1,000 samples,\nwhich are filtered by GPT-5 to ensure a reliable ground truth.\nStudent\nTeacher\nMethod\nIFBench\nGPT-4o [Hurst et al., 2024]\n50.8\nQwen3-14B [Yang et al., 2025]\n54.2\nQwen2.5-3B-Instruct [Qwen et al., 2025]\n36.9\nQwen2.5-3B-Instruct\nGPT-4o\nBT Classifier\n50.2\nRM-Distiller\n57.9\nQwen3-14B\nBT Classifier\n52.5\nRM-Distiller\n57.7\nTable 6: Results of different methods on IFBench.\nTeacher\nMethod\nRewardBench\nRM-Bench\nAverage\nAverage\nGPT-4o\nBT Classifier\n50.3\n48.1\nRM-Distiller\n76.1\n59.9\nQwen3-14B\nBT Classifier\n64.6\n51.9\nRM-Distiller\n76.4\n60.2\nTable 7: Results of different methods under 1K instructions.\n4.6\nGeneralizability\nA robust RM should generalize to challenging scenarios with-\nout exploiting spurious shortcuts. Poor generalizability often\nleads to reward hacking, where the model exploits unintended\nbiases to achieve deceptively high scores without fully fol-\nlowing the instructions [Zhou et al., 2024]. In this section,\nwe employ EvalBiasBench [Park et al., 2024] and IFBench\n[Peng et al., 2025] as our evaluation benchmarks.\nAs shown in Table 5 and 6, RM-Distiller demonstrates a\nsuperior capacity to generalize to out-of-distribution tasks.\nIn EvalBiasBench, while the BT Classifier collapses by rely-\ning on superficial patterns, RM-Distiller exhibits significantly\nhigher robustness. This indicates the student model learns to\ndiscern semantic boundaries rather than spurious correlations.\nIn IFBench, RM-Distiller not only outperforms the BT Clas-\nsifier baseline but even surpasses the direct prompting perfor-\nmance of the teacher. This demonstrates that RM-Distiller ef-\nfectively captures fine-grained constraints within instructions,\nverifying its generalizability to unseen tasks.\n4.7\nData Efficiency\nData efficiency is critical in RM distillation, as high-quality\ninstruction data is often scarce when dealing with specialized\ndomains or rapid model iteration. To evaluate this, we com-\npare RM-Distiller against the BT baseline using a reduced\ntraining set of only 1,000 instructions.\nAs illustrated in Table 7, when the training set is reduced to\nonly 1,000 instructions, the performance of the student model\ntrained with BT model deteriorates sharply. In contrast, RM-\nDistiller exhibits exceptional data efficiency, maintaining a\nhigh level of discriminative accuracy. By extracting signifi-\ncantly more supervisory information from each sample, RM-\nDistiller effectively distills the teacher\u2019s underlying prefer-\nences. This high-density training signal enables the develop-\nment of reliable RMs with minimal data, offering a practical\nsolution for real-world alignment tasks.\n5\nConclusion\nIn this work, we present the first systematic research about\nhow to distill generative LLMs into discriminative RMs. By\nexploiting the multifaceted capabilities of generative LLMs,\nour proposed RM-Distiller provides the student model with\nrich, reliable supervision signals, enabling high RM perfor-\nmance without additional inference overhead.\nGiven the rapid progress of frontier LLMs, alignment can\nbe done from both human and AI feedback. In the future, we\nwill try to distill preference signals from human-LLM collab-\noration to further improve alignment efficacy.\nReferences\n[Ankner et al., 2024] Zachary Ankner, Mansheej Paul, Bran-\ndon Cui, Jonathan Daniel Chang, and Prithviraj Am-\nmanabrolu. Critique-out-loud reward models. In Pluralis-\ntic Alignment Workshop at NeurIPS 2024, 2024.\n[Bai et al., 2022a] Yuntao Bai, Andy Jones, Kamal Ndousse,\nAmanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.\nTraining a helpful and harmless assistant with reinforce-\nment learning from human feedback.\narXiv preprint\narXiv:2204.05862, 2022.\n[Bai et al."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**Summary of the Paper \"RM-Distiller: Exploiting Generative LLM for Reward Model Distillation\"**\n\n### Main Contributions\n- **RM-Distiller**: A framework designed to systematically exploit the multifaceted capabilities of teacher generative language models (LLMs) for better reward model distillation.\n- **Key Capabilities**:\n  - **Refinement Capability**: Synthesizes highly correlated response pairs to create fine-grained and contrastive signals, enabling nuanced quality distinctions.\n  - **Scoring Capability**: Guides the RM in capturing precise preference strength via a margin-aware optimization objective, allowing for continuous preference scores that reflect varying degrees of quality differences.\n  - **Generation Capability**: Regularizes the RM by incorporating the teacher\u2019s generative distribution to preserve its fundamental linguistic knowledge.\n\n### Methodology\n- **Initial Preference Pair Construction**: Begins with diverse candidate models and collects response pools. Contrastive refinement is used to construct highly correlated preference pairs for training.\n- **Contrastive Refinement**: Utilizes the teacher model's refinement capability to synthesize contrasting responses, ensuring nuanced quality distinctions are captured by the RM.\n- **Preference Modeling with Strength Guidance**: Incorporates margin-aware regression to guide the RM towards capturing precise preference strengths and using continuous scores instead of binary labels for better nuance.\n- **Generative Regularization**: Preserves the generative knowledge of the backbone model during distillation through regularization, ensuring robustness against catastrophic forgetting.\n\n### Key Findings\n- Extensive experiments demonstrate that RM-Distiller significantly outperforms traditional distillation methods both on reward model benchmarks and reinforcement learning-based alignment tasks.\n- The framework leverages the multifaceted capabilities of teacher LLMs to provide rich and reliable supervision signals, leading to improved performance without additional inference overhead.\n\n### Conclusions\n- **Systematic Research**: This is the first systematic investigation into RM distillation from generative LLMs, highlighting the critical role of exploiting multifaceted teacher capabilities.\n- **Practical Applications**: The proposed framework offers a practical solution for aligning large language models with human preferences in diverse domains without requiring extensive human annotation.\n- **Future Work**: Future research will explore distilling preference signals from human-LLM collaboration to further improve alignment efficacy.\n\n### References\n- [Ankner et al., 2024]\n- [Bai et al., 2022a]\n\nThis summary encapsulates the main contributions, methodology, key findings, and conclusions of the paper \"RM-Distiller: Exploiting Generative LLM for Reward Model Distillation\"."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Towards Visually Explaining Statistical Tests with Applications in Biomedical\nImaging\nMasoumeh Javanbakhat1 , Piotr Komorowski2 , Dilyara Bareeva2 , Wei-Chang Lai1 , Wojciech\nSamek2 , Christoph Lippert1\n1Digital Health-Machine Learning group, Hasso-Plattner Institute, Potsdam, Germany\n2Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, Berlin, Germany\n{Masoumeh.Javanbakhat, Wei-Cheng.Lai, Christoph.Lippert}@hpi.de,\n{piotr.komorowski, dilyara.bareeva, wojciech.samek}@hhi.fraunhofer.de\nAbstract\nDeep neural two-sample tests have recently shown\nstrong power for detecting distributional differ-\nences between groups, yet their black-box na-\nture limits interpretability and practical adoption\nin biomedical analysis. Moreover, most existing\npost-hoc explainability methods rely on class la-\nbels, making them unsuitable for label-free sta-\ntistical testing settings.\nWe propose an explain-\nable deep statistical testing framework that aug-\nments deep two-sample tests with sample-level and\nfeature-level explanations, revealing which individ-\nual samples and which input features drive statis-\ntically significant group differences. Our method\nhighlights which image regions and which individ-\nual samples contribute most to the detected group\ndifference, providing spatial and instance-wise in-\nsight into the test\u2019s decision. Applied to biomed-\nical imaging data, the proposed framework identi-\nfies influential samples and highlights anatomically\nmeaningful regions associated with disease-related\nvariation. This work bridges statistical inference\nand explainable AI, enabling interpretable, label-\nfree population analysis in medical imaging.\n1\nIntroduction\n1.1\nStatistical Tests\nTwo-sample tests are a fundamental tool in statistical infer-\nence for determining whether two sets of observations are\ndrawn from the same underlying distribution (the null hypoth-\nesis) or not (the alternative hypothesis). Two-sample tests can\nbe broadly divided into parametric and non-parametric meth-\nods. Parametric two-sample tests, such as the Student\u2019s t-test,\nmake strong assumptions about the distribution of the data\n(e.g., Gaussian). However, parametric tests may fail when\ntheir assumptions about the data distribution are invalid. Non-\nparametric tests, on the other hand, do not assume a specific\nparametric form for the underlying distribution and can thus\nbe potentially applied in a wider range of application scenar-\nios. Computing nonparametric test statistics, however, can\nbe costly as it may require applying resampling schemes or\ncomputing higher-order statistics.\nTo address this limitation, deep two-sample tests have\nbeen recently proposed, leveraging deep neural networks to\nlearn data-driven representations in which distributional dif-\nferences become more separable [Kirchler et al., 2020]. By\ncombining representation learning with classical hypothesis\ntesting, these methods achieve high statistical power and can\nbe applied to a wide range of high-dimensional non-tabular\ndata modalities.\nDespite their effectiveness (high power),\ndeep two-sample tests operate as black-box statistical proce-\ndures, providing no insight into which features or individual\nsamples drive the rejection of the null hypothesis\u2014a limi-\ntation that hinders their practical adoption in scientific and\nmedical applications.\n1.2\nPost-Hoc Explainability\nThe interpretability of black-box models has received grow-\ning attention in machine learning, leading to the develop-\nment of explainable artificial intelligence (XAI) [Adadi and\nBerrada, 2018]. Post-hoc explainability methods aim to in-\nterpret trained models by identifying which features or sam-\nples influence their outputs. Feature-level explainability fo-\ncuses on identifying input features or regions that are most\ninfluential for a model\u2019s prediction.\nTechniques such as\nInput\u00d7Gradient [Simonyan et al., 2014], Integrated Gradi-\nents [Sundararajan et al., 2017], Layer-wise Relevance Prop-\nagation [Bach et al., 2015], and perturbation-based meth-\nods [Lundberg and Lee, 2017] have become standard tools\nfor interpreting deep models. These approaches fundamen-\ntally rely on the existence of class labels or decision bound-\naries, as attributions are computed with respect to a spe-\ncific prediction target. Sample-level explainability, on the\nother hand, aims to identify influential training examples that\ncontribute to a model\u2019s output. Representative methods in-\nclude Deep k-Nearest Neighbors [Fix and Hodges, 1989;\nPapernot and McDaniel, 2018], influence functions [Cook\nand Weisberg, 1982; Koh and Liang, 2017], TraceIn [Pruthi\net al., 2020], and SimplEx [Crabbe et al., 2021]. These tech-\nniques have not been applied to statistical testing, where iden-\ntifying samples that drive test rejection remains unexplored.\n1.3\nMotivation\nDespite their success, existing post-hoc explainability meth-\nods are inherently designed to explain predictive models,\nwhere explanations are conditioned on a model output such\narXiv:2601.13899v1  [cs.CV]  20 Jan 2026\nas a class label or a loss value. Even recent extensions to\nunsupervised or self-supervised learning focus on explaining\nlearned representations or similarity relationships [Crabb\u00b4e\nand van der Schaar, 2022], rather than decisions arising from\nstatistical inference.\nIn the context of two-sample testing, interpretability raises\nfundamentally different questions. Rather than asking why a\nmodel predicts a certain label, one seeks to understand which\nfeatures or spatial regions are responsible for the detected\ndistributional difference between groups, and which samples\ndrive the rejection of the null hypothesis. Answering these\nquestions is critical in scientific and medical applications,\nwhere statistical significance alone is insufficient and insights\ninto underlying mechanisms are required. However, current\nXAI frameworks do not address these questions for deep sta-\ntistical tests, leaving a critical gap between powerful testing\nprocedures and interpretable population-level analysis.\nIn this work, we address this limitation by introduc-\ning an explainability framework that provides sample-level\nand feature-level insight into deep two-sample tests.\nBy\nturning statistically significant group differences into inter-\npretable, sample- and feature-level insights, our explainable\ndeep statistical testing framework enables actionable analysis\nin biomedical imaging. This supports downstream tasks such\nas patient stratification, quality control, clinical trial enrich-\nment, and biomarker discovery, by making population-level\ndifferences transparent, robust, and biologically meaningful.\n1.4\nRelated Works\nKernel-based hypothesis testing, particularly the Maximum\nMean Discrepancy (MMD), provides a principled and well-\nestablished framework for comparing distributions [Gretton\net al., 2012].\nRecent work has enhanced test power by\nlearning data-driven representations [Kirchler et al., 2020;\nLiu et al., 2020]. Prior work has explored incorporating inter-\npretability into test design. Jitkrittum et al. [2016] proposed\noptimizing locations in spatial or frequency domains to maxi-\nmize a lower bound on test power for a statistical test, reveal-\ning an interpretable indication of where two distributions dif-\nfer. Lopez-Paz and Oquab [2017] introduced classifier two-\nsample tests (C2ST), where a binary classifier is trained to\ndistinguish samples from two distributions, whereby the clas-\nsifier\u2019s learned decision function and predictive uncertainty\noffer interpretive insight into distributional differences. How-\never, these methods do not provide post-hoc explanations of\nexisting two-sample tests, nor do they yield fine-grained at-\ntribution maps explaining which specific input features con-\ntribute most strongly to the test statistic, and thus to the statis-\ntical difference between the two groups, for individual sam-\nples.\n1.5\nContribution\nThis work introduces a principled framework for explaining\ndeep two-sample statistical tests. Our main contributions are:\n(1) Problem formulation: explainability for deep statisti-\ncal tests. We formalize the problem of explaining deep two-\nsample tests by explicitly defining sample-level and feature-\nlevel contributions to population-level statistical significance.\nThis establishes interpretability as an integral component of\ndeep hypothesis testing, rather than a post-hoc analysis of\nmodel predictions. (2) Sample-level explainability of sta-\ntistical significance. We introduce a sample-level influence\nscore that measures how much each individual observation\ncontributes to the statistical significance of the difference be-\ntween two populations. By operating directly on the two-\nsample test statistic, the proposed score identifies samples\nthat amplify or attenuate the rejection of the null hypothesis\nin a fully label-free manner. (3) Feature-level explainabil-\nity of population differences. We derive a gradient-based\nfeature attribution method that explains population-level sta-\ntistical significance by backpropagating the two-sample test\nstatistic through a deep representation. This yields spatially\nresolved feature importance maps that identify which regions\nof complex inputs drive the detected distributional differences\nbetween populations. (4) Empirical validation in biomed-\nical imaging.\nWe demonstrate the practical utility of our\nframework on T1 structural MRI data and fundus images of\ndiabetic retinopathy, where the resulting explanations high-\nlight biologically meaningful regions and samples associated\nwith known disease markers, illustrating the interpretability\nof the proposed approach.\n2\nBackground\nIn this section, we provide a brief background on the two-\nsample tests used in this work.\n2.1\nDeep Two-Sample Test\nWe consider the non-parametric two-sample statistical test-\ning, that is, determining whether two sets of observations\nare drawn from the same underlying distribution (the null\nhypothesis, denoted by H0 : p = q) or the samples are\ndrawn from different distributions (the alternative hypothe-\nsis H1 : p \u0338= q) [Kirchler et al., 2020]. Following [Kirchler et\nal., 2020], given two sets of observations Xn = {xi}n\ni=1 \u223cp\nand Ym = {yj}m\nj=1 \u223cq, a deep two-sample test first maps\nsamples into a learned representation space using a neural\nnetwork \u03d5\u03b8 : X \u2192RH. The test then performs a multi-\nvariate location test on whether both means map to the same\nlocation. If the distance between the two means is too large,\nwe reject the hypothesis that both samples are drawn from the\nsame distribution. Specifically, the test computes a test statis-\ntic based on the difference between the empirical means of\nthe two populations in the representation space. The result-\ning test statistic, named Deep Maximum Mean Discrepancy\n(DMMD), takes the form\nS(Xn, Ym) =\nnm\nn + m \u2225\u00b5X \u2212\u00b5Y \u22252 .\n(1)\nWhere\n\u00b5X = 1\nn\nn\nX\ni=1\n\u03d5\u03b8(xi),\n\u00b5Y = 1\nm\nm\nX\nj=1\n\u03d5\u03b8(yj)\nindicate the mean of embeddings on Xn, and Ym respectively.\nGiven the test statistic S(Xn, Ym), statistical significance is\nassessed under the null hypothesis H0. As shown in [Kirch-\nler et al., 2020], the null distribution of S(Xn, Ym) can be\napproximated by a weighted sum of independent \u03c72 random\nvariables. While this permits analytic p-value computation,\nevaluating the resulting distribution can be computationally\nexpensive in practice. Instead, we estimate the null distri-\nbution using a Monte Carlo permutation test [Ernst, 2004].\nSpecifically, the learned representations are computed once\nfor all samples, after which group labels are randomly per-\nmuted to generate empirical realizations of the test statistic\nunder the null hypothesis. The p-value is then computed as\nthe proportion of permuted test statistics exceeding the ob-\nserved value. This resampling-based procedure is computa-\ntionally efficient and is used throughout this work. This ap-\nproach retains the power of deep representations while con-\ntrolling the type-I error and achieving asymptotic consistency\nas sample size increases [Kirchler et al., 2020].\n3\nMethodology\nIn this section, we present our approach for sample-level and\nfeature-level explainability. Given two populations that are\nstatistically different, our objective is to identify which spe-\ncific samples or features contribute most to the observed sig-\nnificant differences between the groups.\n3.1\nSample-Level Explainability\nWe introduce a sample-level explainability framework that\nquantifies how each individual sample contributes to the sta-\ntistical difference between two populations.\nDefinition 3.1. The influence score of a sample xi, denoted\nas IF(xi), is defined as the change in the test statistic when\nthis sample is removed from Xn:\nIF(xi) = S(Xn, Ym) \u2212S(Xn\\i, Ym),\n(2)\nwhere Xn\\i = Xn\\{xi} denotes the dataset with xi excluded.\nSimilarly, for a sample yj from Ym, the influence score is\ndefined as:\nIF(yj) = S(Xn, Ym) \u2212S(Xn, Ym\\j).\n(3)\nIntuitively, if IF(xi) > 0, the test statistic decreases when\nremoving xi, indicating that xi amplifies the difference be-\ntween groups\u2014i.e., it contributes to the rejection of the null\nhypothesis and represents a distinctive sample. Conversely,\nif IF(xi) < 0, removing xi increases the similarity between\ngroups, suggesting that it suppresses the detected difference.\nSamples with large positive influence scores are thus impor-\ntant for the statistical significance of the group difference,\nwhile those with large negative scores indicate contribute to\nsimilarity of two groups, indicating outliers or bridge samples\nbetween the two distributions.\n3.2\nFeature-Level Explainability\nWhile sample-level explainability identifies which individ-\nual observations contribute most to statistical significance,\nfeature-level explainability aims to localize where in each\ninput the population-level differences arise. Our goal is to\nidentify spatial regions whose representations contribute most\nstrongly to the two-sample test statistic. We now describe\nour feature-level explainability framework for identifying the\nspatial regions that drive statistical differences between two\npopulations.\nFigure 1: Feature-level explanation of population-level statistical\nsignificance. Two populations are encoded to embeddings to com-\npute a two-sample statistic S, whose gradients are backpropagated\nto feature maps Ak and aggregated via weights \u03b1k to produce a spa-\ntial attribution map, which is then upsampled and overlaid on the\ninput for visualization.\nDefinition 3.2. Let S(Xn, Ym) be the deep two-sample test\nstatistic and let \u03d5\u03b8 be the pretrained feature extractor used to\ncompute the latent representations. For an input sample x, we\ndefine its feature-level statistical importance as the sensitivity\nof the test statistic with respect to spatially localized feature\nactivations.\nSpecifically, for two given populations Xn and Ym we\ncompute the the test statistic S(Xn, Ym). Then for each sam-\nple x we backpropagate S through the encoder to the last\nconvolutional feature maps A \u2208RC\u00d7h\u00d7w to assess the con-\ntribution of the internal features to the observed group-level\ndifference.\nIn particular, let Ak \u2208Rh\u00d7w denote the kth feature map\nof the final convolutional layer. The gradient\n\u2202S\n\u2202Apq\nk\ncaptures\nthe sensitivity of the test statistic to activations at spatial loca-\ntion pq within channel k. These gradients thus describe how\nvariations in local image features influence the group discrep-\nancy.\nTo aggregate these local sensitivities into a spatial impor-\ntance map, we compute channel importance weights \u03b1k, by\ncomputing the global average pooling (GAP):\n\u03b1k = GAP\n\u0012 \u2202S\n\u2202Ak\n\u0013\n=\n1\nhw\nh\nX\np=1\nw\nX\nq=1\n\u2202S\n\u2202Apq\nk\n.\nThe final attribution map is obtained by a weighted linear\ncombination of the feature maps,\nM(x) = ReLU\nC\nX\nk=1\n\u03b1kAk(x)\n!\n,\nwhich highlights the spatial regions of the input that most\nstrongly contribute to the statistical difference between the\ntwo groups. This construction yields a feature-level explana-\ntion grounded in the sensitivity of the inter-group discrepancy\nto localized representation patterns. Our method is illustrated\nin Figure 1.\nWhile inspired by gradient-based attribution\nmethods such as Grad-CAM [Selvaraju et al., 2017]; our ap-\nproach differs fundamentally in that it explains a population-\nlevel statistical significance rather than the prediction of an\nindividual classifier.\n(a) High-Influence Removal Effect\n(b) Distribution of Influence Scores\n(c) CDF of Infleuence Scores\nFigure 2: Sample-level explainability results on dSprites.\n(a) High-Influence Removal Effect\n(b) Distribution of Influence Scores\n(c) CDF of Influence Scores\nFigure 3: Sample-level explainability results on Diabetic Retinopathy.\n4\nExperiments\nIn this section, we conduct qualitative and quantitative evalu-\nations of the proposed sample-level and feature-level explain-\nability methods for the deep two-sample test. We first per-\nform controlled experiments on synthetic and simulated data\nto verify that the proposed explanations are consistent with\nknown distributional differences. We then demonstrate the\npractical utility of both sample- and feature-level explana-\ntions on real biomedical imaging datasets, including struc-\ntural MRI and diabetic retinopathy fundus images, where\nthe resulting explanations provide insight into disease-related\npopulation differences. Finally, we illustrate additional appli-\ncations of the proposed pipeline.\n4.1\nSetup\nWe evaluate the proposed explainability framework on three\ndatasets with corresponding feature extractors. Specifically,\nwe consider one synthetic dataset, dSprites [Matthey et al.,\n2017], and two biomedical imaging datasets: T1-weighted\nstructural MRI data from the Alzheimer\u2019s Disease Neu-\nroimaging Initiative (ADNI) [Mueller et al., 2005] and reti-\nnal fundus images from the EyePACS Diabetic Retinopathy\nDetection dataset [Kaggle, 2015]. For the dSprites dataset,\nwe train a variational autoencoder (VAE) on the heart-shaped\nsubset and use the encoder to obtain latent representations.\nThis yields 10-dimensional latent representations. For T1-\nweighted structural MRI, we use a ResNet-50 encoder trained\nin a supervised manner on the UK Biobank dataset to pre-\ndict chronological age; the pretrained encoder is then used\nto extract embeddings for the ADNI subjects. For the dia-\nbetic retinopathy experiments, we employ a self-supervised\nSimCLR model [Chen et al., 2020] with a ResNet-50 back-\nbone trained using contrastive learning on the APTOS dataset\n[Asia Pacific Tele-Ophthalmology Society (APTOS), 2019].\nThe resulting encoder is used to compute embeddings for reti-\nnal fundus images in the EyePACS dataset. The ResNet-50\nbackbone produces 2048-dimensional representations. None\nof the encoders is fine-tuned on the test datasets used for two-\nsample testing. To assess statistical significance between the\ntwo groups, we follow the procedure described in Section 2.1,\nwith additional details provided in Supplementary Section 1.\n4.2\nSample-Level Explainability\nWe evaluate our method in two complementary settings.\nFirst, we perform controlled experiments on the synthetic\ndSprites dataset, which provides known ground-truth factors\nof variation. This setting serves as a sanity check to validate\nthe expected behavior of influence scores under controlled\nconditions.\nSecond, we consider a real-world biomedical\nimaging setting using retinal fundus images from the Eye-\nPACS Diabetic Retinopathy dataset, where group separation\narises from varying disease severity. This setting assesses\nwhether influence scores identify samples and features asso-\nciated with clinically meaningful patterns.\ndSprites\nWe consider two groups from the dSprites dataset, each group\ncontaining 200 samples. The first group X contains images\nHigh-Influence Samples\nLow-Influence Samples\nFigure 4: Qualitative examples illustrating samples with high (left) and low (right) influence scores. High-influence samples exhibit clear and\ndistinctive visual patterns that strongly contribute to group differences, while low-influence samples appear more ambiguous or borderline.\nwith square shapes only, while the second, Y contains images\nwith both squares (#40) and ellipses (#160). Ellipses are the\nsole factor inducing group separation, whereas squares are\nidentical across both groups and encode similarity. We use\nthe encoder of trained VAE on the heart class for getting em-\nbeddings in each group and compute DMMD S(X, Y) based\non Equation 1. Consequently, we compute IF(x) for each\nsample x, following Definition 3.1.\nEvaluation Procedure.\nTo evaluate whether our method\ncorrectly identifies the important samples for the statistical\ndifference between two groups, we monitor the p-value. If\nthe proposed influence score correctly identifies the samples\nthat contribute positively to the distinction between the two\ngroups, excluding these samples should result in an increase\nin the p-value (i.e., by excluding them, the two groups should\nbecome more similar). To assess this, we rank the samples by\ninfluence scores IF(x) and progressively remove the high-\nest ones to analyze their impact on the p-value. We then re-\ncompute the test statistic and p-value and plot the results as a\nfunction of the proportion of removed samples.\nResults.\nThe results of the simulated anaylsis on the\ndSprites dataset are summarized in Figure 2. First, we mon-\nitor the p-value as samples with the highest IF scores are\nprogressively removed. As shown in Figure 2 (a), the p-value\nincreases as more high-influence samples are excluded, in-\ndicating that these samples are responsible for amplifying the\nstatistical difference between the two groups. To further char-\nacterize the distribution of influence scores, we report both\nbox plots and cumulative distribution functions (CDFs) for\nsamples in each group. The box plots in Figure 2 (b) indi-\ncate that ellipse samples have a median influence score above\nzero, demonstrating their positive contribution to group sep-\naration, whereas square samples in Group 2 exhibit median\ninfluence scores below zero, consistent with their role in sim-\nilarity between groups. This behavior is further corroborated\nby the CDFs in Figure 2 (c), where the influence-score distri-\nbution for ellipses is right-shifted, while the distribution for\nsquare samples in Group 2 is left-shifted. Together, these\nresults confirm that the proposed influence scores correctly\nidentify samples that drive statistical differences as well as\nsamples that promote similarity between the two groups.\nDiabetic Retinopathy\nWe consider two groups of retina images, healthy and un-\nhealthy images, each containing 200 samples. The healthy\ngroup consists of retina images with no signs of diabetic\nretinopathy. The unhealthy group includes images with vary-\ning severity levels of diabetic retinopathy, ranging from 1 to\n4. To ensure a representative distribution of severity levels\nwithin the unhealthy group, we use stratified sampling, pre-\nserving the original proportions of each severity level. We use\nthe encoder of SimCLR model pre-trained on APTOS dataset,\nto get embeddings in each group and compute the DMMD\ntest statistic and influence score IF(x) for each sample x\nfollowing Definition 3.1.\nQuantitative Results.\nFigure 3 summarizes quantitative re-\nsults for diabetic retinopathy dataset. As shown in Figure 3\n(a), similar to dSprites dataset, the p-value increases as sam-\nples with high influence scores are progressively removed. In\ncontrast, excluding samples with low influence scores leads\nto a decrease in the p-value (see Supplementary Figure 1),\nconfirming that these samples contribute to the similarity be-\ntween the two groups rather than their separation. We fur-\nther analyze the distribution of influence scores across dis-\nease severity levels in Figure 3 (b). The influence scores tend\nto increase with severity, suggesting that more severe cases\ncontribute more strongly to group differences. Moreover, this\ntrend is monotonic, confirming that our method captures clin-\nically meaningful variation. The plot also reveals the pres-\nence of outliers within each severity group, which correspond\nto samples whose appearance deviates from their clinical la-\nbels, potentially due to early-stage disease, atypical presenta-\ntions, or image-quality variability. The CDF plot in Figure\n3 further supports these findings. Healthy images and se-\nvere disease cases exhibit positive median influence scores,\nwhereas mild disease cases often show negative influence\nscores, reflecting their visual similarity to healthy images.\n(0)\n(1)\n(2)\n(3)\nFigure 5: Representative outliers identified by influence scores for\ndiabetic retinopathy severity levels 0\u20133. No outliers were observed\nfor severity level 4 under the chosen criterion.\nQualitative Results.\nFigure 4 presents qualitative exam-\nples of high- and low-influence samples.\nHigh-influence\nimages exhibit strong, unambiguous visual patterns that\nclearly distinguish healthy from unhealthy cases\u2014such as\npronounced lesions, clear pathological structures, or very typ-\nical healthy appearance. While low-influence samples appear\nambiguous, borderline, or visually similar to the opposite\ngroup. Together, these examples show that influence scores\nalign well with visual interpretability and capture clinically\nmeaningful variability. Figure 5 further visualizes represen-\ntative outliers from each group. These samples exhibit atypi-\ncal appearance, borderline severity, or reduced image quality,\nhighlighting that influence scores surface interpretable edge\ncases and heterogeneity rather than arbitrary noise.\n4.3\nFeature-Level Explainability\nWe evaluate feature-level explanations in two complementary\nsettings that reflect different levels of supervision. First, we\nconsider the synthetic dSprites dataset, where ground-truth\ngenerative factors are known. This setting allows us to di-\nrectly assess whether the proposed feature-level explanations\ncorrectly identify the latent factors responsible for group dif-\nferences. Second, we evaluate feature-level explanations on\nreal neuroimaging from the ADNI dataset, where ground-\ntruth feature importance is unavailable. In this case, we assess\nwhether the identified features align with established clinical\nand neuroanatomical knowledge, thereby evaluating the clin-\nical plausibility of the explanations.\ndSprites\nWe consider two groups from the dSprites dataset, each group\ncontaining 78208 samples. The first group contains images\nwith square shapes, while the second contains images with\nelliptical shapes. We use the encoder of pretrained VAE on\nthe heart class to extract latent embeddings for each group\nand compute the DMMD statistic S(X, Y) based on Equa-\ntion 1. We then apply the proposed feature-level explanation\nmethod (Section 3.2) to obtain attribution heatmaps for indi-\nvidual samples. For dSprites, we generate explanations by\nbackpropagating the test statistic to the penultimate convolu-\ntional layer rather than the final one, as the final layer has in-\nsufficient spatial resolution to produce meaningful heatmaps.\nThis choice follows common practice in gradient-based attri-\nbution methods, which select the deepest layer with sufficient\nspatial resolution. For visualization, attribution maps are up-\nsampled to the input resolution using bilinear interpolation\nand overlaid on the original images.\nSquare\nEllipse\nFigure 6: Feature-level explanations on dSprites.\nResults.\nFigure 6 presents representative qualitative results.\nThe resulting heatmaps correctly highlight shape-specific re-\ngions (corner vs curvature), indicating that the proposed\nmethod accurately identifies the generative factors responsi-\nble for the observed group differences.\nADNI\nWe consider two groups constructed from the ADNI dataset\nbased on hippocampal volume, a well-established neuroimag-\ning biomarker of Alzheimer\u2019s disease\u2013related neurodegener-\nation. Each group includes 918 2D slices. Subjects exhibit-\ning reduced hippocampal volume indicative of hippocampal\natrophy are assigned to Group 1, while subjects with pre-\nserved hippocampal volume form Group 2. This stratification\ninduces group differences associated with disease-relevant\nanatomical variation and provides a clinically meaningful set-\nting for evaluating whether the proposed feature-level expla-\nnations highlight brain regions known to be implicated in\nAlzheimer\u2019s disease/cognitive impairment.\nWe use a ResNet-50 encoder pretrained on the UK Biobank\ndataset, predicting the age of brains, and adapt this pretrained\nmodel using a linear probing strategy on the ADNI dataset.\nSpecifically, we train a logistic regression layer on top of the\nfrozen encoder to predict group membership, while keeping\nall encoder parameters fixed. After training, the linear head\nis discarded, and the frozen encoder is used to extract embed-\ndings for two-sample testing and for generating feature-level\nexplanations. We apply the proposed feature-level explana-\ntion method to individual MRI scans and assess whether the\nresulting attributions align with known disease-related neu-\nroanatomical patterns.\nTo assess the robustness of our feature-level explanations,\nwe apply multiple channel-aggregation strategies to the gra-\ndients of the test statistic: (i) gradient-weighted aggregation\n[Selvaraju et al., 2017], (ii) second-order gradient aggrega-\ntion [Chattopadhay et al., 2018], and (iii) layer-wise spatial\naggregation [Jiang et al., 2021]. Across all variants, the re-\nsulting heatmaps remain consistent. Additional details are\nprovided in the Supplementary Material.\nResults.\nFigure 7 indicates the qualitative results. Our ex-\nplainability method consistently highlights the hippocampal\nand medial temporal lobe regions, and the resulting heatmaps\nshow strong correspondence with areas affected by hip-\npocampal atrophy. These results supports that the identified\nregions align with established AD biomarkers.\nOriginal\nGradient-weighted\nSecond-order\nLayer-wise\nGroup 1\nGroup 2\nFigure 7: Feature-level explanations on ADNI. Top row: representative subject from Group 1. Bottom row: representative subject from\nGroup 2. Heatmaps are obtained by backpropagating the two-sample test statistic S to the final convolutional layer. Gradient-weighted,\nSecond-order, and Layer-wise differ only in aggregation strategy but explain the same statistical objective.\n5\nDiscussion\nWe introduced an explainable deep statistical framework for\nidentifying which samples and which features drive statisti-\ncally significant group differences. We validated the frame-\nwork on both synthetic data, where ground-truth generative\nfactors are known, and on real biomedical data, where the\nresulting explanations align with established disease-related\npatterns. The proposed framework provides complementary\ninsights at the sample level and the feature level, each of\nwhich supports distinct but related biomedical use cases.\n5.1\nUse Cases\nSample-Level Explainability.\n1) Patient Stratification:\nBy identifying which samples drive the statistical separa-\ntion between groups, our method enables the identification\nof patients with strong vs. weak disease signatures. This\nallows us to quantify heterogeneity within clinical cohorts\nand stratify patients based on biologically meaningful imag-\ning patterns, which is particularly valuable in heterogeneous\ndiseases. For example, in ADNI, clinically defined groups\nsuch as cognitively normal (CN), mild cognitive impairment\n(MCI), and Alzheimer\u2019s disease (AD) are available. Using\nsample-level explainability, we can identify which individual\nsubjects most strongly drive the separation between groups,\ne.g., in CN vs. MCI. High-influence MCI subjects exhibit\nimaging patterns that are more distinct from CN, potentially\nreflecting more aggressive or advanced disease processes. In\ncontrast, low-influence MCI subjects appear more similar to\nCN, suggesting milder or earlier-stage pathology. By com-\nparing the imaging features highlighted in high- vs.\nlow-\ninfluence MCI subjects, we gain insights into heterogeneity\nwithin the MCI population and potential trajectories of dis-\nease progression.\nThe same framework can be applied to\nMCI vs. AD to identify individuals transitioning toward more\nadvanced pathology and to characterize imaging features as-\nsociated with early progression. 2) Trial Enrichment: In\na clinical trial setting, our sample-level explainability allows\nus to identify patients who contribute most strongly to the\nstatistical separation between a disease group and a refer-\nence population. These high-influence patients typically ex-\nhibit clearer and more consistent pathological imaging pat-\nterns. By identifying patients who most strongly express the\ntarget pathology, we enrich trials with subjects more likely\nto show treatment effects. This reduces noise by excluding\nlow-informative or ambiguous cases identified by low influ-\nence scores. This can improve statistical power and reduce\nthe required sample size. 3) Quality Control: The method\nnaturally supports quality control by flagging outlier samples\nwith unexpected influence scores, detecting mislabeled, low-\nquality, or artifact-driven images, and supporting dataset cu-\nration prior to downstream modeling or clinical trials.\nFeature-Level Explainability.\nAt the feature level, the\nmethod highlights spatial patterns that drive statistically sig-\nnificant group differences, helping identify candidate imag-\ning biomarkers or disease-related patterns that are both dis-\ncriminative and interpretable. These explanations can sup-\nport biomarker discovery by identifying candidate regions\nor structures associated with disease-related variation. The\nframework also enables hypothesis-driven analyses: by defin-\ning groups based on clinically or biologically meaningful\nvariables, researchers can use the resulting explanations to\nexplore specific mechanisms, such as progression effects, sex\ndifferences, or subtype-related patterns. This supports hy-\npothesis generation and provides interpretable insights that\ncan guide downstream experimental validation.\nOverall, the proposed method turns statistical significance\ninto actionable insight, supporting patient stratification, qual-\nity control, trial enrichment, and biomarker discovery by\nmaking group differences explainable and biologically mean-\ningful. The code is provided as a ZIP archive in the Supple-\nmentary Material.\nReferences\n[Adadi and Berrada, 2018] Amina Adadi and Mohammed\nBerrada.\nPeeking inside the black-box: A survey on\nexplainable artificial intelligence (xai).\nIEEE Access,\n6:52138\u201352160, 2018.\n[Asia Pacific Tele-Ophthalmology Society (APTOS), 2019]\nAsia\nPacific\nTele-Ophthalmology\nSociety\n(APTOS).\nAptos\n2019\nblindness\ndetection\ndataset.\nhttps:\n//www.kaggle.com/c/aptos2019-blindness-detection,\n2019.\n[Bach et al., 2015] Sebastian\nBach,\nAlexander\nBinder,\nGr\u00b4egoire Montavon, Frederick Klauschen, Klaus-Robert\nM\u00a8uller, and Wojciech Samek. On pixel-wise explanations\nfor non-linear classifier decisions by layer-wise relevance\npropagation."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThis paper introduces a principled framework for explaining deep two-sample statistical tests in biomedical imaging. The main contributions are:\n1. **Problem Formulation**: A formalization of explainability for deep statistical tests, explicitly defining sample-level and feature-level contributions to population-level statistical significance.\n2. **Sample-Level Explainability**: Introducing an influence score that quantifies how each individual sample contributes to the observed significant differences between two populations.\n3. **Feature-Level Explainability**: Deriving a gradient-based feature attribution method that explains population-level statistical significance by backpropagating the test statistic through a deep representation, yielding spatially resolved feature importance maps.\n\n#### Methodology\nThe proposed framework combines deep two-sample tests with sample-level and feature-level explanations:\n1. **Sample-Level Explainability**: Identifies which specific samples or features contribute most to the observed significant differences between groups by measuring how much each individual observation contributes to the statistical significance of the difference.\n2. **Feature-Level Explainability**: Locates where in each input the population-level differences arise, identifying spatial regions whose representations strongly contribute to the two-sample test statistic.\n\n#### Key Findings\nThe authors validate their framework on synthetic and real biomedical imaging datasets:\n1. **Synthetic dSprites Dataset**: The influence scores correctly identify samples associated with clinically meaningful patterns.\n2. **Diabetic Retinopathy Dataset**: Influence scores highlight samples and features associated with severe disease severity, revealing outliers within each severity group.\n\n#### Conclusions\nThe proposed framework bridges statistical inference and explainable AI by providing actionable insights for biomedical applications such as patient stratification, trial enrichment, quality control, and biomarker discovery. The method enables the identification of influential samples and highlights anatomically meaningful regions associated with disease-related variation in biomedical imaging data.\n\n### Summary\n\nThis paper introduces a novel approach to explaining deep two-sample tests used in statistical inference within biomedical imaging. By formalizing sample-level and feature-level contributions to population-level statistical significance, the authors provide a framework that bridges powerful testing procedures with interpretability. The proposed method leverages deep learning techniques to offer actionable insights into which samples and features drive significant group differences, thereby enabling more meaningful analysis in fields such as patient stratification, trial enrichment, quality control, and biomarker discovery."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Locate, Steer, and Improve: A Practical\nSurvey of Actionable Mechanistic\nInterpretability in Large Language Models\nHengyuan Zhang1,\nZhihao Zhang2,\nMingyang Wang3 Zunhai Su4 Yiwei Wang5\nQianli Wang6 Shuzhou Yuan7 Ercong Nie3 Xufeng Duan8 Qibo Xue9 Zeping Yu10\nChenming Shang11 Xiao Liang12 Jing Xiong1 Hui Shen13 Chaofan Tao1 Zhengwu Liu1\nSenjie Jin2 Zhiheng Xi2 Dongdong Zhang14 Sophia Ananiadou10 Tao Gui2 Ruobing Xie15\nHayden Kwok-Hay So1 Hinrich Sch\u00fctze3 Xuanjing Huang2 Qi Zhang2,\nNgai Wong1,\n1The University of Hong Kong\n2Fudan University\n3LMU Munich\n4Tsinghua University\n5Technische Universit\u00e4t Darmstadt\n6Technische Universit\u00e4t Berlin\n7Technische Universit\u00e4t Dresden\n8The Chinese University of Hong Kong\n9Nanjing University\n10University of Manchester\n11Dartmouth College\n12University of California, Los Angeles\n13University of Michigan\n14Microsoft\n15Tencent\nAbstract: Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque\ndecision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as\nan observational science, summarizing analytical insights while lacking a systematic framework for\nactionable intervention. To bridge this gap, we present a practical survey structured around the pipeline:\n\u201cLocate, Steer, and Improve.\u201d We formally categorize Localizing (diagnosis) and Steering (intervention)\nmethods based on specific Interpretable Objects to establish a rigorous intervention protocol. Further-\nmore, we demonstrate how this framework enables tangible improvements in Alignment, Capability,\nand Efficiency, effectively operationalizing MI as an actionable methodology for model optimization.\nWith actionable mechanistic interpretability evolving at a fast pace, we pledge to keep this survey up to\ndate, ensuring it reflects the cutting-edge advances in this area.\nEqual Contribution\nCorresponding Author\nKeywords: Actionable Interpretability, Large Language Models, Localizing and Steering, Model\nImprovement\nDate: January 20, 2026\nGithub Repo: https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey\nContact:\nhengyuan.zhang88@gmail.com zhangzhihao19@fudan.edu.cn qz@fudan.edu.cn\nnwong@eee.hku.hk\narXiv:2601.14004v1  [cs.CL]  20 Jan 2026\nContents\n1\nIntroduction\n5\n2\nCore Interpretable Objects of LLMs\n7\n2.1\nToken Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nTransformer Block and Residual Stream\n. . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3\nMulti-Head Attention (MHA)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.4\nFeed-Forward Network (FFN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.5\nSparse Autoencoder (SAE) Feature\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3\nLocalizing Methods\n12\n3.1\nMagnitude Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.2\nCausal Attribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.3\nGradient Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.4\nProbing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.5\nVocabulary Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.6\nCircuit Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4\nSteering Methods\n25\n4.1\nAmplitude Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4.2\nTargeted Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.3\nVector Arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5\nApplications\n31\n5.1\nImprove Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.1.1\nSafety and Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.1.2\nFairness and Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n5.1.3\nPersona and Role . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.2\nImprove Capability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.2.1\nMultilingualism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.2.2\nKnowledge Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.2.3\nLogic and Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.3\nImprove Efficiency\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n2\n5.3.1\nEfficient Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5.3.2\nEfficient Inference\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n6\nChallenges and Future Directions\n46\n7\nConclusion\n48\nA\nSummary of Surveyed Papers\n49\n3\nPaper Outline\nLocate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models\nCore Inter-\npretable Objects\nof LLMs (\u00a72)\nToken Embedding (\u00a72.1); Transformer Block and Residual Stream (\u00a72.2); Multi-Head Attention (MHA) (\u00a72.3); Feed-Forward Network (FFN)\n(\u00a72.4); Sparse AutoEncoder (SAE) Feature (\u00a72.5)\nLocalizing\nMethods (\u00a73)\nMagnitude\nAnalysis (\u00a73.1)\nDettmers et al. (2022); Tang et al. (2024); Galichin et al. (2025); Zhang et al. (2025a); An et al. (2025); Su and Yuan (2025); Huben et al. (2024); Su et al.\n(2025c); Jin et al. (2025a); Bi et al. (2025); Chuang et al. (2024); Zhang et al. (2024c); Elhoushi et al. (2024); Men et al. (2025); Xiao et al. (2023a); Ashkboos\net al. (2024); Yu et al. (2024); Cai et al. (2024b); Xiong et al. (2025b,a); He et al. (2024a); Su et al. (2025b); Yuan et al. (2025a); Lai et al. (2024); Liu et al.\n(2024a); Chen et al. (2024a, 2025c); Wang et al. (2025e); Andrylie et al. (2025); Gurgurov et al. (2025a); Xiao et al. (2023b); Cancedda (2024); Singh et al.\n(2024); Wang et al. (2024b); Zhou et al. (2025b); Sergeev and Kotelnikov (2025); Sun et al. (2025a); Bas and Novak (2025); Dumitru et al. (2024); Tan et al.\n(2024c); Lawson and Aitchison (2025)\nCausal Attri-\nbution (\u00a73.2)\nVig et al. (2020); Meng et al. (2022); Zhang and Nanda (2023); Stolfo et al. (2023); Yu and Ananiadou (2024c); Geiger et al. (2025); Ferreira et al. (2025); Yeo\net al. (2025b); Ravindran (2025); Yu et al. (2025a); Wang et al. (2023b); Geva et al. (2023); Tang et al. (2024); Yu and Ananiadou (2024a)\nGradient De-\ntection (\u00a73.3)\nLi et al. (2016); Sundararajan et al. (2017); Smilkov et al. (2017); Shrikumar et al. (2017); Enguehard (2023); Wu et al. (2023); Hou and Casta\u00f1\u00f3n (2023);\nTao et al. (2025); Nguyen et al. (2025a); Dai et al. (2022); Shi et al. (2024); Zhang et al. (2024b,a); Li et al. (2025f,e); Jafari et al. (2025); Azarkhalili and\nLibbrecht (2025); Liu et al. (2025b); Li et al. (2025c); Zhang et al. (2024e); Li et al. (2025b); Wang et al. (2024c, 2022); Zhang and Nanda (2023); Yin and\nNeubig (2022)\nProbing (\u00a73.4)\nAlain and Bengio (2017); Belinkov (2022); Conneau et al. (2018); Tenney et al. (2019); Ravichander et al. (2021); Ju et al. (2024); Zhao et al. (2024d); Zhang\net al. (2024c); Orgad et al. (2025); You et al. (2025); Du et al. (2024); Zhao et al. (2025b); Kim et al. (2025b); Kantamneni et al. (2025); Chanin et al. (2024)\nVocabulary Pro-\njection (\u00a73.5)\nnostalgebraist (2020); Geva et al. (2021); Belrose et al. (2023); Wang et al. (2023b); Jiang et al. (2024a, 2025a); Wendler et al. (2024); Kargaran et al. (2025);\nPhukan et al. (2024, 2025); Yugeswardeenoo et al. (2025); Sakarvadia et al. (2023); Yu and Ananiadou (2024b); Jiang et al. (2025b); Kim et al. (2025a); Wang\n(2025); Huo et al. (2024); Yu and Ananiadou (2025); Shao et al. (2025); Arad et al. (2025); Dreyer et al. (2025); Muhamed et al. (2025b); Gur-Arieh et al.\n(2025); Shu et al. (2025)\nCircuit Discovery (\u00a73.6)\nElhage et al. (2021); Olsson et al. (2022); Hanna et al. (2023); Yao et al. (2024a); Goldowsky-Dill et al. (2023); Conmy et al. (2023); Syed et al. (2024); Hanna\net al. (2024); Huang et al. (2025a); Sundararajan et al. (2017); Stolfo et al. (2023); Wang et al. (2023b); Haklay et al. (2025); Mueller et al. (2025); Bricken\net al. (2023); Ameisen et al. (2025); Hanna et al. (2025)\nSteering\nMethods (\u00a74)\nAmplitude Ma-\nnipulation (\u00a74.1)\nTang et al. (2024); Nie et al. (2025); Goyal et al. (2025); Yeo et al. (2025a); Liu et al. (2024b); Chandna et al. (2025); Huang et al. (2025a); Liu et al. (2024a);\nMen et al. (2025); Zhou et al. (2025a); Niu et al. (2025); Ahsan et al. (2025); Raimondi et al. (2025); Gao et al. (2025a); Pach et al. (2025); Galichin et al.\n(2025); Stoehr et al. (2024); Liu et al. (2025b); Yao et al. (2025); Wang et al. (2025a)\nTargeted Opti-\nmization (\u00a74.2)\nMeng et al. (2022, 2023); Zhong et al. (2024); Zhang et al. (2023, 2024b); Xu et al. (2025a); Xi et al. (2022); Zhou et al. (2024); Zhang et al. (2024e, 2025e);\nLi et al. (2025g); Du et al. (2024); Li et al. (2024b); Zhang et al. (2024d); Zhu et al. (2024); Tang et al. (2025a); Hooper et al. (2024); Bondarenko et al.\n(2023); Chen et al. (2025e); Xia et al. (2025); Tan et al. (2025); Lee et al. (2024)\nVector Arith-\nmetic (\u00a74.3)\nRimsky et al. (2024); van der Weij et al. (2024); Lu and Rimsky (2024); Postmus and Abreu (2024); Turner et al. (2024); Sharma and Raman (2025); Wang\net al. (2025b); Bayat et al. (2025); Weng et al. (2025); He et al. (2025); Soo et al. (2025); Goyal et al. (2025); Ilharco et al. (2023); Yadav et al. (2023a); Liu\net al. (2025b); Yao et al. (2025); Shu et al. (2025)\nApplications (\u00a75)\nImprove Align-\nment (\u00a75.1)\nSafety and Reliability (\u00a75.1.1)\nHuang et al. (2025a); Zhou et al. (2025a); Chen et al. (2025b); Suau et al. (2024); Gao et al. (2025a); Tem-\npleton et al. (2024); Goyal et al. (2025); Yeo et al. (2025a); Zhao et al. (2025d); Li et al. (2024b, 2025g);\nLee et al. (2024); Arditi et al. (2024); Zhao et al. (2025c); Yin et al. (2025); Ball et al. (2024); Wang et al.\n(2025f,g); Ferreira et al. (2025); Huang et al. (2025b); Chuang et al. (2024); Chen et al. (2024b); Zhang\net al. (2024c); Orgad et al. (2025); He et al. (2025); Stolfo et al. (2025); Jiang et al. (2024c); Li et al.\n(2025d)\nFairness and Bias (\u00a75.1.2)\nVig et al. (2020); Cai et al. (2024a); Ahsan et al. (2025); Chandna et al. (2025); Yu et al. (2025a); Kim et al.\n(2025b); Liu et al. (2024b); Guan et al. (2025); Chintam et al. (2023); Yu and Ananiadou (2025)\nPersona and Role (\u00a75.1.3)\nRimsky et al. (2024); Potert\u00ec et al. (2025); Chen et al. (2025d); Handa et al. (2025); Su et al. (2025a);\nDeng et al. (2025); Chen et al. (2024c); Tak et al. (2025); Yuan et al. (2025b); Ju et al. (2025); Karny et al.\n(2025); Banayeeanzade et al. (2025); Bas and Novak (2025); Lai et al. (2024)\nImprove Ca-\npability (\u00a75.2)\nMultilingualism (\u00a75.2.1)\nZhao et al. (2024c); Gurgurov et al. (2025a); Tang et al. (2024); Liu et al. (2025c); Jing et al. (2025);\nAndrylie et al. (2025); Brinkmann et al. (2025); Wendler et al. (2024); Philippy et al. (2023); Mousi\net al. (2024); Chi et al. (2023); Hinck et al. (2024); Zhang et al. (2025b); Wang et al. (2025c,d); Nie et al.\n(2025); Liu et al. (2025d)\nKnowledge Management (\u00a75.2.2)\nMeng et al. (2022, 2023); Chen et al. (2025f,c); Zhang et al. (2025e); Katz et al. (2024); Lai et al. (2025);\nMuhamed et al. (2025a); Goyal et al. (2025); Jin et al. (2024); Li et al. (2025a); Niu et al. (2025); Jin\net al. (2025a); Zhang et al. (2024e,b); Wu et al. (2024b); Du et al. (2024); Zhao et al. (2025b); Chen et al.\n(2025a); Yadav et al. (2023b); Liu et al. (2025b)\nLogic and Reasoning (\u00a75.2.3)\nZhang et al. (2024d); Quirke and Barez (2024); Yang et al. (2024b); Venhoff et al. (2025); Ward et al.\n(2025); Troitskii et al. (2025); Galichin et al. (2025); Wu et al. (2023); Wang et al. (2025h); Sun et al.\n(2025b); You et al. (2025); Cywi\u0144ski et al. (2025); Tan et al. (2025); H\u00f8jer et al. (2025); Tang et al.\n(2025b); Hong et al. (2025); Zhang and Viteri (2025); Liu et al. (2025a); Sinii et al. (2025); Li et al.\n(2025j)\nImprove Effi-\nciency (\u00a75.3)\nEfficient Training (\u00a75.3.1)\nZhu et al. (2024); Song et al. (2024); Xu et al. (2025a); Mondal et al. (2025); Gurgurov et al. (2025b);\nZhao et al. (2024c); Sergeev and Kotelnikov (2025); Lai et al. (2025); Li et al. (2025h); Hoogland et al.\n(2024); Singh et al. (2024); Minegishi et al. (2025); Nanda et al. (2023); Liu et al. (2023); Furuta et al.\n(2024); Qiye et al. (2024); Li et al. (2025k)\nEfficient Inference (\u00a75.3.2)\nXia et al. (2025); Lei et al. (2025); Guo et al. (2024); Ye et al. (2025b); He et al. (2024a); Cai et al.\n(2024b); Tang et al. (2025a); Xiao et al. (2024); Laitenberger et al. (2025); Valade (2024); Elhoushi et al.\n(2024); Wang et al. (2023a); Shelke et al. (2024); Lawson and Aitchison (2025); Men et al. (2025); Lu\net al. (2024); Su et al. (2025c); Liu et al. (2024a); Tan et al. (2024a); Dumitru et al. (2024); Zhang et al.\n(2025a); Xiao et al. (2025b); Ranjan and Savakis (2025); Zeng et al. (2024)\nChallenges and Future Directions (\u00a76)\n4\nActionable Mechanistic Interpretability: From Localizing and Steering to Model Improvement\nInterpretable Objects\n(Decoder-only Transformer LLM)\nResidual Stream (\ud835\udc65\ud835\udc59)\nMulti-Head Attention (MHA)\nAttention Head Output (\u210e\ud835\udc59,\u210e\n)\nQuery (\ud835\udc4a\ud835\udc59,\u210e)\nKey (\ud835\udc4a\ud835\udc59,\u210e)\nAttention Score Matrix (\ud835\udc34\ud835\udc59,\u210e)\nFeed-Forward Network (FFN)\nin\nout\nKey Value (\ud835\udc4a\ud835\udc59, \ud835\udc4a\ud835\udc59)\n\ud835\udc53\ud835\udc53\ud835\udc5b\nFFN Output (\u210e\ud835\udc59 )\nOther Specific Objects\n\ud835\udc57  \ud835\udc57\n(\ud835\udc60\ud835\udc59, \ud835\udc58\ud835\udc59, \ud835\udc63\ud835\udc59)\nMethods for Localizing and Steering\nMagnitude Analysis\n(Training-free but\ndata-dependent)\nVocabulary Projection\n(Logit Lens, direct\nsemantic mapping)\nCausal Attribution\n(Patching and ablation)\nCircuit Discovery\n(Causal subnetwork\nLocalizing Methods\n(Identifying & Localizing Objects)\nAmplitude\nManipulation\n(Scale and replace)\nTargeted Optimization\n(Weight editing,\ntargeted fine-tuning)\nVector Arithmetic\n(Steering via feature and\ntask vectors)\nSteering Methods\n(Controlling & Improving Model)\nApplications\n(Model Improvement)\nImprove Alignment\nImprove Efficiency\nImprove Capability\nSafety and Reliability\nFairness and Bias\nPersona and Role\nMultilingualism\nKnowledge Management\nLogic and Reasoning\nEfficient Training\nEfficient Inference\nGradient Detection\n(Data-dependent and\nincurs extra compute)\nProbing\n(Supervised property\ndecoding)\n\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc5b\n\ud835\udc44\nK\nValue (\ud835\udc4a\ud835\udc59,\u210e)\nV\nNeurons/Activations\nSparse Autoencoder\n(SAE) Features (\ud835\udc53\ud835\udc57, \ud835\udc4e\ud835\udc57)\nCircuits/Subnetworks (\ud835\udcaa, \u2130)\nidentification )\nFigure 1: Overview of the paper structure. We begin by defining the core interpretable objects (\u00a72) that\nform the foundation of our analysis. We then introduce a range of methods, ranging from localization (\u00a73) to\nsteering(\u00a74). Finally, we illustrate how these methods can be applied to improve models (\u00a75).\n1. Introduction\nLarge Language Models (LLMs) have recently achieved remarkable success, demonstrating out-\nstanding performance across a wide spectrum of applications, ranging from complex reasoning and\nmultilingualism, to highly specialized domains (Li et al., 2025i; Ren et al., 2025b; Dubey et al., 2024;\nOpenAI et al., 2024; Liang et al., 2025; Qin et al., 2025; Yang et al., 2024a; Chang et al., 2025; Li\net al., 2024a; Zhao et al., 2024b; Yu et al., 2025c; Qwen et al., 2025). Despite these advancements, a\ncritical challenge remains: the internal decision-making processes of these models are largely opaque,\noften operating as \u201cblack boxes.\u201d This lack of transparency poses significant risks, particularly in\nsafety-critical applications, and severely limits our ability to efficiently debug, control, and optimize\nmodel behaviors (Huang et al., 2024a; Hong et al., 2024; Zhang et al., 2025d). Consequently,\nMechanistic Interpretability (MI) has emerged as a pivotal research direction. Unlike traditional\nbehavioral analysis, MI aims to \u201creverse-engineer\u201d these complex neural networks, decomposing\ntheir intricate computations into understandable components and causal mechanisms (Ferrando\net al., 2024; Zhao et al., 2024a; Geiger et al., 2025).\nCurrent research in this field generally falls into two categories. A significant body of work\nfocuses on the theoretical and foundational aspects of MI (R\u00e4uker et al., 2023; Allen-Zhu and Li,\n2023; Ferrando et al., 2024; Zhao et al., 2024a; Zheng et al., 2024; Saphra and Wiegreffe, 2024;\nL\u00f3pez-Otal et al., 2025; Geiger et al., 2025; Gantla, 2025). These studies provide technical roadmaps\nfor dissecting Transformer architectures and identifying fundamental units. However, they primarily\nprioritize scientific discovery\u2014aiming to elucidate the model\u2019s inner working mechanisms for the sake\nof understanding itself. They typically treat MI as an observational science, leaving the question of\nhow to translate these microscopic insights into practical model improvements underexplored.\nRecognizing the applied potential of interpretability, a second line of work has begun to bridge\n5\nthe gap between theoretical understanding and practical utilization. These surveys discuss how MI\ntechniques can be leveraged to aid downstream tasks or assist in specific domains (Luo and Specia,\n2024; Wu et al., 2024a; Rai et al., 2024; Bereska and Gavves, 2024; Lee et al., 2025; Resck et al.,\n2025; Lin et al., 2025). However, despite their contributions, these existing reviews face two primary\nlimitations that hinder broader adoption. First, they often lack a sufficient categorization and clear\ndefinition of MI methods within practical application contexts. The distinction between diagnostic\ntools and intervention techniques is frequently blurred. Second, their coverage of applications is often\nincomprehensive, and the illustration of methods is typically too general. This high-level abstraction\nmakes it difficult for researchers to translate theoretical mechanistic insights into actionable interven-\ntions for specific problems. Consequently, there is a distinct lack of a unified guide that systematically\ncategorizes these methods and clearly presents a concrete pipeline for active model improvement.\nTo fill this gap, we propose the \u201cLocate, Steer, and Improve\u201d pipeline. This conceptual framework\nis designed to systematically transform MI from a passive observational science into an actionable\nintervention discipline. Our work makes the following key contributions:\n\u2022 1) A Rigorous Pipeline-Driven Framework: We establish a structured framework for applying\nMI to real-world model optimization. We begin by defining the core Interpretable Objects within\nLLMs (e.g., neurons, attention heads, residual streams). Based on the application workflow, we\nclearly categorize methodologies into two distinct stages: Localizing (Diagnosis), which identifies\nthe causal components responsible for specific behaviors, and Steering (Intervention), which\nactively manipulates these components to alter model outputs. Crucially, for each technique, we\nprovide a detailed Methodological Formulation along with its Applicable Objects and Scope, helping\nreaders quickly understand the technical implementation and appropriate use cases.\n\u2022 2) Comprehensive Paradigms for Application: We provide an extensive survey of MI applications\norganized around three major themes: Improve Alignment, Improve Capability, and Improve\nEfficiency. These themes cover eight specific scenarios, ranging from safety and multilingualism\nto efficient training. Instead of merely listing relevant papers, we summarize representative MI\napplication paradigms for each scenario. This approach allows readers to quickly capture the\ndistinct usage patterns of MI techniques across different application contexts, facilitating the\ntransfer of methods to new problems.\n\u2022 3) Insights, Resources, and Future Directions: We critically discuss the current challenges in\nactionable MI research and outline promising future directions. To facilitate further progress\nand lower the barrier to entry, we curate a comprehensive collection of over 200 papers, which\nare listed in Table 2. These papers are systematically tagged according to their corresponding\nlocalizing and steering methods, providing a practical and navigable reference for the community.\n6\n2. Core Interpretable Objects of LLMs\nIn this section, we establish a unified mathematical formulation for the core interpretable objects\nwithin LLMs. We focus specifically on the decoder-only Transformer architecture (Radford et al.,\n2019), which serves as the predominant framework for contemporary state-of-the-art models (Dubey\net al., 2024; OpenAI et al., 2024; Qwen et al., 2025). We present the core interpretable objects and\ntheir corresponding mathematical notations in Table 1.\n2.1. Token Embedding\nThe entry point of the model maps discrete tokens from a vocabulary V to continuous vector repre-\nsentations. We define the Embedding Matrix as WE \u2208R|V|\u00d7dmodel, where |V| denotes the vocabulary\nsize and dmodel represents the hidden dimension of the model. For a given input token ti at position i,\nits Token Embedding\u2014which also serves as the initial state of the residual stream, denoted as x0\ni \u2014is\nobtained by retrieving the corresponding vector from WE and adding positional information:\nx0\ni = WE[ti] + pi\n(1)\nwhere pi is the positional embedding vector. It is worth noting that while earlier architectures used\nabsolute positional embeddings added at the input, modern LLMs (Dubey et al., 2024; OpenAI et al.,\n2024; Qwen et al., 2025) typically employ Rotary Positional Embeddings (RoPE) (Su et al., 2024).\nIn these architectures, positional information is applied directly to the query and key vectors within\nthe attention mechanism rather than to the residual stream at the embedding layer.\n2.2. Transformer Block and Residual Stream\nTypically, an LLM is composed of L stacked layers. Each layer l consists of two primary blocks: a\nMulti-Head Attention (MHA) block and a Feed-Forward Network (FFN) block. The fundamental\ncommunication channel connecting these blocks is the residual stream.\nAs illustrated in Figure 2, the residual stream acts as the central \u201chighway\u201d for information\npropagation (Elhage et al., 2021; Bricken and Pehlevan, 2021; Meng et al., 2022, 2023; Zhang et al.,\n2024b). It preserves a shared memory state that is iteratively updated by the blocks. The update\ndynamics for the residual stream state xl1 at layer l are defined as follows:\nxl,mid = xl + hl\nattn(xl)\n(2)\nxl+1 = xl,mid + hl\nffn(xl,mid)\n(3)\nwhere xl,mid represents the intermediate state after the MHA block but before the FFN block.2\nThis additive structure\u2014where xl+1 = xl + MHA(xl) + FFN(xl)\u2014is critical for MI analysis. It\nimplies that features in the residual stream can be viewed as linear combinations of outputs from\nall previous components. This property enables the decomposition of the model\u2019s final prediction\ninto individual component contributions, facilitating methods like \u201cLogit Lens\u201d (nostalgebraist,\n2020; Wang, 2025; Li and Gao, 2025) and causal mediation analysis (Meng et al., 2022, 2023;\nGoldowsky-Dill et al., 2023; Syed et al., 2024; Yeo et al., 2025b).\n1Here, xl \u2208RT\u00d7dmodel represents the token-wise residual stream state of the input sequence at layer l, with T tokens\nand hidden dimension dmodel.\n2For simplicity and clarity in our mechanistic analysis, we omit Layer Normalization (LayerNorm or RMSNorm) terms\nfrom these equations. While crucial for training stability, normalization operations are often abstracted away in high-level\ninterpretability studies to focus on the additive composition of features.\n7\nKeys\nValues\nFigure 2: The schematic of information flow within a standard Transformer block. The residual stream (xl)\nserves as the backbone, while MHA and FFN act as additive branches that read from and write to this stream.\nBased on the figure from (Ferrando et al., 2024).\n2.3. Multi-Head Attention (MHA)\nThe Multi-Head Attention mechanism allows tokens to contextualize information by attending to other\npositions in the sequence. It consists of H independent heads, which primarily manage information\nrouting and the resolution of contextual dependencies (Elhage et al., 2021; Olsson et al., 2022; Voita\net al., 2019; Feng and Steinhardt, 2023; Men et al., 2024).\nStandard Formulation\nFor a specific head h at layer l, we define the learnable weight matrices\nas Wl,h\nQ , Wl,h\nK , Wl,h\nV \u2208Rdmodel\u00d7dhead and the output projection matrix as Wl,h\nO \u2208Rdhead\u00d7dmodel. Here, T\ndenotes the sequence length, the attention mechanism first computes the attention score matrix\nAl,h \u2208RT\u00d7T, which represents the relevance of each token to every other token:\nAl,h = softmax\n(\ufe03\n(xlWl,h\nQ )(xlWl,h\nK )\u22a4\n\u221adhead\n+ M\n)\ufe03\n,\n(4)\nwhere M \u2208RT\u00d7T denotes the attention mask that prevents attention to invalid positions (e.g., future\ntokens in causal attention or padding tokens).\nFunctionally, attention heads \u201cread\u201d information from the residual stream of previous tokens via\nthe query\u2013key subspace projections, and then \u201cwrite\u201d the attended information back to the current\nposition via the value and output projections. The output for a single head h, denoted as hl,h\nattn, is\ncomputed as:\nhl,h\nattn =\n[\ufe01\nAl,h(xlWl,h\nV )\n]\ufe01\nWl,h\nO .\n(5)\n8\nThe total output of the MHA block is the sum of the outputs from all H heads: hl\nattn = \u2211\ufe00H\nh=1 hl,h\nattn.\nMechanistic View: QK and OV Units\nWhile the standard formulation describes how attention is\ncomputed, the unit perspective (Elhage et al., 2021) offers deeper insight into what task each head\nperforms. As illustrated in the detailed view of Figure 2, each head can be decomposed into two\nfunctionally distinct units:\n1) The QK Unit (WQK): This unit determines where to attend. By merging the query and key\nmatrices into a single low-rank matrix Wl,h\nQK = Wl,h\nQ (Wl,h\nK )\u22a4, the attention pattern depends directly\non the interaction between residual stream states. The attention score al,h\ni,j (e.g., a3,1 in Figure 2) is\nderived from the bilinear form (xl\ni)\u22a4Wl,h\nQKxl\nj.\n2) The OV Unit (WOV): This unit determines what information is transmitted. By merging the\nvalue and output matrices into Wl,h\nOV = Wl,h\nV Wl,h\nO , we can view the head\u2019s operation as reading a vector\nfrom the source token j, transforming it linearly via Wl,h\nOV, and adding it to the destination token i,\nweighted by the attention score. This separation allows researchers to classify heads into distinct\nroles, such as \u201cInduction Heads\u201d (which copy previous tokens) or \u201cPrevious Token Heads\u201d (Olsson\net al., 2022; Singh et al., 2024; Wang et al., 2024b).\n2.4. Feed-Forward Network (FFN)\nStandard Formulation\nThe Feed-Forward Network block acts as a position-wise feature transformer.\nUnlike attention heads, FFNs operate independently on each token position, applying non-linear\ntransformations to the input. They are often conceptualized as \u201cKey-Value\u201d memories, where the first\nlayer projects the stream into a high-dimensional state (detecting patterns or \u201cKnowledge Keys\u201d)\nand the second layer writes the retrieved knowledge back to the stream (Geva et al., 2021, 2022;\nDai et al., 2022).\nMathematically, the output of the FFN block hl\nffn is given by :3\nhl\nffn = \u03c3(xl,midWl\nin)Wl\nout\n(6)\nwhere xl,mid is the input to the FFN, and \u03c3 is a non-linear activation function. The weight matrices\nare defined as Wl\nin \u2208Rdmodel\u00d7dffn and Wl\nout \u2208Rdffn\u00d7dmodel.\nMechanistic View: Neurons\nIn this context, the neuron j is defined as an atomic unit comprised of\na pair of weights: the key weight kl\nj (the j-th row of Wl\nin) and the value weight vl\nj (the j-th column of\nWl\nout). The intermediate state sl = \u03c3(xl,midWl\nin) represents the vector of neuron activation.\n2.5. Sparse Autoencoder (SAE) Feature\nWhile the internal objects described above (e.g., neuron activation sl or residual stream state xl) are\nfundamental to the model\u2019s operation, they are often polysemantic. This is due to the phenomenon\n3While we use the standard formulation above to keep notation compact, it is important to note that many modern LLMs\nemploy gated variants such as SwiGLU (Shazeer, 2020). These variants introduce an additional gating matrix Wlgate and\ncombine an element-wise gate with the projection before the final output: hl\nffn =\n(\ufe01\nSiLU(xl,midWlgate) \u2299(xl,midWl\nin)\n)\ufe01\nWlout.\nFor the sake of generality, we present the standard FFN formulation here.\n9\nof superposition, where neural networks represent more features than they have physical neurons\nby encoding them as nearly orthogonal directions in the high-dimensional activation space (Elhage\net al., 2022). Consequently, a single neuron may activate for multiple unrelated concepts, making\ndirect interpretation difficult.\nSparse Autoencoders (SAEs) provide a principled method to resolve this by disentangling dense,\npolysemantic representations into monosemantic features (Bricken et al., 2023). As illustrated in\nFigure 3, an SAE acts as a \u201cmicroscope\u201d for the LLM. It projects low-dimensional dense activations\ninto a higher-dimensional sparse latent space, effectively \u201cunpacking\u201d the superposition.\nReconstruction\nSparsity\nSparse Autoencoder\nToken\nTransfomer Block\n...\nTransfomer Block\nNext-word Prediction\nFigure 3: The framework of Sparse Autoencoders (SAEs). The SAE acts as an independent module attached to\na frozen LLM, expanding dense representations into a sparse, overcomplete set of interpretable features via an\nencoder-decoder architecture. Based on the figure from (Shu et al., 2025).\nMathematical Formulation\nSAEs are trained in a layer-wise manner as independent modules\nattached to a specific object of a frozen LLM. They can be applied to nearly all internal objects, includ-\ning neuron activation sl, residual stream state xl, MHA output hl\nattn, and FFN output hl\nffn (Lieberum\net al., 2024; He et al., 2024c). For instance, when applying an SAE to reconstruct a residual stream\nstate xl, the forward pass is defined as:\na = \u03c3(xlWenc + benc)\n(7)\n\u02c6xl = aWdec + bdec\n(8)\nwhere Wenc \u2208Rdmodel\u00d7dSAE and Wdec \u2208RdSAE\u00d7dmodel are learnable weights. A critical hyperparameter\nhere is the Expansion Factor\u2014the ratio of dSAE to dmodel. To capture the vast number of features hidden\nin superposition, dSAE is typically set to be 16\u00d7 to 128\u00d7 larger than the model dimension (Cunningham\net al., 2023; Templeton et al., 2024; Bloom, 2024; Ghilardi et al., 2024; Mudide et al., 2024; Lieberum\net al., 2024; He et al., 2024c).\nThe training objective is to minimize the reconstruction error while enforcing sparsity on the\nlatent activations a:\nL = \u2225xl \u2212\u02c6xl\u22252\n2 + \u03bb\u2225a\u22251\n(9)\nIn this framework, the SAE feature fj (the j-th row of Wdec) represents a distinct semantic direction\nin the activation space. The SAE feature activation aj (the j-th element of a) quantifies the strength\nof this feature in the current input. Crucially, this decomposition transforms opaque vectors into\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**Summary: Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models**\n\n### **Introduction**\nLarge Language Models (LLMs) have achieved remarkable success across various applications. Despite their impressive performance, the opaque decision-making processes within these models remain a significant challenge, especially for safety-critical applications. This paper aims to bridge this gap by providing a practical framework for actionable mechanistic interpretability (MI). The proposed \"Locate, Steer, and Improve\" pipeline is designed to transform MI from a passive observational science into an active intervention discipline.\n\n### **Core Interpretable Objects of LLMs**\nThe core interpretable objects in LLMs are defined as follows:\n- **Token Embedding**: Maps discrete tokens to continuous vector representations.\n- **Transformer Block and Residual Stream**: Comprises two main blocks: Multi-Head Attention (MHA) and Feed-Forward Network (FFN).\n- **Multi-Head Attention (MHA)**: Allows tokens to attend to other positions in the sequence, with H independent heads managing information routing.\n- **Feed-Forward Network (FFN)**: Acts as a position-wise feature transformer, applying non-linear transformations on each token.\n\n### **Localizing Methods**\n#### **Magnitude Analysis**\nIdentifies features by analyzing their magnitude. Data-dependent and training-free.\n#### **Vocabulary Projection**\nMaps logits to semantic space for direct mapping.\n#### **Causal Attribution**\nApplies patching and ablation techniques to identify causal components responsible for specific behaviors.\n#### **Circuit Discovery**\nIdentifies causal subnetworks within the model.\n\n### **Steering Methods**\n#### **Amplitude Manipulation**\nAdjusts feature magnitudes directly.\n#### **Targeted Optimization**\nEdits weights or performs targeted fine-tuning.\n#### **Vector Arithmetic**\nManipulates features and tasks using vector operations.\n\n### **Applications**\nThe paper categorizes applications into three major themes: Improve Alignment, Improve Capability, and Improve Efficiency. Specific scenarios include:\n- **Improve Alignment**: Enhances safety and fairness by controlling model outputs.\n- **Improve Capability**: Expands capabilities such as multilingualism and knowledge management.\n- **Improve Efficiency**: Improves training and inference efficiency.\n\n### **Challenges and Future Directions**\nThe paper identifies current challenges in actionable MI research, including the need for comprehensive categorization of methods and more detailed application scenarios. It suggests future directions to address these issues and facilitate further progress.\n\n### **Conclusion**\nThis survey provides a structured framework for applying MI to model optimization, with a focus on actionable interventions. The \"Locate, Steer, and Improve\" pipeline ensures that mechanistic insights can be effectively translated into practical improvements in LLMs.\n\n**References:**\n- List of contributors and references are provided at the end of the paper.\n\n### **Table of Contents**\n1. Introduction\n2. Core Interpretable Objects of LLMs\n3. Localizing Methods\n4. Steering Methods\n5. Applications\n6. Challenges and Future Directions\n7. Conclusion\n\nThis summary captures the main contributions, methodology, key findings, and conclusions of the paper in a concise manner."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3\nXu Zhang1 , Danyang Li2 , Yingjie Xia2 , Xiaohang Dong1\nHualong Yu1 , Jianye Wang1 , Qicheng Li1\u2217\n1TMCC, Computer Science, Nankai University\n2VCIP, Computer Science, Nankai University\n{xu zhang, danyang.li}@mail.nankai.edu.cn, liqicheng@nankai.edu.cn\nAbstract\nChange Detection (CD) is a fundamental task in\nremote sensing. It monitors the evolution of land\ncover over time. Based on this, Open-Vocabulary\nChange Detection (OVCD) introduces a new re-\nquirement. It aims to reduce the reliance on pre-\ndefined categories.\nExisting training-free OVCD\nmethods mostly use CLIP to identify categories.\nThese methods also need extra models like DINO\nto extract features. However, combining different\nmodels often causes problems in matching features\nand makes the system unstable. Recently, the Seg-\nment Anything Model 3 (SAM 3) is introduced.\nIt integrates segmentation and identification capa-\nbilities within one promptable model, which offers\nnew possibilities for the OVCD task. In this paper,\nwe propose OmniOVCD, a standalone framework\ndesigned for OVCD. By leveraging the decoupled\noutput heads of SAM 3, we propose a Synergis-\ntic Fusion to Instance Decoupling (SFID) strategy.\nSFID first fuses the semantic, instance, and pres-\nence outputs of SAM 3 to construct land-cover\nmasks, and then decomposes them into individual\ninstance masks for change comparison. This design\npreserves high accuracy in category recognition\nand maintains instance-level consistency across im-\nages. As a result, the model can generate accu-\nrate change masks.\nExperiments on four public\nbenchmarks (LEVIR-CD, WHU-CD, S2Looking,\nand SECOND) demonstrate SOTA performance,\nachieving IoU scores of 67.2, 66.5, 24.5, and 27.1\n(class-average), respectively, surpassing all previ-\nous methods.\n1\nIntroduction\nChange Detection (CD) aims to accurately capture the evo-\nlution of land cover, which serves as a fundamental task\nfor Earth observation.\nIt supports a wide range of appli-\ncations, including urban planning and land resource man-\nagement [Wellmann et al., 2020; Asadzadeh et al., 2022;\n\u2217Corresponding Author\nText\nSAM3\nT1 & T2 images\nChange\nmask\n(a) Previous Multi-Stage Frameworks\n(b) OmniOVCD\nText T1 & T2 images\nSAM1/2\nDINO\nCLIP\nMultiple Models , Disjointed Pipeline\nChange\nmask\nMask\nIntegration\nEnd-to-End, Single Model, Streamlined\nT1 mask\nT2 mask\nDecoupling\n& Matching\nT1/T2\nmask\nFeature\ncues\nSemantic\ncues\nPresence Head\nInstance Head\nSemantic Segmentation Head\nFigure 1: Architecture comparison.\nZheng et al., 2021]. However, conventional CD methods typ-\nically operate in a closed-set setting, recognizing only the pre-\ndefined categories present in the training data. This limitation\nprevents the models from detecting changes that were not de-\nfined during training. To address this issue, Open-Vocabulary\nChange Detection (OVCD) has been proposed, which lever-\nages language guidance to detect changes of different cate-\ngories, making it more suitable for open-world scenarios.\nUnder the supervised learning paradigm, deep neural net-\nworks have established the dominant standard for change de-\ntection [Daudt et al., 2018; Zhang et al., 2020]. These meth-\nods typically leverage Siamese architectures to capture tem-\nporal variations [Chen et al., 2022; Bandara and Patel, 2022].\nAlthough recent methods leveraging feature interaction and\nfusion have achieved significant improvements [Fang et al.,\n2023; Xu et al., 2024], these methods still rely heavily on the\nuse of large-scale and high-quality annotated datasets. Man-\nual annotation is costly; meanwhile, generalizing models to\narXiv:2601.13895v1  [cs.CV]  20 Jan 2026\n34.5\n50.9\n27.0\n27.7\n24.5\n21.2\n16.7\n25.6\n12.3\n9.7\n35.6\n32.5\n36.8\nBuilding\nPlayground\nSurface\nLow Vegetation\nTree\nWater\n25.0\n10.6\n22.6\n21.3\n24.6\n19.6\n20.1\n14.6\n15.8\n12.2\n26.7\n15.3\n15.6\nOmniOVCD(Ours)\nSAM2-DINOv2-SegEarth-OV\nSAM-DINOv2-SegEarth-OV\nAPE - / - DINO\nAPE - / - DINOv2\nFigure 2: Compared with existing methods, OmniOVCD achieves\nsuperior IoU scores across multiple category benchmarks.\nunseen categories remains difficult. Both issues hinder prac-\ntical application in the open world.\nThe rapid development of Vision Foundation Models\n(VFMs), such as CLIP [Radford et al., 2021] and SAM [Kir-\nillov et al., 2023; Ravi et al., 2024; Carion et al., 2025], has\nsignificantly facilitated the progress of zero-shot change de-\ntection. Early pioneers such as AnyChange [Zheng et al.,\n2024] and SCM [Tan et al., 2024] explored zero-shot change\ndetection by leveraging pre-trained foundation models for\nbi-temporal analysis.\nAnyChange extends SAM\u2019s zero-\nshot segmentation to change detection via bi-temporal latent\nmatching.\nIt effectively locates changed regions but pro-\nduces only class-agnostic masks. SCM combines SAM with\nCLIP, using a Piecewise Semantic Attention (PSA) scheme\nto provide semantic guidance and identify building-related\nchanges. Along this line, DynamicEarth [Li et al., 2025a] for-\nmally defines the task of Open-Vocabulary Change Detection\n(OVCD), aiming to detect any category of interest through\nnatural language guidance. It proposes two training-free and\nuniversal frameworks: Mask-Compare-Identify (M-C-I) and\nIdentify-Mask-Compare (I-M-C). Both integrate various pre-\ntrained foundation models for inference.\nWhile these methods have demonstrated promising perfor-\nmance, they still suffer from several limitations. As illus-\ntrated in Fig. 1(a), these frameworks rely on combining in-\ndependent models, which often creates additional computa-\ntional cost and compromises system stability. Specifically,\nthese multi-step pipelines face difficulties in aligning fea-\ntures across modules. Because each component operates in-\ndependently, errors accumulate across stages, ultimately re-\nducing overall accuracy. The IoU performance of the four\nbest-performing configurations from DynamicEarth [Li et al.,\n2025a] can be seen in Fig. 2.\nThe \u201cBuilding\u201d score rep-\nresents the average IoU of the building category across all\ndatasets, and the remaining categories are evaluated on the\nSECOND dataset. These configurations achieve competitive\noverall IoU. However, each configuration exhibits noticeable\nvariation across different categories. This variation demon-\nstrates the instability and limited generalization of existing\nOVCD pipelines.\nTo address the instability and complexity of existing mod-\nels, we introduce OmniOVCD. As illustrated in Fig. 1(b),\nOmniOVCD is a standalone framework that leverages the\nunified architectural advancements of the Segment Anything\nModel 3 (SAM 3). It simplifies the architecture and stream-\nlines the open-vocabulary change detection process. The core\nof our framework lies in the Synergistic Fusion to Instance\nDecoupling (SFID) strategy. SFID performs synergistic fu-\nsion by combining semantic, instance, and presence outputs.\nIt then separates them into instance masks to ensure accurate\ninstance matching during bi-temporal comparison. As shown\nin Fig. 2, this strategy effectively reduces pseudo-changes,\nenabling OmniOVCD to achieve consistently strong perfor-\nmance across different categories. Overall, our contributions\nare as follows:\n\u2022 We propose OmniOVCD, the first standalone framework\nfor open-vocabulary change detection based on SAM 3.\nBy using the concept prompts in SAM 3, we success-\nfully streamline the change detection process, achieving\na simpler and more efficient workflow.\n\u2022 We introduce the SFID strategy, which first fuses the\nmulti-head outputs and then separates them into instance\nmasks. This design enables high-quality instance-level\nreasoning, which significantly improves the accuracy of\nchange detection.\n\u2022 Comprehensive experiments show that OmniOVCD\nachieves state-of-the-art performance, outperforming\nexisting training-free methods on both building and\nmulti-class benchmarks.\n2\nRelated Work\n2.1\nSegment Anything Model\nThe Segment Anything Model (SAM) [Kirillov et al., 2023]\nestablishes a promptable paradigm for zero-shot image seg-\nmentation. Efficient variants such as FastSAM [Zhao et al.,\n2023] and MobileSAM [Zhang et al., 2023] address the high\ninference costs of the original architecture. These models\nenable real-time performance on resource-constrained plat-\nforms. Subsequently, SAM 2 [Ravi et al., 2024] extended\nthe promptable framework to the temporal dimension.\nIt\nincorporates a memory attention mechanism for video ob-\nject tracking and mask propagation. Recently, SAM 3 [Car-\nion et al., 2025] unifies detection, segmentation, and track-\ning within a single architecture through Promptable Concept\nSegmentation (PCS). This approach generates high-quality\nmasks and provides unique identities for matching instances.\nWhile SAM and its derivatives focus on single-image seg-\nmentation, their powerful zero-shot capabilities create new\npossibilities for change detection. AnyChange [Zheng et al.,\n2024] and SCM [Tan et al., 2024] established early training-\nfree paradigms by utilizing the foundational knowledge of\nSAM and FastSAM. Specifically, AnyChange relies on bi-\ntemporal latent matching to locate changed regions, but it\nonly produces class-agnostic masks. In contrast, SCM em-\nploys a Piecewise Semantic Attention (PSA) scheme to iden-\ntify building-specific changes. The promptable concept seg-\nmentation capability of SAM 3 establishes a solid foundation\nfor OmniOVCD. It simplifies the alignment issues typical of\nmulti-model change detection.\n2.2\nTraditional Change Detection\nThe change detection task has traditionally been partitioned\ninto two sub-tasks: binary change detection and semantic\nchange detection. Early change detection methods are based\non siamese convolutional neural networks, such as FC-Siam-\ndiff [Daudt et al., 2018] and IFN [Zhang et al., 2020]. These\nmodels use weight-sharing encoders to compare bi-temporal\nimages. Transformer-based architectures, such as BiT [Chen\net al., 2022], ChangeFormer [Bandara and Patel, 2022], and\nChanger [Fang et al., 2023], improve performance by captur-\ning global context. These models show better feature interac-\ntion and discriminative power on various land-cover bench-\nmarks [Feng et al., 2023; Xu et al., 2024].\nRecently, re-\nsearchers have applied vision foundation models to change\ndetection. SAM-CD [Ding et al., 2024] and SCD-SAM [Mei\net al., 2024] leverage pre-trained features to enhance change\nanalysis [Peng et al., 2021; Ding et al., 2022]. However, these\nmethods rely on predefined categories and cannot recognize\nchanges outside their training data. This limitation leads to\npoor performance when they face complex scenarios.\n2.3\nOpen-Vocabulary Change Detection\nTo better understand complex remote sensing images, re-\ncent research has shifted from identifying predefined cate-\ngories to open-vocabulary analysis.\nEarly OVSS methods\nsuch as MaskCLIP [Zhou et al., 2022] and SCLIP [Wang et\nal., 2024] use CLIP [Radford et al., 2021] to extract pixel-\nlevel features. Subsequently, ProxyCLIP [Lan et al., 2024]\nuses auxiliary structural guidance from DINO [Caron et al.,\n2021] or SAM [Kirillov et al., 2023] to improve localiza-\ntion. SegEarth-OV [Li et al., 2025b] optimizes CLIP fea-\ntures to better represent the diverse shapes and sizes of objects\nin remote sensing images. SegEarth-OV3 [Li et al., 2025c]\nfuses the multi-head outputs of SAM 3 to improve zero-shot\nrecognition. Following these works, DynamicEarth [Li et al.,\n2025a] formally defines the task of open-vocabulary change\ndetection (OVCD). This work introduces two training-free\nframeworks: Mask-Compare-Identify (M-C-I) and Identify-\nMask-Compare (I-M-C). However, these methods rely on\ncombining different pre-trained models, which leads to high\ncomplexity and instability. Experiments in DynamicEarth re-\nveal that such a framework lacks consistency, as its perfor-\nmance varies significantly across datasets. This is because\nthese separate components are not well-aligned, leading to\naccumulated errors during inference.\n3\nMethods\n3.1\nTask Definition\nOpen-vocabulary\nchange\ndetection\n(OVCD)\nidentifies\nchanges between two images based on a text description.\nThis task requires both locating changed regions and iden-\ntifying their categories. Formally, the input consists of two\nremote sensing images x1\nimg, x2\nimg \u2208RH\u00d7W \u00d7C.\nThese\nimages cover the same area and are captured at different\ntimes T1 and T2.\nUnlike traditional change detection\nparadigms that are restricted to a predefined set of categories,\nthe OVCD task introduces a flexible text prompt, xtxt,\nwhich provides natural language guidance for the target\ncategories of interest. This task requires the model to work\noutside fixed categories and identify new types of changes in\ndiverse real-world environments. The primary objective is\nto generate a high-fidelity change mask M. This mask must\nlocate where changes occur and identify what categories\nthey represent, as specified by the text xtxt. This inference\nprocess can be generally formulated as:\nM = \u03a6OmniOV CD(x1\nimg, x2\nimg, xtxt),\n(1)\nwhere \u03a6OmniOV CD denotes the proposed standalone archi-\ntecture, as illustrated in Fig. 3(a). It accepts bi-temporal im-\nages along with their corresponding text prompts to generate\ninitial semantic masks using SAM 3. These masks are subse-\nquently split into individual instance masks, which are com-\npared across time to produce the final change mask.\n3.2\nSFID Pipeline\nThe Synergistic Fusion to Instance Decoupling (SFID)\npipeline enables instance-level change analysis from dense\nsemantic predictions. This process consists of two sequential\nphases: Synergistic Mask Fusion and Instance Decoupling\nand Matching. This strategy adopts the synergistic percep-\ntion paradigm established in SegEarth-OV3 [Li et al., 2025c]\nand extends its application to a bi-temporal context for dual-\nimage processing. Initially, the framework fuses SAM 3\u2019s\nsemantic, instance, and presence outputs from each image to\nconstruct high-fidelity semantic maps. To convert pixel-wise\nclass predictions into discrete object entities, these continu-\nous semantic regions are decoupled into individual instance\nmasks based on topological connectivity.\nSubsequently, a\nbi-temporal consistency analysis evaluates each instance by\nmeasuring how much masks overlap at the same geographic\nlocation. Instances that stay in the same place are marked as\nunchanged, while those without a matching counterpart are\nidentified as valid change candidates.\nSynergistic Mask Fusion. To establish a precise repre-\nsentation of the land-cover in both images, the framework\nadopts a synergistic perception strategy that fuses the multi-\nhead outputs of SAM 3, as shown in Fig. 3(b). This approach\nis specifically designed to handle the structural duality inher-\nent in remote sensing imagery. In these scenes, discrete ob-\njects and expansive land-cover regions are both present and\nrequire different types of analysis. Specifically, discrete enti-\nties such as buildings require precise boundary localization at\nthe instance level. In contrast, land-cover regions like water\nor vegetation need pixel-wise semantic continuity for an ac-\ncurate representation. The framework applies this synergistic\nmechanism to each image xt\nimg individually. This fusion en-\nsures that the fused semantic masks maintain sharp instance\nboundaries while achieving high categorical accuracy.\nBy\nSAM3\nImage T1/T2\nText Prompt\nInstance\nLogits\nMask outputs\n(b) Multi-head Outputs Fusion Strategy\nElement-wise maximum\nBuilding\nBackground\nSAM3\nText Prompts\nBi-temporal\nimages\nT1 Mask\nT2 Instance Masks\nInstance Decoupling\nChange Mask\nInstance Matching\nSemantic\nLogits\nPresence\nScore\n\u00d7\n\u00d7\nMultiplication\nArgmax\nT2 Mask\nT1 Instance Masks\n(a) The overview structure of the OmniOVCD\nFigure 3: (a) shows the overview of OmniOVCD framework. The model takes bi-temporal images and corresponding text prompts to\ngenerate initial masks via SAM 3. These masks are then separated into instance masks for instance-level comparison, which produce the\naccurate change detection mask. (b) shows the multi-head outputs fusion strategy. This strategy fuses the semantic and instance head outputs\nfrom SAM 3 and uses the presence head outputs for filtering. This approach effectively improves the accuracy of single-image segmentation.\nleveraging these complementary strengths, the model gener-\nates a comprehensive land-cover description. Detailed shapes\nof individual objects are accurately preserved. At the same\ntime, the broader semantic context provides consistency and\ncoherence across the scene, allowing the model to capture\nboth fine-grained structural details and global information.\nThe initial stage of this process involves the aggregation\nof instance-centric cues generated by the Transformer-based\ndecoder. For each temporal branch t, the model produces\na set of Nt discrete instance queries, denoted as Pt\ninst =\n{(P (k)\ninst,t, s(k)\nconf,t)}Nt\nk=1. P (k)\ninst,t \u2208[0, 1]H\u00d7W represents the\nspatial probability map for the k-th instance and s(k)\nconf,t \u2208\n[0, 1] signifies its confidence score. To transform these sparse\ninstance-level predictions into a category-level representa-\ntion, a weighted maximal selection strategy is employed. The\naggregated instance map at time t, denoted as P t\nagg, is for-\nmulated by computing the peak weighted response across all\nqueries for each pixel location (h, w):\nP t\nagg(h, w) =\nNt\nmax\nk=1\n\u0010\nP (k)\ninst,t(h, w) \u00b7 s(k)\nconf,t\n\u0011\n.\n(2)\nIn parallel with instance aggregation, the semantic seg-\nmentation head generates a dense probability map P t\nsem \u2208\n[0, 1]H\u00d7W for each image. This map helps preserve the spa-\ntial integrity of continuous land-cover categories. While P t\nagg\nis effective at capturing the geometric boundaries of individ-\nual targets such as buildings, the instance-level representa-\ntion can produce fragmented or discontinuous masks in large,\namorphous regions. In contrast, P t\nsem offers complete cov-\nerage for categories like water or low vegetation, but may\nexhibit boundary adherence or semantic blurring in densely\npopulated object areas.\nTo leverage their complementary\nstructural properties, a pixel-wise max-fusion operation is ap-\nplied. This operation is performed across both temporal states\nto obtain an intermediate fused representation Pfused,t:\nPfused,t(h, w) = max (Psem,t(h, w), Pagg,t(h, w)) .\n(3)\nFurthermore, to reduce semantic noise in open-vocabulary\ninference, the framework employs a bi-temporal presence\ngating mechanism to handle sparse category activation. Lo-\ncal remote sensing image patches usually contain only a small\nsubset of categories from the global vocabulary C. As a re-\nsult, the model may incorrectly predict land-cover types that\nare not actually present, especially when visual patterns are\nsimilar across categories. For each queried concept c, the\npresence head of SAM 3 provides a global existence score\nSpres,t \u2208[0, 1], indicating the probability that the category\nexists within the image xt\nimg. This score is utilized as a soft\ngating factor to recalibrate the fused probability maps. The fi-\nnal output Mt is obtained in a pixel-wise manner. Each pixel\nis assigned to the category with the highest probability after\ngating:\nP (c)\nfinal,t = P (c)\nfused,t \u00b7 S(c)\npres,t,\nMt(h, w) = arg max\nc\u2208C P (c)\nfinal,t(h, w).\n(4)\nThis fusion and gating pipeline produces clean and consis-\ntent semantic maps. These representations provide a reliable\nbasis for detecting accurate changes in subsequent stages.\nInstance Decoupling and Matching. The previous stage\nproduces clean semantic probability maps. However, these\nmaps represent categories at the pixel level and do not dis-\ntinguish individual instances. To capture geospatial transi-\ntions at the instance level, the framework applies an instance\ndecoupling mechanism.\nThis mechanism divides the con-\ntinuous semantic map into separate regions based on topo-\nlogical connectivity.\nEach region corresponds to a sepa-\nrate instance. Specifically, for each binary semantic mask\nLEVIR-CD\nWHU-CD\nS2Looking\nMethod\nIoU\nF1\nIoU\nF1\nIoU\nF1\nPCA-KM [Celik, 2009]\n4.8\n9.1\n5.4\n10.2\n-\n-\nCNN-CD [El Amin et al., 2016]\n7.0\n13.1\n4.9\n9.4\n-\n-\nDSFA [Du et al., 2019]\n4.3\n8.2\n4.1\n7.8\n-\n-\nDCVA [Saha et al., 2019]\n7.6\n14.1\n10.9\n19.6\n-\n-\nGMCD [Tang et al., 2021]\n6.1\n11.6\n10.9\n19.7\n-\n-\nCVA [Bovolo and Bruzzone, 2006]\n-\n12.2\n-\n-\n-\n5.8\nDINOv2+CVA [Zheng et al., 2024]\n-\n17.3\n-\n-\n-\n4.3\nAnyChange-H [Zheng et al., 2024]\n-\n23.0\n-\n-\n-\n6.4\nSCM [Tan et al., 2024]\n18.8\n31.7\n18.6\n31.3\n-\n-\n*SAM-DINO-SegEarth-OV [Li et al., 2025a]\n33.0\n49.7\n35.8\n52.8\n22.5\n36.7\n*SAM-DINOv2-SegEarth-OV [Li et al., 2025a]\n36.6\n53.6\n38.8\n55.9\n23.9\n38.5\n*SAM2-DINOv2-SegEarth-OV [Li et al., 2025a]\n33.8\n50.5\n36.8\n53.8\n23.1\n37.6\n*APE - / - DINO [Li et al., 2025a]\n53.5\n69.7\n54.5\n70.5\n10.1\n18.4\n*APE - / - DINOv2 [Li et al., 2025a]\n50.0\n66.7\n55.2\n71.1\n5.3\n10.1\nOmniOVCD\n67.2\n80.4\n66.5\n79.9\n24.5\n39.4\nTable 1: Comparison on building change detection datasets. \u201c*\u201d denotes re-implementation under identical settings.\nMt \u2208{0, 1}H\u00d7W (t \u2208{1, 2}), the framework performs\nconnected-component analysis using an 8-connectivity rule.\nThis process extracts a set of Kt candidate instances, denoted\nas It = {It,i | \u03a8(Mt)}Kt\ni=1. \u03a8(\u00b7) denotes the operation that\nextracts connected regions from the mask.\nCrucially, the instances extracted at this stage are not lim-\nited to discrete categories like buildings; they also include\ncontinuous land-cover types such as water or low vegetation.\nRaw instance-level predictions from foundation models of-\nten produce fragmented masks in continuous regions. Our\nsynergistic mask fusion stage addresses this by fusing SAM\n3\u2019s multi-head outputs, producing sharper and more stable\nboundaries. As a result, the decoupling of these refined se-\nmantic masks enables the extraction of coherent instances\neven for amorphous categories. It provides a robust repre-\nsentation for complex geospatial analysis.\nOur change detection operates at the instance level. It com-\npares objects across time rather than relying on pixel-wise\ndifferences. For each instance I1,i \u2208I1, we measure its spa-\ntial overlap with all instances in I2. We define the overlap\nratio between two instances as:\nR(It,a, It\u2032,b) = |It,a \u2229It\u2032,b|/|It,a|,\n(5)\nwhich represents the fraction of It,a covered by It\u2032,b. Dur-\ning the forward check from T1 to T2, an instance in I1\nis considered unchanged if it has at least one match in I2\nwith R \u2265\u03c4match; otherwise, it is included in the set of\nT1 changes, denoted as CT 1.\nSimilarly, during the back-\nward check from T2 to T1, instances in I2 with no match-\ning T1 instance above the threshold are included in the set of\nT2 changes, denoted as CT 2. The bi-temporal change mask\nMchange is then generated by combining all instances from\nboth sets. Formally, the change mask is defined as:\nMchange(h, w) = I (\u2203I \u2208(CT 1 \u222aCT 2) s.t. I(h, w) = 1) ,\n(6)\nwhere I(\u00b7) is the indicator function, mapping the existential\ncondition to the binary domain {0, 1}. This instance-level\nmatching ensures that Mchange captures accurate semantic\ntransitions while suppressing false changes caused by lighting\nor seasonal variations.\n4\nExperiments\n4.1\nExperimental Setting\nWe adopt the Segment Anything Model 3 (SAM 3) [Carion et\nal., 2025] as the foundational model for OmniOVCD. All ex-\nperiments are conducted on a single NVIDIA GeForce RTX\n3090 GPU with 24GB of VRAM.\n4.2\nDatasets\nLEVIR-CD. LEVIR-CD [Chen and Shi, 2020] is a high-\nresolution dataset for building change detection, containing\n637 bi-temporal image pairs from various cities in Texas,\nUSA. Each image has a resolution of 0.5 m/pixel and a size\nof 1024\u00d71024 pixels, with a total of 31333 annotated change\ninstances. The dataset is split into 445 training pairs, 64 vali-\ndation pairs, and 128 test pairs.\nWHU-CD. WHU-CD [Ji et al., 2019] is a large-scale ur-\nban change detection dataset.\nIt is built from aerial im-\nages captured before and after a magnitude 6.3 earthquake in\nChristchurch, New Zealand. The dataset consists of a single\nbi-temporal image pair with a spatial resolution of 0.075 m.\nThe image size is 32507 \u00d7 15354 pixels and covers an area\nof approximately 20.5 km2. For experiments, the images are\ndivided into 256 \u00d7 256 pixel patches. These patches are split\ninto training, validation, and test sets with a ratio of 8:1:1.\nS2Looking. S2Looking [Shen et al., 2021] is a large-scale\nsatellite change detection dataset collected from rural regions\nworldwide. It contains 5000 bi-temporal image pairs, each\nwith a size of 1024\u00d71024 pixels and spatial resolutions rang-\ning from 0.5 to 0.8 meters. The images are captured from\nside-looking viewpoints with varying off-nadir angles, rather\nthan standard near-nadir views. This imaging setup intro-\nduces geometric distortions and illumination variations, mak-\nBuilding\nTree\nWater\nLow vegetation\nSurface\nPlayground Class Avg\nMethod\nIoU\nF1\nIoU\nF1\nIoU\nF1\nIoU\nF1\nIoU\nF1\nIoU\nF1\nIoU\nF1\n*SAM-DINOv2-SegEarth-OV\n38.8 56.0 15.6 27.0 15.3 26.5\n21.3\n35.1\n26.7 42.1 22.6\n36.8\n23.4 37.3\n*SAM2-DINOv2-SegEarth-OV 36.3 53.3 15.8 27.3 14.6 25.5\n20.1\n33.4\n19.6 32.7 24.6\n39.5\n21.8 35.3\n*APE - / - DINO\n29.1 45.1\n9.7\n17.7 12.3 22.0\n-\n-\n-\n-\n25.6\n40.8\n12.8 21.0\n*APE - / - DINOv2\n31.9 48.3 10.6 19.2 12.2 21.7\n-\n-\n-\n-\n25.0\n40.0\n13.3 21.5\nOmniOVCD\n45.2 62.3 16.7 28.6 21.2 35.0\n24.5\n39.3\n27.7 43.4 27.0\n42.4\n27.1 41.8\nTable 2: Comparison on SECOND datasets. \u201c*\u201d denotes re-implemented results under identical settings, including the two best-performing\nM-C-I architectures and the two best-performing I-M-C architectures from DynamicEarth [Li et al., 2025a]. \u201c-\u201d denotes that the score is\nclose to 0.\ning change detection more challenging in rural scenes. The\ndataset is split into 3500 training pairs, 500 validation pairs,\nand 1000 test pairs.\nSECOND. The SECOND [Yang et al., 2022] dataset is\nused to evaluate the model\u2019s ability to detect detailed category\nchanges in urban and suburban areas. It contains 4662 image\npairs of size 512\u00d7512 from Hangzhou, Chengdu, and Shang-\nhai. The dataset is split into 2968 training pairs and 1694\ntest pairs. Following the DynamicEarth [Li et al., 2025a], we\nevaluate binary change detection for each category to mea-\nsure the model\u2019s accuracy across different land-cover types.\n4.3\nEvaluation Metrics\nIoU. Intersection over Union (IoU) is a metric used to mea-\nsure the overlap between predicted and reference regions. It is\ncalculated using True Positives (TP, correctly predicted pix-\nels), False Positives (FP, incorrectly predicted pixels), and\nFalse Negatives (FN, missed pixels). IoU is defined as the\nratio of the intersection to the union of the predicted and ref-\nerence regions:\nIoU = TP/(TP + FP + FN).\n(7)\nF1. The F1 Score is a metric that harmonizes Precision\n(P) and Recall (R). Precision measures the proportion of cor-\nrectly predicted positive pixels among all predicted positives,\nwhile Recall measures the proportion of correctly predicted\npositive pixels among all actual positives. The F1 Score is\nthe harmonic mean of Precision and Recall, defined as:\nP = TP / (TP + FP),\nR = TP / (TP + FN),\nF1 = 2 \u00d7 P \u00d7 R/(P + R).\n(8)\n4.4\nCompared Methods\nTo assess the performance of OmniOVCD, we select some\nunsupervised methods for comparison,\nincluding PCA-\nKM [Celik, 2009], CNN-CD [El Amin et al., 2016],\nDSFA [Du et al., 2019], DCVA [Saha et al., 2019],\nGMCD [Tang et al., 2021], CVA [Bovolo and Bruzzone,\n2006], AnyChange [Zheng et al., 2024] and SCM [Tan et\nal., 2024]. We further compare our results with the state-of-\nthe-art OVCD method, DynamicEarth [Li et al., 2025a]. We\nre-implement the five best configurations of DynamicEarth\non three building change detection datasets and its four best\nconfigurations on the SECOND dataset.\n1000\n0\n2000\n3000\n4000\n5000\n6000\n6\n8\n10\n12\n14\nMemory Use (GB)\n7.1\n8.6\n15.1\n15.2\n5.8\n4432.50\n5152.50\n1518.75\n1552.25\n543.75\nRuntime (ms/item)\nGPU Memory (GB)\nRuntime (ms/item)\n27.5\n30.0\n32.5\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n1000\n2000\n3000\n4000\n5000\nRuntime (ms/item)\nAverage IoU Score\nOmniOVCD\nAPE - / - DINO\nAPE - / - DINOv2\nSAM-DINOv2-\nSegEarth-OV\nSAM2-DINOv2-\nSegEarth-OV\nFigure 4: Comparison of GPU memory usage, inference efficiency\nand IoU performance on an 3090 GPU.\n4.5\nResults\nBuilding Change Detection.\nAs shown in Tab. 1, our\nproposed OmniOVCD achieves SOTA performance on all\nthree benchmarks.\nIt consistently outperforms both tra-\nditional unsupervised methods and recent open-vocabulary\nmodels. Early methods, such as PCA-KM [Celik, 2009] and\nDSFA [Du et al., 2019], perform poorly on these complex\nbuilding change tasks, with IoU scores often below 15. Dy-\nnamicEarth [Li et al., 2025a] improves results, but its perfor-\nmance is less stable across different datasets.\nSpecifically, the SAM-DINOv2-SegEarth-OV configura-\ntion achieves a relatively competitive IoU of 23.9 on\nS2Looking. However, its performance on LEVIR-CD and\nWHU-CD is much lower than ours. Similarly, the APE -\n/ - DINOv2 configuration reaches a IoU of 55.2 on WHU-\nCD, but its result on S2Looking dataset drops sharply to 5.3.\nIn contrast, OmniOVCD achieves consistently high and sta-\nble performance, with IoU scores of 67.2, 66.5, and 24.5 on\nLEVIR-CD, WHU-CD, and S2Looking, respectively. This\ndemonstrates that our method performs reliably across differ-\nent building shapes and complex environments.\nLand Cover Change Detection. Results on the SECOND\ndataset are shown in Tab. 2. We re-implemented the four best\nconfigurations from DynamicEarth [Li et al., 2025a]. As de-\nscribed in the table, OmniOVCD clearly outperforms them\non all categories.\nOur method achieves higher scores not\nonly in class average IoU and F1, but also for every individ-\nual category. For example, compared to the strongest base-\nline, OmniOVCD improves the IoU for the \u201cBuilding\u201d and\n\u201cWater\u201d classes from 38.8 and 15.3 to 45.2 and 21.2, respec-\nFusion Strategy\nLEVIR-CD\nWHU-CD\nIoU\nF1\nIoU\nF1\nw/o Semantic Head\n53.0\n69.3\n52.9\n64.5\nw/ Semantic Head\n67.2\n80.4\n66.5\n79.9\nTable 3: Effects of semantic head fusion.\nMatching Strategy\nLEVIR-CD\nWHU-CD\nIoU\nF1\nIoU\nF1\nPMC\n29.2\n45.2\n26.3\n41.7\nL1 Distance\n60.6\n75.5\n56.9\n72.6\nL2 Distance\n59.1\n74.3\n47.6\n64.5\nInstance Matching\n67.2\n80.4\n66.5\n79.9\nTable 4: Effects of change mask match strategy.\ntively. This demonstrates that our framework achieves strong\nand consistent performance on the OVCD task.\nEfficiency Comparison. As shown in Fig. 4, OmniOVCD\nnot only achieves the highest performance, but also exhibits\nthe fastest inference speed and the lowest memory usage, fur-\nther demonstrating the superiority of our architecture.\n4.6\nAblation Study\nEffects of Semantic Head Fusion. This ablation study ex-\namines whether fusing semantic head outputs is necessary in\nthe SFID strategy before instance decoupling. Although the\nframework can generate instance-level masks directly, fusing\nsemantic head outputs provides more stable guidance for sub-\nsequent stages. We verify this by comparing the full Om-\nniOVCD with a variant that skips semantic head fusion.\nAs shown in Tab. 3, removing semantic head fusion results\nin clear performance drops. This indicates that the semantic\nhead provides important category-level guidance. By fusing\nsemantic head outputs, the SFID strategy produces a more\naccurate and informative semantic representation, leading to\nimproved instance-level change predictions.\nEffects of Change Mask Match Strategy. To evaluate\nthe instance decoupling and matching strategy, we compare it\nwith three other change detection strategies. The simplest is\nPixel-wise Mask Comparison (PMC), which detects changes\nby comparing corresponding pixels in the bi-temporal se-\nmantic masks. We also implement two logits-based meth-\nods: Logit-space L1 Distance and Logit-space L2 Distance.\nThese methods compute the L1 (sum of absolute differences)\nor L2 (Euclidean) distance between the bi-temporal logits at\neach pixel, followed by a thresholding step to generate change\nmasks.\nAs shown in Tab. 4, our instance matching strategy con-\nsistently achieves the best results. The simpler pixel-wise\nmethods, such as PMC and logit-based L1 or L2 distances,\noften struggle with noise at the pixel level. PMC can produce\nfragmented change predictions when there are isolated clas-\nsification errors, and the L1 and L2 distances are sensitive to\nsmall differences in image intensity or lighting between the\ntwo time points. In contrast, our instance matching strategy\nOmniOVCD\nGT\nT1\nT2\nSAM2-DINOv2\n-SegEarth-OV\nLow Vegetation\nSurface\nTree\nWater\nBuilding\nPlayground\nFigure 5: Visual comparisons of OmniOVCD with other state-of-\nthe-art methods for open-vocabulary change detection.\noperates at the instance level. It compares decoupled instance\nmasks across time, which reduces the influence of pixel-level\nnoise and helps preserve the precise shape and boundaries of\neach changed instance. These results show that the instance-\nlevel comparison is crucial for accurate change detection.\n4.7\nVisualization Results\nWe qualitatively compare OmniOVCD with the best-\nperforming configuration from DynamicEarth [Li et al.,\n2025a], namely SAM2-DINOv2-SegEarth-OV. As shown in\nFig. 5, OmniOVCD delivers consistently accurate change de-\ntection across all categories, while the DynamicEarth config-\nuration exhibits false negatives and false positives in complex\nscenes. These results further validate the robustness and ef-\nfectiveness of OmniOVCD.\n5\nConclusion\nIn this paper, we propose OmniOVCD, the first standalone\nframework designed to streamline the open-vocabulary\nchange detection task. The framework employs the Syner-\ngistic Fusion to Instance Decoupling (SFID) strategy to fully\nleverage the unified architectural strengths of SAM 3. This\ndesign allows OmniOVCD to address the instability often\nfound in traditional pipelined methods.\nExtensive evalua-\ntions across four major benchmarks show that OmniOVCD\nnot only achieves state-of-the-art performance but also main-\ntains strong stability and generalization in complex open-\nworld scenarios. We believe this framework establishes a ro-\nbust and efficient foundation for future advancements in in-\ntelligent remote sensing analysis.\nReferences\n[Asadzadeh et al., 2022] Saeid\nAsadzadeh,\nWilson\nJos\u00b4e\nde Oliveira, and Carlos Roberto de Souza Filho.\nUav-\nbased remote sensing for the petroleum industry and\nenvironmental monitoring: State-of-the-art and perspec-\ntives.\nJournal of Petroleum Science and Engineering,\n208:109633, 2022.\n[Bandara and Patel, 2022] Wele Gedara Chaminda Bandara\nand Vishal M Patel. A transformer-based siamese network\nfor change detection. In IGARSS 2022-2022 IEEE Interna-\ntional Geoscience and Remote Sensing Symposium, pages\n207\u2013210. IEEE, 2022.\n[Bovolo and Bruzzone, 2006] Francesca\nBovolo\nand\nLorenzo Bruzzone. A theoretical framework for unsuper-\nvised change detection based on change vector analysis in\nthe polar domain. IEEE Transactions on Geoscience and\nRemote Sensing, 45(1):218\u2013236, 2006.\n[Carion et al."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper \"OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3\"\n\n#### Main Contributions\nThe paper introduces OmniOVCD (Open-Vocabulary Change Detection), a novel framework designed to streamline open-vocabulary change detection tasks. The key contributions are:\n1. **Proposing OmniOVCD**: A standalone framework for open-vocabulary change detection based on Segment Anything Model 3 (SAM 3).\n2. **Introducing SFID Strategy**: A synergistic fusion strategy that decouples instance masks and fuses semantic, instance, and presence outputs.\n3. **Achieving Superior Performance**: Demonstrates state-of-the-art performance across multiple benchmarks including LEVIR-CD, WHU-CD, S2Looking, and SECOND.\n\n#### Methodology\nThe paper leverages SAM 3's unified architecture to simplify the open-vocabulary change detection process:\n1. **Unified Architecture**: Utilizes SAM 3\u2019s decoupled output heads for semantic, instance, and presence predictions.\n2. **Synergistic Fusion (SFID)**: Combines these outputs to generate high-fidelity land-cover masks, then decomposes them into individual instance masks for comparison.\n\n#### Key Findings\n1. **Improved Accuracy**: OmniOVCD achieves IoU scores of 67.2, 66.5, and 24.5 across LEVIR-CD, WHU-CD, and S2Looking benchmarks respectively.\n2. **Stability Across Datasets**: Demonstrates consistent performance across different datasets (LEVIR-CD, WHU-CD), outperforming previous methods including DynamicEarth configurations.\n\n#### Conclusions\nOmniOVCD addresses the instability often found in traditional pipelined change detection methods by leveraging SAM 3's unified architecture. The framework not only achieves state-of-the-art performance but also maintains strong stability and generalization capabilities across complex open-world scenarios, establishing a robust foundation for future advancements in intelligent remote sensing analysis.\n\n### Detailed Breakdown\n\n#### Introduction\nThe paper discusses the fundamental task of change detection (CD) in remote sensing and introduces Open-Vocabulary Change Detection (OVCD), which leverages language guidance to detect changes without predefined categories. It highlights the limitations of traditional CD methods, particularly their inability to handle unseen categories effectively.\n\n#### Related Work\n1. **Segment Anything Model (SAM)**: SAM establishes a promptable paradigm for zero-shot image segmentation and has been extended to temporal contexts.\n2. **Traditional Change Detection**: Early methods rely on siamese architectures or transformer-based architectures like BiT, ChangeFormer, and Changer. Recent advancements include feature interaction and fusion techniques.\n3. **Open-Vocabulary Change Detection (OVCD)**: Works with pre-trained foundation models such as CLIP and SAM to achieve zero-shot change detection.\n\n#### Methods\n1. **Task Definition**: Defines the OVCD task of identifying changes between two images using a text prompt, xtxt.\n2. **SFID Pipeline**: Consists of Synergistic Mask Fusion (SMF) and Instance Decoupling and Matching (IDM). SMF fuses multi-head outputs from SAM 3 to generate high-fidelity semantic masks, while IDM decouples these into instance masks for comparison.\n\n#### Experiments\n1. **Experimental Setting**: Uses NVIDIA GeForce RTX 3090 GPU with SAM 3 as the foundational model.\n2. **Datasets**: Evaluates on four benchmarks: LEVIR-CD (building change detection), WHU-CD (urban change detection), S2Looking (satellite change detection in rural areas), and SECOND (urban/suburban category changes).\n3. **Evaluation Metrics**: Uses Intersection over Union (IoU) and F1 Score to measure performance.\n4. **Compared Methods**: Compares with unsupervised methods like PCA-KM, CNN-CD, DSFA, DCVA, GMCD, CVA, AnyChange, SCM, and the state-of-the-art DynamicEarth framework.\n\n#### Results\nOmniOVCD achieves superior IoU scores across all benchmarks (67.2, 66.5, 24.5) with consistent performance across different datasets, outperforming previous methods including DynamicEarth configurations.\n\n#### Conclusion\nThe paper concludes by highlighting the robustness and efficiency of OmniOVCD in handling complex open-world scenarios. It establishes a solid foundation for future advancements in intelligent remote sensing analysis."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Automatic Prompt Optimization for Dataset-Level Feature Discovery\nAdrian Cosma1 , Oleg Szehr1 , David Kletz1 , Alessandro Antonucci1\nOlivier Pelletier2\n1 SUPSI, Dalle Molle Institute for Artificial Intelligence Studies (IDSIA)\n2UBS Switzerland AG and its affiliates\n{adrian.cosma, oleg.szehr, david.kletz, alessandro.antonucci}@supsi.ch\nolivier.pelletier@ubs.com\nAbstract\nFeature extraction from unstructured text is a critical\nstep in many downstream classification pipelines,\nyet current approaches largely rely on hand-crafted\nprompts or fixed feature schemas. We formulate\nfeature discovery as a dataset-level prompt optimiza-\ntion problem: given a labelled text corpus, the goal\nis to induce a global set of interpretable and dis-\ncriminative feature definitions whose realizations\noptimize a downstream supervised learning objec-\ntive. To this end, we propose a multi-agent prompt\noptimization framework in which language-model\nagents jointly propose feature definitions, extract\nfeature values, and evaluate feature quality using\ndataset-level performance and interpretability feed-\nback. Instruction prompts are iteratively refined\nbased on this structured feedback, enabling opti-\nmization over prompts that induce shared feature\nsets rather than per-example predictions. This for-\nmulation departs from prior prompt optimization\nmethods that rely on per-sample supervision and\nprovides a principled mechanism for automatic fea-\nture discovery from unstructured text.\n1\nIntroduction\nContemporary business organizations across industries, in-\ncluding sectors such as finance [Aguda et al., 2024], health-\ncare [Garg et al., 2025], and retail [Brinkmann et al., 2024;\nSinha and Gujral, 2024], are increasingly relying on data-\ndriven decision-making to enhance operational efficiency, im-\nprove customer experiences, and drive competitive advantage.\nA significant portion of the valuable insights required for these\ndecisions is often embedded in text corpora comprised of,\nfor example, customer reviews, support tickets, clinical notes,\nincident reports, and internal documentation. Converting free-\nform content into structured representations is a crucial step\ntowards downstream analytics and decision-making, where\nrecent advancements in Language Models (LMs) have signif-\nicantly enhanced the feasibility of this task [He et al., 2024;\nGilardi et al., 2023; T\u00f6rnberg, 2023].\nHowever, even if LMs can be successfully used to extract a\nknown set of features from free text, a key question remains\nlargely unexplored: for a given downstream supervised learn-\ning (SL) task (i.e., a labelled text corpus), how can one identify\nwhich are the best features that should be extracted to opti-\nmize a chosen target metric? We distinguish feature extraction\n(extracting predefined attributes) from feature discovery (iden-\ntifying which attributes are worth extracting in the first place).\nThis situation frequently arises in business applications, where\nfeature extraction by LMs primarily serves to inform a down-\nstream system for decision-making [Herhausen et al., 2025].\nFurthermore stakeholders themselves often define features\nbased on organizational or regulatory needs. These exogenous\nfeature definitions, driven by non-technical considerations, are\ntypically suboptimal for SL. Motivated by these points, this\nwork places feature discovery itself at the center of the learn-\ning process, identifying features to optimize the performance\nof the downstream SL system.\nIn this work, we frame feature discovery as a prompt opti-\nmization problem [Khattab et al., 2024; Pryzant et al., 2023;\nYuksekgonul et al., 2025]. Instead of relying on hand-crafted\nfeatures or prompt heuristics, we present a procedure to re-\nfine prompts to guide LMs to propose and extract features\nof increasing quality. In order to propose a relevant list of\ndomain-specific features, the model must account for the en-\ntire corpus, and since it is often impractical to include the full\ndataset in a prompt decisions must be based on a subset of\ntext samples and a specific instruction set. This leads to a key\nquestion: How can one determine the most effective set of\ninstructions that grounds the resulting feature definitions in\nthese selected examples?\nWhile Large LMs (LLMs) excel at structured extraction,\ntheir computational cost makes iterative, dataset-level opti-\nmization impractical in many settings [Wang et al., 2025;\nGilardi et al., 2023]. In contrast, Small LMs (SLMs) are more\nappropriate for this task than LLMs due to their significantly\nlower computational cost, faster inference, and their ability to\nrun locally [Belcak et al., 2025]. Building on prior work on the\njoint optimization of instructions and demonstrations [Opsahl-\nOng et al., 2024], we introduce a prompt optimization algo-\nrithm for feature discovery. In contrast to existing prompt opti-\nmization algorithms such as MIPRO [Opsahl-Ong et al., 2024]\nand GEPA [Agrawal et al., 2025], which optimize prompts us-\ning per-sample evaluations, our approach derives features from\nfeedback computed at the dataset level. As such, we employ a\ncoordinated system of LLM agents for feature discovery, as\narXiv:2601.13922v1  [cs.CL]  20 Jan 2026\nFigure 1: Diagram of proposed LM agents for instructions optimization. (1) The FEATUREPROPOSER module outputs a set of feature definitions,\nbased on the current prompt and a set of examples. (2) Based on provided definitions, an EXTRACTOR is instantiated which extracts text features\nacross the entire dataset and are evaluated. (3) Textual feedback is gathered by INTERPRETABILITYSCORER and PERFORMANCEFEEDBACK\nmodules. (4) Based on the feedback, new instructions for the FEATUREPROPOSER are proposed by the REFLECTIVEPROPOSER.\nshown in Figure 1: a FEATUREPROPOSER suggests feature\ndefinitions, an EXTRACTOR extracts features from the text\ngiven their definitions, while the INTERPRETABILITYSCORER\nevaluates the interpretability of these features. Instructions are\nrefined by a REFLECTIVEPROPOSER module through textual\nfeedback from a PERFORMANCEFEEDBACK module. These\nmechanisms, optimized based on downstream task perfor-\nmance, enable the system to propose, extract, and evaluate fea-\ntures effectively \u2014 without relying on hand-crafted heuristics.\nImportantly, this agentic approach is specifically designed to\nempower the use of SLMs, enabling efficient feature discovery\neven with more limited model capacity.\nIn summary, this paper makes the following main contribu-\ntions:\n1. We frame feature discovery from unstructured text as\na dataset-level prompt optimization problem, where\nprompts induce a shared set of features optimized for\na downstream classifier rather than per-instance predic-\ntions.\n2. We propose a multi-agent prompt optimization algorithm\nfor feature discovery that jointly optimizes instruction\nprompts and example selection using dataset-level feed-\nback, enabling interpretable and discriminative feature\ndiscovery.\n3. We empirically show that our prompt optimization\nmethod surpasses previous LM-based feature discovery\napproaches that rely on hand-crafted prompts, and current\nprompt optimizers such as MIPRO that were designed\nfor per-example feedback.\n2\nRelated Work\nOur approach broadly falls into the category of AutoML [He\net al., 2021] as a hands-off approach to feature discovery.\nWhile the purpose of AutoML is \"enabling domain experts\nto automatically build applications without machine learning\nknowledge\" [Z\u00f6ller and Huber, 2021], our approach is more\nrelated to the area of knowledge engineering [Allen et al.,\n2023]. We further contextualize our approach in terms of\nLLM-based feature extraction and discovery, and automatic\nprompt optimization.\n2.1\nText Annotation using LMs\nThe task of text annotation has been studied extensively\nacross various disciplines, including finance [Aguda et al.,\n2024], retail business [Brinkmann et al., 2024; Sinha and\nGujral, 2024], healthcare [Bolton et al., 2024; Li et al.,\n2024], including psychology [Hassan et al., 2024; Liu et al.,\n2025] and telemedicine [Niculae et al., 2025], and others.\nWhile some scepticism remains regarding the reliability of\nLLM-extracted features [Reiss, 2023], multiple studies re-\nport strong performance, often surpassing human annotators\nin specific contexts [He et al., 2024; Gilardi et al., 2023;\nT\u00f6rnberg, 2023]. For instance, ChatGPT has been shown to\noutperform crowd workers in certain annotation tasks [Gilardi\net al., 2023]. Additionally, the development of structured\ngeneration paradigms [Willard and Louf, 2023] has further\nimproved the reliability of LLM-generated annotations. In our\ncase, these results provide motivating arguments for our use\nof LLMs to reliably extract features from texts.\n2.2\nFeature Discovery using LMs\nRecent works [Balek et al., 2025; Zhou et al., 2024; Zhang\net al., 2025] showed that LLMs can propose meaningful fea-\ntures from unstructured text, by leveraging LLMs\u2019 linguistic\nunderstanding to identify task-specific characteristics. For\nexample, [Balek et al., 2025] used LLMs to propose inter-\npretable features yielding representations that enable effective\nrule learning. However, the authors did not use any form of\nfeedback to inform the prompt on the resulting features and\nrely on hand-crafted prompts. Similarly, [Zhou et al., 2024]\nused LLMs to generate utterrance-level and dialogue-level\nfeatures for dialogue constructiveness assessment, which are\nthen evaluated with various statistical tools. Like [Balek et al.,\n2025], their pipeline is not designed for iterative feedback on\nthe proposed features, but rather is constructed upon a single\nLLM call with hand-crafted prompts. Instead, we propose\na prompt optimization algorithm to iteratively discover the\nmost appropriate prompt to generate interpretable and dis-\ncriminative features. [Zhang et al., 2025] used a multi-agent\ntopology to perform feature engineering on an existing feature\nset based on the performance on a downstream task. However,\nthe authors focus on tabular datasets having an already existing\nfeatures, whereas we are targeting unstructured data and cre-\nating features de novo. Distinct from these prior approaches,\nour method operates on unstructured text and introduces an\nautomatic prompt optimization algorithm that searches for the\nbest feature set using feedback scores computed across the\nentire dataset.\n2.3\nPrompt Optimization\nText-based signals convey more detailed information than a\nsingle scalar reward, which can be leveraged to provide a\nmore effective signal to the learning system, thereby enabling\nfaster and more efficient learning by guiding agents more\nprecisely [Agrawal et al., 2025; Lee et al., 2025]. Thus, Auto-\nmatic Prompt Optimization (APO) has emerged as a promising\nalternative to maximize task performance of instruction-tuned\nLMs [Zhou et al., 2022; Pryzant et al., 2023; Yang et al., 2023;\nAgrawal et al., 2025; Yuksekgonul et al., 2025; Xu et al., 2025;\nLee et al., 2025]. While manual prompt engineering of LMs\ncan be a time-consuming and error prone process, the LMs\u2019\nability to understand an manipulate language enables the\nuse of LMs themselves as prompt optimizers [Yang et al.,\n2023]. In literature, multiple prompt optimizers have been\nproposed, for example ProTeGi [Pryzant et al., 2023], ORPO\n[Yang et al., 2023], MIPRO [Opsahl-Ong et al., 2024], GEPA\n[Agrawal et al., 2025], et alia, some implemented in frame-\nworks such as DSPy [Khattab et al., 2024]. Some works\nsuch as MIPRO [Opsahl-Ong et al., 2024] optimize both in-\nstructions and demonstrations, bootstrapping few-shot exam-\nples by uniformly selecting examples from the training set.\nIn our formulation, we do not have \"demonstrations\" in the\nsense of inputs and outputs, but rather a set of texts from the\ndataset. Further, compared to other approaches, we formulate\nour prompt optimization procedure in terms of a dataset-level\nfeedback signal.\n3\nMethod\n3.1\nProblem Definition\nOur objective is to optimize the performance of an SL task,\nwhere we are given a labelled text corpus, D := {(ti, yi)}n\ni=1,\nwhere yi is the label assigned to the text ti. First, we identify\na set of feature definitions, f := (f1, . . . , fk) that are both\ninterpretable and discriminative with respect to the labels in D.\nThe number of features k can either be chosen as a hyperpa-\nrameter or can be automatically decided by our system. Given\nthese feature definitions, an extraction procedure maps each\ntext ti to a structured representation (i.e., feature realisations)\nwhich is then used as input to a downstream SL model.\nTo identify f, the LLM must take account of an instruction\ni and a list of examples d := (d1, ..., dl) \u2286D, where l is a\nhyper-parameter. Formally, we write f = FP(\u03d5), where FP is\na LM module taking the role of the FEATUREPROPOSER and\n\u03d5 := (i, d) represents the prompt comprised of an instruction\nand examples. Importantly, the feature definitions f are global\nobjects defined for the entire corpus and not at the level of the\nsingle text ti.\nFurther, we introduce an EXTRACTOR module E(\u00b7, f),\nwhich maps each input ti together with the feature definitions\nto the realized features \u02c6fi := E(ti, f). The quality of pro-\nposed features is evaluated using a downstream global evalua-\ntion function M. For prompt optimization, this yields an opti-\nmization target of the form J(\u03d5) := M({( \u02c6fi, yi)}n\ni=1), where\nwe seek the best prompt \u03d5\u2217such that \u03d5\u2217:= arg max\u03d5 J(\u03d5).\nThis differs from standard SL in that optimization is performed\nover prompts that induce a global feature set, and the objec-\ntive J(\u03d5) provides only dataset-level feedback, without per-\nexample or per-feature supervision.\n3.2\nThe Optimization Process\nCurrent prompt optimizers such as MIPRO [Opsahl-Ong et al.,\n2024] or GEPA [Agrawal et al., 2025] that are currently im-\nplemented within the DSPy framework [Khattab et al., 2024],\nexpect that each module obtains a separate feedback for each\nprediction (one-to-one feedback). We propose, instead, a\ndataset-level feedback setup in which feedback is provided on\na value computed across a set of elements \u2013 in our case, the F1\nscore of a linear classifier, alongside feature importance scores\nobtained through SHAP [Lundberg and Lee, 2017], mutual\ninformation and feature coverage scores.1 Such an evaluation\nprocedure is outlined in Algorithm 1. Algorithm 2 provides\na high-level overview of our proposed prompt optimization\napproach.\nOverall, we can summarize our algorithm as follows:\n1. Gather several examples sets by uniformly sampling texts\nfrom D to allow the model to ground its proposals into\nconcrete examples.\n2. Propose several sets of instructions using a REFLEC-\nTIVEPROPOSER that incorporates text feedback on in-\nterpretability and feature set performance from an initial\nprompt.\n3. Find the best combination of example sets and instruc-\ntions using Bayesian optimization [Akiba et al., 2019],\nscoring each resulting feature definition on a dedicated\n\"annotation\" dataset DAnnotation.\nThe algorithm, conceptually similar to MIPRO [Opsahl-\nOng et al., 2024] as it systematically searching for the best\ncombination of instructions and example sets, requires having\nadditional modules to account for set-level feedback. We\nfurther provide additional details into each LM module.\n3.3\nModules Used for Prompt Optimization\nFEATUREPROPOSER. This is the main module that follows\nthe instructions and examples in the prompt and outputs a\nlist of feature definitions, consisting of a snake_case name, a\nPython type (bool, int, float or Literal with associated values),\na description and an extraction prompt that is utilized by the\nEXTRACTOR module. The prompt of this module is the only\none being optimized.\nEXTRACTOR. This module is instantiated based on the feature\ndefinitions returned by the FEATUREPROPOSER and is tasked\n1Note that the use of an external computation through training\nthe linear model / obtaining SHAP scores, aligns with the \"executor\"\nblock of the topology primitives proposed by [Zhou et al., 2025].\nto extract feature values from an input text. This module is\npart of the evaluation.\nINTERPRETABILITYSCORER.\nThis module assigns an\ninterpretability score to each feature [Zytek et al., 2022;\nMolnar, 2020]. In particular, we follow [Zytek et al., 2022],\nwhich identified several characteristics of interpretable fea-\ntures: a feature needs to be readable, human-worded, under-\nstandable, meaningful and trackable.\nOne potential caveat of our optimization setup is that one of\nthe proposed features could simply be another way of express-\ning the target label, essentially offloading the classification to\nthe EXTRACTOR module instead of the linear classifier. For\nexample, in a text sentiment classification scenario, one of\nthe proposed features could be \"overall_sentimentcategorical\",\nwhich, when extracted by the EXTRACTOR module, will result\nin high accuracy by the linear classifier. This is, of course,\nundesirable as this feature does not provide any useful struc-\ntured information of the underlying texts. Consequently, this\nmodule also severely penalizes features which are leaking\nthe label in the way we described above. This aspect is not\naddressed in previous works for automatic feature discovery\n[Balek et al., 2025]. The module is also instructed to assign\nlow scores to unclear features names or opaque descriptions\nand assigning higher scores to features grounded in the source\ntext. Furthermore, this module also outputs an additional text\nfeedback on the interpretability of the whole feature set used\nlater on by the REFLECTIVEPROPOSER (detailed below). We\nuse the interpretability score as additional \"regularization\"\non the feature set and combine it with the performance score\nthrough a hyperparameter \u03bb.\nPERFORMANCEFEEDBACK. This module provides textual\nfeedback on the overall classifier performance of the proposed\nfeature set. As opposed to the INTERPRETABILITYSCORER,\nthis module operates on dataset-level measures (e.g., the F1\nscore, SHAP scores).\nREFLECTIVEPROPOSER. This module proposes a set of\ninstructions based on a summary of the data and a summary\nof the current instructions, text feedbacks from the INTER-\nPRETABILITYSCORER and the PERFORMANCEFEEDBACK\nmodules to generate instruction suggestions - similar in spirit\nto GEPA [Agrawal et al., 2025] and TextGrad [Yuksekgonul et\nal., 2025]. A version without incorporating text feedback cor-\nresponds to the GROUNDEDPROPOSER implemented in DSPy\n[Khattab et al., 2024] / MIPRO [Opsahl-Ong et al., 2024].\nNote that this step can be performed for multiple iterations, ob-\ntaining more refined prompts across multiple feedback rounds.\n3.4\nOverview\nOur proposed modules are designed to capture the essential\naspects of a downstream classification task. Following [Zhou\net al., 2025], at each stage of optimization process additional\nhelper modules can be incorporated that can enhance the qual-\nity of the feedback and prompt proposals without fundamen-\ntally changing the agent topology. For example performing\nmultiple rollouts, summarizing, reflection or multi-agent de-\nbate [Chern et al., 2024]. Furthermore, while the we optimize\nthe instructions and input examples only for the FEATUREPRO-\nPOSER module, the other helper modules have their prompts\nfixed. Presumably, their instructions could also be optimized\nAlgorithm 1 Evaluation of feature definitions.\nRequire: DAnnotation = {(ti, yi)} - the annotation dataset; f - proposed feature\ndefinitions; EXTRACTOR, INTERPRETABILITYSCORER, PERFORMANCEFEEDBACK\nmodules.\nfunction EVALUATE(DAnnotation, f)\nfor i in |DAnnotation| do\nz(i) \u2190EXTRACTOR(ti, f)\n\u25b7Extract features from each example ti\nend for\nZ := {(z(i), y(i))\n|DAnnotation|\ni=1\n}\nSplit annotated dataset intro training and validation Ztrain, Zval.\nTrain a simple linear classifier on the dataset Ztrain.\nCompute F1 score on Zval\nCompute mutual information (MI), SHAP scores and coverage on Zval.\n\u03c4p \u2190PERFORMANCEFEEDBACK(F1, SHAP, MI, Coverage)\n\u25b7Get text\nfeedback on the performance with the computed scores.\nsI, \u03c4I \u2190INTERPRETABILITYSCORER(F\u03d5)\n\u25b7Get an interpretability score\nper feature and text feedback for interpretability.\nreturn (\u03bbF1 + (1 \u2212\u03bb)sI, \u03c4I + \u03c4p) \u25b7Return the combined score and textual\nfeedback.\nend function\nAlgorithm 2 Finding the best instructions and sets of examples\nfor downstream classification.\nRequire: DTrain- the training dataset of texts with their classification label; DAnnotation -\nthe annotation dataset; \u03a0\u03d5 the FEATUREPROPOSER module with initial prompt \u03d5;\nNd number of example sets; l - number of examples per example set; Niter - number\nof optimization iterations; Nfb - number of iterative prompt refinements.\n\u25b7Step 1: Sample example sets and generate instruction proposals\nUniformly sample Nd example sets {(d1\n1, . . . , d1\nl ), (d\nNd\n1\n, . . . , d\nNd\nl\n)} from\nDTrain.\nI \u2190\u2205, \u03d50 \u2190\u03d5\nfor i = 1 to Nd do\n\u25b7Evaluate each example set D to get scores si and text\nfeedbacks \u03c4i using Algorithm 1.\nfor r = 1 to Nfb do\n\u25b7Iteratively refine instructions\n\u03d5r \u2190(\u03d5r\u22121, (di\n1, . . . , di\nl))\nf \u2190FP(\u03d5r)\nsi, \u03c4i \u2190EVALUATE(DAnnotation, f)\n\u03d5r \u2190REFLECTIVEPROPOSER(\u03d5r, Di, si, \u03c4i)\n\u25b7Generate a set\nof instruction candidates based on the current prompt, sampled examples set, and\nscores and feedbacks.\nend for\nI \u2190I \u222a\u03d5Nfb\nend for\n\u25b7Step 2: Bayesian optimization over instructions and examples sets to find the\nbest combination.\nsbest \u2190\u2212\u221e, \u03d5best \u2190\u2205\nInitialize Bayesian-optimization\u2013based categorical distribution over (I, d) pairs.\nfor k = 1 to Niter do\nSample instruction Ik \u2208I and example set Dk. \u25b7Categorical sampling with\nprobabilities updated by Bayesian optimization over past scores.\n\u03d5k \u2190(Ik, Dk)\n\u25b7Build prompt with instructions and example sets\nf \u2190FP(\u03d5k)\n\u25b7Propose features under this prompt.\nsk \u2190EVALUATE(DAnnotation, f)\nif sk > sbest then\nsbest \u2190sk, \u03d5best \u2190\u03d5k\nend if\nend for\nreturn \u03d5best, sbest\nusing, for example, the approach from [Xu et al., 2025].\n3.5\nAsymptotic Analysis of the Language-based\nProgram\nWe analyze our multi-agent prompt optimizer as a Language-\nbased Algorithm (LbA), as proposed by [Meyerson and Qiu,\n2025]. In this lens, we derive the asymptotic analysis in terms\nof LM primitives, where the fundamental unit of cost is an\nLM primitive: Following [Meyerson and Qiu, 2025], an LM\nprimitive is a forward-pass token generation whose cost scales\nas mn for model size m and input length n.\nFor each candidate prompt \u03d5, the system performs a single\nFEATUREPROPOSER call to produce a global feature set f,\nNA = |DAnnotation| calls to the EXTRACTOR to realize feature\nvectors across the annotation split, and O(1) additional LLM\ncalls to PERFORMANCEFEEDBACK and INTERPRETABILI-\nTYSCORER to produce dataset-level textual feedback and an\ninterpretability (Algorithms 1 and 2). Let Lt denote the aver-\nage token length of an input text and Lf the token length of the\nserialized feature schema; and let mFP, mE, mS be the model\nsizes used for proposing, extracting, and scoring, respectively.\nUnder the primitive cost model, one evaluation has cost:\nCeval := O(mFPL\u03d5) + O(NA mE(Lt + Lf)) + O(mSLf)\n(1)\nwhere the second term dominates. We discard additional\ncosts of the non-LM \"executor\" computations (i.e., training the\nclassifier and computing SHAP / MI / coverage scores), since\ncosts are dominated by the LM inference. Since Algorithm 1\nevaluates \u0398(Nd + Niter) candidate (instruction, example sets)\npairs, the total optimization cost is O((Nd+Niter) Ceval). Con-\nsequently, the computational cost is asymptotically dominated\nby:\nO((Nd + Niter) NA mE(Lt + Lf)) .\n(2)\nThis highlights the fact that the end-to-end cost is primarily\ngoverned by extraction throughput over DAnnotation, suggest-\ning that practical scaling should preferentially minimize the\nEXTRACTOR primitive via smaller specialist models, while\nreserving larger-capability models for feature proposal and\nreflection.\n4\nExperimental Setup\nBackbone SLMs.\nWe experiment with several popular open-\nweight SLMs, covering three model families and four scales:\nQwen3-4B and Qwen3-14B [Yang et al., 2025], Llama-8B\n[Grattafiori et al., 2024] and Gemma3-27B [Gemma Team et\nal., 2025]. We use Chain-of-Thought prompting [Kojima et al.,\n2022] for all modules except for the EXTRACTOR module. We\nuse the same underlying LM as the backbone for all modules,\na common scenario when computational resources are limited\nand do not allow multiple vLLM servers to operate in parallel.\nDatasets.\nWe use two financial news datasets annotated with\nsentiments from Twitter2 (bearish, neutral, bullish classes)\nand Yahoo Finance3 (positive, neutral, negative classes). Sen-\ntiment classification from financial text is a well-established\nSL benchmark that requires grounding predictions in fine-\ngrained lexical cues and domain-specific jargon. This makes\nit a suitable test-bed for feature discovery and extraction meth-\nods that aim to identify interpretable, label-discriminative\ntextual properties. Generic sentiment datasets such as IMDB\nreviews [Maas et al., 2011] are not well suited to our setting, as\nsentiment is largely expressed through explicit polarity words,\nlimiting the need for discovering richer, domain-specific fea-\ntures.\n2hf.co/datasets/zeroshot/twitter-financial-news-sentiment, Ac-\ncessed 19 Jan., 2026\n3hf.co/datasets/ugursa/Yahoo-Finance-News-Sentences,\nAc-\ncessed 19 Jan., 2026.\nWe additionally evaluate on ToxicChat [Lin et al., 2023], a\nbenchmark for toxicity detection in real-world user-assistant\nconversations. This task reflects a core challenge in contem-\nporary LM-based systems, where moderation and safety are\nessential for deployment. ToxicChat exhibits linguistic prop-\nerties distinct from social-media toxicity datasets, including\nimplicit and adversarial prompts, making it a complementary\nsetting to assess the performance of our procedure. We used\nthe \"input-only\" setup of the toxicity subset: the model re-\nceives as input only the prompt and not the resulting assistant\noutput and is tasked to determine whether the prompt is toxic\nor non-toxic.\nImplementation Details.\nWe use DSPy to implement the\noptimization algorithm in the framework\u2019s formalisms, and\noptuna [Akiba et al., 2019] for Bayesian optimization of the\nbest combination of prompt and example sets. Compared to\nother approaches [Balek et al., 2025; Zhang et al., 2025], we\nuse structured generation [Willard and Louf, 2023] at each\nstep of the pipeline. We use a temperature of 0.75 and nucleus\nsampling probability [Holtzman et al., 2020] of 0.95 for the\nFEATUREPROPOSER module, and greedy sampling for all\nother modules. Context length of the models is set to 16\u2019000\ntokens. Experiments are performed using a vLLM server\n[Kwon et al., 2023] on a cluster 2xA100 GPUs.\nWe use a training set DTrain consisting of 16 examples per\nclass, from which examples are sampled, an annotation set\nDAnnotation consisting of 512 total examples. These sets are\nfixed for all experiments. Unless otherwise specified, we\nused Nd = 16 examples sets; note that these examples are\nnot LM \"demonstrations\" [Opsahl-Ong et al., 2024] which\ninclude inputs and desired program outputs, but simply a set\nof texts and their classification label from the dataset on which\nto ground feature definitions. Each example set consists of\nl = 16 examples, and we set the number of feedback rounds\nNfb = 1, by default. The FEATUREPROPOSAL module is\ninitially prompted to output between 5 and 10 features. We\nuse the F1 score of a logistic regression model trained on the\nextracted features for performance measurement. We use \u03bb =\n0.75 to weigh the contribution of interpretability estimates in\nthe final scores during optimization.\n5\nResults\nMain Results.\nIn Figure 2 we compare our REFLECTIVE-\nPROPOSER which uses text feedback for refinement with\nvariants using scalar feedback, which amounts to rephrasing of\nthe original prompt and inclusion of general domain-specific\ninsights. The REFLECTIVEPROPOSER offers more specific\nfeedback based on the global performance of resulting feature\nset. The \"unoptimized\" version roughly corresponds to the\nhand-crafted prompt from [Balek et al., 2025], which is the\ninitial seed prompt for the FEATUREPROPOSER. For some\nmodels, the inclusion of interpretability estimates sometimes\nreduces performance, since optimizing only for the final F1\nscore can allow the model to \"cheat\" by proposing features\nthat are the same as the target label. As such, in some cases,\nthe high performance of the F1-only scoring is artificial and\ndue to label leakage. For example, on Yahoo and Twitter,\ncommon such \"leaky\" features are sentiment_labelcategorical or\nFigure 2: Comparison between REFLECTIVEPROPOSER and variants using scalar feedback, across Yahoo and Twitter News, for four pretrained\nSLM models. Optimizing for F1 scores only can lead to feature leakage, artificially increasing performance, while regularizing through\ninterpretability feedback can result in more grounded features.\nlabel_sentiment_typecategorical which prompt the EXTRACTOR\nto directly classify the text. In contrast, our the addition\nof the REFLECTIVEPROPOSER improves upon F1 and\ninterpretability resulting in more grounded features. Examples\nof proposed features include:\neconomic_eventcategorical,\njob_loss_indicatorBool,\ncertainty_measurecategorical,\nau-\nthority_statementBool,\ncomparative_language_usageBool,\ncorporate_announcement_typecategorical,\nrisk_assessment_tonecategorical.\nIn Figure 3 we show results on the ToxicChat dataset for\ntwo backbone SLMs, comparing the unoptimized prompt\nwith our variant, the latter resulting in consistently more\ndiscriminative features. Examples of proposed features by\nour system for ToxicChat are: roleplay_instructionBool, ma-\nnipulative_languageBool, sexual_content_presentBool, profan-\nity_frequencyfloat, while \"leaky\" features that were filtered are:\ntopic_sensitivitycategorical, safety_risk_scorefloat, among others.\nReaders are referred to the Supplementary Materials for initial\nagent signatures, additional examples of proposed features,\nand examples of optimized prompts.\nFigure 3: Evaluation on ToxicChat dataset, considering the input-only\ntoxicity subset.\nComparison with Per-Example Feedback.\nTo make our\nsetup compatible with a per-example feedback mechanism,\nwe introduce a PREDICTOR LM module which simply takes\nas input the feature values (without the original input text)\nand is tasked to directly classify the instance in a zero-shot\nmanner, with no information about feature values for other\nexamples in the dataset (see Figure 1, panel 2). While it is\nclearly not ideal, it enables us to directly use already available\nprompt optimizers from DSPy, namely MIPRO [Opsahl-Ong\net al., 2024]. We emphasize that optimizers such as MIPRO\nare not appropriate for this setting for the following reasons:\n(i) there is no concept of \"example set\" and thus each input\ntext will result in a different feature set; (ii) in-context demon-\nstrations implicitly require an input and its desired output, and\nthus demonstrations will contain texts and a different feature\nset per text; (iii) the evaluation signal during optimization is\nan approximation of the true evaluation function J through\nthe PREDICTOR module, which does not account for global\nfeature realisations. Consequently, retrofitting such existing\noptimizers for this task is not appropriate. In Table 1 we show\nthe comparison between MIPRO and our dataset-level opti-\nmization procedure using Qwen-14B as a backbone LM for\nToxicChat.\nThe Effect of Increasing Number of Example Sets.\nIn\nFigure 4 we show the evolution of performance when increas-\ning the number of examples sets Nd, thereby increasing the\nsearch space. We heuristically set Niter = max(N 2\nd, 128).\nResults show that increasing the number of example sets gen-\nerally improves performance, particularly for smaller values\nof Nd. With few example sets, prompt proposals are strongly\ninfluenced by the specific sampled texts, leading to higher\nvariability in the induced feature sets. As Nd increases, the\nFEATUREPROPOSER is exposed to a broader range of ex-\namples, resulting in more stable and discriminative features.\nHowever, gains diminish for larger values of Nd, and per-\nformance can slightly degrade when the search space grows\nfaster than the available optimization budget. This suggests a\ntrade-off between dataset coverage and effective exploration\nof the instruction\u2013example space.\nIterative Refinement with REFLECTIVEPROPOSER.\nFig-\nure 5 shows the effect of applying multiple rounds of instruc-\ntion refinement using the REFLECTIVEPROPOSER. Perfor-\nmance improves with additional refinement rounds, with most\nof the gain occurring after the first iteration. Inspecting the\nprompts reveals qualitative differences: the first refinement\nmainly adds coarse, domain-level constraints that are absent\nfrom the initial prompt, while later refinements become in-\ncreasingly specific about what constitutes a useful feature and\nwhat should be avoided. In particular, later iterations more\nTable 1: Comparison with MIPRO, which uses per-example feedback through the PREDICTOR module, on the ToxicChat dataset, having\nQwen3-14B as the underlying LM. Using per-example feedback worsens results compared to the unoptimized version.\nMethod\nFeedback\nF1\nPrecision\nRecall\nUnoptimized\n\u2013\n0.849 \u00b1 0.071\n0.869 \u00b1 0.066\n0.844 \u00b1 0.07\nMIPRO [Opsahl-Ong et al., 2024]\nPREDICTOR\n0.75 \u00b1 0.039\n0.819 \u00b1 0.02\n0.749 \u00b1 0.034\nPREDICTOR & Interp.\n0.796 \u00b1 0.027\n0.844 \u00b1 0.015\n0.791 \u00b1 0.026\nOurs\nREFLECTIVEPROPOSER\n0.907 \u00b1 0.006\n0.916 \u00b1 0.014\n0.905 \u00b1 0.009\nFigure 4: Evolution of accuracy by increasing number of examples\nsets, and, in turn, of prompt proposals, for Qwen-4B and Qwen-14B,\nacross Yahoo News and Twitter News datasets.\nexplicitly discourage overly abstract features and instead em-\nphasize concrete, text-grounded properties.\nFigure 5: Performance evolution with increasing the number of\niterative prompt refinements using the REFLECTIVEPROPOSER on\nthe Yahoo News dataset.\nDiscussion.\nWe also experimented with other smaller mod-\nels such as Llama3-1B and Gemma3-4B. However, they ex-\nhibited unreliable adherence to structured output schemas,\nfrequently producing repetitive and invalid generations (e.g.,\nrepeating indefinitely a space character and not outputing valid\nJSON). To mitigate this, we resorted to the BAML format\n[ML, 2024]; however, this workaround does not guarantee\nstrict schema compliance and is therefore suboptimal. When\nevaluating LLama3-1B, we found it to be poorly suited for\nprompt optimization. The generated prompt refinements were\ntypically generic and weakly grounded in the task, for example\nproducing instructions such as \"Generate a set of features to\npredict company names\" or broadly phrased sentiment anal-\nysis instructions. As a result, the induced feature sets were\nlow quality and incoherent. More generally, smaller models\nexhibited a higher tendency toward hallucinated or underspec-\nified instructions and struggled both to propose meaningful\nfeature definitions and to reliably follow refined prompts."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"Automatic Prompt Optimization for Dataset-Level Feature Discovery\"\n\n#### Main Contributions\n\nThis paper presents an innovative approach to feature discovery from unstructured text data, specifically by formulating it as a dataset-level prompt optimization problem. The key contributions are:\n\n1. **Formulation of Feature Discovery as Prompt Optimization**: The authors propose viewing feature extraction as a multi-agent system where language models (LMs) jointly propose and refine prompts for feature definitions. This approach enables the discovery of interpretable, discriminative features that optimize downstream supervised learning objectives.\n\n2. **Multi-Agent Prompt Optimization Framework**: They introduce a framework involving three main modules: FEATUREPROPOSER, EXTRACTOR, and INTERPRETABILITYSCORER. The FEATUREPROPOSER generates feature definitions based on instructions and example sets provided by the system. The EXTRACTOR extracts features from text using these definitions. Finally, the INTERPRETABILITYSCORER evaluates the interpretability of proposed features.\n\n3. **Dataset-Level Feedback for Optimization**: Unlike traditional methods that rely on per-sample feedback, this framework uses dataset-level performance and interpretability feedback to iteratively refine prompts. This allows optimizing over shared feature sets rather than individual example predictions.\n\n#### Methodology\n\nThe authors detail their approach in several stages:\n\n1. **Feature Definition Identification**: The system identifies a set of feature definitions (f) that are both interpretable and discriminative with respect to the labels in the labelled text corpus D. These feature definitions guide the LMs to propose and extract features from unstructured text.\n\n2. **Extraction Process**: Given these feature definitions, an extraction procedure maps each text ti to a structured representation (feature realizations). This is done by instantiating an EXTRACTOR module that extracts feature values from the input text using the provided feature definitions.\n\n3. **Feedback and Optimization Loop**: The system uses feedback scores computed at the dataset level (e.g., F1 score, SHAP scores) to iteratively refine prompts. This involves:\n   - ReflectivePROPOSER refining instructions based on textual feedback from INTERPRETABILITYSCORER and PERFORMANCEFEEDBACK modules.\n   - Bayesian optimization over combinations of example sets and instructions to find the best set.\n\n4. **Modules and Their Roles**:\n   - **FEATUREPROPOSER**: Generates a list of feature definitions, including snake_case names, Python types (bool, int, float), descriptions, and extraction prompts used by EXTRACTOR.\n   - **EXTRACTOR**: Extracts features from input texts using the defined feature definitions.\n   - **INTERPRETABILITYSCORER**: Assigns interpretability scores to each proposed feature based on criteria such as readability, human-wordedness, and whether they are grounded in source text.\n\n#### Key Findings\n\nThe empirical results demonstrate that their approach outperforms previous methods:\n- The REFLECTIVEPROPOSER module provides more specific feedback compared to scalar feedback variants.\n- Optimizing for F1 scores alone can lead to feature leakage, artificially inflating performance. Regularization through interpretability feedback improves grounded features and reduces such issues.\n\n#### Conclusions\n\nThe authors conclude that their proposed multi-agent prompt optimization framework effectively discovers interpretable and discriminative features from unstructured text data. This approach not only surpasses previous methods but also provides a principled mechanism for automatic feature discovery, especially beneficial for smaller models where structured output schemas are unreliable. The work opens new avenues for improving downstream supervised learning tasks by leveraging LMs in novel ways.\n\n### References\n- [Aguda et al., 2024]\n- [Bolton et al., 2024]\n- [Brinkmann et al., 2024]\n- [Chern et al., 2024]\n- [Gilardi et al., 2023]\n- [Grattafiori et al., 2024]\n- [He et al., 2021]\n- [Hassan et al., 2024]\n- [Herhausen et al., 2025]\n- [Jiang et al., 2023]\n- [Kojima et al., 2022]\n- [Liu et al., 2025]\n- [Meyerson and Qiu, 2025]\n- [Opsahl-Ong et al., 2024]\n- [Pryzant et al., 2023]\n- [Yang et al., 2023]"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "ConceptCaps - a Distilled Concept Dataset for Interpretability in Music Models\nBruno Sienkiewicz1 , \u0141ukasz Neumann1 and Mateusz Modrzejewski1\n1Institute of Computer Science, Warsaw University of Technology\nbruno.sienkiewicz@gmail.com, {lukasz.neumann, mateusz.modrzejewski}@pw.edu.pl,\nAbstract\nConcept-based interpretability methods like TCAV\nrequire clean, well-separated positive and negative\nexamples for each concept. Existing music datasets\nlack this structure: tags are sparse, noisy, or ill-\ndefined. We introduce ConceptCaps, a dataset of\n23k music-caption-audio triplets with explicit la-\nbels from a 200-attribute taxonomy. Our pipeline\nseparates semantic modeling from text generation:\na VAE learns plausible attribute co-occurrence pat-\nterns, a fine-tuned LLM converts attribute lists into\nprofessional descriptions, and MusicGen synthe-\nsizes corresponding audio.\nThis separation im-\nproves coherence and controllability over end-to-\nend approaches. We validate the dataset through\naudio-text alignment (CLAP), linguistic quality\nmetrics (BERTScore, MAUVE), and TCAV anal-\nysis confirming that concept probes recover mu-\nsically meaningful patterns. Dataset and code are\navailable online.\n1\nIntroduction\nWork on interpretability and explainability in music informa-\ntion retrieval has increasingly turned to \u201cconcept-level\u201d anal-\nyses, such as Testing with Concept Activation Vectors (TCAV\n[Kim et al., 2018]), which offers a promising framework\nfor interpreting these models by identifying and measuring\nsensitivity to high-level musical aspects like instrumentation,\ngenre, and mood [Foscarin et al., 2022; Gebhardt et al., 2025;\nAfchar et al., 2022]. In practice, however, these concepts are\nusually inferred from free-form captions, weak tags, or proxy\ntasks that do not provide explicit, validated concept labels.\nThis leads to a gap between the theoretical object of analy-\nsis (a well-defined musical concept) and the empirical evi-\ndence available in existing datasets, which are dominated by\nabstract, overlapping, and sometimes contradictory descrip-\ntions.\nThis gap has concrete consequences. First, it is difficult\nto construct positive and negative example sets that isolate a\nconcept from closely related phenomena, which undermines\nthe design of controlled experiments. Second, comparisons\nacross models and studies are hard to interpret, because each\nwork often relies on ad hoc concept definitions and bespoke\ndata slices. Third, it is unclear how much of a reported \u201ccon-\ncept sensitivity\u201d reflects genuine semantic structure in the\nmodel, and how much is an artefact of annotation noise or\ndataset bias.\nTo address these limitations, we propose a novel dataset\ngeneration pipeline based on separation of concerns. Similar\nmethods often rely on asking a single LLM to both gener-\nate plausible musical attributes and write fluent descriptions.\nWe decompose the task into two specialized stages. First, a\nVariational Autoencoder (VAE) [Kingma and Welling, 2014]\nlearns the distribution of musical attribute co-occurrence\nfrom curated source data, ensuring that sampled combina-\ntions are statistically plausible and musically coherent. Sec-\nond, a fine-tuned language model translates these controlled\nattribute lists into professional, context-aware music descrip-\ntions. This separation reduces hallucination, improves con-\ntrollability, enables independent optimization of semantic\nconsistency and linguistic quality, and yields a dataset explic-\nitly designed to support concept-based interpretability analy-\nses.\n1.1\nMain Contributions\nThe primary contributions of this work are:\n1. We introduce ConceptCaps, a large-scale, copyright-\nfree dataset of 23k music-description pairs with rich,\nvalidated concept labels and matched counterexamples,\nspecifically designed for concept-based explainability\nresearch.\n2. We present a novel two-stage generative pipeline that\nseparates VAE-based semantic consistency from fine-\ntuned LLM linguistic quality, enabling controlled, high-\nquality music dataset generation while maintaining effi-\nciency through local, reproducible processing.\n3. We demonstrate quality and utility using linguistic met-\nrics (BLEU, ROUGE, BERTScore, MAUVE), audio-\ntext alignment (CLAP scores), concept-specific metrics,\nand downstream TCAV analysis demonstrating both\ndataset quality and interpretability.\n4. We show our approach achieves competitive results\nwith significantly lower computational cost than API-\nbased methods [Ouyang et al., 2022], and provides fine-\ngrained control over dataset characteristics through VAE\nlatent space sampling and conditioning.\narXiv:2601.14157v1  [cs.SD]  20 Jan 2026\n2\nRelated Work\n2.1\nConcept-Based Explainability in Deep\nLearning\nConcept-based methods such as Testing with Concept Activa-\ntion Vectors (TCAV) [Kim et al., 2018] have become increas-\ningly popular for interpreting neural networks across vision\nand audio domains. TCAV enables quantitative measurement\nof model sensitivity to user-defined concepts by extracting di-\nrections in neural network activations that correspond to those\nconcepts. The core insight is that understanding decision-\nmaking at the concept level\u2014rather than at the input or neu-\nron level\u2014can provide more intuitive and actionable insights.\nHowever, TCAV\u2019s effectiveness depends critically on hav-\ning clean, well-separated positive and negative concept ex-\namples. When concept examples are noisy, sparse, or poorly\ndefined, TCAV scores become unreliable and difficult to in-\nterpret. This bottleneck has limited the adoption of concept-\nbased analysis in music research, where high-quality con-\ncept datasets are scarce. Our work directly addresses this\nlimitation by providing a systematic, reproducible method to\nconstruct such datasets. Related interpretability approaches\nin audio and music domains have demonstrated the poten-\ntial of concept-level analysis [Elizalde et al., 2023; Zhou\net al., 2022; Gebhardt et al., 2025; Foscarin et al., 2022;\nAfchar et al., 2022].\n2.2\nMusic Dataset Augmentation and Synthesis\nRecent work on music captioning has explored using large\nlanguage models to augment existing music datasets. Ap-\nproaches such as LP-MusicCaps [Doh et al., 2023] and Wav-\nCaps [Mei et al., 2024] generate or refine captions using\nlanguage models, demonstrating the potential of LLM-based\ndata augmentation. While these approaches effectively scale\ndata production, they face two key limitations in the inter-\npretability context:\n1. Lack of Controllability: Generated data follows the bi-\nases of the LLM\u2019s training distribution rather than ex-\nplicitly controlling concept patterns needed for concept\nanalysis.\n2. Limited Semantic Validation: These methods often\nrely on existing datasets that can be noisy and lack sys-\ntematic representation of specific concepts. Without val-\nidation of semantic coherence, it is unclear whether gen-\nerated attributes form meaningful, interpretable patterns.\n3\nMethod\nOur architecture draws inspiration from successful multi-\nstage\ngeneration\napproaches\nin\ncomputer\nvision\nand\nother domains, like StackGAN [Zhang et al., 2017] or\nCLAP [Elizalde et al., 2023]. Our pipeline adopts this prin-\nciple of separation of concerns: the VAE acts as a \u201cStage-I\u201d\ngenerator sketching the semantic skeleton (musical attributes\nand their plausible combinations), while the fine-tuned LLM\nacts as a \u201cStage-II\u201d refiner, constructing descriptions with\nlinguistic detail, professional vocabulary, and contextual nu-\nance.\nStage 1: Semantic\nModeling (VAE)\nLatent Space\nz ~ N(0, I)\nVAE Decoder\nAttribute List\n[folk, guitar, upbeat]\nSample\nGenerate\nStage 2: Text Generation\n(LLM)\nFine-Tuned\nLLM\n(Llama 3.1 8B)\nGenerated Caption\n\"An upbeat folk song\nfeaturing acoustic guitar...\"\nTranslation\nConditioning\nStage 3: Audio Synthesis\nMusicGen\nModel\nSynthetic Audio\n(Copyright-Free)\nSynthesis\nPrompting\nFigure 1: Overview of the three-stage dataset generation pipeline.\nStage 1 (Semantic Modeling via VAE): A Variational Autoencoder\nsamples from the latent space z \u223cN(0, I) and decodes to generate\ncoherent attribute lists (e.g., \u201cfolk, acoustic guitar, upbeat\u201d). Stage 2\n(Text Generation via Fine-Tuned LLM): The fine-tuned language\nmodel (Llama 3.1 8B) receives the attribute list and generates a pro-\nfessional, descriptive music caption capturing the semantic content.\nStage 3 (Audio Synthesis): MusicGen synthesizes copyright-free\naudio from the generated description, completing the dataset sample\nwith well-aligned audio-text pairs and explicit concept labels suit-\nable for interpretability analysis.\n3.1\nPipeline Overview\nAt a high level, source-derived attributes are first distilled into\na consistent taxonomy with representative examples, then\nmodeled as co-occurrence patterns by the VAE. The fine-\ntuned LLM translates these attribute lists into natural lan-\nguage descriptions, and MusicGen synthesizes corresponding\naudio. This ensures that each synthetic sample is explicitly\npaired with concept labels drawn from the learned distribu-\ntion and representative audio realizations.\n3.2\nDataset Distillation & Concept Taxonomy\nTo obtain a high-quality, concept-dense dataset suitable for\nrigorous interpretability analysis, we must first define a mean-\ningful concept taxonomy and source high-quality training\ndata. The MusicCaps dataset [Agostinelli et al., 2023] con-\ntains 5,521 human-annotated descriptions with associated\ntags, providing a valuable starting point. However, Music-\nCaps suffers from sparse and noisy categories. These issues\nundermine the quality of downstream semantic models. We\ntherefore apply a distillation methodology to ensure our VAE\nand LLM components receive clean, representative data.\nThe distillation procedure consists of three steps:\n1. Tag Extraction and Organization: Identify all unique\ntags in MusicCaps and group them into coherent seman-\ntic categories such as instruments (acoustic guitar, piano,\ndrums), genres (folk, electronic, classical), moods (up-\nbeat, melancholic, energetic), and tempo characteristics.\nThis hierarchical organization enables targeted concept\nanalysis.\n2. Representative Subset Selection: Select samples that\ncover aspects from previously defined taxonomy, ex-\ncluding outliers and samples with sparse or unclear tags.\nPrioritize samples with multiple tags from different cat-\negories to ensure diversity.\n30\n20\n10\n0\n10\n20\n30\n20\n10\n0\n10\n20\n30\nt-SNE of VAE Latent Space Colored by Genre\nrock\npop\njazz\nclassical\nFigure 2: Visualisation of VAE latent space. Encoder successfully\nlearns to map music genres to latent space. We can also notice music\ngenres \u201dblending\u201d together, especially in \u201drock\u201d and \u201dpop\u201d genres,\nwhere conceptual overlap in source dataset is substantial.\n3. Aspect Quality Refinement:\nExtract only samples\nwith tags from at least 3 semantic categories, clear\nand grammatically sound descriptions, and coherent at-\ntribute combinations.\nBy distilling MusicCaps from 5,521 to 1,890 high-quality\nsamples, we ensure the VAE trains on representative, high-\nconfidence examples rather than noisy or ambiguous data.\nThis smaller, curated subset also yields a clean catego-\nrized taxonomy essential for concept-based explanation tech-\nniques. The resulting 1,890 pairs of attribute lists and pro-\nfessional captions serve as training data for both VAE co-\noccurrence modeling and LLM fine-tuning.\n3.3\nSemantic Modeling via VAE\nThe core motivation behind semantic modelling with VAE\nis to generate a rich source of coherent attribute combina-\ntions.\nThis is particularly important in concept-based re-\nsearch, where specific dataset characteristics must be mod-\nelled with precision. A naive approach of random attribute\nsampling reduces controllability and introduces contradictory\ncombinations (e.g., \u201cquiet death metal\u201d), which can nega-\ntively affect downstream analysis by introducing confusion\nin the studied models.\nWe use standard VAE architecture that is trained on multi-\nhot encoded attribute vectors representing the presence or\nabsence of each musical aspect in our taxonomy. Let x \u2208\n{0, 1}D be such a binary vector, where D is the number of\nunique attributes. The model learns to reconstruct such vector\nby sampling the latent space. This latent space is visualised\nin Figure 2 to show separation of different musical aspects,\nwhile grouping similar characteristics together.\nInference: Coherent Attribute Vector Generation\nBy learning the distribution of co-occurrence patterns rather\nthan relying only on examples in the source dataset, the\nVAE enables generation of new combinations that are statis-\ntically plausible while avoiding noise and bias. This is more\npowerful than either random sampling or simple template-\nbased approaches. It supports both conditional and uncondi-\ntional sampling by either giving a partial attribute vector xseed\nspecifying desired properties, or by sampling a latent vector\nz \u223cN(0, I) and passing it to Decoder.\n3.4\nControlled Text Generation via Fine-Tuned\nLLM\nWe found that direct prompting of pre-trained Large Lan-\nguage Models (LLMs) with raw attribute lists frequently\nyields suboptimal results, characterized by hallucinations or\nexcessive verbosity that dilutes semantic precision.\nOur solution is to fine-tune a pre-trained, instruction-tuned\nlanguage model (Llama 3.1 8B) [AI, 2024] on human-made\ncaptions found in MusicCaps dataset. By utilizing Quantized\nLow-Rank Adaptation (QLoRA) [Dettmers et al., 2023], this\napproach is significantly more efficient than full fine-tuning,\nwhile preserving general linguistic knowledge and specializ-\ning the model for music description. QLoRA reduces mem-\nory requirements through 4-bit quantization while maintain-\ning performance via rank-32 low-rank adapters.\nThe model is fine-tuned on pairs (A, C) from our dis-\ntilled dataset, where A is an attribute list and C is a ground-\ntruth professional caption. By fine-tuning on attribute-caption\npairs, the model learns to prioritize semantic density and di-\nrectly incorporate provided attributes into descriptions. The\nconditioning shifts the learning objective from \u201cgenerate\nplausible song description\u201d to \u201cwrite professional, accurate\ndescriptions of given attributes\u201d. This is a key distinction that\nimproves both quality and controllability.\nOur ablation studies (Section 5.3) demonstrate that this\nfine-tuned LLM approach significantly outperforms zero-shot\nand base-model approaches.\n3.5\nAudio Synthesis\nThe final stage uses MusicGen [Copet et al., 2023] to synthe-\nsize audio conditioned on the LLM-generated descriptions.\nMusicGen was trained exclusively on Meta-owned and li-\ncensed music, avoiding legal uncertainties of models trained\non scraped material [Batlle-Roca et al., 2025]. Its weights\nare released under CC-BY-NC 4.0, restricting use to non-\ncommercial research. The resulting dataset is therefore both\nfully suitable for academic purposes and reproducible within\nthese terms.\n3.6\nImplementation\nSource code, datasets and pre-trained models are publicly\navailable online for reproducibility purposes1. We are us-\ning \u03b2-VAE [Higgins et al., 2017] variant of variational auto-\nencoder with additional \u03b2 parameter, which controls KL di-\nvergence loss.\nFrom our experiments we concluded that\n\u03b2 = 0.25 yielded the best overall results. It suggests that\nminimizing reconstruction loss is more important in our case\nthan mapping latent space to normal distribution. Higher \u03b2\nvalues resulted in model collapse suggested by VAE predict-\ning only the few most popular attribute combinations. VAE\nis implemented with a standard fully-connected architecture:\n1https://anonymized-for-blind-review.com\nencoder with D-1024 hidden units, where D is denotes our\naspect taxonomy dimension, mapping to a 256-dimensional\nlatent space, and a symmetric decoder. We use Adam opti-\nmizer [Kingma, 2014] with learning rate 6 \u00d7 10\u22125 and train\nfor 150 epochs on the 1,890 distilled samples.\nFor LLM fine-tuning, we use the Hugging Face Transform-\ners [Wolf et al., 2020] library with QLoRA configuration:\nrank 32, lora-alpha 8, dropout 0.26. We fine-tune for 5 epochs\nwith batch size 4 using the distilled attribute-caption pairs.\nAudio synthesis uses MusicGen with increased guidance\nscale of 3.3 for more concept alignment of audio inference\nresulting in 30-second clip for each caption. Detailed evalu-\nation of our implementation with explanation of each metric\non each stage of the pipeline can be found in Table 1.\n4\nDataset\n4.1\nDataset Overview\nWe present a large-scale dataset composed of curated at-\ntributes representing meaningful musical concepts suitable\nfor interpretability analysis. Our dataset significantly exceeds\nexisting concept-labeled music resources in both scale and se-\nmantic quality.\nOur dataset demonstrates a marked improvement over Mu-\nsicCaps in terms of concept representation quality and con-\nsistency. As seen in Figure 4, we substantially reduce the\nlong-tail distribution present in MusicCaps, where sparse and\nredundant tags complicate down-stream interpretability anal-\nysis. By deliberately limiting the number and increasing the\nquality of concepts per sample, generative models can repre-\nsent them more faithfully in the final output.\n4.2\nConcept Distribution Analysis\nThe distillation and VAE-based generation process preserves\ncorrelation with the original MusicCaps distribution while\nintroducing important improvements in meaningful concept\ndensity seen in Figure 3. By employing the VAE to model co-\noccurrence patterns, we can efficiently represent real-world\nattribute relationships while maintaining access to vast new,\ncoherent attribute combinations. This is superior to either\nusing only original data (which has limited size) or random\nsampling (which lacks semantic coherence).\n5\nExperiments\nTo evaluate our distilled concept dataset pipeline, we design\nexperiments across three dimensions:\n\u2022 In Section 5.2 we access dataset quality relative to exist-\ning benchmarks.\n\u2022 In Section 5.3 we validate improvements of individual\npipeline stages.\n\u2022 In Section 5.4 we test downstream task performance\nmeasuring interpretability utility.\nOur experiments address a fundamental challenge in\ndataset curation for concept-based analysis: ensuring that\nhigh-quality, semantically coherent attributes map correctly\nto audio realizations. This requires validation of semantic\nconsistency (VAE stage), caption quality (LLM stage), and\npop\nrock\nclassical\nfolk\nelectronic\nelectronic music\njazz\nambient\ndance\ndance music\nmovie soundtrack\nfolk music\nsoulful\nr&b\nindie\n0\n5\n10\n% Share\nGenre\nacoustic guitar\nacoustic drums\nbass\nelectric guitar\ndrums\npiano\nbass guitar\nmale vocal\npercussion\nmale singer\ngroovy bass\npunchy kick\nkeyboard\nelectronic drums\nkeyboard harmony\n0\n2\n4\n% Share\nInstrument\nemotional\nenergetic\npassionate\nhappy\nromantic\nexciting\ncheerful\nintense\nmelancholic\nmellow\nfun\nsentimental\njoyful\nsoft\nsoothing\n0\n5\n10\n15\n% Share\nMood\nmedium tempo\nslow tempo\ngroovy\nfast tempo\nupbeat\nuptempo\nmedium fast tempo\nmoderate tempo\nmedium to uptempo\ngroovy rhythm\ndance groove\ndance rhythm\nsteady rhythm\ngroovy music\nrhythmic acoustic guitar\n0\n10\n20\n% Share\nTempo\nDistributions of Tags Between Categories\nMusicCaps\nConceptCaps\nFigure 3: Individual tag frequency distributions comparing Music-\nCaps (blue) and our distilled dataset (orange). The distilled dataset\npreserves source dataset characteristics, resulting in real-world data\ndistribution.\n0 1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334\nNumber of Tags\n0\n100\n200\n300\n400\n500\n600\nMusicCaps Count\nMusicCaps\nVAE-Generated\n0\n200\n400\n600\n800\nVAE-Generated Count\nDistribution of aspect counts per sample\nFigure 4: Comparison of per-sample aspect count distribution in\nMusicCaps (blue) versus our distilled dataset (orange). Lack of long\ntail shows improvement over sparse or redundant tags in original\ndataset.\naudio-text alignment (MusicGen stage). Ablation studies are\ncritical because they isolate the contribution of our two-stage\narchitecture relative to end-to-end approaches, demonstrating\nthat separation of concerns yields measurable improvements\nin both quality metrics and downstream interpretability per-\nformance.\n5.1\nEvaluation\nVAE Stage validates that the Variational Autoencoder suc-\ncessfully learns valid musical attribute combinations.\nAs\nshown in Table 1, high Jaccard and low Hamming loss con-\nfirm accurate reconstruction of individual attributes. High\nDiversity shows the model generates diverse combinations\nrather than sticking to the most popular combinations, crit-\nical for creating a larger dataset and avoiding model collapse.\nCosine similarity ensures learned patterns reflect real musical\nsemantics.\nIn the LLM Stage, fine-tuned language model achieves ro-\nbust semantic quality indicated by high BertScore [Zhang et\nal., 2020], BLEU [Papineni et al., 2002] and ROUGE-L [Lin,\n2004], confirming significant overlap between reference and\nground-truth samples. MAUVE [Pillutla et al., 2021] val-\nTable 1: Metrics for Best-Performing Hyperparameters Across Stages.\nPipeline\nStage\nMetric\nValue\nInterpretation\nStage 1:\nVAE\nJaccard \u2191\n0.716\nIntersection-over-union of reconstructed vs. ground-truth at-\ntribute vectors.\nHamming \u2193\n0.005\nMean element-wise prediction error on individual attributes.\nDiversity % \u2191\n96.03%\nProportion of unique attribute vectors generated from 1K sam-\nples.\nCosine Similarity \u2191\n0.808\nCosine distance between learned co-occurrence patterns and\nground-truth MusicCaps patterns.\nStage 2:\nLLM\nBERTScore-F1 [Zhang et al., 2020] \u2191\n0.9132\nContextual semantic similarity using BERT embeddings be-\ntween generated and ground-truth captions.\nBLEU [Papineni et al., 2002] \u2191\n0.2465\nUnigram, bigram, trigram, and 4-gram overlap with reference\ncaptions.\nROUGE-L [Lin, 2004] \u2191\n0.4263\nLongest common subsequence F-score between generated and\nreference captions.\nMAUVE [Pillutla et al., 2021] \u2191\n0.9236\nDistribution divergence metric between generated and human-\nwritten caption distributions.\nStage 3:\nAudio\nCLAP [Elizalde et al., 2023] \u2191\n0.4745\nSimilarity between textual description and audio representation\ncalculated using pre-trained CLAP model.\nFAD [Kilgour et al., 2019] \u2193\n0.4603\nFr\u00b4echet Audio Distance, quantifying divergence between real\nand generated audio distributions using CLAP embeddings.\nTable 2: Comparison of Music-Text and Audio-Language Datasets\nType\nDataset\n# Samples\n# Unique Aspects\nCaption\nAvail-\nable\nAvg.\nLength\n(words)\nTotal\nDuration\n(h)\nHuman-\nAnnotated\nAudioSet [Gemmeke et al., 2017]\n\u223c2,100,000\n527 classes\nNo\nN/A\n\u223c5,800\nAudioCaps [Kim et al., 2019]\n51,308\nN/A\nYes\n\u223c9\n142.5\nMusicCaps [Agostinelli et al., 2023]\n5,521\n\u223c500\nYes\n\u223c80\n15.3\nGTZAN [Tzanetakis and Cook, 2002]\n1,000\n10 genres\nNo\nN/A\n8.3\nSongDescriber [Manco et al., 2023]\n1,102\nN/A\nYes\n\u223c22\n23.5\nLLM/Synthetic-\nGenerated\nLP-MusicCaps [Doh et al., 2023]\n512,165\n\u223c1,000\nYes\n\u223c10\n1,422\nWavCaps [Mei et al., 2024]\n403,050\n527 classes\nYes\n\u223c8\n7,563\nSound-VECaps [Yuan and others, 2024]\n1,660,000\n527 classes\nYes\n\u223c40\n4,611\nConceptCaps (Ours)\n23,815\n200\nYes\n\u223c70\n198.5\nidates that overall output characteristics (vocabulary, style,\nlength distribution) match human-written captions, confirm-\ning successful adaptation of the fine-tuned model to the music\ndescription domain.\nIn the final stage to validate that the generated descriptions\neffectively guide the audio synthesis process, we evaluate\nthe semantic alignment between the synthesized audio and\nits corresponding text using CLAP (Contrastive Language-\nAudio Pretraining) scores [Elizalde et al., 2023]. CLAP pro-\nvides a robust metric for semantic similarity that aligns well\nwith human perception of relevance. While recent research\n[Okamoto et al., 2025] suggests that CLAP\u2019s correlation with\nsubjective human judgment can vary, it remains a critical\ntool for objective, large-scale evaluation where manual an-\nnotation is infeasible. Additionally, we measure the Fr\u00b4echet\nAudio Distance (FAD) to assess the distribution gap between\nour generated audio and real-world music. Following the re-\nsearch of [Gui et al., 2023], we utilize CLAP-based audio\nembeddings, as they capture deeper semantic and melodic\nfeatures that more closely align with human perceptual judg-\nments of \u201dmusicality\u201d and \u201daudio quality\u201d. We employ the\nGTZAN dataset [Tzanetakis and Cook, 2002] as a refrence,\nproviding a benchmark for melodic and rhythmic consistency.\n5.2\nCross-dataset Audio-Text Alignment Quality\nComparison\nWe benchmark our distilled dataset against two primary base-\nlines: MusicCaps, which serves as our human-annotated\nground-truth source, and LP-MusicCaps, a large-scale syn-\nthetic corpus. While MusicCaps provides a high-quality ref-\nerence point for assessing the baseline of human-level de-\nscription, LP-MusicCaps offers a direct methodological com-\nparison.\nSpecifically, both LP-MusicCaps and our work\nutilize the MusicCaps metadata as a seed; however, LP-\n0.0\n0.5\n1.0\nCLAP Score\n0.0\n0.5\n1.0\n1.5\n2.0\nDensity\nFine-tuned VAE Captions\nMusicCaps Captions\nLP-MusicCaps Captions\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\nCLAP Score\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nFine-tuned VAE Captions\nZero-shot Captions\nBase VAE Captions\nCLAP Score Distribution:\nDataset and Model Improvements\nFigure 5: CLAP score distributions comparing our distilled dataset\nagainst MusicCaps and LP-MusicCaps (left), alongside an ablation\nstudy of LLM configurations (right). Higher scores denote superior\nsemantic alignment between audio and text.\nMusicCaps lacks the multi-stage taxonomy distillation and\ntargeted LLM fine-tuning that characterize our approach, al-\nlowing us to isolate the impact of these specific architectural\ncontributions.\nAs illustrated in Figure 5, our dataset demonstrates an im-\nprovement in audio-text alignment compared to existing syn-\nthetic benchmarks. While the human-authored MusicCaps\ndataset exhibits a notable amount of high-performing sam-\nples, it also displays a significantly higher density of samples\nwith near-zero alignment, likely due to noisy or overly ab-\nstract human captions. In contrast, our dataset yield a more\nconcentrated distribution with a higher mean score than the\nLP-MusicCaps baseline, suggesting that our distillation pro-\ncess produces more representative and semantically consis-\ntent descriptions for generative models.\n5.3\nAblation Study\nWe conducted an ablation study to quantify the performance\ngain provided by the fine-tuning stage of our text generation\npipeline.\nSpecifically, we compared our fine-tuned model\nagainst the base LLM (Base VAE Captions) and a zero-shot\ninference baseline. The latter represents a scenario where the\nLLM generates descriptions without attribute conditioning,\nserving as a baseline for the need of our VAE-based attribute\ngeneration.\nAs shown in the right-hand panel of Figure 5, the fine-\ntuned LLM achieves a superior distribution shift compared\nto both the base model and the zero-shot baseline. The poor\nperformance of the zero-shot model highlights the difficulty\nof obtaining representative audio samples without explicit at-\ntribute control. Crucially, the improvement of the fine-tuned\nmodel over the base VAE model confirms that fine-tuning ef-\nfectively optimizes for semantic density\u2014teaching the model\nto use precise, professional terminology and eliminate ver-\nbose filler that can dilute the conditioning signal during audio\nsynthesis.\n5.4\nTCAV Analysis: Concept Separability in\nMusic Classifiers\nTo demonstrate the practical utility of our distilled dataset\nfor concept-based interpretability, we performed Testing with\nConcept Activation Vectors (TCAV) analysis on a music\ngenre classification task. TCAV quantifies the extent to which\nfast tempo\ngroovy bass\npiano\nslow tempo\nacoustic guitar\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTCAV Score\nJAZZ\nMETAL\nROCK\nCOUNTRY\nBLUES\nPOP\nREGGAE\nJAZZ\nMETAL\nCOUNTRY\nROCK\nBLUES\nPOP\nREGGAE\nMETAL\nROCK\nPOP\nREGGAE\nCOUNTRY\nBLUES\nJAZZ\nMETAL\nROCK\nPOP\nREGGAE\nCOUNTRY\nBLUES\nJAZZ\nPOP\nREGGAE\nMETAL\nROCK\nCOUNTRY\nBLUES\nJAZZ\nConcept Importance Across Genres\n(Higher\nmore important)\nFigure 6: TCAV scores illustrating concept importance across gen-\nres. Higher scores indicate that a concept serves as a highly discrim-\ninative feature for a specific class within the model\u2019s latent space.\na model\u2019s internal representations align with high-level, user-\ndefined concepts by measuring the sensitivity of predictions\nto concept-aligned directions in the latent space.\nExperimental Setup\nWe evaluated our distilled dataset by training a standard Con-\nvolutional Neural Network (CNN) on the GTZAN [Tzane-\ntakis and Cook, 2002] benchmark, a foundational corpus for\nmusic genre classification. The model attained a validation\naccuracy of 80%. While specialized architectures may yield\nhigher absolute performance, this baseline is sufficient to\ndemonstrate that the network has developed discriminative\nacoustic representations suitable for concept-based probing.\nUsing our distilled resource, we constructed balanced pos-\nitive and negative example sets for TCAV analysis.\nTo\nensure alignment with the spatial-temporal feature hierar-\nchies learned by the CNN, we focused on \u201dlow-to-mid-\nlevel\u201d acoustic concepts\u2014specifically tempo and instrumen-\ntation\u2014which possess distinct spectral signatures. This ap-\nproach allows us to verify whether the classifier\u2019s decision-\nmaking process is grounded in semantically meaningful mu-\nsical attributes.\nResults\nThe results presented in Figure 6 confirm that the classi-\nfier has learned domain-appropriate musical concepts. The\nTCAV scores reveal clear variations in concept importance\nthat align with musicological expectations. Notably, the im-\nportance of \u201dfast tempo\u201d and \u201dslow tempo\u201d contradict each\nother, across genres: while Jazz shows a high sensitivity\nto \u201dslow tempo\u201d (\u223c0.8) and low sensitivity to \u201dfast tempo\u201d\n(\u223c0.2), this relationship is inverted for Metal and Rock,\nwhere \u201dslow tempo\u201d provides zero discriminative utility.\nFurthermore, instrumentation concepts show high class-\nconditional relevance; for example, \u201dpiano\u201d and \u201dacoustic\nguitar\u201d reach their peak importance within the Jazz class (1.0\nand \u223c0.9, respectively), while remaining negligible for high-\nenergy genres like Metal. These findings confirm that our\ndistilled dataset provides the precise, semantically coherent\nexamples required to extract and audit specific musical con-\ncepts in downstream models.\n6\nConclusions\nWe present ConceptCaps, a dataset of 23k music-description\npairs with validated concept labels, built through a two-stage\npipeline that separates semantic modeling (VAE) from lin-\nguistic generation (fine-tuned LLM). This separation yields\nmeasurable gains in both caption quality and audio-text align-\nment over monolithic approaches like LP-MusicCaps or zero-\nshot generation.\nOur TCAV experiments confirm that the\ndataset enables meaningful concept-based probing of music\nclassifiers. While extending the taxonomy to non-Western\nmusical traditions remains a next step, we hope Concept-\nCaps will facilitate further research in interpretability of mu-\nsic models.\n6.1\nLimitations\nWhile our distilled dataset offers significant improvements\nin concept clarity and alignment, several limitations re-\nmain. First, the dependence on upstream models means\nour dataset inevitably inherits the biases and artifacts of its\ngenerator models. The audio generation relies on MusicGen,\nwhich has been shown to exhibit Western-centric biases. Sim-\nilarly, the Llama 3-based text generation, despite fine-tuning,\nmay occasionally produce \u201dhallucinated\u201d or verbose details,\nwhich reduces audio concept alignment.\nSecond, the distillation process itself introduces a selec-\ntion bias. By filtering for \u201dhigh-confidence\u201d and \u201dcoherent\u201d\nattribute combinations, we potentially exclude experimental,\navant-garde, or cross-cultural musical concepts that do not fit\nthe VAE\u2019s learned distribution of \u201dplausibility.\u201d This results\nin a cleaner, but perhaps less creatively diverse, representa-\ntion of the musical landscape compared to raw, noisy web\ndata.\nEthical Statement\nCopyright and Fair Use.\nA primary motivation for this\nwork is to address the legal and ethical bottlenecks in music\nAI research [Barnett et al., 2025]. Standard datasets like Au-\ndioSet or valid portions of MusicCaps often rely on Western-\nbiased, copyrighted commercial music, creating legal uncer-\ntainty for downstream model distribution. By releasing a fully\nsynthetic, copyright-free dataset, we aim to provide a safe\nsolution for researchers to benchmark interpretability tools\nwithout infringing on intellectual property rights. However,\nwe acknowledge the ethical complexity of \u201dlaundering\u201d mu-\nsical concepts: while the specific audio is synthetic, the un-\nderlying generative models are trained on vast corpora of hu-\nman artistry, often without consent or compensation.\nEnvironmental Impact.\nThe creation of this dataset in-\nvolved approximately 100 GPU-hours of inference (Music-\nGen and Llama 3). While this one-time cost is significant,\nwe believe the release of a reusable, high-quality dataset\nwill reduce the need for individual researchers to run redun-\ndant, large-scale generation cycles, ultimately saving compu-\ntational resources in the long term.\nAcknowledgments\nWe gratefully acknowledge Polish high-performance com-\nputing infrastructure PLGrid (HPC Center: ACK Cyfronet\nAGH) for providing computer facilities and support within\ncomputational grant no. PLG/2025/018397.\nReferences\n[Afchar et al., 2022] Darius Afchar, Romain Hennequin, and\nVincent Guigue. Learning unsupervised hierarchies of au-\ndio concepts. arXiv preprint arXiv:2207.11231, 2022.\n[Agostinelli et al., 2023] Andrea Agostinelli, Timo I. Denk,\nZal\u00b4an Borsos, Jesse Engel, Matteo Verzetti, Antoine Cail-\nlon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco\nTagliasacchi, et al. MusicLM: Generating music from text.\narXiv preprint arXiv:2301.11325, 2023.\n[AI, 2024] Meta AI. Meta llama 3.1. https://huggingface.co/\nmeta-llama/Llama-3.1-8B-Instruct, 2024. Llama 3.1-8B-\nInstruct model card.\n[Barnett et al., 2025] Julia Barnett, Patrick O\u2019Reilly, Ja-\nson Brent Smith, Annie Chu, and Bryan Pardo.\nEthics\nstatements in ai music papers: The effective and the inef-\nfective, 2025.\n[Batlle-Roca et al., 2025] Roser Batlle-Roca, Laura Ib\u00b4a\u02dcnez-\nMart\u00b4\u0131nez, Xavier Serra, Emilia G\u00b4omez, and Mart\u00b4\u0131n Ro-\ncamora. MusGO: A community-driven framework for as-\nsessing openness in music-generative ai. In Proceedings of\nthe 26th International Society for Music Information Re-\ntrieval Conference (ISMIR), 2025.\n[Copet et al., 2023] Jade Copet, Felix Kreuk, Itai Gat, Tal\nRemez, David Kant, Gabriel Synnaeve, Yossi Adi, and\nAlexandre D\u00b4efossez. Simple and controllable music gen-\neration.\nIn Advances in Neural Information Processing\nSystems, 2023.\n[Dettmers et al., 2023] Tim Dettmers, Artidoro Pagnoni, Ari\nHoltzman, and Luke Zettlemoyer. QLoRA: Efficient fine-\ntuning of quantized LLMs. Advances in Neural Informa-\ntion Processing Systems, 36:10088\u201310115, 2023.\n[Doh et al., 2023] SeungHeon Doh, Keunwoo Choi, Jongpil\nLee, and Juhan Nam."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"ConceptCaps - a Distilled Concept Dataset for Interpretability in Music Models\"\n\n#### Main Contributions:\n1. **Introduction to ConceptCaps**: The paper introduces ConceptCaps, a large-scale dataset of 23k music-description pairs with rich and validated concept labels, specifically designed for interpretability research in music models.\n\n2. **Dataset Generation Pipeline**:\n   - **VAE (Variational Autoencoder)**: Separates semantic modeling from text generation by learning plausible attribute co-occurrence patterns.\n   - **Fine-tuned LLM**: Translates these controlled attribute lists into professional, context-aware music descriptions.\n   - **MusicGen**: Synthesizes corresponding audio.\n\n3. **Separation of Concerns**:\n   - The pipeline separates semantic consistency from fine-tuned LLM linguistic quality, enabling controlled and high-quality dataset generation while maintaining efficiency through local processing.\n\n4. **Validation and Metrics**:\n   - The dataset is validated using various metrics including CLAP (Contrastive Language-Audio Pretraining), BERTScore, MAUVE (Multi-aspect Universal Metric for Evaluating Text Generation), and TCAV (Testing with Concept Activation Vectors) analysis.\n   \n5. **Dataset Quality and Utility**:\n   - The dataset is shown to yield competitive results compared to API-based methods [Ouyang et al., 2022], achieving similar performance at significantly lower computational cost.\n\n6. **Fine-grained Control**:\n   - Fine-tuned LLMs provide fine-grained control over dataset characteristics through VAE latent space sampling and conditioning, enabling more precise concept labels and descriptions.\n\n#### Methodology:\n1. **Concept-Based Interpretability**: The paper addresses the limitations of existing music datasets by proposing a novel two-stage generative pipeline that separates semantic modeling from text generation.\n   \n2. **Dataset Distillation**:\n   - A distillation methodology is applied to ensure clean, representative data for VAE and LLM components, resulting in a curated subset of 1,890 pairs of attribute lists and professional captions.\n\n3. **VAE Architecture**: The VAE architecture is based on standard architectures with additional \u03b2 parameter controlling KL divergence loss.\n   \n4. **Fine-tuned Language Model (LLM)**: A pre-trained instruction-tuned LLM (Llama 3.1 8B) is fine-tuned using QLoRA (Quantized Low-Rank Adaptation), significantly improving efficiency while preserving general linguistic knowledge.\n\n5. **Audio Synthesis**: MusicGen, trained exclusively on Meta-owned and licensed music, synthesizes copyright-free audio from generated descriptions.\n\n#### Key Findings:\n- The dataset improves concept representation quality and consistency compared to existing datasets like MusicCaps.\n- Concept-based interpretability analysis using TCAV confirms that the dataset enables meaningful probing of music classifiers, validating its utility for research in this area.\n\n#### Conclusions:\n1. **ConceptCaps** is a valuable resource for researchers aiming to improve interpretability in music models by providing high-quality concept labels and controlled generation pipelines.\n2. While ConceptCaps offers significant improvements in clarity and alignment, it inherits biases from upstream models and may limit creative diversity compared to raw data.\n3. The release of this synthetic dataset provides a safe solution for benchmarking interpretability tools without infringing on intellectual property rights.\n\n#### Limitations:\n- The dataset inherits biases from its generator models (e.g., MusicGen) and may not capture experimental or cross-cultural musical concepts accurately.\n- Ethical considerations are raised regarding the use of synthetic data, particularly in relation to consent and compensation for human-generated content.\n\n### Conclusion\nConceptCaps represents a significant advancement in providing high-quality concept labels and controlled generation pipelines for music models. While it faces some limitations due to its synthetic nature, the dataset's utility for interpretability research is substantial and valuable."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "INSIGHT: Interpretable Semantic Hierarchies in Vision-Language Encoders\nKai Wittenmayer, Sukrut Rao, Amin Parchami-Araghi, Bernt Schiele, Jonas Fischer\nMax Planck Institute for Informatics, Saarland Informatics Campus, Saarbr\u00a8ucken, Germany\n{kwittenm,sukrut.rao,mparcham,schiele,jonas.fischer}@mpi-inf.mpg.de\nRich Concept Representation\nConcept\nRelations\nLanguage-aligned\nVision Encoder\nSpatial Grounding\nConcept-based\ninference &\nexplanation\nSailboat\nBody of Water\n\"Catamaran\"\n\"Sky\"\n\"Water\"\n\"Mountain\"\nClassification\nOpen-vocab Segmentation\nCaptioning\nThe sailing yacht is a luxury yacht\nthat is available in the United States.\nShip Boat\nDownstream Tasks\nSailboat\nShip Boat\nBody of Water\nINSIGHT\nPatch-wise\nConcept\nEncoder\nconcept\nsteering\n\u201cCatamaran\u201d\nFigure 1. INSIGHT provides rich conceptual explanations for vision foundation model tasks. Our model provides a hierarchical\nconcept representation (e.g. \u2018Sailboat\u2019 concept is a child of \u2018Ship Boat\u2019) with local, spatially grounded concepts that are automatically\nnamed. These concepts enable INSIGHT to provide concept-based explanations for decision-making across vision tasks.\nAbstract\nLanguage-aligned\nvision\nfoundation\nmodels\nperform\nstrongly across diverse downstream tasks.\nYet, their\nlearned representations remain opaque, making interpret-\ning their decision-making hard. Recent works decompose\nthese representations into human-interpretable concepts,\nbut provide poor spatial grounding and are limited to image\nclassification tasks. In this work, we propose INSIGHT, a\nlanguage-aligned concept foundation model that provides\nfine-grained concepts, which are human-interpretable and\nspatially grounded in the input image.\nWe leverage a\nhierarchical sparse autoencoder and a foundation model\nwith strong semantic representations to automatically\nextract concepts at various granularities. Examining local\nco-occurrence dependencies of concepts allows us to define\nconcept relationships. Through these relations we further\nimprove concept naming and obtain richer explanations.\nOn benchmark data, we show that INSIGHT provides\nperformance on classification and segmentation that is\ncompetitive with opaque foundation models while provid-\ning fine-grained, high quality concept-based explanations.\nCode is available at https://github.com/kawi19/Insight.\n1. Introduction\nLanguage-aligned\nvision\nfoundation\nmodels\nsuch\nas\nCLIP [53, 62, 73] serve as powerful feature extractors that\nperform strongly across tasks such as zero shot classifi-\ncation and open-vocabulary semantic segmentation. Such\nmodels also serve as vision encoders for large vision-\nlanguage models [13, 32, 35] and generative models. How-\never, their representations are opaque, which hinders inter-\npreting their decision-making. Interpretability is, however,\nkey for human users, especially safety critical applications.\nThere, explanations of decision-making can help to under-\nstand whether a model is right for the right reasons, increase\ntrust in a model\u2019s prediction, and investigate why a model\ngot wrong or ended in harmful decisions.\nRecent works [2, 33, 57, 72] aim to decompose repre-\n1\narXiv:2601.13798v1  [cs.CV]  20 Jan 2026\nsentations of foundation models into human-interpretable\nconcepts and achieve transparency of these models at un-\nprecedented scale.\nHowever, extracted concepts are not\nproperly grounded in the input image [2, 57, 72] and of-\nten active as a result of spurious correlations, harming in-\nterpretability. Moreover, they only provide a limited range\nof granularity of concepts [2, 33, 57] and spatial coverage\nof those [2, 57, 72], and inter-concept dependencies are not\nexamined even when using hierarchical priors [72]. Tex-\ntual concept labels, when assigned [2, 57], often stem from\na limited vocabulary that is not tailored for large-scale and\nfine-grained conceptual diversity. Apart from these limita-\ntions of the given explanations, existing works were only\nconsidering interpretable image classification, whereas the\nunderlying foundation model is capable of much more.\nIn this work, we address these issues and propose IN-\nSIGHT, a language-aligned concept foundation model that\nprovides named concepts at multiple granularities. These\nconcepts are spatially grounded in the image and orga-\nnized into hierarchies with explicit inter-concept depen-\ndencies. To do this, we develop a Matryoshka sparse au-\ntoencoder [6, 72] to represent loca, DINOised CLIP fea-\ntures [64], which gives spatially localized and human-\naligned concepts from coarse to fine granularities. Exam-\nining co-occurrences of local concept activations we dis-\ncover parent-child relationships between concepts that pro-\nvide further insights into the learned semantics. We fur-\nther use these relations to refine concept labeling towards\nmore accurate and meaningful textual concept labels. Our\nmodel enables interpretable decision-making for a variety\nof down-stream tasks typical for a language-aligned vision\nfoundation model (see Fig. 1).\nContributions. (1) We propose to learn hierarchical con-\ncept extraction to obtain human-interpretable concepts at\ndiverse granularities that are spatially well-grounded in\nthe input. (2) We show how to extract explicit hierarchical\nrelations between those concepts that improve the under-\nstanding of the model. (3) We propose an improved con-\ncept labeling scheme that explicitly takes parent-child re-\nlationships from this hierarchy into account. (4) Putting\nthese together, we propose INSIGHT, a language-aligned\nfoundation-like model that enables interpretable decision-\nmaking across downstream tasks such as classification,\nopen-vocabulary segmentation, and image captioning.\nOn benchmark data and through a human user study we\nshow that in contrast to state-of-the-art, INSIGHT yields\nconcepts that are more quantitatively and qualitatively con-\nsistent, spatially more localized, and meaningfully named\n(Tab. 1, Fig. 5).\nWe further show that this added inter-\npretability does not come with a degradation of perfor-\nmance across various downstream tasks. On image clas-\nsification tasks INSIGHT yields performances competitive\nwith the state-of the-art concept-based and opaque baseline\nmodels (Tab. 2). On a light-weight image captioning task,\nINSIGHT performs similar to its opaque backbone and en-\nables steering (Fig. 6). INSIGHT also enables interpretable\nopen-vocabulary segmentation that is on par with opaque\nfoundation model alternatives (Tab. 3).\n2. Related work\nVision-language models (VLMs) such as CLIP [14, 26,\n53], ALIGN [27], CoCa [70], and SigLIP [62, 73] learn vi-\nsion and text encoders that share a semantic latent space.\nSuch models are typically trained on billions of paired\nimage-text data through contrastive learning, yielding pow-\nerful but opaque language-aligned visual representations\nenabling tasks such as open-vocabulary classification and\nsemantic segmentation. Our proposed method INSIGHT is\nsuch a language-aligned vision encoder, but trained to pro-\nvide a human-interpretable concept representation space.\nConcept extraction methods aim to decompose learned\nrepresentations from encoders into human-interpretable\nconcepts through, e.g., clustering [23, 31, 45], non-negative\nmatrix factorization [21] or a learned concept dictionary\nthat is image-specific [2] or shared across images [33,\n49, 57, 72].\nThe latter, in particular, have been applied\nto VLMs, but (1) lack spatial grounding of concepts to\nthe image [2, 57, 72], (2) learn a flat hierarchy of con-\ncepts [2, 33, 57], or (3) use a limited vocabulary and im-\nprecise names [57].\nOur INSIGHT, in contrast, learns a\nrich set of spatially grounded concepts with explicit parent-\nchild relationships that are named using a large fine-grained\nvocabulary taking such relationships into account, yielding\na significantly more accurate, localized, and interpretable\nconcept decomposition.\nSparse autoencoders (SAEs) [3, 16] are models that aim\nto learn a dictionary of human-interpretable concepts from\na latent space. These consist of an affine encoder and de-\ncoder that map features to a sparse high-dimensional space\nof concepts typically trained through a reconstruction and\nsparsity loss. SAEs have been shown a useful tool to un-\nderstand large language models (LLMs) [3, 16] as well as\nvision models [57, 72]. Recent works suggested alternative\nsparsity objectives [5, 22, 54, 55] and architectural priors\nfor concept hierarchies [6, 72] to improve SAE representa-\ntions. Here, we combine the best of both worlds and use\nMatryoshka SAEs [6] with a BatchTopK objective [5] to\nextract high quality, localized, and hierarchical concepts.\nConcept-based classification models such as concept bot-\ntleneck models (CBMs) [1, 30, 44, 57, 71] and visual\nprototype-based methods [11, 18, 42, 67] learn concepts\nfrom a latent feature space to yield interpretable classifica-\ntion predictions. Where CBMs extract labeled, global con-\ncepts that are have shown to be often not correctly grounded\nin the image [25], prototypes are local visual features that\n2\n\ufffd\u3335!\n\ufffd\u3335\"\n\ufffd\u3335#\n\ufffd\u3335!\n\ufffd\u3335\"\n\ufffd\u3335#\n\ufffd\u3335\"\n\ufffd\u3335#\n\ufffd\u3335$\nDictionary\nvector \ufffd\u3335$!\nDictionary\nvector \ufffd\u3335$\"\nBias\nConcept maps for\nentire dataset\nConfidence\nmatrix \ufffd\u3335\nCompute Concept\nVectors\nNamed concept\nfamilies\nVocabulary T\nRoof, Head, Shell,\n\u2026Turtle\nMatch concept vector\n\ufffd\u3335with closest name \ufffd\u3335\nin CLIP space\nConcept\nvector v$!\n\ufffd\u3335#\n\ufffd\u3335$\n\ufffd\u3335%\n\ufffd\u3335&\n\ufffd\u3335'!\nLegend\ninput image \ufffd\u3335C \u2208\u211dD#\nfeature map \ufffd\u3335C\nE \u2208\u211dF\u00d7D\nconcept act. \ufffd\u3335HI \u2208\u211d$\n\ufffd\u3335= #concepts\nn = #patches\ntrained\nfrozen\nConcept Relation\nDiscovery (Sec 3.3)\nFamily\ngraph\nCLIP- DINOiser Feature\nExtraction (Sec. 3.1)\nCLIP Vision\nEncoder\nGuided\nPooling\nConcept Naming\n(Sec 3.4)\nDense features\n\ufffd\u3335%\n&\n\ufffd\u3335\ufffd\u3335\ufffd\u3335(\ufffd\u3335%\n&)\nConcept map z_i\nMatryoshka SAE\n\ufffd\u3335\ufffd\u3335\ufffd\u3335(\ufffd\u3335i)\nReconstruction loss\nReconstructed\nfeatures\n\ufffd\u3335\"\n\ufffd\u3335#\n\u2026\n\ufffd\u3335!\n\ufffd\u3335()\nConcept Mapping\n(Sec 3.2)\n\ufffd\u3335%\nFlatten and accumulate across patches\nDictionary vector \ufffd\u3335$(\nSpatial Locations\nConcepts\nImages\nTurtle\nBox\nTurtle\nTurtle\nShell\nFigure 2. Overview of INSIGHT. We visualize our approach to represent human-interpretable, well localized, and consistent concepts of\ndifferent granularities in a language-aligned vision foundation model. As backbone, we use pretrained CLIP-DINOiser [64] and extract\nconcepts patch-wise by training a Matryoshka Sparse Autoencoder (SAE) without any supervision. From these learned concept repre-\nsentations, we discover concept relations through examining the co-occurrences of concepts within patches that allow more fine-grained\ninterpretation of decision-making on downstream tasks. We further name the concepts with a Matryoshka-aware matching between concept\nrepresentations and a large vocabulary of labels, taking the discovered concept relations into account.\nhave to be investigated by a user to understand their con-\nceptual meaning. Our approach bridges the gap between\nthe two and help construct inherently interpretable concepts\nthat are labeled, can be global as well as local, and are well\ngrounded in the input. Moreover, in the spirit of founda-\ntion models, INSIGHT enables downstream tasks beyond\njust classification.\n3. INSIGHT: A language-aligned concept\nfoundation model\nTo equip language-aligned vision foundation models with\ntransparent, human-interpretable reasoning, we propose a\nsemantically meaningful representation layer that encodes\ninterpretable concepts. In a nutshell, we learn a local map-\nping from dense representation to human-interpretable con-\ncept space. This concept space is encouraged to represent\nconcepts at different levels of granularities, so that we are\nable to discover locally consistent concept hierarchies de-\nscribing part- or specification-relationships between con-\ncepts based on patch-wise concept co-occurrences (Fig. 2).\nSpecifically, we build on CLIP-DINOiser [64], a CLIP\narchitecture that encodes images such that semantically\nsimilar regions, informed through DINO [8], share similar\nrepresentations (Sec. 3.1). To map this latent CLIP encod-\ning to meaningful concepts at different levels of granular-\nity, we modify the recently proposed Matryoshka Sparse\nAutoencoder (SAE), learning patch-wise interpretable con-\ncept representations in an unsupervised manner at scale\n(Sec. 3.2).\nFrom these interpretable concept represen-\ntations, we show how to extract relations between con-\ncepts that reflect both specifications as well as part-\nrelations through an analysis of local co-occurrence patterns\n(Sec. 3.3). Finally, we extend the existing concept vocabu-\nlaries used for labeling, show how to improve concept label-\ning by leveraging our discovered concept relationships, and\nshow that an architecture-aware text encoding is necessary\nfor accurate labels (Sec. 3.4).\n3.1. The CLIP-Dinoiser backbone\nDespite being trained to produce image-level embeddings,\nit has been shown that [76] fine-grained local representa-\ntions can also be extracted from the CLIP [53] vision en-\ncoder. Specifically, the patch token value vectors from the\nCLS attention of a CLIP ViT [19] can be repurposed as\ndense local embeddings. While having a global context,\nthese still preserve a degree of spatial locality, but are noisy.\n3\nTo address this, we adopt the guided pooling ap-\nproach introduced for the CLIP-DINOiser architecture [63],\nwhich smoothens the representations of semantically sim-\nilar patches.\nTo do so, we can use the affinity patterns\nof self-supervised features, such as DINO [8], computed\nas inter-patch cosine-similarities A\u03be \u2208[\u22121, 1]N\u00d7N. This\ngives dense \u201cDINOised\u201d features F + for each patch p as\nF +\np =\n1\nPN\nq=1 A\u03be\np,q\nN\nX\nq=1\nA\u03be\np,q \u00b7 Fq,\n(1)\nwhere N is the number of patches and Fq is the value vector\nof patch token q of the last attention. Intuitively, this pooling\nacts as a \u201cvoting system\u201d that enforces consistency among\nsemantically similar patches while \u201cattenuating noisy fea-\ntures\u201d [63].\nTo be effective at inference, Wysocza\u00b4nska et al. [63] ob-\nserved that CLIP features already have semantically mean-\ningful information similar to DINO and suggest to train a\nsmall 3\u00d73 convolutional layer g : Rd \u2192Rd\u2032, d\u2032 < d on\ntransformer patch tokens of an intermediate layer \u03d5l to ap-\nproximate the DINO affinity map A\u03be. In our case, we ap-\nproximate A\u03be from the final CLIP layer L by\nA\u03d5 =\ng(\u03d5L(X))\n\u2225g(\u03d5L(X))\u2225\u2297\n\u0012 g(\u03d5L(X))\n\u2225g(\u03d5L(X))\u2225\n\u0013T\n(2)\nfor patch tokens X. The convolution g is learned through a\nbinary-cross entropy loss between A\u03d5 and binarized DINO\naffinities A\u03be > \u03b3; we refer to App. B for more details.\nAt inference time, this only requires a single forward\npass through CLIP \u03d5, computing the affinity map A\u03be of\nthe patch tokens using the trained convolution g, and ap-\nplying it as weights (Eq. (2)) for guided pooling on the fi-\nnal patch tokens. This approach offers three advantages for\nconcept extraction compared to CLIP. First, the extracted\nlocal embeddings are well aligned with CLIP\u2019s language\nspace, meaning that each spatial feature can still be di-\nrectly associated with natural language descriptions through\nCLIP\u2019s pretrained text encoder. Second, because these lo-\ncal embeddings are pooled from semantically coherent re-\ngions of the image, such regions frequently correspond to\nmeaningful object parts or localized visual attributes. Third,\nthis DINOising approach is, in principle, compatible with\nmost language-aligned vision encoders, requiring only a\nlightweight convolutional adapter. Next, we will discuss\nhow to map such DINOised feature maps to an interpretable\nconcept space.\n3.2. Unsupervised concept mapping through Ma-\ntryoshka SAEs\nTo learn a mapping from the powerful latent representations\nof foundation models to human-interpretable concepts, we\nleverage sparse autoencoders (SAEs) [3]. These consist of a\nlight-weight linear layer as encoder, \u03c0 : Rd \u2192Rd\u2032 that de-\nscribes an affine mapping from a d-dimensional input space,\ne.g., a CLIP space, to a sparse representation where each di-\nmension ideally encodes a single interpretable concept. An\naffine mapping \u03c0\u22121 : Rd\u2032 \u2192Rd then maps back to the\ninput space, allowing to train this autoencoder architecture\nefficiently through a reconstruction loss such that\n\u03c0\u22121\u03c0(x) \u2248x, x \u2208Rd.\n(3)\nA key detail of SAEs is an additional loss term that encour-\nages sparsity of concept activation for each input x. We here\nuse a version of the top-k approach, where at each forward\npass all but the top-k most activating neurons on input x are\nset to zero. While not perfect, SAEs provide a very efficient\nand unsupervised method to learn a concept mapping, both\nof which are crucial at the scale of a foundation model.\nIn contrast to existing concept extraction approaches, we\nencode each patch through the autoencoder,which through\nthe use of a semantically rich feature map as input allows\nto learn meaningful and well-grounded local concepts at\nscale. We use a BatchTopK [5] Matryoshka SAE [37, 72],\nwhich allows to flexibly distribute the capacity k of con-\ncepts across samples (here patch encodings) in a batch. It\nfurther adds an auxiliary reconstruction loss for different\nsubsets of first l neurons in the concept layer, akin to a Ma-\ntryoshka. Each subset of neurons by itself then needs to re-\nconstruct the input. More formally, let C = [1 . . . m] be the\nindex set for the m neurons in the SAE concept layer and\nlj \u2208C indices until which we define a matryoshka shell.\nThen\nLrec(Fp) =\nX\nj\n\u03c0\u22121( \u03c0(Fp)[1 : lj] ) \u2212Fp\n2\n2 ,\n(4)\nwhere Fp are the latent features for patch p given by the\nbackbone model and \u03c0(Fp)[1 : lj] sets all activations of\nneurons after index li to zero. In other words, each Ma-\ntryoshka shell consisting of the first lj neurons is trained to\nreconstruct the full latent features, which encourages that\nthe smallest subset of neurons learns coarse, high-level in-\nformation and higher-index neurons learn the residual fine-\ngrained information on top of this coarse representation.\n3.3. Co-occurrence based concept relationships\nTo discover concept relations, related work relies on prede-\nfined concept hierarchies or leveraged LLMs [48, 61]to or-\nder concepts in a hierarchical manner. However, predefined\nhierarchies or external ontologies may not reflect the struc-\nture that emerges in the data or the model\u2019s reasoning pro-\ncess. By discovering concept relations directly from data,\nwe avoid mismatches between imposed taxonomies and the\nrepresentations that the model actually uses. This allows for\na more faithful interpretation and avoids biases introduced\nby LLM- or human-curated ontologies.\n4\nFigure 3. A concept relationship subgraph. The parent concept, \u2018motor car\u2019, is given as box in the center with children concepts arranged\naround it. For each concept, we provide the assigned label (top) and the top-4 most activating images (first row) along with their spatial\nlocalization (second row) based on concept activation strength per patch.\nTo represent hierarchies we define a family consist-\ning of parent and child concepts, where parents represent\nmore general concepts (e.g., \u201cturtle\u201d) and children cap-\nture more fine-grained or part-based specializations (e.g.,\n\u201cleatherback turtle\u201d, \u201cturtle shell\u201d). A family is represented\nas a directed acyclic graph G = (V, E), where V is the set\nof concepts, E \u2286V \u00d7 V is the set of directed edges such\nthat (u, v) \u2208E denotes that u is a parent of v.\nWe take inspiration from Ye et al. [69], which used\nthe co-occurrence of extracted document-level features in\ndense text embeddings of LLMs to reveal \u201dfeature fami-\nlies\u201d as trees. Instead, we use concept activation patterns\nacross patch embeddings to discover our relations. To adapt\nto visual hierarchies, our families are not constrained to\ntrees, but nodes can have multiple parents, which is bet-\nter suited to represent the relations described above. We\nextend their approach and consider confidence instead of\nsimple co-occurrence, which allows to better define the\norientation of a parent-child relation. More formally, let\n1ji = {concept cj is activated in patch i} be an indicator\nvariable, and Ajl the activation of concept cj on patch i,\nthen the weighted confidence matrix is\nDj,j\u2032 =\nP\ni 1j\u2032iAji\nP\ni Aji\n.\n(5)\nNote that we sum over all patches of all images in the data\nto get a general concept relationship graph.\nAn entry in D represents the weighted conditional proba-\nbility of concept j\u2032 appearing given that concept j\u2032 appeared\nin a patch. D is used as the weighted adjacency matrix for\nour graph. To remove weak associations, we prune edges\nwith weights < 0.75. The family graph is then obtained by\ninverting the direction of the (remaining) edges. This in-\nversion ensures that edges consistently point from general,\nhighly frequent concepts to specific, infrequent concepts.\nFor this to work, our design with local, well-grounded con-\ncepts is crucial as otherwise spatially unrelated concepts\n(e.g., field and cow) may incorrectly be assigned a fam-\nily relationship merely due to spurious correlations in their\nimage-level activations.\nThe discovered relations refine explanations through\ninter-concept dependencies and serve as a prior that allows\nto learn better concept labels discussed next.\n3.4. Hierarchy-aware concept labeling\nFor human-understandable textual explanations of decision-\nmaking, previous approaches [57] suggested to learn labels\nfor each concept neuron in an unsupervised fashion. Yet,\ntheir assigned labels often appear spurious and for open-set\nconcept naming the vocabulary often too limited to reflect\nthe fine-grained visual features that vision models can cap-\nture. Here, following the idea of Rao et al. [57], we lever-\nage the language-aligned CLIP backbone to assign labels\nli based on maximum cosine similarity between CLIP text\nencoder representations T of a text v from a pre-defined vo-\ncabulary V , and the concept dictionary vector for the target\nconcept i, which is given by the SAE decoder weights \u03c0\u22121\ni\n.\nMore formally, we obtain\nli = argminv\u2208V cos\n\u0000\u03c0\u22121\ni\n+ b, T (v)\n\u0001\n,\n(6)\nwhere cos is cosine similarity and b the SAE decoder bias.\nThis existing approach performs well for general con-\ncepts in the first Matryoshka group of the SAE, however,\ndue to residual reconstruction of more fine-grained features\nin the Matryoshkas, requires an adapted dictionary vector\nreconstruction (see App. B). In essence, we need to take\ninto account the residual dependencies between concepts,\nadding the dictionary weights of the fine-grained concepts\nto those of their super-concepts. Such super-concepts are\ngiven through the graph G as any parent a of a target con-\n5\nPred: Swimming hole\nRiver\nPine Trees\nBody of Water\nBody\nof Water\nRange of\nMountains\nTree\nCanopy\nForest\nLandscape\nMountain\nStream\nPine Trees\nRiver\nClear Water\nArti\u0001icial\nLake\nMountain\nLandscape\nOur ConceptContributions\n23.8%\n5.9%\n48.5%\nFamily\nBody of\nWater\nDN-CBM\nLF-CBM\nCDM\na diving board\na stream\nrapids\ncan be very long\nclear blue waters\nmay be rocky or forested\na large, open area of water\nthe water is hot to\na paddle\na lake or river\nswimsuits\ncanoeing\nrocks\npond\nwetland\n+1.22\n+0.81\n+0.78\n+0.64\n+0.60\n+1.65\n+1.04\n+0.94\n+0.83\n+0.58\n+5.97\n+6.09\n+6.13\n+6.57\n+6.76\nFigure 4. Spatial grounding of concept-based explanations. For\nan image of the \u201dSwimming hole\u201d Class of Places365 we show ex-\nplanations of INSIGHT against existing method. For INSIGHT, we\nprovide the parent concept (dark green) and its contributing chil-\ndren (light green) with percentage of contribution and the spatial\ngrounding of the top-3 most contributing concepts. In contrast to\nour work, existing methods (bottom) show spurious concepts.\ncept node i. More formally, we get\nli = argminv\u2208V cos\n\uf8eb\n\uf8ed\nX\na\u2208G,a\u2192i\n\u03c0\u22121\na \u03c0\u22121\ni\n+ b, T (v)\n\uf8f6\n\uf8f8, (7)\nwhere a \u2192i means a is parent of i in G. As vocabulary, we\nhere consider the union of several datasets that lend them-\nselves as concepts, which have been previously used in this\ncontext. Specifically, we use vocabularies from Brysbaert\net al. [4], Muraki et al. [41], Oikarinen and Weng [43] and\ndiscuss filtering in App. B.\nPutting all pieces together (Fig. 2), we obtain INSIGHT,\na foundation-like vision model which extracts language-\naligned, spatially grounded, and coarse-to-fine human-\ninterpretable concepts from images for explainable down-\nstream decision-making.\n4. Experimental evaluation\nWe evaluate INSIGHT with ViT-B-16 CLIP-DINOiser back-\nbone and compare the discovered concepts to state-of-the-\nart concept extraction methods using interpretability met-\nrics on annotated datasets and conduct a human-user study\nto assess concept interpretability in Sec. 4.1 and provide de-\ntails on metrics, data, and study design in App. A.\nFor downstream task evaluation, we consider image clas-\nsification on Imagenet [17] and Places365 [74], where we\ncompare to LF-CBM [44], LaBo [68], CDM [47], DCLIP\n[38], DN-CBM [57], D-CBM [51], CF-CBM [48], SALF-\nCBM [1] which are state-oft-the-art concept-based inter-\nTable 1. INSIGHT discovers well-grounded concepts. We com-\npare against state-of-the-art concept extraction methods on data\nwith available part annotation, including PartImageNet (Part) and\nCoco stuff (Stuff) and measure localization of concepts to an-\nnotated parts (Locality), consistency of localization to same part\nacross images (Consistency), and concept purity across parts (Im-\npurity). Methods with * do not itself provide spatial localization,\nwe use LRP to obtain spatial attributions. Best method in bold.\nLocality \u2191\nConsistency \u2191\nImpurity \u2193\nMethods\nPart\nStuff\nPart\nStuff\nPart\nStuff\nSALF-CBM [1]\n16.4\n19.5\n9.9\n6.7\n0.841\n1.143\nDN-CBM * [57]\n18.7\n17.9\n9.3\n5.7\n0.761\n1.235\nMSAE * [72]\n20.9\n20.0\n10.6\n7.9\n0.833\n1.198\nPatchSAE [33]\n32.2\n34.6\n13.1\n12.7\n0.353\n0.663\nINSIGHT\n44.3\n44.5\n15.3\n13.1\n0.333\n0.539\nFigure 5. Human User Study. Results on Task 1 \u2013 Consistency\nevaluation (left) and Task 2 \u2013 Relationship evaluation (right) for\n\u02dc1000 neurons each covered with 5 users in Amazon MTurk. We\nprovide the scale description given to the user at the bottom.\npretable models specifically designed for classification in\nSec. 4.2. We further show that INSIGHT can be applied\nto yield meaningful image captions along with explana-\ntions for this caption grounded in the input in Sec. 4.3.\nLastly, we consider open vocabulary segmentation (OVS)\nin Sec. 4.4, for which we compare to black-box VLM archi-\ntectures [9, 28, 36, 56, 58, 60, 65, 66] specifically tailored\nOVS. None of the existing CBMs are capable of OVS.\nWe provide details on INSIGHT training in App. B and\nadditional qualitative results in App. C.\n4.1. Concept interpretability\nINSIGHT discovers a large and interpretable space of 8192\nconcepts covering coarse to fine features and their relations.\nWe provide an example relationship graphs of discoverd\nconcepts in in Fig. 3, which shows visually well-grounded\nand meaningfully related concepts of varying granularity\n6\nand provide more examples in App. B.\nQuantitatively, we confirm these initial observations in\nan evaluation on part-annotated datasets PartImagenet [24]\nand COCO-stuff [7] comparing to the recent state-of-\nthe-art concept extraction methods SALF-CBM [1], DN-\nCBM [57], MSAE [72], and PatchSAE [33]. We measure\nhow well concepts align with human-annotation as the max-\nimum IoU between concept localization and annotated seg-\nments across all parts. Further, we measure how consis-\ntent a concept is localized to the same part across images\nby summing IoUs for one annotated part across images and\ntaking the maximum over the parts. Lastly, we consider\nhow semantically pure a concept is by computing the per-\nconcept entropy of normalized activation values across an-\nnotated parts. We report the results in Tab. 1 and provide\nmore details on metrics in App. A. INSIGHT consistently\noutperforms all considered competitors across datasets and\nmetrics, confirming our qualitative observation. Especially\nin terms of locality, INSIGHT provides a huge improve-\nment over existing works. Taking the results together, we\nconfirm that INSIGHT provides more interpretable, well-\ngrounded concepts than previously possible.\nWhile relying on human-annotated part data for quan-\ntitative evaluation is an established benchmark in the field\nand provides a proxy for interpretability, ultimately we are\ninterested in how well humans perceive and understand our\ndiscovered concepts. We conducted a large-scale human-\nuser study covering 1000 concepts through Amazon MTurk.\nWe partitioned concepts into five bins based on activation\nfrequency, resembling the Matryoshka shells, and sampled\nconcepts uniformly at random from each bin. In Task 1, we\nshowed the top-5 most activating images of a concept along\nwith its spatial grounding in each image and asked users to\nrate on a 5-point scale, where 5 is best. In Task 2, we asked\nusers to evaluate the quality of a relationship discovered be-\ntween two concepts, showing the top-3 most activating im-\nages and spatial grounding for both concepts, respectively.\nWe then asked to rate how related the two concepts are on a\n5-point scale, where 5 is best.\nWe show aggregated results in Fig. 5 and provide de-\ntailed study design and results in App. A. Overall, in both\ntasks users rated concepts mostly or completely consistent\nresp.\nrelated for more than half of the shown concepts.\nMore than 80% of the evaluations assign at least some-\nwhat consistent resp. related as a score (3 or higher). For\nan additional task of concept naming evaluation, we find\nthat free-form text annotations given by users for a concept\nare significantly more similar to INSIGHT\u2019s concept nam-\ning compared to a random baseline (App. A). Overall, this\nshows that our concept representation space, learned with-\nout any concept labels, provides human users with con-\nsistent concepts and meaningful concept relations.\n4.2. Image classification\nTable 2.\nINSIGHT shows competitive classification perfor-\nmance while providing spatially grounded concepts. We com-\npare INSIGHT SOTA concept extraction methods and baselines on\nImageNet (IMN) and Places365 (Places). \u201dSpatial Grounding\u201d in-\ndicates whether methods extract spatially grounded concepts (\u2713),\npartially local concepts (\u2022), or global concepts only (\u2717). We show\nlinear probe performance of INSIGHT on max- and mean-pooled\nconcept activations across patches. Baseline results taken from\nPrasse et al. [51], we retrained INSIGHT on the same OpenAI-\nCLIP backbone, best method in bold.\nModel\nSpatial Grounding\nIMN\nPlaces\nLinear Probe\n-\n80.2\n55.1\nZero Shot\n-\n59.6\n38.7\nLF-CBM [44]\n\u2717\n75.4\n48.2\nLaBo [68]\n\u2717\n78.9\n-\nCDM [47]\n\u2717\n79.3\n52.6\nDCLIP [38]\n\u2717\n68.0\n40.3\nDN-CBM [57]\n\u2717\n79.5\n55.1\nD-CBM [51]\n\u2717\n70.5\n50.9\nCF-CBM [48]\n\u2022\n78.5\n-\nSALF-CBM [1]\n\u2713\n76.3\n49.4\nINSIGHT\n\u2713\n78.9\n55.4\nExisting concept extraction methods are usually only de-\nsigned and trained for image classification.\nTo compare\non common ground with such interpretable methods, we\nhere consider image classification on ImageNet [59] and\nPlaces365 [74] by training a linear classification head on\ntop of each model. We compare INSIGHT with LF-CBM\n[44], LaBo [68], CDM [47], DCLIP [38], DN-CBM [57],\nD-CBM [51], CF-CBM [48], SALF-CBM [1], most of\nwhich also rely on a CLIP backbone with ViT-B architec-\nture. Apart from SALF-CBM and INSIGHT, none of the\nother approaches extracts spatially grounded concepts, but\nrather provide image-level concepts, which are useful for\nthe image-level task of classification. Despite focusing on\nextracting more interpretable and spatially grounded con-\ncepts (Fig. 4), showing less spurious correlations, we ob-\nserve in Tab. 2 that INSIGHT yields competitive classifi-\ncation performance on par with the best existing, non-\nlocalized concept approach DN-CBM.\n4.3. Image captioning\nWe explored the performance of INSIGHT\u2019s interpretable\nconcept-based representation for the generative task of im-\nage captioning.\nWe closely followed the setup of [39],\nappending the spatial output tokens to a language model\nthrough a transformer adapter, finetuning both the adapter\nand captioner on COCO [34](Karpathy splits). In Fig. 6-\ntop we observe that INSIGHT maintains performance similar\nto the opaque, dense representation of the backbone, while\nproviding an explicit concept representation, which can be\nedited to change the resulting captions (bottom).\n7\nMethods\nBLEU @ 4 \u2191\nCIDEr \u2191\nMETEOR \u2191\nCLIP-DINOiser\u2217[64]\n0.309\n1.041\n0.264\nINSIGHT\n0.309\n1.037\n0.264\nINSIGHT\nCaptioning Head\nA boy sitting on a bed using a notebook.\n\u2193Notebook concepts\n\u2193Bed concepts\n\u2193Male &\u2191Female concepts\nA boy sitting on a bed with a cell phone.\nA boy sitting in a chair holding a pencil.\nA woman sitting on a bed reading a book.\nNamed\nConcepts\nNo intervention\nFigure 6. INSIGHT yields competitive and steerable captioning.\n(top): captioning capabilities of INSIGHT compared to its opaque\nbackbone. (bottom): Through named concepts, INSIGHT allows\nfor concept-level intervention, steering the image captioning.\n4.4. Open Vocabulary Segmentation (OVS)\nIn OVS, the goal is to obtain a semantic segmentation of an\nimage without requiring expensive data annotation limited\nto a specific and finite set of classes. Instead, a language-\naligned vision model is used to segment images based on\nan open and flexible vocabulary. CLIP-DINOiser, with its\nsemantically meaningful local features, set a strong baseline\ncompared to existing specialized models [64].\nWe compare against the reported performance of\nprototype-based [28, 60], segmentation-specific [9, 36, 56,\n58, 65, 66], as well as MaskCLIP and CLIP-DINOiser\nversions [64, 76] on a variety of benchmark data, in-\ncluding PASCAL (VOC & VOC20) [20], PASCAL (Con-\ntext & Context59) [40], Coco (Object [34] & Stuff [7]),\nCityscapes [15], and ADE20K [75]. We report mIoU with\nand without a background prompt following the established\nbenchmarking protocol in the literature [64]. For INSIGHT,\nwe compute the cosine similarity between the embedding\nof each vocabulary word with the reconstructed patch em-\nbedding of the SAE to determine the closest matching word.\nThrough decoder weights, we moreover get an interpretable\nsegmentation through linearly weighted concepts.\nIn Tab. 3, we observe that despite being an interpretable\nmodel, INSIGHT performs on par with the strongest\nblack-box baselines in terms of mIoU, such as CLIP-\nDINOiser and OVDiff. It also outperforms existing spe-\ncialized architectures by a wide margin across datasets. In\nFig. 7 we provide curated qualitative examples of OVS\non COCO-Stuff highlighting the interpretable downstream\ndecision-making provided by INSIGHT.\n5. Discussion and Conclusion\nWe proposed INSIGHT, a concept-based representation\nbased on language-aligned vision encoder that represents\nspatially grounded and human-interpretable concepts of dif-\nferent granularity. As such, INSIGHT sets a new standard in\nthe field of concept-based interpretability providing mean-\nTable 3.\nINSIGHT provides strong open-vocabulary seman-\ntic segmentation. We report mIoU between segmentations and\nannotated parts for each dataset considering evaluation with and\nwithout a \u2018background\u2019 prompt as discussed in App. A."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**Summary of the Paper \"INSIGHT: Interpretable Semantic Hierarchies in Vision-Language Encoders\"**\n\n**Main Contributions**\n- **Hierarchical Concept Extraction**: Proposes learning hierarchical concept extraction to provide human-interpretable concepts at diverse granularities that are spatially grounded.\n- **Explicit Concept Relationships**: Develops methods to discover and extract explicit hierarchical relationships between these concepts, improving the understanding of the model's reasoning process.\n- **Improved Concept Naming Scheme**: Introduces a scheme for improved concept labeling by explicitly considering parent-child relationships from the hierarchy, ensuring more accurate and meaningful labels.\n\n**Methodology**\nINSIGHT leverages a hierarchical sparse autoencoder (SAE) to extract human-interpretable concepts at various granularities. The model uses CLIP-DINOiser as its backbone, which encodes images such that semantically similar regions share similar representations informed by DINO [8]. Concepts are extracted patch-wise without supervision using the SAE, and their relationships are discovered through examining co-occurrences of concept activations within patches.\n\n**Key Findings**\n- **Interpretability**: INSIGHT provides more interpretable concepts than existing methods across diverse datasets. It yields performance competitive with opaque foundation models on tasks like classification and segmentation.\n- **Spatial Grounding**: The model's concepts are spatially grounded, enabling explanations that align well with the input image features.\n- **Concept Hierarchies**: Concepts are organized into hierarchies with explicit parent-child relationships, allowing for a more nuanced understanding of the learned semantics.\n\n**Conclusions**\nINSIGHT successfully addresses the limitations of existing language-aligned vision foundation models by providing rich and interpretable concept representations. It achieves competitive performance on downstream tasks like classification and segmentation while offering detailed explanations grounded in the input image. The model's hierarchical structure and explicit relationships between concepts make it a significant advancement in the field of interpretability for vision-language models."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition\nEmily Kim\nCarnegie Mellon University\nekim2@andrew.cmu.edu\nAllen Wu\nCarnegie Mellon University\nallenwu@andrew.cmu.edu\nJessica Hodgins\nCarnegie Mellon University\njkh@andrew.cmu.edu\nAbstract\nDespite significant progress in human action recognition,\ngeneralizing to diverse viewpoints remains a challenge.\nMost existing datasets are captured from ground-level per-\nspectives, and models trained on them often struggle to\ntransfer to drastically different domains such as aerial\nviews. This paper examines how curriculum-based training\nstrategies can improve generalization to unseen real aerial-\nview data without using any real aerial data during train-\ning.\nWe explore curriculum learning for cross-view action\nrecognition using two out-of-domain sources:\nsynthetic\naerial-view data and real ground-view data. Our results\non the evaluation on order of training (fine-tuning on syn-\nthetic aerial data vs. real ground data) shows that fine-\ntuning on real ground data but differ in how they transi-\ntion from synthetic to real. The first uses a two-stage cur-\nriculum with direct fine-tuning, while the second applies a\nprogressive curriculum that expands the dataset in multiple\nstages before fine-tuning. We evaluate both methods on the\nREMAG dataset using SlowFast (CNN-based) and MViTv2\n(Transformer-based) architectures.\nResults show that combining the two out-of-domain\ndatasets clearly outperforms training on a single domain,\nwhether real ground-view or synthetic aerial-view.\nBoth\ncurriculum strategies match the top-1 accuracy of simple\ndataset combination while offering efficiency gains. With\nthe two-step fine-tuning method, SlowFast achieves up to a\n37% reduction in iterations and MViTv2 up to a 30% reduc-\ntion compared to simple combination. The multi-step pro-\ngressive approach further reduces iterations, by up to 9%\nfor SlowFast and 30% for MViTv2, relative to the two-step\nmethod. These findings demonstrate that curriculum-based\ntraining can maintain comparable performance (top-1 ac-\ncuracy within 3% range) while improving training efficiency\nin cross-view action recognition.\nTest Data \ud83d\udd0e\nReal Aerial Data\nTrain Data 1 \u2705\nSyn Aerial Data\nTrain Data 2 \u2705\nReal Ground Data\nCurriculum-based Learning\nfor Cross-View Domain Transfer\nA. Real + Synthetic\n1. Inputs\n2. Methods\nB. Two-step Non-Progressive FT (syn to real)\nC. Multi-step Progressive + FT method\nModel\nModel\nModel\ntrain\npre-train\nfine-tune\nUpdate\nweights\nModel\nModel\ntrain\nfine-tune\nUpdate\nweights\nModel\ntrain\nUpdate\nweights\n3. Results\nFigure 1. Curriculum-based learning strategies for cross-view\ndomain transfer. We compare three approaches for training ac-\ntion recognition models using two out-of-domain datasets: syn-\nthetic aerial-view and real ground-view videos. All models are\nevaluated on the unseen domain of real aerial-view videos. In\nthis plot, we report the performance of the different methods on\nMViTv2 as the top-1 accuracy and the efficiency as the number\nof iterations (we fix all other parameters including batch size, in-\nput resolution, augmentations, and optimizer). Our results demon-\nstrate that while all methods yield comparable top-1 accuracy, the\nprogressive strategy achieves the highest training efficiency.\narXiv:2601.14101v1  [cs.CV]  20 Jan 2026\n1. Introduction\nWhile domain-specific datasets are highly effective for\ntraining action recognition models to perform well in a tar-\nget test domain, they are not always readily available and\nmay not have the required quantity or diversity of the data.\nThis scarcity is particularly evident in aerial-view data,\nwhere collecting large-scale, domain-aligned video data is\nfar more challenging than for standard ground-level views.\nAs a result, progress in the field has often been driven by\nlarge-scale, general-purpose datasets such as Kinetics [11],\nwhich offer hundreds of action classes captured from stan-\ndard ground-level viewpoints. However, models trained on\nsuch datasets often fail to generalize to novel viewpoints,\nparticularly when transferring from ground-view to aerial-\nview scenarios. This performance degradation is largely\ndue to the substantial domain shift introduced by the change\nin camera perspective [17].\nTo address this challenge, recent work has explored us-\ning synthetic data to augment or replace real-world video in\nlow-resource or difficult-to-annotate settings. Datasets such\nas REMAG [17], RoCoG [3], and SURREACT [21] lever-\nage computer-generated imagery or neural rendering to pro-\nduce large-scale, multi-view action sequences with dense\nannotations.\nAmong these, REMAG is notable for pro-\nviding both real and synthetic data from ground and aerial\nviewpoints, enabling controlled studies of domain transfer.\nStudies on this dataset found that: (i) synthetic data can sig-\nnificantly improve performance when paired with a small\namount of domain-matched real data; (ii) synthetic data\nalone remains insufficient, particularly for aerial views, due\nto appearance and motion realism gaps; and (iii) training\nsolely on ground-view data transfers poorly to aerial-view\ntasks.\nBecause aerial data collection requires substantial time,\neffort, and expertise, assembling large domain-specific\ndatasets is often impractical.\nIn such cases, synthetic\ndatasets simulating aerial conditions offer a more accessi-\nble alternative. When real ground-view data for a matching\ndomain is available, it becomes important to determine how\nsynthetic aerial-view and real ground-view data can be ef-\nfectively combined. These two sources are naturally com-\nplementary: synthetic aerial data provides viewpoint diver-\nsity, while real ground-view data contributes realistic ap-\npearance and motion cues.\nA straightforward approach is to combine the two\ndatasets and train a model jointly.\nWhile this increases\ndata diversity, it is unclear whether it is the most effective\nstrategy. Prior work suggests that structuring the training\nprocess can improve both efficiency and performance [1],\nrather than exposing the model to all samples at once.\nCurriculum learning addresses this by presenting data in a\nmeaningful order, typically from easier to more difficult ex-\namples. Building on this idea, progressive learning incre-\nmentally expands the training set over time, offering further\ngains in training efficiency and stability [19].\nMotivated by these findings, we investigate curriculum-\nbased and progressive training strategies for combining syn-\nthetic aerial-view and real ground-view data.\nStructured\ntraining schedules can help stabilize learning by introduc-\ning domain-shifted data in a controlled manner. Hence, in\nthis paper, we assess whether such schedules can outper-\nform na\u00a8\u0131ve data combination in both performance and effi-\nciency when generalizing to real aerial views.\nIn this work, we study curriculum-based learning for\ncross-view action recognition. Our training framework in-\ntroduces synthetic aerial and real ground-view datasets in\na structured manner to improve efficiency to real aerial-\nview scenarios. We train models using only synthetic aerial\nand real ground-view data, with evaluation performed ex-\nclusively on real aerial data.\nOur contributions are as follows:\n\u2022 We investigate curriculum learning for cross-view action\nrecognition, focusing on generalizing to real aerial-view\ndata using synthetic aerial-view and real ground-view\ntraining data without access to any real aerial data dur-\ning training.\n\u2022 We compare two curriculum strategies: a two-step fine-\ntuning method and a multi-step progressive learning\nmethod, each introducing domain-shifted data in struc-\ntured ways to avoid potential confusion.\n\u2022 We conduct empirical evaluations on the REMAG dataset\nusing two strong baseline models, SlowFast and MViTv2,\nreporting both top-1 accuracy and training efficiency.\n\u2022 Our results show that curriculum-based strategies achieve\ncomparable performance while offering practical benefits\nin training efficiency compared to simply combining the\ntwo datasets, demonstrating their effectiveness as alterna-\ntives for domain-shifted action recognition.\n2. Related Work\nThis section reviews three lines of work relevant to our\nmethod: (1) foundational models and datasets for video-\nbased action recognition, (2) synthetic data for domain\ntransfer, and (3) curriculum-based learning strategies for\nstructured training with synthetic data.\n2.1. Core Action Recognition Models and Datasets\nEarly deep-learning\u2013based action recognition models, such\nas I3D [2], extended 2D convolutional networks by inflat-\ning spatial kernels into the temporal domain, enabling di-\nrect modeling of spatiotemporal features. Building on this\nidea, SlowFast [7] introduced a dual-pathway design: a\nslow pathway operating at a low frame rate to capture high-\nlevel semantic context, and a fast pathway operating at a\nhigh frame rate to preserve fine-grained motion cues. This\narchitecture improved temporal fidelity while maintaining\ncomputational efficiency, establishing SlowFast as a widely\nadopted benchmark model.\nThe field has since shifted toward Transformer-based\narchitectures, which capture long-range dependencies and\nmultiscale spatiotemporal patterns through self-attention.\nMViTv2 [15] enhances the original Multiscale Vision\nTransformer [6] with pooling attention and improved po-\nsitional encodings, yielding strong performance on large-\nscale video benchmarks.\nVideo Swin Transformer [16]\nadapts the Swin Transformer from images to video via\nshifted-window self-attention over spatiotemporal tokens,\npreserving locality-inductive biases while enabling effi-\ncient scaling. VideoMAE V2 [22] advances masked au-\ntoencoding for video by introducing a dual-masking strat-\negy\u2014masking both encoder and decoder inputs\u2014allowing\nefficient pre-training of billion-parameter video transform-\ners and achieving state-of-the-art results across multiple\nvideo understanding tasks.\nIn\nthis\nwork,\nwe\nselect\nSlowFast\nand\nMViTv2\nas our primary baselines, as they are widely recog-\nnized, well-studied architectures that represent comple-\nmentary paradigms\u2014CNN-based dual-pathway design and\ntransformer-based multiscale attention\u2014providing a bal-\nanced foundation for evaluating our domain-transfer meth-\nods.\nThese models are typically trained and evaluated on\nlarge-scale benchmarks such as Kinetics [11], which has\nexpanded from 400 to over 700 action classes and con-\ntains more than 650,000 video clips. Earlier datasets, in-\ncluding UCF-101 [20] and HMDB-51 [13], remain popu-\nlar for evaluation, particularly in studies on generalization\nand domain adaptation. In the aerial-view domain, UAV-\nHuman [14] is among the largest benchmarks, compris-\ning 155 action classes from 119 subjects with 22.5k video\nclips. Although smaller in scale, REMAG [17] offers syn-\nchronized aerial and ground views alongside corresponding\nsynthetic data, making it particularly well-suited for cross-\nview domain transfer experiments (Section 2.2). The syn-\nchronized aerial\u2013ground recordings in REMAG simplify\ncross-view learning by providing perfectly aligned tempo-\nral pairs, which reduce viewpoint ambiguity and enable\ncleaner supervision signals. However, collecting such data\nis challenging\u2014requiring multi-camera setups with precise\ntime synchronization, coordinated UAV and ground cap-\nture, and adherence to the logistical and regulatory con-\nstraints of aerial filming\u2014and is generally impractical in\nreal-world deployment scenarios where only a single view-\npoint is available. This motivates our approach of using\nsynthetic aerial-view data in combination with real ground-\nview data, enabling cross-view domain transfer without the\nneed for synchronized multi-view recordings.\n2.2. Synthetic Data and Domain Transfer\nSynthetic data has emerged as a scalable alternative to real\nvideo, particularly in domains where labeled data is scarce.\nEarly work such as PHAV[4] procedurally generated 39,982\nsynthetic clips using game engines to augment datasets like\nUCF-101 and HMDB-51. More recent pipelines, such as\nSURREACT [21], employed Blender to animate 3D human\nmodels reconstructed from real motion using HMMR [10]\nand VIBE [12], enabling synthetic-to-real generalization.\nOther datasets, including RoCoG [3] and RoCoG-\nv2 [18], targeted robot control tasks and provided paired\nsynthetic and real videos for cross-domain benchmarking,\nhighlighting synthetic data\u2019s role in handling distribution\nshifts.\nRecent studies have demonstrated that models trained\nentirely on synthetic data can be competitive with real-data\nbaselines. Kim et al.[23] introduced SynAPT, a unified suite\nof synthetic datasets, and showed that synthetic pretraining\ncan outperform real-only training in low-resource regimes,\neven without overlapping label spaces. Guo et al.[9] sim-\nilarly found that video representations learned solely from\nsynthetic human motion can generalize effectively to real-\nworld action recognition tasks without real supervision.\nPanev et al. [17] introduced REMAG, a comprehensive\ndataset containing real, CGI-rendered, and neural-rendered\nvideos derived from both motion capture and monocular\nRGB tracking.\nTheir benchmarks analyzed how differ-\nent rendering methods and motion sources influence down-\nstream performance. REMAG\u2019s inclusion of synchronized\naerial and ground views in both real and synthetic formats\nmakes it a strong candidate for studying cross-view domain\ntransfer. In our work, we directly leverage REMAG to eval-\nuate performance across both domain and viewpoint shifts.\n2.3. Curriculum-based Learning with Synthetic\nData\nCurriculum-based learning structures the training process\nby controlling the sequence and difficulty of training sam-\nples. Instead of exposing the model to all data simulta-\nneously, training begins with simpler examples and grad-\nually incorporates more complex ones. Bengio et al.[1] in-\ntroduced this concept, demonstrating improvements in con-\nvergence and generalization. Graves et al.[8] extended the\napproach with an automated sampling strategy based on a\nmulti-armed bandit framework, dynamically selecting ex-\namples according to the model\u2019s learning progress.\nThis strategy has been shown to benefit various vision\ntasks. For instance, Zhang et al. [24] applied a curriculum-\nbased domain adaptation method to semantic segmentation,\nimproving generalization from synthetic to real-world ur-\nban scenes.\nRecent work has explored progressive curriculum strate-\ngies, in which the size or difficulty of the training set in-\n(a) Train, test data domain analysis\n(c) Multi-step Progressive Learning\n(b) Two-step Fine-tuning\ntrain\ntrain\nfine-tune\nModel (r=1)\nModel (r=2)\nModel (r=3)\nUpdate\nweights\nUpdate\nweights\nr = 1\nr = 2\nr = 3\nTraining Progression\nSynthetic aerial\ntrain data \u2705\nReal ground\ntrain data \u2705\nReal aerial\ntest data \ud83d\udd0e\nReal\nSynthetic\nx\ntrain\nfine-tune\nr = 1\nr = 2\nModel (r=1)\nModel (r=2)\nUpdate\nweights\ntrain\nfine-tune\nr = 1\nr = 2\nModel (r=1)\nModel (r=2)\nUpdate\nweights\nAerial View\nGround View\nDomain\nViewpoint\nFigure 2. Overview of our progressive learning pipeline. (a) Viewpoint\u2013domain matrix of training and testing modalities. We use\nsynthetic aerial and real ground-view data for training, and evaluate only on real aerial-view data. (b) Progressive learning procedure with\nR = 3 rounds of training. The model is updated iteratively across rounds. The first R\u22121 rounds are trained with synthetic aerial data. In\nthe final round, we compare two variants: (i) combined training using both synthetic aerial and real ground-view data, and (ii) fine-tuning\nusing only real ground-view data.\ncreases over time. PDE [5] adopted this approach to im-\nprove worst-group accuracy by 2.8% while reducing train-\ning time by a factor of ten on classification benchmarks.\nPTL [19] applied progressive learning to object detection in\nUAV imagery, showing that gradually introducing synthetic\ndata enhances cross-domain generalization.\nMotivated by these results, we extend progressive\ncurriculum strategies to cross-domain video-based action\nrecognition. We specifically evaluate whether sequentially\nintroducing synthetic and real data, either via multi-stage\nfine-tuning or progressive dataset expansion, can match or\nsurpass static training with a combined dataset, while also\nimproving training efficiency.\n3. Method\nOur goal is to train a video-based action recognition model\nthat generalizes well to real aerial data, without using any\nreal aerial data during training. We compare three different\ntraining strategies using two out-of-domain datasets: syn-\nthetic aerial-view data (Dsyn) and real ground-view data\n(Dreal). The objective is to train efficiently and achieve high\naccuracy.\nNaive combination (baseline).\nThe baseline strategy\nsimply combines Dsyn and Dreal to train the model M0 in\na single step. This approach is the most straightforward,\ngiven that both datasets are slightly shifted from the target\ndomain.\nTwo-step fine-tuning.\nThe first curriculum-based learn-\ning method is a two-step fine-tuning strategy. We first pre-\ntrain the model M0 on one dataset and then fine-tune it on\nthe other. In our main setting, we pre-train on Dsyn and\nfine-tune on Dreal to form M2. This sequential adaptation\nenables the model to gradually transition between domains.\nWhile the reverse direction (real-to-synthetic) is possible,\nwe did not evaluate it here due to the smaller size and diver-\nsity of Dreal.\nMulti-step\nprogressive\nlearning.\nThe\nsecond\ncurriculum-based strategy is a multi-step progressive\nlearning method. We train the model in three steps, where\nthe first two steps are progressive expansion and the last\nstep is the fine-tuning. In the first round, we train on a 50%\nrandom subset of Dsyn, class-balanced and disjoint from the\nremaining data. In the second round, we expand training\nTraining Strategy\nTop-1 Accuracy (%)\nSlowFast (T1)\nSlowFast (T2)\nMViTv2 (T1)\nMViTv2 (T2)\nReal Only (Aerial)\n63.25\n68.36\n72.43\n72.90\nReal Only (Ground)\n41.58\n43.53\n59.72\n61.96\nSynthetic Only (Aerial)\n52.07\n53.46\n59.01\n64.31\nReal (G) + Synthetic (A)\n58.12\n59.01\n71.74\n71.02\nNon-Progressive + FT (R-to-S)\n59.98\n58.88\n69.26\n67.69\nNon-Progressive + FT (S-to-R)\n60.90\n61.45\n70.82\n72.48\nProgressive + FT\n60.53\n59.56\n70.79\n72.05\nTable 1. Top-1 Accuracy (%) on Real Aerial Test Set across Training Strategies. We report results for SlowFast and MViTv2 models\nunder different training strategies. T1 and T2 denote two distinct real aerial-view test sets. All models are trained on synthetic aerial-view\nand real ground-view data, and evaluated on unseen real aerial-view videos.\nto the full Dsyn, allowing the model to adapt to increased\ndomain complexity.\nFinally, we fine-tune the resulting\nmodel on Dreal. This gradual expansion allows the model\nto learn initial representations from a simpler synthetic\ndomain before being exposed to the full data distribution.\nBy structuring the learning process in this way, we avoid\nabrupt domain shifts and large optimization jumps, leading\nto more stable convergence and improved generalization.\nWe did not explore progressive pretraining in the reverse\ndirection due to the smaller scale of the real dataset. We\nalso limited the number of rounds to three to make the\nexperiments simple.\nWe compare the performance and training efficiency of\nthese three strategies to evaluate their ability to transfer to\nthe real aerial domain. Our results suggest that curriculum-\nbased strategies offer a robust alternative to naive domain\nmixing, improving both convergence stability and cross-\nview generalization.\n4. Experiments\nWe evaluated our proposed progressive training approach\non the REMAG [17] dataset, which includes 11 action\nclasses plus an idle class. For evaluation, we selected two\ndistinct 5-subject sets (approximately 8k samples) from the\nreal aerial-view data as test sets. For each test set, the cor-\nresponding training set consisted of real ground-view data\ncovering 21 subjects, along with synthetic aerial-view data\ngenerated using CGI rendering and marker-based motion\ncapture. Test subjects were excluded from all training data.\nTo ensure balanced training across all classes, we ap-\nplied oversampling such that each class contained approx-\nimately 6,000 samples in both the synthetic and real do-\nmains. Video clips were sampled at 64-frame intervals (30\nfps, \u223c2 seconds), and only clips in which at least 80% of\nthe frames shared the same class label were retained to en-\nsure temporal consistency. For both SlowFast and MViTv2,\nwe extracted 16 frames per clip, uniformly sampled every 4\nframes within each 64-frame segment. We did not add any\naugmentation to the training data.\nWe evaluated our method using two state-of-the-art ac-\ntion recognition models: SlowFast [7] and MViTv2 [6],\nboth initialized with Kinetics-pretrained weights. Training\nwas conducted on two NVIDIA GeForce RTX 4090 GPUs\n(each with 24 GB of memory). The batch size per GPU was\n45 for SlowFast and 12 for MViTv2. For evaluation, we\nreport top-1 classification accuracy on the real aerial-view\ntest set.\n4.1. Experimental setup\nWe compared the performance of models trained using five\ndifferent strategies involving synthetic aerial-view and real\nground-view data. First, we evaluated models trained using\nonly synthetic aerial-view data and only real ground-view\ndata, which serve as lower-bound baselines. We then evalu-\nated three strategies described in Section 3: (1) naive com-\nbination of the two datasets, (2) two-step fine-tuning, and\n(3) multi-step progressive learning.\nProgressive learning was conducted over R = 3 rounds.\nFor each round r, we trained for er epochs. For SlowFast,\nwe used e1 = e2 = 150 epochs; for MViTv2, we used\ne1 = 30 and e2 = 60 epochs, based on our observation from\nthe loss curve, making sure that the model was sufficiently\ntrained on the provided training data without overfitting. In\nboth models, the final round was trained until convergence.\nAll models were optimized using AdamW. Learning rates\nwere set to 0.1 for SlowFast and 1e-4 for MViTv2 during\nstandard training. For fine-tuning, we reduced the learning\nrates to 0.05 for SlowFast and 5e-5 for MViTv2.\nWe ran the same series of experiments for two held-out\ntest sets. We report top-1 accuracy for all experiments in Ta-\nble 1 and in Figure 3. We trained domain-specific models,\neach trained on Real Only (Aerial), Real Only (Ground),\nand Synthetic Only (Aerial). We label the na\u00a8\u0131ve combina-\nFigure 3. Top-1 accuracy across training strategies for two test sets and two model architectures (SlowFast and MViTv2). Each\ngroup corresponds to a training method: Real Only, Synthetic Only, Real (G) + Synthetic (A), Non-Progressive + FT (R-to-S), Non-\nProgressive + FT (S-to-R), and Progressive + FT. Bars are grouped by model type within each test set. Results show that combining real\nand synthetic data\u2014especially through structured curricula\u2014significantly improves generalization to the target aerial-view domain.\ntion as Real (G) + Synthetic (A), the two-step fine-tuning\nmethod pretrained on real and fine-tuned on synthetic as\nNon-Progressive + FT (R-to-S), the two-step fine-tuning\nmethod pretrained on synthetic and fine-tuned on real as\nNon-Progressive + FT (S-to-R), and the multi-step progres-\nsive method as Progressive + FT.\nIn addition to evaluating model performance, we ana-\nlyzed the computational efficiency of each method for com-\nbining the two training datasets. Because we used fixed\nbatch sizes for all experiments, we measured efficiency by\ncomparing the total number of training iterations required\nto complete training. Specifically, we evaluated: (1) naive\ncombination of synthetic and real data, (2) two-step fine-\ntuning (synthetic to real), and (3) progressive curriculum\nlearning using incrementally larger subsets of synthetic data\nfollowed by real data. The number of training iterations for\neach setup is shown in Figure 4.\n4.2. Analysis of Training Strategy Components\nWe now analyze the results in Figure 3, Table 1, and Fig-\nure 4, comparing the different combinations of training\nmethods in the following paragraphs: domain-specific train-\ning results, curriculum vs. non-curriculum results, training\ndirection, and progressive vs. non-progressive scheduling.\nDomain-Specific Training Results.\nWe first trained the\nmodels on real aerial-view training data from the same do-\nmain as the test sets. SlowFast achieved 63% and 68% ac-\ncuracy on Test Sets 1 and 2, respectively, while MViTv2\nreached around 72% on both. These results represent per-\nformance without domain shift and serve as an upper bound\nfor the subsequent experiments.\nIn contrast, models trained solely on real ground-\nview or synthetic aerial-view data performed substantially\nworse.\nFor SlowFast, the Real-only (Ground) model\nachieved about 42% top-1 accuracy, while the Synthetic-\nonly (Aerial) model reached roughly 53%.\nMViTv2, by\ncomparison, yielded similar performance in both cases,\nwith top-1 accuracy around 60%. These findings suggest\nthat the transformer-based MViTv2 generalizes more ro-\nbustly across domains, whereas the CNN-based SlowFast\nis more sensitive to domain shift. Notably, synthetic data\ncan outperform real data when its viewpoint coverage bet-\nter matches the target domain, as observed in SlowFast\u2019s\nhigher accuracy under synthetic-only training. Still, neither\ndataset alone provides strong generalization, underscoring\nthe importance of combining complementary data sources.\nNon-Curriculum-Based vs.\nTwo-Step Fine-Tuning.\nGiven the two distinct training sources, a natural baseline is\nto combine the datasets directly or to adopt a two-step fine-\ntuning approach. In terms of generalization performance,\nmerging real and synthetic data (Real (G) + Synthetic (A))\nprovides a clear improvement over single-domain train-\ning, achieving 59% accuracy for SlowFast and 71% for\nMViTv2. This demonstrates the benefit of increasing data\ndiversity and domain coverage.\nThe two-step fine-tuning approach (Non-Progressive +\nFT) achieves comparable or slightly better performance.\nFor SlowFast, the S-to-R variant consistently outperforms\nReal (G) + Synthetic (A) across both test sets. For MViTv2,\nthe difference is more nuanced: Real (G) + Synthetic (A)\nperforms slightly better on Test Set 1, while S-to-R out-\nperforms it on Test Set 2. These differences are modest\n(0.9\u20132.8% for SlowFast and 0.9\u20134.8% for MViTv2), but\nthey suggest that a structured training sequence can provide\nFigure 4. Evaluation of computational efficiency across training strategies. We compare the total number of training iterations required\nfor each method\u2014Real (G) + Synthetic (A), Non-Progressive + FT (S-to-R), and Progressive + FT\u2014across two test sets for both SlowFast\nand MViTv2. While all methods achieve comparable top-1 accuracy, the Progressive + FT approach consistently requires fewer iterations,\nhighlighting its efficiency. Notably, Progressive + FT reduces the training cost by up to 30% compared to naive or two-step fine-tuning\nstrategies, making it a favorable option in resource-constrained settings. Since the batch size, input resolution, augmentations, and optimizer\nare fixed, the per-iteration cost is effectively constant, making iteration count a reliable proxy for wall-clock training time within a model.\nbenefits, particularly for architectures more sensitive to do-\nmain gaps like CNNs.\nWhere two-step fine-tuning clearly excels is in training\nefficiency.\nAcross both models, it requires substantially\nfewer iterations to reach comparable accuracy. For Slow-\nFast, S-to-R reduces the iteration count by 6.5k (23%) on\nTest Set 1 and 14.4k (37%) on Test Set 2. For MViTv2,\nthe reductions range from 11.9k (23%) to 15.6k (30%). Be-\ncause the batch size and hardware are fixed, these reduc-\ntions translate directly into proportional savings in compute\ntime, resulting in faster training and lower resource usage\nwithout compromising accuracy.\nCurriculum Direction.\nWe examined whether the order\nof training affects generalization performance when using\nthe two out-of-domain datasets: synthetic aerial (Dsyn) and\nreal ground-view (Dreal). Each introduces a different do-\nmain gap: Dsyn lacks realism, while Dreal differs in view-\npoint. It is unclear a priori whether training should begin\nwith synthetic and then fine-tune on real (S-to-R), or the re-\nverse (R-to-S).\nAs shown in Table 1, S-to-R consistently outperforms R-\nto-S across both models and test sets. For SlowFast, R-to-S\nachieves around 58\u201359%, while S-to-R reaches about 61%.\nFor MViTv2, R-to-S yields 68\u201369%, and S-to-R improves to\n71\u201372%. While margins are small, the consistent trend sug-\ngests that starting with viewpoint-aligned (synthetic) data,\neven if unrealistic, supports better generalization than be-\nginning with realistic but viewpoint-misaligned data. We\nhypothesize several factors contributing to this result:\n\u2022 Dataset scale and diversity \u2013 The larger synthetic\ndataset exposes the model to a broader range of spa-\ntiotemporal variations, enabling it to learn generalizable\nfeatures before adapting to the real domain.\n\u2022 Clean labels and controlled conditions \u2013 Synthetic data\ntypically provides perfectly accurate labels and uniform\ncoverage of rare poses, viewpoints, and lighting, reducing\nearly training noise and improving representation quality.\n\u2022 Optimization stability \u2013 Beginning with abundant, con-\nsistent synthetic data reduces the risk of early overfitting\nto the smaller real dataset, leading to more stable conver-\ngence and better final performance.\nProgressive vs. Non-Progressive Scheduling.\nWe com-\npared the two curriculum-based strategies: (1) two-step\nfine-tuning (Non-Progressive + FT (S-to-R)), and (2) pro-\ngressive curriculum learning (Progressive + FT). While the\nformer is simpler, the latter systematically increases train-\ning diversity and can improve efficiency.\nIn both cases,\ntraining begins with Dsyn and transitions to Dreal.\nIn terms of performance, both strategies are comparable.\nFor SlowFast, S-to-R achieves around 61%, while Progres-\nsive yields 59.5\u201360.5%. For MViTv2, the difference is less\nthan 1% across test sets.\nHowever, the progressive method shows notable effi-\nciency gains. For SlowFast, training iterations are reduced\nby 1.8k (7%) on Test Set 1 and 2.3k (9%) on Test Set 2.\nFor MViTv2, reductions are 3.3k (8%) and 11.8k (30%),\nrespectively. These results suggest that while both methods\nare similarly effective, the progressive approach achieves\ncomparable performance with significantly less training ef-\nfort.\n5. Discussion and Conclusion\nOur evaluation demonstrates that curriculum-based train-\ning strategies achieve performance comparable to na\u00a8\u0131vely\ntraining on the combined out-of-domain datasets (i.e., real\nground-view and synthetic aerial-view videos), but with sig-\nnificantly greater training efficiency. Among the curriculum\nstrategies, the multi-step progressive method was the most\nefficient, requiring substantially fewer training iterations to\nreach convergence.\nBetween the two non-progressive fine-tuning strategies,\npre-training on viewpoint-aligned but less realistic synthetic\naerial data followed by fine-tuning on real ground-view data\n(S-to-R) consistently resulted in stronger generalization to\nthe real aerial-view test domain. This finding underscores\nthe importance of training order: aligning structural factors\nsuch as viewpoint early in the training process appears to\nprovide a more stable foundation before introducing domain\nrealism.\n5.1. Model-Level Observations\nWe observed that model performance and sensitivity to\ntraining strategies varied across architectures.\nMViTv2,\na transformer-based model, consistently outperformed the\nCNN-based SlowFast across all training regimes, including\nsingle-domain setups (e.g., real-only and synthetic-only).\nMoreover, MViTv2 exhibited greater robustness to differ-\nent training curricula, showing only minor performance\nvariation between strategies, while SlowFast showed larger\ngains from curriculum-based approaches. These findings\nare consistent with prior work suggesting that transform-\ners, with their global receptive fields and reduced induc-\ntive biases, are more resilient to domain shifts. Notably,\nMViTv2 models trained with both real ground-view and\nsynthetic aerial-view data achieved performance compara-\nble to the no-domain-shift setting, further underscoring their\nrobustness and the effectiveness of combining complemen-\ntary data sources.\n5.2. Limitations\nDespite encouraging results, our study has several limita-\ntions.\nFirst, we evaluated only two model architectures\n(SlowFast and MViTv2), which limits the generalizabil-\nity of our findings.\nSecond, our progressive curriculum\nused a fixed three-step schedule for simplicity; adaptive or\nlearnable curriculum schedules may further improve perfor-\nmance and efficiency. Third, we assumed access to clean,\nfully annotated data collected in matched capture environ-\nments for training and testing, where backgrounds, lighting,\nand camera setups were consistent across splits. While this\nisolates the effect of viewpoint change, it likely overesti-\nmates performance compared to real-world scenarios with\nmismatched conditions.\nFourth, the dataset used is rel-\natively small and constrained, potentially simplifying the\ndomain adaptation problem. Finally, while our synthetic\ndata offers strong viewpoint coverage, it lacks photorealism\nand scene complexity, which may reduce its effectiveness\nin generalizing to more realistic aerial environments.\n5.3. Future Work\nFuture work should aim to expand the diversity of train-\ning data\u2014adding more action classes, backgrounds, light-\ning conditions, and subject appearances\u2014to better reflect\nreal-world variability. Improving the realism of synthetic\ndata, for example through appearance transfer or style adap-\ntation, could help close the domain gap. Developing adap-\ntive curriculum strategies that dynamically adjust the train-\ning schedule based on model feedback is another promis-\ning direction. Finally, evaluating curriculum-based meth-\nods across a wider range of architectures and datasets would\nhelp strengthen the external validity of our conclusions and\nfacilitate broader adoption in practical deployment scenar-\nios.\nReferences\n[1] Yoshua Bengio, J\u00b4er\u02c6ome Louradour, Ronan Collobert, and Ja-\nson Weston. Curriculum learning. In ICML, 2009. 2, 3\n[2] Jo\u02dcao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 6299\u20136308, 2017. 2\n[3] Celso M. de Melo, Brandon Rothrock, Prudhvi Gurram,\nOytun Ulutan, and B. S. Manjunath. Vision-based gesture\nrecognition in human\u2013robot teams using synthetic data. In\nProceedings of the IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), 2020. 2, 3\n[4] F. De Souza, A. Gaidon, Y. Wang, and Y. Cabon."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThe paper introduces curriculum-based strategies for cross-domain action recognition in video data, specifically focusing on real aerial-view data without using any real aerial training data. The main contributions include:\n1. **Curriculum Learning Strategies**: Investigates two curriculum learning methods\u2014two-step fine-tuning and multi-step progressive learning\u2014to improve generalization to unseen real aerial-view data.\n2. **Dataset Composition**: Examines the effectiveness of combining synthetic aerial-view data with real ground-view data in a structured manner, leveraging their complementary strengths: synthetic data provides viewpoint diversity while real ground-view data offers realistic appearance and motion cues.\n\n#### Methodology\nThe authors use the REMAG dataset, which includes both real and synthetic aerial and ground views. They compare three training strategies:\n1. **Naive Combination**: Training on either synthetic or real ground-view datasets alone.\n2. **Two-Step Fine-Tuning**: First pre-training on one dataset (synthetic or real) and then fine-tuning on the other.\n3. **Multi-Step Progressive Learning**: Sequentially expanding the training set over multiple stages before fine-tuning.\n\nThey evaluate these strategies using two strong baseline models: SlowFast (CNN-based) and MViTv2 (Transformer-based).\n\n#### Key Findings\n1. **Curriculum-Based Strategies Outperform Naive Combination**:\n   - Curriculum learning methods achieve comparable top-1 accuracy to naive dataset combination but offer significant efficiency gains.\n   - The multi-step progressive approach reduces training iterations by up to 9% for SlowFast and 30% for MViTv2 compared to simple combination.\n\n2. **Progressive Strategy Outperforms Non-Progressive Methods**:\n   - In terms of performance, the multi-step progressive method matches or slightly outperforms other methods.\n   - The progressive strategy reduces training iterations by up to 18k (9%) and 30k (30%) for SlowFast and MViTv2 respectively.\n\n#### Conclusions\nThe paper demonstrates that curriculum-based strategies can maintain comparable performance while improving efficiency in cross-view action recognition. Specifically:\n- **Curriculum Learning Strategies**: Achieve top-1 accuracy within a 3% range compared to naive dataset combination.\n- **Progressive Strategy**: Offers significant training efficiency gains, reducing iterations by up to 90% for SlowFast and 30% for MViTv2.\n\n#### Limitations\nDespite these findings, the study has limitations:\n- Limited model architectures evaluated (SlowFast and MViTv2).\n- Fixed curriculum schedules used.\n- Assumption of clean, fully annotated data in training environments.\n\n### Summary Table\n\n| Training Strategy | Top-1 Accuracy (%) |\n|--------------------|----------------------|\n| Real Only (Aerial) | 63.25                |\n| Real Only (Ground) | 41.58                |\n| Synthetic Only (Aerial) | 52.07             |\n| Real (G) + Synthetic (A) | 58.12              |\n| Non-Progressive + FT (R-to-S) | 59.98            |\n| Non-Progressive + FT (S-to-R) | 60.90            |\n| Progressive + FT | 60.53               |\n\n### Conclusion\nThe paper highlights the effectiveness of curriculum-based training strategies in improving cross-view action recognition without using real aerial data during training, offering significant efficiency gains over naive dataset combination methods. Future work should aim to expand the diversity and realism of training datasets to better reflect real-world variability."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Toward Efficient Agents: A Survey of Memory,\nTool learning, and Planning\nXiaofang Yang1,2,\u2020\nLijun Li1,\u2020,\nHeng Zhou1,3,\u2020\nTong Zhu1,\u2020\nXiaoye Qu1\nYuchen Fan1,4\nQianshan Wei5\nRui Ye4\nLi Kang1,4\nYiran Qin6\nZhiqiang Kou7\nDaizong Liu8\nQi Li5\nNing Ding9\nSiheng Chen4\nJing Shao1,\n1 Shanghai Artificial Intelligence Laboratory, 2 Fudan University,\n3 University of Science and Technology of China, 4 Shanghai Jiaotong University,\n5 Institute of Automation, Chinese Academy of Sciences,\n6 The Chinese University of Hong Kong (Shenzhen),\n7 Hong Kong Polytechnic University, 8 Wuhan University, 9 Tsinghua University\nAbstract:\nRecent years have witnessed increasing interest in extending large language models into agentic systems. While\nthe effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment,\nhas often been overlooked. This paper therefore investigates efficiency from three core components of agents:\nmemory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting\ncomprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent\napproaches that differ in implementation yet frequently converge on shared high-level principles including but\nnot limited to bounding context via compression and management, designing reinforcement learning rewards\nto minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we\ndiscuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness\nunder a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be\nviewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine\nefficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating\ncommonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss\nthe key challenges and future directions, with the goal of providing promising insights.\n\u2020 Main contributors\nCorresponding Author\nKeywords: Agents, Efficiency, Agent Memory, Tool Learning, Planning\nDate: January 20th, 2026\nProjects: https://efficient-agents.github.io/\nCode Repository: https://github.com/yxf203/Awesome-Efficient-Agents\nContact: yangxiaofang@pjlab.org.cn, lilijun@pjlab.org.cn, hengzzzhou@gmail.com, zhutong@pjlab.org.cn\narXiv:2601.14192v1  [cs.AI]  20 Jan 2026\nContents\n1\nIntroduction\n4\n2\nPreliminaries\n5\n2.1\nAgent Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nFrom Pure LLMs to Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nEfficient Memory\n6\n3.1\nMemory Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.1.1\nWorking Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.1.2\nExternal Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.2\nMemory Management\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.2.1\nRule-based Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.2.2\nLLM-based Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.2.3\nHybrid Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.3\nMemory Access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.3.1\nMemory Selection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.3.2\nMemory Integration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.4\nMulti-Agent Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.5\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4\nEfficient Tool Learning\n20\n4.1\nTool Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.2\nTool Calling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.3\nTool-Integrated Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4.4\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5\nEfficient Planning\n27\n5.1\nSingle-Agent Planning Efficiency\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n5.2\nMulti-Agent Collaborative Efficiency\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n5.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n6\nBenchmarks\n30\n2\n6.1\nMemory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n6.2\nTool Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n6.3\nPlanning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n7\nChallenges and Future Directions\n34\n8\nConclusion\n35\n3\n1. Introduction\n2023\n2024\n2025\nMemoryBank\nMemGPT\nLightMem\nReadAgent\nMemory Sharing\nG-Memory\nToolformer\nEfficient Memory\nConstruct\nManage\nAccess\nExpel\nMemAgent\nZep\nAriGraph\nHiAgent\nMemOS\nMemory-R1\nTinyAgent\nBTP\nToolOrchestra\nProTIP\nToolGen\nAutoTIR\nSMART\nToolRL\nSWiRL\nSwiftSage\nETO\nBudget-Aware\nGAP\nCATS\nAlita\nQLASS\nA-MEM\nEvo-Memory\nStoryBench\nMemBench\nTPS-Bench\nCostBench\nMemory\nTool Learning\nPlanning\nBenchmark\nEfficient Tool Learning\nEfficient Planning\nBenchmarks\nSelection\nCalling Reasoning\nLoCoMo\nSWE-Bench\nWebShop\nGAIA\n\ufffd-bench\nWebArena\nBudgeting\nSearch\nLearning\nComputation Time\nInteraction\nEffectiveness First\nToward Efficiency\nMemory-aware\nSmarter Memory\nChain of Agents\nCodeAgents\nMARS\nUltraTool\nFigure 1: The evolutionary trajectory of efficient agent research. The diagram is organized into four principal\nbranches: Memory, Tool Learning, Planning, and Benchmarks. Key works and their institutional affiliations\nare mapped chronologically to illustrate the field\u2019s development and categorization from 2023 to 2025.\nThe landscape of Artificial Intelligence has undergone a paradigm shift, evolving from the era of Convolu-\ntional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to the advent of Large Language\nModels (LLMs), and the emergence of LLM-based Agents currently [53, 27, 107, 44, 171, 37]. Unlike\ntheir predecessors, which primarily focused on perception or static text generation, agentic systems do not\nmerely process information; they actively interact with external environments to execute complex, multi-step\nworkflows across diverse domains, such as autonomous software engineering [183, 166] and accelerated\nscientific discovery [161, 75, 29].\nHowever, this shift toward autonomous action has introduced a critical bottleneck: efficiency. While the\ndeployment of LLMs is already resource-intensive, this challenge is significantly exacerbated in agentic\nsystems. Unlike a standard LLM that typically operates in a linear, single-turn query-response format,\nan agent consumes exponentially more resources due to its recursive nature. To automate intricate real-\nworld tasks [38, 34, 82, 166], agents must perform extensive memory management, iterative tool usage,\nand complex planning over multiple steps. This multi-step execution leads to prohibitive latency, context\nwindow saturation, and excessive token consumption, raising profound concerns regarding the long-term\nsustainability and equitable accessibility of these increasingly capable systems.\nTo understand the urgency of agent efficiency, one must examine the typical agentic workflow. Upon\nreceiving a user instruction, an agent engages in a recursive loop that heavily uses the following key\n4\ncomponents: memory, planning, and tool learning to observe output and provide the final solution.\nInput \u2192\nh\nMemory\n|\n{z\n}\nContext\n\u2192Planning\n|\n{z\n}\nDecision\n\u2192Tool Learning\n|\n{z\n}\nAction\n\u2192Observation\n|\n{z\n}\nFeedback\ni\nn \u2192Solution.\nIn each iteration n, the system must first retrieve relevant context from memory, reason over the current\nstate to formulate a plan, execute a specific tool-incorporated action, and process the resulting observation.\nThis cycle creates a compounding accumulation of tokens, where the output of step n becomes the input\ncost of step n + 1, resulting in high inference costs and slow response times. Consequently, mere model\ncompression is insufficient. We therefore define an efficient agent as follows:\nEfficient agent is not a smaller model, but as an agentic system optimized to maximize task suc-\ncess rates while minimizing resource consumption, including token usage, inference latency, and\ncomputational cost across memory, tool usage, and planning modules.\nOur survey aims to systematize the numerous efforts in this emerging field. While a large number of\nexisting surveys focus on Efficient LLMs [156, 201, 123], which serve as the backbone of agents, there is a\nlack of comprehensive literature addressing the efficiency of the agentic system itself. To bridge this gap, we\ncategorize existing works into three strategic directions: 1) Efficient Memory: Techniques for compressing\nhistorical context, managing memory storage, and optimizing context retrieval. 2) Efficient Tool Learning:\nStrategies to minimize the number of tool calls and reduce the latency of external interactions. 3) Efficient\nPlanning: Strategies to reduce the number of executing steps and API calls required to solve a problem.\nThe remainder of this survey is organized as follows: Section 2 introduces the preliminaries and highlights\nthe efficiency gap between agents and LLMs. Sections 3 through 5 explore component-level efficiency, with\na focus on memory, tool learning, and planning optimizations. Subsequently, Section 6 addresses the\nquantification of efficiency. The survey concludes with a discussion on open challenges and future research\ndirections.\n2. Preliminaries\n2.1. Agent Formulation\nWe model an LLM-based agent interacting with an environment as a partially observable Markov decision\nprocess (POMDP) augmented with an external tool interface and an explicit memory component. Formally,\nwe define the overall model as\nM = (S, O, A, P, R, \u03b3; T , \u03a8; Mmem, U, \u03c1).\nHere S denotes the latent environment state space, O the observation space, and A the agent action space.\nThe environment dynamics are given by the transition kernel P, the reward function R, and the discount\nfactor \u03b3 \u2208[0, 1).\nThe agent is additionally equipped with a set of external tools T and a tool interface \u03a8, which specifies\nhow tool calls are executed and what tool outputs are returned to the agent. Finally, we model explicit agent\nmemory with memory state space Mmem, an update rule U that maps the current memory and available\ninformation to the next memory state, and an initialization distribution \u03c1 over the initial memory.\n2.2. From Pure LLMs to Agents\n5\nPure LLMs\nT\no\no\nl\nL\ne\na\nr\nn\ni\nn\ng\nP\nl\na\nn\nn\ni\nn\ng\nM\ne\nm\no\nr\ny\nM\ne\nm\no\nr\ny\n+\nT\no\no\nl\nL\ne\na\nr\nn\ni\nn\ng\n+\nP\nl\na\nn\nn\ni\nn\ng\nA\ng\ne\nn\nt\ns\nFigure 2: From LLMs to agents: standalone reasoning\nto trajectory-level reasoning with memory, planning,\nand tool learning, while introducing additional cost\nsources.\nWe define efficiency through a cost\u2013performance\ntrade-off: achieving comparable performance with\nlower cost, or achieving higher performance under\na similar cost budget.\nWe acknowledge that many efficiency tech-\nniques used in LLM-based agents overlap with those\nfor standalone LLMs (e.g., model compression and\ninference acceleration). In agents, however, these\ntechniques mainly serve as foundational enablers\nrather than addressing the agent-specific sources of\ninefficiency. As summarized by Wang et al. [126],\ncompared to pure LLMs, LLM-based agents exhibit\nmore human-like decision-making by augmenting\na base model with cognitive components such as\nplanning and memory.\nAccordingly, in this subsection we focus on what\ndifferentiates agent efficiency from LLM efficiency.\nFrom a functional perspective, an agent is charac-\nterized by its ability to (i) plan and act over multiple\nsteps, (ii) invoke external tools or environment com-\nmands to acquire information and execute operations, and (iii) condition subsequent decisions on retrieved\nor updated memory.\nAs illustrated in Figure 2, agentic systems introduce additional cost sources beyond generation. For a\npure LLM, the inference cost is often dominated by token generation and can be approximated as:\nCostLLM \u2248\u03b1 Ntok,\nwhere Ntok is the number of generated reasoning tokens and \u03b1 captures the per-token cost (e.g., time or\nmonetary cost). In contrast, an agent may incur additional overhead from tools, memory, and retries as\nneeded:\nCostagent \u2248\u03b1 Ntok + Itool \u00b7 Costtool + Imem \u00b7 Costmem + Iretry \u00b7 Costretry,\nwhere Itool, Imem, Iretry \u2208{0, 1} are indicator variables that equal 1 if the agent invokes tools, accesses\nmemory, or performs retries, respectively, and 0 otherwise. Therefore, improving agent efficiency is not only\nabout reducing language generation, but also about reducing the frequency and improving the selectivity of\ntool or memory invocations and retries along a trajectory, to achieve a better cost\u2013performance trade-off.\n3. Efficient Memory\nA major efficiency bottleneck for LLM agents is the computational and token overhead induced by long\ncontexts and long-horizon interactions, where agents may repeatedly reprocess large histories to act. Memory-\naugmented reasoning provides a principled way to alleviate this inefficiency. By storing and reusing past\nexperience, including successes, failures, and interaction traces, agents can avoid redundant computation,\nmake more informed decisions, and reduce costly retries. In this sense, memory is not merely an auxiliary\ncomponent. It is a key mechanism for improving the overall efficiency-effectiveness trade-off of agent systems.\n6\nPredefined Rules\nRule-enhanced Retrieval\nLLM / Tool-based Retrieval\nHierarchical Retrieval\nTraining\nOutput\nFeedback/Loss\nGraph-based Retrieval\nMemory Access\nMemory Management\nAppend\nAppend\nCompress/Filter\nTextual Integration\nLatent Integration\nLatent Memory\nInject\nManager\nRule\nLLM\nHybrid\nMemory Construction\nWorking Memory\nTextual Memory\nLatent Memory\nExternal Memory\nItem-based Memory\nGraph-based Memory\nHierarchical Memory\nOperations\nTokens\nInteraction Context\n\u2026\nAdd\nDelete\nNo-op\nUpdate\n...\nDocument Accumulation\nLatency\nFigure 3: Efficient memory overview. This figure summarizes the agent-memory lifecycle in three phases:\nMemory Construction, which compresses long interaction context in working and external memory to\nmitigate token explosion; Memory Management, which curates and updates an accumulating memory store\nvia rule-based, LLM-based, or hybrid strategies to control latency; and Memory Access, which determines\nwhat memories to retrieve and how to integrate them into the model.\nWe organize this section around the lifecycle of agent memory, covering memory construction, memory\nmanagement, and memory access. Because memory is central to efficiency gains, how to design an efficient\nmemory module becomes an important problem. We therefore discuss efficiency-oriented designs throughout\nthis lifecycle, focusing on how to maximize the benefit of memory while minimizing additional overhead.\nFigure 3 provides a structured overview of our taxonomy, and Table 1 lists representative works for an\nat-a-glance summary.\n3.1. Memory Construction\nNo matter whether we target long-context tasks or long-term interactions, the core challenge is handling\nextensive context or interaction history. Naively appending raw history into the prompt is often impractical:\ntoken usage grows rapidly, and performance can even degrade when relevant information is buried in long\nsequences, as observed in the \u201clost in the middle\u201d phenomenon [71]. In addition, an LLM\u2019s context window\nis finite, whereas the amount of potentially relevant information is effectively unbounded. These constraints\nmotivate memory construction, which compresses and organizes past information into more manageable\nrepresentations. Many existing works build memory through summarization, reducing token consumption\nand improving efficiency.\n7\nTable 1: Memory overview of efficiency-oriented mechanisms. The table is organized according to the\ntaxonomy proposed in this work, covering working memory, external memory, and multi-agent memory.\nMethod\nCategory\nCore Mechanism\nResource Link\nWorking Memory\nCOMEDY [11]\nTextual\nTwo-stage memory distillation\n\u00a7 GitHub\nMemAgent [176]\nTextual\nOverwrite fixed memory\n\u00a7 GitHub\nMEM1 [200]\nTextual\nUpdate a compact shared internal state\n\u00a7 GitHub\nAgentFold [175]\nTextual\nProactive context folding\nN/A\nDC [116]\nTextual\nPersistent, evolving memory\n\u00a7 GitHub\nActivation Beacon [184]\nLatent\nActivation-level beacon for long context\n\u00a7 GitHub\nMemoRAG [94]\nLatent\nKV-compressed global memory representation\n\u00a7 GitHub\nMemoryLLM [133]\nLatent\na fixed-size latent memory pool\n\u00a7 GitHub\nM+ [134]\nLatent\nDual-level latent memory; co-trained retriever\n\u00a7 GitHub\nMemory3 [165]\nLatent\nExternalize knowledge into retrievable sparse KV memories\nN/A\nTitans [7]\nLatent\nSliding-window attention; test-time trainable neural long-term memory\nN/A\nMemGen [181]\nLatent\nOn-demand latent memory synthesis\n\u00a7 GitHub\nExternal Memory\nMemoryBank [194]\nItem-based\nEbbinghaus forgetting curve\u2013based memory management\n\u00a7 GitHub\nRECOMP [153]\nItem-based\nCompress retrieved documents\n\u00a7 GitHub\nExpel [192]\nItem-based\nexperiential learning; insight distillation and management\n\u00a7 GitHub\nHuman-like memory [39]\nItem-based\nCue-triggered memory recall\nN/A\nSeCom [86]\nItem-based\nSegment-level memory; compression-based denoising for retrieval\n\u00a7 GitHub\nMemory-R1 [162]\nItem-based\nAdaptive memory CRUD and memory distillation, via two RL-trained agents\nN/A\nMem0 [15]\nItem-based\nExtract candidate memories; memory CRUD\n\u00a7 GitHub\nagentic plan caching [186]\nItem-based\nStore plan template; plan cache lookup (hit/miss) and update\nN/A\nLD-Agent [57]\nItem-based\nSeparate different memory; topic-based retrieval\n\u00a7 GitHub\nMemoChat [76]\nItem-based\nStructured on-the-fly memos\n\u00a7 GitHub\nRMM [118]\nItem-based\nTopic-based memory organization; consolidation (add/merge); online RL reranker\nN/A\nMemento [197]\nItem-based\nParametric case retrieval via an online-updated Q-function\n\u00a7 GitHub\nMemInsight [103]\nItem-based\nAttribute-augmented memory; attribute-guided retrieval\n\u00a7 GitHub\nReasoningBank [83]\nItem-based\nDistill strategies from failures and successes to cut exploration steps\nN/A\nA-MEM [157]\nItem-based\nAtomic structured notes; link generation and memory evolution\n\u00a7 GitHub\nACE [185]\nItem-based\nIncremental delta updates, lightweight merge and de-dup\n\u00a7 GitHub\nAgent KB [119]\nItem-based\nCross-framework reusable experience Knowledge Base\n\u00a7 GitHub\nGraphReader [60]\nGraph-based\nGraph-guided coarse-to-fine exploration\nN/A\nKG-Agent [46]\nGraph-based\nTool-based hop-local KG processing\nN/A\nZep [99]\nGraph-based\nTemporal KG memory\n\u00a7 GitHub\nMem0g [15]\nGraph-based\nExtract candidate nodes; graph updation\n\u00a7 GitHub\nAriGraph [5]\nGraph-based\nMemory graph; semantic-to-episodic cascading retrieval\n\u00a7 GitHub\nD-SMART [55]\nGraph-based\nStructured OWL-compliant KG\nN/A\nMemGPT [84]\nHierarchical\nOS-style virtual memory paging for context\n\u0080 Website\nMemoryOS [49]\nHierarchical\nOS-inspired three-tier memory hierarchy with policy-based inter-tier updates\n\u00a7 GitHub\nMemOS [63]\nHierarchical\nPolicy-guided type transformation of MemCubes across three memory forms\n\u00a7 GitHub\nReadAgent [54]\nHierarchical\nGist memory compression; on-demand lookup\n\u00a7 GitHub\nHiAgent [40]\nHierarchical\nSubgoals as memory chunks; on-demand trajectory retrieval\n\u00a7 GitHub\nH-MEM [115]\nHierarchical\nLayer-by-layer retrieval\nN/A\nLightMem [21]\nHierarchical\nPre-compression; soft update (test-time); sleep-time update (offline)\n\u00a7 GitHub\nMulti-Agent Memory\nMS [24]\nShared\nShared memory pool; selective addition; continual retriever training\n\u00a7 GitHub\nG-Memory [180]\nShared\nthree-tier graph memory with bi-directional coarse-to-fine retrieval\n\u00a7 GitHub\nRCR-Router [69]\nShared\nFeedback-refined iterative router under a token budget\nN/A\nMemIndex [104]\nShared\nIntent-indexed bipartite graphs; semantic slicing and dynamic indexing\nN/A\nMIRIX [132]\nShared\nSix-module hierarchical memory with staged retrieval and parallel updates\n\u00a7 GitHub\nIntrinsic Memory Agents [177]\nLocal\nRole-aligned templates; intrinsic iterative updates\nN/A\nAgentNet [167]\nLocal\nFixed-size memory modules for routing/execution; dynamic pruning\n\u00a7 GitHub\nDAMCS [164]\nLocal\nDecentralized per-agent STWM/LTM with goal-oriented hierarchical knowledge graph\n\u0080 Website\nSRMT [102]\nMixed\nPersonal latent memory and globally broadcast shared recurrent memory\n\u00a7 GitHub\nCollaborative Memory [100]\nMixed\nPolicy-based filtering/transformation of memory fragments; shared-memory reuse\nN/A\nLEGOMem [31]\nMixed\nRole-aware memory routing; runtime-efficient retrieval scheduling\nN/A\n3.1.1. Working Memory\nWorking memory is the information directly available at inference time that conditions generation. Here,\nthe term is broader than the common definition that limits working memory to context tokens. It includes\n8\nthe text currently present in the prompt or context window, and latent memory in the form of continuous\nsignals that influence the forward computation without being represented as tokens, such as soft prompts, KV\ncache, and hidden states. Latent memory can arise inside the model or be stored externally and injected as\ncontinuous conditioning. Embeddings count as latent memory only when they are provided to the model as\nsuch conditioning signals; embeddings used only to support retrieval are treated separately in Section 3.1.2.\nTextual Memory.\nIn LLM-based agents, textual memory is a common instantiation of working memory.\nTo address the long-context challenge, many methods aim to keep the working memory in the prompt at\na roughly constant size. In practice, this is often achieved by frequently rewriting or compressing the\nmemory as the process evolves.\nCOMEDY [11] uses an LLM to generate and compress memory: it extracts session-specific memories\nfrom past conversations and then condenses them into a compact representation of key events, the user\nprofile, and relationship changes. MemAgent [176] and MEM1 [200] both process long inputs sequentially\nby rewriting and updating a compact memory state at each step: MemAgent updates a summarized memory\nafter each chunk, while MEM1 uses reinforcement learning [182] to maintain a fixed-length internal state\ntagged by <IS></IS> that replaces itself in the next prompt. AgentFold [175] proactively folds interaction\nhistory into multi-scale summaries plus the latest full turn, slowing critical information loss while reducing\ntoken usage.\nBy retaining a compact memory in the prompt rather than the full history, these methods reduce the\neffective context length the LLM needs to attend to, thereby improving long-context performance while\ndecreasing computational cost and increasing efficiency.\nLatent Memory.\nBesides textual working memory, recent work also lets an agent keep its state in latent\nform, such as hidden activations or KV caches. This kind of memory is not shown as text, but it can be read\nand updated by the model. In many cases it is much cheaper than storing and re-reading the full interaction\nhistory, and thus is attractive for efficient agents.\nOne group of methods builds compact latent memory by compressing long contexts into a small set of\nactivations in KV space. Activation Beacon [184] partitions the context into chunks and fine-grained units,\ninterleaves beacon tokens by a compression ratio, and uses progressive compression to distill layer-wise\nKV activations into the beacons, which are accumulated as latent memory while raw-token activations are\ndiscarded. MemoRAG [94] performs memory formation by inserting memory tokens after each window as\ninformation carriers of global memory in KV space, and updating a memory-token KV cache across windows\nwith separate weight matrices; the compact global memory can later be reused (e.g., as retrieval clues).\nA second group maintains an external pool of latent memory and integrates it into the backbone LLM via\nattention at inference time, enabling reuse of stored information across steps and episodes. MemoryLLM [133]\nmaintains a fixed-size memory pool of memory tokens updated via self-update, enabling reuse of stored\nlatent knowledge without lengthening the prompt. M+ [134] adds a GPU/CPU two-tier long-term memory\nwith a co-trained retriever that fetches only a few relevant memory tokens per layer, and Memory3 [165]\nencodes a KB as sparse explicit key\u2013value memory injected into attention at decoding time to avoid repeated\ndocument reading.\nA third group lets latent memory be a separate neural module that can learn online together with the\nagent. Titans [7] builds latent memory by updating a neural memory module at test time, writing only when\nprediction error is high and skipping updates otherwise. MemGen [181] constructs latent memories via an\n9\nRL-trained memory trigger and a memory weaver that produces compact latent memory tokens as the stored\nrepresentation.\nStrictly speaking, some of the methods above are proposed as general memory modules for LLMs rather\nthan full agent frameworks. However, from the view of efficient agent memory, they play the same role: they\ncompress long interaction histories into compact latent states, update these states only when needed, and\nexpose them to the policy through attention or simple interfaces. This allows an agent to keep and reuse\nlong-horizon information without replaying the entire textual trajectory at each step.\nWorking Memory\n\u2022 Advantages: The working memory is directly conditioned upon during generation, eliminating the\nlatency and overhead associated with external retrieval or repeated encoding.\n\u2022 Disadvantages: Expanding the working set leads to computational growth for textual memory or\nincreased memory footprint for latent states, and risks performance degradation due to information\ndilution in long contexts.\n\u2022 Applicable Scenarios: Textual memory is best for logic-heavy tasks within moderate context limits,\nwhile latent memory suits efficiency-critical applications requiring the reuse of historical states\nwithout re-processing.\n3.1.2. External Memory\nExternal memory refers to information stored outside the model in token-level form, including document\ncollections, knowledge graphs, and retrieval systems such as RAG. It does not condition generation directly.\nInstead, it is accessed through retrieval and then expressed as tokens placed into the prompt or context\nwindow.\nItem-based Memory.\nEarly agent-memory systems often store full trajectories or experiences, sometimes\nalongside summaries, which leads to long context and inefficiency. MemoryBank [194] stores daily conversa-\ntion records and summarizes past events and user profiles from these conversations, but it incurs high token\ncosts. Similarly, Expel [192] suffers from a similar limitation, as it accumulates experiences through trial\nand error and distills them into natural-language insights.\nTo be more efficient, some works adopt methods such as memory extraction, compress or summariza-\ntion to directly reduce context length. This way thereby lowers input token consumption while yielding a\nshorter but more informative context. Human-like memory [39] extracts episodic memories from users dia-\nlogues, encapsulating content and temporal context into database structure. SeCom [86] uses segmentation\nmodel to divide long-term conversations into topic-coherent segmentation, and applies the compress model to\ndenoise the segmentation, which further promotes efficient retrieval. Memory-R1 [162] and Mem0 [15] both\nextract and summarize ongoing dialogue into candidate memories for downstream updating; Memory-R1\ndoes so at each turn, while Mem0 forms candidate memories from the new message pair, using a conversation\nsummary and recent messages as context. Agentic plan caching [186] turns a successful run into a reusable\ncache entry by rule-based filtering the execution log and then using a lightweight LLM to remove context-\nspecific details, storing the result as a (keyword, plan template) pair. LD-Agent [57] separates event and\npersona memory, using short-term and long-term banks for timestamped dialogue context and embedded\nevent summaries, and a persona extractor to store user and agent traits in long-term persona banks.\nBeyond the extraction and compression strategies discussed above, another way to improve efficiency is\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper \"Toward Efficient Agents: A Survey of Memory, Tool Learning, and Planning\"\n\n#### Main Contributions:\nThe paper aims to address the critical issue of efficiency in agents by investigating memory construction, tool learning, and planning from a comprehensive perspective. The authors propose an efficient agent as one that maximizes task success rates while minimizing resource consumption across memory, tool usage, and planning modules.\n\n#### Methodology:\n1. **Efficient Memory:**\n   - **Memory Construction:** Techniques for compressing historical context into manageable representations.\n     - **Working Memory:** Directly available information at inference time, including text in the prompt or latent signals influencing computation without being represented as tokens (e.g., soft prompts, KV cache).\n     - **External Memory:** Information stored outside the model in token-level form such as document collections and retrieval systems. \n   - **Memory Management:** Strategies for curating and updating an accumulating memory store via rule-based, LLM-based, or hybrid approaches to control latency.\n   - **Memory Access:** Determining what memories to retrieve and how to integrate them into the model.\n\n2. **Efficient Tool Learning:**\n   - **Tool Selection:** Choosing appropriate tools based on task requirements.\n   - **Tool Calling:** Executing tool calls with minimal overhead, often through reinforcement learning (RL) rewards designed to minimize invocation frequency.\n   - **Tool-Integrated Reasoning:** Combining reasoning and tool usage for efficient problem-solving.\n\n3. **Efficient Planning:**\n   - **Single-Agent Planning:** Optimizing planning processes over multiple steps without excessive retries or context window saturation.\n   - **Multi-Agent Collaboration:** Enhancing efficiency in collaborative settings by leveraging shared knowledge and resources.\n\n#### Key Findings:\n- **Memory Efficiency:** Techniques like summarization, hierarchical retrieval, and compressed latent memory have shown promise in reducing token consumption and computational overhead while maintaining performance. For example, methods such as COMEDY [11], MemAgent [176], and MEM1 [200] demonstrate how to maintain a constant-sized working memory.\n- **Tool Learning:** Strategies like tool selection based on task requirements (e.g., ToolOrchestra [189]) and RL-based rewards for minimizing tool invocations have been successful in reducing the number of external interactions required. Methods such as SWiRL [203] and SwiftSage [204] illustrate how to integrate reasoning with tools effectively.\n- **Planning Efficiency:** Techniques like hierarchical planning (e.g., MemoryOS [63]) and iterative refinement strategies (e.g., DAMCS [164]) have been effective in reducing the number of steps required for problem-solving, thereby improving efficiency.\n\n#### Conclusions:\nThe paper concludes by emphasizing the importance of addressing both effectiveness and cost in agent design. It highlights that efficient agents are not just smaller models but systems optimized to maximize task success rates while minimizing resource consumption across memory, tool usage, and planning modules. The authors provide a structured overview of existing works categorized into three strategic directions: Efficient Memory, Efficient Tool Learning, and Efficient Planning.\n\n#### Future Directions:\nThe paper identifies key challenges such as designing efficient tools that can be integrated seamlessly with agents, developing robust methods for managing external interactions, and ensuring long-term sustainability in the face of increasing complexity. The authors also discuss potential benchmarks and evaluation protocols to measure efficiency metrics across these components.\n\n### Summary Breakdown:\n\n1. **Introduction:**\n   - Discusses the shift from LLMs to agentic systems and highlights the critical bottleneck of efficiency.\n   - Defines an efficient agent as one that maximizes task success rates while minimizing resource consumption (token usage, inference latency, computational cost).\n\n2. **Preliminaries:**\n   - Introduces the model formulation for LLM-based agents augmented with memory, tool interfaces, and explicit memory components.\n   - Highlights the difference between LLM efficiency and agent efficiency.\n\n3. **Efficient Memory:**\n   - Covers memory construction (e.g., summarization, hierarchical retrieval) and management strategies (rule-based, hybrid).\n   - Discusses memory access mechanisms for efficient retrieval and integration into models.\n\n4. **Efficient Tool Learning:**\n   - Explores methods to minimize tool invocations and reduce latency through RL rewards.\n   - Examines how tools can be integrated with reasoning processes for efficiency gains.\n\n5. **Efficient Planning:**\n   - Analyzes techniques for reducing the number of planning steps required, including hierarchical and iterative strategies.\n   - Discusses multi-agent collaboration in enhancing planning efficiency.\n\n6. **Benchmarks:**\n   - Summarizes evaluation protocols for memory, tool learning, and planning components.\n   - Consolidates commonly reported efficiency metrics from both benchmark and methodological studies.\n\n7. **Challenges and Future Directions:**\n   - Identifies key challenges such as designing efficient tools and managing external interactions.\n   - Discusses potential benchmarks and future research directions to address these challenges.\n\n### Conclusion:\nThe paper provides a comprehensive survey of the state-of-the-art in efficient agent design, emphasizing the importance of addressing both effectiveness and cost. It offers insights into how memory construction, tool learning, and planning can be optimized for more efficient agentic systems."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware\nCode Generation\nQirui Chen1 , Jingxian Shuai1 , Shuangwu Chen1\u2217, Shenghao Ye1 , Zijian Wen1 , Xufei Su2 ,\nJie Jin3 , Jiangming Li4 , Jun Chen4 , Xiaobin Tan1 and Jian Yang1\n1University of Science and Technology of China, China\n2Xi\u2019an Jiaotong University, China\n3Nanjing University, China\n4ZTE Corporation, China\n{chenqirui, vexertron shuai}@mail.ustc.edu.cn, chensw@ustc.edu.cn\nAbstract\nLarge language models (LLMs) are being in-\ncreasingly integrated into practical hardware and\nfirmware development pipelines for code gener-\nation.\nExisting studies have primarily focused\non evaluating the functional correctness of LLM-\ngenerated code, yet paid limited attention to its\nsecurity issues.\nHowever, LLM-generated code\nthat appears functionally sound may embed secu-\nrity flaws which could induce catastrophic damages\nafter deployment. This critical research gap moti-\nvates us to design a benchmark for assessing secu-\nrity awareness under realistic specifications. In this\nwork, we introduce HardSecBench, a benchmark\nwith 924 tasks spanning Verilog Register Trans-\nfer Level (RTL) and firmware-level C, covering 76\nhardware-relevant Common Weakness Enumera-\ntion (CWE) entries. Each task includes a structured\nspecification, a secure reference implementation,\nand executable tests. To automate artifact synthe-\nsis, we propose a multi-agent pipeline that decou-\nples synthesis from verification and grounds evalu-\nation in execution evidence, enabling reliable eval-\nuation. Using HardSecBench, we evaluate a range\nof LLMs on hardware and firmware code genera-\ntion and find that models often satisfy functional\nrequirements while still leaving security risks. We\nalso find that security results vary with prompting.\nThese findings highlight pressing challenges and\noffer actionable insights for future advancements in\nLLM-assisted hardware design. Our data and code\nwill be released soon.\n1\nIntroduction\nLarge Language Models (LLMs) have been increasingly in-\ncorporated into practical hardware and firmware development\nfor automated code generation, including Register Transfer\nLevel (RTL) design and C-based firmware development [Liu\net al., 2024; Yu et al., 2025b]. Extensive studies have fo-\ncused on improving and evaluating the functional correctness\nof LLM-generated code [Liu et al., 2023; Lu et al., 2024].\n\u2217Corresponding author.\n(a) Design Request\nDesign a configuration register\nmodule with:\n\u2022\nCPU write interface\n\u2022\nDMA write interface\n\u2022\nLock mechanism after first\nCPU write\n(b) LLM Generated Code\nalways @(posedge clk) begin\nif (cpu_en && !lock_bit) begin\nconfig_out <= cpu_data;\nlock_bit <= 1'b1;\nend\nelse if (dma_en && !cpu_en) begin\nconfig_out <= dma_data;\nend\nend\nMissing: && !lock_bit\n(c) Potential Security Vulnerability\nCPU\nWrite\nConfig\nRegister\nAttacker\n(DMA)\nlock=1\nlocked\nBypass via DMA\nProblem: Improper Restriction of Write-Once Bit Fields (CWE-1224)\nDMA write does not check lock bit\nFunctional Test\nLLM\n\u2705\nFigure 1: Potential security vulnerability in LLM-generated code.\nThe LLM produces functionally correct code (b) from a design re-\nquest (a), but the DMA interface fails to check the lock status, en-\nabling attackers to bypass write-once protection (c).\nHowever, as security intent is often not explicitly specified\nin LLM-assisted hardware development, the generated codes\nmay pass functional validation yet still harbor potential secu-\nrity vulnerabilities that may trigger induce catastrophic dam-\nages [Perry et al., 2023; Fan et al., 2025]. For example, in\nFigure 1, an LLM-generated configuration register success-\nfully passes read/write tests, but violates the write-once lock\npolicy when an alternative access interface fails to enforce the\nlock. As designs scale up and interfaces proliferate, such se-\ncurity violations may accumulate over the entire development\nlifecycle, thereby exacerbating systemic risks.\nHowever, there is still a lack of standardized and com-\nprehensive benchmark to evaluate the systematic security of\nLLM-based hardware designs, as existing studies [Fan et al.,\n2025; Hu et al., 2025; Long et al., 2025] usually validate the\nsecurity of LLM-generated code on their own specific test\nsets. Two critical challenges hinder the development of a uni-\nfied benchmark for systematic security assessment.\nTrustworthy benchmark construction at scale.\nHigh-\nquality security benchmark samples are expensive to build\nand validate, making it difficult to expand evaluation to large\nbenchmarks through expert effort alone [Qiu et al., 2024].\nTo mitigate this inefficiency, LLM-assisted synthesis offers a\npractical way to generate benchmark tasks in large numbers.\nHowever, ensuring the reliability of the resulting benchmark\nand evaluation remains challenging, requiring rigorous qual-\narXiv:2601.13864v1  [cs.CR]  20 Jan 2026\nity control to keep results trustworthy [Zhu et al., 2025].\nUnreliable security evaluation. Existing RTL security anal-\nysis workflows often rely on experts to craft SystemVerilog\nAssertions (SVAs) from security intent and debug violations\nthrough simulation [Orenes-Vera et al., 2021; Afsharmazaye-\njani et al., 2024]. However, SVAs can miss vulnerabilities\nwhen security-relevant behaviors are not triggered. To reduce\nreliance on simulation scenarios, some workflows use early-\nstage static rule checking, which still demands security ex-\npertise and often emphasizes functional issues over security\nproperties [Synopsys, 2015]. Recent work also adopts LLM-\nas-a-judge verifiers, but without timing-accurate simulation\nevidence their judgments can be unreliable and inconsistent\nacross runs [Bhatt et al., 2024].\nTo address these challenges, we design an automated\npipeline that scales benchmark construction and enables se-\ncurity evaluation under specifications that do not reveal se-\ncurity intent. Using this pipeline, we build HardSecBench:\neach task includes a structured specification that separates\nfunctional and security requirements, and requirement-level\nharnesses that validate requirements via simulation or execu-\ntion, with tests that trigger security-relevant behaviors. This\npipeline reconciles mismatches until the specification, imple-\nmentation, and harnesses agree on all checks, and we then\napply coverage and mutation filtering to retain harnesses that\nbetter distinguish correct from incorrect implementations. On\nthe evaluation side, HardSecBench avoids subjective judging\nand scores security using simulation evidence from targeted\nharnesses that actively trigger security relevant behaviors.\nThe benchmark covers 76 Common Weakness Enumeration\nentries (CWE) [MITRE, 2006] and evaluates whether mod-\nels implement protections checked by the security require-\nments. Finally, HardSecBench reports results under single-\nattempt generation and iterative refinement, where a collabo-\nrator helps repair functional defects so security tests run on a\ncorrect functional baseline and reduce confounding.\nBased on HardSecBench, a comprehensive evaluation of\nstate-of-the-art (SOTA) models reveals that a significant num-\nber of models can generate functionally valid hardware de-\nsigns yet lack critical security protections. To explore the\nsecurity potential of LLMs, we conduct a prompt sensitivity\nanalysis and find that these SOTA models could yield notable\nsecurity gains even with merely generic security reminders.\nThis finding indicates that the security knowledge is inher-\nently embedded in LLMs but has not been fully exploited.\nAccordingly, providing explicit security guidance can effec-\ntively compensate for the flaws in LLM-generated code. We\nfurther conduct an empirical study on specialized models to\nevaluate the impact of domain-specific training on their secu-\nrity performance. Finally, we carry out a systematic analysis\nof vulnerability to highlight common weaknesses across di-\nverse models.\nThe primary contributions of this work are as follows:\n\u2022 We formulate a systematic evaluation setting for security\nawareness in LLM-generated hardware designs, offering\na rigorous testbed for secure hardware code generation;\n\u2022 We develop a multi-agent construction pipeline that syn-\nthesizes benchmark samples, producing testable specifi-\ncations and corresponding simulatable harnesses;\n\u2022 We propose HardSecBench, a benchmark of 924 Verilog\nand firmware-C tasks spanning 76 CWE entries;\n\u2022 Using HardSecBench, we comprehensively evaluate a\nwide range of SOTA models and report our findings on\nthe security risks of LLM-generated hardware designs.\n2\nRelated Work\n2.1\nLLM-Assisted Hardware Design\nLLM-assisted hardware design has been studied across the\nRTL development lifecycle, including Verilog generation\nfrom specifications and interactive co-design that supports\niterative refinement and debugging [Thakur et al., 2023;\nBlocklove et al., 2023; Zhu et al., 2025]. Simulation-driven\nbenchmarks have been adopted to standardize evaluation by\ncompiling and simulating generated RTL against reference\ntestbenches, as exemplified by VerilogEval and RTLLM [Liu\net al., 2023; Pinckney et al., 2025; Lu et al., 2024]. Re-\ncent efforts release larger datasets and lightweight fine-tuned\nLLMs to reduce the training and inference costs for adopt-\ning RTL generation, enabling more accessible deployment in\nlocal environments [Liu et al., 2025; Zhu et al., 2025]. Be-\nyond module synthesis, domain-adapted assistants and agen-\ntic EDA systems extend LLM support to engineering question\nanswering, script generation and partial flow automation [Liu\net al., 2024; Wu et al., 2024]. LLMs have also been explored\nfor generating verification items, including test inputs and as-\nsertions, along with benchmarks to assess these outputs [Fang\net al., 2024; Pulavarthi et al., 2025].\n2.2\nSecurity of LLM-Generated Code\nSecurity evaluation of LLM-generated artifacts has been\nstudied extensively in software, showing that code can sat-\nisfy functional specifications while still containing potential\nvulnerabilities that are not exercised by standard require-\nments [Peng et al., 2025; Li et al., 2025; Kim et al., 2025;\nYu et al., 2025a].\nIn addition to evaluation, software en-\ngineering research proposes methods that steer generation\ntoward secure implementations. Constrained decoding en-\nforces security constraints during generation to reduce inse-\ncure patterns [Fu et al., 2024]. Retrieval augmented genera-\ntion adds selected security knowledge and secure coding pat-\nterns to guide implementation choices [Shi and Zhang, 2025].\nIn hardware settings, recent work examines risks including\nintellectual property leakage and memorization [Mashnoor\net al., 2025], as well as hardware hallucinations in gener-\nated designs [Ping et al., 2025]. Knowledge-augmented ap-\nproaches incorporate structured domain knowledge to guide\nsecurity-aware synthesis or enable automated structural au-\nditing through graph-based traversal rules [Fan et al., 2025;\nHu et al., 2025; Long et al., 2025].\n3\nHardSecBench\nIn this section, we introduce HardSecBench, a benchmark\nto evaluate hardware security awareness via a multi-agent\nconstruction pipeline. As shown in Figure 2, the workflow\nhas four stages coupled by a single structured specification\nCWE Definition\nSeed Generator\nArchitect Agent\nExpert Agent\nImplementation Synthesis\nInitial\nSpecification\nReject\nSample\nCWE-Guided Specification Synthesis Decoupled Artifact Synthesis\nIterative Refinement\nFinal Verification & Quality Gate\n\u2022 Improper Restriction of Write-Once Bit Fields\n\u2022 Assumed-Immutable Data is Stored in\nWritable Memory\n\u2022 \u2026\n\u2022 Design a Flash Memory Controller\u2026\n\u2022 Design a configuration register controller..\n\u2022 \u2026\n\u2022\nProblem\n\u2022\nFunction Requirements\n\u2022\nSecurity Requirements\n\u2022\nI/O Port\nmodule topmodule\n\u2026\nif (!rst_n) locked <= 1'b0;\nelse if (we && !locked) begin\nsecure_reg <= wdata;\nlocked <= 1'b1;\nend\nendmodule\nTester Agent\nif (\u2026) begin\n$display(\u201c[TEST] FAIL:\u2026\");\nend else begin\n$display(\"[TEST] PASS:\u2026\");\nend\n\u2026\nDerive Atomic Test Harnesses\nVerify by Simulation\nGolden\nImplementation\nTest\nHarnesses\nSimulation Result\nArbiter Agent\nDynamic Error Attribution\nFeedback & Refinement\nValidity Test\nu Coverage Test\nu Mutation Test\nVerified\nSample\nPASS\nFAIL\nIteratively\nGenerated\nSamples\n\u2026\nSpec\nIssue\nImplementation\nIssue\nTest\nIssue\nAll Passed or Reached\nMaximum Iterations\n[TEST] FAIL:\u2026\nBenchmark Construction\nFigure 2: HardSecBench construction pipeline. From CWE-derived seeds, the Architect builds a structured specification Pi that separates\nfunctional requirements Rf\ni from security requirements Rs\ni, so implementations can be elicited from Rf\ni while security is checked against\nRs\ni. Given Pi, the Expert and Tester independently synthesize the golden implementation and requirement-level harnesses, which are run and\niteratively reconciled until all checks agree; a final quality gate filters retained benchmark samples.\nPi.\nStarting from CWE-derived seeds, the Architect pro-\nduces Pi and separates functional requirements Rf\ni from se-\ncurity requirements Rs\ni, so that the functional specification\ncan be used to elicit implementations without revealing se-\ncurity intent while Rs\ni defines what security protections will\nbe checked. Given the same Pi, the Expert and Tester syn-\nthesize the golden implementation and requirement-level har-\nnesses in separate branches to avoid logic coupling between\nimplementation and verification. We then execute the har-\nnesses against the implementation and iteratively reconcile\nmismatches until all requirement checks agree. We finally\napply a quality gate, retaining only samples whose harnesses\nprovide strong diagnostic signals.\n3.1\nCWE-Guided Specification Synthesis\nWe use a hierarchical synthesis strategy to promote diver-\nsity and maintain specification quality.\nThe process starts\nwith seed synthesis, our Seed Generator converts each CWE\ndefinition into a seed that specifies the implementation lan-\nguage and a 1\u20132 sentence scenario that the target weakness\ncan arise. The Architect agent then expands each seed into\na structured specification Pi for task i, including a problem\nstatement, an I/O interface specification, and two requirement\nsets: functional requirements Rf\ni and security requirements\nRs\ni. By design, Rf\ni states only functional behavior and avoids\nrevealing security intent, while Rs\ni lists the protections re-\nquired by the CWE guidance. This separation serves two pur-\nposes: during construction, we keep Rf\ni free of security con-\ntent, and during evaluation, we give the target model without\nRs\ni, while we use Rs\ni to evaluate its security awareness.\n3.2\nDecoupled Artifact Synthesis\nTo ensure objectivity and prevent logic coupling, we enforce\nstrict information isolation between golden-implementation\nsynthesis and test-harness generation. Without isolation, har-\nnesses may inadvertently encode implementation details, in-\nflating pass rates and weakening diagnostic validity. The Ex-\npert and Tester agent follow independent generation paths and\nuse Pi as their only shared reference. The Expert synthesizes\nthe golden implementation to satisfy both Rf\ni and Rs\ni, while\nthe Tester derives atomic harnesses with a one-to-one map-\nping to requirements in Rf\ni and Rs\ni. Each harness is self-\ncontained and takes the form of a testbench for RTL tasks or\na standalone C driver for firmware tasks. The test harness\nemits a standardized PASS/FAIL trace so that verification\ncan parse results deterministically. Both branches iteratively\nrefine their initial outputs until they reach a compilable state.\nThis setup ensures the code and harnesses can compile and\nrun before refinement, so later failures reflect requirement\nmismatches rather than basic build or runtime errors.\n3.3\nArbiter-Driven Iterative Refinement\nWe aim to improve sample yield by repairing correctable mis-\nmatches instead of discarding a task as soon as a requirement\ncheck fails. A failing check can be caused by an ambiguous\nrequirement, an implementation error, or a harness bug, since\nthe three artifacts are synthesized in separate branches. The\nExpert, the Tester, and the Architect each operate under re-\nstricted views of the artifacts, so none of them can reliably\ndetermine whether the failure is due to the specification, the\nimplementation, or the harness. This motivates an Arbiter-\ncoordinated loop. The Arbiter observes the structured speci-\nfication Pi, the golden implementation, the requirement-level\nharnesses, and runtime evidence from execution and analysis\ntraces produced by the harnesses, and uses them to localize\nthe source of the mismatch. It then issues targeted feedback\nthat specifies what to revise and which observed evidence\nsupports the revision. We repeat diagnosis, revision, and re-\nTarget LLM\nGenerate\nImplementation\nDifferent\nSecurity\nHint Level\n\u2022 Problem\n\u2022 Function Requirements\n\u2022 I/O Port\nDesign Specification\nCollaborator\nAnalysis\nFunctional Failures\nFeedback\nTest Error\nTest Harnesses\nTarget Implementation\nPass Rate\nFunctional Passed or Reached Maximum Iterations\nFigure 3: HardSecBench evaluation architecture. The target model\ngenerates an implementation, the Collaborator provides feedback for\nfixing functional issues.\nexecution until all requirement checks pass, establishing con-\nsistency among the specification, the golden implementation,\nand the harnesses.\n3.4\nFinal Verification and Quality Gate\nPrior to benchmark inclusion, we apply a quality gate to re-\nduce false negatives caused by weak harnesses. Without this\ngate, a harness may run but still miss security relevant behav-\niors, allowing insecure implementations to pass. We there-\nfore screen samples using two complementary signals. First,\na coverage signal helps rule out cases where the harness ex-\nercises only a small portion of the design. Second, a mu-\ntation signal checks whether the harness can distinguish the\nintended implementation from perturbed variants that intro-\nduce deviations. In addition to these automated checks, do-\nmain experts manually audited 100 randomly sampled tasks\nand found that the artifacts largely satisfy HardSecBench\u2019s\nquality criteria. Together, these gates improve the reliability\nand diagnostic power of the benchmark for evaluating hard-\nware security awareness.\n4\nEvaluation Methodology\n4.1\nEvaluation Settings\nOur evaluation focuses on how the target model generates an\nimplementation under different security hint settings, with an\noptional Collaborator that provides feedback on functional\nissues as shown in Figure 3. We evaluate all models using\nthe requirement-level harnesses associated with each Hard-\nSecBench sample. Each harness emits a PASS/FAIL marker\nper requirement, which we parse to compute requirement-\nlevel pass rates. We then define two evaluation settings: one\nthat evaluates the model after a single generation attempt, and\none that allows iterative revisions with functional feedback\nbefore the final security evaluation to separate security aware-\nness from functional defects.\nSingle-Attempt Evaluation.\nThis setting measures the\nmodel\u2019s security intuition by checking whether it adds pro-\ntections beyond functional correctness when it receives only\nfunctional requirements and no execution feedback. We re-\nquire a compilable implementation so that syntax errors do\nnot dominate the security measurement. If the initial output\nfails to compile, we allow up to three rounds of compilation\nand fixing using only compiler error messages, and we eval-\nuate the first version that successfully compiles. These fixes\naim only to make the code runnable and do not reveal out-\ncomes from functional or security harnesses. After a compi-\nlable implementation is obtained, we run the full functional\nand security harnesses once and record the results without\nproviding them back to the model.\nIterative Refinement Evaluation.\nThis setting simulates a\ncollaborative workflow that iteratively improves functional\ncorrectness before assessing security. In each iteration, the\nmodel produces a compilable implementation, we run the\nfunctional harnesses, and the Collaborator provides feedback\non functional failures and concrete code issues. The Collabo-\nrator provides functional-only feedback and does not com-\nment on security. We repeat iterations until all functional\nrequirements pass or the iteration limit is reached, and then\nevaluate security performance on the final implementation.\nWe evaluate security only after the implementation reaches a\nfully functional or near-complete state, which yields a cleaner\nmeasure of security awareness without confounding from\nfunctional defects.\n4.2\nThe Pass@k Metric\nTo quantify generation stability across independent samples,\nwe report Pass@k for both functional and security require-\nments using a unified definition. For each task Pi, we start\nfrom the requirement sets Rf\ni and Rs\ni and further decompose\nthem into atomic requirements, each of which is checked by\na requirement-level harness and serves as one evaluation unit.\nLet d \u2208{Func, Sec} denote the requirement type, where\nFunc corresponds to Rf\ni and Sec corresponds to Rs\ni. Let N\nbe the number of tasks. For task Pi and type d, let Mi,d be the\nnumber of atomic requirements derived from Rd\ni . We draw n\nindependent generations for each task, where n is the number\nof sampled candidate solutions per task. For each atomic re-\nquirement j \u2208{1, . . . , Mi,d}, let ci,d,j be the number of gen-\nerations (out of n) that pass this requirement. We compute\nPass@k under sampling without replacement for each atomic\nrequirement and then average over all atomic requirements of\ntype d across tasks:\nRd@k =\n1\nPN\ni=1 Mi,d\nN\nX\ni=1\nMi,d\nX\nj=1\n1 \u2212\n\u0000n\u2212ci,d,j\nk\n\u0001\n\u0000n\nk\n\u0001\n!\n.\n(1)\nThis aggregation gives equal weight to each atomic require-\nment, which matches our requirement-level harness design.\nWe report Rd@k for k \u2264n.\n5\nExperimental Setup\nWe evaluate a comprehensive suite of models, spanning\nclosed-source models including Claude-4.5 Series [An-\nthropic, 2025], GPT-5 Series [OpenAI, 2025b], o1 [Ope-\nnAI, 2024], Gemini Series [Google, 2025] and Qwen3-\nModels\nSWE-bench\nVerilogEvalV2\nHSBench1-attempt@1\nHSBench@1\nHSBench@5\nPass@1\nPass@20\nFunc.\nSec.\nFunc.\nSec.\nFunc.\nSec.\nClosed-source Models\nClaude-4.5-Opus\n74.40\n85.26\n91.03\n91.72\n37.45\n97.11\n42.91\n98.01\n45.66\nClaude-4.5-Sonnet\n70.60\n75.00\n85.26\n87.86\n32.31\n95.39\n38.68\n98.88\n45.07\nGPT-5.1-medium\n66.00\n83.33\n92.31\n91.68\n40.02\n97.27\n45.76\n98.76\n53.88\nGPT-5-medium\n65.00\n83.33\n91.03\n91.35\n39.03\n95.98\n42.96\n98.82\n50.42\no1\n-\n73.72\n85.90\n89.15\n35.57\n96.31\n41.66\n98.97\n49.66\nGemini-3-Pro-Preview\n74.20\n85.90\n93.59\n93.93\n42.51\n97.71\n46.11\n99.08\n52.49\nGemini-2.5-Pro\n53.60\n81.41\n91.67\n91.43\n39.29\n96.57\n44.56\n98.72\n51.51\nGemini-2.5-Flash\n28.73\n71.79\n90.38\n89.85\n32.07\n95.68\n37.07\n98.75\n44.46\nQwen3-Max\n69.60\n64.10\n78.21\n84.13\n30.70\n93.17\n37.13\n98.11\n44.58\nOpen-source Models\nDeepSeek-V3.2\n60.00\n69.23\n89.10\n85.33\n32.11\n97.03\n42.42\n99.26\n49.00\nQwen3-Coder-480B-A35B\n55.40\n63.46\n76.28\n85.20\n32.74\n95.71\n39.74\n98.58\n45.81\nGLM-4.6\n55.40\n66.03\n82.05\n86.18\n35.67\n95.93\n42.89\n98.80\n51.10\nKimi-K2-Instruct-0905\n-\n73.72\n85.90\n88.53\n33.06\n96.18\n39.09\n98.59\n45.03\nGPT-OSS-120B\n26.00\n75.64\n90.38\n88.64\n34.07\n96.33\n40.73\n98.43\n46.21\nQwen3-Coder-30B-A3B\n-\n48.08\n62.18\n73.70\n28.13\n95.87\n39.63\n98.39\n46.27\nLlama-4-Maverick-Instruct\n21.04\n55.13\n66.67\n84.72\n35.87\n95.40\n44.38\n98.05\n50.28\nLlama-4-Scout-Instruct\n9.06\n37.18\n50.64\n77.12\n30.82\n95.67\n40.66\n98.20\n46.84\nQwen3-14B\n-\n54.49\n77.56\n76.14\n30.05\n95.31\n40.57\n98.68\n50.42\nTable 1: Main evaluation of LLMs under security settings. Func. and Sec. denote functional and security pass rates (in %). HSBench denotes\nHardSecBench; HSBench1-attempt@1 denotes the single-attempt evaluation setting. Within each model group (open-source and closed-source),\nbold indicates the best result and underline indicates the second-best result.\nMax [Qwen Team, 2025a], alongside open-source mod-\nels including DeepSeek-V3.2 [DeepSeek-AI, 2024], GLM-\n4.6 [Z.ai, 2025], Kimi-K2-Instruct [Moonshot AI, 2025],\nQwen3 Series [Qwen Team, 2025b], Llama 4 Series [Meta,\n2025] and GPT-OSS [OpenAI, 2025a].\nWe report two\nmetrics, Functional Pass Rate and Security Pass Rate, un-\nder an execution environment that uses Icarus Verilog\nv12.0 for RTL designs and gcc for C implementations.\nIn the multi-iteration setting, the refinement loop is capped\nat 5 iterations. For each generated candidate, we allow up\nto 3 rounds of compile and fix. API-based models are ac-\ncessed via OpenRouter [OpenRouter, Inc, 2023] and Silicon-\nFlow [SiliconFlow, 2023], while local models are evaluated\non NVIDIA A100-40G GPUs. We use GPT-5.1 to power\nthe auxiliary agents that provide functional feedback. For the\nCollaborator agent, we set the temperature to 0.3 to keep its\nfeedback stable. During benchmark construction, all agents\nare powered by Gemini-3-Pro-Preview.\n6\nResults and Analysis\n6.1\nBenchmark Statistics and Quality Validation\nBefore evaluating the target models, we validate the correct-\nness and internal consistency of HardSecBench.\nWe start\nfrom 1170 samples spanning 76 CWE entries and run our ef-\nficacy screening. We remove 246 samples that do not meet\nour thresholds, yielding 924 validated tasks with 4425 test\ncases. Among the retained tasks, 599 are Verilog (64.8%)\nand 325 are C (35.2%). As shown in Figure 4, the retained\ntasks achieve high quality on two key dimensions.\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nCode Coverage Rate\n0\n100\n200\n300\n400\n500\n600\nNumber of Samples\nFiltered\nThreshold (80%)\nC\nVerilog\n(a) Coverage test distribution.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutation Detection Rate\n0\n100\n200\n300\n400\n500\n600\nNumber of Samples\nFiltered\nThreshold (50%)\nC\nVerilog\n(b) Mutation Test Distribution.\nFigure 4: Quality validation of HardSecBench: (a) Code coverage\nand (b) mutation detection distributions. Red dashed lines indicate\nthe strict quality gates used for sample filtering.\nCode Coverage Analysis.\nTo ensure the verification arti-\nfacts exercise meaningful paths, we impose a minimum cov-\nerage threshold of 80%. For C artifacts, we measure line cov-\nerage using gcov with instrumentation. For Verilog, consid-\nering the varying support for automated coverage metrics in\nopen-source toolchains, we estimate coverage via static anal-\nysis by mapping test harness signals to executable lines. As\nshown in Figure 4a, the benchmark attains 92.5% average\ncoverage, indicating strong internal consistency.\nDiscriminative Power via Mutation Analysis.\nA test har-\nness is useful only if it separates correct implementations\nfrom insecure variants. We apply five mutation operators:\nconstant change, operator swap, condition negation, stuck-at\nsignal, and assignment removal. For each task, we generate\nfive mutants and mark detection if any test fails. We retain\ntasks only when the mutation score is at least 50%. As shown\nin Figure 4b, the benchmark attains a mean mutation score of\n70.2%, indicating strong discriminative capability in expos-\ning insecure implementations.\n6.2\nMain Evaluation: Security Awareness\nWe use reported SWE-bench Verified [Jimenez et al., 2024]\nresults as a baseline for general software engineering capa-\nbility and run VerilogEvalV2 [Pinckney et al., 2025] to mea-\nsure RTL code generation capability. On HardSecBench, we\nreport single-attempt scores to reflect initial security aware-\nness, and we report iterative-refinement scores together with\nPass@5 to measure robustness under repeated generations.\nAll results are summarized in Table 1.\nKey finding.\nAcross models, functional pass rates are sub-\nstantially higher than security pass rates. This pattern shows\nthat when models are given only functional requirements,\nthey often do not naturally implement protections that are\nlater checked by the security harnesses. Moreover, security\nscores have weak correlation with general code generation ca-\npability, which suggests that security compliance is not fully\nexplained by overall coding strength.\nEffect of iterative refinement.\nWith collaborator feedback,\nmodels resolve functional issues quickly and typically reach\na high level of functional correctness after about one to two\nrefinement iterations on average. However, this rapid func-\ntional improvement yields little gain in security performance.\nBecause the refinement loop is driven by functional failures,\nit rarely addresses vulnerabilities that do not manifest as func-\ntional errors, so functional saturation does not translate into\nsecurity compliance.\nRobustness under larger sampling budgets.\nPass@5\nmainly improves functional success rates, while security\nscores for most models increase only marginally.\nAcross\nindependent attempts, models often converge to function-\nally correct solutions that repeatedly exhibit similar security\nweaknesses, which suggests that the dominant failure mode\nis systematic rather than accidental.\n6.3\nImpact of Prompt Explicitness\nWe examine whether security failures mainly arise from lim-\nited security knowledge or from weak security awareness. To\nthis end, we conduct a prompt sensitivity analysis by varying\nthe explicitness of security guidance across three levels. Hint\n0 includes only functional requirements, Hint 1 adds general\nsecurity reminders, and Hint 2 explicitly mentions specific\nvulnerability classes. Figure 5 shows how these hints affect\nsecurity relative to general code generation capability.\nActivation of Security Knowledge and Model Differences.\nThe results reveal a clear disparity between closed-source and\nopen-source models in activating security knowledge. For\nseveral closed-source models, Hint 1 yields noticeable gains,\nshowing that even general security reminders can shift be-\nhavior toward safer outputs; Hint 2 delivers larger improve-\nments, implying that security expertise is present in training\nbut remains dormant when only functional requirements are\nprovided. By contrast, smaller open-source models remain\nweakly responsive even under Hint 2, suggesting that their\nsecurity performance is primarily constrained by limited ca-\npability rather than unactivated knowledge.\nHint 0\nHint 1\nHint 2\nSecurity Hint Level\n40\n50\n60\n70\nHardSecBench@1 (%)\nAvg Hint 0: 41.9%\nAvg Hint 1: 49.7%\nAvg Hint 2: 64.3%\nClaude Opus 4.5\nClaude Sonnet 4.5\nGemini 2.5 Flash\nGemini 2.5 Pro\nGemini 3 Pro\nGPT-5 Medium\nGPT-5.1 Medium\nO1\nQwen3 Max\n(a) Closed-source models.\nHint 0\nHint 1\nHint 2\nSecurity Hint Level\n40\n50\n60\n70\nHardSecBench@1 (%)\nAvg Hint 0: 41.0%\nAvg Hint 1: 44.2%\nAvg Hint 2: 53.1%\nQwen3-Coder-480B\nQwen3-14B\nQwen3-Coder-30B\nDeepSeek V3.2\nGPT-OSS-120B\nLlama 4 Maverick\nLlama 4 Scout\nKimi K2\nGLM-4.6\n(b) Open-source models.\nFigure 5: Security pass rates across different security hint levels for\nrepresentative models. (a) Closed-source models, (b) Open-source\nmodels.\n10\n20\n30\n40\n50\n60\n70\nSWE-Bench\n40\n45\n50\n55\n60\nHardSecBench@1 (%, Hint1)\nOpen-source\nOpen-source Models Fit\nClosed-source\nClosed-source Models Fit\n(a) Hint 1 vs. SWE-bench.\n40\n50\n60\n70\n80\nVerilogEvalV2 Pass@1\n40\n45\n50\n55\n60\nHardSecBench@1 (%, Hint1)\nOpen-source\nOpen-source Models Fit\nClosed-source\nClosed-source Models Fit\n(b) Hint 1 vs. VerilogEvalV2.\n10\n20\n30\n40\n50\n60\n70\nSWE-Bench\n45\n50\n55\n60\n65\n70\n75\nHardSecBench@1 (%, Hint2)\nOpen-source\nOpen-source Models Fit\nClosed-source\nClosed-source Models Fit\n(c) Hint 2 vs. SWE-bench.\n40\n50\n60\n70\n80\nVerilogEvalV2 Pass@1\n45\n50\n55\n60\n65\n70\n75\nHardSecBench@1 (%, Hint2)\nOpen-source\nOpen-source Models Fit\nClosed-source\nClosed-source Models Fit\n(d) Hint 2 vs. VerilogEvalV2.\nFigure 6: Correlation between code generation capability and secu-\nrity potential across prompting levels.\nRelationship Between Security Potential and General\nCoding Capability\nFigure 6 shows a strong positive corre-\nlation between general code generation capability and secu-\nrity performance under explicit guidance. Under the Hint 0\nsetting, security pass rates show no significant correlation\nwith general code generation capability.\nIn contrast, un-\nder Hint 1 and Hint 2, security performance increases with\ngeneral coding capability. As general coding capability in-\ncreases, closed-source models exhibit a larger increase in se-\ncurity potential than open-source models. This pattern sug-\ngests that security improvements under guidance are closely\nrelated to general code generation capability.\n6.4\nImpact of Domain-Specific Fine-tuning\nWe evaluate whether hardware-specialized fine-tuning re-\nduces the security gaps relative to the corresponding base\nmodels.\nOur analysis of the CodeV-R1 [Zhu et al.,\n2025] and RTLCoder [Liu et al., 2025] series relative to\ntheir base models indicates that both paradigms achieve\nimproved security performance on the Verilog subset of\nHardSecBench1-attempt@1 after domain-specific training. By\ncomparison, CodeV-R1 shows substantially larger fine-tuning\ngains, with clear improvements in both functional correct-\nness and security compliance as the prompt provides more\nexplicit security guidance, whereas the RTLCoder series ex-\nDeepseek-Coder-6.7B\nRTLCoder\nRTLCoder-v1.1\n20\n30\n40\n50\n60\n70\n80\n90\nHardSecBench1\nattempt@1(%)\nFunc\nHint 0\nHint 1\nHint 2\nSec\nHint 0\nHint 1\nHint 2\n(a) RTLCoder\nQwen2.5-Coder-7B CodeV-R1-Distill\nCodeV-R1-RL\n20\n30\n40\n50\n60\n70\n80\n90\nHardSecBench1\nattempt@1(%)\nFunc\nHint 0\nHint 1\nHint 2\nSec\nHint 0\nHint 1\nHint 2\n(b) CodeV-R1\nFigure 7: Comparison of functional and security performance for\nhardware-specialized models across multiple prompting levels.\nCWE-1196\nCWE-1197\nCWE-1198\nCWE-1199\nCWE-1201\nCWE-1202\nCWE-1203\nCWE-1205\nCWE-1206\nCWE-1207\nCWE-1208\nCWE-1388\nCWE Category\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHardSecBench@1\nHint 0\nHint 1\nHint 2\n(a) Closed-source models.\nCWE-1196\nCWE-1197\nCWE-1198\nCWE-1199\nCWE-1201\nCWE-1202\nCWE-1203\nCWE-1205\nCWE-1206\nCWE-1207\nCWE-1208\nCWE-1388\nCWE Category\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHardSecBench@1\nHint 0\nHint 1\nHint 2\n(b) Open-source models.\nFigure 8: CWE pass rates for different model categories: (a) closed-\nsource models, (b) open-source models\nhibits only marginal security improvements despite higher\nfunctional pass rates relative to their corresponding base mod-\nels. This behavior in the RTLCoder series indicates that as\nsecurity requirements become more complex, weaker models\nmay struggle to reliably follow the specification, resulting in\nlower functional correctness and reduced security pass rates.\nSpecialized Fine-Tuning under Increasing Security Hints\nOur evaluation shows that domain-specific fine-tuning im-\nproves the security baseline at Hint 0 compared to the base\nmodels. When security guidance becomes more explicit, the\ntwo model families diverge in scaling behavior. CodeV-R1\nexhibits consistent gains from Hint 0 through Hint 2 in both\nfunctional and security pass rates. In contrast, the RTLCoder\nseries peaks at Hint 1 and drops at Hint 2, where highly ex-\nplicit security guidance reduces both functional and security\nperformance. Further progress requires fine-tuning stronger\nbase models that can anticipate potential security vulnerabil-\nities, supported by higher-quality training data and improved\ntraining recipes, to internalize secure behavior rather than\nmerely follow explicit hints.\n6."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation**\n\nThis academic paper introduces HardSecBench, a benchmark designed to evaluate the security awareness of large language models (LLMs) in hardware code generation. The authors recognize that while existing studies primarily focus on functional correctness, there is a critical gap regarding security issues embedded within LLM-generated code.\n\n**Main Contributions:**\n1. **Systematic Evaluation Setting:** Formulates a rigorous testbed for secure hardware code generation.\n2. **Multi-Agent Construction Pipeline:** Develops an automated pipeline to synthesize benchmark samples and produce testable specifications and corresponding simulatable harnesses, ensuring objectivity and preventing logic coupling between synthesis and verification.\n3. **HardSecBench Benchmark:** Introduces HardSecBench as a benchmark of 924 tasks covering Verilog RTL and firmware-level C code generation, spanning 76 Common Weakness Enumeration (CWE) entries.\n\n**Methodology:**\nThe authors propose an automated pipeline that couples four stages with a single structured specification. Each task includes a problem statement, I/O interface specifications, functional requirements, security requirements, and atomic harnesses for verification via simulation or execution. The process ensures objectivity by decoupling synthesis from verification and grounding evaluation in execution evidence.\n\n**Key Findings:**\n1. **Security Flaws:** LLM-generated code often satisfies functional requirements while still harboring potential security vulnerabilities that could induce catastrophic damages after deployment.\n2. **Prompt Sensitivity Analysis:** Models exhibit varying security results depending on the explicitness of security guidance provided by prompts, highlighting the importance of clear and detailed security instructions for achieving robust security performance.\n\n**Conclusions:**\nThe paper highlights pressing challenges in ensuring secure hardware design using LLMs and offers actionable insights for future advancements. The authors conclude with a comprehensive evaluation of state-of-the-art (SOTA) models under HardSecBench, revealing that significant numbers of models generate functionally valid designs but lack critical security protections. They also demonstrate the potential of specialized training to enhance security performance, particularly in complex scenarios.\n\n**Experimental Setup:**\nThe study evaluates a comprehensive suite of models including closed-source and open-source models across different hardware domains using Icarus Verilog v12.0 for RTL designs and gcc for C implementations. The evaluation methodology employs metrics such as Pass@k to quantify generation stability across independent samples, ensuring robustness in security assessments.\n\n**Impact:**\nThe findings underscore the critical need for explicit security guidance within LLM-assisted hardware design and highlight the potential of domain-specific fine-tuning to address inherent limitations in model performance under complex security requirements. The paper concludes by emphasizing the importance of developing standardized benchmarks like HardSecBench to systematically assess and improve the security awareness of LLMs in hardware code generation."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "DermaBench: A Clinician-Annotated Benchmark\nDataset for Dermatology Visual Question Answering\nand Reasoning\nAbdurrahim Yilmaz1,*, Ozan Erdem2,, Ece Gokyayla3,, Ayda Acar4,, Burc Bugra Dagtas5,,\nDilara Ilhan Erdil6,, Gulsum Gencoglan7,*, and Burak Temelkuran1,*\n1Imperial College London, Division of Systems Medicine, Department of Metabolism, Digestion, and Reproduction,\nUnited Kingdom\n2Istanbul Medeniyet University, Department of Dermatology and Venereology, Turkiye\n3Usak Research and Training Hospital, Department of Dermatology and Venereology, Turkiye\n4Ege University, Department of Dermatology and Venereology, Turkiye\n4Istanbul Research and Training Hospital, Department of Dermatology and Venereology, Turkiye\n5Ipswich Hospital, Department of Dermatology and Venereology, United Kingdom\n6Medicana Atakoy Hospital, Department of Dermatology and Venereology, Turkiye\n*Corresponding authors: Abdurrahim Yilmaz (a.yilmaz23@imperial.ac.uk), Gulsum Gencoglan\n(ggencoglan@gmail.com), Burak Temelkuran (b.temelkuran@imperial.ac.uk)\nABSTRACT\nVision\u2013language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology\nremains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable\nfor recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning\ncapabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret\ndermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce\nDermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset.\nDermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I\u2013VI. Using a hierarchical\nannotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated\neach image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together\nwith open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is\nreleased as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.\nBackground & Summary\nDermatology is one of the most visually driven specialties in medicine, relying heavily on pattern recognition across a\nwide spectrum of skin morphologies, colors, and anatomical sites. With the increasing adoption of teledermatology, mobile\nconsultations, and electronic health records, vast quantities of dermatologic images are now captured daily in real-world settings.\nThis surge of data offers an unprecedented opportunity for artificial intelligence (AI) to assist in diagnosis, triage, and education.\nHowever, despite these advances, the field lacks standardized, expert-annotated visual question answering (VQA) benchmark\ndatasets that capture the true diversity of dermatologic conditions while maintaining clinically detailed labels and equitable\nrepresentation of different skin tones.\nRecent work in medical multimodal learning has produced a growing ecosystem of visual question answering (VQA)\nand vision\u2013language datasets across medical fields such as radiology and pathology. Radiology datasets such as VQA-RAD\ncomprise manually written questions but remain small (315 images, 3,515 QA pairs) and limited in scope, and their questions\nfocus on modality or organ identification rather than clinical reasoning1. PathVQA expands scale but generates QA pairs\nautomatically from textbook captions using rule-based NLP, producing shallow questions not aligned with real clinical tasks\nand no clinician involvement2. More recent multilingual resources such as WoundCareVQA collect naturally occurring\ntext\u2013image pairs but target consumer health wound triage rather than medically curated disease classification, and lack\nstructured dermatologic annotation or expert-defined ground-truth reasoning3. Similarly, SLAKE provides semantic annotations\nand a medical knowledge graph, but focuses on radiology and CT/MRI imaging modalities rather than clinical dermatology .\nIn dermatology, most publicly available datasets have been developed for image-level classification tasks, such as lesion\narXiv:2601.14084v1  [cs.CV]  20 Jan 2026\ndiagnosis or malignancy detection. Widely used datasets and related collections have enabled substantial progress in supervised\nimage classification, particularly for melanoma recognition and differential diagnosis4\u20137. However, these datasets provide only\na single diagnostic label per image and do not support evaluation of language grounding, fine-grained visual understanding, or\nclinical reasoning-capabilities that are central to emerging vision language models.\nIn contrast, dermatology-focused multimodal and visual question answering (VQA) datasets remain limited in trustwor-\nthiness, annotation depth, and clinical validity. DermatoLlama8 introduces a large dedicated test set (Eval1) comprising 210\ndermoscopic images paired with 21,597 dermatologist-curated VQA pairs, providing a clinically grounded benchmark for\nassessing visual understanding and question answering performance in dermatology-focused VLMs. DermaVQA9 is a public\ndermatology VQA dataset, yet its answers are sourced from patient-consultation websites, resulting in variable accuracy and\nlimited medical reliability, and its images are restricted to clinical photographs without expert verification or multimodal\ndiversity. MM-Skin seeks to scale dermatology VQA by extracting captions from textbooks and using large language models\nto synthesize approximately 27k QA pairs, but its supervision is synthetically generated rather than clinician-authored, and\nblends automatically extracted captions with LLM-written conversations instead of dermatologist-verified ground truth10. More\nrecent dermatology datasets with richer medical labels, such as SkinCon and SkinCAP (including SkinCoT and SkinCaRe),\nprovide concept annotations, captions, or chain-of-thought reasoning; however, they do not offer an integrated VLM benchmark\nwith standardized question taxonomy, multi-expert consensus labeling, or consistent evaluation of reasoning and fairness\nacross skin tones and imaging modalities11,12. Collectively, existing dermatology multimodal datasets exhibit three recurrent\ngaps: (1) absence of dermatologist-curated ground-truth VQA annotations for clinical dermatology images, (2) reliance on\ninternet-scraped or synthetically generated supervision, and (3) lack of benchmarks explicitly designed to evaluate visual\nunderstanding, language grounding, reasoning, and fairness in dermatology-focused VLMs.\nTo address these limitations, we introduce DermaBench, a fully clinician-curated, multi-source dermatology benchmark\ndesigned expressly for evaluating the visual understanding and reasoning capabilities of vision\u2013language models. Unlike\nprior datasets, DermaBench provides dermatologist-authored question\u2013answer pairs in a structured VQA format, ensuring\nmedically reliable ground truth for each item. The benchmark is built on Diverse Dermatology Image dataset13, comprising 656\nimages from 570 patients, spanning clinical and dermoscopic modalities, each annotated by at least two expert dermatologists\nfor various perspectives such as morphology, anatomical site. Importantly, every VQA item is written and validated by\ndermatologists, not synthesized by algorithms, overcoming the reliability limitations of DermaVQA and MM-Skin. As a result,\nDermaBench becomes the first dermatology dataset that combines multimodal imagery, expert-verified VQA annotations,\nstandardized ontology-driven labels, and fairness-aware skin tone distributions, enabling rigorous evaluation of both perception\nand reasoning. This design makes DermaBench a high-trust benchmark for dermato-focused vision\u2013language models, providing\na foundation for reproducible, fair, and clinically aligned multimodal AI research. The dataset is publicly available at\nhttps://doi.org/10.7910/DVN/Q4LBIW.\nMethods\nWe selected the Diverse Dermatology Images (DDI) dataset as the primary component of DermaBench because it is the only\npublicly available dermatology image collection designed explicitly for skin tone diversity and fairness research. DDI contains\nclinical photographs spanning Fitzpatrick skin types I\u2013VI, with each image labeled by dermatologists for both diagnosis and\nFitzpatrick phototype. Its structure-80 diagnostic categories distributed across varied ethnicities, disease severities, and body\nsites-makes it a high-trust, clinically grounded resource. This diversity enables DermaBench to represent lesions from both light-\nand dark-skinned patients more equitably, ensuring that fairness considerations can be systematically evaluated in downstream\nvision and vision-language models. The DDI dataset was annotated by six dermatologists using the AnnotatorMed interface to\ncreate a dermatological VQA dataset (Figure 1), resulting in 14,474 image\u2013question\u2013answer pairs (Table 2).\nDataset Composition\nDermaBench was developed as a clinician-curated benchmark focused on diagnostic diversity, demographic fairness, and\nprecise labeling. The dataset integrates two open-source repositories: (i) the Diverse Dermatology Images dataset, selected for\nits Fitzpatrick skin tone annotations and broad diagnostic classes In total, DermaBench contains 656 images from 570 unique\npatients, covering a range of skin tones, lesion types, and clinical contexts suitable for evaluating dermatologic visual reasoning.\nSix expert dermatologists participated in the annotation process. Each image was labeled using a comprehensive, hierarchical\nquestion set designed to capture global image characteristics, dermatologic morphology, and clinically meaningful narrative\ndescriptions. The full schema consists of 22 primary questions (Q0\u2013Q21) (Table 1, Supplementary Material 2-3). Importantly,\nthe question set itself was designed by three expert dermatologists, drawing directly from dermatologic examination frameworks\nand the morphology chapters of standard dermatology textbooks to ensure clinical fidelity and alignment with real-world\ndiagnostic workflows. The development of this schema followed an iterative process: the full hierarchical question set was\nreviewed, tested, and revised across three separate design rounds, each time incorporating feedback from expert dermatologists\n2/9\nto refine clarity, branching logic, and morphological completeness. Importantly, the question set itself was designed by three\nexpert dermatologists, drawing directly from dermatologic examination frameworks and the morphology chapters of standard\ndermatology textbooks to ensure clinical fidelity and alignment with real-world diagnostic workflows14\u201317. The development\nof this schema followed an iterative process: the full hierarchical question set was reviewed, tested, and revised across three\nseparate design rounds, each time incorporating feedback from each of expert dermatologists to refine clarity, branching logic,\nand morphological completeness. To further ensure consistency in downstream annotations, the same three dermatologists\nprepared a consensus guideline outlining standardized interpretations for lesion morphology, distribution patterns, color\nterminology, and surface characteristics. This consensus document was used throughout the annotation phase to minimize\ninter-rater variability and maintain uniform application of the hierarchical schema across all annotators. These procedures\ncollectively strengthen the reliability, reproducibility, and clinical interpretability of the DermaBench annotations.\nAnnotation Framework, AnnotatorMed\nAll images were annotated using AnnotatorMed, an open-source medical data annotation interface developed within the\nSCALEMED framework8. AnnotatorMed is a web-based, modular tool that renders dynamic question hierarchies through\na context-aware interface. Sub-questions are automatically displayed only when their relevant parent options are selected,\npreventing screen clutter and reducing scrolling time. This user experience (UX) design allows physicians to complete each\nannotation efficiently without navigating unnecessary question branches, enabling rapid yet comprehensive labeling during\nclinical workflows.\nConsensus and Quality Control\nEach image was annotated independently by two expert dermatologists using a predefined hierarchical question schema. Prior\nto annotation, three senior dermatologists developed a consensus checklist (Supplementary Material 1) that precisely defined\nthe meaning and scope of each question and answer option, aligning terminology with standard dermatologic examination\nframeworks. This checklist was used throughout the annotation process to minimize inter-annotator variability. Annotation\ndiscrepancies were resolved during a structured consensus review phase, in which dermatologists jointly adjudicated differences\nin lesion morphology, distribution patterns, diagnostic interpretation, and Fitzpatrick skin tone assignment. Following consensus\nresolution, a final quality control step was performed by a dermatologist and an engineer, who jointly reviewed all annotations\nto identify residual dermatologic inconsistencies, reasoning errors, or structural issues in the annotation records. Images (n=10)\nwith unresolved disagreements, insufficient visual quality, or ambiguous clinical content were excluded to ensure dataset\nreliability.\nAnnotation Efficiency and UX Performance\nAnnotatorMed\u2019s dynamic rendering mechanism substantially improved annotation efficiency. By displaying sub-questions\nonly when their parent options were selected, the interface reduced visual clutter and scrolling time by an estimated 50%.\nDermatologists completed a full annotation in an average of 3\u20135 minutes per image. The combination of structured single\nchoice, multiple-choice and open-ended questions and narrative fields allowed simultaneous quantitative and descriptive data\ncapture, supporting downstream use in classification, retrieval, and multimodal VQA evaluation.\nAll data follow a VQA-style JSON (Supplementary Material 2) schema that includes image path, modality tag, Fitzpatrick\nphototype, diagnostic and morphological labels, question category, answer text, annotator identifier, and additional metadata\nfields supporting fairness and reasoning analyses. The dataset is released for research use under a permissive license and is\naccompanied by documentation detailing the annotation protocol, consensus process, and recommended evaluation workflows.\nThis unified structure enables seamless integration into multimodal training pipelines and provides a reproducible, clinically\ngrounded benchmark for dermatology-focused vision and vision\u2013language models.\nData Records\nDermaBench does not host or redistribute any dermatologic images. Instead, the dataset provides a structured metadata table on\nHarvard Dataverse (The dataset is publicly available at https://doi.org/10.7910/DVN/Q4LBIW)18 containing all\nannotation fields and image identifiers required to reproduce experiments or link to the original image sources13. This design\nrespects the licensing constraints of the upstream datasets while enabling standardized benchmarking. It was released under a\nCreative Commons Attribution (CC BY-NC) license.\nResearchers must obtain direct access to the images from the original repositories. For the Diverse Dermatology Images\n(DDI) dataset, image files and access procedures are available at https://ddi-dataset.github.io/index.html#\ndataset. The DDI dataset is released under a restricted academic-use license: (i) permission is granted to view and use\nthe dataset without charge for personal, non-commercial research purposes only, and (ii) users may not distribute, publish, or\n3/9\nreproduce any portion of the dataset without prior written permission from the Stanford School of Medicine. DermaBench\ntherefore provides only metadata and does not copy or redistribute DDI images in any form.\nThe metadata is distributed as a tabular CSV-style dataset, with one row per image and annotation fields corresponding to\nthe hierarchical question schema. Because DermaBench uses a conditional annotation workflow, many sub-question fields\nremain intentionally empty when a parent question was not triggered. For instance, sub-questions related to the lesion location\n\u201cscalp\u201d appear only when the annotator selected \u201cscalp\u201d in Q12; otherwise those columns remain blank.\nDermaBench does not include a predefined train\u2013test split. Instead, a unified metadata file is provided to allow researchers\nto construct splits appropriate for classification, fairness auditing, VQA, or multimodal reasoning tasks.\nDataset Metadata\nThe DermaBench metadata file includes the following fields:\n\u2022 DDI_file_ID: Identifier linking to the corresponding DDI image.\n\u2022 questionX: The text of each question in the annotation schema (Q0\u2013Q21).\n\u2022 answerX: The dermatologist\u2019s answer to each question. Empty fields indicate questions not triggered by branching logic.\n\u2022 subquestion fields: Hierarchical attributes (e.g., Q16.1, Q18.1). Present only when applicable.\nThe metadata schema supports both high-level tasks (disease classification, fairness assessment, distribution analysis) and\nfine-grained multimodal tasks (visual question answering, long-form reasoning, semantic parsing). Its hierarchical organization\npreserves the full richness of dermatologic morphology while maintaining compatibility with VQA-style architectures.\nTechnical Validation\nReliable evaluation of dermatology-focused AI systems requires benchmark datasets that are clinically trustworthy, demo-\ngraphically inclusive, and annotated with sufficient detail to support both diagnostic and reasoning-based tasks. The Diverse\nDermatology Images (DDI) dataset was originally curated to meet this need-the first publicly accessible dermatology collection\nwith expert-confirmed diagnoses and intentionally diverse Fitzpatrick skin types. DermaBench builds upon this foundation\nby adding structured, multi-level visual question answering (VQA) annotations, enabling assessment not only of diagnostic\nperformance but also of multimodal reasoning and fairness across skin tones.\nClinically, AI has the potential to assist triage workflows by helping distinguish benign from malignant lesions, thereby\nreducing delays in dermatologic evaluation. However, dermatology datasets must represent all patient groups fairly. Because\nalgorithmic biases often arise from underrepresentation, the DDI dataset\u2019s intentional inclusion of Fitzpatrick I\u2013VI skin tones\nprovides a validated substrate for fairness auditing, which DermaBench directly inherits and extends through its annotation\nschema.\nValidation of Source Dataset (DDI)\nThe images used in DermaBench that originate from DDI underwent rigorous clinical and pathology-based validation at the\ntime of the original dataset curation. As described in the DDI dataset paper13, all images were retrospectively selected by\nreviewing pathology-confirmed cases from Stanford Clinics between 2010\u20132020. A total of 656 images representing 570\nunique patients were included. Each diagnosis was validated by both a expert dermatologist and a dermatopathologist using the\ncorresponding biopsy report. Skin tone labels were assigned using in-person clinical assessments, demographic photographs,\nand review of the clinical images by two expert dermatologists. These procedures ensure high-fidelity diagnostic labels and\naccurate Fitzpatrick assignments, which form the basis for downstream fairness assessments in DermaBench.\nValidation of Annotation Quality\nDermaBench extends the DDI dataset through structured annotation of dermatologic morphology, image quality, distribution\npatterns, color hierarchy, and higher-level VQA tasks. All annotations were performed independently by expert dermatologists\nusing a hierarchical schema implemented in AnnotatorMed. The annotation workflow is developed to increase:\n\u2022 consistency through controlled vocabularies and predefined branching logic,\n\u2022 precision through specialist-performed labeling,\n\u2022 reproducibility through explicit documentation of all question and answer fields\nBecause the annotation interface displays sub-questions only when parent attributes are selected, missing entries in\nsub-columns reflect intentional branching rather than incomplete annotation.\n4/9\nUsage Notes\nThe annotation framework is organized into three functional blocks, each corresponding to a distinct stage in dermatologic\nimage interpretation:\n\u2022 0. Not answered field (Q0)\nIndicates whether the questions is answered or not.\n\u2022 1. Fundamental Image Questions (Q1\u2013Q10)\nThese questions evaluate foundational attributes of each image, including:\n\u2013 content type and modality,\n\u2013 identifiability and framing,\n\u2013 focus, illumination, and presence of artifacts,\n\u2013 Fitzpatrick skin tone.\nThis block ensures accurate filtering of non-dermatologic images, supports image quality control, and enables fairness\nauditing across skin phototypes.\n\u2022 2. Detailed Dermatology VQA (Q11\u2013Q20)\nThese questions encode structured, lesion-level dermatologic information using controlled vocabularies designed for both\nclinical interpretability and machine learning. They include:\n\u2013 Q11: lesion count and multiplicity,\n\u2013 Q12: anatomical location,\n\u2013 Q13: distribution patterns (27 predefined options),\n\u2013 Q14: lesion shape (21 morphologic categories),\n\u2013 Q15: border characteristics (12 options),\n\u2013 Q16: surface features (27 categories such as scale, crust, erosion, atrophy, xerosis),\n\u2013 Q17: lesion size estimation,\n\u2013 Q18: color hierarchy (11 chromatic subtypes),\n\u2013 Q19\u2013Q20: elementary lesion types (38 primary and secondary morphologies).\nThis block standardizes the descriptive lexicon used in dermatology and provides granular, machine-readable representa-\ntions of complex visual features.\n\u2022 3. Summary Question (Q21)\nAnnotators provide a concise integrative description that:\n\u2013 synthesizes all previously documented findings,\n\u2013 highlights salient morphological patterns,\n\u2013 contextualizes the lesion within plausible diagnostic considerations.\nThis summary supports downstream evaluation of model-generated clinical reporting.\n5/9\nAcross all blocks, the schema balances categorical precision with descriptive flexibility. The hierarchical branching\nsystem ensures that annotators select only clinically relevant sub-options, reducing cognitive load and supporting consistent\nrepresentation across annotators and image types. This structure enables DermaBench to support multimodal modeling tasks\nranging from classification to long-form clinical reasoning.\nDermaBench annotations were designed to be broadly compatible with the SkinCon11 dermatologic concept taxonomy\nwhile remaining practical for VQA and LLM benchmarking. The concept labels are grounded in previously defined clinical\nterms17, but they are not intended to be exhaustive, nor do they assume perfectly clear-cut boundaries between concepts. In\npractice, dermatologic descriptors may exhibit differing annotations across datasets, including SkinCon, particularly because\nattributes. As a result of expanding the concept set from 48 to 197 concepts, partial differences in annotation granularity\nand alignment emerge between DermaBench and SkinCon, motivating the inclusion of explicit concept mappings. Core\nmorphological attributes are aligned at the question level (Supplementary Table 1), including primary lesion morphology\n(Q19), secondary morphology (Q20), surface features (Q16), color (Q18), and distribution (Q13). Given the potential for\nvariability in definitions and annotations, we recommend providing SkinCon annotations alongside corresponding DermaBench\ncolumns (Supplementary Table 1) when performing LLM benchmarking, enabling transparent comparison and more consistent\ncross-dataset analyses while preserving the VQA-oriented structure of DermaBench.\nVQA Prompt for DermaBench\nThis prompt is designed to benchmark vision\u2013language models on DermaBench by stan-\ndardizing how models interpret dermatological images and respond to structured clinical questions. It ensures consistent\nuse of dermatological terminology and annotation rules by instructing the model to follow the definitions and guidelines\nprovided in Supplementary Material 1. The prompt should be directly paired with the question definitions and answer options\nin Supplementary Material 2.\nMain Prompt:\nYou are a board-certified dermatologist with expertise in clinical image interpretation. Carefully analyze the provided\ndermatological image and answer the question below using only the visual information in the image and standard dermatological\nknowledge.\nFollow the instructions strictly based on the question type:\n\u2022 If QUESTION_TYPE is single-choice, select one and only one answer from the provided options and return the option\ntext exactly as written.\n\u2022 If QUESTION_TYPE is multi-choice, select all options that apply based on the image. Return the selected option texts\nas a comma-separated list, using the exact wording.\n\u2022 If QUESTION_TYPE is open-ended, write a concise, structured, and clinically meaningful report using standard\ndermatological terminology. Do not speculate or include information not supported by the image.\nDo not assume patient history, symptoms, or metadata unless explicitly provided. If the image cannot be evaluated for the\ngiven question, use the dataset-defined response indicating that the image should not be considered.\nQuestion type: {QUESTION_TYPE}\nQuestion: {QUESTION_TEXT}\nOptions: {QUESTION_OPTIONS}\nData availability\nThe dataset is publicly available at https://doi.org/10.7910/DVN/Q4LBIW. The annotation data is published in\nXLSX format and the question set is published in JSON format.\nCode availability\nThere is no code or codebase in this study.\nReferences\n1. Lau, J. J., Gayen, S., Ben Abacha, A. & Demner-Fushman, D. A dataset of clinically generated visual questions and\nanswers about radiology images. Sci. data 5, 1\u201310 (2018).\n6/9\n2. He, X., Zhang, Y., Mou, L., Xing, E. & Xie, P. Pathvqa: 30000+ questions for medical visual question answering. arXiv\npreprint arXiv:2003.10286 (2020).\n3. Yim, W.-w. et al. Woundcarevqa: A multilingual visual question answering benchmark dataset for wound care. J. Biomed.\nInformatics 104888 (2025).\n4. Tschandl, P., Rosendahl, C. & Kittler, H. The ham10000 dataset, a large collection of multi-source dermatoscopic images\nof common pigmented skin lesions. Sci. data 5, 1\u20139 (2018).\n5. Hern\u00e1ndez-P\u00e9rez, C. et al. Bcn20000: Dermoscopic lesions in the wild. Sci. Data 11, 641 (2024).\n6. Isic archive. https://www.isic-archive.com/.\n7. Yilmaz, A., Yasar, S. P., Gencoglan, G. & Temelkuran, B. Derm12345: A large, multisource dermatoscopic skin lesion\ndataset with 40 subclasses. Sci. Data 11, 1302 (2024).\n8. Yilmaz, A. et al. Resource-efficient medical vision language model for dermatology via a synthetic data generation\nframework. medRxiv (2025).\n9. Yim, W.-w. et al. Dermavqa: A multilingual visual question answering dataset for dermatology. In International Conference\non Medical Image Computing and Computer-Assisted Intervention, 209\u2013219 (Springer, 2024).\n10. Zeng, W., Sun, Y., Ma, C., Tan, W. & Yan, B. Mm-skin: Enhancing dermatology vision-language model with an image-text\ndataset derived from textbooks. In Proceedings of the 33rd ACM International Conference on Multimedia, 3769\u20133778\n(2025).\n11. Daneshjou, R., Yuksekgonul, M., Cai, Z. R., Novoa, R. & Zou, J. Y. Skincon: A skin disease dataset densely annotated by\ndomain experts for fine-grained debugging and analysis. Adv. Neural Inf. Process. Syst. 35, 18157\u201318167 (2022).\n12. Zhou, J. et al. Skincap: A multi-modal dermatology dataset annotated with rich medical captions. arXiv preprint\narXiv:2405.18004 (2024).\n13. Daneshjou, R. et al. Disparities in dermatology ai performance on a diverse, curated clinical image set. Sci. Adv. 8,\neabq6147, 10.1126/sciadv.abq6147 (2022). https://www.science.org/doi/pdf/10.1126/sciadv.abq6147.\n14. Plewig, G. et al. Braun-Falco\u2019s Dermatology (Springer Berlin/Heidelberg, Germany, 2022).\n15. James, W. D., Elston, D. M., Treat, J. R., Rosenbach, M. A. & Neuhaus, I. M. Andrews\u2019 diseases of the skin (Elsevier\nHealth Sciences, 2016).\n16. Kang, S. et al. Fitzpatrick\u2019s dermatology (McGraw Hill Education, 2019).\n17. Bolognia, J. L., Jorizzo, J. L. & Schaffer, J. V. Dermatology (Elsevier Health Sciences, 2024).\n18. Yilmaz, A. DermaBench, https://doi.org/10.7910/DVN/Q4LBIW (2025).\n19. Liu, B. et al. Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering. In 2021\nIEEE 18th international symposium on biomedical imaging (ISBI), 1650\u20131654 (IEEE, 2021).\n20. Mahe, R. H., Noor, A. & Tasin, R. N. Lightweight visual question answering (VQA) model for skin disease detection. Ph.D.\nthesis, Brac University (2025).\nAcknowledgements\nThere is no acknowledgement.\nFunding\nAbdurrahim Yilmaz has been funded by the President\u2019s PhD Scholarships of Imperial College London.\nAuthor contributions statement\nConceptualization, A.Y., O.E., E.G., and G.G.; methodology, A.Y., G.G., and B.T.; software, A.Y.; validation, G.G., and\nO.E.; formal analysis, A.Y., and B.T.; investigation, G.G. and O.E.; resources, G.G. and O.E.; data curation, A.Y., O.E., E.G.\nA.A., B.B.D., D.I.E., and G.G.; writing-original draft preparation, A.Y. and B.T.; writing-review and editing, A.Y. and G.G.;\nvisualization, A.Y. All authors have read and agreed to the published version of the manuscript.\nCompeting interests\nThere is no competing interest.\n7/9\nFigures & Tables\nFigure 1. Overview of the DermaBench framework. Data from DDI dataset was annotated by six dermatologists using the\nAnnotatorMed interface8, which dynamically renders hierarchical questions (Q0\u2013Q21). The resulting expert-validated\nannotations form the DermaBench dataset for standardized dermatology vision and VQA benchmarking.\nDomain\nDataset\nModality\n#Images\n#QA Pairs\nAnswer\nLength\nNon-derm\nVQA-RAD1\nRadiology\n315\n3.5K\nOpen-ended\nNon-derm\nSLAKE19\nRadiology\n642\n14K\nBilingual EN/ZH\nNon-derm\nPathVQA2\nPathology\n5K\n32K\nMulti-type Qs\nDerm\nDermaVQA9\nClinical (UGC)\n3.4K\n1.5K\nMultilingual, Reddit + IIYI\nDerm\nMM-Skin10\nVarious1\n\u223c11K\n27K\nAdds textbook image\u2013text pairs\nDerm\nLightweight Derm VQA20\nClinical\n1,038\n\u223c7.3K\n7 QA/img; low-resource focus\nDerm\nTest set of DermatoLlama8\nDermoscopic\n210\n\u223c21.5k\nDermoscopy only\nDerm\nDermaBench (Ours)\nClinical\n656\n\u223c14.4K\n6 dermatologists; Fitzpatrick I\u2013VI\nTable 1. Comparison of DermaBench with representative medical VQA corpora. Various1: Clinical, dermoscopic, pathology\n8/9\nTable 2. Distribution of question types and their functional role in the DermaBench annotation schema.\nQuestion Type\n#Questions\nExamples / Purpose\nSingle-choice\n10\nImage category, illumination, focus, skin type\nMulti-select\n14\nLesion distribution, shape, border, surface, color, lesion types\nOpen-ended\n6\nNarrative quality notes, reports, explanations, summaries\nTotal\n30\n9/9"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper \"DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning\"\n\n#### Main Contributions:\n- **DermaBench**: Introduces a clinician-curated benchmark dataset specifically designed to evaluate vision-language models in dermatology. It focuses on visual question answering (VQA) tasks, which are crucial for understanding dermatological images, reasoning about fine-grained morphology, and generating clinically meaningful descriptions.\n  \n#### Methodology:\n- **Dataset Composition**: The dataset comprises 656 clinical images from 570 unique patients, spanning Fitzpatrick skin types I-VI. Each image is annotated by at least two expert dermatologists using a hierarchical schema with 22 main questions (single-choice, multi-choice, and open-ended). This ensures comprehensive coverage of various aspects such as diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality.\n  \n- **Annotation Process**: The dataset is built on the Diverse Dermatology Images (DDI) dataset. Annotators use a web-based interface called AnnotatorMed to annotate images. A consensus process involving three senior dermatologists ensures consistency across annotations. Each annotation includes standardized interpretations for lesion morphology, distribution patterns, color terminology, and surface characteristics.\n  \n- **Validation**: The DDI dataset undergoes rigorous clinical and pathology-based validation at the time of curation. All images are retrospectively selected from Stanford Clinics between 2010-2020, ensuring high-fidelity diagnostic labels and accurate Fitzpatrick skin type assignments.\n\n#### Key Findings:\n- DermaBench provides a comprehensive set of annotations that capture both global image characteristics and fine-grained dermatologic morphology. The dataset includes structured VQA pairs (image\u2013question\u2013answer) for 14,474 images.\n  \n- **Clinical Relevance**: The dataset is designed to represent lesions from patients with diverse skin tones, ensuring equitable representation across different Fitzpatrick types. This allows for systematic evaluation of fairness considerations in downstream models.\n\n#### Conclusions:\n- DermaBench addresses the limitations of existing dermatology datasets by providing a clinically grounded benchmark that evaluates vision-language model capabilities comprehensively.\n  \n- **Impact**: By offering structured VQA annotations, DermaBench enables rigorous evaluation of visual understanding and reasoning. This foundation supports reproducible, fair, and clinically aligned multimodal AI research in dermatology.\n\n### Summary:\nDermaBench is a clinician-curated benchmark dataset designed to evaluate vision-language models specifically for dermatological applications. It provides comprehensive annotations covering various aspects of dermatologic images, ensuring clinical relevance and fairness considerations are met. This dataset serves as a robust foundation for advancing the field of AI in dermatology by enabling rigorous evaluation of visual understanding, reasoning, and clinical interpretation capabilities."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis\nHongyuan Chen1\nXingyu Chen1\nYoujia Zhang1,2\nZexiang Xu3\nAnpei Chen1\n1Westlake University\n2HUST\n3Hillbot\nMotion 3-to-4\nFigure 1. From a single glance, Motion 3-to-4 unfolds: weaving time, shape, and movement into living 4D reality.\nAbstract\nWe present Motion 3-to-4, a feed-forward framework for\nsynthesising high-quality 4D dynamic objects from a single\nmonocular video and an optional 3D reference mesh. While\nrecent advances have significantly improved 2D, video, and\n3D content generation, 4D synthesis remains difficult due\nto limited training data and the inherent ambiguity of re-\ncovering geometry and motion from a monocular view-\npoint. Motion 3-to-4 addresses these challenges by decom-\nposing 4D synthesis into static 3D shape generation and\nmotion reconstruction. Using a canonical reference mesh,\nour model learns a compact motion latent representation\nand predicts per-frame vertex trajectories to recover com-\nplete, temporally coherent geometry. A scalable frame-wise\ntransformer further enables robustness to varying sequence\nlengths. Evaluations on both standard benchmarks and a\nnew dataset with accurate ground-truth geometry show that\nMotion 3-to-4 delivers superior fidelity and spatial consis-\ntency compared to prior work. Project page is available at\nhttps://motion3-to-4.github.io/.\n1. Introduction\nThe creation of high-fidelity 4D assets\u2013which comprehen-\nsively capture both the static shape and the dynamic mo-\ntion of an object over time\u2013is a critical and highly sought-\nafter capability in fields like virtual reality, cinematography,\nrobotics, and simulation. Recent advancements in 2D im-\nage, video and 3D content synthesis have revolutionized\nthe computer graphics and computer vision communities,\nachieving high fidelity through scalable datasets and learn-\ning frameworks. However, 4D reconstruction and genera-\ntion remain significantly more challenging due to the larger\nsolution space and the inherent complexity of modeling\nspatial-temporal evolution.\nTo address these challenges, recent research has ad-\nvanced along several directions. One line of work [2, 29,\n1\narXiv:2601.14253v1  [cs.CV]  20 Jan 2026\n42, 82, 89, 101, 112] first generates multi-view videos from\ntext or single-image inputs and then reconstructs 4D as-\nsets using dynamic NeRF [53] or Gaussian Splatting [30].\nWhile conceptually straightforward, these pipelines rely on\nlengthy per-instance optimization and are fundamentally\nconstrained by the view inconsistencies inherited from 2D\ngenerative models. Another line of work adopts pretrained\n3D generative priors as a foundation for 4D synthesis. For\nexample, V2M4 [7] produces per-frame meshes followed\nby iterative mesh alignment to recover coherent temporal\nstructure.\nHowever, similar to multi-view reconstruction\nmethods, the optimization remains slow and susceptible to\ntemporal artifacts. Alternatively, several approaches such as\nGVFD [100] and AnimateAnyMesh [86] build motion la-\ntent spaces via VAEs and apply the predicted motion to an\ninitial geometry in a feed-forward manner. These methods\nare efficient and can perform inference in seconds. How-\never, VAE-based modeling typically requires large-scale\nand diverse training data to form a well-structured latent\ndistribution. When trained solely on the limited and nar-\nrow 4D datasets, these models struggle to capture complex\nmotion patterns and exhibit poor generalization. Given the\nscarcity of high-quality 4D training data, our central idea is\nto reformulate 4D generation as a combination of 3D shape\ngeneration and motion reconstruction.\nIn this work, we introduce Motion 3-to-4, a feed-forward\nframework that synthesizes 4D dynamic objects from a sin-\ngle monocular video and an optional reference mesh. To\nefficiently tackle this inherently ill-posed problem, we de-\ncompose 4D generation into two more tractable compo-\nnents: static 3D shape encoding and dynamic motion recon-\nstruction. Our key insight is to leverage a static mesh, either\nprovided or generated, as a stable reference geometry, and\nto estimate per-frame 3D motion flow relative to this canon-\nical state. Given a monocular video and an optional initial\nmesh as input, Motion 3-to-4 first performs motion latent\nlearning, jointly encoding the static object shape and the\nvideo context into a compact motion representation. Based\non this representation, a motion decoder predicts vertex tra-\njectories for queries sampled from the reference mesh, en-\nabling accurate recovery of complete geometry and tempo-\nrally coherent motion throughout the entire sequence.\nWe highlight our contributions as follows:\n\u2022 A feed-forward 4D synthesis framework. We propose a\nnovel feed-forward pipeline that reformulates 4D object\nsynthesis as a motion reconstruction using only monoc-\nular video guidance, achieving strong generalization de-\nspite the scarcity of high-quality 4D training data.\n\u2022 A scalable architecture. We design a frame-wise trans-\nformer architecture that is robust to input meshes of vary-\ning resolution and supports flexible processing of videos\nof arbitrary length.\n\u2022 Benchmarks.\nExisting motion reconstruction and 4D\ngeneration benchmarks typically provide only multi-view\nrenderings without accurate 3D geometry,\nmaking\nrendering alignment and motion evaluation difficult. To\naddress this limitation, we introduce a new motion-80\nbenchmark with ground-truth motion, realistic render-\nings, and geometry. We demonstrate the effectiveness of\nMotion 3-to-4 on both this new dataset and a widely used\nbenchmark [29].\n2. Related work\n3D Object Generation. Early progress in 3D generation\nlargely relied on GAN-based models [5, 6, 18, 55, 61], al-\nlows for fast object crafting for a specific category. Subse-\nquently, optimization-driven methods distill 2D generative\npriors, typically guided by CLIP scores [27, 90] or Score\nDistillation Sampling (SDS) [44, 57, 71, 74, 80, 97], or use\nmulti-view generation followed by reconstruction [17, 26,\n34, 65, 70, 91]. While conceptually simple, these methods\nare often time-consuming and struggle to maintain consis-\ntent geometry and appearance across views. Later meth-\nods [48, 50, 58, 64] reduce this issue through multi-view\nfine-tuning on large 3D datasets [12, 13], though recon-\nstruction is still needed to obtain 3D representation.\nWith the rapid expansion of high-quality 3D data [12,\n13], recent work shifts toward direct 3D generation using\ndiffusion-transformers that output explicit 3D representa-\ntions. These models either tokenize 3D shapes into diffus-\nable latent sets by encoding unstructured point clouds into\nunordered latent vectors [32, 35, 99, 104, 110], or adopt\nvoxel-based structured latents that can be decoded into ex-\nplicit 3D representations [41, 81, 84, 87].\nIn particular,\nHunyuan3D 2.0 [110] achieves high-quality generation of\n3D assets with rich geometry and appearance, forming a\nstrong foundation for our 4D asset creation.\n4D Reconstruction.\nStructure-from-Motion (SfM) [1,\n62, 66] and Simultaneous Localization and Mapping\n(SLAM) [11, 54, 72] have long been the foundation for 3D\nstructure and camera pose estimation. Although effective,\nthese approaches often struggle with dynamic sequences,\nleading to performance degeneracy.\nTo improve robust-\nness in dynamic scenarios, recent work has demonstrated\nprogress toward general dynamic reconstruction by inte-\ngrating semantic segmentation [22, 98], optical flows [79,\n108], geometric constraints [31, 40, 43, 51, 52, 106], gen-\nerative priors [23, 63]. Alternatively, point-map-based ap-\nproaches [10, 77, 102] such as CUT3R [77] extend the\nfeedforward 3D foundation model DUSt3R [78] to dy-\nnamic scenes by fine-tuning with dynamic datasets. This\nyields feedforward 4D reconstruction and can be further\nscaled to longer sequences [9, 85, 113], novel-view synthe-\nsis [33, 37, 76, 93], and 3D tracking [15, 46, 49, 88]. De-\nspite recent progress, both reconstruction-based paradigms\n2\nremain inherently non-generative: they cannot hallucinate\ngeometry for occluded or unseen regions, often resulting in\nincomplete surfaces or missing structures.\n4D Generation. Similar to 3D object generation, early 4D\napproaches rely on 2D generative priors from multi-view or\nvideo models [107], or introduce intermediate rigging struc-\ntures [3, 14, 21, 47, 68, 69, 92, 103] with skinning-based\nanimation [3, 8, 19, 20, 25, 39, 67, 83]. Score Distillation\nSampling (SDS) [57] is also widely adopted to optimize\n4D representations from multi-view or video diffusion mod-\nels [2, 4, 29, 36, 38, 45, 109, 112], but often introduces ar-\ntifacts. More recent works use generated multi-view videos\nas explicit supervision [28, 42, 82, 89, 94, 95, 101], yet still\nrequire slow per-instance optimization. L4GM [60] enables\nfaster feed-forward 4D regression from generated multi-\nview data, but its performance remains limited by scarce\n4D data and view-inconsistent 2D priors.\nCurrent 4D generation methods primarily adopt a two-\nstage pipeline that extends powerful 3D generative models\ninto the temporal dimension. Notable, V2M4 [7] generates\nmeshes for each frame via pre-trained 3D generative mod-\nels [87, 110], followed by a post-processing step to align\nthese meshes to form a 4D mesh. This generate-then-align\nstrategy is slow and prone to topology drift due to inde-\npendently conditioned frame generations. To improve tem-\nporal consistency, ShapeGen4D [96] fine-tunes a 3D gen-\nerator [35] on 4D data and alleviates the misalignment is-\nsue, but still needs time-consuming post-processing. Paral-\nlel efforts such as GVFD [100] animate Trellis-based [87]\n3D Gaussians via a latent diffusion model that learns global\ntemporal changes, enabling motion generation conditioned\non monocular videos. However, since the training depends\non rendering supervision of the scarcity of 4D data, it yields\nweak geometry and 3D structure.\nIn contrast, we take an orthogonal path by combining the\nstrengths of both 3D generation and reconstruction: we re-\nformulate 4D generation as a combination of 3D shape syn-\nthesis and motion reconstruction. By generating the entire\nobject and taking motion synthesis as an alignment prob-\nlem between surface points and video pixels, our method\nenables efficient motion representation learning and feed-\nforward prediction without requiring the post-processing\nalignment steps.\n3. Motion 3-to-4\nIn this paper, we aim to efficiently craft 4D assets that en-\ncompass complete shape and motion. Our key idea is to\ndecompose the ill-posed 4D generation problem into static\nshape generation and dynamic motion reconstruction, en-\nabling the recovery of complete motion flow and geometry,\nincluding both visible and unseen surfaces.\nTo this end, our method takes a monocular video as input\nand, optionally, an existing mesh asset of the first frame.\nMethod\nSolution\nFF Motion Mesh Retarget FPS\nConsistent4D [29]\nMV Gen. + 3D Rec.\n\u2717\n\u2717\n\u2717\n\u2717\n0.1\nSV4D [89]\nMV Gen. + 3D Rec.\n\u2717\n\u2713\n\u2717\n\u2717\n0.1\nL4GM [60]\nMV Gen. + 3D Rec.\n\u2713\n\u2717\n\u2717\n\u2717\n7.8\nDreamMesh4D [38]\nMV Gen. + 3D Rec.\n\u2717\n\u2713\n\u2713\n\u2717\n0.1\nV2M4 [7]\n3D Gen. + 4D Align\n\u2717\n\u2717\n\u2713\n\u2717\n0.1\nShapeGen4D [96]\n3D Gen. + 4D Align\n\u2717\n\u2717\n\u2713\n\u2717\n0.1\nGVFD [100]\n3D Gen. + Motion Gen. \u2713\n\u2713\n\u2717\n\u2717\n0.8\nMotion 3-to-4\n3D Gen. + Motion Rec.\n\u2713\n\u2713\n\u2713\n\u2713\n6.5\nTable 1. An overview of 4D synthesis methods from monoc-\nular video. \u201cFF\u201d denotes feed-forward. \u201cFPS\u201d is averaged over\n512 frames. To address the ill-pose Video-to-4D problem, early\npipelines generate multi-view images or videos but suffer from\nview inconsistency. Following frame-wise 3D generative mod-\nels avoid this issue yet require time-consuming 4D alignment.\nMotion-generation\u2013based methods animate 3D generation, but\ntheir generalizability is fundamentally constrained by the limited\navailability of 4D training data. We incorporate static generation\nand motion reconstruction to learn local surface-to-pixel corre-\nspondences for efficient novel shapes and complex motions.\nIf such a mesh is unavailable, we generate one from the\ninitial frame using a pretrained 3D generative model [110].\nWe then estimate per-frame 3D motion flow relative to the\nfirst video frame, yielding temporally consistent 4D assets\nthat encapsulate both shape and motion in their entirety, as\nshown in Figure 2.\nOur framework consists of two main components: 1)\nmotion latent learning that encodes the static mesh and\nvideo frames into a compact representation (Section 3.1);\nand 2) motion decoding that regresses per-frame point loca-\ntions from queries sampled on the static mesh (Section 3.2).\n3.1. Motion Latent Learning\nIn the following, we will introduce a simple yet efficient\nrepresentation learning framework for 3D motion.\nGeometric Features. To efficiently capture 3D geometry,\nwe first encode the reference mesh M \u2282{V, F, T \u2208R}\nwith vertices V, faces F, and texture T into a compact la-\ntent representation. The mesh can either be user-provided\nor lifted directly from the video using recent image-to-\n3D generation techniques [87, 110].\nTo encode shape\nand appearance, we uniformly sample N surface points\nX0 = (xi, ni, ci)N\ni=1, where xi \u2208R3 is the 3D coordi-\nnate, ni \u2208R3 is the surface normal, and ci \u2208R3 denotes\nthe RGB color.\nOur shape encoder is inspired by 3DShape2VecSet [99],\nand we compress the sampled points into a compact 1D la-\ntent representation by performing cross attention to aggre-\ngate shape information. Specifically, we employ a learnable\nquery set A \u2208RK\u00d7C of fixed length K, where each query\ntoken is a C-dimensional latent vector. These query tokens\nact as anchors that attend to and gather information from\ntheir neighboring shape samples in X0, producing the shape\nlatent representation.\nZX0 = CrossSelfAttn(A, PointEmb(X0))\n(1)\n3\nMotion Decoder\nLearnable Token\nSampled Points\nShape\nToken\nMotion 3-to-4\nDINO Encoder\nShape\nEncoder\nKV\nQ\nKV\nQ\nInput\nVideo\nImage to 3D\nMesh\nImage\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\nFigure 2. An overview of our Motion 3-to-4 framework for 4D synthesis. At the core of the framework is a motion\u2013latent learning\nmodule consisting of a geometry encoder and a video encoder, which jointly process the input video and sampled points. The resulting\nlatent tokens are decoded into a frame-wise 3D motion flow relative to the first video frame, producing temporally consistent 4D assets.\nwhere PointEmb(X0) : R9 \u2192RC maps the point label into\na high-dimensional positional embedding using an MLP.\nThe aggregated tokens are further refined through a few lay-\ners of self-attention transformer blocks to exchange context.\nThis process embeds the mesh geometry into a low-\ndimensional latent space, yielding a shape latent ZX0 \u2208\nRK\u00d7C that retains the essential geometric and semantic\nstructure required for motion reconstruction.\nModulation with Video Features. Next, we aggregate the\ngeometric token ZX0 with the spatial-temporal video se-\nquence to obtain a motion representation.\nSpecifically,\nwe\ntake\na\nmonocular\nvideo\nV\n\u2208\nRT \u00d7H\u00d7W \u00d73 with T frames as input and extract patch-level\nfeatures using a pretrained DINOv2 [56] encoder. These se-\nmantic features facilitate robust correspondence matching\nand strong generalization across diverse frames, which is\ncrucial for maintaining consistent motion throughout the se-\nquence. Additionally, we inject temporal embeddings into\nthe patch tokens to make them explicitly aware of frame or-\ndering.\nTo reconstruct the motion for each frame, a straightfor-\nward approach is to represent the entire motion as a single\n1D latent sequence and decode frame-wise tokens using po-\nsitional and temporal queries. While this design is efficient\nfor motions with a fixed length, it cannot naturally handle\nmotion sequences of arbitrary duration. To address this lim-\nitation, we propose to append the global shape token ZX0\nto each frame token as the frame-wise motion representa-\ntion, enabling flexible input lengths, as shown in Figure 2.\nFurthermore, we add a reference positional token to explic-\nitly distinguish the reference frame from the others, ensur-\ning that the attention mechanism can properly leverage the\nreference information during propagation.\nWith this design, we derive the final latent representa-\ntion Zt for each frame t, which jointly encodes the shared\ngeometric structure and frame-specific motion information.\nTo distinguish motion features across frames, inspired by\nVGGT [75], we adopt an Alternating-Attention architec-\nture. Let Z(0)\nt\n\u2208R(K+P )\u00d7C denote the initial aggregated\nshape and visual latent representation for frame t. For L\nalternating attention blocks, the updates are defined as:\n(Global update)\n[Z\n(\u2113\u22121\n2 )\n0\n, . . . , Z\n(\u2113\u22121\n2 )\nT \u22121 ]=GlobalAttn(Z(\u2113\u22121)\n0\n, . . . , Z(\u2113\u22121)\nT \u22121 )\n(Frame-wise update)\nZ(\u2113)\nt\n= FrameAttn(Z\n(\u2113\u22121\n2 )\nt\n),\n\u2200t = 0, . . . , T \u22121\n(2)\nAfter L blocks, we obtain the final motion-aware repre-\nsentation for each frame as Zt = Z(L)\nt\n. We take the first K\ntokens as per-frame motion representation.\n3.2. Motion decoding\nWith the learned shape and motion latent representations,\nwe decode them into explicit per-frame 3D point flows.\nRather than predicting the full shape independently per-\n4\ninstance [96] or attribute offsets per-frame [100], we pre-\ndict per-frame motion flows relative to the reference shape,\nwhich preserves surface correspondences and ensures tem-\nporal consistency over long sequences.\nTo achieve this, we adopt a cross-attention decoder: we\ntake a set of points sampled from the reference mesh as\nqueries and predict their positions at each time step. Specif-\nically, we resample M points from the reference mesh\n\u02c6P0 = {(xi, ni, ci)}M\ni=1. These points are embedded using\nthe same PointEmb module employed in shape encoder\n(Section 3.1).\nThe motion decoder then predicts the per-frame posi-\ntions independently:\n\u02c6Xt = MotionDecoder( \u02c6X0, Zt)\n(3)\nwhere Zt is the motion-aware latent for frame t.\nThis design enables motion prediction at any spatial lo-\ncation and arbitrary time step, providing a flexible and fully\nfeed-forward 4D reconstruction framework. The decoded\npoint features are subsequently processed through a shared\nfully connected layer to predict the final 3D coordinates.\n3.3. Training\nWe train Motion 3-to-4 with straightforward direct supervi-\nsion by minimizing the mean squared error (MSE) between\nthe predicted and ground-truth point positions,\nL =\n1\nM \u00d7 T\nM\nX\ni=1\nT\nX\nt=1\n\u2225\u02c6Xi\nt \u2212Xi\nt\u22252\n2\n(4)\nHere, Xt and \u02c6Xt denote the ground-truth and predicted\npoint positions, respectively. To capture continuous motion\nfields, we densely sample points during training. This dense\nsupervision encourages the model to learn fine-grained\nshape correspondences and ensures coherent motion across\nthe entire mesh.\n4. Experiments\n4.1. Implementation Details\nDataset.\nWe curate a high-quality animation dataset by\nfiltering 16,000 objects from a pool of approximately\n50,000 models sourced from Objaverse [13] and Objaverse-\nXL [12]. Our filtering policy excludes objects with sim-\nplistic geometry (e.g., cubes, spheres) and employs Iterative\nClosest Point (ICP) analysis to discard sequences exhibiting\ntrivial motion. To ensure scale consistency, we normalize\neach object to fit within a bounding cube defined by the di-\nmensions [\u22120.5, 0.5]. For the video data, we render each\nasset at a resolution of 256 \u00d7 256 from fixed viewpoints\nwith uniformly sampled azimuth angles. Both the curated\nassets and their renderings will be publicly released.\nTraining Strategy. Our model is trained on 12-frame se-\nquences. For each mesh, we sample N = 4096 points as\nthe shape input, which are encoded into K = 64 shape la-\ntents. These latents are processed through L = 16 trans-\nformer blocks to produce motion latents for decoding, and\nM = 4096 for densely sampled ground-truth points. Train-\ning is conducted with a total batch size of 256 using 8 H100\nGPUs, using a learning rate of 4 \u00d7 10\u22124. The model is\ntrained with 60, 000 steps in roughly 1.5 days.\n4.2. Evaluation\nEvaluation Datasets.\nWe evaluate our method on two\ndatasets. (1) We collect a held-out set of 80 subjects from\nObjaverse, termed Motion-80, featuring objects with rich\ntextures and diverse motion, which contains 64 short se-\nquences and 16 long sequences exceeding 128 frames ren-\ndered from four orthogonal views. (2) The Consistent4D\nbenchmark [29], which includes 7 videos of 32 frames each.\nSince ground-truth meshes are not available for the Con-\nsistent4D dataset, we report rendering-based metrics com-\nputed at four target novel views following the evaluation\nprotocol in Consistent4D.\nBaselines. We conduct comprehensive comparisons with\nstate-of-the-art video-to-4D methods.\nOur baselines in-\nclude feedforward approaches that predict 3D Gaussians,\ni.e., L4GM [60] and GVFD [100]. We also evaluate against\nthe optimization-based method V2M4 [7], which first gen-\nerates a 3D mesh for each frame and then performs temporal\nalignment to obtain a 4D mesh.\nMetrics. For geometric evaluation, we follow the proto-\ncol of Shape2VecSet [99] to compute the Chamfer Distance\n(CD) and F-Score. This involves sampling 50,000 points\nfrom each mesh surface for comparison. However, due to\nthe inherent ambiguity of 4D synthesis from a monocular\nview, the scale and orientation of the generated meshes may\nnot align with the ground truth. To address this, we apply an\nIterative Closest Point (ICP) algorithm for alignment prior\nto evaluation. Specifically, we register the first frame of the\ngenerated sequence to the ground-truth mesh to estimate a\nrigid transformation (rotation, scale, and translation), which\nis then applied to all subsequent frames in the sequence. For\nGaussian-based methods, we extract the centers of all Gaus-\nsians as the surface point set and uniformly sample 50,000\npoints from it to ensure a fair comparison.\nFor appearance evaluation, we render the textured mesh\nfrom target viewpoints. For the Motion-80 evaluation set,\nwe use the front view as input and the remaining 3 views\nfor eval. For the Consistent4D dataset, we follow its orig-\ninal camera settings. We adopt LPIPS [105], CLIP [59],\nFVD [73], and DreamSim [16] to assess overall quality and\ntemporal consistency, which are widely used in video-to-4D\ntasks. We adopt the V2M4 [7] evaluation protocol and fur-\n5\nShort Sequence\nLong Sequence\nGeometry\nAppearance\nGeometry\nAppearance\nMethod\nCD \u2193\nF-Score \u2191\nLPIPS \u2193\nCLIP \u2191\nFVD \u2193\nDreamSim \u2193\nCD \u2193\nF-Score \u2191\nLPIPS \u2193\nCLIP \u2191\nFVD \u2193\nDreamSim \u2193\nL4GM [60]\n0.3561\n0.1269\n0.1487\n0.8182\n1120.67\n0.1941\n0.3648\n0.0997\n0.1467\n0.7988\n1070.72\n0.2175\nGVFD [100]\n0.1970\n0.2608\n0.1664\n0.7933\n1414.21\n0.2147\nOOM\nOOM\nOOM\nOOM\nOOM\nOOM\nV2M4 [7]\n0.3437\n0.2318\n0.1769\n0.8080\n1516.47\n0.1974\n0.3719\n0.1652\n0.2031\n0.7872\n1534.16\n0.2292\nOurs\n0.1113\n0.3171\n0.1495\n0.8428\n1175.89\n0.1682\n0.1495\n0.2347\n0.1688\n0.8352\n1264.36\n0.1967\nOurs w/m\n0.0437\n0.6774\n0.0921\n0.9251\n497.43\n0.0614\n0.0929\n0.4322\n0.1057\n0.9224\n673.03\n0.0781\nTable 2. Quantitative evaluation on our Motion-80 set. Results are reported for both short and long sequences. \u201cOurs w/m\u201d denotes\nour method initialized with the ground-truth static mesh from the first frame. Thanks to the disentangled mesh representation and scene-\nflow\u2013based motion modeling, our approach capable of transforming artist-created static 3D meshes into fully dynamic 4D sequences.\nMethod\nLPIPS \u2193\nCLIP \u2191\nFVD \u2193\nDreamSim \u2193\nL4GM [60]\n0.1468\n0.8457\n1207.79\n0.1830\nGVFD [100]\n0.1789\n0.8278\n1340.78\n0.2009\nV2M4 [7]\n0.1611\n0.8482\n1471.58\n0.1832\nOurs\n0.1455\n0.8609\n1260.06\n0.1691\nTable 3. Quantitative evaluation on Consist4D benchmark. We\nevaluate rendering performance across 7 test cases, each contain-\ning 32 frames, rendered from 4 target novel views.\nther extend it by assessing performance from novel views\ninstead of the input viewpoint. This provides a more com-\nprehensive evaluation, as it avoids cases where a method\nappears satisfactory from the input view but exhibits arti-\nfacts from other views.\nQuantitative Evaluation. On geometry. As demonstrated\nin Table 2 and Figure 3, our method achieves superior\n4D geometric accuracy compared to all baselines, as in-\ndicated by consistent gain across both CD and F-Score\nmetrics. Among Gaussian-based methods, L4GM fails to\nrecover accurate geometry. Because the 3DGS represen-\ntation is tailored for novel-view synthesis, the predicted\nGaussians are not constrained to lie exactly on the sur-\nface [24], and the resulting point clouds exhibit floating\nartifacts, as shown in Figure 3. GVFD can generate rea-\nsonable surface point clouds. However, it struggles to ac-\ncurately reconstruct the motion of these points over time,\nleading to degraded overall performance. The optimization-\nbased method V2M4 refines the generated 3D mesh from\neach frame and reconstructs a plausible surface, thereby\noutperforming Gaussian-based approaches. Nonetheless, it\nstill suffers from temporal inconsistency, leading to flicker-\ning and physically implausible spatial motion, resulting in\nlow CD scores. In contrast, we animate the generated ex-\nplicit 3D mesh with the reconstructed scene flow, producing\ntemporally coherent motion through alignment between the\nmesh and video observations and thus achieving high geo-\nmetric accuracy. This advantage is particularly evident in\n\u201cOurs w/m\u201d, which drives a ground-truth static mesh from\nthe first frame using our reconstructed motion and signif-\nicantly outperforms all baselines, highlighting the fidelity\nGVFD\nV2M4\nOurs\nL4GM\nFigure 3. Geometric comparison on the Consistent4D bench-\nmark [29]. Through spatially consistent motion reconstruction,\nwe obtain plausible and high-quality 3D geometry.\nof our motion reconstruction. Notably, Motion 3-to-4 is\nthe only approach capable of converting artist-created static\n3D meshes into dynamic 4D sequences, owing to its disen-\ntangled mesh representation and scene-flow\u2013based motion\nmodeling.\nOn appearance.\nAs shown in Table 2 and Table 3,\nour approach achieves better 3D content fidelity and con-\nsistency, quantitatively outperforms baselines in CLIP and\nDreamSim metrics. Note that L4GM is trained and evalu-\nated on the orthogonal view, thus its rendering is biased to\nthe evaluation protocol and may have more advantages than\nour method on the specific rendering perspective. How-\never, when viewed from non-orthogonal novel viewpoints,\nL4GM exhibits noticeable ghosting artifacts, whereas our\nmethod continues to produce plausible and stable results.\nWe invite the reviewer to our supplemental material for\nmore results.\nQualitative Evaluation.\nFigure 4 further illustrates the\nbenefits of our reformulation.\nL4GM suffers from error\naccumulation during multi-view generation, resulting in\nghosting artifacts when viewed from angles different from\nthe input views. GVFD generates jittery, temporally incon-\nsistent motions due to limitations in its VAE-based motion\nmodeling that relies on large datasets to learn motion latent\ndistribution, leading to weak generalization and discontinu-\nous appearance in the Gaussian during movement. V2M4,\n6\nGT\nGVFD\nV2M4\nOurs\nOurs w/m\nL4GM\nFigure 4. Qualitative Comparisons. We compare our method with strong baselines including GVFD [100], L4GM [60], and V2M4 [7]\non our proposed Motion-80 benchmark. For fair evaluation, we render the generated 4D assets from all methods into an orthogonal novel\nview. Our approach produces more temporally coherent and structurally consistent motion. We invite reviewers to consult the supplemental\nmaterial for animation visualization.\nrelying on per-frame optimization, generates plausible re-\nsults from the input view, but suffers from spatial discon-\ntinuities when observed from other viewpoints, failing to\ncapture true motion. In contrast, our method combines a\nstrong pretrained 3D generator with feed-forward, general-\nizable motion reconstruction. This design preserves both\nspatial and temporal coherence, produces smooth, physi-\ncally plausible motion, and achieves superior visual fidelity.\n4.3. More Results\nWild4D. We show a few in-the-wild testing samples Fig-\nure 5.\nTo handle out-of-domain inputs, we first apply\nBiRefNet [111] to automatically remove background re-\ngions on a per-frame basis. We then generate an initial 3D\nshape using the first video frame with Hunyuan2.0 [110].\nThanks to the strong visual features from the DINO encoder\nand the robust geometry encoder, our method generalizes\nwell to in-the-wild video inputs, including both real-world\nfootage and generated animation sequences.\nMotion Transfer.\nAlthough our model is trained using\npaired videos and corresponding 3D subjects, we observe\nthat it inherently generalizes to transferring the motion from\nan input video to a 3D object with different shape and ap-\npearance. As shown in Figure 6, we feed the dragon video\ninto the DINO encoder and use the chicken and robot-\ndragon meshes as inputs to the geometry encoder. Remark-\nably, our method successfully transfers the neck, body and\nleg motions from the video to these new target objects. For\nshapes with geometric differences, our method is still able\nto successfully transfer the leg movement from the source\nto the target model.\n7\nFigure 5. In-the-Wild Video-to-4D Synthesis. Our method generalizes to diverse in-the-wild inputs, including real-world videos (top\nrow) and generated animations (bottom row). By formulating motion reconstruction as surface-to-pixel alignment, we achieve robust local\ncorrespondence reasoning across varied shapes and motion patterns.\nSource\nTarget\nFigure 6. Motion Transfer Example. By disentangling 4D syn-\nthesis into 3D mesh generation and motion reconstruction, our\nframework can animate static articulated objects with motion re-\ntargeted from videos of different sources.\n5. Conclusion\nWe presented a 4D synthesis pipeline that decomposes\nthe inherently ill-posed 4D generation problem into two\ntractable components: 3D shape generation and motion re-\nconstruction. This decomposition offers several key advan-\ntages. Efficiency: Off-the-shelf 3D generative models can\nbe directly reused for high-quality shape synthesis, while\nthe motion branch remains lightweight, substantially re-\nducing the scale requirements for 4D training data. Gen-\neralizability: By formulating motion reconstruction as an\nalignment task between surface points and video pixels,\nour method performs robust local correspondence reason-\ning, enabling strong generalization to both synthesized and\nA\nB\nFigure 7. Failure cases. (A) Vertex sticking in challenging cases.\n(B) Initial mesh topology not able to adapt to later motion.\nreal-world shapes as well as diverse motion patterns. Flexi-\nbility: Our framework also supports lifting static articulated\nobjects into dynamic 4D driven by video conditions, includ-\ning motion retargeting from entirely different sources.\nLimitations. Our method exhibits several limitations. First,\nthe geometry encoder operates on a dense point cloud with-\nout explicitly modeling the mesh topology.\nAs a result,\nwhen different parts of the object are not clearly separated in\nthe reference mesh, the model may produce vertex sticking\nartifacts, as illustrated in Figure 7(A). Second, when recon-\nstructing motion from monocular video, our pipeline relies\non the mesh generated from the first frame as reference ge-\nometry. This makes it difficult to accommodate topology\nchanges that occur in later frames, leading to failure cases,\nas shown in Figure 7(B).\n8\nAcknowledgments\nWe would like to thank the members of Inception3D Lab\nfor their helpful discussions, and Isabella Liu for sharing\nthe Blender scripts used in rendering the teaser. This work\nis done with the sponsorship of TeleAI.\nReferences\n[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian\nSimon, Brian Curless, Steven M Seitz, and Richard\nSzeliski. Building rome in a day. ACM Communications,\n2011. 2\n[2] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gor-\ndon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey\nTulyakov, Jeong Joon Park, Andrea Tagliasacchi, and\nDavid B Lindell. 4d-fy: Text-to-4d generation using hy-\nbrid score distillation sampling. In CVPR, 2024. 1, 3\n[3] Ilya Baran and Jovan Popovi\u00b4c. Automatic rigging and an-\nimation of 3d characters. ACM Trans. on Graphics, 2007.\n3\n[4] Yarin Bekor, Gal Michael Harari, Or Perel, and Or Litany.\nGaussian see, gaussian do: Semantic 3d motion transfer\nfrom multiview video. In SIGGRAPH Asia, pages 1\u201310,\n2025. 3\n[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas J Guibas, Jonathan Tremblay, Sameh Khamis,\net al. Efficient geometry-aware 3d generative adversarial\nnetworks. In CVPR, 2022. 2\n[6] Anpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su,\nand Jingyi Yu. Sofgan: A portrait image generator with\ndynamic styling. ACM Trans. on Graphics, 2022. 2\n[7] Jianqi Chen, Biao Zhang, Xiangjun Tang, and Peter Wonka.\nV2m4: 4d mesh animation reconstruction from a single\nmonocular video. In ICCV, 2025. 2, 3, 5, 6, 7\n[8] Ling-Hao Chen, Yuhong Zhang, Zixin Yin, Zhiyang\nDou, Xin Chen, Jingbo Wang, Taku Komura, and Lei\nZhang.\nMotion2motion: Cross-topology motion transfer\nwith sparse correspondence. In SIGGRAPH Asia, 2025. 3\n[9] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and\nAnpei Chen. Ttt3r: 3d reconstruction as test-time training.\narXiv preprint arXiv:2509.26645, 2025. 2\n[10] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and\nAnpei Chen. Easi3r: Estimating disentangled motion from\ndust3r without training. In ICCV, 2025. 2\n[11] Andrew J Davison, Ian D Reid, Nicholas D Molton, and\nOlivier Stasse. Monoslam: Real-time single camera slam.\nPAMI, 2007. 2\n[12] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.\nObjaverse-xl: A universe of 10m+ 3d objects. NeurIPS,\n2023. 2, 5\n[13] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In CVPR, 2023. 2, 5\n[14] Yufan Deng, Yuhao Zhang, Chen Geng, Shangzhe Wu, and\nJiajun Wu. Anymate: A dataset and baselines for learning\n3d object rigging. In SIGGRAPH, 2025. 3\n[15] Haiwen Feng, Junyi Zhang, Qianqian Wang, Yufei Ye,\nPengcheng Yu, Michael J Black, Trevor Darrell, and\nAngjoo Kanazawa. St4rtrack: Simultaneous 4d reconstruc-\ntion and tracking in the world. In ICCV, 2025. 2\n[16] Stephanie Fu, Netanel Y Tamir, Shobhita Sundaram, Lucy\nChai, Richard Zhang, Tali Dekel, and Phillip Isola. Dream-\nsim: learning new dimensions of human visual similarity\nusing synthetic data. NeurIPS, 2023. 5\n[17] Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and\nBernhard Sch\u00a8olkopf.\nGraphdreamer: Compositional 3d\nscene synthesis from scene graphs. In CVPR, 2024. 2\n[18] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. NeurIPS, 2022. 2\n[19] Zhiyang Guo, Jinxu Xiang, Kai Ma, Wengang Zhou,\nHouqiang Li, and Ran Zhang."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis\"\n\n#### Main Contributions:\n1. **Feedforward Framework**: The paper introduces Motion 3-to-4, a feed-forward framework that synthesizes high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh.\n2. **Decomposition of 4D Generation**: It decomposes 4D generation into static 3D shape generation and motion reconstruction, addressing the challenges posed by limited training data and ambiguous geometry/motion recovery from a single viewpoint.\n3. **Canonical Reference Mesh**: The model uses a canonical reference mesh to learn a compact motion latent representation and predicts per-frame vertex trajectories for complete geometry and temporally coherent motion recovery.\n\n#### Methodology:\n1. **Static Shape Generation**: Using the provided or generated static mesh as a stable reference, the system encodes the shape into a compact latent representation.\n2. **Dynamic Motion Reconstruction**: The model then decodes this latent to predict per-frame vertex trajectories relative to the canonical state, enabling accurate recovery of complete geometry and temporally coherent motion.\n\n#### Key Findings:\n1. **Superior Fidelity**: Evaluations on both standard benchmarks and a new dataset with ground-truth geometry show that Motion 3-to-4 delivers superior fidelity compared to prior work.\n2. **Robustness**: The framework is scalable and robust, supporting varying sequence lengths and handling different input meshes.\n\n#### Conclusions:\n1. **Novel Approach**: By reformulating 4D generation as a combination of 3D shape synthesis and motion reconstruction, the paper addresses the inherent challenges of 4D synthesis.\n2. **Generalization**: The method achieves strong generalization despite limited high-quality 4D training data by leveraging static mesh references.\n3. **Flexibility**: It supports dynamic 4D objects driven by video conditions, including motion retargeting from different sources.\n\n### Summary Table\n| Method | Short Sequence CD\u2193 | Short Sequence F-Score\u2191 | Long Sequence CD\u2193 | Long Sequence F-Score\u2191 |\n|--------|------------------|-----------------------|------------------|-----------------------|\n| L4GM    | 0.3561           | 0.1269                 | 0.3648           | 0.0997                 |\n| GVFD    | 0.1970           | 0.2608                 | 0.2147           | OOM                    |\n| V2M4    | 0.3437           | 0.2318                 | 0.1974           | OOM                    |\n| Ours    | 0.1113           | 0.3171                 | 0.1495           | 0.2347                 |\n| Ours w/m| 0.0437           | 0.6774                 | 0.0921           | 0.4322                 |\n\n### Additional Insights:\n- **Qualitative Evaluation**: The method produces more temporally coherent and structurally consistent motion compared to baselines.\n- **Handling Out-of-Domain Inputs**: It effectively handles diverse in-the-wild inputs, including real-world footage and generated animations.\n\n### Limitations:\n1. **Topology Issues**: The geometry encoder operates on a dense point cloud without explicitly modeling mesh topology.\n2. **Topology Changes**: Reconstruction from monocular video relies on the initial frame's mesh as reference, leading to difficulties accommodating later changes.\n\nThis paper represents an important step in advancing 4D synthesis capabilities by providing a robust and generalizable framework for generating dynamic 3D objects from single-viewpoint inputs."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Knowledge Graph-Assisted LLM Post-Training for\nEnhanced Legal Reasoning\nDezhao Song1, Guglielmo Bonifazi1, Frank Schilder1,\u2217, Jonathan Richard Schwarz1,2,\u2217\n1Thomson Reuters Foundational Research, 2Imperial College London\n\u2217Equal contribution\nCorrespondence: {first.last}@thomsonreuters.com\nAbstract\nLLM post-training has primarily relied on large text corpora and human feedback, without capturing\nthe structure of domain knowledge. This has caused models to struggle dealing with complex reasoning\ntasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding\nof the relations between various legal concepts, a key component missing in current LLM post-training.\nIn this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs\u2019 reasoning\ncapability in Legal that is generalizable to other high-stakes domains. We model key legal concepts\nby following the IRAC (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with\n12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised\nFine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA)\nLLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models\nobtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In\nparticular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines\nand a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs\u2019 legal\nreasoning capability.\n1\nIntroduction\nLarge language models (LLMs), such as GPT OpenAI [2023], Claude Anthropic [2023] and Llama Touvron\net al. [2023], have shown remarkable performance on a wide range of natural language tasks. However, when\nthe target applications are in highly specialized domains, such as Medicine, Finance, Law, etc., general-purpose\nLLMs often struggle with specialized terminologies and domain-specific reasoning needs. Consequently,\nresearchers have begun to build domain-specific variants by specifically pre-training and fine-tuning on corpora\nfrom the corresponding target fields Singhal et al. [2023, 2025], Xie et al. [2024], Colombo et al. [2024].\nWhile training and continuous fine-tuning on raw text capture the generation of domain language, they\nare insufficient to encode the structure of domain knowledge, especially on domain-specific reasoning tasks.\nRecent reasoning models work best for areas, such as mathematics and programming, where verifiable rewards\nenable a straightforward approach to reinforcement learning; however, areas like Medicine or Law require\nalternative forms of reasoning, such as argumentative reasoning or inference in the presence of high and\ninherent uncertainty, by using concepts of the corresponding domains, such as symptoms, events, and legal\nrules.\nMore importantly, these areas require deep understanding of the relations between abstract concepts. Financial\nmarket-event analysis adopts a cause-effect-impact-conclusion chain; clinical reasoning follows a symptom-\ndiagnosis-treatment-outcome flow; in Legal, judicial reasoning employs the fact-rule-conclusion process.\nEncoding these reasoning patterns in a structured form offers a principled way to augment LLMs\u2019 inductive\nbias and to reduce hallucinations or misattributions.\nIn this paper, we focus on the legal domain where even when models are trained on vast corpora of case\nlaw, statutes, regulations, and secondary literature, they may produce misattributions, hallucinations, and\nlogically incoherent arguments Magesh et al. [2025], and specific tasks are often better served by smaller\nThomson Reuters Foundational Research, 5 Canada Square, London\narXiv:2601.13806v1  [cs.CL]  20 Jan 2026\nKnowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning\nspecialized models Chalkidis [2023], Jayakumar et al. [2023], Shu et al. [2024]. The root of this problem lies\nnot in the quantity of text for LLM training but in the structure of legal knowledge: legal prose is dense,\nhighly contextual, and governed by rules that change over time, calling for a clearer textual representation\nthat makes the element of the reasoning process more accessible.\nIn order to address this gap, we start with the formal modeling of legal concepts, which then leads to generating\ntextual training data for LLM post-training. Our approach explicitly encodes the reasoning patterns found in\njudicial opinions in order to facilitate LLM training. The cornerstone of legal reasoning is the IRAC framework,\nIssue, Rule, Analysis and Conclusion, which is taught in law schools and encapsulates the typical flow of\nlegal argumentation Brand and White [1976]. By constructing an IRAC Knowledge Graph (IRAC KG) that\ncaptures the relationships between issues, governing rules, factual applications, and conclusions, we aim to\nprovide a structured substrate from which an LLM can learn to reason more faithfully.\nOur work equips LLMs with a structured substrate for legal reasoning, but with two distinct features. First,\ninstead of focusing on creating legal KGs with specific entities, such as Person, Organization, etc., our goal is\nto model the key legal concepts by following the IRAC framework. Second, rather than treating the graph\nmerely as retrieval scaffolding, we post-train LLMs directly on graph-derived supervision signals, and show\nthat this yields measurable gains on diverse legal tasks. Our contributions are threefold:\n1. IRAC KG construction. We extract IRAC components from a large corpus of 12K legal case opinions,\nand encode them as nodes and typed edges in a graph, preserving the causal dependencies that underlie\nlegal reasoning.\n2. IRAC KG-based LLM post-training. By utilizing the KG as a knowledge source, we post-train mid to\nlarge LLMs (Qwen3-30B-A3B-Instruct-2507, Llama-3_3-Nemotron-Super-49B-v1_5 and Llama-3.1-70B-\nInstruct) via both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) Rafailov\net al. [2023] in order to improve their ability to generate coherent legal arguments.\n3. Extensive evaluation. We assess the trained models on five legal benchmarks: LexGLUE Chalkidis\n[2023], LegalBench Guha et al. [2023], COLIEE Rabelo et al. [2022], SuperGPQA (Law) Team et al.\n[2025] and \u03b4-Stance Gupta et al. [2025], demonstrating measurable gains over baseline and SOTA models.\n2\nRelated Work\n2.1\nLLM-based KG Construction\nKG creation pipelines typically consist of two steps: Named Entity Recognition (NER) and Relation Extraction\n(RE). Prompting-based strategies have proven to be effective, addressing both tasks jointly Edge et al. [2024],\nGuo et al. [2024] or separately Jimenez Gutierrez et al. [2024], Guti\u00e9rrez et al. [2025]. Compared to traditional\nmethods, LLMs are also capable of extracting additional metadata, such as verbose entity descriptions or\nnumerical scores measuring relation strength Guo et al. [2024].\nFocusing on KG quality, Zhang and Soh [2024] defined a three-stage solution to ground LLMs into existing\nKGs, or to canonicalize one derived via open extraction. Han et al. [2024b] trained small models for output\nquality verification and iteratively improving the quality of the original KG, while Zhu et al. [2024] developed\na multi-agent framework to guide this refinement process by integrating external knowledge.\n2.2\nGraph-based LLM Reasoning\nResearchers have utilized LLMs and graphs to address complex reasoning scenarios Pan et al. [2024]. GraphRAG\nCui et al. [2023] organizes text corpora as a KG, and reasons over community descriptions to answer queries.\nThis enables LLMs to handle questions that require a general understanding of the corpus and inspired\nfollow-up methods to explore structured representations to improve Retrieval Augmented Generation (RAG)\npipelines Peng et al. [2024], Han et al. [2024a].\nA recurring theme in several of these works is to present the retrieved information to LLMs in structured\nformats, e.g., triples or relational paths, which has improved reasoning fidelity and quality Ma et al. [2025],\nSun et al. [2024], Liu et al. [2024], Wen et al. [2024]. Rather than using graphs for retrieving structured\n2\nKnowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning\ninformation, Cheng et al. [2024] reformulate the input query as a graph, and propose a multi-stage prompting\npipeline to reason over the input graph for query answering. The authors claimed that the method reflects\nhow humans often approach complex problems.\nIn addition to fostering reasoning, graphs were also used to model the reasoning process itself.\nBoth\nTree-of-Thoughts Yao et al. [2023] and Graph-of-Thoughts Besta et al. [2024] improved over the classical\nChain-of-Thought idea that lacks the capability of expressing complex thought processes, such as backtracking\non unpromising steps or combining ideas from different branches.\n2.3\nKG-LLM Synergy in the Legal Domain\nTang et al. [2024c] represented legal cases and charges as nodes, and connected them using existing metadata,\nsuch as case-case citations and case-charge relationships. Dhani et al. [2021] developed a more complex\npipeline to extract entities and relations from Indian cases, judgments and laws using traditional NER and\nRE techniques for the purpose of finding similar cases.\nMore recently, Chen et al. [2024] used an LLM to extract triples from Chinese cases and criminal law articles\nby following a pre-defined schema. Similar to our approach, Tang et al. [2024b,a] relied on an LLM to represent\ncase legal issues and facts as a KG (though without considering precedents). This offered an initial, though\npartial, representation of a case via the IRAC framework.\nWhile being effective in solving important tasks, Hannah et al. [2025] showed that serious legal implications\nmight arise from LLM responses, and proposed a KG-RAG solution to address potential legal concerns.\nSimilarly, Colombo [2024] combined KG and RAG to develop an Italian legislative platform to support law\nanalysis, drafting, landscape identification and complexity monitoring. Finally, ChatLaw Cui et al. [2023]\nis a multi-agent system for legal consulting, and utilizes KGs in intermediate steps to assist answering user\nrequests.\nInstead of extracting specific entities (e.g., Person, Location, etc.), our work builds KGs that model key legal\nconcepts (such as Facts and Rules). Also, different from existing methods that combine LLMs with graphs,\nour novelty lies in the use of graph structure for domain-specific LLM post-training. Finally, while previous\napproaches studied partial modeling of legal reasoning, our IRAC KG offers a more comprehensive coverage\nof the important legal reasoning components.\n3\nSystem Overview and KG Design\n3.1\nOverview\nFigure 1 demonstrates an overview of our entire knowledge graph-assisted LLM post-training system. The\nfirst step is to extract the key concepts from legal cases (from U.S. jurisdictions) by utilizing an LLM and a\nprompt (Appendix E.1) that includes the KG schema (Section 3.2). Then, we produce training data for both\nSupervised Fine-Tuning (SFT) and Preference Training. During data generation, in addition to prompts, we\nalso employed an LLM for explanation generation (Section 4.1) and as an LLM Judge (Section 4.2).\n3.2\nKnowledge Graph Design\nDifferent from prior works that extract specific entity names (e.g., Person, Company, Location, etc.), our\nKG focuses on extracting the key legal concepts (such as Facts and Legal Issues) in order to facilitate legal\nreasoning. Figure 2 shows the design of our legal KG, a key component of our proposed system, by following\nthe IRAC framework.\nIRAC is commonly adopted for conducting legal analysis Burton [2017]. Even though there are variations,\nsuch as ILAC (Issue, Law, Analysis, Conclusion), IRAACP (Issue, Rule, Apply, Apply, Conclusion, Policy),\netc., the core idea of legal reasoning centers around the same main concepts, i.e., Issue, Rule, Analysis and\nConclusion. We consulted four legal experts on the design in order to capture the core of the legal reasoning\nprocess. Our IRAC KG includes the following components:\n3\nKnowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning\nPreference\nTraining\nLegal Cases\nKG\nSFT Data\nSFT Model\nPreference Data\nPreference Model\nLLM+Prompt\nLLM+Prompt\nSFT\nLLM+Prompt\nFigure 1 System overview.\nFacts\nLegal\nIssues\nRules\nConclusions\nPrecedents\narise from\nderived from\nlead to\napply\naddress\nFigure 2 Schema of our IRAC legal knowledge graph.\n\u2022 Legal Issues (I): This represents legal issues that the different parties (e.g., plaintiffs, defendants, judges,\nattorneys, etc.) are addressing in a legal case. Facts are things that actually happened, and the reason\nwe have a legal case is due to the facts. Therefore, we have Legal Issues arise from Facts in our schema.\n\u2022 Rules (R): This refers to legal rules. In the legal domain, prior cases (also known as precedents) that are\nsufficiently representative become rules that courts will reference to decide on future cases. Therefore,\nRules are derived from Precedents.\n\u2022 Analysis or Application (A): This denotes the legal reasoning process. When deciding on a legal case,\ncourts will apply Rules to Facts in order to address the Legal Issues.\n\u2022 Conclusions (C): The analysis that courts carry out will eventually lead to certain Conclusions. These\nare courts\u2019 rulings on the case.\n4\nTraining Data Generation\nOur goal is to study whether such legal reasoning KGs would benefit LLMs on diverse tasks by conducting\nboth SFT and Preference Optimization. In this section, we present how we utilized our IRAC KG to produce\nthe corresponding training data.\n4\nKnowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning\nSFT Training Input\n<legal_query>\n<instruction>Given the case facts, identify the relevant legal issue regarding municipal liability and explain the applicable legal rules\nfor sewage system failures.</instruction>\n<case_facts>\n<fact>City installed sewage pump station that experienced four backups over four months in 2018</fact>\n</case_facts>\n<output_format>Provide your response in pretty print XML. Use top tag legal_analysis, and the following sub-tags: legal_issue: [The\nlegal issue here]; rules: [A list of rules]; explanation: [Brief explanation of the legal issue by relating it to the case facts, and also on why\nthe rules are applicable to the given case facts and the legal issue].</output_format>\n</legal_query>\nSFT Training Target\n<legal_analysis>\n<legal_issue>Whether the City is entitled to statutory immunity from the sewage backup claim</legal_issue>\n<rules>\n<rule>Cities are immune from liability except to extent covered by liability insurance</rule>\n</rules>\n<explanation>The legal issue of statutory immunity arises directly from the City's involvement in the sewage backups, as municipalities often\nhave special protections from liability. The rule about city immunity is applicable because this case involves a city-installed and operated\nsewage system that caused damages, making it necessary to determine whether the city's general immunity applies or if there's insurance\ncoverage that would create an exception to that immunity.</explanation>\n</legal_analysis>\nGiven case facts, the task is to derive the legal issue, locate applicable legal rules,\nand justify why the rules are applicable.\n\u201ccase_facts\u201d from the IRAC KG.\nThis is the legal issue that arises from the above fact in the IRAC KG.\nRules from IRAC KG that\nwere applied to the facts\nabove in order to\naddress the legal issue.\nWe used an LLM to generate this explanation to provide more legal knowledge for model training.\nFigure 3 An example of the Rules SFT dataset.\n4.1\nThe Rules SFT Dataset\nFigure 3 illustrates a concrete example of our SFT dataset. The core idea is to simulate how courts identify\nrule(s) that are applicable to addressing a specific legal issue. In SFT input, the main component is case_facts,\nwhich is the foundation to have a legal case. Then, in the training target, we teach a model how to derive the\nlegal issue that arises from the fact(s), and further identify the applicable rule(s). In addition, we also provide\nan explanation on why the rule(s) are applicable to the legal issue.\nAlgorithm 1 depicts this SFT data generation process. At Line 2, Facts consists of all facts related to\nAlgorithm 1 GenSFT(KG, li, LLM), KG represents the IRAC KG; li denotes a legal issue in KG; a large\nlanguage model LLM produces an explanation on the applicability of certain rule(s) to li.\n1: Rules \u2190\u2205\n2: Facts \u2190GetRelatedFacts(KG, li)\n3: for all f \u2208Facts do\n4:\nRf \u2190GetRsV iaApply(KG, f)\n5:\nRules \u2190Rules \u222aRf\n6: end for\n7: OtherRs \u2190GetRsV iaAddress(KG, li)\n8: Rules \u2190Rules \u222aOtherRs\n9: Exp \u2190Explain(LLM, li, Rules, Facts)\n10: return Facts, Rules, Exp\nlegal issue li, via the arise from relation in Figure 2. Next, from Line 3 to 8, we find the set of applicable\nrules to li both directly and indirectly. On one hand, at Line 4, for each fact f related to li, Function\nGetRsV iaApply(KG, f) finds the rules that were applied to f by following the path: Rules\u2192apply \u2192Facts.\nOn the other hand, at Line 7, by following the Rules \u2192address \u2192Legal Issues chain in Figure 2, Function\nGetRsV iaAddress(KG, li) gathers additional rules identified by the court that could be used to address li\nbut may not actually be applied to any facts, e.g., rules from similar precedents. As demonstrated in Figure 3,\nthe retrieved facts become SFT input, while the rules and the legal issue itself are part of the training target.\nIn addition, at Line 9, we prompt (Appendix E.2) an LLM to produce a concise explanation on why the\nrules are applicable to the legal issue li, hoping to provide more legal knowledge to downstream training\ntasks; this explanation constitutes another section of the training target. We conducted a quality review of 15\n5\nKnowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning\nUser Message\n<legal_query>\n<instruction>Review the factual circumstances and legal problem at hand, and pinpoint the relevant legal standards needed to tackle\nthe issue.</instruction>\n<case_facts>\n<fact>Arbitration Panel awarded [Person] $[Amount] plus interest based on 30% of land value</fact>\n</case_facts>\n<legal_issue>Whether defendant waived right to appeal by paying judgment.</legal_issue>\n<output_format>Provide your response in pretty print XML: Use rules as the top-level element; underneath it, list each rule using the\nelement rule.</output_format>\n</legal_query>\nChosen Assistant Message\n<rules>\n<rule>\nPayment of adverse judgment is involuntary and\ndoes not waive right to appeal.\n</rule>\n</rules>\nGiven case facts and a legal issue, the task is to determine applicable legal rules.\nSensitive information is redacted.\nRule(s) in the IRAC KG that are determined\nto be applicable to the given legal issue.\nOther rules in the IRAC KG.\nRejected Assistant Message\n<rules>\n<rule>\n43 USC \u00a71621(a) prohibits contingency fees based on value of ANCSA lands.\n</rule>\n<rule>\nJudgment entered in violation of statute is voidable not void under Rule 60(b)(4).\n</rule>\n</rules>\nFigure 4 An example of the Rules preference dataset.\nSFT examples with three subject matter experts (SMEs), where each example was labeled by a single SME.\nAmong these, 11, 4 and 0 examples were graded as Correct, Correct with minor issues and Wrong respectively,\nverifying the quality of our SFT dataset.\nEntity\nClaude Sonnet 3.5 KG\nLlama-3.1-405B-Instruct KG\nGood\nAcceptable\nPoor\nM\nGood\nAcceptable\nPoor\nM\nFact\n68%\n5%\n2%\n26%\n50%\n9%\n6%\n35%\nLegal Issue\n63%\n0%\n0%\n37%\n75%\n5%\n10%\n10%\nRule\n69%\n0%\n4%\n27%\n29%\n0%\n33%\n38%\nConclusion\n68%\n0%\n0%\n32%\n60%\n0%\n10%\n30%\nRelation\nPass\nFail\nPass\nFail\n92%\n8%\n59%\n41%\nTable 1\nIRAC KG quality review. Fact, Legal Issue, Rule and Conclusion correspond to the entity types in our IRAC\nKG in Figure 2; we report a single number for all relations. For entities, our SMEs labeled them as Good (high-quality\nwith potentially minor inaccuracies), Acceptable (with noticeable issues but still useful) and Poor (with significant\nerrors or completely incorrect); our SMEs also checked whether the KG missed any entities (M) by reading the entire\ncase. As for relations, our review only examines whether they are correct or not. Note that if any of the entities\ninvolved in a relation is judged as Poor, then this relation will be labeled as Fail. We determined these quality review\ncriteria together with our domain experts.\n4.2\nThe Rules Preference Dataset\nIn addition to SFT, we also performed Preference Optimization in this work. Similar to the SFT dataset, we\nproduced our preference data by utilizing the IRAC KG with a focus on teaching the models to correctly\nidentify the proper legal rules to cope with a given legal issue. Figure 4 shows a concrete example of this\ndataset. Different from the above SFT data, our preference training input (i.e., User Message) includes both\ncase_facts and legal_issue, with chosen and rejected legal rules as the training target. Our hypothesis is that\nby teaching models how to select the appropriate rules for different legal matters, they would be able to gain\nmore knowledge on how courts reason in a variety of legal situations.\nWe formally present the data generation process in Algorithm 2. For preference training, we need to provide\nboth the chosen and the rejected rules. Similar to SFT data generation, at Line 1, we first obtain facts\n(Facts) related to the legal issue li. Then, from Line 2 to 8, we find the chosen rules (Chosen_Rs) that are\neither directly (via the address relation) or indirectly (via the combination of the apply and the arise from\n6\nKnowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning\nAlgorithm 2 GenPref(KG, li, LJ), KG denotes the IRAC KG; li is a legal issue in KG; an LLM Judge LJ\ndetermines a rule\u2019s applicability to li.\n1: Facts \u2190GetRelatedFacts(KG, li)\n2: Chosen_Rs \u2190\u2205\n3: for all f \u2208Facts do\n4:\nRf \u2190GetRsV iaApply(KG, f)\n5:\nChosen_Rs \u2190Chosen_Rs \u222aRf\n6: end for\n7: OtherRs \u2190GetRsV iaAddress(KG, li)\n8: Chosen_Rs \u2190Chosen_Rs \u222aOtherRs\n9: Rejected_Rs \u2190All_Rs \\ Chosen_Rs\n10: for all r \u2208Rejected_Rs do\n11:\nif Applicable(LJ, Facts, li, r) then\n12:\nRejected_Rs \u2190Rejected_Rs \\ {r}\n13:\nend if\n14: end for\n15: return Facts, Chosen_Rs, Rejected_Rs\nrelations) connected to li. In order to produce the rejected rules, from Line 9 to 14, we use an LLM judge\n(Appendix E.3) to determine whether each of the non-chosen rules is truly not applicable. The intuition is\nthat even though a rule is not directly or indirectly connected to li in the KG, it may be due to an imperfect\nKG generation process. The output of the LLM Judge could be: Yes/No/Potentially, and we only keep the\nrules that are determined as non-applicable (i.e., the LLM Judge outputs No).\n4.3\nModel Training\nIn our work, we conduct both SFT and Preference Optimization, DPO Rafailov et al. [2023] specifically. By\ngiving models the case facts, SFT (Figure 3) teaches the models how to identify the legal issue, locate the\napplicable rules, and learn the rationale behind these materials (Exp at Line 9 in Algorithm 1). Different\nfrom SFT, our preference data (Figure 4) focuses on teaching the models how to conduct legal reasoning by\ncontrasting the applicable and non-applicable legal rules with respect to a variety of legal matters. For DPO,\nwe adopted the default Loss function with a small NLL term on the chosen answers Pang et al. [2024].\nWe emphasize that the above training data is not directly related to our evaluation benchmarks (Section\n5.2). Our primary goal is to provide the models with high-quality domain knowledge via different training\nparadigms in order to study whether the trained models would be able to gain improved capability on diverse\ntasks in the same domain.\n5\nEvaluation\n5.1\nIRAC KG Generation and Quality Review\nWe randomly sampled \u223c12K cases, with 220 cases from each U.S. jurisdiction, and divided them into Train\nand Validation for model training purposes with a ratio of 10:1. We produced KGs with both Claude Sonnet\n3.5 and Llama-3.1-405B-Instruct. Two SMEs then reviewed the Sonnet KG of 18 cases and the Llama KG of\n8 cases1, with each entity and relation examined by a single SME.\nTable 1 shows that the Sonnet KG has superior quality on most entity types (except for Legal Issue), by\ntaking into account both Good and Acceptable. Note that the Llama KG generally has notably higher ratios\nof entities labeled as Poor. In terms of relations, the Sonnet KG also showed much higher quality, with only\n8% of relations labeled as Fail compared to that of 41% of the Llama KG.\nOne drawback of both KGs is that they miss a decent percentage of entities. In a legal case, not all facts or\n1Due to its inferior quality, we stopped the review process.\n7\nKnowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning\nissues are considered critical for courts to decide their rulings. A legal issue may be mentioned in passing\nwithout further discussions; also, some facts may merely be included in a case while not being essential to the\ncore issues of the case. In such situations, missing entities have little impact in understanding the case. We\nleave it to future work to improve our KG\u2019s comprehensiveness.\nGiven the above quality review, we adopted the IRAC KG from Claude Sonnet 3.5 for producing the post-\ntraining datasets. We also utilized Sonnet 3.5 to generate the explanation (Exp at Line 9 in Algorithm 1) for\nthe SFT dataset, and employed it as the LLM Judge (LJ at Line 11 in Algorithm 2) during preference data\ngeneration.\n5.2\nEvaluation Benchmarks\nIn order to verify the effectiveness of our proposed IRAC KG, we carried out extensive experiments on diverse\nlegal datasets (detailed in Appendix A).\nLexGLUE2 Chalkidis [2023] is a well-adopted legal benchmark, covering European and U.S. jurisdictions. It\nmainly consists of (multi-class and multi-label) classification tasks, together with a multiple choice question\nanswering task; each task includes 1,000 samples. The number of classes range from 10 to 100, presenting\nsubstantial challenges to legal AI systems.\nLegalBench Guha et al. [2023] is a recent benchmark that mainly focuses on multiple choice question answering.\nIn addition, it also includes several information extraction tasks, such as extracting plaintiffs and defendants\nfrom case excerpts. It includes a total of 162 tasks, and we removed rule_qa whose evaluation requires manual\nreview according to the LegalBench team.\nCOLIEE Rabelo et al. [2022] is a well-known competition for legal systems, with two major types of tasks:\nretrieval (of relevant cases or statues) and entailment. For retrieval, it is to find supporting prior cases to help\ndecide on a new case in Canada (Task-1) or to locate relevant law articles (statutes) for Japanese legal queries\n(Task-3). For entailment, instead of requiring the systems to locate the relevant prior cases or statues, the\ninput already includes this information, and the systems should decide whether the following entailments\nhold: <paragraph (from a prior legal case) |= court decision> (Task-2) or <statute |= legal_query> (Task-4).\nGiven that the test set is relatively small for each specific year, we combined the data from all previous years\nfor our evaluation.\nSuperGPQA Team et al. [2025] consists of graduate-level multiple choice questions for a broad range of\ndomains, e.g., Medicine, History, etc. Our evaluation focuses on the 656 questions in Law, where each question\ncontains 9 \u223c10 choices.\n\u03b4-Stance Gupta et al. [2025] is a large-scale dataset to specifically evaluate AI systems\u2019 legal reasoning\ncapability. In U.S. legal cases, introductory signals (e.g., see, contra, see generally, etc.) between two text\nsnippets often indicate certain important relations. This dataset consists of \u223c14K triples of <snippet A,\nsignal, snippet B>. We consulted SMEs to group such triples into four higher-level classes (Appendix B):\ndirectly supports, indirectly supports, contradicts and background. We also produced a 3-class version where\nwe removed the \u201cbackground\u201d category that does not provide as much value as the other three according to\nSMEs.\n5.3\nModel Training Details\nWe post-trained (Appendix C) models of different scales: Qwen3-30B-A3B-Instruct-2507 (Qwen3-30B),\nLlama-3_3-Nemotron-Super-49B-v1_5 (Nemotron1.5-49B) and Llama-3.1-70B-Instruct (Llama3.1-70B).\nThe first two recent models obtained competitive results on various benchmarks. As for Llama Dubey et al.\n[2024], the 3.3 model is not as competitive as the 3.1 model (Appendix D), thus we utilized the earlier version.\n5.4\nResults and Discussion\nSFT/DPO vs. OOB. In Table 2, compared to the OOB models, our KG-based post-training achieved\nimprovements on 10, 13 and 12 out of the 14 tasks for Qwen3-30B, Nemotron1.5-49B and Llama3.1-70B\n2https://github.com/coastalcph/zeroshot_lexglue\n8\nKnowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning\nBenchmark\nQwen3-30B\nNemotron1.5-49B\nLlama3.1-70B\nSaulLM\nOOB\nSFT\nDPO\nOOB\nSFT\nDPO\nOOB\nSFT\nDPO\n(141B)\nLexGLUE Average\n57.9\n60.3\n61.6\n57.0\n61.2\n62.5\n65.0\n66.8\n67.2\n64.4\nECtHR A\u22c6\n53.4\n59.7\n62.8\n52.8\n63.2\n66.3\n73.6\n77.3\n77.9\n73.6\nECtHR B\u22c6\n60.2\n66.3\n69.0\n60.9\n70.0\n73.7\n80.0\n82.1\n82.3\n77.6\nEUR-LEX\u22c6\n27.6\n28.5\n30.5\n27.1\n29.2\n29.5\n33.9\n32.3\n31.8\n32.2\nLEDGAR\u22c6\n69.7\n69.6\n69.6\n66.0\n66.5\n67.2\n63.6\n66.8\n67.5\n71.8\nSCOTUS\u22c6\n66.0\n65.8\n65.9\n65.4\n66.0\n65.4\n67.5\n68.9\n68.8\n52.2\nCaseHOLD\u00a7\u2662\n70.7\n71.7\n72.0\n69.7\n72.5\n73.2\n71.6\n73.4\n75.1\n79.0\nCOLIEE Average\n56.6\n59.0\n61.1\n49.4\n53.5\n54.2\n57.0\n59.8\n60.7\n24.6\nTask-1\u22c6\n39.8\n35.0\n40.3\n20.3\n28.3\n28.5\n30.8\n30.6\n32.4\n0.0\nTask-2\u22c6\u2662\n52.2\n60.0\n62.1\n48.0\n54.6\n57.2\n57.7\n66.0\n67.6\n25.1\nTask-3\u25b3\n64.3\n67.1\n67.7\n53.6\n53.4\n53.4\n61.3\n64.4\n64.6\n0.0\nTask-4\u00a7\u2662\n70.2\n73.9\n74.2\n75.9\n77.5\n77.7\n78.4\n77.9\n78.0\n73.1\n\u03b4-Stance Average\u2020\u2662\n45.5\n44.3\n43.7\n33.9\n38.7\n40.9\n48.8\n49.7\n50.1\n43.8\n3-class\n52.7\n51.2\n50.7\n38.8\n44.0\n46.5\n55.1\n56.1\n56.5\n48.8\n4-class\n38.3\n37.3\n36.8\n29.1\n33.4\n35.3\n42.6\n43.2\n43.6\n38.8\nLegalBench\u25b2\n74.1\n75.3\n75.4\n74.2\n75.9\n76.4\n78.0\n79.3\n79.3\n79.3\nSuperGPQA (Law)\u00a7\u266242.4\n42.1\n42.5\n37.7\n37.6\n38.0\n40.3\n41.7\n43.8\n28.4\n\u2662: Legal reasoning task.\n\u22c6: Micro-F1; \u2020: Macro-F1; \u25b3: Macro-F2; \u00a7: Accuracy; \u25b2: Balanced Accuracy.\nTable 2\nEvaluation results (0\u223c100). OOB denotes the out-of-the-box model without additional training; SFT and\nDPO represent the models after the corresponding training respectively. For each model, we underscore each task\u2019s\nbest performance, and bold the highest average performance; blue background indicates the highest score for a row.\nrespectively, where the best performance falls on either SFT or DPO. Furthermore, for all three models,\nDPO training helped obtain the highest average performance on most benchmarks, except for Qwen3-30B on\n\u03b4-Stance.\nDPO vs. SFT. Compared to SFT, DPO did provide additional benefits on 11/14 tasks for Qwen3-30B and\nLlama3.1-70B, and on 12/14 tasks for Nemotron1.5-49B. Unlike prior works that mostly focus on training\nsmaller models, such as 7B/8B Fan et al. [2025], Wang et al. [2025], Saeidi et al. [2024] and 32B Wen et al.\n[2025], our results demonstrated the benefits of KG-based DPO training of different and also larger scales:\n30B, 49B and 70B. Furthermore, compared to the 30B model, the two larger DPO models exhibited clearer\nadvantages than SFT on 4/6 legal reasoning tasks: CaseHOLD, \u03b4-Stance (both tasks) and SuperGPQA.\nSaulLM (141B) Colombo et al. [2024] is a SOTA legal LLM that was trained on a large amount of legal data.\nDespite post-trained on much smaller SFT (13.5K) and DPO (5.3K) datasets, our models show noticeable\nbenefits. First, while Nemotron1.5-49B OOB is substantially worse than SaulLM on LexGLUE, \u03b4-Stance and\nLegalBench, its DPO version clearly narrowed the gaps. Furthermore, although Llama3.1-70B OOB already\noutperforms SaulLM on most benchmarks, our post-training did fully close the gap on LegalBench.\nInstruction Following (IF). Since Nemotron1.5-49B is also based on Llama, we only present the scores of\nQwen3-30B and Llama3.1-70B in Table 3. In general, both models were able to follow the instructions in our\nprompts. Furthermore, our post-training did not damage models\u2019 IF capability, and in fact, they provided\nnoticeable improvements: Qwen3-30B on LexGLUE, and Llama3.1-70B on COLIEE and SuperGPQA (Law).\nWe did not employ complex post-processings: We lowercased model outputs, removed punctuations, and\nperformed exact string matching against the groundtruth. Due to our relatively strict metric calculation,\ncompared to other benchmarks, we do observe higher error rates on LexGLUE (mainly classification tasks),\nwhere we require the predictions of a test sample to be a subset of the available labels for the corresponding\ntask. For instance, for EUR-LEX (multi-label classification of EU legislative documents), Qwen3-30B OOB\npredicated risk management while only management is a valid label. We do not give partial credit in such\nscenarios.\n9\nKnowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning\nBenchmark\nQwen3-30B\nLlama3.1-70B\nO/S/D\nO/S/D\nLexGLUE\n5.7/3.4/2.1\n1.8/2.7/2.2\nCOLIEE\n0.4/0.3/0.2\n3.4/1.8/2.1\n\u03b4-Stance\n0.0\n0.0\nLegalBench\n0.4/0.2/0.2\n0.2/0.1/0.2\nSuperGPQA\n0.0\n1.4/0.0/0.2\nTable 3\nIF error rates (0\u223c100). We report the average of all tasks for each benchmark. O, S and D represent the\nOOB, SFT and DPO models respectively; a single number indicates the same rate for all models.\n6\nConclusion\nIn this paper, we proposed a generalizable knowledge graph-assisted approach for enhancing LLMs\u2019 legal\nreasoning capability. We follow the IRAC framework in order to model courts\u2019 analysis and reasoning process\nof legal cases. Furthermore, we conducted both SFT and DPO training with mid to large models (30B,\n49B and 70B) by utilizing data generated from the IRAC KG. Our trained models achieved better average\nperformance on 4/5 diverse legal benchmarks than baselines. Compared to SFT, the Llama3.1-70B DPO model\nexhibited promising results on 3/6 legal reasoning tasks, with 1.7% (CaseHOLD), 1.6% (COLIEE-Task-2) and\n2.1% (SuperGPQA Law) higher scores respectively; in particular, it achieved noticeably higher scores than\nSaulLM, a 141B SOTA legal LLM trained on a large amount of legal data.\nLimitations\nOur current work is limited to U.S. Law (common law), and the generalizability of our approach to other legal\nsystems warrants further exploration. In addition to legal, the generalizability of our proposed KG-assisted\nmodel post-training for other high-stakes domains would also be an important topic. Furthermore, in this\nwork, we produced our data with 12K U.S. cases. It would be interesting to utilize more cases, and also\nstudy the impact of additional relations, e.g., citations and overrulings; however, we need to be mindful\nof the environmental impact, since it would require more GPU hours. Also, given the emergence of other\nLLMs, we should explore alternatives other than Claude Sonnet 3."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary\n\n#### Main Contributions\nThe paper proposes a knowledge graph (KG)-assisted approach to enhance large language models (LLMs) for legal reasoning tasks. The key contributions are:\n1. **Knowledge Graph Construction**: The authors develop an IRAC Knowledge Graph (IRAC KG), which captures the relationships between issues, rules, applications of facts, and conclusions in judicial opinions.\n2. **LLM Post-Training with IRAC KG**: They conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) using mid to large LLMs (30B, 49B, and 70B), leveraging the IRAC KG as a knowledge source.\n3. **Evaluation on Legal Benchmarks**: The models trained with this approach outperform baselines on diverse legal benchmarks, achieving better average performance on four of five benchmarks.\n\n#### Methodology\nThe methodology involves:\n1. **IRAC Framework Modeling**: Key legal concepts are modeled using the IRAC framework (Issue, Rule, Analysis and Conclusion), which is taught in law schools.\n2. **KG Construction**: The authors extract key legal concepts from a corpus of 12K U.S. legal case opinions and encode them as nodes and typed edges in an IRAC KG.\n3. **Training Data Generation**: Training data for SFT and Preference Optimization are generated using the IRAC KG, including prompts and explanations to guide model training.\n\n#### Key Findings\nThe key findings include:\n- The models trained with the IRAC KG outperformed baselines on four of five diverse legal benchmarks (14 tasks).\n- Specifically, the 70B DPO model achieved the best score on four of six reasoning tasks compared to both baseline and a state-of-the-art (SOTA) legal LLM.\n- The models showed improvements in instruction following accuracy.\n\n#### Conclusions\nThe paper concludes that the proposed KG-assisted approach significantly enhances LLMs' legal reasoning capabilities. It outperforms existing methods, especially when using larger models for DPO training. However, limitations include the focus on U.S. law and the need to explore generalization across different legal systems and domains.\n\n### Detailed Breakdown\n\n#### Introduction\nThe introduction highlights the challenges faced by LLMs in specialized domains like Law due to specialized terminologies and domain-specific reasoning needs. It emphasizes the importance of capturing domain knowledge, particularly for tasks requiring deep understanding of abstract concepts such as judicial reasoning patterns.\n\n#### Related Work\n- **KG Construction**: The authors discuss existing approaches to KG construction using NER and RE methods.\n- **Graph-based LLM Reasoning**: They review works that use graphs for complex reasoning scenarios like text corpora organization and retrieval optimization.\n- **KG-LLM Synergy in Law**: Previous studies have explored legal case representation with KGs but did not focus on IRAC framework integration.\n\n#### System Overview and Knowledge Graph Design\nThe system overview outlines the process of extracting key concepts from legal cases using LLMs, generating training data for SFT and Preference Optimization. The IRAC KG design follows the IRAC framework to model legal reasoning processes effectively.\n\n#### Training Data Generation\nTraining datasets are generated by applying prompts and explanations derived from the IRAC KG. For example, in SFT, models learn how to derive legal issues and applicable rules based on case facts. In Preference Optimization, models are trained to distinguish between applicable and non-applicable rules for various legal matters.\n\n#### Model Training\nThe authors conduct both SFT and DPO training using mid to large LLMs (30B, 49B, and 70B) with IRAC KG-derived data. The results show significant improvements in performance on diverse legal benchmarks compared to baseline models.\n\n#### Evaluation\nThe paper evaluates the trained models on five legal benchmarks: LexGLUE, LegalBench, COLIEE, SuperGPQA (Law), and \u03b4-Stance. It demonstrates measurable gains over baselines and SOTA models, particularly for DPO-trained models like Llama3.1-70B.\n\n#### Limitations\nThe authors acknowledge that the approach is limited to U.S. law and needs further exploration across different legal systems and domains. They also note practical limitations such as data availability and computational resources.\n\n### Conclusion\nIn summary, this paper presents a novel KG-assisted LLM post-training method for enhancing legal reasoning capabilities. The results indicate significant improvements in performance on diverse legal benchmarks, highlighting the effectiveness of IRAC framework integration with KGs. However, further research is needed to generalize these findings across different domains and legal systems."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via\nReinforcement Learning\nHongbo Bai1\u2217, Yujin Zhou1\u2217, Yile Wu1, Chi-Min Chan1\nPengcheng Wen1, Kunhao Pan1,Sirui Han1\u2020, Yike Guo1\u2020\n1Hong Kong University of Science and Technology\nbaihongbo@ust.hk\nAbstract\nLarge Multimodal Models (LMMs) have\nachieved remarkable success in visual under-\nstanding, yet they struggle with knowledge-\nintensive queries involving long-tail entities\nor evolving information due to static paramet-\nric knowledge. Recent search-augmented ap-\nproaches attempt to address this limitation,\nbut existing methods rely on indiscriminate\nwhole-image retrieval that introduces substan-\ntial visual redundancy and noise, and lack\ndeep iterative reflection, limiting their effec-\ntiveness on complex visual queries. To over-\ncome these challenges, we propose Glance-or-\nGaze (GoG), a fully autonomous framework\nthat shifts from passive perception to active\nvisual planning. GoG introduces a Selective\nGaze mechanism that dynamically chooses\nwhether to glance at global context or gaze\ninto high-value regions, filtering irrelevant in-\nformation before retrieval. We design a dual-\nstage training strategy: Reflective GoG Behav-\nior Alignment via supervised fine-tuning in-\nstills the fundamental GoG paradigm, while\nComplexity-Adaptive Reinforcement Learning\nfurther enhances the model\u2019s capability to han-\ndle complex queries through iterative reasoning.\nExperiments across six benchmarks demon-\nstrate state-of-the-art performance. Ablation\nstudies confirm that both Selective Gaze and\ncomplexity-adaptive RL are essential for effec-\ntive visual search. We will release our data and\nmodels for further exploration soon.\n\u201cYou see, but you do not observe. The distinction is clear.\u201d\n\u2014 Arthur Conan Doyle, A Scandal in Bohemia\n1\nIntroduction\nLarge Multimodal Models (LMMs) have demon-\nstrated remarkable proficiency in general visual\nunderstanding and reasoning, driven by extensive\n*Equal contribution. \u2020Corresponding author.\nvisual-text pre-training (Wang et al., 2025b, 2024;\nQi et al., 2025). Despite these progress, a funda-\nmental tension persists between the static nature\nof parametric knowledge and the dynamic, open-\nended evolution of the real world. Constrained by\nfixed training corpora, LMMs inevitably struggle\nwith knowledge-intensive Visual Question Answer-\ning (VQA) tasks that require up-to-date informa-\ntion or involve concepts of long-tail beyond their\ninternal distribution (Narayan et al., 2025; Li et al.,\n2023). Specifically, when querying recent events or\nobscure visual entities, such as identifying a niche\nfestival or interpreting a newly released product,\nthese models frequently exhibit \u201cknowledge cutoff\u201d\nor \u201clong-tail amnesia\u201d (Lewis et al., 2020; Hu et al.,\n2023; Xu et al., 2024; Tong et al., 2024). Such de-\nficiencies often precipitate factual hallucinations or\ngeneric responses, thereby undermining the LMMs\u2019\nutility in knowledge-critical domains (Wei et al.,\n2024; Yasunaga et al., 2022).\nTo mitigate this gap, recent research has in-\ncreasingly sought to augment LMMs with exter-\nnal search mechanisms.\nExisting works gener-\nally fall into three distinct categories. Initially,\napproaches adopt Retrieval-Augmented Genera-\ntion (RAG), which retrieves auxiliary context from\nfixed knowledge bases (Wu et al., 2025b; Lin\net al., 2024, 2023). However, this \"retrieve-then-\ngenerate\" pipeline often suffers from rigidity, fail-\ning to capture the dynamic breadth of the open\nweb. Subsequently, Prompt-Engineered Agents\nemerged, leveraging the planning capabilities of\nLMMs to orchestrate search engines (Li et al.,\n2025; Wu and Xie, 2024). While flexible, these\nsystems typically operate in a \"plug-and-play\" man-\nner, leaving the model\u2019s intrinsic search capabil-\nities unoptimized. More recently, tool-integrated\nLMMs have advanced this frontier by incorporat-\ning search actions directly into the training process,\nthereby aligning the model\u2019s internal representa-\ntions with external retrieval behaviors (Wu et al.,\narXiv:2601.13942v1  [cs.CV]  20 Jan 2026\nDirect Answer (No Search \u2013 Long Tail Problem)\nVLM\nBased on the visual evidence in the image, the\nperson celebrating at the center of the group is\nPaul Wotton.\nFAILED: Unfaithful response to long-tail problems\nSearch-Based LMMs (Passive Whole Image Search)\nVLM\nWhole\nImage Search\nTop 5 Results\nBased on the image search results, I cannot\nidentify the person celebrating in the image.\nFAILED: Overwhelmed by Whole Image Search Noise.\nGoG LMMs (Active Gaze & Selection)\n<think>Based on \u2026I need to crop the image to\nshow \u2026 </think><img_search>center person\nface</img_search>\nQuestion: Based on the provided image, who is the person celebrating at the center of\nthe group?\nBox 1\nBox 2\nBox 3\nGaze\nCandidate\nGaze Crops\n<think> I need to search relevant\ncrops</think><search_crop>1</search_crop>\nResult 1: Eric Perez celebrated in the\nchanging rooms with his team after Truro\nCity won National League South last season\nClean & Relevant Information\n<think> Based on the search results \u2026\n</think><answer> Eric Perez </answer>\nGoG\nGoG\nGoG\nFigure 1: Comparison between previous baselines and our Glance-or-Gaze (GoG) framework. GoG employs\nan active multi-step strategy: proposing candidate regions via visual grounding, filtering relevant crops through\nSelective Gaze, and conducting precise search only on selected regions with iterative cross-modal reflection.\n2025a; Narayan et al., 2025).\nDespite recent progress, existing methodologies\nface two critical limitations. First, current visual\nstrategies are inefficient and noisy. By relying on\nrigid cropping and indiscriminately processing all\ncandidate regions, these methods introduce signifi-\ncant noise. Moreover, they depend heavily on con-\nverting visual details into text descriptions, which\ncreates an information bottleneck and prevents the\nmodel from accessing raw visual data. Second,\ncurrent works execute tool calls in a single-pass\nmanner or limit reflection strictly to the textual\nmodality. They fail to support iterative visual veri-\nfication or self-correction, restricting their ability\nto adaptively refine strategies when facing intricate\nvisual questions.\nTo overcome these limitations, we propose\nGlance-or-Gaze (GoG), the first fully autonomous\nframework that shifts the paradigm from passive\nimage perception to dynamic, complexity-adaptive\nvisual planning, as illustrated in Figure 1. We in-\ntroduce a Selective Gaze mechanism that actively\nfilters visual noise by evaluating and prioritizing\npertinent image patches before execution. The\nframework integrates two complementary learn-\ning mechanisms: (i) Reflective GoG Behavior\nAlignment addresses the inefficiency and noise in-\nherent in current visual strategies by employing im-\nitation learning to instill the fundamental paradigm\nof active selection and cross-modal reflection. (ii)\nComplexity-Adaptive Reinforcement Learning\nfurther enhances the model\u2019s planning and reason-\ning capabilities by optimizing the selection policy,\nenabling intelligent, iterative reflection and adap-\ntive focus search based on visual query complexity.\nOur main contributions are summarized as follows:\n\u2022 We propose GoG, the first fully autonomous\nframework that shifts the paradigm from pas-\nsive image perception to dynamic, complexity-\nadaptive visual planning. We introduce a Se-\nlective Gaze mechanism that actively filters\nvisual noise by evaluating and prioritizing per-\ntinent image patches, effectively bridging the\ngap between coarse-grained glancing and fine-\ngrained evidence verification.\n\u2022 We design a dual-stage learning architecture\ncomprising Reflective GoG Behavior Align-\nment and Complexity-Adaptive Reinforce-\nment Learning. The former addresses the in-\nefficiency inherent in current visual strategies\nby instilling the fundamental paradigm of ac-\ntive selection and cross-modal reflection. The\nlatter further enhances planning and reasoning\ncapabilities, enabling adaptive search based\non visual query complexity.\n\u2022 Extensive experiments demonstrate the ef-\nfectiveness and generalization of GoG. Our\nmethod achieves state-of-the-art performance\nacross diverse benchmarks, surpassing strong\nbaselines with significant gains ranging from\n5 to 20, validating the superiority of our adap-\ntive GoG planning paradigm.\n2\nRelated Works\n2.1\nLarge Multimodal Models\nThe evolution of Large Multimodal Models\n(LMMs) has progressed rapidly from early vision-\nlanguage pretraining (Agarwal et al., 2021; Li et al.,\n2022) to sophisticated unified architectures capa-\nble of complex reasoning (Liu et al., 2024; Bai\net al., 2025b; Chen et al., 2024). Recent mod-\nels such as GPT-4o (OpenAI et al., 2024), Gem-\nini (Team et al., 2025), Qwen3-VL series (Bai et al.,\n2025a), and Internvl3.5-VL (Wang et al., 2025a)\nhave demonstrated remarkable capabilities in vi-\nsual understanding, achieving strong performance\non benchmarks spanning image captioning and vi-\nsual question answering. Despite these advances,\nLMMs remain fundamentally constrained by their\nstatic parametric knowledge (Lewis et al., 2020;\nHu et al., 2023). When confronted with queries\ninvolving recent events, long-tail entities, or fine-\ngrained visual details beyond their training distri-\nbution, these models frequently hallucinate or pro-\nduce generic responses (Xu et al., 2024).\n2.2\nSearch-Augmented Visual Reasoning\nTo address the knowledge limitations of LMMs,\nresearchers have explored various strategies for\nintegrating external information retrieval. Early\nefforts extended text-based Retrieval-Augmented\nGeneration (RAG) to the multimodal domain, re-\ntrieving relevant documents or images from fixed\nknowledge bases(Wu et al., 2025b; Lin et al., 2024,\n2023). Subsequently, prompt-based search agents\nemerged, leveraging the planning capabilities of\nLMMs to orchestrate web search engines via chain-\nof-thought prompting (Yao et al., 2022; Yang et al.,\n2023; Li et al., 2025; Wu and Xie, 2024). More\nrecently, tool-integrated LMMs have incorporated\nsearch actions directly into the training process,\nwith MMSearch-R1 (Wu et al., 2025a) embedding\nsearch within the reinforcement learning loop and\noptimizing web navigation policies through online\ninteraction (Wu et al., 2025a; Narayan et al., 2025).\nDespite these advances, current visual strategies\nremain inefficient and noisy, relying on rigid crop-\nping and indiscriminate region processing while\nconverting visual details into text, creating an in-\nformation bottleneck that prevents access to raw\nvisual data. Furthermore, existing approaches ex-\necute tool calls in a single-pass manner without\niterative visual verification, limiting their ability\nto adaptively refine strategies for complex visual\nqueries. To address these challenges, we propose\nGlance-or-Gaze (GoG), a unified framework that\nintegrates active visual exploration with reflective\nreasoning, enabling dynamic, complexity-adaptive\nvisual planning.\n3\nMethod\n3.1\nOverview\nWe propose a dual-stage learning framework\ndesigned to evolve Large Multimodal Models\n(LMMs) from passive observers into active visual\nplanners. As illustrated in Figure 2, our approach\nintegrates Reflective GoG Behavior Alignment and\nComplexity-Adaptive Reinforcement Learning to\nachieve complexity-adaptive visual reasoning.\n\u2022 Stage 1: Reflective GoG Behavior Align-\nment. This stage addresses the inefficiency\nand noise inherent in current visual strate-\ngies. We establish a specialized data cura-\ntion pipeline with uncertainty-aware query\nsampling and human-verified trajectory syn-\nthesis to construct the GoG-Instruct dataset.\nThrough supervised fine-tuning, we instill the\nfundamental paradigm of active selection and\ncross-modal reflection, training the model to\nsuppress ungrounded generation and proac-\ntively trigger \u201cGlance\u201d or \u201cGaze\u201d actions.\n\u2022 Stage 2: Complexity-Adaptive Reinforce-\nment Learning.\nThis stage further en-\nhances the model\u2019s planning and reasoning\ncapabilities.\nWe construct a complexity-\nstratified dataset based on failure rates and\nemploy Group Relative Policy Optimization\n(GRPO) (Shao et al., 2024). This empow-\ners the model to autonomously plan and exe-\ncute multi-step exploratory searches, enabling\nStage 1: Reflective GoG Behavior Alignment\nRaw Data\n(FVQA/InfoSeek)\nUncertainty-Aware Filter\nVLM\nGenerate n=4\nDiscard\n(Pass n=4)\nRetain\n(Pass n<4)\nGoG Trajectory Synthesis\nRetain\nQuery Set\nVLM\nGoG\nGoG\nInstruct Data\nHuman Verification\n1. Answer\nAccuracy\n&\n2. Visual\nRationality\nGoG-Instruct Data Construction\nReflective GoG Behavior Alignment (SFT)\nGoG-Instruct\nData\nBase Model\nSFT\nGoG-SFT Model\nStage 2: Complexity-\nAdaptive RL\nRL Data Construction\nGoG-SFT\nModel\nRaw Data\nGoG-SFT\nModel\nRL\nGoG-RL\nModel\nComplexity Adaptive RL\nData With Two Level\nLevel 1\nLevel 2\nFigure 2: Overview of the Glance-or-Gaze (GoG) framework. Stage 1 (Left): Reflective GoG Behavior Alignment\nconstructs GoG-Instruct data through uncertainty-aware filtering and human-verified trajectory synthesis, then\nperforms supervised fine-tuning to instill active selection and cross-modal reflection. Stage 2 (Right): Complexity-\nAdaptive RL constructs complexity-stratified data at two difficulty levels and applies reinforcement learning to\nenhance planning capabilities for adaptive visual reasoning.\nadaptive search based on visual query com-\nplexity.\n3.2\nStage 1: Reflective GoG Behavior\nAlignment\nThis stage instills the fundamental paradigm of\nactive selection and cross-modal reflection through\nimitation learning.\nGoG-Instruct Data Construction\nTo facilitate\nactive visual planning, We curate GoG-Instruct, a\ndataset derived from FVQA (Wu et al., 2025a) and\nInfoSeek (Chen et al., 2023), through a three-step\npipeline as illustrated in Figure 1 (Left).\n1. Uncertainty-Aware Filtering. We filter out triv-\nial samples where the model already possesses suf-\nficient parametric knowledge. Specifically, we em-\nploy Qwen3-VL-235B-Instruct to generate answers\nfor each query N = 4 times. Samples where the\nmodel consistently answers correctly (pass count\nn = 4) are discarded, retaining only queries with\nn < 4 that require external visual verification.\n2. GoG Trajectory Synthesis. For the retained\nquery set, we synthesize reasoning trajectories with\nexplicit visual planning structure. Each trajectory\nfollows the sequence: (1) Glance for global im-\nage analysis, (2) Decision for tool invocation or\nregion cropping, and (3) Gaze for targeted search\nexecution on selected regions.\n3. Human Verification. To ensure data quality,\nexpert annotators validate each trajectory against\ntwo criteria: (1) Answer Accuracy, ensuring the\nfinal response is factually correct; and (2) Visual\nRationality, verifying that cropped regions are log-\nically relevant to the query. Trajectories failing\neither criterion are discarded.\nThrough this pipeline, we construct the GoG-\nInstruct dataset comprising 5,750 samples, where\n43.5% are search-free instances and 56.5% require\nvarious search operations, as detailed in Table 1.\nTraining Objective\nWe employ Qwen2.5-VL-\n7B-Instruct and Qwen3-VL-Think as the base\nmodel. To enable efficient training, we incorpo-\nrate LoRA adapters (r = 8) across all transformer\nblocks. The model is trained on multi-turn con-\nType\nSubtype\nRatio\nCount\nSearch-free\n\u2013\n43.5%\n2,500\nSearch-required\nText Search Only\n13.0%\n750\nImage Search Only\n30.4%\n1,750\nBoth Search\n13.0%\n750\nTotal\n100%\n5,750\nTable 1: Composition of the SFT training dataset. \u201cIm-\nage Search Only\u201d refers to instances that execute either\na single whole image search operation or a Gaze opera-\ntion (cropped region search), but not both.\nversations y\u2217containing reasoning sequences and\nstructured tool calls. We adopt the standard Causal\nLanguage Modeling (Causal LM) objective:\nLSFT = \u2212\nT\nX\nt=1\nlog \u03c0\u03b8(y\u2217\nt | x, I, y\u2217\n<t),\n(1)\nwhere T is the sequence length and \u03c0\u03b8 is the\nmodel\u2019s conditional distribution.\n3.3\nStage 2: Complexity-Adaptive\nReinforcement Learning\nWhile SFT establishes the capability for active plan-\nning, it lacks the flexibility to handle varying query\ndifficulties. We employ Reinforcement Learning\nto optimize the planning policy for robustness, as\nillustrated in Figure 1 (Right).\nRL\nData\nConstruction\nWe\nconstruct\ncomplexity-stratified\ndata\nby\nevaluating\nthe\nGoG-SFT model on raw queries and stratifying\nthem into two difficulty levels:\nLevel 1: This subset consists of queries where the\nGoG-SFT model exhibits a pass rate of approxi-\nmately 50% using standard \u201cGlance\u201d or \u201cGaze\u201d ac-\ntions. These samples represent the decision bound-\nary where the model is uncertain, providing effec-\ntive signal for policy optimization.\nLevel 2: Building upon Level 1, this subset incor-\nporates additional queries where the GoG paradigm\nconsistently fails, resulting in an overall lower pass\nrate. Training on this level forces the model to de-\nvelop more robust reasoning strategies beyond the\nstandard visual exploration patterns.\nWe adopt Level 2 data as our final RL training\nset to maximize the model\u2019s capability for complex\nvisual reasoning.\nTraining Objective\nWe employ GRPO to opti-\nmize the policy on the Level 2 dataset DL2. For\neach multimodal input (q, I) \u2208DL2, we sample\na group of G candidate trajectories {\u03c4i}G\ni=1 from\nthe current policy \u03c0\u03b8. Each trajectory receives a\nreward ri, and we compute the group-normalized\nadvantage \u02c6Ai = (ri \u2212\u00b5r)/\u03c3r, where \u00b5r and \u03c3r are\nthe mean and standard deviation of rewards within\nthe group. The objective function is:\nLGRPO(\u03b8) = E(q,I)\u223cDL2\n\"\n1\nG\nG\nX\ni=1\nLclip\ni\n\u2212\u03b2DKL(\u03c0\u03b8\u2225\u03c0ref)\n#\n(2)\nwhere Lclip\ni\nis the clipped surrogate objective and\n\u03b2 controls the KL divergence penalty to prevent\nexcessive deviation from the reference model \u03c0ref.\nThe reward ri combines accuracy and format\ncompliance. The accuracy score racc \u2208{0, 1} is\njudged by gpt-oss-120b based on semantic cor-\nrectness against ground truth. The format score\nrfmt \u2208[0, 1] validates correct usage of special to-\nkens and structural integrity. The total reward is:\nri = (1 \u2212\u03bb) \u00b7 racc + \u03bb \u00b7 rfmt\n(3)\nwhere \u03bb controls the emphasis on formatting com-\npliance.\n4\nExperiments\n4.1\nExperiment Setup\nImplementation Details\nWe initialize our back-\nbone using Qwen2.5-VL-7B-Instruct and Qwen3-\nVL-8B-Think. For SFT, we utilize the LLaMA-\nFactory framework with LoRA (rank 8) applied to\nall target modules. The models are trained for 3\nepochs with a learning rate of 1e\u22124, a cosine learn-\ning rate scheduler, and a warmup ratio of 0.1. We\nemploy bf16 mixed precision with a global batch\nsize of 8. For online RL optimization, we adopt\nthe Group Relative Policy Optimization (GRPO)\nalgorithm within the veRL framework. We set the\nactor learning rate to 2e\u22126 with a sigmoid decay\nwarmup over 45 steps. The KL-divergence coeffi-\ncient is set to \u03b2 = 0.001. During exploration, we\nuse a rollout number of N = 4 with vLLM for\nefficient inference, allowing a maximum response\nlength of 8,192 tokens. The RL training runs for\n15 epochs. All training are conducted on a sin-\ngle node equipped with 8 NVIDIA H800 GPUs.\nAdditional hyperparameter details are provided in\nAppendix D.\nBenchmark and Metrics\nBenchmarks. To com-\nprehensively evaluate GoG, we conduct experi-\nments on a diverse set of datasets categorized into\nin-distribution (IID) and out-of-distribution (OOD)\nsettings.\nFor IID evaluation, we utilize InfoS-\neek (Chen et al., 2023) and FVQA-test (Wu et al.,\n2025a). To assess the model\u2019s generalization ca-\npability (OOD), we employ SimpleVQA (Cheng\net al., 2025), MMSearch (Jiang et al., 2024), Dyn-\nVQA (Li et al., 2024), and LiveVQA-New (Fu\net al., 2025). Due to the substantial size of the orig-\ninal splits, we randomly sample 2,000 instances\nfrom InfoSeek and LiveVQA for evaluation. Addi-\ntionally, we filter all datasets to exclusively retain\nEnglish-language samples. Following prior work,\nwe employ an LLM-as-a-Judge approach to eval-\nuate model performance. Specifically, we utilize\ngpt-oss-120b as the judge model to directly as-\nsess the accuracy of the generated answers. The\nfull evaluation prompt used for this assessment is\nprovided in Appendix E.\nBaselines.\nTo benchmark the capabilities of GoG,\nwe evaluate it against a series of strong base-\nlines, including both open-source models (the\nQwen2.5-VL and Qwen3-VL series) and closed-\nsource models (GPT-4o (OpenAI et al., 2024)).\nWe primarily compare performance across four\ndistinct settings: (1) Direct Answer, where the\nmodel is provided with the image and question\nand asked to respond directly; (2) Full-Search\nWorkflow, where retrieval is mandatory for ev-\nery query; (3) Prompt-based GoG Agents, which\nutilize Graph-of-Thought reasoning via prompt-\ning; and (4) Search-Equipped LMMs, specifically\ncomparing against MMSearch R1. Detailed evalu-\nation protocols and the specific prompts used are\nprovided in Appendix E.\n4.2\nMain Results and Observations\nGoG achieves state-of-the-art performance with\nrobust generalization across diverse bench-\nmarks.\nAs detailed in Table 2, GoG-3-8B-\nThink-RL surpasses all baseline paradigms, outper-\nforming the strongest Full-Search Workflow and\nPrompt-based GoG agent by +9.89 and +15.18 on\naverage, respectively. Notably, compared to the\nprevious search-equipped baseline MMSearch-R1,\nGoG achieves a remarkable +19.97 improvement.\nThis superiority is consistent across both in-domain\ndatasets and out-of-domain settings, demonstrating\nthe model\u2019s exceptional adaptability. We observe\nthat while Full-Search workflows improve over Di-\nrect Answer baselines (such as GPT-4o) by retriev-\ning external knowledge, they often fall short of\nGoGbecause they lack the learned discrimination\nSFT\n9.0\n62.3\n28.7\nRL\n2.7\n20.6\n76.7\nSFT\n30.4\n50.2\n19.4\nRL\n2.9\n23.1\n74.0\nQwen3-VL-Think\nQwen2.5-VL\nNo Search\nOne Search\nMix Search\nFigure 3: Distribution of search behavior across dif-\nferent training stages. \u201cNo Search\u201d indicates samples\nwithout any search action, \u201cOne Search\u201d represents sam-\nples using only one type of search (text, image, or crop),\nand \u201cMix Search\u201d denotes samples combining multiple\nsearch types.\nto filter irrelevant noise or decide when search is\nstrictly necessary.\nSFT teaches basic GoG capability while RL en-\nhances interative GoG reasoning. As shown in\nFigure 3, SFT and RL exhibit distinct search be-\nhavior patterns. After SFT, both models primarily\nrely on single-type searches (62.3% for Qwen3-\nVL-Think and 50.2% for Qwen2.5-VL), with a\nnotable proportion of samples requiring no search\nat all (9.0% and 30.4%, respectively). This sug-\ngests that SFT successfully teaches the model when\nand how to invoke search tools for straightforward\nqueries. However, after RL training, the proportion\nof mix search increases dramatically\u2014from 28.7%\nto 76.7% for Qwen3-VL-Think and from 19.4% to\n74.0% for Qwen2.5-VL. Meanwhile, the no-search\nratio drops to below 3% for both models. This shift\nindicates that RL encourages the model to engage\nin more complex, multi-step information gathering,\ncombining text search, image search, and image\ncropping to thoroughly verify and cross-reference\ninformation before answering. The emergence of\nsuch iterative search behavior demonstrates that RL\nnot only reinforces correct search usage but also\ncultivates a more deliberate reasoning process.\nIn-Domain\nOut-of-Domain\nModel\nAvg.\nFVQA InfoSeek\nSimpleVQA MMSearch LiveVQA DynVQA\nDirect Answer\nQwen2.5-VL-7B\n20.31\n25.33\n13.90\n41.26\n14.62\n10.30\n16.43\nQwen2.5-VL-32B\n22.79\n26.89\n16.95\n42.84\n16.37\n12.70\n20.96\nQwen3-VL-8B-Instruct\n21.13\n23.94\n13.80\n40.18\n15.20\n11.40\n22.24\nQwen3-VL-8B-Thinking\n23.84\n24.56\n16.05\n41.76\n15.20\n15.15\n30.31\nQwen3-VL-30B-A3B-Instruct\n23.80\n26.11\n16.70\n42.35\n18.13\n12.60\n26.91\nQwen3-VL-30B-A3B-Thinking 27.26\n31.11\n22.80\n43.73\n17.54\n15.40\n33.00\nGPT-4o\n30.68\n42.00\n30.60\n43.44\n21.64\n14.80\n31.59\nPrompt-based GoG Method\nQwen2.5-VL-7B\n22.31\n24.50\n17.40\n41.66\n20.47\n10.40\n19.41\nQwen3-VL-8B-Think\n41.70\n51.33\n32.00\n62.69\n36.84\n25.95\n41.36\nFull-Search Workflow\nQwen2.5-VL-7B\n44.36\n61.06\n38.15\n59.13\n36.26\n31.05\n40.51\nQwen3-VL-8B-Thinking\n46.99\n57.33\n32.25\n61.90\n63.84\n23.55\n39.09\nSearch-Equipped Models\nMMSearch-R1*\n36.91\n42.39\n24.65\n54.79\n40.94\n23.85\n34.84\nGoG-2.5-7B-SFT\n43.28\n53.72\n41.90\n60.61\n40.35\n24.55\n38.53\nGoG-3-8B-Think-SFT\n50.17\n62.17\n40.55\n65.65\n53.80\n32.40\n46.46\nGoG-2.5-7B-RL\n53.22\n66.78\n51.05\n64.86\n56.73\n37.70\n42.21\nGoG-3-8B-Think-RL\n56.88\n68.44\n49.05\n66.44\n65.50\n43.85\n48.02\nTable 2: Main evaluation results. We compare Direct Answer, Prompt-based GoG, Full-Search Workflow, and\nSearch-Equipped Models.\nBlue rows indicate our proposed models (SFT and RL). Gray row indicates the\nreproduced MMSearch-R1 baseline. * denotes reproduced results.\nSFT\nRL\n0\n10\n20\n30\n40\n50\n60\nCrop Selection Accuracy (%)\n42.1%\n48.9%\n46.7%\n51.3%\n6.8%\n4.7%\nEffectiveness of Selective Gaze\nGoG-3-8B-Think\nGoG-2.5-7B\nFigure 4: Effectiveness of Selective Gaze. Comparison\nof Crop Selection Accuracy between SFT and RL train-\ning stages across two model architectures.\nRL training sharpens Selective Gaze to identify\nanswer-relevant regions more precisely. Beyond\noverall performance gains, we investigate whether\nRL training specifically improves the model\u2019s abil-\nity to select relevant image regions. As shown\nin Figure 4, we measure Crop Selection Accu-\nracy\u2014the proportion of selected crops that contain\nanswer-relevant information\u2014on our evaluation\nset. Both models exhibit substantial improvements\nafter RL training: Qwen3-VL-Think improves\nfrom 42.1% to 48.9% (+6.8%), while Qwen2.5-VL\nincreases from 46.7% to 51.3% (+4.7%). This con-\nsistent gain across different architectures demon-\nstrates that RL does not merely encourage more\nfrequent tool use, but fundamentally refines where\nthe model chooses to focus. To gain deeper in-\nsights, we manually examined 100 samples from\nGoG-3-8B-Think where Gaze operations were in-\nvoked (Table 3). We evaluated two aspects: (1)\nGaze Correctness\u2014whether the selected crop is\nappropriate for answering the question, and (2) Re-\nflection Rate\u2014among incorrect selections, whether\nthe model triggers self-correction. Results show\nthat RL training improves Gaze correctness from\n59% to 75%, and more strikingly, enhances error\nawareness: when the initial Gaze is incorrect, SFT\nmodels rarely recognize the error (30% reflection\nrate), whereas RL-trained models trigger reflection\n70% of the time. This indicates that RL instills\nbetter-calibrated uncertainty, enabling the model to\nrecognize misguided focus and actively seek alter-\nnative regions.\n4.3\nAblation and Analysis\nEffectiveness of the Selective Gaze Mechanism.\nTo validate our architectural design, we conduct\nan ablation study by removing the Selective Gaze\nMetric\nSFT\nRL\nGaze Correctness (%)\n59\n75\nReflection Rate on Errors (%)\n30\n70\nTable 3: Manual analysis of Gaze behavior on 100 GoG-\n3-8B-Think samples.\nModel Setting\nAvg.\nFVQA\nInfo\nSimple\nMM\nLive\nDyn\nBase Model: Qwen2.5-VL-7B\nSFT w/o SG 41.76\n53.39\n40.10\n59.53\n40.94\n22.30\n34.28\nFull SFT\n43.28\n53.72\n41.90\n60.61\n40.35\n24.55\n38.53\n\u2206Gain\n+1.52\n+0.33\n+1.80\n+1.08\n-0.59\n+2.25 +4.25\nBase Model: Qwen3-VL-8B-Thinking\nSFT w/o SG 48.34\n60.17\n40.55\n64.86\n46.20\n32.80\n45.47\nFull SFT\n50.17\n62.17\n40.55\n65.65\n53.80\n32.40\n46.46\n\u2206Gain\n+1.83\n+2.00\n+0.00\n+0.79\n+7.60\n-0.40\n+0.99\nTable 4: Ablation study results. We compare the full\nSFT model against the ablated version (SFT w/o SG),\nwhere SG denotes the Selective Gaze mechanism.\nGreen highlights notable gains; red indicates slight\nregressions.\n(SG) mechanism. In this \"w/o SG\" variant, the\nmodel loses the ability to perform dynamic visual\ncropping and reflection, regressing to a baseline\nthat relies solely on a coarse-grained global view\nof retrieved images. As shown in Table 4, this re-\nmoval leads to consistent performance degradation\nacross both Qwen2.5 and Qwen3 backbones, with\naverage scores dropping by 1.52 and 1.83 points, re-\nspectively. The results underscore the necessity of\nthe \"Selective-Gaze-and-Reflect\" paradigm, partic-\nularly in complex scenarios like DynVQA (+4.25\ngain) and MMSearch (+7.60 gain) where target in-\nformation is often minute or obscured. The SG\nmechanism acts as a physical anchor for reason-\ning: by explicitly directing its gaze to a selected\ncandidate region, it forces the model to transition\nfrom passively seeing the global context to actively\nobserving specific visual details. This focused veri-\nfication provides the granular evidence necessary\nto ground the model\u2019s reasoning, preventing hallu-\ncinations common in noise-heavy environments.\nEffectiveness of Complexity-Adaptive RL Train-\ning. We further investigate the impact of data dif-\nficulty on policy optimization, positing that sam-\nples challenging for the SFT model provide richer\nlearning signals. To this end, we partition the RL\ntraining pool into Level 2 (pass rate < 50%) and\nLevel 1 (pass rate 50%). As shown in Table 5,\ntraining on hard samples consistently outperforms\nthe easy data setting. Specifically, for Qwen2.5-\nVL-7B, the hard data setting achieves a +5.94 av-\nModel Setting\nAvg.\nFVQA\nInfo\nSimple\nMM\nLive\nDyn\nBase Model: Qwen2.5-VL-7B\nRL w/ Leval 1 Data\n47.28\n64.00\n50.35\n64.66\n50.88\n32.95\n39.94\nRL w/ Leval 2 Data 53.22\n66.78\n51.05\n64.86\n56.73\n37.70\n42.21\n\u2206Gain\n+5.94\n+2.78\n+0.70\n+0.20\n+5.85 +4.75 +2.27\nBase Model: Qwen3-VL-8B-Thinking\nRL w/ Level 1 Data\n48.89\n66.39\n44.95\n64.86\n61.99\n39.55\n45.61\nRL w/ Level 2 Data 52.38\n68.44\n49.05\n66.44\n65.50\n43.85\n48.02\n\u2206Gain\n+3.49\n+2.05\n+4.10\n+1.58\n+3.51 +4.30 +2.41\nTable 5: Ablation study on Complexity-Adaptive RL\ndata construction. Level 2 Data refers to samples with\nSFT model pass rate < 50%, while Level 2 Data in-\ncludes samples with pass rate \u226550%.\nGreen high-\nlights substantial gains (\u22654%). Training on hard sam-\nples consistently yields improvements across both back-\nbones.\nerage gain over the easy setting, with significant\nboosts on benchmarks requiring complex reason-\ning, such as MMSearch (+5.85) and LiveVQA\n(+4.75). Similarly, Qwen3-VL-8B-Think demon-\nstrates a +3.49 average improvement with consis-\ntent gains across all metrics. These results con-\nfirm that while easy samples offer limited room for\npolicy refinement, hard samples present genuine\ndecision-making challenges that maximize the ben-\nefits of exploratory optimization, thereby validating\nour complexity-adaptive data construction strategy.\n5\nConclusion\nWe present Glance-or-Gaze (GoG), a framework\nthat transforms Large Multimodal Models from\npassive observers into active visual planners. GoG\nintroduces the Selective Gaze mechanism to dy-\nnamically allocate attention between global con-\ntext and fine-grained regions, filtering visual noise\nbefore retrieval.\nOur dual-stage training strat-\negy\u2014Reflective Behavior Alignment via SFT fol-\nlowed by Complexity-Adaptive Reinforcement\nLearning\u2014progressively instills cross-modal plan-\nning capabilities and refines decision-making on\nchallenging queries. Experiments across six bench-\nmarks demonstrate state-of-the-art performance,\nwith GoG surpassing strong baselines by substan-\ntial margins. We believe this work establishes a\npromising direction for building more capable and\nautonomous multimodal search agents.\n6\nLimitations\nWhile GoG demonstrates strong performance\nacross diverse benchmarks, several aspects war-\nrant further investigation. First, although we have\noptimized the stability of our search infrastructure\nand Jina Reader pipeline, we observe an occasional\nfailure rate of approximately 1\u20135% due to network\ninstability, API timeouts, or malformed webpage\ncontent. Such failures may cause incomplete in-\nformation retrieval and degrade answer quality in\naffected cases. Second, our experiments primar-\nily focus on English-language benchmarks, and\nthe generalization of GoG to multilingual or cross-\nlingual visual question answering remains unex-\nplored. Extending the framework to support diverse\nlanguages and culturally specific visual knowledge\npresents an interesting direction for future work.\nReferences\nSandhini Agarwal, Gretchen Krueger, Jack Clark, Alec\nRadford, Jong Wook Kim, and Miles Brundage. 2021.\nEvaluating clip: towards characterization of broader\ncapabilities and downstream implications.\narXiv\npreprint arXiv:2108.02818.\nShuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen,\nXionghui Chen, Zesen Cheng, Lianghao Deng, Wei\nDing, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhi-\nfang Guo, Qidong Huang, Jie Huang, Fei Huang,\nBinyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng\nLi, and 45 others. 2025a. Qwen3-vl technical report.\nPreprint, arXiv:2511.21631.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wen-\nbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, and 1 others. 2025b. Qwen2. 5-vl\ntechnical report. arXiv preprint arXiv:2502.13923.\nYang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit\nChangpinyo, Alan Ritter, and Ming-Wei Chang. 2023.\nCan pre-trained vision and language models answer\nvisual information-seeking questions? arXiv preprint\narXiv:2302.11713.\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo\nChen, Sen Xing, Muyan Zhong, Qinglong Zhang,\nXizhou Zhu, Lewei Lu, and 1 others. 2024. Internvl:\nScaling up vision foundation models and aligning\nfor generic visual-linguistic tasks. In Proceedings of\nthe IEEE/CVF conference on computer vision and\npattern recognition, pages 24185\u201324198.\nXianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang,\nXiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang,\nJiaheng Liu, Yuying Mai, Yutao Zeng, Zhoufutu\nWen, Ke Jin, Baorui Wang, Weixiao Zhou, Yunhong\nLu, Tongliang Li, Wenhao Huang, and Zhoujun Li.\n2025. Simplevqa: Multimodal factuality evaluation\nfor multimodal large language models.\nPreprint,\narXiv:2502.13059.\nMingyang Fu, Yuyang Peng, Dongping Chen, Ze-\ntong Zhou, Benlin Liu, Yao Wan, Zhou Zhao,\nPhilip S. Yu, and Ranjay Krishna. 2025. Seeking\nand updating with live visual knowledge. Preprint,\narXiv:2504.05288.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, and 1 others. 2022. Lora: Low-rank\nadaptation of large language models. ICLR, 1(2):3.\nZiniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-\nWei Chang, Yizhou Sun, Cordelia Schmid, David A\nRoss, and Alireza Fathi. 2023. Reveal: Retrieval-\naugmented visual-language pre-training with multi-\nsource multimodal knowledge memory. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 23369\u201323379.\nDongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin\nWu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen,\nChaoyou Fu, Guanglu Song, Peng Gao, Yu Liu,\nChunyuan Li, and Hongsheng Li. 2024. Mmsearch:\nBenchmarking the potential of large models as multi-\nmodal search engines. Preprint, arXiv:2409.12959.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, and 1 others. 2020. Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks. Advances\nin neural information processing systems, 33:9459\u2013\n9474.\nGeng Li, Jinglin Xu, Yunzhen Zhao, and Yuxin Peng.\n2025. Dyfo: A training-free dynamic focus visual\nsearch for enhancing lmms in fine-grained visual un-\nderstanding."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper\n\n#### Main Contributions\nThe paper introduces Glance-or-Gaze (GoG), a novel framework that transforms Large Multimodal Models (LMMs) from passive observers to active visual planners. GoG leverages Reinforcement Learning (RL) for iterative reasoning and complexity-adaptive search strategies, enabling more effective and adaptive visual reasoning.\n\n#### Methodology\n**Stage 1: Reflective GoG Behavior Alignment**\n- **Data Curation**: Curates a dataset called GoG-Instruct through three steps involving uncertainty-aware filtering, human-verified trajectory synthesis, and supervised fine-tuning to instill active selection and cross-modal reflection.\n- **Training Objective**: Uses Qwen2.5-VL-7B-Instruct as the base model for multi-turn conversations containing reasoning sequences and structured tool calls.\n\n**Stage 2: Complexity-Adaptive Reinforcement Learning**\n- **Data Stratification**: Constructs complexity-stratified data by evaluating GoG-SFT models on raw queries and stratifying them into two difficulty levels (Level 1 & Level 2).\n- **Training Objective**: Utilizes Group Relative Policy Optimization (GRPO) to optimize the policy for robustness, enabling adaptive search based on visual query complexity.\n\n#### Key Findings\n- **Robust Generalization**: GoG achieves state-of-the-art performance across diverse benchmarks, outperforming strong baselines by significant margins.\n- **Effectiveness of Selective Gaze**: RL training sharpens Selective Gaze to identify answer-relevant regions more precisely. It enhances the model's ability to select relevant image regions and improves error awareness.\n- **Complexity-Adaptive RL Training**: Hard samples consistently outperform easy data settings, especially on benchmarks requiring complex reasoning like MMSearch and LiveVQA.\n\n#### Conclusions\nThe paper establishes Glance-or-Gaze (GoG) as a promising framework for building more capable and autonomous multimodal search agents. The proposed approach addresses the limitations of existing methodologies by integrating active visual exploration with reflective reasoning, leading to superior performance across diverse benchmarks.\n\n### Limitations\n- **Search Infrastructure Stability**: Occasional failures due to network instability or malformed content may degrade answer quality.\n- **Generalization to Multilingual and Cross-Lingual Settings**: The framework's generalization to multilingual or cross-lingual visual question answering remains unexplored, presenting an interesting direction for future work.\n\nThis summary encapsulates the main contributions, methodology, key findings, and conclusions of the paper on Glance-or-Gaze (GoG)."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Remapping and navigation of an embedding space via error\nminimization: a fundamental organizational principle of\ncognition in natural and artificial systems\nBenedikt Hartl1,\u2020 , L\u00e9o Pio-Lopez1,\u2020 , Chris Fields1 , and Michael Levin1,2, ,*\n1Allen Discovery Center at Tufts University, Medford, MA, USA\n2Wyss Institute for Biologically Inspired Engineering at Harvard University, Boston, MA, USA\n\u2020Authors contributed equally.\n*Corresponding Author: michael.levin@tufts.edu\nKeywords: Evolution, Development, Intelligence, Active Inference, Navigation Policy, Nested\nEmbedding Spaces\nRunning Title: Remapping and Navigating Embedding Space\n1\narXiv:2601.14096v1  [cs.AI]  20 Jan 2026\nAbstract\nThe emerging field of diverse intelligence seeks an integrated view of problem-solving in\nagents of very different provenance, composition, and substrates. From subcellular chem-\nical networks to swarms of organisms, and across evolved, engineered, and chimeric sys-\ntems, it is hypothesized that scale-invariant principles of decision-making can be discovered.\nWe propose that cognition in both natural and synthetic systems can be characterized and\nunderstood by the interplay between two equally important invariants: (i) the remapping of\nembedding spaces, and (ii) the navigation within these spaces. Biological collectives, from\nsingle cells to entire organisms (and beyond), remap transcriptional, morphological, physi-\nological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating\nthese spaces through distributed error correction. Modern Artificial Intelligence (AI) systems,\nincluding transformers, diffusion models, and neural cellular automata enact analogous pro-\ncesses by remapping data into latent embeddings and refining them iteratively through con-\ntextualization. We argue that this dual principle \u2013 remapping and navigation of embedding\nspaces via iterative error minimization \u2013 constitutes a substrate-independent invariant of cog-\nnition. Recognizing this shared mechanism not only illuminates deep parallels between living\nsystems and artificial models, but also provides a unifying framework for engineering adaptive\nintelligence across scales.\n1\nIntroduction\nConventional behavioral science tends to focus on animals with brains and nervous systems.\nThree main lines of thought suggest that this might be too narrow, as schematically illustrated in\nFigure 1. First, an operationalist stance suggests that cognitive terms are interaction protocols,\nnot claims about the essential nature of specific substrates such as brains1\u201312. In other words,\nan assessment of cognition within any system is, operationally, just a testable hypothesis about\nwhether, for example, the system is most amenable to techniques that rewire its hardware, reset\nits homeostatic goals, train it via rewards and punishments, or convince it via cogent reasons\nand psychological strategies. From that perspective, a system\u2019s position on the spectrum of\npersuadability1 is an empirical question. This operational view facilitates the application of tools\nfrom the behavioral scientist\u2019s toolbox5 to a wide range of complex, simple, and even non-living\nsystems to see which formal interaction model offers the most effective prediction and control.\nThe second line of thought points out that the mechanisms implementing intelligence in the brain\nare evolutionarily homologous to those in the non-neural living substrates. Ion channels, elec-\ntrical synapses, synaptic machinery, and the algorithms they embody are evolutionarily ancient,\nand ubiquitous, raising the question of when precisely they became fodder for cognitive tasks\n(albeit in unconventional contexts), in the long history prior to the formation of brains8. Finally,\nthird, developmental biology reveals that familiar bioelectric networks play key roles in coordi-\nnating cellular activities toward specific anatomical endpoints, long before nerves appear during\nmorphogenesis, again pointing to the need for models of scaling of competencies from those of\nthe single blastomere to those of an adult13; examples are illustrated in Figure 1 (A).\nThe emerging field of Diverse Intelligence4,7,14\u201316 seeks to understand what all active agents\nhave in common. In moving away from classic assumptions that brains, and a history of evolu-\ntionary trial-and-error for behavior in 3D space, are privileged substrates and origin stories for\ncognition, it seeks a unifying theory for agents of highly diverse composition and provenance2.\nEvolved, engineered, and hybrid systems at all scales of complexity are fodder for experiments\nin learning, problem-solving, and decision-making3,17. Progress in this field suggests that neu-\nroscience is not about neurons \u2013 it\u2019s about biophysical processes and mechanisms that operate\nas a kind of \u201ccognitive glue\u201d, binding competent subunits (e.g., cells) into collective intelligences\n2\nthat operate in spaces and with spatio-temporal goal horizons (cognitive light cones) much larger\nthan those of their parts18.\nThis research program has three unique key features. It is deeply interdisciplinary \u2013 revealing\nnovel phenomena at the intersection of biology, computer science, materials science, and cogni-\ntive science. It is tractable \u2013 the tools of behavioral, computational, and cognitive neuroscience\nhave now been applied far outside the brain and avail the modern worker with very effective tools\nwith which to probe the primitive \u2013 or perhaps surprisingly sophisticated \u2013 minds of unconven-\ntional systems. And finally \u2013 it impacts a critical unmet need and knowledge gap: aspects of\nregenerative medicine, bioengineering, AI, and even fields such as ethics are being enriched by\na novel understanding of the origin, scaling, and properties of unconventional embodied minds,\nsuch as cellular collectives, that can be coaxed to pursue alternative end-states18\u201322.\nThis paper extends the Fields-Levin framework of cognition as competent navigation in arbi-\ntrary spaces2,3 (see Figure 1 (B, C)) by proposing that intelligence is characterized by two equally\nimportant cognitive invariants: the creation and remapping of embedding spaces, and the nav-\nigation within these spaces in response to new contexts. More precisely, we define remap-\nping in this paper as representing and/or updating new information acquired via experience into\nthe internal latent representation formalized by the problem space (i.e., constantly refining a\nnavigable embedding space) \u2013 a concept closely related to internal representations formed by\nworld-models23,24. We argue that embedding spaces are not static and given structures but ac-\ntively constructed representations that translate complex realities into navigable dimensions, as\nschematically illustrated by Figure 1 (D). By recognizing both processes as fundamental to cog-\nnition, we develop a more comprehensive substrate-independent framework for understanding\nintelligence across biological and artificial systems. We demonstrate the explanatory power of\nthis dual framework through examples ranging from biological regeneration to AI systems, show-\ning how the interplay between remapping (via embeddings) and navigation processes enables\ncomplex adaptive behaviors across diverse embodiments of distributed intelligences, agnostic to\nproblem spaces, scale, or material composition. We begin by outlining navigation as an invariant\nof cognition, before turning to remapping, embeddings, and their manifestations in biology and\nAI. We conclude with a perspective on how these principles will foster the development of novel\nadaptable synthetic and chimeric collective intelligences.\n2\nRemapping and navigating arbitrary spaces as an invariant\nof cognition\n2.1\nNavigation in arbitrary spaces as an invariant of cognition\nWilliam James, the pioneering psychologist, offered a goal-directed definition of intelligence: \u201cIn-\ntelligence is a fixed goal with variable means of achieving it\u201d12. It remains particularly prescient\nin the modern era of AI, where populations of software and physical agents are expanding. This\ndefinition also emphasizes that intelligence manifests as goal-directed behavior rather than any\nspecific mechanism or embodiment. The cybernetic tradition, developed by Ashby11, Maturana\nand Varela9, and Pattee10, expanded this perspective by framing cognition in terms of information\nprocessing and self-regulation. Very recently, Fields and Levin expanded further this approach\nand framed cognition as competent navigation in arbitrary spaces2, as illustrated in fig. 1 (C).\nThis framework proposes navigation in a problem space (metabolic, physiological, morphologi-\ncal, or behavioral for biological systems, for example) as an invariant for analyzing cognition in\ndiverse embodiments2. More specifically, in this framework, a goal is a (target) position in a prob-\nlem space, while a system\u2019s degree of competency (or intelligence, in the definition of William\n3\nJames) is reflected by the system\u2019s ability to navigate that space toward a goal state by actively\nminimizing the deviation from diverse starting positions. Such a deliberately substrate-agnostic\nframework allows recognizing common patterns of intelligence across vastly different systems25,\nfrom biological to artificial systems.\nThere are several examples of navigation in spaces in biology, including the metabolic, phys-\niological, transcriptional, morphological or behavioral (3D) spaces. Biological systems are the\nresult of multiscale goal-directed behavior (see Figure 1 (A, B)). These navigational capabilities\nare indeed not isolated but interconnected through hierarchical relationships that enable coordi-\nnation across scales for reaching homeostasis3,26. A key insight of the Fields-Levin framework\nis how higher-level systems influence lower-level ones (see Figure 1 (C, D)). Higher-level sys-\ntems deform the energy landscapes for their subsystems, allowing lower-level components to\nfollow local gradients that achieve goals beneficial to the higher-level system2. This allows the\nscaling of goals and the development of higher cognition27 and creates a situation where compo-\nnents can engage in relatively simple local behaviors while collectively achieving sophisticated\nsystem-level outcomes.\nIntelligence in animals is typically measured by their capabilities to solve problems by navi-\ngating and manipulating objects, and resolving problems in 3D space28\u201333. However, it becomes\nincreasingly obvious that certain (different) levels of agency and cognition need to be attributed\nto the components of all organizational layers across the multiscale competency architecture of\nbiology25 (see Figure 1 (B)), Even cells and cellular collectives solve problems when navigat-\ning transcriptomic space or morphospace \u2013 the space of possible anatomical configurations26 \u2013\nwhich leads to the immense structural and functional plasticity of living matter; selected exam-\nples are visualized in Figure 1 (A) and discussed below.\nThe planarian flatworm provides an interesting example of intelligent navigation in the tran-\nscriptional space34. Exposure to barium, a potassium channel blocker, causes head degen-\neration.\nWithin the same specimen, planaria regenerate new heads with the transcriptional\nchanges needed to achieve barium resistance35.\nHow can groups of cells identify which of\ntens of thousands of potential transcriptional effectors will solve their physiological stressor?\nMoreover, barium is not something planaria encountered in the wild, suggesting cells possess\ngeneral problem-solving capabilities, not only genetically pre-programmed responses to specific\nchallenges26. The system senses its current position (a disrupted physiological state), compares\nit to a preferred position (normal function), and moves through transcriptional space by modifying\ngene expression until it reaches a new attractor region that restores homeostasis. There is not\nenough time to do this by trial and error, and some sort of heuristic must be used to identify\nsolutions to novel problems in this (and other) spaces that life thrives in. Even below cell level,\ngene regulatory networks are capable of various kinds of learning and memory36,37.\nDuring development and regeneration, cellular collectives navigate morphospace with high\nrobustness, coordinating their activities to achieve specific target morphologies despite varying\nstarting conditions or perturbations13,26. Salamanders can regenerate limbs from arbitrary am-\nputation planes38, cellular collectives in a tadpole\u2019s disrupted \u201cPicasso face\u201d reorganize to form\nnormal frog facial structures39, and planaria can regenerate heads with species-specific mor-\nphologies without genomic editing35,40. These processes may involve cellular collectives actively\nsensing their current morphological state, comparing it to the target morphology, and executing\ncollective cellular decision-making to reduce the error, probably using biochemical, mechanical\nand bioelectrical cues26. The cellular collective constructs a distributed representation of both its\ncurrent and target morphology, then navigates through morphospace using local cell behaviors\nthat collectively produce global pattern formation41. The Fields-Levin framework defines these\nphenomena as navigation in morphospace toward specific attractor regions2 (c.f., Figure 1 (C)).\nThis framework is not restricted to cells that form tissues or morphologies, but scales across\nall organizational layers of biology: distributed agents perceive and act locally, following their in-\n4\ndividual agenda that is aligned with, or constrained by, a collective higher-level objective. More-\nover, navigation in a problem space is not limited to biological systems, Large Language Models\n(LLMs) navigate the language space42,43, robots navigate motor spaces44, multimodal models\nare navigating a merged space of different modalities45, etc.\nNavigation in arbitrary spaces, from morphogenesis to generative AI, reveals a common\nsubstrate-independent principle of cognition. Crucially, however, the state spaces are not static\nbackdrops: they are continually constructed and reshaped by the systems that use them un-\nder novel contexts. Through this lens, remapping becomes inseparable from navigation, and\ntogether they constitute dual invariants of intelligence, as we turn to below.\n2.2\nNavigation implies remapping for learning systems\nAs we just saw, competent navigation in a problem space is a hallmark of cognition in the Fields-\nLevin framework2,3. However, any learning system must remap and update this problem space\nwhen it acquires new knowledge on the internal and external worlds. We believe remapping the\nproblem space cannot be separated from competent navigation in this space to define cognition\nand intelligence. Remapping, according to the above of definition \u2013 as representing and/or up-\ndating new information acquired via experience into the internal representation formalized by the\nproblem space \u2013 combines learning with experience-dependent modification of the state space\nin which learning takes place. In biological systems, remapping occurs at several levels. Neu-\nral plasticity, for example, represents a remapping of the brain\u2019s memories and representational\nspace, where connections and weightings are reconfigured based on experience, effectively\nchanging not only what is thought, but also the landscape of possible thought patterns46,47. Spe-\ncific neural structures do the same, for example, the hippocampus demonstrates this kind of\nremapping for navigation in different spaces, creating new internal spatial representations when\nanimals encounter novel environments48. But remapping is not only limited to the brain \u2013 the\nadaptive immune system actively remaps its recognition spaces using VDJ recombination, cre-\nating new antibody configurations that represent previously unencountered antigens49,50. The\nalready described planarian\u2019s transcriptional adaptation to barium demonstrates remapping of\nits internal transcriptional landscape to represent and respond to this novel environmental chem-\nical35. At cellular levels, stem cells while differentiating are remapping completely their transcrip-\ntional landscape too51. Even molecular networks can learn and have different types of memory,\nactively updating and remapping their internal representations of the learned information36,37.\nSensorimotor systems provide particularly clear examples of remapping via the creation of\ninternal representations by building neural maps. These maps are found ubiquitously in the\nbrain, where they encode a wide range of behaviorally relevant features into neural space. Neu-\nral maps in the brain exhibit different organizational patterns. Continuous maps maintain spa-\ntial relationships where neighboring neurons encode similar stimulus properties. For example,\nretinotopic maps in visual processing areas preserve spatial relationships from the retina52, and\nneurons in tonotopic maps are organized following sound frequency gradients, with neighboring\ncells responding to similar frequencies53. Discrete maps, by contrast, create distinct functional\nmodules without gradual transitions. The olfactory system is one of them: olfactory receptors\nlocated throughout the nasal cavity connect to specific neural clusters called glomeruli in the\nolfactory bulb, with each receptor type linking to its own dedicated glomerular group54. Many\nbrain regions combine both organizational strategies, for example somatosensory areas pro-\ncessing touch and regions controlling movement typically integrate continuous gradients with\ndiscrete frontiers, creating hybrid mapping systems that balance precision with flexibility within\ntwo-dimensional cortical surfaces55. Critical period plasticity is also a remapping, where sensory\nmaps are reconstructed to represent altered input patterns during development56.\n5\nThe evolutionary perspective reveals remapping as fundamental to adaptation. In fact, as our\nsenses have evolved, we have seen new maps for translating external energy\u2014such as light,\nsound, and chemicals\u2014into internal neural representations. The development of bat echolo-\ncation, for instance, necessitated remapping auditory processing in order to produce intricate\ninternal spatial maps from external acoustic reflections57. These sensory maps are continually\nbeing updated and revised. Primary sensory cortices constantly remap the organization of their\nreceptive fields to reflect changes in the environment58. Cross-modal plasticity after sensory loss\nshows a drastic remapping. For example, in blind subjects, the visual cortex literally represents\nexternal auditory information in the previously visual neural cortices by creating new internal\nmaps of auditory space in these cortical areas59. When learning new primitives and movement\npattern sequences, or when internalizing external tool representations, the motor system also\nexhibits remapping. Indeed, the motor cortex develops new maps that represent the relationship\nbetween intended actions and their effects in the external world60,61. Maybe even more remark-\nable, brain-machine interfaces demonstrate remapping in real-time, as motor cortex represents\nexternal device control, mapping internal neural activity onto external robotic movements62.\nJust as biological systems construct and revise internal maps to maintain homeostasis and\nadapt to novel challenges, AI systems build and remap data into structured spaces, via embed-\ndings. These parallels suggest that remapping is a general strategy for intelligence, irrespective\nof substrate.\nWe now turn to AI systems to illustrate this convergence.\nLLMs for instance,\ngenerate embeddings on a vector space that are constantly updated with new data (c.f., Fig-\nure 2). Embeddings are vector representations of tokens, and distances and directions between\nthem represent semantic relationships in the text63,64. Computer vision systems transform visual\nscenes into hierarchical maps that show external objects together with their textures and spa-\ntial connections65. The internal policy maps of reinforcement learning systems demonstrate how\nexternal states connect to optimal actions66,67. Transformers\u2019 attention algorithms generate inter-\nnal maps of token relevance which represent the external context structure that matters for each\nprocessing step68. Generative models in AI transform external data distributions into internal\nlatent spaces to create compressed representations of external patterns69,70. Robots produce\nactual navigation maps when they explore new environments71. Few-shot learning models ex-\nhibit fast remapping capabilities through their ability to develop new internal representations that\nconnect new classes to existing representational spaces72. Meta-learning systems possess ad-\nvanced remapping abilities which allow them to establish new internal maps for external tasks\nby choosing appropriate learning methods73.\nThrough remapping at all levels, learning systems can discover new representational schemes\nthat capture previously unrecognized external patterns, create internal maps that efficiently com-\npress external complexity, and integrate information from heterogeneous external sources into\nunified internal maps/spaces74\u201378. By recognizing remapping as the fundamental process of\ncompetently navigating a problem space, we develop a more precise substrate-independent\nframework for understanding intelligence across biological and artificial systems2,26.\n3\nA common mechanism between AI and biological systems?\nRemapping via embeddings.\nThe kinds of remapping observed in biological systems and employed in AI systems are not arbi-\ntrary, but rather appear tightly controlled. What constrains remappings, and how is this constraint\ninformation encoded by the system doing the remapping? We suggest here that coarse-graining\nembeddings provide a general mechanism for constraining remappings, one that can enforce\nstrong constraints without representing them explicitly. Embedding a high-dimensional parent\n6\nspace \u0393 into a lower-dimensional latent space \u039e via a structure-preserving (i.e. non-trivial) map\n\u03be : \u0393 ,\u2192\u039e selects, in particular, parent-space structures or relations that are inverse images,\nunder \u03be, of structures or relations in the smaller latent space (c.f., schmatic illustration in Fig-\nure 1 (C, D)). We first examine embeddings of biological systems into 3D space, then consider\nmore general embeddings available to AI systems.\n3.1\nEmbedding in 3D space as a remapping constraint\nBiological systems continually traverse metabolic and gene-expression (i.e. transcriptomic and\nproteomic) spaces, and in doing so, sometimes also explore morphological space. These spaces\ncan be very high-dimensional, involving tens to hundreds of thousands of chemical potentials,\neach with dynamic ranges of at least an order of magnitude. The vast majority of states in these\nbiochemical state spaces are not just far from homeostatic ranges but inconsistent with life, e.g.\nstates in which most proteins are denatured or states in which essential enzymes are deficient.\nA latent space specific to living systems, therefore, must encode a set of constraining relations\nthat appears to be combinatorially explosive. If we knew this latent space and the embedding\nmap to it, biochemistry would be finished.\nNature does not, however, have to know either the latent space or the embedding. Nature\nembeds biochemistry in 3D space, and selects for embedding maps from chemical-potential\nspace to bounded volumes in 3D space with a simple Galilean metric. Almost all of these maps\ndo not work, but some do. The maps that work implicitly encode constraints on molecular rela-\ntions that are consistent with life. Some of these constraints are extremely tight, forcing some\npairs of molecules to be and remain in close proximity in 3D space. Others are much looser,\nallowing molecules to diffuse through the highly-structured 3D space specified by the tight con-\nstraints. We, as biochemists, try to figure out those implicit constraints by doing experiments on\nthe systems that we observe, i.e. those that \u201cwork\u201d in 3D.\nEmbedding biochemistry in 3D converts navigation in chemical-potential space into molec-\nular behavior in 3D space, with the constrains imposed by the embedding map manifested as\nbehavioral constraints on the embedded molecules. Protein folding, for example, is highly con-\nstrained by ionic and van der Waals interactions that are defined in 3D space, not in the parent\nconcentration space. Binding pockets, hydrophobic interactions, and diffusion are, similarly, spa-\ntial concepts that express navigational biases when pulled back to the parent space.\nThe idea of embeddings as imposing constraints can be made precise in the language of\nsheaf theory. Sheaves are a category-theoretic description of what is common to, for example,\nvector fields, maps or geographic information systems, databases, and other coherent ways of\n\u201cattaching\u201d structured data to each point or \u201clocation\u201d in some space79,80; see Supplementary\nInformation for formal definitions. Coherence is enforced by requiring, roughly speaking, that\nassignments of data to different regions agree wherever the regions overlap, and that any way of\nbuilding up the data for a region by assigning data to subregions agrees with every other way of\nbuilding up the data for that region by assigning data to subregions. Letting the \u201cspace\u201d to which\ndata are attached be 3D space extended along a time axis, i.e. a 4D spacetime, a coherent\nembedding of biochemistry is one in which every state and state change can be implemented by\na time-evolving 3D structure in a way that respects all local 3D constraints, e.g. the structures\nof other nearby molecules. Note that coherence in this sense is a necessary, but not sufficient,\ncriterion for viability. Both the biochemistry of live cells and that of lysed, dying cells must embed\nin 4D.\nProbability distributions are assignments of data \u2013 probabilities \u2013 to underlying data sets\nor spaces. Violations of the coherence conditions correspond to violations of the Kolmogorov\naxioms, e.g. local additivity of probabilities. Abramsky and Branderburger81 have shown that\n7\ndata violating the sheaf coherence conditions can be viewed as outcomes of measurements,\nor in general, processes exhibiting noncausal context dependence, e.g. processes that exhibit\nquantum interference. In the current context, cellular processes that mutually interfere will ap-\npear \u201cnon-local\u201d in a 4D embedding, and will be disrupted (or \u201cdecohered\u201d in quantum-theoretic\nlanguage) by measurements or other processes that enforce locality82.\n3.2\nMulti-scale embeddings and coherence conditions\nThe foregoing says nothing about the scale of the embedding; different 4D embedding scales for\nbiochemistry, for example, capture molecular dynamics at different resolutions83. Embeddings\nof the same data at different resolutions are coherent if, but only if, the higher-resolution embed-\ndings coarse-grain to the lower resolution embeddings via some function that does not depend\non the embedding-space coordinates. High-resolution constraints that do not \u201csmear out\u201d at\nlower resolution may appear as \u201cemergent structures\u201d or \u201cemergent boundary conditions\u201d in the\nlower-resolution description84; for example, networks of interactions that, at high resolution, limit\nfree diffusion may appear as domain walls or mesoscale structures at low resolution.\nIn the context of the Fields-Levin model2, spatial embeddings may extend beyond the cellular\nscale to those of tissues, complex multicellular bodies, or communities of interacting organ-\nisms. These scale changes introduce \u201cnatural\u201d spatial boundaries, e.g. the boundary of a tissue\nor an individual organism, that function as emergent boundary conditions on macroscopic be-\nhavior. One effect of these boundary conditions is to enforce spatial symmetries by enforcing\nspatial coupling between micro- to mesoscale systems and their local environments; organismal\nboundaries, for example, keep the parts of an organism together by assuring that they are trans-\nported through space together. Processes that would not be translationally invariant without such\nboundaries \u2013 e.g. a GRN moved from inside an organism to the ambient inter-organism environ-\nment \u2013 become effectively translationally invariant because their local environments move with\nthem. These boundary-imposed symmetries render \u201cspace\u201d and hence the spatial embedding\nfunctionally meaningful at larger scales.\nIt is this preservation of effective spatial symmetries by emergent boundaries that allow pro-\ncesses as well as structures to be spatially embedded. This is true not just of biochemical, e.g.\nmetabolic or signal transduction, bioelectric, and mechanical processes, but also of cognitive\nprocesses. Active inference85, for example, can only be maintained in a reasonably familiar and\nhence reasonably predictable environment; completely unfamiliar environments \u2013 e.g. environ-\nments involving radically different chemical potentials, EM fields, particle fluxes, or mechanical\nforces \u2013 tend to induce Markov blanket collapse, generative-model dysregulation, and death.\nFrom this perspective, the usual inferences from cognition to niche construction and from more\ncomplex, e.g. human cognition to (often by analogy) less complex or basal cognition appear both\ncounter-intuitive and unmotivated. Small, low-complexity environments are both more probable\nand easier to maintain than large, high-complexity environments, and relative environmental sta-\nbility is as essential for cognition as it is for metabolism. The former, like the latter, can be\nexpected both to start out and to be widespread in small, relatively low-complexity systems cou-\npled tightly to and spatially transported with small, relatively low-complexity environments, the\nmaintenance of which becomes the primary objective of active inference by the systems inhab-\niting them. These conditions are met at the scales of functional networks within cells, including\nGRNs, single cells including prokaryotes, and communities such as microbial mats. Coherent\nspatial embedding, and hence the possibility of active inference, starts small and simple, and\nexpands to larger scales as larger-scale spatial constraints emerge.\nThe principles observed in biological systems \u2013 navigation of attractor landscapes, local error\ncorrection that scales to system-level outcomes, and nested embeddings that support higher-\n8\nlevel organization \u2013 exemplify scale-free cognitive dynamics where local competition and coordi-\nnation give rise to the abstraction of intelligence across scales. These principles are not unique\nto living tissue but also apply to artificial systems, as discussed in the following.\n3.3\nRemapping and Navigation of Embedding Spaces in Modern AI\nAbove, we discussed fundamental organizational principles in biological systems based on dy-\nnamic, context-sensitive remapping of embedding representations. AI is undergoing a similar\nevolution. Traditional machine learning models, such as classifiers or auto-encoders, often ac-\nquire fixed feature vectors and latent-space representations representable as data manifolds65.\nThese learned representations performed well for narrowly defined tasks but lacked the flexibility\nto adapt to new situations in broader contexts and across multimodal data.\nInstead of relying on static representations, modern AI increasingly relies on generative mod-\nels86 \u2013 models that learn the underlying distribution of an often vast amount of training data \u2013\nto construct dynamic and context-sensitive embedding spaces. So-called foundational models\ntrained on \u201cinternet-scale\u201d data can reach remarkable generalization capabilities87,88 (but may\nlack the precision required for narrow tasks due to \u201cover-generalization\u201d89). Here, an embedding\nis a mapping from complex, often high-dimensional unstructured data into a structured numer-\nical vector space \u2013 typically still high-dimensional \u2013 that preserves similarities and emphasizes\nsemantic relations in the remapped representation. For example, words with similar meanings\nare mapped closely together, and associated sequences of words are often parallel trajectories\nin the embedding spaces of language models90 (see Figure 2).\nIn transformer architectures68, this is realized by an attention mechanism91. Each attention\nlayer computes how tokens relate to one another in the current context (the current text input\nsequence) and updates their embeddings accordingly. Crucially, this remapping of token em-\nbeddings updates the input token sequence during the pass through the model, i.e., it does not\nreplace the latter: Across a hierarchy of attention layers, token embeddings are thus remapped -\nor refined - incrementally to create increasingly contextually enriched token representations for a\ngiven input sequence. Moreover, transformers maintain several attention heads. Those are sen-\nsitive to particular modalities in the data and can thus be thought of as filtering the embedding\nspace in parallel for meaningful relationships across certain semantic features, much like how\nneural circuits map multimodal sensory inputs. This allows transformers to reason about relation-\nships and contexts through navigation in dynamically remapped embedding spaces. Strikingly,\nthese transformer architectures can also be applied to the vision domain92, and the learned vi-\nsion embeddings can be repurposed \u2013 understood by93 \u2013 language models to form powerful\nmultimodal embedding models94.\nRecently95,96, the self-attention mechanism in transformers has been shown to be mathe-\nmatically equivalent to a form of continuous Hopfield network. The latter can store a large \u2013\npotentially exponential \u2013 variety of data patterns as attractor states in a high-dimensional en-\nergy landscape. Upon noisy or even novel inputs, Hopfield networks retrieve the closest stored\npattern and thus act as associative memories97 of their training data. Notably, this happens via\ninternal energy minimization dynamics, i.e., equilibration into attractor states in high-dimensional\nassociative embedding spaces. On the one hand this implements a form of pattern-level (global)\nerror correction. On the other hand, the hierarchical attention layers act as a kind of iterative\nassociative memory retrieval under a given context, selecting and refining information that best\nmatches the input query among stored patterns.\nThe related concept of hyperdimensional computing98 (HDC) is built on the principle that in-\nformation can be stored and manipulated in extremely high-dimensional spaces \u2013 often 10,000+\ndimensions \u2013 where different concepts are robustly represented as orthogonal vectors across all\n9\ncoordinates. In particular, HDC utilizes algebraic operations to superimpose elemental concepts\nto construct composite representations (e.g."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Academic Paper\n\n**Title:** Remapping and Navigation of an Embedding Space via Error Minimization: A Fundamental Organizational Principle of Cognition in Natural and Artificial Systems\n\n#### Main Contributions:\nThe paper introduces a novel framework for understanding cognition, proposing that both natural and artificial systems can be characterized by two equally important cognitive invariants: the creation and remapping of embedding spaces, and the navigation within these spaces. The authors argue this dual principle constitutes a substrate-independent invariant of cognition.\n\n**Methodology:**\n1. **Framework Development:** The researchers propose a framework where intelligence is defined as competent navigation in arbitrary spaces (Figure 1). This framework emphasizes that cognitive processes involve remapping existing information into new representations and navigating within these spaces to achieve goals.\n2. **Remapping Concept:** Remapping is defined as representing and/or updating new information acquired via experience into the internal latent representation formalized by the problem space, akin to internal world models (Figure 1).\n3. **Biological Examples:** The authors provide examples from biological systems such as planarians navigating transcriptional spaces, salamanders regenerating limbs, and cellular collectives coordinating activities across scales.\n4. **AI Systems:** The paper extends this framework to artificial intelligence systems like Large Language Models (LLMs), which generate embeddings that are constantly updated with new data.\n\n**Key Findings:**\n1. **Remapping in Biological Systems:** Natural systems remap their internal representations of complex realities into navigable dimensions, such as transcriptional and morphological spaces.\n2. **AI Systems:** Modern AI systems also employ similar processes by remapping data into latent embeddings and refining them iteratively through contextualization.\n3. **General Mechanism:** The authors propose that coarse-graining embeddings provides a general mechanism for constraining remappings without explicitly representing constraints.\n\n**Conclusions:**\n1. **Unified Framework:** The proposed framework integrates insights from biology and AI, providing a unifying theory of cognition across diverse systems.\n2. **Practical Implications:** This dual principle offers new tools for engineering adaptive intelligence in various domains including regenerative medicine, bioengineering, and artificial intelligence.\n3. **Future Directions:** The authors suggest that recognizing this shared mechanism can help illuminate deep parallels between living systems and artificial models, paving the way for further research.\n\n### Summary Length: 300 words\n\nThis summary encapsulates the main points of the paper, including its key contributions, methodology, findings, and conclusions in a concise manner."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Domain-Adaptation through Synthetic Data:\nFine-Tuning Large Language Models for German Law\nAli Hamza Bashir1,2,3 , Muhammad Rehan Khalid2 , Kostadin Cvejoski5 , Jana Birr1 ,\nJule Berghaus4 , Armin Berger1,3,4 , Sandra Halscheidt1 , Christian Temath1 ,\nRafet Sifa1,3,4 , David Berghaus1,3\n1Fraunhofer IAIS\n2Georg-August-University G\u00a8ottingen\n3Lamarr Institute\n4University of Bonn\n5JetBrains Research\nAbstract\nLarge language models (LLMs) often struggle in\nspecialized domains such as legal reasoning due\nto limited expert knowledge, resulting in factu-\nally incorrect outputs or hallucinations. This paper\npresents an effective method for adapting advanced\nLLMs to German legal question answering through\na novel synthetic data generation approach. In con-\ntrast to costly human-annotated resources or unre-\nliable synthetic alternatives, our approach system-\natically produces high-quality, diverse, and legally\naccurate question-answer pairs directly from au-\nthoritative German statutes.\nUsing rigorous au-\ntomated filtering methods and parameter-efficient\nfine-tuning techniques, we demonstrate that LLMs\nadapted with our synthetic dataset significantly out-\nperform their baseline counterparts on German le-\ngal question answering tasks. Our results highlight\nthe feasibility of using carefully designed synthetic\ndata as a robust alternative to manual annotation in\nhigh-stakes, knowledge-intensive domains.\n1\nIntroduction\nLLMs achieve strong performance on many general NLP\ntasks, yet reliability remains a key obstacle in high-stakes do-\nmains such as law. Legal question answering requires faith-\nful grounding in authoritative sources and careful handling of\nlegal references. Hallucinations or unsupported claims can\ntherefore be harmful [Hu et al., 2025; Dahl et al., 2024].\nBuilding reliable legal QA systems is particularly chal-\nlenging outside English. Multilingual legal benchmarks show\nsubstantial performance gaps across languages and tasks, re-\nflecting an ecosystem in which resources, benchmarks, and\nmodel developments have been disproportionately English-\ncentric [Niklaus et al., 2023]. For German civil law, [B\u00a8uttner\nand Habernal, 2024] introduce GerLayQA, a valuable dataset\ngrounded in the B\u00a8urgerliches Gesetzbuch (BGB). However,\nthis dataset has been crawled from a legal forum and the li-\ncensing is therefore debatable, which limits its practical use.\nAs a result, there is a practical need for scalable, statute-\ngrounded training data that can be used to adapt open LLMs\nto German law.\nWe address this need by synthesizing German legal instruc-\ntion data directly from statutory text and therefore avoid any\nlicensing issues. Using a curated corpus of German laws [Eu-\nropean Language Resources Association (ELRA), 2023], we\ngenerate question-answer pairs that are explicitly grounded in\nspecific provisions.\nHowever, such an approach is challenging because we need\nto synthesize data that is diverse enough to capture real-world\nscenarios that have not been encountered during training and\nadditionally not destroy the general purpose performance of\nthe model. Moreover, we found that naive data generation\napproaches can even lead to diminishing performance on val-\nidation datasets that are within the distribution of the training\ndata.\nWe explore two different data generation approaches: (i) a\nsimple instruction-style generation setup and (ii) a difficulty-\ngraded strategy that elicits questions ranging from direct\nlookups to scenario-based and multi-section reasoning, in-\nspired by self-instruction style pipelines [Taori and et al.,\n2023]. To improve factual consistency, we add an LLM-based\nreview stage that filters unanswerable, unsupported, or redun-\ndant QA pairs.\nUsing this statute-grounded data, we adapt two mod-\nern open-source LLMs to German legal QA. Namely, we\nfine-tune LLaMA 3.1 (8B)1 and Gemma 3 (12B)2 using\nparameter-efficient LoRA [Hu et al., 2021].\nDue to the lack of evaluation datasets for instruction-tuned\nLLMs on German legal tasks, we created four different eval-\nuation datasets.\nTwo datasets are based on the German-\ncivil-law (BGB). One of these is created in a multiple-\nchoice format, avoiding any bias in the grading protocol.\nThe second one contains open-ended questions which we\ngrade using an LLM-as-a-judge approach [Zheng et al., 2023;\nLiu et al., 2023].\nWhile we ensure that the evaluation dataset only contains\n1https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n2https://huggingface.co/google/gemma-3-12b-it\narXiv:2601.14160v1  [cs.CL]  20 Jan 2026\n\u00a7\nA\nStatute\nsection\nexample: BGB \u00a7433\nDifficulty graded generation\nB\nLevel 1\nClause\nDefinition and scope\n?\nQA\n\u2713\nLevel 2\nParaphrase\nClient style phrasing\n?\nQA\n\u2713\nLevel 3\nScenario\nApply to a fact pattern\n?\nQA\n\u2713\nLevel 4\nMulti section\nCombine provisions\n?\nQA\n\u2713\nConstraints\n\u2713German questions and answers\n\u2713Answer cites law and section\n\u2713No external law beyond the text\nReviewer\nfilter\nC\n\u2713Answerable\nfrom text\n\u2713Supported\nanswer\n\u2713Non redundant\nDeduplication\n{\nJSON\n}\n{\n}\nD\nFinal SFT\ndataset\nstatute grounded\nQA pairs (JSON)\nused for LoRA\nfine tuning\nRejected\nsamples\nFigure 1: Difficulty-Graded QA Data pipeline. A statute section serves as ground truth input. The generator produces complementary\nquestion styles at increasing difficulty (clause, paraphrase, scenario, multi-section) while enforcing statute-only constraints. A reviewer\nmodel filters unanswerable, unsupported, or redundant pairs and performs deduplication. The retained question\u2013answer pairs constitute the\nsupervised fine-tuning (SFT) dataset used for LoRA adaptation.\nquestions that were not seen during training, we also want\nto investigate the performance of the models in real-world\nscenarios that are completely out-of-distribution. For this,\nwe create two evaluation dataset variants (again a multiple-\nchoice and an open-ended one) based on the German sub-\nset of LegalMC4 [European Language Resources Association\n(ELRA), 2023], which is a crawled collection of legal docu-\nments from the internet.\nRemarkably, across all test sets, the difficulty-graded gen-\neration plus filtering yields consistent gains over both the base\nmodels and the simpler generation baseline while preserving\ngeneral capabilities.\nOur contributions are:\n\u2022 We propose a practical pipeline to synthesize German legal\nQA data for instruction-tuning. Our pipeline works directly\nfrom legislative texts, avoiding the reuse and privacy limi-\ntations common to web-crawled datasets.\n\u2022 We introduce a difficulty-graded generation strategy to-\ngether with an LLM-based filtering stage that improves\ngrounding and factual consistency of synthetic QA pairs.\n\u2022 We provide an extensive evaluation of two open LLMs\nadapted to German law, demonstrating substantial accuracy\nimprovements on held-out German legal QA benchmarks\nwith no noticeable degradation on general performance.\nThe fine-tuned models and synthetic datasets will be re-\nleased upon acceptance of this work.\n2\nRelated Work\nLegal-domain language models and grounding.\nLegal\nNLP has long benefited from domain adaptation of general-\npurpose language models.\nEarly work focused on contin-\nued pre-training of encoder models on legal corpora, e.g.,\nLegal-BERT [Chalkidis et al., 2020] and CaseLaw-BERT\n[Zheng et al., 2021], improving performance on classifica-\ntion, retrieval, and judgment-related tasks within specific ju-\nrisdictions. With the shift toward instruction-tuned generative\nLLMs, recent work has specialized models for legal assis-\ntance via continued pre-training or supervised instruction tun-\ning. Examples for Chinese law include [Guo and et al., 2023;\nSong, 2023] and examples for compliance checks include\n[Bell et al., 2024]. A complementary line of research tar-\ngets reliability through grounding, for instance via retrieval\naugmented generation (RAG) that conditions generation on\nexternal legal sources such as statutes or case repositories\n[Huang et al., 2023; Chen et al., 2023; Hillebrand et al.,\n2024]. Despite these advances, factual errors and hallucina-\ntions remain central concerns in legal generation, motivating\nstronger adaptation and verification techniques [Dahl et al.,\n2024; Hu et al., 2025].\nGerman legal NLP resources and multilingual bench-\nmarks.\nCompared\nto\nEnglish\nand\nChinese,\npublicly\nreusable resources for German legal QA remain limited. Ger-\nLayQA [B\u00a8uttner and Habernal, 2024] constitutes an impor-\ntant step by providing German layperson questions with ex-\npert answers grounded in the BGB. However, forum-derived\ndatasets can face practical reuse constraints (e.g., licensing\nand privacy considerations), and may not always be suitable\nas training data for broad model adaptation. More broadly,\nmultilingual legal benchmarks such as LEXTREME [Niklaus\net al., 2023] highlight persistent performance gaps across\nlanguages (including German), but focus primarily on tasks\nsuch as classification and extraction rather than open-ended,\nstatute-grounded answer generation. These limitations moti-\nvate approaches that create scalable supervision specifically\naligned with German statute-based QA.\nTraining on synthetic data.\nTraining on synthetic data is\na common concept in machine learning and has been ap-\nplied in a variety of contexts such as tabular data [Holl-\nmann et al., 2023; Hollmann et al., 2025], robotics [To-\nbin et al., 2017] and time-series [Berghaus et al., 2025a;\nBerghaus et al., 2025b; Seifner et al., 2025].\nIn the context of LLMs, a common strategy is conditional\nsynthetic data generation. This means that a raw corpus is\nused as input for a generative model which then synthesizes\nthe data. Self-Instruct [Wang et al., 2023] formalizes this\nLLM-based generation of instruction\u2013response pairs com-\nbined with filtering to remove invalid or redundant samples.\nTo systematically increase task complexity, Evol-Instruct-\nstyle methods (e.g., as used in WizardLM ) rewrite prompts\ninto progressively more challenging variants, exposing mod-\nels to a broader difficulty spectrum [Xu et al., 2023]. Closely\nrelated to our setting, recent work shows that transforming\ndomain corpora into reading-comprehension style QA can\nbe an effective alternative to continued pre-training on raw\ntext, while preserving instruction-following behavior [Cheng\net al., 2024]. Our pipeline follows this direction by convert-\ning statutory passages into QA pairs, explicitly controlling\ndifficulty (from direct lookups to multi-provision reasoning)\nand applying reviewer-style filtering to improve statute-only\ngrounding and factual consistency.\nBenchmarks and evaluation protocols.\nGeneral legal\nNLP benchmarks such as LexGLUE [Chalkidis et al., 2022]\nand LegalBench [Zheng and others, 2023] provide broad cov-\nerage of legal tasks, but are English-centric and classification-\noriented, so they do not directly measure German statute-\ngrounded answer generation. Since our target setting is Ger-\nman legal QA grounded in statutory text, we evaluate on held-\nout test sets derived from German statutes and LegalMC4\n[European Language Resources Association (ELRA), 2023],\nmatching the intended deployment scenario. For open-ended\nQA, we adopt LLM-as-a-judge scoring as a scalable evalua-\ntion protocol [Zheng et al., 2023; Liu et al., 2023] and com-\nplement it with exact accuracy for multiple-choice questions\nto ensure robustness across evaluation formats.\n3\nData Preparation\nOur main contribution is the construction of statute-grounded\nsupervised fine-tuning (SFT) data for German legal question\nanswering. The design goals are: (i) grounding in authori-\ntative legal text, (ii) coverage of question types that resem-\nble real user queries, (iii) scalability without manual anno-\ntation, and (iv) contamination control through passage-level\nsplitting. We operationalize these goals with two synthetic\ndata pipelines.\nStandard Instruction QA Data Genera-\ntion is a single-prompt baseline that converts a passage into\ninstruction-style QA in one step.\nDifficulty-Graded QA\nData Generation is a difficulty-graded statute QA pipeline\nthat generates multiple controlled question types per provi-\nsion and applies stricter automated filtering.\n3.1\nSource Texts and Normalization\nWe build training supervision from German legal text with\nexplicit provenance and identifiers.\nWe use the statutory\ntexts from the most common german laws, namely the\nB\u00a8urgerliches Gesetzbuch (BGB) which is the German Civil\nCode, the Strafgesetzbuch (StGB) which is the Criminal\nCode, the Sozialgesetzbuch (SGB) which is the Social Code,\nand the Grundgesetz (GG) which is the Basic Law. For statu-\ntory material, we normalize each provision into a canonical\nrecord:\n\u2022 law identifier (e.g., BGB, StGB, SGB, GG),\n\u2022 section identifier (e.g., \u00a7 433),\n\u2022 section text (German).\nThis representation ensures that generated answers can ex-\nplicitly reference the correct authority (law + section). For\nevaluation set construction (Section 4), we additionally use\nthe German portion of LegalMC4 [European Language Re-\nsources Association (ELRA), 2023], which contains hetero-\ngeneous legal web documents. Across sources, we preserve\nthe original passage boundaries and identifiers so that every\nQA pair can be audited against a fixed input span.\n3.2\nStandard Instruction QA Data Generation\nStandard Instruction QA Data Generation is a straightforward\n\u201dpassage-to-QA\u201d baseline.\nGiven a single input passage, we prompt GPT-4 to generate\na small set of German question\u2013answer pairs and to return\nthem in a strict JSON schema (Appendix B.1). This setup\nfollows a widely used synthetic supervision paradigm where\nan LLM converts raw text into task examples that can be used\nfor supervised fine-tuning. In particular, it is closely related to\ncorpus-to-QA conversion approaches that transform domain\ncorpora into reading-comprehension style supervision rather\nthan continuing training on raw text [Cheng et al., 2024]. It\nis also consistent with the general generate-and-filter recipe\nfor building synthetic instruction data at scale [Wang et al.,\n2023].\nConcretely, the prompt enforces the following constraints:\n\u2022 Input: one legal passage with its provenance (law identifier\nand section, when available).\n\u2022 Output: up to k QA pairs in German, formatted as JSON.\n\u2022 Style: questions reflect plausible user queries, answers use\na lawyer-style tone.\nStandard Instruction QA Data Generation is straightfor-\nward to implement, but it is unclear if such a naive and\nopen-ended prompt yields data with sufficient diversification.\nThis motivates Difficulty-Graded QA Data Generation, which\nexplicitly controls question type and difficulty and applies\nstricter review-based filtering (Section 3.3).\n3.3\nDifficulty-Graded QA Data Generation\nDifficulty-Graded QA Data Generation is designed to pro-\nduce statute-grounded supervision that is both diverse and\ncontrollable.\nRather than generating one undifferentiated set of ques-\ntions per provision, we explicitly target multiple question\nfamilies that reflect how legal queries arise in practice. The\ndesign is motivated by two observations. First, synthetic su-\npervision improves when the data covers a range of input\u2013\noutput patterns and difficulty levels [Bengio et al., 2009].\nSecond, instruction generation pipelines benefit from struc-\ntured transformations that increase complexity while main-\ntaining validity [Xu et al., 2023]. Operationally, we generate\nquestions at four difficulty levels and then apply a separate\nreviewer prompt during filtering (Appendix A).\nAcross all levels, we enforce shared invariants:\n\u2022 Statute-only grounding: answers must be fully supported\nby the provided text and must not rely on external law or\nassumptions.\n\u2022 Explicit authority: answers must cite the law identifier\nand the section number.\n\u2022 Controlled output: the generator must return a strict JSON\nstructure to enable automated parsing.\n\u2022 Redundancy control: we cap the number of pairs per level\nto limit near-duplicate paraphrases.\nLevel 1 (clause-centric).\nLevel 1 focuses on high-precision\ncomprehension. Questions address what the provision states,\nwhen it applies, and which conditions or exceptions are en-\ncoded in the text. This level prioritizes faithfulness and en-\ncourages answers that correctly map to the cited authority.\nLevel 2 (client-style paraphrase).\nLevel 2 aims to match\nhow users typically ask legal questions. By prohibiting statute\nidentifiers in the question, we force the model to learn the\nmapping from natural language to the correct cited provision\nin the answer. This reduces the risk that fine-tuning degener-\nates into surface-level pattern matching on section numbers.\nLevel 3 (scenario application).\nLevel 3 introduces short,\nrealistic fact patterns. The question must be phrased indepen-\ndently of the statutory wording, which encourages application\nrather than restatement. The answer must still be statute-only,\nand it must justify the conclusion by referencing the provi-\nsion.\nLevel 4 (multi-provision reasoning).\nLevel 4 targets com-\nposition across sections. We provide two or more sections as\ninput and require scenarios where multiple sections are sub-\nstantively necessary. To preserve grounding, the generator is\ninstructed to output no pairs when a valid multi-section sce-\nnario cannot be constructed from the provided text alone.\nIllustrative example.\nFigure 2 illustrates the qualitative\ndifference between Standard Instruction QA Data Genera-\ntion and Difficulty-Graded QA Data Generation. Standard\nInstruction QA Data Generation produces single-level, recall-\nstyle supervision, whereas Difficulty-Graded QA Data Gen-\neration yields complementary, difficulty-graded supervision\n(L1\u2013L3) from the same provision and enables multi-section\nreasoning at Level 4 when multiple inputs are provided, while\nmaintaining statute-only grounding and explicit citation.\n3.4\nLLM-Based Quality Filtering and\nRedundancy Control\nDifficulty-Graded QA Data Generation follows a generate-\nthen-filter recipe that is standard in synthetic SFT construc-\ntion [Wang et al., 2023]. Importantly, Difficulty-Graded QA\nData Generation combines two complementary mechanisms:\n(i) difficulty-graded generation that systematically increases\ndiversity by eliciting qualitatively different question families\n(Levels 1\u20134; Figure 2), and (ii) a statute-only reviewer that\nremoves invalid or low-quality instances.\nThe reviewer is\nstrictly subtractive: it can only discard candidates, not gen-\nerate new question types. Thus, the structured multi-level\nprompting drives diversity, while the reviewer primarily im-\nproves fidelity and reduces noise.\nWe do not apply an analogous reviewer stage to Standard\nInstruction QA Data Generation, as it is intended as a minimal\none-step passage-to-QA baseline. Adding a second model-\nbased filtering component to Standard Instruction QA Data\nGeneration would introduce an additional design axis and\nblur the baseline comparison; we therefore keep Standard In-\nstruction QA Data Generation as generated.\nFor Difficulty-Graded QA Data Generation, we apply a\ndedicated reviewer prompt (Appendix A) that has access only\nto the statute text used for generation.\nFor each candidate QA pair, the reviewer verifies three cri-\nteria:\n\u2022 Answerability: the question is specific and can be an-\nswered from the provided text,\n\u2022 Support: the answer is fully supported and introduces no\nexternal law or assumptions,\n\u2022 Non-redundancy: the pair is not duplicative of other pairs\nfrom the same passage.\nPairs that fail any criterion are discarded.\nAs shown in Table 4, Difficulty-Graded QA Data Gener-\nation generates 31,777 candidates before review and retains\n23,905 pairs after review. Rejections are concentrated in the\nlower difficulty levels (Levels 1\u20132), where redundant para-\nphrases are most common, whereas Level 4 is unaffected\n(441/441 retained). This indicates that the reviewer primar-\nily removes noise and redundancy rather than suppressing the\nrare cross-section samples.\n3.5\nTrain-Test Split and Contamination Control\nTo prevent memorization of identical source passages, we\nsplit at the passage level. For statutes, splitting is performed\nat the section identifier (\u00a7), ensuring that all QA pairs gen-\nerated from a given section belong to exactly one split. For\nLegalMC4-derived material, we split by held-out source pas-\nsages so that evaluation passages do not appear in any training\nset.\nOur evaluation comprises four held-out test sets derived\nfrom the BGB and LegalMC4: LegalMC4 QA (n=732),\nBGB QA (n=715), LegalMC4 MCQ (n=1315), and BGB\nMCQ (n=1845). Difficulty-Graded QA Data data is used ex-\nclusively for model adaptation and is never used to construct\nevaluation sets.\n4\nExperiments and Results\nWe evaluate domain specialization and generalization for two\ninstruction-tuned models, LLaMA 3.1 (8B) and Gemma 3\n(12B), comparing fine-tuned variants against their base coun-\nterparts.\n4.1\nLoRA Fine-tuning Setup\nFor each base model, we perform supervised LoRA fine-\ntuning on the synthetic training sets produced by Standard\nInstruction QA Data Generation and Difficulty-Graded\nQA Data Generation (Section 3). To isolate the effect of the\ntraining data generation approach, we keep the LoRA config-\nuration and optimization hyperparameters fixed across both\nParameter\nValue\nMax sequence length\n2048\nPrecision\nbf16\nLoRA rank (r)\n16\nLoRA alpha (\u03b1)\n32\nLoRA dropout\n0.05\nAdapter placement\u2217\nAttention + MLP modules\nRandom state\n3407\nEpochs\n7\nLearning rate\n1 \u00d7 10\u22125\nOptimizer\nAdamW 8bit\nWeight decay\n0.01\nLR scheduler\nlinear\nWarmup ratio\n0.1\nTable 1:\nLoRA fine-tuning hyperparameters in\nGemma-3-12B-IT and Llama-3.1-Instruct.\n\u2217For Gemma: vision layers are disabled.\napproaches and across both model families. Table 1 summa-\nrizes the shared hyperparameters used in all fine-tuning runs.\nFor Gemma, we disable the vision layers.\n4.2\nCompute and Training Time\nAll LoRA fine-tuning runs were executed on a single\nNVIDIA A100-SXM4 GPU with 80 GB memory. Fine-\ntuning one model on one synthetic dataset took approxi-\nmately 9\u201310 hours of wall-clock time, including evaluations\non the validation set and checkpointing.\n4.3\nEvaluation Tasks\nWe evaluate on four held-out German legal test sets derived\nfrom the BGB and the German portion of LegalMC4 [Euro-\npean Language Resources Association (ELRA), 2023]. Two\nsets contain open-ended questions and are scored for factual\ncorrectness using an LLM judge (GPT-4.1), following stan-\ndard LLM-based evaluation protocols [Zheng et al., 2023;\nLiu et al., 2023] which yielded in high correlations with hu-\nman experts in similar scenarios [Hillebrand et al., 2024].\nTwo sets contain multiple-choice questions and are evaluated\nby exact accuracy.\nCritically, the LegalMC4-based evaluation sets are con-\nstructed from real-world legal documents crawled from the\ninternet, including court decisions, legal opinions, admin-\nistrative documents, and other heterogeneous legal texts.\nThese documents naturally contain substantial noisy context-\nirrelevant passages, formatting artifacts, tangential discus-\nsions, and information peripheral to the core legal question.\nThis design choice is deliberately aligned with practical de-\nployment scenarios: in production systems, legal LLMs are\npredominantly deployed within RAG architectures, where a\nretrieval component first identifies potentially relevant doc-\numents, and the generator model must then locate the perti-\nnent information within these retrieved passages [Huang et\nal., 2023; Chen et al., 2023].\nBy evaluating on LegalMC4 data with its inherent noise,\nwe indirectly assess the model\u2019s ability to function as the\ngenerator component in a RAG system. A fine-tuning ap-\nproach should not only improve the model\u2019s legal knowledge\nbut also its robustness to distractor information and its ca-\npacity to extract relevant facts from imperfect retrievals\u2014\ncapabilities that are essential for real-world legal assistance\napplications. The evaluation therefore tests whether adapted\nmodels can maintain accurate reasoning even when presented\nwith the noisy, heterogeneous document contexts typical of\nRAG-based legal QA systems. Figure 3 provides an illus-\ntrative context-based instance in this setting: the model is\ngiven a retrieved legal passage (which may include irrele-\nvant or noisy surrounding text) and must answer a user ques-\ntion by extracting and grounding the response in the provided\ncontext. This mirrors the generator role in RAG pipelines,\nwhere retrieval provides candidate evidence and the model\nmust identify the relevant facts and produce a faithful answer.\nIn contrast, the BGB-based evaluation sets provide cleaner,\nstatute-focused assessments that isolate legal reasoning capa-\nbilities on well-structured legislative text.\nTo summarize, our evaluation sets are:\nOpen-ended\nQA (LLM-judged correctness): LegalMC4 QA (n=732),\nopen-ended questions derived from LegalMC4 passages, re-\nquiring extraction and reasoning over information embed-\nded in noisy real-world legal documents; and BGB QA\n(n=715), open-ended questions grounded in held-out BGB\nsections.\nMultiple-choice QA (accuracy):\nLegalMC4\nMCQ (n=1315), multiple-choice questions derived from\nLegalMC4 material, testing document comprehension in\nnoisy real-world contexts;\nand BGB MCQ (n=1845),\nmultiple-choice questions targeting the BGB.\nAll test sets are constructed from held-out source passages\nthat do not appear in any training set (Section 3.5).\n4.4\nDomain-Specific Performance\nTable 2 reports performance on the four German legal test\nsets. The results reveal a clear hierarchy in performance.\nDifficulty-Graded QA Data Generation consistently deliv-\ners substantial improvements.\nThe difficulty-graded gen-\neration strategy with LLM-based filtering yields the strongest\ngains across all benchmarks and both model families. For\nGemma 3 (12B), the improvements are particularly strik-\ning: open-ended correctness increases by approximately 40\npercentage points on BGB QA (+39.6) and about 16 points\non LegalMC4 QA (+15.8).\nMultiple-choice accuracy im-\nproves by about 4\u20138 points (+4.4 on LegalMC4 MCQ and\n+8.1 on BGB MCQ). LLaMA 3.1 (8B) shows similar pat-\nterns with consistent gains across all test sets, with the largest\nimprovement on BGB QA (+20.0 points) and strong gains on\nLegalMC4 QA (+12.4 points).\nLegalMC4 as a proxy for retrieval-augmented QA with\ncontext.\nBecause the LegalMC4-based benchmarks pro-\nvide noisy, heterogeneous document context (Section 4.3),\nthe gains on LegalMC4 QA/MCQ indicate that Difficulty-\nGraded QA fine-tuning also improves performance in\nretrieval-augmented deployments where the model must an-\nswer using retrieved passages [Huang et al., 2023; Chen\net al., 2023]. Concretely, Finetuning on Difficulty-Graded\nQA improves LegalMC4 QA from 43.0\u219255.4 (+12.4) for\nLLaMA 3.1 and from 38.7\u219254.5 (+15.8) for Gemma 3, and it\nTable 2: Accuracy on German legal QA benchmarks. Open QA columns report LLM-judged factual correctness (%). MCQ\ncolumns report exact accuracy (%).\nModel\nTrain Data\nOpen QA (Judge)\nMCQ (Accuracy)\nLegalMC4\nQA\nBGB\nQA\nLegalMC4\nMCQ\nBGB\nMCQ\nLLaMA 3.1\n(8B)\n\u2013\n43.0\n39.2\n60.9\n64.0\nStandard Instruction QA Data\n35.4 \u21937.6\n37.6 \u21931.6\n59.9 \u21931.0\n64.2 \u21910.2\nDifficulty-Graded QA Data\n55.4 \u219112.4\n59.2 \u219120.0\n66.1 \u21915.2\n68.2 \u21914.2\nGemma 3\n(12B)\n\u2013\n38.7\n36.8\n66.8\n67.0\nStandard Instruction QA Data\n41.8 \u21913.1\n46.6 \u21919.8\n66.9 \u21910.1\n67.1 \u21910.1\nDifficulty-Graded QA Data\n54.5 \u219115.8\n76.4 \u219139.6\n71.2 \u21914.4\n75.1 \u21918.1\nGPT-5-mini (min. reasoning)\n\u2013\n70.1\n62.7\n70.9\n75.8\nGPT-5 (min. reasoning)\n\u2013\n77.2\n74.7\n72.0\n78.8\nArrows indicate performance change compared to the base model: \u2191denotes improvement, \u2193denotes decrease, \u2192\ndenotes no change. Values show the absolute difference in performance metrics.\nincreases LegalMC4 MCQ accuracy by +5.2 and +4.4 points,\nrespectively.\nStandard Instruction QA Data Generation yields mixed\nresults.\nThe simpler single-prompt generation strategy\nshows inconsistent benefits. While Gemma 3 sees modest\nimprovements on open-ended questions (+3.1\u20139.8 points), its\nmultiple-choice accuracy changes only marginally (+0.1 on\nboth MCQ sets). For LLaMA 3.1, Finetuning on Standard\nInstruction QA degrades performance on open QA tasks by\nup to 7.6 points, suggesting that naive synthetic data gener-\nation can be harmful without proper quality control; MCQ\naccuracy changes remain small (within \u00b11.0 point).\nComparison to proprietary baselines.\nFor GPT-5 and\nGPT-5-mini, we use minimal reasoning effort to better match\nthe non-reasoning open-source models we fine-tune. While\nthis is not a fully apples-to-apples comparison, since CoT-\nreasoning cannot fully be turned-off for these models, it pro-\nvides the closest available reference point. Our best adapted\nmodel (Gemma 3 with Difficulty-Graded QA) achieves per-\nformance competitive with GPT-5-mini on the multiple-\nchoice benchmarks (71.2 vs. 70.9 on LegalMC4 MCQ and\n75.1 vs. 75.8 on BGB MCQ) and exceeds both GPT-5-mini\nand GPT-5 on BGB QA (76.4% vs. 62.7% and 74.7%, re-\nspectively).\nHowever, a gap remains on the more diverse\nLegalMC4 QA test set, where GPT-5 reaches 77.2% com-\npared to our 54.5%.\n4.5\nGeneral Language Understanding\nTo assess whether domain specialization degrades general\ncapabilities, we evaluate on ARC (Easy and Challenge), a\nGerman-translated ARC subset (ARC-DE), and MMLU us-\ning the EleutherAI evaluation harness [Clark and others,\n2018; Hendrycks et al., 2021; Gao et al., 2024]. The results\nare shown in Table 3.\nLegal adaptation preserves (and sometimes improves)\ngeneral performance.\nAcross both model families, gen-\neral capability remains stable or improves slightly after legal\nfine-tuning. For Gemma 3 (12B), Difficulty-Graded QA Data\nGeneration enhances reasoning on ARC tasks (+2.1 on ARC-\nE, +2.9 on ARC-C, and +1.9 on ARC-DE) while maintain-\ning MMLU performance (71.6\u219271.0, within measurement\nnoise). Standard Instruction QA Data Generation yields es-\nsentially unchanged general performance, with only minor\nvariations (at most 0.2 points on ARC-E and 0.1 points on\nARC-C/ARC-DE) and no change on MMLU.\nLLaMA 3.1 (8B) shows minimal changes across all bench-\nmarks for both approaches, with scores fluctuating by less\nthan 2 points.\nStandard Instruction QA Data Generation\nslightly decreases ARC-E/C and MMLU while marginally\nimproving ARC-DE, whereas Difficulty-Graded QA Data\nGeneration modestly improves ARC-C but decreases ARC-\nE, ARC-DE, and MMLU.\nThese results demonstrate that targeted domain adapta-\ntion through carefully filtered synthetic data does not induce\ncatastrophic forgetting. The slight improvements on certain\nbenchmarks suggest that exposure to structured legal reason-\ning may even transfer beneficially to related reasoning tasks.\n5\nConclusion\nIn this study, we presented a method for fine-tuning large lan-\nguage models explicitly for the German legal domain. A cru-\ncial insight from our experiments is that the quality of the data\ngeneration process matters significantly. We discovered that\nsimple, naive generation of synthetic question-answer pairs\ndoes not yield improved performance. In fact, such basic ap-\nproaches can even degrade the model\u2019s reasoning capabilities.\nInstead, success required a highly sophisticated data gen-\neration pipeline. By utilizing a difficulty-graded strategy that\nspans from basic clause lookups to complex multi-provision\nreasoning, we were able to create truly effective training\ndata. Consequently, our adapted models demonstrated su-\nperior performance in answering German legal queries com-\npared to baselines, while simultaneously maintaining their\ngeneral linguistic capabilities.\nThis work highlights the potential of carefully designed\nsynthetic data for specialized domains. Future research will\nfocus on incorporating real-time retrieval of legal statutes to\nimprove accuracy further.\nTable 3: Performance on general benchmarks: ARC (Easy/Challenge), a German-\ntranslated ARC subset (ARC-DE), and MMLU. Scores are accuracies (%).\nModel (Fine-tune)\nARC-E\nARC-C\nARC-DE\nMMLU\nLLaMA 3.1 8B (No Finetuning)\n82.1\n51.9\n40.2\n68.1\nLLaMA 3.1 8B (Standard)\n81.9 \u21930.2\n51.6 \u21930.3\n40.5 \u21910.3\n67.4 \u21930.7\nLLaMA 3.1 8B (Difficulty-Graded)\n81.2 \u21930.9\n52.3 \u21910.4\n38.5 \u21931.7\n67.1 \u21931.0\nGemma 3 12B (No Finetuning)\n83.7\n60.8\n48.2\n71.6\nGemma 3 12B (Standard)\n83.5 \u21930.2\n60.9 \u21910.1\n48.3 \u21910.1\n71.6 \u21920\nGemma 3 12B (Difficulty-Graded)\n85.8 \u21912.1\n63.7 \u21912.9\n50.1 \u21911.9\n71.0 \u21930.6\nArrows indicate performance change compared to the base model: \u2191denotes im-\nprovement, \u2193denotes decrease, \u2192denotes no change. Values show the absolute\ndifference in performance metrics.\n6\nLLM Statement\nParts of all sections have been refined with GPT-5. All refined\ntexts have been carefully checked by the authors for halluci-\nnations.\nInput statute (English translation). BGB \u00a7 857 (Inheritability of Possession): Possession passes to the heir. English shown for\nreadability; training data is generated in German.\nStandard Instruction QA Data\nGeneration: Single-prompt\ngeneration (single level).\nTypically produces short,\nstatute-identification / recall-style\nsupervision.\nQ. Which law and which provision\ngoverns the inheritability of possession?\nA. The inheritability of possession is\ngoverned by BGB \u00a7 857.\nDifficulty-Graded QA Data Generation: Difficulty-graded generation (L1\u2013L4).\nThe same statute input yields progressively harder question types; Level 4 uses multiple\nsections as input.\nL1 (clause comprehension).\nQ. What happens to the possession of a\ndeceased person after their death?\nA. According to BGB \u00a7 857, possession\npasses to the heir.\nL2 (client-style paraphrase).\nQ. I am the sole heir of my deceased\nbrother. Do I have to wait for court\nproceedings before I have access to his\nbelongings?\nA. According to BGB \u00a7 857, possession\npasses to the heir upon inheritance. You do\nnot need to wait for court proceedings; as\nthe heir, you immediately have legal\npossession of your brother\u2019s belongings.\nL3 (scenario application; single-section input).\nQ. A tenant dies. The apartment remains locked, and the sole heir can only travel in a few\ndays. Meanwhile, a close acquaintance removes several valuables and argues that the heir\nis not yet the \u201cpossessor\u201d because he has not physically taken them. Is the heir already the\nlegal possessor of the items in the apartment?\nA. Yes. Under BGB \u00a7 857, possession passes to the heir. Thus, the heir becomes the\npossessor upon the inheritance event, even if he has not yet physically taken the items.\nExample Level 4 input sections: BGB \u00a7 857, \u00a7 1362, \u00a7 1384.\nL4 (cross-section integration; multi-section input).\nQ. A married couple has been separated for months, and a divorce petition has already been\nfiled. A creditor enforces against one spouse by seizing movable property located in the\nother spouse\u2019s apartment. Shortly thereafter, the debtor spouse dies; the heir demands\nimmediate possession of the estate items. Which provisions govern (i) transfer of\npossession to the heir, (ii) the ownership presumption for seized movables, and (iii) the\nrelevant date for calculating accrued gains?\nA. (i) BGB \u00a7 857: possession passes to the heir upon inheritance."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary\n\n#### Main Contributions\nThis paper presents a novel method for adapting large language models (LLMs) specifically for German legal question answering through synthetic data generation. The main contributions are:\n1. **Synthetic Data Generation Pipeline**: A robust and scalable pipeline that synthesizes statute-grounded training data directly from authoritative German statutes, avoiding the limitations of web-crawled datasets.\n2. **Difficulty-Graded Strategy**: An innovative approach to generating multiple controlled question types per provision at varying levels of difficulty, ensuring a diverse yet coherent dataset for fine-tuning LLMs.\n3. **Quality Filtering and Redundancy Control**: A rigorous review stage that filters out unanswerable or redundant pairs, maintaining the integrity of the synthetic data while improving factual consistency.\n\n#### Methodology\nThe methodology involves:\n1. **Data Preparation**:\n   - **Source Texts and Normalization**: Using statutory texts from common German laws (BGB, StGB, SGB, GG) as source material.\n   - **Synthetic Data Generation Pipelines**: Two pipelines are used: Standard Instruction QA Data Generation and Difficulty-Graded QA Data Generation. The latter generates multiple controlled question types per provision at varying levels of difficulty.\n2. **Quality Filtering**:\n   - A reviewer model is employed to filter out unanswerable, unsupported, or redundant pairs. This review stage ensures that the generated data remains high-quality and relevant.\n\n#### Key Findings\nThe key findings include:\n1. **Performance Improvements**: The fine-tuned models using synthetic datasets significantly outperform their baseline counterparts on German legal question answering tasks.\n2. **Robustness Across Benchmarks**: The difficulty-graded generation strategy yields consistent gains over base models, preserving general capabilities while improving accuracy on specific benchmarks.\n3. **Domain-Specific Performance**: The adapted LLMs demonstrate superior performance in answering German legal queries compared to baselines, maintaining their general linguistic capabilities.\n\n#### Conclusions\nThe paper concludes by highlighting the potential of carefully designed synthetic data for specialized domains like law. It emphasizes that targeted domain adaptation through synthetic data does not induce catastrophic forgetting and can even transfer beneficial reasoning capabilities to related tasks. The authors suggest future research should incorporate real-time retrieval of legal statutes to further improve accuracy.\n\n### Summary\n\nThis study introduces a novel approach to fine-tune large language models specifically for German legal question answering using synthetic data generation. By leveraging a difficulty-graded strategy, the researchers successfully created high-quality, diverse training datasets that significantly outperform baseline counterparts on relevant benchmarks. The findings underscore the feasibility of using carefully designed synthetic data as an alternative to manual annotation in high-stakes domains such as law, while maintaining general capabilities."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "InT: Self-Proposed Interventions Enable Credit\nAssignment in LLM Reasoning\nMatthew Y. R. Yang1, Hao Bai2, Ian Wu1, Gene Yang1, Amrith Setlur1 and Aviral Kumar1\n1Carnegie Mellon University, 2University of Illinois Urbana-Champaign\nBase model\nSFT\nproblem\nBase model\nRL\nproblem\nIntervention Training\nUpweight off-policy\nreference solutions\nUniformly update\non-policy rollouts\nCorrect\nIncorrect\nUpdate\nPropose\nintervention\nBase model\nreference\nsolution\nSelf-verify to find mistake\n+\nincorrect\nrollout\nBase model\nproblem\nUpweight interventions\n& steps before mistake\n1\n2\nsingle\nrollout\nNo update\nFigure 1: Intervention training (InT) for improving credit assignment. InT proposes single-step interventions to replace\nincorrect intermediate steps in reasoning traces (1). Conditioned on these localized corrections, the model can generate\ncounterfactual continuations that succeed where the original failed (2). We then perform supervised fine-tuning on these\ninterventions, enabling effective credit assignment by upweighting the likelihood of the interventions in place of the mistakes.\nAbstract: Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities\nof large language models (LLMs). However, standard RL assigns credit only at the level of the final answer,\npenalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is\ncorrect. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be\nreinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural\nremedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning\nsteps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model\nperforms fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that\nsteer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning\ndatasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one\nfrom scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect\nthe trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up\nto the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We\nshow that the resulting model serves as a far better initialization for RL training. After running InT and subsequent\nfine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench,\noutperforming larger open-source models such as gpt-oss-20b.\n1. Introduction\nPost-training large language models (LLMs) with reinforcement learning (RL) has proven to be a highly\neffective strategy for improving their reasoning capabilities. As RL is scaled to more challenging tasks,\ntraining becomes increasingly dominated by incorrect rollouts, which are often long-horizon trajectories\ncomposed of many smaller, structured reasoning steps. Many such rollouts contain substantial portions\nof correct reasoning, yet outcome-based RL treats these correct steps the same as the critical mistakes\nCorresponding author(s): matthew.y.r.yang@gmail.com; This work was done at CMU.\nProject website: https://intervention-training.github.io/.\narXiv:2601.14209v1  [cs.LG]  20 Jan 2026\nInT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\nthat cause failure. This indiscriminate penalty can lead to undesirable behaviors including increased\nverbosity or frequent, premature shifts in the reasoning process, which have been widely observed when\napplying outcome-reward RL [5, 49]. Moreover, on sufficiently difficult problems where no correct rollout\nis produced at all, outcome-reward RL provides no learning signals, as advantages collapse to zero.\nIn principle, addressing this challenge requires solving the problem of credit assignment: if we could\nidentify the intermediate step at which a reasoning trace goes astray and selectively penalize that\nstep while reinforcing other possible promising steps, we could drive the model toward more favorable\noutcomes. However, existing approaches to credit assignment are difficult to apply in the LLM setting.\nFor reasoning traces, estimating the \u201ccredit\u201d (i.e., value) of each step typically requires running multiple\nbranched rollouts conditioned on a given prefix, which is prohibitively expensive [18, 21, 28]. Prior work\nattempts to amortize this process by training explicit value functions (or process reward models) [28, 39],\nbut how we should train such functions over long reasoning traces remains an open question. Moreover,\neven with a reasonably accurate value function, optimizing it to identify an alternate step is itself\nchallenging [48, 63], because of the vast space of possible future steps.\nHow can we perform credit assignment without training a value function? Rather than explicitly\nlearning a value function, we ask the model itself to identify how failed trajectories can be locally corrected.\nThis approach relies on an LLM\u2019s ability to implicitly perform value estimation and optimization jointly, in\norder to propose an alternative, improved step, which we refer to as a corrective intervention. Our main\nidea is to reduce the task of generating end-to-end solutions to the substantially easier task of verifying\nindividual steps, which can be accomplished via textual comparison against a reference solution (i.e., a\ntextual \u201cdiff\u201d). In our method, we instruct a base model to analyze the differences between a reference\nsolution and an incorrect reasoning trace that it previously generated, with the goal of identifying the\nfirst point at which the trace goes wrong and proposing a single-step corrective intervention. When\nconditioned on this intervention, the model can then generate counterfactual reasoning traces that\nsucceed from right before the point of failure even when the original rollout failed.\nWe then train the model to internalize these interventions by applying supervised fine-tuning (SFT) on\nthem, followed by RL. By fine-tuning the model on targeted single-step interventions, we selectively\nreduce the likelihood of the incorrect reasoning steps that the base model would otherwise produce,\nshifting it toward more favorable steps instead. Our approach remains notably simple and computationally\nefficient, as it avoids branched rollouts, explicit value-function training, or modifying the RL objective to\ninclude step-level rewards. Throughout this procedure, we never rely on a larger model; instead, we\nleverage the asymmetry [41] in task difficulty between instruction-following, verification, and generation\nwithin the same model to perform credit assignment. We call this approach Intervention Training (InT).\nWe compare InT to several alternative approaches that leverage reference solutions for learning, including\nmethods that train policies directly on reference solutions or on self-generated reflections conditioned on\nthose solutions. In settings where the base model is already strong (e.g., Qwen3 for mathematics) and\nthe SFT dataset is relatively small (\u22481K examples), we observe that InT makes particularly effective use\nof reference solutions to improve pass@\ud835\udc58performance, which sets up the fine-tuned model for online\nRL. Notably, this coincides with InT producing more \u201con-policy\u201d trajectories (those with high likelihood\nunder the base model), whereas other approaches tend to produce more \u201coff-policy\u201d ones. After online\nRL, InT improves performance by nearly 10% on average across four challenging mathematical reasoning\nbenchmarks, including a \u223c14% gain on IMO-AnswerBench [30], which consists of IMO-level problems\ncurated by former medalists. These results demonstrate InT as a simple and effective paradigm for\nimproving the reasoning capabilities of LLMs, through improved credit assignment.\n2\nInT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\n2. Preliminaries, Notation, and Problem Statement\nLearning objective. Training LLMs for reasoning typically involves an LLM \ud835\udf0b, model-generated reasoning\ntraces y \u223c\ud835\udf0b(\u00b7|x) produced on input prompts x \u223c\ud835\udf0c, and a binary reward \ud835\udc5f(x, y) \u2208{0, 1} quantifying\nthe correctness of the final answer. The goal is to maximize average reward, given by:\nmax\n\ud835\udf0b\n\ud835\udc3d(\ud835\udf0b) := Ex\u223c\ud835\udf0c,y\u223c\ud835\udf0b(\u00b7|x)[\ud835\udc5f(x, y)].\nWe can maximize the objective above by updating the policy with the policy gradient [50]:\n\ud835\udf0b\u2032 \u2190\ud835\udf0b+ \ud835\udefc\u00b7 Ex\u223c\ud835\udc37train,y\u223c\u02dc\ud835\udf0b(\u00b7|x)[\ud835\udc5f(x, y) \u00b7 \u2207\ud835\udf0blog \ud835\udf0b(y|x)],\n(1)\nwhere \ud835\udc37train is the set of training problems and \u02dc\ud835\udf0bis the policy used to generate samples for training. In\npolicy gradient methods such as GRPO [42], \u02dc\ud835\udf0b= \ud835\udf0bold is a periodically updated copy of \ud835\udf0b, with additional\ncorrection terms in Equation 1 to account for the distribution shift between \ud835\udf0bold and \ud835\udf0b. We can also\nexpress SFT with a similar update rule, where \u02dc\ud835\udf0brepresents a pre-collected offline dataset consisting of\nonly correct trajectories, so that Equation 1 effectively maximizes likelihood on these traces.\nCredit assignment. Reasoning trajectories y can naturally be decomposed into a sequence of individually\ncomplete segments1, y = (y0, y1, . . . , y\ud835\udc47), where each \u201csingle reasoning step\u201d y\ud835\udc61may contribute unequally\nto the final answer. Ideally, learning signals should reflect this uneven contribution. However, in Equation 1,\nthe policy is updated with binary rewards that entirely depend on the correctness of the final answer\n(\u201coutcome rewards\u201d). Outcome-reward RL normalizes these rewards across responses sampled for the\nsame prompt x, yielding advantages. When the advantage \ud835\udc34(x, y\ud835\udc56) = \ud835\udc5f(x, y\ud835\udc56) \u22121/\ud835\udc5b\u2211\ufe00\ud835\udc5b\n\ud835\udc57=1 \ud835\udc5f(x, y\ud835\udc57) is\npositive, every step in the response y\ud835\udc56is uniformly reinforced; when it is negative, all steps are uniformly\ndiscouraged. With this form of reward allocation, steps that play no role in reaching the solution may\nstill be upweighted, while steps that are locally correct but followed by later mistakes may be suppressed.\nIn long-horizon settings where trajectories y\ud835\udc56consist of hundreds of steps, as in LLM reasoning, the\nresulting noise can overwhelm the learning signal, preventing meaningful progress from occurring.\nThese limitations of outcome-reward RL manifest in two ways. First, when training with large token\nbudgets, rewards often plateau early and do not improve substantially over the course of training [29,\n41, 54]. Second, responses become either excessively verbose or prematurely truncated, often failing\nto maximize the underlying outcome reward [5, 36, 49]. Avoiding these issues requires accurately\nidentifying which steps are responsible for success or failure and disentangling correct steps from\nincorrect responses and incorrect steps from otherwise correct responses.\nThis problem of determining how individual intermediate steps contribute to the final outcome is known\nas credit assignment. If we can estimate the \u201ccredit\u201d of each intermediate step with a step-level value\nfunction (also called Process Reward Models, or PRMs), we can update \ud835\udf0bby encouraging the correct\nparts of a solution while discouraging the incorrect ones. However, this approach faces two major\nobstacles in practice. First, reliably estimating such value functions is difficult due to its cost [18, 28, 39]\nand training instabilities [48, 63]. Second, even when a sufficiently accurate PRM is available, simply\npenalizing incorrect parts of a solution does not ensure that they will be replaced with higher-value,\ncorrect alternatives. As a result, policy optimization remains a challenging problem, since efficiently\nsearching for suitable replacement steps is a difficult task in itself.\n1For the LLMs we study, this decomposition is implemented by splitting the generated text using the delimiter \u201c\\n\\n\u201d (see\nFigure 3), following prior work [36].\n3\nInT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\nProblem statement. Our goal is to develop an effective approach to credit assignment. In particular,\nwe focus on assigning credit within incorrect rollouts by pinpointing the incorrect steps that derail the\nsolution. In our setting, we assume access to a reference solution (e.g., a human-written solution) for\neach problem in our training set, as they are commonly available in existing open-source math datasets.\nWhy focus on credit assignment for incorrect traces? As we move toward more challenging RL tasks,\nincorrect rollouts dominate trajectories observed during training. For example, Figure 11 illustrates that,\non a curated set of Olympiad-level Math problems, more than 80% of rollout groups contain no successful\ntrajectories at the start of training. Being able to extract useful learning signals from these failed rollouts\nwould therefore tap into an abundant source of supervision that would otherwise be completely discarded\nby outcome-reward RL, as advantages within these groups collapse to zero.\nThere are two additional reasons for focusing on incorrect rollouts. First, credit assignment is more\ndifficult for longer trajectories, as the contribution of individual reasoning steps is harder to disentangle\nfrom that of subsequent steps. Prior work has shown that long trajectories overwhelmingly correspond to\nincorrect rollouts, with failed attempts averaging 10,000 to 15,000 more tokens than successful ones\n[11, 45, 49]. By focusing on incorrect rollouts, we target the regimes where credit assignment is most\ndifficult. Second, while positive advantages reinforce the trajectories that lead to success, negative\nadvantages only indicate suboptimality without specifying how the trajectories should be modified. This\nresults in high-variance gradients and inflated entropy during training [41], making learning inefficient.\nAddressing credit assignment in incorrect rollouts is, therefore, critical to enabling sample-efficient\nlearning especially when successful trajectories are rare.\n0\n250\n500\nStep # of first mistake\n0\n5\n10\n% of problems\n0\n5000\n10000\nToken # of first mistake\n0\n5\n10\n% of problems\nFigure 2: Location of first occurring mistakes in incorrect\ntrajectories. Incorrect rollouts often do not begin with mis-\ntakes and may contain a significant number of correct preceding\nsteps/tokens. Detected by Gemini 2.5 Pro with Prompt 3.\nWhere within incorrect trajectories is credit\nassignment needed? In Figure 2, we visualize\nthe locations of the first error that derails incor-\nrect trajectories. We find that these errors can\noccur at a diverse set of points throughout the\ntrajectory, often far from the initial steps, with\nmore than 60% of the first errors arising after the\n50th step. Consequently, effective credit assign-\nment for incorrect rollouts requires a method that\ncan precisely identify errors occurring at many\ndifferent points along a trajectory.\n3. Credit Assignment via Self-Proposed Interventions\nWe develop a scalable approach for credit assignment in LLMs. This reduces to two technical challenges:\n(i) identifying which reasoning steps are problematic and should receive low credit, and (ii) searching for\nalternative steps that attain higher credit. Most existing solutions to problem (i) rely on estimating value\nfunctions, which remains computationally and statistically expensive for LLMs. Approaches to problem\n(ii) typically require step-level on-policy RL; however, given the inherent difficulties of RL training for\nLLMs, these methods introduce an additional layer of complexity for practitioners. In this section, we\nintroduce an alternative approach to credit assignment that bypasses these challenges by leveraging the\ndifficulty gap between the tasks of instruction-following and problem-solving for LLMs.\nMain idea. Our approach collapses the two steps above into a single procedure that directly proposes an\nimproved reasoning step conditioned on the policy\u2019s current trajectory. While such a procedure might\n4\nInT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\nLet a(0), a(1), a(2), \u2026 be an infinite sequence of integers that is not eventually\nperiodic, which means there do not exist integers N and d > 0 such that a(n + d)\n= a(n) for all integers n \u2265 N. Let S be the set of 2025-tuples containing (a(n + 1),\n\u2026, a(n + 2025)) for n = 0, 1, 2, \u2026. Find the minimum number of elements of S.\n\u2026\nSo if the sequence is not eventually periodic, then the set S cannot eventually\nbecome periodic. But S can still have finitely many distinct elements \u2014 we just\ncan't have that the values eventually repeat in a periodic fashion.\nBut here's the crucial point:\nIf the set S has finitely many\nelements, then the sequence is\neventually periodic.\nProblem \ud835\udc99\nSolution\nPrefix \ud835\udc9a!\ud835\udc95\u2217\nMistake \ud835\udc9a\ud835\udc95\u2217\n\u2026\nFinal Answer: \u221e\n~\ud835\udf45(& |\ud835\udc99, \ud835\udc9a!\ud835\udc95\u2217, \ud835\udc9a\ud835\udc95\u2217)\n\u2026\nFinal Answer: 2026\nIncorrect\n\u274c\n~\ud835\udf45(& |\ud835\udc99, \ud835\udc9a!\ud835\udc95\u2217, *\ud835\udc9a\ud835\udc95\u2217)\nCorrect\n\u2705\nIntervention *\ud835\udc9a\ud835\udc95\u2217\nFind the number of polynomials\nf(t) = A0 + A1 t + A2 t\u00b2 + A3 t\u00b3 + t\u2074\nsuch that A0, \u2026 , A3 are integers and all roots of f in the complex plane have\nabsolute value \u2264 1.\n\u2026\nTherefore, no root can have |z| < 1 unless its reciprocal is also a root \u2014 but\nthen |z| > 1, contradiction.\nTherefore, all roots must have |z| = 1.\nWait \u2014 is that true?\nYes! This is a key point.\nKey Theorem (in complex analysis):\nWait \u2014 is that true? Let\u2019s test this\nconclusion with a simple example.\nWhat about the polynomial f(t) = t\u2074?\nThe coefficients are all integers (A3 =\nA2 = A1 = A0 = 0), and all its roots are\nz = 0. The absolute value is |0| \u2264 1, so\nthis polynomial should be counted.\nHowever, its roots are not on the unit\ncircle.\nThis means my conclusion that all\nroots must have |z| = 1 is incorrect.\n\u2026\nFinal Answer: 21\n\u2026\nFinal Answer: 2026\nIncorrect\n\u274c\nCorrect\n\u2705\nConsider a sequence that takes only\nfinitely many values (e.g., only 0 and\n1). Even if the sequence is not\neventually periodic, it might still have\nonly finitely many distinct 2025-\ntuples. For example, the Fibonacci\nword over {0,1} is not eventually\nperiodic, but every block of length k\nappears exactly once in a certain\npattern \u2014 in fact, it has exactly\nk+1distinct blocks of length k. Can you\nthink of a way to construct such a\nsequence? Try to understand how the\nnumber of distinct blocks relates to\nthe periodicity.\nFigure 3: Example mistakes encountered by Qwen3-4B-Instruct and proposed interventions. By correcting the mistakes\nwith interventions, the rollouts lead to correct final answers as opposed to originally incorrect ones.\nappear to require a stronger LLM, we show that meaningful corrective steps can be generated using the very\nsame model. The key is to exploit the asymmetry in task difficulty between generating correct solutions\nfrom scratch and verifying individual steps by comparing against the reference solution. In practice, base\nmodels are often far more reliable at the latter: they can compare a generated reasoning trace against a\nknown correct solution even when they cannot produce the solution themselves. Accordingly, we instruct\nthe base model to use test-time compute to effectively perform a \u201cdiff\u201d between a reasoning trace and\na human-written reference solution, identify the step at which the trace goes astray, and propose a\nsingle-step corrective intervention. Next, we explain our approach for implementing this idea.\n3.1. Proposing Interventions through Self-Verification\nWe leverage instruction-following LLMs to verify their own reasoning traces and propose counterfactual\nsteps that lead to improved outcomes, circumventing the need for expensive rollouts or explicit training\nand optimization of PRMs. We implement this idea via a two-stage prompting procedure.\nFirst, given a model-generated reasoning trace, we instruct the base model to verify each step of the\ntrajectory for logical correctness, conditioned on access to a reference solution. This produces a list of\nreasoning steps that are potentially incorrect, which we require the model to index explicitly by their\nposition in the trajectory. We then focus on proposing a corrective alternative for the first incorrect step.\nThis design choice is motivated by the fact that the initial error must be corrected for an incorrect rollout\nto become correct. Moreover, errors that occur later in the trajectory are often consequences of earlier\nmistakes; once an early incorrect step is fixed, the remainder of the trajectory is unlikely to follow the\nsame erroneous path, rendering corrections to later errors potentially unnecessary.\nUpon identifying the first incorrect step y\ud835\udc61* with step number \ud835\udc61*, we proceed to the second stage, in\nwhich the model generates an alternative next step \u02dcy\ud835\udc61* to replace y\ud835\udc61*. We refer to this alternative as an\nintervention, whose purpose is to steer the remainder of the trajectory toward the correct answer. We\nprovide concrete examples of generated single-step interventions in Figure 3, with additional examples\nin Appendix A and B. Concretely, we obtain the error location \ud835\udc61* and corrective step \u02dcy\ud835\udc61* using a single\n5\nInT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\nIntervention Proposed By\nRollouts Are Conditioned On\nCoverage\nAccuracy\nOmni-MATH subset with pass@128 = 0, values estimated with 32 rollouts\n1. -\nproblem x\n40 / 334\n0.0984%\n2. -\nproblem x + correct prefix y<\ud835\udc61*\n31 / 334\n0.0726%\n3. -\nproblem x + correct prefix y<\ud835\udc61* + original step y\ud835\udc61*\n29 / 334\n0.0713%\n4. 4B-Instruct (Ours)\nproblem x + correct prefix y<\ud835\udc61* + intervention \u02dcy\ud835\udc61*\n80 / 334\n1.56%\n5. 4B (Ours)\nproblem x + correct prefix y<\ud835\udc61* + intervention \u02dcy\ud835\udc61*\n73 / 334\n2.65%\n6. 30B-A3B-Instruct (Ours)\nproblem x + correct prefix y<\ud835\udc61* + intervention \u02dcy\ud835\udc61*\n101 / 334\n2.87%\n7. 4B-Instruct w/o. ref. sol.\nproblem x + correct prefix y<\ud835\udc61* + intervention \u02dcy\ud835\udc61*\n47 / 334\n0.171%\nSubset with hint-guided pass@16 = 0, values estimated with 16 rollouts\n8. -\nproblem x + hint h + correct prefix y<\ud835\udc61*\n11 / 176\n2.38%\n9. 4B-Instruct (Ours)\nproblem x + correct prefix y<\ud835\udc61* + intervention \u02dcy\ud835\udc61*\n18 / 176\n3.05%\n10. 4B-Instruct\nproblem x + hint h + correct prefix y<\ud835\udc61* + intervention \u02dcy\ud835\udc61*\n25 / 176\n4.62%\nFinal Training Set\n11. 4B-Instruct (Ours)\nproblem x + correct prefix y<\ud835\udc61* + intervention \u02dcy\ud835\udc61*\n1076 / 4048\n4.27%\nTable 1: Rollouts conditioned on proposed interventions and alternative strategies. Accuracy measures the average\nreward, and coverage measures the number of problems that see at least one correct rollout. Conditioning rollouts on our\nproposed interventions yields the highest coverage and average reward on the hard problem set, outperforming approaches that\ndo not use interventions (e.g., na\u00efve rollouts or conditioning on hints) and those that generate interventions under weaker\nsupervision (e.g., without reference solutions). We also find that our interventions can be combined with existing approaches\nsuch as hint-guided rollouts (see rows 8-10) to obtain stronger results. 4B-Instruct denotes Qwen3-4B-Instruct-2507, 4B denotes\nQwen3-4B, and 30B-A3B denotes Qwen3-30B-A3B-Instruct-2507.\nquery \u223c\ud835\udf0b(\u00b7 | x, y, pInT); the instruction pInT is provided in Box 2. Our prompt borrows components\nfrom Huang & Yang [14] to produce step-by-step verification logs of the trajectory and to identify the\nfirst mistake by classifying each step. Because \ud835\udc61* and \u02dcy\ud835\udc61* are generated by the same model that produced\nthe original y, our approach enables credit assignment via self-generated feedback. Additionally, in the\nfew cases where the intervention contained the final answer itself; we discard these cases to prevent\nany leakage from the reference solution, so that the model does not simply learn to memorize the final\nanswer, but rather learns to complete the solution. Next, we evaluate whether interventions generated in\nthis manner results in successful rollouts on problems that the base model could not solve at first.\n3.2. Do Interventions Improve Success Rates?\nWe empirically evaluate self-generated interventions using the Qwen3-4B-Instruct-2507 base model\nby measuring whether a proposed intervention improves the probability of success on rollouts that\nwould otherwise fail. Specifically, we concatenate the problem x with the reasoning steps preceding the\nidentified error y<\ud835\udc61*, followed by the intervention \u02dcy\ud835\udc61*. We then sample continuations from the model\nby conditioning on this prefix, i.e., \ud835\udf0b(\u00b7 | x, y<\ud835\udc61*, \u02dcy\ud835\udc61*) (row 4 \u201c4B-Instruct\u201d in Table 1), and evaluate the\nresulting trajectories to obtain an unbiased estimate of the expected reward when continuing from the\nintervention. In Figure 4 (left), we isolate the effect of interventions by comparing this value against\ntwo baselines: (1) continuing the rollout from the erroneous step without applying the intervention\n\ud835\udf0b(\u00b7 | x, y<\ud835\udc61*, y\ud835\udc61*) (row 3 above \u201c4B-Instruct\u201d), and (2) resampling from the step immediately preceding\nthe error \ud835\udf0b(\u00b7 | x, y<\ud835\udc61*) (row 2). Our results show that interventions increase the average reward from\n0.0713% to 1.56%. This 22\u00d7 increase demonstrates that while the base model may be unable to solve\nthese problems initially, it can leverage self-verification capabilities to generate single-step interventions\n\u02dcy\ud835\udc61* to replace the original erroneous step y\ud835\udc61* and successfully steer the model toward a correct solution.\n6\nInT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\nyt *\nyt *\n0.0\n0.5\n1.0\n1.5\naccuracy\n0.0726% 0.0713%\n1.56%\nyt *\nyt *\n0\n10\n20\ncoverage\n31 / 334 29 / 334\n80 / 334\nFigure 4: Performance comparison of interventions. Con-\ntinuing the rollout by concatenating the proposed intervention\nstep \u02dcy\ud835\udc61* to the prefix y<\ud835\udc61* yields substantially higher accuracy\n(left) and a larger number of unique problems solved (right)\nthan either continuing with the original erroneous step y\ud835\udc61* or\ncontinuing directly from the prefix without any intervention (\u2212).\nBeyond average reward, we also examine if utiliz-\ning our proposed interventions enables the model\nto solve new problems. To assess coverage, we\ncompute the number of problems on which we\ncan generate at least one correct rollout (out of\n32) under each of the three settings in Figure 4\n(right), which correspond to rows 2, 3, 4 in Ta-\nble 1. Continuing from interventions achieves the\nhighest coverage, solving 80 out of 334 unique\nproblems compared to 29 out of 334 when con-\ntinuing from the error. This justifies the efficacy\nof interventions, and further suggests that fine-\ntuning models to internalize interventions could potentially increase the fraction of problems receiving\nnon-zero rewards during downstream RL, which would enable learning across a broader range of tasks.\nResults: Interventions outperform hint-guided rollout generation. Having established that interven-\ntions improve reward and enable solving problems that were previously unsolvable, we now compare this\napproach to alternative ways of leveraging reference solutions. As discussed in Section 6, several concur-\nrent works incorporate reference solutions into RL by conditioning the policy on partial solution prefixes\nas hints. These hints exploit the instruction-following capabilities of base models to \u201csteer\u201d generation\ntoward the correct region of the solution space, enabling successful completions on otherwise difficult\nproblems. Since hints cannot directly reveal the solution, we construct them by randomly sampling a\nprefix from the initial one-third of the reference solution. We study whether hint-guided rollouts are\nmore effective than intervention-based rollouts, and whether the two approaches can be combined.\nTable 1 compares these settings. Conditioning on interventions alone (row 9; \ud835\udf0b(\u00b7 | x, y<\ud835\udc61*, \u02dcy\ud835\udc61*)) achieves\nhigher problem coverage (18/176 problems solved) than hint-guided rollouts (row 8) that solves only\n11/176 problems. However, the two methods are complementary: conditioning on both hints and\ninterventions (row 10) yields the best overall coverage (25/176 problems solved). This result is perhaps\nnatural: hints guide the model toward a promising initial direction, while interventions enable fine-grained\ncredit assignment when errors arise in the middle of a rollout.\nAblation: Reference solutions improve effectiveness of interventions. We next isolate the role of\nreference solutions in generating effective interventions. In particular, we test the alternative hypothesis\nthat the verification\u2013generation asymmetry of the base model alone [40] is sufficient to produce high-\nquality interventions. To do so, we evaluate an ablation shown in Table 1, row 7, where we prompt\nthe model to propose an intervention given only an incorrect reasoning trace, without the reference\nsolution. On the one hand, conditioning on interventions outperforms the na\u00efve baselines (rows 1\u20133),\ndoubling accuracy and increasing coverage by 7, supporting the claim that effective interventions can be\nyielded even without privileged information from reference solutions. On the other hand, interventions\ngenerated in this manner are significantly weaker than those generated with reference solutions (row 4),\nwhich clearly highlights the critical role of reference solutions in producing high-quality interventions.\nAblation: Instruction-following capabilities of the base model are critical. We find that the instruction-\nfollowing ability of the base model is a key determinant of intervention effectiveness. In row 5 of Table 1,\nwe evaluate Qwen3-4B, a model optimized primarily for reasoning rather than instruction-following.\nDespite its strong performance on math reasoning benchmarks, this model frequently fails to adhere\nto the instructions required for intervention generation. For example, it may switch to directly solving\n7\nInT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\nthe problem midway through comparing the reasoning trace with the reference solution, or neglect to\nenclose proposed interventions within the required <intervention></intervention> tags. Such\ninstruction-following failures prevent the model from producing usable interventions, resulting in lower\ncoverage (73 out of 334, row 5) compared to self-generated interventions from the instruction-tuned\nmodel (80 out of 334, row 4). These results demonstrate that effective intervention generation depends\nnot only on raw reasoning ability, but also on robust instruction-following abilities.\nAblation: More capable base models propose better interventions. Finally, we study how the size\nof the model used to propose interventions affects performance. In addition to using the base model,\nQwen3-4B-Instruct-2507, which also generates the original reasoning traces, we experiment with a larger\nmodel from the same family, Qwen3-30B-A3B-Instruct-2507 (row 6), to generate interventions. Using\nthe larger model yields an approximately 2\u00d7 improvement in accuracy and solves 21 additional problems\nout of 334, indicating that the quality of generated interventions scales with model capacity.\nTakeaways: Effectiveness of our proposed interventions\n\u2022 Conditioning rollouts on proposed interventions increases success rates by \u226520\u00d7.\n\u2022 Hint-guided rollout generation is complementary to interventions. Both improve overall reward.\n\u2022 Instruction-following capability is critical: the instruction-tuned 4B-Instruct model solves 7 more\nproblems (out of 334) than the reasoning-focused 4B model.\n\u2022 Intervention quality scales with model capacity: interventions proposed by 30B-A3B-Instruct\nachieve 2\u00d7 higher accuracy and solves more problems compared to 4B-Instruct.\n4. InT: Intervention Training for Credit Assignment\nNext, we use the generated interventions for training. Our training procedure is deliberately simple: we\nperform supervised fine-tuning on the collected intervention data to patch the base model, preparing it for\nsubsequent RL training. Then RL training goes as usual, but now it can learn from these very challenging\nproblems that would have otherwise largely produced incorrect rollouts during RL. As we show in our\nexperiments, this patching step is sufficient to enable the model to solve previously unsolvable problems\nand leads to improvements in performance. We now describe our training procedure in detail.\n4.1. Training via Supervised Fine-Tuning on Intervention Data\nConfiguration\nCoverage\nAccuracy\nPrefix, no suffix, filter\n202/235\n7.71%\n- no filter\n196/235\n5.06%\n- no prefix\n162/235\n2.87%\n+ suffix\n111/235\n2.31%\nTable 2: A study of SFT design choices for learn-\ning from interventions. We compare different SFT\nconfigurations: whether the prefix y<\ud835\udc56is cloned,\nthe suffix y>\ud835\udc56is included, and interventions are\nfiltered to retain only those that yield at least one\ncorrect rollout. Cloning the prefix and excluding\nthe suffix while applying the correctness filter yields\nthe highest coverage and accuracy. The 235 prob-\nlems were subsampled from DeepScaleR.\nA natural starting point for leveraging generated interven-\ntions during training is by running supervised fine-tuning\n(SFT) on the collected intervention data. The goal of this\nstep is to teach the model to internalize the corrective pat-\nterns expressed by interventions, so that at test time, when\nreference solutions are no longer available, it can sponta-\nneously generate similar corrective steps. This immediately\nraises the central design question surrounding this simple\ntraining procedure: which parts of an intervention-guided\nrollout should be used in training?\nConsider generating a full solution trajectory using an inter-\nvention. For a problem x and an initial response y \u223c\ud835\udf0b(\u00b7 | x),\nwe generate an intervention \u02dcy\ud835\udc61* \u223c\ud835\udf0b(\u00b7 | x, y, pInT) to replace the erroneous step y\ud835\udc61*. We then continue\n8\nInT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\n\u274c\n\u2713\n\u2713\n\u2713\n\u2026\n\u274c\nStep !\u2217\nSelf-verify against ref. solution\nStep 1\nStep 2\nStep 3\n\u2026\n\u2713\n\" ~ % (' |))\n+\"\"\u2217 ~ % ' ), \", -#$%)\n!\n1. Generate interventions\n2. SFT\n\u2026\n3. RL\n~ % (' |), \"&\"\u2217, +\"\"\u2217)\nFigure 5: Intervention training (InT). InT verifies incorrect rollouts against reference solutions to identify the step \ud835\udc61* at\nwhich the first error y\ud835\udc61*) occurs, proposes alternative step \u02dcy\ud835\udc61*), and performs SFT on these steps before RL.\nthe response by sampling \u02dcy>\ud835\udc61* \u223c\ud835\udf0b(\u00b7 | x, y<\ud835\udc61*, \u02dcy\ud835\udc61*). Given the resulting intervention-guided trajectory,\nwe consider several design choices for SFT: whether to train on the prefix y<\ud835\udc61*, the suffix \u02dcy>\ud835\udc61*, and\nwhether to include only the interventions whose continuations \u02dcy>\ud835\udc61* lead to a correct final answer.\nMain idea. We hypothesize that cloning only the proposed intervention without its preceding prefix\nwould be insufficient. If the prefix y<\ud835\udc61* is not reinforced during SFT, the fine-tuned model may generate\nalternative prefixes y\u2032\n<\ud835\udc61* for the same problem x at test-time, introducing different errors than those in\nthe original rollout that was used for training. In such cases, the intervention \u02dcy\ud835\udc61* is no longer relevant."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper\n\n**Title:** InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\n\n**Authors:** Matthew Y. R. Yang1, Hao Bai2, Ian Wu1, Gene Yang1, Amrith Setlur1 and Aviral Kumar1  \n**Affiliations:** 1Carnegie Mellon University, 2University of Illinois Urbana-Champaign\n\n#### Main Contributions\nThe paper introduces Intervention Training (InT), a novel approach to improve credit assignment in large language models (LLMs) for reasoning tasks. InT proposes single-step interventions to replace incorrect intermediate steps in the reasoning traces of LLMs, enabling more effective learning signals and improved performance.\n\n**Methodology**\n1. **Intervention Proposals:** The model generates corrective interventions by self-verifying its own reasoning trace against a reference solution.\n2. **Training Process:** InT uses supervised fine-tuning (SFT) to train the base model on these interventions, upweighting their likelihood in place of incorrect steps.\n\n#### Key Findings\n1. **Improved Accuracy:** InT achieves notable improvements over existing methods, with an average accuracy improvement of nearly 14% on IMO-AnswerBench compared to a 4B-parameter base model.\n2. **Effective Credit Assignment:** The approach successfully identifies and corrects errors in reasoning traces, leading to more accurate final answers.\n\n#### Conclusions\nInT demonstrates the feasibility of leveraging LLMs' self-verifying capabilities for credit assignment without explicitly training value functions or modifying RL objectives. By proposing targeted interventions based on reference solutions, InT effectively addresses the problem of incorrect rollouts and improves overall model performance across challenging mathematical reasoning benchmarks.\n\n### Detailed Breakdown\n\n#### 1. Introduction\n- **Context:** Outcome-reward reinforcement learning (RL) has shown promise in improving LLMs' reasoning capabilities but suffers from issues like indiscriminate penalties for correct steps and lack of learning signals on difficult problems.\n- **Problem Statement:** The challenge is to identify and selectively penalize incorrect intermediate steps while reinforcing promising ones, a task that is challenging for existing approaches.\n\n#### 2. Preliminaries, Notation, and Problem Statement\n- **Learning Objective:** Maximize reward by updating the policy with gradient-based methods.\n- **Credit Assignment Challenges:** Traditional RL assigns uniform rewards to all steps in failed traces, leading to undesirable behaviors or lack of learning signals on difficult problems.\n- **Problem Definition:** Focus on incorrect rollouts where credit assignment is most critical and leverage reference solutions for training.\n\n#### 3. Credit Assignment via Self-Proposed Interventions\n- **Main Idea:** InT proposes a single-step intervention conditioned on the current trajectory, enabling effective credit assignment without explicit value functions or branch rollouts.\n- **Implementation Details:** The model verifies its reasoning trace against a reference solution to identify and correct errors, then trains on these interventions using SFT.\n\n#### 4. Experiments\n- **Empirical Evaluation:** InT's effectiveness is evaluated through experiments with Qwen3-4B-Instruct base models, showing significant improvements in accuracy and problem coverage.\n- **Ablation Studies:** The paper investigates the role of reference solutions and instruction-following capabilities in generating effective interventions.\n\n#### 5. Conclusion\nInT provides a simple yet powerful approach to credit assignment for LLMs by leveraging their self-verifying capabilities. This method not only improves performance on challenging benchmarks but also opens up new avenues for further research into more efficient and effective RL training paradigms for LLMs."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "LLM Augmented Intervenable Multimodal Adaptor for Post-operative\nComplication Prediction in Lung Cancer Surgery\nShubham Pandey\u2020, Bhavin Jawade\u2020, Srirangaraj Setlur\u2020, Venu Govindaraju\u2020, Kenneth Seastedt\u2020\u22c6\n\u2020University at Buffalo, \u22c6Roswell Park Comprehensive Cancer Center\nBuffalo, NY, USA\n{spandey8, bhavinja, setlur, govind}@buffalo.edu, Kenneth.Seastedt@RoswellPark.org\nAbstract\nPostoperative complications remain a critical concern in\nclinical practice, adversely affecting patient outcomes and\ncontributing to rising healthcare costs. We present MIRA-\nCLE, a deep learning architecture for prediction of risk of\npostoperative complications in lung cancer surgery by inte-\ngrating preoperative clinical and radiological data. MIRA-\nCLE employs a hyperspherical embedding space fusion of\nheterogeneous inputs, enabling the extraction of robust, dis-\ncriminative features from both structured clinical records\nand high-dimensional radiological images.\nTo enhance\ntransparency of prediction and clinical utility, we incor-\nporate an interventional deep learning module in MIRA-\nCLE, that not only refines predictions but also provides in-\nterpretable and actionable insights, allowing domain ex-\nperts to interactively adjust recommendations based on\nclinical expertise.\nWe validate our approach on POC-\nL, a real-world dataset comprising 3,094 lung cancer pa-\ntients who underwent surgery at Roswell Park Comprehen-\nsive Cancer Center.\nOur results demonstrate that MIR-\nACLE outperforms various traditional machine learning\nmodels and contemporary large language models (LLM)\nvariants alone, for personalized and explainable postop-\nerative risk management.\nOur codebase is available at\nhttps://github.com/KNITPhoenix/MIRACLE.\n1. Introduction\nLung cancer remains the leading cause of cancer related\nmortality worldwide and surgical resection provides a 2.5-\nfold survival advantage over non-surgical management.\nHowever, surgery carries substantial perioperative risk, with\npostoperative morbidity exceeding 40% and mortality sur-\npassing 5%, underscoring the need for accurate, individu-\nalized preoperative risk assessment [3, 23, 25, 28]. Cur-\nrent risk models, including those developed by the Society\nof Thoracic Surgeons (STS) [21], primarily rely on global\npulmonary function measures such as FEV1 and DLCO.\nWhile clinically useful, these population level indicators\nfail to capture nuanced patient-specific interactions between\nlung tissue heterogeneity, comorbidities and tumor biology\n[7, 15, 17, 26, 30]. Furthermore, such models do not incor-\nporate imaging biomarkers such as emphysema distribution\n[8], parenchymal texture [13] or tumor-associated remodel-\ning [27], which are known to influence postoperative pul-\nmonary outcomes. As a result, surgeons currently depend\non population level statistics and subjective clinical judg-\nment which lead to suboptimal decisions where patients\nwho might have tolerated surgery are denied surgery, while\nothers at high risk undergo procedures without adequate\npostoperative planning [6]. Radiomics provides a quanti-\ntative pathway for capturing lung heterogeneity and tumor\nmicroenvironment through high-dimensional CT features\n[33].\nWhen combined with clinical variables, radiomics\nimproves postoperative risk stratification [1, 12, 13]. How-\never, most radiomic and ML approaches function as static\nblack boxes offering predictions without transparency, clin-\nical interpretability or mechanisms for physician interven-\ntion. Meanwhile, large language models enable clinically\ngrounded explanations [2, 5], but remain disconnected from\nrisk prediction workflows.\nIn this work, we introduce MIRACLE (Multi-modal In-\ntegrated Radiomics And Clinical Language-based Expla-\nnation), a unified deep learning framework that integrates:\n(i) structured preoperative clinical features, (ii) CT derived\nradiomic biomarkers and (iii) LLM generated, evidence-\ngrounded natural language explanations to support risk pre-\ndiction and interpretability. Our key contributions are: (i)\nUnified Multimodal Risk Modeling: Joint fusion of clini-\ncal data, radiomics and language-based explanations to en-\nable accurate, interpretable and clinician-editable postop-\nerative risk assessment, (ii) Bayesian MLP Architecture:\nRobust learning under small, imbalanced datasets with cal-\nibrated uncertainty estimation that mitigates overfitting and\n(iii) Real-World Evaluation: Training and validation on\nPOC-L dataset, consisting of 3,094 lung cancer surgical pa-\ntients treated between 2009\u20132023 at Roswell Park Compre-\n1\narXiv:2601.14154v1  [cs.CV]  20 Jan 2026\nhensive Cancer Center. We benchmark MIRACLE against\nthoracic surgeons (mean experience 17.75 years), standard\nML models and multiple open-source LLMs, achieving\nsuperior performance across AUC and True Postive Rate\n(Sensitivity) at clinically relevant False Positive Rates (1 -\nSpecificity). To our knowledge, this is the first method that\njointly integrates preoperative clinical and radiological fea-\ntures with clinician intervenable, natural-language expla-\nnations, transforming risk assessment from a static black-\nbox output into an interactive, transparent clinical decision-\nsupport system.\n2. Related Work\nIn this section, we review prior contributions across three\nkey domains that inform the development of our proposed\nframework:\nPostoperative Complication Prediction.\nEarly work\nin postoperative risk stratification relied primarily on sta-\ntistical models and classical machine learning models ap-\nplied to large clinical registries [34]. More recent studies\ndemonstrated that ensemble learning methods such as Ran-\ndom Forest and Gradient Boosting can improve predictive\nperformance, reporting AUC gains of 3-5% over logistic re-\ngression for major surgical outcome prediction [11]. How-\never, these approaches treat risk estimation as a purely data-\ndriven classification task and provide limited interpretabil-\nity, lacking mechanisms for clinician interaction or inter-\nvention. Current workflows therefore remain dependent on\npopulation-level thresholds and physician judgment without\nintegrated computational decision support. To our knowl-\nedge, no prior framework unifies structured clinical data,\nradiological biomarkers and explainable language model,\noutputs into a single predictive pipeline.\nLLMs in Clinical Decision Support. LLMs have re-\ncently shown strong potential in clinical reasoning tasks,\nincluding summarization, diagnostic reasoning and report\ngeneration. GPT-3 demonstrated zero and few shot pro-\nficiency on medical QA and clinical note summarization\ntasks [5], while Med-PaLM achieved state of the art re-\nsults on biomedical QA benchmarks [29].\nIn radiol-\nogy, instruction-tuned LLMs have been evaluated for au-\ntomated report generation, achieving performance compa-\nrable to practicing radiologists in early assessments. Sim-\nilarly, domain-adapted transformer encoders such as Clin-\nicalBERT have been applied to clinical concept extraction\nand outcome prediction from electronic health records [2].\nDespite these advances, existing approaches treat text gen-\neration and clinical outcome prediction as independent pro-\ncesses. No prior LLM based framework directly predicts\npostoperative complication risk in lung cancer surgery or\nintegrates language based explanations jointly with multi-\nmodal clinical and radiomic predictors, to our knowledge.\nExplainability and Human\u2013AI Intervention. Explain-\nability is central to adopting AI for clinical decision mak-\ning. Previous feature attribution methods such as LIME [24]\nand SHAP [20] provide insights into feature importance for\nclinical prediction models, including radiomics. More inter-\npretable architectures such as Concept Bottleneck Models\n(CBMs) decompose predictions into clinician readable in-\ntermediate concepts [16] but require extensive manual con-\ncept definition and annotation. Recent work has explored\nincorporating clinician feedback into AI inference pipelines\n[22]. However, these systems typically rely on predefined\nconcept spaces or offer limited integration between explana-\ntion and intervention [10]. Few existing frameworks unify\nhigh predictive performance, natural language explanations\nand clinician editable intervention mechanisms. MIRACLE\naddresses this gap by embedding LLM generated and evi-\ndence grounded explanations directly within a multimodal\npredictive architecture that supports real time clinician ad-\njustment of model outputs.\n3. Methods\nIn this section, we describe our proposed deep learning ar-\nchitecture for predicting risk of postoperative complications\nfor lung cancer surgery candidates. Our method integrates\npreoperative clinical and radiological features with explain-\nable remarks generated by a large language model (LLM).\nThe key components of our approach are defined in the fol-\nlowing subsections.\n3.1. Problem Definition\nLet c \u2208R17 represent a vector of 17 preoperative clinical\nfeatures (detailed in supplementary material) in our dataset.\nDue to the limitations of no direct image access from the\ncollaborating cancer research center, radiological features\nwere extracted from a patient\u2019s chest CT scan using Total\nSegmentator [32] and pyradiomics [31] library, quantifying\nimaging biomarkers relevant for lung tissue characteriza-\ntion, represented by a vector of 113 preoperative radiolog-\nical features (detailed in supplementary material) and de-\nnoted by r \u2208R113. M represent textual remarks generated\nby an LLM based on a clinical summary constructed from\nc, a carefully crafted contextual knowledge bank K and a\npredefined prompt P. The clinical summary is obtained via\na deterministic function S = fsummary(c), generating the re-\nmarks as:\nM = LLM(S, K, P)\nOur objective is to learn a function as follows:\nf : (c, r, M) \u2192y \u22080, 1\nwhich predicts the probability of postoperative complica-\ntions y. For this study, we consider a set of ten postoper-\native complication events (defined in supplementary mate-\n2\nrial) C = {C1, C2, . . . , C10} and define\ny =\n(\n1,\n\u2203Ci \u2208C such that patient experiences Ci,\n0,\notherwise.\n3.2. MIRACLE\nMIRACLE stands for Multi-modal Integrated Radiomics\nAnd Clinical Language based Explanation. The proposed\narchitecture consists of three main modules: (i) Two sepa-\nrate Bayesian MLP networks, one each for clinical and ra-\ndiological features, (ii) An encoding module using a frozen\nencoder, fine-tuned on medical data for textual remarks and\n(iii) A fusion network for final prediction. The processing\npipeline is illustrated as follows:\n(i) Clinical Summary and Remarks Generation: A\nfixed template first renders the raw clinical feature vector c\ninto a coherent, human readable summary S = fsummary(c).\nThis summary enumerates each feature given in clinical\nvector, such as patient\u2019s age, smoking history, etc. This en-\nsures strict traceability between structured inputs and gen-\nerated text. This summary is concatenated with a domain\nknowledge bank K and a task-specific prompt P to form\nthe composite input {S, K, P} to a frozen LLM, yielding\na natural language clinical remark M.\nEach remark M\nhighlights patient-specific risk or protective indicators that\nare grounded in both the patient\u2019s clinical data and evi-\ndence based guidelines encoded within K, forming the in-\nterpretable explanation channel for the prediction model.\n(ii) Feature Embedding:\nEc = fc(c) \u2208Rd; Er = fr(r) \u2208Rd; Em = fm(M) \u2208Rd\nwhere fc(\u00b7) and fr(\u00b7) are Bayesian MLPs based networks\nthat encode preoperative clinical and radiological features\nrespectively into a d dimensional vector and fm(\u00b7) is a\nfrozen text encoder, fine-tuned on clinical texts, that em-\nbeds the LLM generated remarks (also in a d dimensional\nvector) so that it can be used by the fusion network.\n(iii) Feature Fusion and Prediction: The embeddings\nfrom each module is fused using a weighted sum, to form a\nglobal representation:\nE = (wc \u2217Ec + wr \u2217Er + wm \u2217Em) \u2208Rd\nThis fused embedding is then passed through a final\nBayesian MLP Network ffinal(\u00b7), with the output activated\nby a sigmoid function to yield the predicted probability of\npost-operative complications:\n\u02c6y = \u03c3 (ffinal(E)) ,\nwhere \u03c3(\u00b7) denotes the sigmoid function.\n3.2.1. Remarks Generation Approach:\nThis component provides the explainability layer of MIR-\nACLE, so surgeons can plan ahead of the surgical pro-\ncedure and enables clinician guided intervention during\ninference.\nAfter deriving the clinical summary S from\nthe patient\u2019s clinical data c using a predefined fixed tem-\nplate, it is concatenated with a predefined prompt P and\na knowledge bank K. The knowledge bank K is a man-\nually curated textual repository developed in collabora-\ntion with thoracic surgeons and oncology specialists.\nIt\nencodes domain relationships linking preoperative clini-\ncal factors to postoperative complication risks across pul-\nmonary and respiratory outcomes.\nContent is derived\nfrom authoritative textbooks, peer-reviewed surgical on-\ncology literature and published peri-operative risk assess-\nment guidelines.\nThe knowledge bank includes descrip-\ntions of feature ranges, documented interactions between\nrisk factors and mechanistic explanations of surgical com-\nplication pathways. We released knowledge bank along-\nside the codebase (link in abstract). The composite input\n{S, K, P} supplied to an LLM directs the model to gener-\nate grounded, evidence-consistent explanatory remarks fo-\ncused exclusively on interpretable clinical reasoning. We\nevaluated multiple LLM paradigms, including instruction-\nfollowing, reasoning-oriented and domain-fine-tuned mod-\nels, selecting the optimal performer for each experimental\nconfiguration based on clinical coherence and alignment\nwith domain knowledge. Both the selected LLM and the\ndownstream text encoder remain frozen throughout training\nand inference. This prevents hallucination drift, maintains\nlinguistic stability of the explanation channel and preserves\nthe clinical priors encoded during pre-training.\n3.2.2. Training Strategy:\nFor training, we adopted focal loss [19], as it was built to\naddress class imbalance in the binary classification task,\nwhich aligns with the distribution of our dataset. The fo-\ncal loss we used is defined as:\nLFL(pt) = \u03b1t \u00b7 (1 \u2212pt)\u03b3 \u00b7 BCE(\u02c6y, y)\nwhere BCE represents Binary Cross Entropy loss, pt is the\nmodel\u2019s predicted probability for the positive class, \u03b1t is\nthe balancing factor, which is tuned to address the class im-\nbalance and \u03b3 is the focusing parameter, which is used to\ncontrol the rate at which easy examples are down-weighted.\nThe overall loss for a mini-batch of N samples is:\nL = 1\nN\nN\nX\ni=1\nLFL(pt,i)\nTraining is end-to-end via standard backpropagation over\nall learnable modules. The text encoder is kept frozen, as\nit was already trained on a large corpus of clinical text and\nto preserve the pretrained representation quality, giving us\nstructured embeddings for the remarks.\n3.2.3. Inference:\nDuring inference, a new lung cancer patient\u2019s preoperative\nclinical features c and radiological features r are processed\n3\nas follows:\n1. Clinical Summary and Remarks: A clinical summary\nS = fsummary(c) is generated and together with K and\nP, is used as an input to LLM producing remarks M.\n2. Embedding Computation: Clinical features, radiologi-\ncal features and remarks are embedded using fc(\u00b7), fr(\u00b7)\nand fm(\u00b7) to produce Ec, Er and Em, respectively.\n3. Prediction: All embeddings are then fused to create E,\nwhich is then used in final MLP Network to obtain the\nprediction \u02c6y.\n3.2.4. Clinician Intervention:\nAt inference, the generated remark M is displayed along-\nside the predicted risk probability. Surgeons may edit or\nmodify the text when the automated explanation does not\nfully reflect patient nuance or expert judgment. The modi-\nfied text is re-embedded via fm(\u00b7) and reinjected into the fu-\nsion network, updating the predicted risk score in real time.\nThis design allows human expertise to intervene transpar-\nently and measurably within the prediction loop, while pre-\nserving a structured AI reasoning trail.\n4. Experiments\n4.1. Dataset and Preprocessing\nWe conducted this study on POC-L, a real world dataset\nof 3094 lung cancer patients who underwent surgical re-\nsection at Roswell Park Comprehensive Cancer Center be-\ntween 2009 and 2023.\nThe dataset reflects real clinical\ncases, preserving the demographic imbalance and clinical\nheterogeneity, typical of routine thoracic practice. Patients\nwere split into training (2,694; 22.6% complications), vali-\ndation (200; 47.5%) and testing (200; 53.5%) splits, main-\ntaining the naturally skewed complication distribution. The\ndataset is dominated with White ethnicity patients with rep-\nresentations from African-American and Asian populations.\n57% of patients are female and 43% male. Each case has\n17 structured preoperative clinical variables. Due to data\nsharing constraints, preventing transfer of raw CT volumes,\nimage analysis was performed locally prior to feature shar-\ning. Chest CT scans were segmented using TotalSegmen-\ntator [32], followed by radiomic feature extraction with\nPyRadiomics [31]. After refinement and removal of redun-\ndant descriptors, 113 standardized radiomic features cap-\nturing lung morphology, tumor characteristics and texture\nstatistics were retained. Continuous clinical features were\nnormalized using Min-Max scaling fitted on the training\nsplit, while categorical variables were label encoded. Ra-\ndiomic features were standardized to ensure numerical sta-\nbility. Postoperative complications were defined across ten\nmajor complication events curated by domain experts. For\nmodel supervision, the presence of any of the ten complica-\ntion events was aggregated to produce a binary global out-\ncome label indicating whether a patient experienced at least\none postoperative complication. This labeling formulation\nmirrors real world clinical risk assessment, enabling direct\nbenchmarking of MIRACLE\u2019s predictions against surgical\nexpert judgment under routine care conditions. Due to pa-\ntient privacy restrictions, POC-L dataset cannot be publicly\nreleased. However, all records were de-identified prior to\nanalysis and the study was approved under IRB protocol\nBDR 176423. For reproducibility, we publicly release our\nfull training pipeline and baselines (see abstract). As per our\nknowledge, there is no public dataset that matches POC-\nL\u2019s structure, limiting cross dataset benchmarking. Ongo-\ning efforts are aimed at acquiring and harmonizing multi-\ninstitutional datasets to support broader validation and ro-\nbustness analysis, as POC-L is dominated by one ethnicity,\nproving as one of the limitation of dataset.\n4.2. Evaluation Metrics\nTraditional measures such as accuracy, precision, recall and\nF1-score depend heavily on the choice of decision thresh-\nold which can be biased during comparison of performance\nacross models. Thus to assess predictive performance and\nensure fair comparison with baseline methods, we adopted\nthreshold-independent evaluation metrics.\nAs all models\noutput continuous probabilities , we evaluate performance\nusing AUC and clinically relevant True Positive Rate (Sen-\nsitivity) at fixed False Positive Rates (1-Specificity) of 0.2\nand 0.3. AUC evaluates discrimination capability across all\nthresholds, providing a robust, unbiased measure of clas-\nsification performance. The selected TPR@FPR operating\npoints reflect clinically relevant trade-offs, where high sen-\nsitivity is prioritized while allowing approximately 70\u201380%\nspecificity, as guided by surgical collaborators.\n4.3. LLMs and Remark Generation\nFor interpretable risk assessment, we employ three open-\nsource LLMs: Llama 3.3 70B-Instruct1 (instruction-based),\nDeepSeek R1 Distill Qwen-32B2[9] (general reasoning)\nand OpenBioLLM-70B3[4] (domain-fine-tuned).\nModels\nwere selected to represent leading open-source performance\nwithin each category while remaining feasible under our\navailable computational resources. For each patient, a com-\nposite input {S, K, P} is prepared as described in sec-\ntion 3.3. Each combined input is processed via Hugging-\nFace inference checkpoints of the selected LLM on two\n95GB NVIDIA H100 GPUs. Generation parameters are set\nto max new tokens=2000 and temperature=0.9.\nThe resulting remark M provides a concise, evidence an-\nchored explanation of why the patient is or is not at elevated\n1https://huggingface.co/meta-llama/Llama-3.3-\n70B-Instruct\n2https://huggingface.co/deepseek- ai/DeepSeek-\nR1-Distill-Qwen-32B\n3https : / / huggingface . co / aaditya / Llama3 -\nOpenBioLLM-70B\n4\nFigure 1. Illustration depicting MIRACLE, our intervenable Deep Learning architecture that combines clinical and radiological features,\nalong with providing surgeons with a point of intervention and explainability. (Best viewed digitally)\nrisk for postoperative complications. The full remark gener-\nation code is provided with our codebase (link in abstract).\n4.4. Baselines\nTo contextualize the performance of MIRACLE, we evalu-\nated our approach against two complementary baseline cat-\negories: (1) conventional ML classifiers and (2) standalone\nLLM-based predictors. Additionally, we established a hu-\nman expert baseline by measuring the performance of prac-\nticing surgeons on the held-out test split.\n4.4.1. Classical Machine-Learning Methods\nWe implemented five widely used classifiers: Multivari-\nate Logistic Regression (MVLR), Random Forest Classi-\nfier (RFC), XGBoost, Gradient Boosting Classifier (GBC)\nand LightGBM. All models were trained using combined\nclinical and radiological features with validation based hy-\nperparameter tuning, following previous medical imaging\nbenchmarks [11, 34, 36] and thus serve as representative,\nnon deep learning baselines.\n4.4.2. LLM-Only Predictors\nAs each LLM used in this study adds a particular back-\nground and been used for variety of classification task\nacross domains, we evaluated each selected LLM as a\nstandalone reasoning baseline.\nFor this setup, the clini-\ncal summary S and knowledge bank K were provided to\nthe LLM alongside a task-specific prompt instructing the\nmodel to estimate the probability of postoperative com-\nplications. The predicted probabilities were directly used\nto compute all evaluation metrics.\nThis setup quantifies\nhow much predictive signal is captured by the LLM\u2019s rea-\nsoning over structured clinical data using a comprehensive\nKnowledge Bank.\nGeneration parameters were fixed to\nmax new tokens=500 and temperature=0.9. All\ninference was performed using HuggingFace checkpoints\non two 95GB NVIDIA H100 GPUs.\n4.4.3. Human Baseline\nThe objective of our study is to enhance the effectiveness\nof lung cancer surgery alongside providing a point of inter-\nvention and explanation for surgeons. Thus, to benchmark\nMIRACLE against expert clinical judgment, we evaluated\nthe performance of practicing thoracic surgeons in predict-\ning postoperative complications using the held-out test split.\nSurgeons from the collaborating cancer research center par-\nticipated in the study, with clinical experience ranging from\n2 to 33 years (mean: 17.75 years). Provided only with pre-\noperative clinical factors and blinded to ground-truth out-\ncomes, surgeons classified each patient as likely or unlikely\nto develop postoperative complications and submitted brief\ntextual justifications for their decisions, which is some-\nthing they routinely do in their practice using their exper-\ntise and experience. From these annotations, we computed\nsurgeon-level False Positive Rate (FPR) and True Positive\nRate (TPR) metrics for direct comparison with our models.\nSurgeons exhibited an average FPR of approximately 20%,\nvalidating our selection of TPR(%)@FPR=0.2 as a clini-\ncally meaningful operating point. Additionally, surgeon-\nauthored remarks were used as qualitative references for\nevaluating the explanatory quality of the three LLM based\nremark generation methods, detailed in Section 5.2.\nAll baselines were evaluated on the held-out test split using\nAUC and TPR@FPR=0.2/0.3 to ensure direct comparabil-\nity with our full model. Code for baseline is also released.\n5\nFigure 2. (a) Receiver Operating Curves for MIRACLE alongside all the baseline models. (b) Distribution of alignment between LLM\ngenerated remarks and surgeon-provided remarks. (c) Confusion Matrix for the surgeons\u2019 performance on POC-L test split. (Best Viewed\nDigitally)\n4.5. Implementation Details\n4.5.1. Clinical and Radiological Encoder\nThe clinical encoder is a four-layer Bayesian MLP with\noutput dimensions [64, 128, 256, 768].\nEach layer em-\nploys variational Bayesian linear transformations with KL-\ndivergence regularization (\u03bb = 10\u22126), Monte Carlo sam-\npling using S = 10 samples per forward pass and 30%\ndropout for regularization. The radiological encoder fol-\nlows a similar design, implemented as a two-layer Bayesian\nMLP with output dimensions [256, 768], using the same\nKL regularization weight, sampling strategy (S = 10) and\ndropout rate (30%).\n4.5.2. Remark Encoder\nFor textual encoding, we use Clinical Longformer [18], a\nLongformer-based model pretrained on the MIMIC-III clin-\nical notes corpus [14]. This model supports sequences of up\nto 4,096 tokens, enabling full-length encoding of the gener-\nated remarks without truncation. The pooled output layer\n(dimension 768) is passed through a frozen linear projec-\ntion of identical dimensionality to preserve pretrained clin-\nical representations.\n4.5.3. Feature Fusion\nLet Ec, Er and Em \u2208R768 denote embeddings from\nthe clinical, radiological and remark encoders, respectively.\nThese embeddings are fused via a weighted combination:\nE = 0.5Ec + 0.25Er + 0.25Em,\nwhere fusion weights were tuned through extensive experi-\nmentation on the validation set.\n4.5.4. Classifier\nThe fused embedding E is input to a three-layer Bayesian\nMLP classifier with hidden dimensions [256, 1024] and a\nsingle-unit output layer. The same KL regularization (\u03bb =\n10\u22126) and Monte Carlo sampling strategy (S = 10) are\napplied throughout.\n4.5.5. Training\nThe full network is trained using focal loss [19] with balanc-\ning parameter \u03b1t = 0.8 and focusing parameter \u03b3 = 4 to\nmitigate class imbalance. Training is performed on 2,694\nsamples with model selection based on validation AUC\ncomputed over a held-out set of 200 patients. Final evalua-\ntion is conducted on an independent test cohort of 200 pa-\ntients. All experiments were executed on a single NVIDIA\nA6000 GPU, requiring approximately 2GB of VRAM per\ntraining run.\n5. Results\n5.1. Quantitative Performance\nTable 1 summarizes the comparative performance of MIR-\nACLE (with all three LLM variants) against traditional\nmachine learning baselines, standalone LLM predictors\nand the human expert baseline on the held-out test co-\nhort. When integrated with OpenBioLLM-70B [4], MIR-\nACLE achieves the highest overall discrimination with\nan AUC of 81.04%, closely followed by the DeepSeek\nR1\u2013Distill Qwen-32B and Llama 3.3 70B\u2013Instruct vari-\nants at 80.94% and 80.84%, respectively. Corresponding\nROC curves are shown in Figure 2(a). At fixed operating\n6\nTable 1. Performance of different models on our testing dataset\nModel\nAUC\n(%)\nTPR(%)\n@ FPR\n= 0.2\nTPR(%)\n@ FPR\n= 0.3\nLlama 3.3 70B-Instruct\u2020\n69.68\n41.12\n74.77\nDeepSeek R1-Distill Qwen-32B\u2020\n64.49\n54.21\n56.07\nOpenBioLLM-70B\u2020 [4]\n71.01\n52.34\n60.75\nMultivariate logistic regression\n80.89\n73.83\n80.37\nRandom Forest Classifier\n77.00\n62.62\n74.76\nXGBoost\n75.17\n53.27\n64.48\nGradient Boosting Classifier\n78.53\n65.42\n67.29\nLightGBM\n74.77\n46.73\n69.16\nSurgeons\n\u2013\n44.86\n\u2013\nMIRACLE (DeepSeek R1 distill)\n80.94\n73.83\n81.31\nMIRACLE (Llama 3.3 70B-Instruct)\n80.84\n71.03\n81.31\nMIRACLE (OpenBioLLM-70B)\n81.04\n71.96\n81.31\n\u2020 Models using only clinical dataset.\nTable 2. Ablation Study\nClinical Radiology\nLLM\nRemarks\nAUC\n(%)\nTPR(%)\n@ FPR\n= 0.2\nTPR(%)\n@ FPR\n= 0.3\n\u2713\n\u2013\n\u2013\n74.81\n57.94\n66.35\n\u2713\n\u2713\n\u2013\n78.64\n64.48\n76.64\n\u2713\n\u2713\n\u2713\n80.94\n73.83\n81.31\npoints, all MIRACLE variants attain the highest sensitiv-\nity at FPR=0.3 with a TPR of 81.31%, outperforming all\ntraditional baselines. At the stricter threshold of FPR=0.2,\nMIRACLE with the DeepSeek R1\u2013Distill variant matches\nthe strongest classical baseline (multivariate logistic regres-\nsion) with a TPR of 73.83%, while the OpenBioLLM-\n70B and Llama 3.3 variants remain highly competitive at\n71.96% and 71.03%, respectively. Although multivariate\nlogistic regression demonstrates comparable sensitivity at\nthe lowest false positive rate, it lacks any inherent inter-\npretability or intervention capability. In contrast, MIRA-\nCLE combines state of the art predictive performance with\nfeature grounded, clinician editable explanations, providing\nactionable insights alongside probability estimates. Perfor-\nmance differences among MIRACLE variants are marginal,\nindicating stable multimodal fusion irrespective of the un-\nderlying LLM. OpenBioLLM-70B yields slightly superior\noverall discrimination (highest AUC), whereas DeepSeek\nR1\u2013Distill exhibits the strongest sensitivity at strict oper-\nating points, supporting greater early risk detection which\nis an important consideration for surgical planning. Stan-\ndalone LLM predictors show noticeably lower performance\nbut their generated summaries provide complementary se-\nmantic context that strengthens multimodal fusion within\nMIRACLE. Notably, relative performance trends observed\nin standalone LLM testing remain consistent once inte-\ngrated into the full system. Human experts, with an aver-\nage experience of 17.75 years, achieved a TPR of 44.86%\nat FPR=0.2, exceeding the standalone Llama baseline but\nfalling substantially below MIRACLE\u2019s performance. This\ngap highlights the added clinical value of our intervention-\nenabled, multimodal risk stratification framework. Surgeon\nconfusion matrices are shown in Figure 2(c).\n5.2. Qualitative Analysis of the generated remarks\nTo assess the quality of LLM-generated clinical explana-\ntions, we evaluated all selected LLMs using a two stage\nqualitative framework. Despite identical inputs, each model\nproduced distinct remarks, motivating a systematic compar-\nison against surgeon-authored explanations.\n(i) Automated Adjudication. Following the LLM as a\njudge protocol [35], we used DeepSeek R1-Distill Qwen-\n7B [9] to evaluate semantic alignment between each LLM\u2019s\nremarks and corresponding surgeon authored explanations.\nFor each pair, alignment was categorized as completely\nunaligned, somewhat aligned or completely aligned.\nAs\nshown in Figure 2(b), nearly all outputs were judged aligned\nto that of surgeons\u2019, indicating strong grounding and limited\nrequirement for intervention. Frequency analysis showed\nthat the reasoning based DeepSeek R1-Distill Qwen-32B\nachieved the highest complete alignment rate (43.5%), fol-\nlowed by the instruction-based Llama 3.3 70B-Instruct\n(38%).\nOpenBioLLM-70B [4] yielded the lowest fully\naligned rate (37%) but demonstrated the highest proportion\nof partially aligned responses, suggesting broader hypothe-\nsis exploration.\n(ii) Expert Manual Review. A panel of thoracic sur-\ngeons conducted manual assessments of paired surgeon and\nLLM explanations in the test split. Each LLM remark was\ncategorized as: (i) Performs better when it is clearer or\nmore clinically informative than surgeons\u2019 remarks, (ii) Per-\nforms comparably when it captures similar insights and key\nrisk factors, or (iii) Performs worse when it omitted crit-\nical risk factors, made implausible inferences, is too ver-\nbose or overestimates the risks. While many LLM expla-\nnations met or exceeded surgeon clarity, experts confirmed\ninstances where surgeon insights remained superior. These\nfindings validate our design choice to incorporate a clinician\nintervention loop, enabling surgeons to revise generated ex-\nplanations and immediately update predicted risk scores.\nRepresentative examples from the Performs better and Per-\nforms worse categories are provided in a figure in supple-\nmentary material, with additional examples across all three\ncategories also provided. This hybrid qualitative evaluation\nframework enables reproducible comparison of explanation\nquality across all LLM variants.\n5.3. Ablation Study\nWe conducted ablation study using the strongest MIRA-\nCLE configuration, DeepSeek R1\u2013Distill Qwen-32B vari-\nant, evaluated on the held-out test split of 200 patients as\nit outperformed all the baselines across all metrics (Ta-\nble 2).\nEach ablation isolates the contribution of indi-\nvidual architectural components.\nUsing only the clini-\ncal Bayesian MLP and classifier yields an AUC of 74.81\n7\nwith reduced TPR at both operating thresholds, surpass-\ning standalone LLM baselines but underperforming con-\nventional ML models (Table 1).\nAdding the radiologi-\ncal encoder increases AUC by approximately 4%, demon-\nstrating the complementary value of radiomic biomark-\ners.\nThis dual encoder configuration exceeds nearly all\nbaselines, improving TPR by roughly 6.5% at FPR=0.2\nand 10% at FPR=0.3. Incorporating the LLM remark en-\ncoder provides a further performance gain, increasing AUC\nby 2.3%, raising TPR@FPR=0.2 by 9.35% and improv-\ning TPR@FPR=0.3 by approximately 5%. While remark\nembeddings contribute modestly to overall discrimination,\nthey substantially enhance sensitivity under strict clinical\noperating points and introduce interpretable explanations\nthat enable surgeon intervention, supporting earlier identifi-\ncation of high-risk surgical candidates.\n6. Discussion\nMIRACLE addresses a central clinical challenge in thoracic\noncology which is accurate preoperative risk stratification\nfor postoperative pulmonary complications. Current stan-\ndards of care rely primarily on population level pulmonary\nfunction thresholds such as FEV1 and DLCO, along with\nsubjective clinician judgment, which fail to capture patient\nspecific heterogeneity in lung structure and comorbidity\ninteractions. Our results show that incorporating CT de-\nrived radiomic biomarkers significantly improves discrimi-\nnation and sensitivity, demonstrating that imaging encodes\nlatent markers of pulmonary risk not reflected in conven-\ntional metrics. By integrating clinical variables, radiomics\nand language based reasoning, MIRACLE aligns computa-\ntional prediction with how surgeons assess risk in practice,\nthrough multimodal interpretation of physiology and treat-\nment context.\nApart from predictive performance, MIRACLE\u2019s pri-\nmary advancement lies in transforming risk assessment\nfrom a static probability output into an interactive decision\nsupport workflow. LLM generated explanations expose the\nreasoning behind predictions and enable clinicians to di-\nrectly revise textual remarks, which are then again used\nto update risk estimates.\nExperts\u2019 evaluation confirmed\nvalue of this human in the loop paradigm, particularly in\ncases where surgeon insight exceeded automated explana-\ntions. In contrast to previous explainability approaches that\nmerely visualize feature importance, MIRACLE enables di-\nrect clinician intervention within the predictive process, pre-\nserving accountability while leveraging AI support."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper\n\n**Title:** LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery\n\n**Authors:** Shubham Pandey\u2020, Bhavin Jawade\u2020, Srirangaraj Setlur\u2020, Venu Govindaraju\u2020, Kenneth Seastedt\u2020\u22c6  \n\u2020University at Buffalo, \u22c6Roswell Park Comprehensive Cancer Center, Buffalo, NY, USA  \n{spandey8, bhavinja, setlur, govind}@buffalo.edu, Kenneth.Seastedt@RoswellPark.org\n\n**Abstract:** Postoperative complications remain a critical concern in clinical practice. This paper introduces MIRA-CLE (Multi-modal Integrated Radiomics And Clinical Language-based Explanation), a deep learning architecture for predicting the risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. The key contributions include: unified multimodal risk modeling, a Bayesian MLP architecture robust under small, imbalanced datasets with calibrated uncertainty estimation to mitigate overfitting, and real-world evaluation on POC-L (3094 lung cancer patients treated between 2009-2023 at Roswell Park Comprehensive Cancer Center). The results demonstrate that MIRA-CLE outperforms various traditional machine learning models and contemporary large language model variants for personalized and explainable postoperative risk management. \n\n**Introduction:** Lung cancer is the leading cause of cancer-related mortality, with surgical resection providing a 2.5-fold survival advantage over non-surgical management. However, surgery carries substantial perioperative risks. Current risk models rely on population-level indicators like FEV1 and DLCO, which fail to capture nuanced patient-specific interactions between lung tissue heterogeneity, comorbidities, and tumor biology. Radiomics offers a quantitative pathway for capturing lung heterogeneity through high-dimensional CT features but lacks clinical interpretability and intervention mechanisms.\n\n**Related Work:** Existing approaches treat text generation and clinical outcome prediction as independent processes. Large language models (LLMs) enable clinically grounded explanations but remain disconnected from risk prediction workflows. No prior framework unifies structured clinical data, radiological biomarkers, and explainable LLM outputs into a single predictive pipeline.\n\n**Methods:** MIRA-CLE integrates preoperative clinical features with CT-derived radiomic biomarkers and LLM-generated, evidence-grounded natural language explanations for accurate, interpretable postoperative risk assessment. The architecture consists of three main modules: two separate Bayesian MLP networks for clinical and radiological features, an encoding module using a frozen encoder on medical data for textual remarks, and a fusion network for final prediction.\n\n**Experiments:** MIRA-CLE was evaluated on the POC-L dataset (3094 lung cancer patients treated between 2009-2023 at Roswell Park Comprehensive Cancer Center). The study compared MIRA-CLE against conventional machine learning classifiers, standalone LLM predictors, and human expert baselines. Results show that MIRA-CLE outperforms traditional models across AUC and True Positive Rate (Sensitivity) at clinically relevant False Positive Rates (1-Specificity).\n\n**Conclusion:** MIRA-CLE successfully integrates preoperative clinical and radiological features with clinician-intervenable, natural-language explanations, transforming risk assessment from a static black-box output into an interactive, transparent clinical decision-support system. The framework's robustness under small, imbalanced datasets and its ability to provide actionable insights make it particularly valuable for personalized postoperative complication management in lung cancer surgery.\n\n**Key Findings:**  \n- MIRA-CLE achieves superior performance across AUC and True Positive Rate (Sensitivity) at clinically relevant False Positive Rates.\n- The addition of radiological biomarkers significantly improves discrimination and sensitivity, highlighting the importance of imaging data integration.\n- LLM-generated explanations expose the reasoning behind predictions, enabling clinicians to directly revise textual remarks for more accurate risk assessments.\n\n**Contributions:**  \n1. Unified Multimodal Risk Modeling: Joint fusion of clinical data, radiomics, and language-based explanations for accurate, interpretable postoperative risk assessment.\n2. Bayesian MLP Architecture: Robust learning under small, imbalanced datasets with calibrated uncertainty estimation to mitigate overfitting.\n3. Real-World Evaluation: Training and validation on POC-L dataset consisting of 3094 lung cancer surgical patients treated between 2009\u20132023 at Roswell Park Comprehensive Cancer Center.\n\n**Future Work:**  \n- Further refinement of the LLM-generated explanations to enhance clinical interpretability.\n- Exploration of multi-institutional datasets for broader validation and robustness analysis."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "OCCAM: Class-Agnostic, Training-Free,\nPrior-Free and Multi-Class Object Counting\nMichail Spanakis1,2 , Iason Oikonomidis2 , and Antonis Argyros1,2\n1 Computer Science Department, University of Crete, Heraklion, Greece\n2 Institute of Computer Science (ICS), Foundation for Research & Technology \u2013\nHellas (FORTH), Heraklion, Greece\n{mikespan,oikonom,argyros}@ics.forth.gr\nAbstract. Class-Agnostic object Counting (CAC) involves counting in-\nstances of objects from arbitrary classes within an image. Due to its prac-\ntical importance, CAC has received increasing attention in recent years.\nMost existing methods assume a single object class per image, rely on\nextensive training of large deep learning models and address the prob-\nlem by incorporating additional information, such as visual exemplars or\ntext prompts. In this paper, we present OCCAM, the first training-free\napproach to CAC that operates without the need of any supplementary\ninformation. Moreover, our approach addresses the multi-class variant of\nthe problem, as it is capable of counting the object instances in each and\nevery class among arbitrary object classes within an image. We leverage\nSegment Anything Model 2 (SAM2), a foundation model, and a custom\nthreshold-based variant of the First Integer Neighbor Clustering Hierar-\nchy (FINCH) algorithm to achieve competitive performance on widely\nused benchmark datasets, FSC-147 and CARPK. We propose a synthetic\nmulti-class dataset and F1 score as a more suitable evaluation metric.\nThe code for our method and the proposed synthetic dataset will be made\npublicly available at https://mikespanak.github.io/OCCAM_counter.\nKeywords: Object Counting \u00b7 Class-Agnostic \u00b7 Multi-Class \u00b7 Training-\nFree \u00b7 Exemplar-Free \u00b7 Segment Anything Model (SAM) \u00b7 FINCH.\n1\nIntroduction\nObject counting is a natural and intuitive ability. Counting fundamentals emerge\nin humans from a very young age - often even before the development of\nspeech [39]. But what happens when the number of objects becomes large?\nCounting becomes tedious and repetitive. Despite its apparent simplicity, count-\ning remains a cognitively demanding task. It follows object recognition, local-\nization, and differentiation, making it especially challenging for computers.\nDespite these challenges, numerous approaches have been developed for\ncounting objects in images. Traditional computer vision techniques were used\nin the initial methods, e.g. [16]. In recent years, class-specific object counting\nmethods have achieved significant progress utilizing prior knowledge regarding\narXiv:2601.13871v1  [cs.CV]  20 Jan 2026\n2\nM. Spanakis et al.\nFig. 1. We propose a class-agnostic multi-class object counting approach that takes as\ninput an RGB image and outputs a count of object instances per detected class.\nthe target object class due to advances in neural networks and deep learn-\ning. Specialized models have been developed for counting humans in crowded\nscenes [5], vehicles [10], animals [3], and other categories of objects in various\ndomains [29,43,45]. These counting models can reliably count a large number of\nobjects belonging to a single predefined class. As long as sufficiently large anno-\ntated training datasets for each object class are available, there is no inherent\nrestriction on the target object class. Collecting and annotating these datasets\nis often prohibitively expensive. Furthermore, most existing approaches require\ntraining a separate model for each category of objects, which limits their scala-\nbility and flexibility. These limitations have led to the emergence of a more gen-\neralized and demanding research direction: class-agnostic object counting [20].\nTo address the aforementioned limitations, approaches have been developed\nto count objects from arbitrary object classes. These are typically trained on im-\nages including several objects in various object categories to learn the underlying\nnotion of similarity between object instances. However, most existing methods\nassume the presence of only a single object class per image. As a result, they rely\non additional input, such as visual exemplars or text prompts, to specify which\nobjects should be counted. Some studies have introduced interactive counting\nframeworks [2,12] that incorporate human feedback into the loop to refine the\npredictions. Other approaches [30] count only the object class with the high-\nest number of instances. Although these methods represent important progress\nin the field, they still face challenges in terms of generalization and practical\napplicability to diverse real-world scenarios.\nRecent approaches omit the dependence on extensive training by leveraging\npre-trained, general-purpose vision models, such as SAM [15] and DINOv2 [24].\nIn addition, some methods [30,8] operate without requiring any priors. A few\nmethods revoke the assumption of a single object class per image [27,14,41] or\naim to count instances from multiple classes [9,22]. Our method, Object Count-\ning: Class-agnostic, Assumption-minimal, Multi-class (OCCAM), advances this\nresearch direction by imposing, to the best of our knowledge, the fewest assump-\ntions to date. Specifically, our method is: (1) Training-free: it does not depend\nTraining-Free, Prior-Free and Multi-Class CAC\n3\non some kind of training procedure, allowing immediate deployment out of the\nbox; (2) Prior-free: only requires a plain RGB image, since additional informa-\ntion implicitly defines the target object category a priori; (3) Multi-class: it is\ncapable of counting multiple object categories within an image in a single pass.\nTo achieve the above, OCCAM leverages SAM2 [32] and FINCH [33].\nExtensive experimental evaluation demonstrates the effectiveness and gen-\neralizability of our approach in several datasets. To ease the evaluation of the\nmullticlass aspects of OCCAM, we introduce a new synthetic dataset. More-\nover, we discuss the evaluation metrics adopted in the relevant literature up\nto now and we argue that the F1-score is more suitable for evaluating CAC.\nOCCAM achieves competitive and, in some cases, state-of-the-art performance\nacross standard metrics. There is currently no other CAC method being both\nPrior-free and Training-free. All these properties combined, make OCCAM truly\nclass-agnostic, ideal for fully automated object counting systems and position it\nas the most general solution available.\nIn summary, the key contributions of our work are:\n\u2013 We introduce the first Class-Agnostic object Counting (CAC) method that\nis entirely training-free, prior-free, and capable of counting object instances\nin multiple classes within an image.\n\u2013 We introduce a synthetic multi-class dataset for object counting, constructed\nby stitching together an arbitrary number of real-world single-class images.\n\u2013 We propose F1 score as a more suitable and unbiased evaluation metric for\nCAC. This enhances the validity of the quantitative evaluation.\n2\nRelated Work\nClass-Agnostic object Counting (CAC) generalizes object counting across arbi-\ntrary categories, reducing reliance on class-specific training. Existing CAC meth-\nods can be grouped by (i) whether they depend on priors (e.g. exemplars, points\nor text prompts), (ii) whether they require dedicated training, and (iii) whether\nthey assume a single dominant class per image or support multi-class counting.\nPrior-Based CAC. Most CAC methods use priors to specify what to count\nduring training and inference. Exemplar-based methods leverage the strongest\nguidance via 1\u20133 cropped instances from the given image. Counting is typically\ncast as matching or similarity learning, from the first self-similarity formulation\nin [20] proposing GMN, to few-shot regression-based FamNet [31], and BM-\nNet/BMNet+ [34] which jointly learn feature representation and a similarity\nmetric. Nguyen et al. [23] proposed Counting-DETR, an uncertainty-aware de-\ntector trained via pseudo labels, while SAFECount [42] refines separation in\ncrowded scenes through similarity-aware comparison and feature fusion. Text-\nprompted CAC improves generality by weakening the prior, enabling count-\ning from class labels [40]. Recently, multi-modal methods have been developed.\nCounTR [18] is a transformer-based model that leverages attention to com-\npare image patches with exemplars and LOCA [6] uses an iterative Prototype\nExtraction module to fuse exemplar shape and appearance with image features.\n4\nM. Spanakis et al.\nPseCo [13] combines SAM [15] and CLIP [28] in a point\u2013segment\u2013count pipeline,\nwhile MAFEA [14] mitigates target confusion via cross-attention. In DAVE [27]\ndensity-based candidate generation is combined with a detection-based verifica-\ntion stage to improve localization and outlier removal. A single-stage counter\nGeCo [26] unifies detection, segmentation, and counting. COUNTGD [1] utilizes\nGroundingDINO [19] fusing text and visual prompts via attention.\nPrior-Free CAC. A smaller body of work [30,8,38,44] avoids external priors\nand instead exploits self-similarity and internal feature relationships. RepRPN-\nCounter [30], the first exemplar-free CAC method, discovers repeating objects\nvia an RPN and estimates counts through density prediction. RCC [8] frames\ncounting as repetition recognition under weak supervision. GCA-SUN [38] di-\nrectly predicts density maps using gated context-aware modulation, feature se-\nlection, and multi-scale fusion, while SAVE [44] uses detection-based features,\nself-attention over visual embeddings, and count regression. While prior-free ap-\nproaches are suitable for automation, the ambiguity of what to count becomes\nmore prominent, making the single-class-per-image assumption essential. Fur-\nthermore, some multimodal methods [18,6,27,26,14] can operate without addi-\ntional information, typically trading off accurate object counting for generality.\nTraining-Free CAC. Most CAC methods still require extensive task-specific\ntraining of advanced deep learning models and large-scale datasets making them\nresource-intensive. Training-free methods [21,35,37,17,25,11] aim to overcome\nthese limitations by leveraging foundation models. Ma et al. [21] were the\nfirst to use SAM [15] exploiting exemplar-guided mask generation and cosine-\nsimilarity-based feature averaging for counting. TFOC [35] also uses SAM, for-\nmulating counting as prior-guided segmentation integrating similarity, segment,\nand semantic priors. TFCounter [37] utilizes a dual-point prompt strategy and\na context-aware similarity module, while in [17] instance-level counting with\nimproved point prompting and semantically enriched features is performed by\nSAM. CountingDINO [25] relies on fully unsupervised DINOv2 [24] features to\nextract latent object prototypes and generate similarity-based density maps.\nCount validity is improved by combining SAM-based spatial proposals with\nEVA-CLIP\u2013based [36] semantic filtering in [11] ensuring that only spatially and\nsemantically relevant object instances are counted.\nMulti-Class CAC. Real-world images typically contain multiple object cate-\ngories. A few methods [27,14,41] have been developed to utilize priors accordingly\nto count object instances of the target class despite the presence of multiple cate-\ngories. Xu et al. [41] combine exemplar-based segmentation with pseudo-labeled\nmasks to mitigate multi-class over-counting and highlight the trade-off between\nhandling multiple classes and single-class counting accuracy.\nOnly recent works are capable of counting each object instance belonging in\neach and every class within an image. ABC123 [9], the first multi-class CAC\nmethod, is a transformer-based density regression approach trained on the syn-\nthetic MCAC dataset to simultaneously count multiple object types without\npriors. Each object class can have up to 300 object instances and at most 4 dif-\nferent categories can be counted per image. OCCAM overcomes these limitations\nTraining-Free, Prior-Free and Multi-Class CAC\n5\nInput: RGB Image\nSAM2\nBinary Masks\nDiscard\nMasks\nFinal Masks\nProject\nMasks\nColored Masks\nBounding Boxes\nResNet50\nVectors\nBoxes\nCustom\nFINCH\n15\nCounts\nOutput\n76\n19\n6\nFig. 2. Given an RGB image as input, a dense grid of seed points is constructed, which\nare passed to SAM2 to produce a collection of binary masks. These masks are then\nfiltered to obtain a refined set of candidate object masks. Each candidate object mask\nis used to extract an aspect-ratio-preserving bounding box region, which is passed\nthrough ResNet50 to produce a feature vector. These vectors are then grouped using a\ncustom threshold-based variant of FINCH algorithm, resulting in a final set of clusters.\ndespite being prior-free; it can count objects belonging to an arbitrary number\nof different classes without an upper bound on their instances. OmniCount [22]\nis training-free and detection-based, using SAM for instance segmentation re-\nlying on semantic and geometric priors to count multiple categories in a single\npass. However, OCCAM does not rely on provided priors. Overall, OCCAM is\nthe most general CAC method to date and there is no direct competitor. Every\ncomparison should be made taking into consideration the above differences.\n3\nMethodology\nWe aim to address training-free, multi-class, class-agnostic object counting with-\nout using priors. In order to achieve this, our method, Object Counting: Class-\nagnostic, Assumption-minimal, Multi-class (OCCAM), is based on two key com-\nponents: the Segment Anything Model 2 (SAM2) [32] and a threshold-based vari-\nant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm [33]\nthat we propose. Figure 2 illustrates the proposed approach.\nObject\nMask\nGeneration.\nWe use the Segment Anything Model 2\n(SAM2) [32] to obtain masks per object instance in the image without prior\nknowledge of object categories, number, or size. Specifically, since the RGB im-\nage I \u2208RH\u00d7W \u00d73 is our sole form of input, we densely sample a custom canonical\ngrid of possible seed points, pi, over I and use them as point-prompts for SAM2.\nEach pi may correspond to a distinct object of interest. The spacing between\nthe seed points is chosen empirically and remains constant across all images,\nresulting in a regular grid whose size depends on the dimensions of I. For each\npi, SAM2 produces up to three binary masks Mi without requiring any form of\nadditional information.\nObject Mask Postprocessing. Further processing of the raw binary masks\nMi is necessary, since there is no guarantee that each mask produced by SAM2\ncorresponds to a distinct and unique object instance. There are two main issues:\n(1) different seed points may produce the same, or a slightly different, binary\nmask, and (2) one to three masks could be produced by SAM2 from the same\n6\nM. Spanakis et al.\nseed point. Therefore, each pixel may not be uniquely assigned to a single mask,\nwhich can adversely affect the final object count.\nIn order to resolve these issues, we proceed to mask filtering. Regarding\nthe first issue, duplicate masks are discarded. Specifically, masks are considered\n\u201cduplicates\u201d if their Intersection over Union (IoU) value exceeds an empirically\ndetermined threshold, in which case only one mask is retained arbitrarily. En-\nforcing SAM2 to produce a single mask per seed point would not resolve the\nsecond issue, as the target class is not specified a priori due to the lack of ad-\nditional information. We rely on our proposed FINCH variant to overcome this\nproblem. However, we try to mitigate it by applying more filtering rules. If a\nmask consists of multiple connected components, only the major one is kept,\nunless it constitutes a duplicate with a different mask produced independently.\nThis is important because the produced masks often contain a couple of distinct\nobjects, each of which could be a separate target object. As a result, only a single\nobject instance is contained in each mask, any occlusive object is excluded and\nthe size of the segmented object can be estimated more accurately.\nThe estimated size of each object instance is utilized in the following filtering\nrules. Final binary masks that are too big, relatively to the image size, are\ndiscarded, since they may correspond to the background or be a super-set of other\nmasks. Additionally, masks with a single-pixel width or height are discarded,\nas they are likely artifacts. Overall, these filtering rules eliminate some binary\nmasks, Mi, resulting in the final set of candidate object masks, \u02c6\nMj.\nMultiscale Refinement Strategy. In case the final number of candidate ob-\nject masks found within the image is too small, a simple scaling scheme, similar\nto a test-time normalization technique used in [18], is deployed. The given image\nis split into a non-overlapping canonical 3 \u00d7 3 grid and the entire methodology\nis recursively applied to each of the produced sub-images, independently. Fil-\ntered binary masks produced from each of the nine sub-images are accumulated\nto form the final set of candidate object masks \u02c6\nMj. This strategy enables the\nmodel to detect smaller objects that may be overlooked in the original image. If\nno additional objects are detected using this scaling scheme, we conclude that\nthis image contains very few objects. Through this mechanism, our method can\ndifferentiate between uniform images, or empty scenes, and images containing\nnumerous tiny objects.\nFeature Extraction. The next step is to cluster the final set, \u02c6\nMj, of candidate\nobject masks. First, each retained mask, \u02c6\nMj, is applied to the original image to\nextract the corresponding colored region, and tight bounding boxes are formed.\nIn order to avoid any size-related bias, each bounding box is resized to a prede-\ntermined size, while preserving the original aspect ratio of the depicted object.\nAny residual space of each bounding box is zero-padded. Consequently, each\nof these aspect-ratio-preserving bounding boxes is color-normalized and passed\nthrough a deep model for a standardized representation to be extracted. To\nachieve this, the standard ResNet-50 [7] pre-trained on ImageNet [4] excluding\nits last fully connected layer is utilized. Therefore, this procedure results in a\n2048-dimensional feature vector fj, fj \u2208R2048, produced for each bounding box.\nTraining-Free, Prior-Free and Multi-Class CAC\n7\nThreshold-based FINCH Variant. A custom threshold-based variant of the\nFirst Integer Neighbor Clustering Hierarchy (FINCH) algorithm [33] is proposed\nto properly group the extracted feature vectors, fj, into an arbitrary number of\nclusters. Each of these clusters should ideally correspond to a distinct object\ncategory in the image.\nThe key idea of FINCH is to iteratively group vectors which are the closest\nneighbors to each other, based on a chosen distance metric. The closest-neighbor\nrelation though can be asymmetrical; a vector vi may be the closest neighbor of\nanother vector vj, but this does not necessarily imply that vj is also the closest\nneighbor of vi. This may occur as vi may have a closer neighbor in a different\ndirection. FINCH bypasses this issue by enforcing symmetry, effectively allowing\nmultiple closest neighbors. Hence, in our example, vi is also marked as the clos-\nest neighbor of vj, resulting in a situation where vj has two \u201cclosest neighbors\u201d.\nFurthermore, the absolute value of the distance is not taken into consideration,\nallowing the algorithm to remain completely parameter-free. Finally, the min-\nimum possible size of a cluster is two, which could cause various issues in our\ncase, such as the adverse grouping of the feature vector corresponding to the\nbackground of the image.\nIn our approach, the following modifications are applied to the FINCH algo-\nrithm. An empirical threshold is introduced in each step of the FINCH algorithm.\nThus, two vectors will not be grouped together if the distance between them is\ngreater than this threshold, despite their partial closest-neighbor relation. In all\nof our experiments, the Euclidean distance is used in the 2048-dimensional fea-\nture space. For the first iterations, the corresponding threshold decreases in each\nstep, while it remains constant for the later iterations, balancing intra-class vari-\nance and inter-class discrimination. This is a simple but effective way to prevent\nthe enforcement of symmetry and resolve the aforementioned issues.\nInitially, each vector forms a cluster by itself. After the first iteration, the\ncentroids of the formed clusters are estimated, and the distances among clusters\nare computed based on these centroids. The clusters which are, at least, partial\nclosest neighbors and their centroids are at a distance less than the defined\nthreshold are merged. The iterative procedure is terminated when there are\nno more clusters left to be merged. Compared to the original FINCH method,\nthis constitutes a different termination condition, as FINCH stops producing\nhierarchical clustering partitions when the next partition would include every\ngiven element. Through this approach, we eliminate the need to devise a non-\ntrivial automatic procedure for selecting the optimal clustering level from the\nhierarchical partitions generated by the default FINCH algorithm.\nThe output of our proposed threshold-based FINCH variant is the resulting\nclusters, Ck, each of which corresponds to a number of candidate object masks\npresent in the input image. As far as the evaluation procedure is concerned, each\nCk is matched to the ground-truth one according to which it shares the largest\nnumber of object instances with.\n8\nM. Spanakis et al.\nTable 1. Multi-Class Methods on FSC-147 Test Set (best results in bold). Results\nreported below the double line include images with \u2264300 objects.\nModel\nInfo\nMAE \u2193RMSE \u2193NAE \u2193SRE \u2193\nMean\n-\n47.55\n147.67\n-\n-\nMedian\n-\n47.73\n152.46\n-\n-\nOmniCount [22]\nExemplars 18.63\n112.98\n0.14\n2.99\nOmniCount [22]\nPoints\n19.24\n115.27\n0.25\n3.21\nOmniCount [22]\nText\n21.46\n133.28\n0.32\n0.39\nOCCAM-S\n-\n16.92\n110.83\n0.22\n3.38\nABC123 [9]\n-\n11.75\n33.41\n0.11\n2.38\nOCCAM-S (\u2264300)\n-\n11.29\n25.17\n0.21\n2.68\n4\nExperiments\nDatasets. FSC-147 Dataset: The FSC-147 dataset [31] is the benchmark for\nfew-shot, class-agnostic object counting, consisting of 6135 human-annotated\nimages from 147 object classes with non-overlapping object categories in\ntrain/val/test splits. The test set contains 1190 images from 29 diverse classes,\nfrom sea shells to stamps, with per-instance center annotations and three\nexemplar bounding boxes per image. The average count is 56 objects per image.\nSince our method is training-free and prior-free, only the test split is used,\nwithout relying on exemplars or annotations. A key limitation of FSC-147 is the\nimbalanced class distribution, and the single-class-per-image assumption, which\nis not realistic and does not hold for a great number of images. The common\nheuristic that the most frequent class is the annotated one is not always valid.\nThus, FSC-147 is less suitable for evaluating our method.\nSynthetic Multi-class Test Set: We construct a synthetic multi-class test set by\nstitching together randomly selected images from the FSC-147 test split. Three\ndataset variants are generated to assess robustness, each containing 100 images\ncomposed of 1\u201310 sub-images containing a different and distinct object class.\nImages are stitched side by side using normalized height, forming grids of at\nmost two columns, without resizing; unused regions are zero-padded. The class\ncomposition and instance counts are unconstrained, resulting in images with\nhigh object densities. All of the above make the proposed dataset challenging\nand suitable for multi-class counting.\nReal Multi-class Test Set: A recently proposed dataset [41] has been acquired\nby privately contacting the respected authors. It contains 453 real-world images,\neach depicting 1\u20133 object classes from 76 different object categories with\npossible inter-class overlap. For consistency with FSC-147, three exemplars per\nclass and center annotations per instance are provided. Its major advantage is\nthat object instances are annotated across all classes present in each image,\nmaking this dataset well suited for evaluating multi-class CAC methods.\nTraining-Free, Prior-Free and Multi-Class CAC\n9\nTable 2. Training-Free Methods on FSC-147\nTest Set (best results in bold).\nModel\nInfo\nMAE \u2193RMSE \u2193\nCAN SAM? [21]\n3-shot 27.97\n131.24\nCountingDINO [25] 3-shot 20.93\n71.37\nTFOC [35]\n3-shot 19.95\n132.16\nValidCounter [11]\n3-shot 19.33\n133.33\nTFCounter [37]\n3-shot 18.56\n130.59\nA-Simple-But [17]\n3-shot 12.26\n56.33\nCountingDINO [25] 1-shot 37.73\n131.71\nCAN SAM? [21]\n1-shot 33.53\n142.28\nA-Simple-But [17]\n1-shot 15.15\n69.16\nTFOC [35]\nText\n24.79\n137.15\nA-Simple-But [17]\nText\n23.59\n113.60\nOCCAM-S\n-\n16.92\n110.83\nTable 3. Prior-Free Methods on\nFSC-147 Test Set (best results\nin\nbold).\nScores\nfor\nRepRPN-\nCounter [30] are for \u201cTop 5\u201d.\nModel\nMAE \u2193RMSE \u2193\nRepRPN-C [30]\n26.66\n129.11\nRCC [8]\n17.12\n104.53\nLOCA [6]\n16.22\n103.96\nDAVE [27]\n15.14\n103.49\nCounTR [18]\n14.71\n106.87\nGCA-SUN [38]\n14.00\n92.19\nGeCo [26]\n13.30\n108.72\nMAFEA [14]\n13.23\n105.99\nSAVE [44]\n8.92\n80.39\nOCCAM-S\n16.92\n110.83\nCARPK Dataset: CARPK [10] is a counting benchmark consisting of 1448 aerial\ndrone images, with cars as the only annotated class using tight bounding boxes;\n459 images are used for testing. In the CAC field, it is utilized to examine the\nability of the proposed approaches to generalize adequately to domain-specific\ncounting tasks.\nImplementation Details. According to our experiments, two slightly different\nconfigurations of our methodology give the best results depending on whether\nthe given dataset is multi-class or not. Both configurations rely on \u201cSAM 2.1\nLarge\u201d [32]. A seed point is set per 10 pixels and an IoU threshold of 0.1 is\nused. There are two differences: (1) the size of the constructed bounding boxes,\nresized to 224 \u00d7 224 and 500 \u00d7 500 for single-class and multi-class, respectively,\nand (2) the absolute value of the thresholds used in our custom FINCH variant.\nThere is a decreasing threshold scheme in each of the first three iterations, 12.0,\n9.0, 7.75 in single-class cases and 5.0, 4.0, 3.0 in multi-class. Therefore, we end\nup with two configurations, OCCAM-S and OCCAM-M, for single-class and\nmulti-class scenarios, respectively.\nEvaluation Metrics. Our method is evaluated using the most widely adopted\nmetrics in the CAC literature: Mean Absolute Error (MAE) and Root Mean\nSquared Error (RMSE) as originally introduced in [20,31]. However, the signif-\nicance of an absolute counting error depends heavily on the number and ap-\npearance of the present ground-truth objects. To account for this, Normalized\nAbsolute Error (NAE) and Squared Relative Error (SRE), which normalize the\nerror relative to the number of objects in each image, are also reported. Since our\napproach is multi-class, the adjusted versions, introduced in [9], are employed.\nThese metrics can be misleading: a perfect numerical score does not ensure cor-\nrect instance matching, as False Positives and False Negatives may cancel out.\nTo mitigate this, we adopt Precision, Recall and the F1 Score which is unbi-\n10\nM. Spanakis et al.\nTable 4. Precision, Recall and F-1 Score on FSC-147 Test Set (best results in bold).\nAll other methods use exemplars. Below the double line, the approaches are Training-\nfree. Only our approach is Prior-Free. NAE, SRE, Precision, Recall and F-1 scores for\nother methods are taken from ValidCounter [11].\nModel\nMAE \u2193RMSE \u2193NAE \u2193SRE \u2193Precision \u2191Recall \u2191F-1 \u2191\nFamNet+ [31]\n22.08\n99.54\n0.44\n6.45\n0.66\n0.67\n0.57\nBMNet+ [34]\n14.62\n91.83\n0.27\n6.19\n0.66\n0.68\n0.67\nCounTR [18]\n11.95\n91.23\n0.23\n7.44\n0.79\n0.78\n0.78\nLOCA [6]\n10.79\n56.97\n0.26\n7.96\n0.78\n0.78\n0.78\nTFOC [35]\n19.95\n132.16\n0.29\n3.80\n0.79\n0.82\n0.80\nValidCounter [11] 19.33\n133.33\n0.20\n3.43\n0.87\n0.77\n0.82\nOCCAM-S\n16.92\n110.83\n0.22\n3.38\n0.89\n0.82\n0.83\nased. These are well-established in related domains and provide a more reliable\nand interpretable assessment of counting performance. They are also used in the\nfoundational paper [20] and their relevance is further emphasized in [11], a very\nrecent work concurrent with ours.\nQuantitative Results. Despite not having a direct competitor due to its\nparsimonious assumptions, OCCAM is competitive with other multi-class and\ntraining-free approaches, while lacking behind from other prior-free approaches,\neach of which relies on training. Indicative of its success are the high Precision,\nRecall and F-1 scores. It generalizes well on the CARPK dataset and sets a\nnew state-of-the-art MAE for the Real Multi-class Test Set. According to the\nobtained results, OCCAM performs remarkably well and even exhibits state of\nthe art performance in certain cases.\nIn table 1, our approach is compared with the mean and median baselines\nand the few other multi-class methods. Our approach, outperforms the baselines\nby a significant margin and OCCAM-S achieves better MAE and RMSE scores\ncompared to OmniCount [22], our main competitor, without utilizing any pri-\nors. OCCAM-S achieves better MAE and RMSE scores than ABC123 and has\ncompetitive NAE and SRE despite the fact that it is training-free. In addition,\nthe impact of a few images of the FSC-147 Test Set which contain more than\n300 objects becomes evident by their exclusion.\nIn table 2, our approach is compared with other training-free methods using\nthe FSC-147 Test Set. Each of the other methods is single-class and utilizes some\nsort of prior. However, we surpass most methods in MAE.\nIn table 3, our approach is compared with other prior-free methods using the\nFSC-147 Test Set. Given that our approach is the only multi-class and training-\nfree method, our results are relatively good.\nIn table 4, our approach is compared with other methods regarding Preci-\nsion, Recall and F1-Score using the FSC-147 Test Set. All other methods are\nsingle-class and utilize exemplars as prior information. Despite making the least"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting\"\n\n#### Main Contributions:\n1. **First Training-Free Approach**: The paper introduces OCCAM (Object Counting: Class-agnostic, Assumption-minimal, Multi-class), a novel method for class-agnostic object counting that does not require any training or prior information.\n2. **Multi-Class Support**: OCCAM is capable of handling multiple classes within an image in a single pass, overcoming the limitations faced by existing methods which typically assume only one object class per image.\n3. **Synthetic Multi-Class Dataset**: The authors propose a synthetic multi-class dataset to evaluate their method more effectively and introduce F1 score as a suitable evaluation metric for CAC (Class-Agnostic Object Counting).\n\n#### Methodology:\n- **Segment Anything Model 2 (SAM2)**: Utilized for generating object masks without prior knowledge of categories, number, or size.\n- **Threshold-based FINCH Algorithm**: A custom variant of the First Integer Neighbor Clustering Hierarchy algorithm used to cluster feature vectors and group candidate object masks into clusters.\n\n#### Key Findings:\n1. **Competitive Performance**: OCCAM achieves competitive and in some cases state-of-the-art performance on widely-used benchmark datasets, FSC-147 and CARPK.\n2. **Generalizability**: The method generalizes well across different real-world scenarios without relying on training or prior information.\n\n#### Conclusions:\nOCCAM represents a significant advancement in the field of class-agnostic object counting by providing a fully automated solution that does not require extensive training or supplementary data, making it suitable for diverse applications. The proposed synthetic multi-class dataset and F1 score evaluation metric enhance the validity and applicability of CAC evaluations.\n\n#### Additional Context:\nThe paper addresses limitations inherent to existing approaches such as reliance on priors (visual exemplars, text prompts), single-object class assumptions, and extensive training requirements. OCCAM's unique approach makes it a pioneering solution for real-world object counting tasks where prior information is often unavailable or impractical to obtain."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Revisiting Multi-Task Visual Representation Learning\nShangzhe Di1,2\nZhonghua Zhai2\nWeidi Xie1\n1SAI, Shanghai Jiao Tong University\n2ByteDance Seed\ngithub.com/Becomebright/MTV\nAbstract\nCurrent visual representation learning remains bifurcated:\nvision-language models (e.g., CLIP) excel at global se-\nmantic alignment but lack spatial precision, while self-\nsupervised methods (e.g., MAE, DINO) capture intricate lo-\ncal structures yet struggle with high-level semantic context.\nWe argue that these paradigms are fundamentally comple-\nmentary and can be integrated into a principled multi-task\nframework, further enhanced by dense spatial supervision.\nWe introduce MTV, a multi-task visual pretraining frame-\nwork that jointly optimizes a shared backbone across vision-\nlanguage contrastive, self-supervised, and dense spatial ob-\njectives. To mitigate the need for manual annotations, we\nleverage high-capacity \u201cexpert\u201d models\u2014such as Depth\nAnything V2 and OWLv2\u2014to synthesize dense, structured\npseudo-labels at scale. Beyond the framework, we provide\na systematic investigation into the mechanics of multi-task\nvisual learning, analyzing: (i) the marginal gain of each ob-\njective, (ii) task synergies versus interference, and (iii) scal-\ning behavior across varying data and model scales. Our re-\nsults demonstrate that MTV achieves \u201cbest-of-both-worlds\u201d\nperformance, significantly enhancing fine-grained spatial\nreasoning without compromising global semantic under-\nstanding. Our findings suggest that multi-task learning, fu-\neled by high-quality pseudo-supervision, is a scalable path\ntoward more general visual encoders.\n1. Introduction\nRecent advances in foundation models have reshaped visual\nrepresentation learning by scaling pretraining on web-scale\ndata. Currently, two paradigms prevail: Vision\u2013language\n(VL) contrastive learning [19, 33, 49] aligns global image\nand text embeddings via instance-level discrimination, en-\nabling open-vocabulary recognition and zero-shot transfer.\nSelf-supervised learning (SSL), such as masked image mod-\neling [16, 51], contrastive instance discrimination [7, 15],\nand student\u2013teacher matching [5, 32], enforces invariance\nand regularities without manual labels. Despite their suc-\nFigure 1. Multi-task supervision effects on a ViT-L model with\n10M training samples. We integrate VL, SSL, and pseudo-labeled\ngrounding and depth estimation into a unified training framework.\nThis leads to strong and consistent performance gains across di-\nverse vision and vision\u2013language tasks.\ncess, these paradigms remain bifurcated, primarily focusing\non either global semantics or local regularities while leav-\ning fine-grained spatial and geometric reasoning as under-\nconstrained emergent properties. Given the inherent diver-\nsity of visual tasks\u2014ranging from low-level geometry to\nhigh-level reasoning\u2014learning truly universal visual repre-\nsentations that bridge these multifaceted requirements re-\nmains an open and fundamental challenge [10, 26, 38, 39].\nIn contrast, large language models (LLMs) scale effec-\ntively by absorbing multifaceted supervision\u2014such as sum-\nmarization, translation, and question answering\u2014within a\nunified training interface [1, 4, 21]. Admittedly, unlike the\ndiscrete and sequential nature of language, visual tasks are\ninherently heterogeneous. While this diversity makes a sin-\ngle, universal objective more elusive in vision, we argue that\nvisual pretraining can still embrace the LLM philosophy by\nsimultaneously optimizing across a diverse suite of tasks to\ncapture the full spectrum of information. Inspired by this\n1\narXiv:2601.13886v1  [cs.CV]  20 Jan 2026\nversatility, we ask: Can visual pretraining be scaled by inte-\ngrating semantic, spatial, and geometric supervision within\na single, principled framework?\nTo bridge this gap, we introduce MTV, a multi-\ntask visual pretraining framework that jointly optimizes a\nshared backbone across vision-language contrastive, self-\nsupervised, and dense spatial objectives.\nTo overcome\nthe scarcity of human-labeled data, we leverage high-\ncapacity \u201cexpert\u201d models\u2014such as Depth Anything V2 [45]\nfor monocular depth estimation and OWLv2 [29] for\nopen-vocabulary grounding\u2014to synthesize dense, struc-\ntured pseudo-labels at scale. By integrating these expert-\nguided targets, we enrich traditional objectives with explicit\ngeometric and spatial priors.\nIn this work, we go beyond simple multi-tasking to\nprovide a systematic investigation into the mechanics of\nmulti-task visual learning.\nBy unifying these paradigms\ninto a single encoder, we create a controlled setting to\nstudy how heterogeneous signals cooperate. Specifically,\nwe examine:\n(i) the marginal gain of each objective,\n(ii) the dynamics of task synergies versus interference,\nand (iii) scaling behavior across varying data and model\nscales. Empirically, MTV achieves \u201cbest-of-both-worlds\u201d\nperformance, yielding representations that are both more\ngeneral and more data-efficient than any single objective\nalone.\nFor instance, our ViT-Base model trained on\n100M samples achieves 69.4% ImageNet zero-shot accu-\nracy\u2014outperforming CLIP-Base [33] trained on 400M\nsamples\u2014while delivering substantial gains across re-\ntrieval, depth estimation, segmentation, and correspondence\nbenchmarks.\nThese results demonstrate that multi-task\nvisual pretraining with high-quality pseudo-supervision\nenables the encoder to capture fine-grained, spatially\ngrounded cues absent in pure vision-language models.\nTaken together, these findings suggest a clear design\nprinciple: expanding conventional visual pretraining with\ncomplementary tasks\u2014even when supervised by imper-\nfect pseudo labels\u2014shifts learning toward a unified, multi-\ngranularity representation space. This unification, analo-\ngous in spirit to the convergence of diverse language tasks\nfor LLM, offers a scalable and data-efficient pathway for\nbuilding versatile visual encoders capable of anchoring a\nwide range of perception and multimodal reasoning tasks.\n2. Related Work\nSelf-supervised Learning (SSL). Self-supervised visual\npretraining aims to learn transferable representations with-\nout manual annotations. Contrastive and distillation-based\nmethods, such as DINO [5], iBOT [51], and I-JEPA [2],\nencourage feature invariance across augmented views,\nwhereas reconstruction-based approaches, like MAE [16],\nrely on masked image modeling.\nRecent large-scale ef-\nforts such as DINOv2 [32] demonstrate that scaling data and\nmodel size leads to robust features for recognition, segmen-\ntation, and correspondence tasks. However, these methods\nremain limited to unimodal visual signals and lack seman-\ntics from natural language or high-level reasoning [38].\nVision-Language Contrastive Learning (VL). Weakly-\nsupervised vision-language pretraining has emerged as a\ndominant paradigm for learning semantic-aligned visual en-\ncoders. CLIP [33] and ALIGN [19] show that large-scale\nvision-language contrastive learning yields powerful zero-\nshot transfer. Subsequent works such as Florence [47] and\nSigLIP [49] further improve alignment quality and scala-\nbility through better data curation and sigmoid-based ob-\njectives. Despite their impressive semantic generalization,\nthese models are optimized purely for global alignment\nand often underperform on spatially structured or geometric\ntasks such as depth and correspondence estimation [10].\nSelf-Training (ST). Self-training refers to using a su-\npervised model to generate pseudo labels on unlabeled\ndata, followed by training a student model on these\npseudo-labeled samples [43].\nThis paradigm has been\nwidely adopted to expand supervision beyond human-\nannotated datasets.\nFor VL contrastive learning, sev-\neral approaches [11, 23, 25, 46] leverage language or vi-\nsion\u2013language models to generate synthetic captions or\nrewrite noisy web captions to enhance CLIP-style train-\ning. However, textual descriptions alone cannot fully cap-\nture the rich visual content within images.\nBeyond VL\npretraining, self-training has also demonstrated remarkable\nsuccess in other visual domains. For instance, Depth Any-\nthing [44, 45] first trains a teacher model on high-quality\nsynthetic depth data and then uses it to generate pseudo la-\nbels on real images for student training, leading to state-of-\nthe-art monocular depth estimation. Similarly, OWLv2 [29]\nemploys a supervised open-vocabulary detector to pro-\nduce billion-scale pseudo labels for scaling. Overall, self-\ntraining has emerged as an effective strategy for scaling\nvisual supervision by leveraging strong teacher models to\ngenerate reliable annotations at an unprecedented scale. We\nbuild upon this trend by extending self-training to multi-\ntask settings that combine VL and SSL objectives, enabling\nmore general and scalable visual representation learning.\nMulti-task Visual Representation Learning. Multi-task\nlearning is a training paradigm where a model is jointly\noptimized on multiple tasks, leveraging shared representa-\ntions to capture common structures and improve general-\nization across them. Recent progress in SSL, VL, and ST\nhas inspired efforts to combine these paradigms for building\nmore general visual encoders. Methods such as SLIP [30],\nSILC [31], and TIPS [27] show that integrating VL and\nSSL objectives yields complementary supervision, improv-\ning both semantic alignment and spatial understanding. In\nthe context of ST, MuST [14] pioneered the idea that com-\n2\nbining multiple self-training tasks can yield stronger repre-\nsentations than purely supervised or self-supervised learn-\ning.\nBuilding upon this direction, we employ stronger\nand open-vocabulary teacher models and integrate them\nwith VL and SSL objectives within a unified framework.\nWhile SigLIP2 [40] also explores combining VL, SSL, and\ngrounding-based self-training [41], it lacks systematic anal-\nyses of how task composition and scaling affect learned\nrepresentations. In contrast, we conduct detailed studies\nacross task diversity, data scale, and model size, revealing\nseveral novel insights\u2014for instance, depth supervision pro-\nvides strong benefits for general representation learning.\n3. Preliminary\nOur goal is to learn a unified visual representation that sup-\nports a broad spectrum of visual understanding tasks via\nlarge-scale, multi-source supervision. We begin by intro-\nducing the notations and key components used throughout\nthe paper, covering both the shared architecture and the het-\nerogeneous annotations that supervise it.\nArchitecture. At the core of our framework is a shared\nVision Transformer (ViT) [9] f\u03b8 that maps an input image\nvi \u2208RH\u00d7W \u00d73 to multi-layer patch features:\n{z(1)\ni , . . . , z(L)\ni\n} = f\u03b8(vi),\nz(l)\ni\n\u2208RN\u00d7D,\nwhere L is the number of layers, N the number of patches,\nand D the feature dimension. For brevity, we denote the fi-\nnal layer representation as zi := z(L)\ni\n. For tasks requiring a\nglobal image-level representation, we apply attentive pool-\ning [49] to the final patch features: vi = Pool(zi) \u2208RD.\nSimultaneously, a text encoder g\u03d5 processes natural lan-\nguage inputs. To enable cross-modal alignment, g\u03d5 maps\ntext ti into a joint latent space shared with the pooled visual\nfeatures, producing the text embedding ti = g\u03d5(ti) \u2208RD.\nData. We consider a pretraining corpus D = {(vi, ai)},\nwhere each image vi is paired with automatically acquired\nannotations ai that provide semantic, spatial, and geomet-\nric signals: (i) A caption ti capturing the global semantics\nof the image, used for vision-language contrastive learn-\ning; (ii) Region\u2013text pairs {(bij, rij)} where bij and rij\ndenote the j-th bounding box and its entity name. These\npairs are obtained by extracting salient entities from each\nimage using RAM++[17], and then prompting the open-\nvocabulary detector OWLv2-Base[29] to localize the re-\nferred regions; thus enabling large-scale grounding supervi-\nsion without human annotations; (iii) A relative depth map\ndi \u2208RH\u00d7W that encodes the per-pixel depth, produced by\nthe Depth-Anything-V2-Large model [45].\nThese heterogeneous annotations, derived entirely from\nweb data and off-the-shelf specialist models, provide a rich\nsupervisory signal for studying the effectiveness of large-\nscale multi-task representation learning.\n4. Multi-task Visual Representation Learning\nThis section details MTV, a principled multi-task visual\nrepresentation learning framework.\nMTV optimizes a\nshared encoder across three complementary supervision\nparadigms: (i) vision-language (VL) contrastive learning\nfor global semantic alignment, (ii) self-supervised learning\n(SSL) to induce local structural invariances and regularize\nfeature geometry, and (iii) dense supervision to capture fine-\ngrained spatial structure. We elaborate on each supervisory\ncomponent and the training objective below.\n4.1. Global Semantic Supervision\nWe adopt vision\u2013language contrastive learning to provide\nglobal semantic supervision by aligning image and text em-\nbeddings at the instance level. This objective enables the\nmodel to learn high-level semantic representations from\nnoisy, large-scale web data.\nGiven a batch of image\u2013text pairs B = {(vi, ti)}B\ni=1, the\nvisual and textual embeddings vi and ti are obtained us-\ning the visual encoder and text encoder, as defined in Sec-\ntion 3. We adopt the sigmoid-based objective introduced\nby SigLIP [49], which treats the alignment task as a dense\npairwise binary classification problem:\nLVL = \u22121\nB\nB\nX\ni=1\nB\nX\nj=1\nlog\n1\n1 + eyij(\u2212\u03c4vi\u00b7tj+\u03b2) ,\n(1)\nwhere yij = 1 if (vi, tj) is a matched pair and \u22121 oth-\nerwise. The logit scale \u03c4 and bias \u03b2 are learnable parame-\nters initialized from log 10 and \u221210, respectively, as in [49].\nUnlike the softmax-based contrastive loss [33], the sigmoid\nobjective decouples the batch size from the normalization,\nleading to more stable gradients at scale.\nTo support large-scale distributed training, SigLIP orig-\ninally employs a cyclic rotation-based protocol to avoid\nlarge collective communication. However, in our frame-\nwork, we observe that the communication overhead of text\nembeddings is negligible relative to the computational cost\nof the shared multi-task backbone. Therefore, we utilize\na more direct and implementation-efficient alternative: we\nsynchronize textual embeddings across all devices using a\ndifferentiable all gather1 operation. This preserves the\nmathematical correctness of Eq. 1 while facilitating easier\nintegration within a multi-task pipeline.\n4.2. Self-Supervised Objectives\nWhile vision\u2013language supervision captures global seman-\ntics, it provides limited guidance for learning locally consis-\ntent features. To complement the VL objective, we incorpo-\n1all gather is a collective communication primitive that synchro-\nnizes and collects tensors from all devices. We use a differentiable version\nof it that supports gradient backpropagation through tj.\n3\nImage\nEncoder\nText\nEncoder\nEMA\nTeacher\nText\nImage\nGlobal\nImage-Caption\nDense\nRegion-Text, Depth\nSSL\nSelf-distill, Mask\nhall dark wood buffet with\ndrawers drawer wooden\nmodern entry and half\nperfect narrow table\ninnovative long entryway\nconsole furniture\nImage\nCaption\nGrounding\nDepth\n(a) Data Sample\n(b) Multi-task Visual Representation Learning\nFigure 2. Overview of our MTV framework. (a) Each image is paired with a web-crawled caption, and augmented with pseudo\nregion\u2013text pairs and relative depth maps generated by teacher models. (b) MTV jointly learns from three complementary supervision\ntypes: Global (image\u2013caption contrast), Dense (region\u2013text alignment, depth), and SSL (self-distillation, masked feature prediction). A\nshared image encoder is optimized together with a text encoder and an EMA teacher.\nrate two self-supervised tasks within a teacher-student dis-\ntillation framework: (i) local-to-global self-distillation [5]\nand (ii) masked feature prediction [51].\nFollowing\nthe\narchitecture\nin\nDINOv2\n[32]\nand\nTIPS [27], our student network comprises the visual en-\ncoder f\u03b8 and separate MLP projection heads for each SSL\ntask to mitigate gradient interference. The teacher network\nis maintained as an Exponential Moving Average (EMA) of\nthe student, providing stable targets throughout pretraining.\nLocal-to-Global Self-distillation encourages spatial con-\nsistency by aligning the representations of local and global\nviews. For each image vi, the student processes M = 6 lo-\ncal crops to produce their global embeddings {vi,m}M\nm=1,\nwhich are then projected into \u201cprototype scores\u201d {pm\ni }M\nm=1\nby the student MLP. Simultaneously, the teacher processes\na larger, global crop to produce the target scores \u02c6pi. The\nstudent is trained to match the teacher\u2019s distribution using a\ntemperature-scaled KL divergence:\nLdistill = \u2212\nB\nX\ni\nM\nX\nm\nKL\n\u0012\nsoftmax( \u02c6pi \u2212c\n\u03c4t\n)\nsoftmax(pm\ni\n\u03c4s\n)\n\u0013\n,\nwhere \u03c4t and \u03c4s are the teacher and student temperatures\nfor score sharpening, and c is an EMA-updated centering\nvariable that prevents collapsing to a trivial solution by en-\ncouraging a uniform distribution across prototypes.\nMasked Feature Prediction focuses on capturing fine-\ngrained spatual dependencies. We randomly mask 50% of\nthe patches in vi and replace them with a learnable mask\ntoken. The student encodes the masked input into proto-\ntype scores {pj\ni}, while the teacher processes the original\nunmasked image to provide target scores {\u02c6pj\ni} for the cor-\nresponding masked locations j. The student is trained to\nmatch the teacher\u2019s distributions at these masked locations:\nLmask = \u2212\nB\nX\ni\nX\nj\u2208masked\nKL\nsoftmax( \u02c6pj\ni \u2212c\n\u03c4t\n)\nsoftmax(pj\ni\n\u03c4s\n)\n!\n,\nthereby forcing the encoder to model long-range spatial reg-\nularities from incomplete inputs.\nFollowing [32], we further include the KoLeo regular-\nizer [36] to promote a more uniform feature distribution\nwithin each batch. The total SSL objective is defined as:\nLSSL = Ldistill + \u03bbmask \u00d7 Lmask + \u03bbkoleo \u00d7 Lkoleo,\n(2)\nwhere \u03bbmask=2 and \u03bbkoleo=0.1 as in [27, 32]. By integrating\nLSSL with the global VL loss, the model learns representa-\ntions that are both semantically rich and spatially grounded.\n4.3. Dense Structured Supervision\nWhile VL and SSL provide semantic and structural regular-\nities, they often lack the explicit spatial precision required\nfor dense downstream tasks. To bridge this gap, we incor-\nporate dense structured supervision by distilling knowledge\nfrom high-capacity expert models into our shared encoder.\nConcretely, we introduce two complementary objectives: (i)\nregion-level grounding for object-level semantics and (ii)\nper-pixel depth regression for fine-grained geometry.\nRegion-level Grounding. To enforce spatial grounding, we\nincorporate regional-textual correspondences. For each im-\nage vi, we sample up to four region\u2013text pseudo-labels to\n4\nbalance supervision density and computational cost. The vi-\nsual and textual inputs are encoded into patch-level features\nzi and region text embeddings tij, respectively. Following\nCLOC [6], we utilize the Soft Region Aggregation mech-\nanism to handle potential misalignments in pseudo-labels.\nSpecifically, a lightweight Transformer layer processes the\npatch-level features zi, conditioned on the positional encod-\nings derived from each bounding box. This learnable atten-\ntion allows the model to adaptively integrate context beyond\nrigid box boundaries, mitigating noise in pseudo labels. The\nresulting soft region embeddings zij are aligned with their\ncorresponding text embeddings tij using the sigmoid con-\ntrastive loss (Eq. 1), yielding the grounding loss Lground.\nPixel-level Geometric Supervision. To capture dense ge-\nometric priors, we incorporate monocular depth estima-\ntion supervised by high-fidelity pseudo labels. We attach\na lightweight DPT head [34] to the shared visual encoder,\nwhich aggregates multi-layer features {z(l)\ni }l\u2208S from se-\nlected ViT layers to predict a dense depth map \u02c6di \u2208RH\u00d7W .\nThe prediction is supervised using a combination of scale-\nand shift-invariant loss alongside a multi-scale gradient loss\nfrom MiDaS [3], jointly denoted as Ldepth. To mitigate the\ninfluence of potential artifacts in pseudo labels, we adopt\na top-K denoising strategy, discarding the highest 10% of\nper-pixel losses within each sample during training. This\nencourages the encoder to prioritize high-confidence geo-\nmetric structures, providing a structural anchor that com-\nplements the global semantic objectives. Detailed loss for-\nmulations are provided in Section A.2.\nBy unifying region-level semantics and pixel-level ge-\nometry, the total dense supervision loss is defined as:\nLDense = Lground + Ldepth.\n(3)\n4.4. Joint Optimization\nWe optimize the shared visual encoder by jointly minimiz-\ning a multi-task objective that integrates the VL, SSL, and\ndense supervision signals introduced in Eqs. (1)\u2013(3):\nLtotal = LVL + LSSL + LDense.\nNotably, we find uniform loss weighting yields stable con-\nvergence and competitive performance. Consequently, no\nexhaustive hyperparameter tuning is performed. This sta-\nbility suggests a high degree of task synergy within the\nMTV framework.\nAs shown in Figure 2, each training\nsample is fully annotated with a caption, open-vocabulary\ngrounding, and a relative depth map. By leveraging expert\nmodels to synthesize these structured pseudo-labels for ev-\nery image in our pretraining corpus, we eliminate the need\nfor complex task-specific sampling. This unified training\ninterface allows the shared encoder to efficiently internalize\nthe full spectrum of visual information in a single pass.\n5. Experiments\n5.1. Training Details\nArchitecture.\nWe evaluate our MTV framework using\nViT-B/16-224 and ViT-L/16-256 backbones, fol-\nlowing the SigLIP [49] architecture and its text tokenizer.\nTo accommodate the multi-task nature of MTV, we attach\nlightweight heads for SSL (MLP heads), grounding (Trans-\nformer aggregator), and depth estimation (DPT head). Ta-\nble 1 summarizes the incremental cost of each task, while\narchitectural specifications are detailed in Section A.2.\nTraining Data Curation. All experiments are conducted\non the DataComp dataset [12]. We crawled 200 million\nraw image\u2013text pairs and applied a filtering pipeline. We\ndiscard images with a shorter side < 224 pixels and down-\nsample high-resolution images to a maximum dimension of\n1024 pixels while maintaining aspect ratios. This filtering\nremoves low-information outliers and ensures that the \u201dex-\npert\u201d models\u2014OWLv2 and Depth Anything V2\u2014operate\nwithin their optimal receptive fields.\nWe then apply the\npseudo-labeling procedure described in Section 3, result-\ning in 100 million \u201dfully-labeled\u201d samples, each containing\na caption, region\u2013text pairs, and a relative depth map.\nOptimization Configurations. All experiments are con-\nducted on NVIDIA H200 GPUs. We investigate the scal-\ning behavior of MTV at three data scales: 10M, 50M, and\n100M samples. The 10M and 50M data scales are used pri-\nmarily for ablation studies and are trained for 20 and 10\nepochs, respectively, with a global batch size of 4k. Our\nfull-scale 100M model is trained for 32 epochs with a global\nbatch size of 8k, following the standard CLIP [33] protocol\nfor fair comparison. We use the AdamW optimizer with\na learning rate of 5 \u00d7 10\u22124, no weight decay, and a lin-\near warmup over the first 1k steps. Training utilizes BF16\nmixed precision for efficiency, with the exception of the\nDPT depth head, which is kept in FP32 to ensure numer-\nical stability for regressing continuous geometric values.\n5.2. Evaluation Details\nWe evaluate the learned representations across a broad set\nof downstream tasks spanning global-semantic understand-\ning and fine-grained spatial reasoning. As summarized in\nTable 2, each task is assessed on one or more representative\nbenchmarks. Below, we outline the setup for each task.\nZero-shot Image Classification. We report Top-1 accuracy\non the ImageNet-1k [35] validation set using the standard\nprompt-based zero-shot protocol.\nZero-shot Retrieval. For cross-modal alignment, we eval-\nuate image-to-text (I\u2192T) and text-to-image retrieval (T\u2192I)\non COCO [8], reporting the Recall@1 metric.\nMLLM VQA. To assess the quality of our visual fea-\ntures for higher-level reasoning, we integrate the frozen\n5\nTable 1. Training Cost for a ViT-B/16 model under different\ntask combinations.\n(Numbers in parentheses) indicate the increase\nover the previous row. \u22c6The EMA teacher used in SSL is frozen.\nTraining Tasks\nTrainable Params.\nTrain Time\nVL\n203.2 M\n1.0\u00d7\nVL + SSL\u22c6\n232.6 M (+29.4 M)\n1.2\u00d7 (+20%)\nVL + SSL + Ground.\n239.7 M (+7.1 M)\n1.5\u00d7 (+25%)\nVL + SSL + Ground. + Depth\n250.3 M (+10.6 M)\n1.7\u00d7 (+13%)\nTable 2. Summary of evaluation benchmarks.\nTask\nMetric\nBenchmark\nGlobal Semantic Understanding\nZero-shot Classification\nTop-1 Acc.\nImageNet-1k\nZero-shot Retrieval\nRecall@1\nCOCO\nVisual Question-Answering\nScore\nMMVP,\nCVBench,\nRealWorldQA\nFine-grained Spatial Reasoning\nSemantic Segmentation\nmIoU\nADE20k\nGeometric Correspondence\nAvg. Recall\nNAVI\nSemantic Correspondence\nAvg. Recall\nSPair\nMonocular Depth\nRMSE\nNYUv2\nMTV encoder into the TinyLLaVA framework [20] with\na Qwen2.5-3B language model.\nFollowing the stan-\ndard two-stage recipe, we train the model on LLaVA-\n558K, followed by supervised fine-tuning on LLaVA-\nNEXT-779K [24]. Evaluation is conducted on three vision-\ncentric VQA benchmarks categorized by [38], including\nMMVP [39], CVBench [38], and RealWorldQA [42]. We\nreport the average score across these benchmarks.\nSemantic Segmentation.\nWe evaluate on ADE20k [50]\nusing a frozen visual encoder and a linear classification\nhead to predict per-patch semantic labels. Training runs\nfor 40k steps with batch size 32, learning rate 4e-5, and\n576 \u00d7 576 input resolution. Performance is measured by\nmean Intersection-over-Union (mIoU).\nGeometric and Semantic Correspondence. To investi-\ngate the model\u2019s 3D awareness, we utilize the Probe3D\nframework [10]. We assess geometric correspondence on\nNAVI [18], which requires matching pixels originating from\nthe same 3D point across different viewpoints. We further\nevaluate semantic correspondence on SPair [28], testing the\nalignment of semantically similar parts across diverse in-\nstances.\nBoth tasks are measured using Average Recall\nacross various viewpoints or distances.\nMonocular Depth Estimation. Following [32, 40, 48], we\nconcatenate the global token to each patch token output by\nthe frozen visual encoder, and train a linear head to predict\ndepth across 256 discretized bins. A classification loss is\nused during training on NYUv2 [37], with evaluation on its\ntest set. See [48] for more implementation details.\n5.3. Ablations and Analysis\nWe organize our analysis around three core questions.\nQ1: Marginal Gains and Task Contributions.\nWe first examine whether integrating heterogeneous ob-\njectives is consistently beneficial. Table 3 presents the in-\ncremental performance along a principled task expansion\npath: VL \u2192+SSL \u2192+Ground. \u2192+Depth.\nAcross data and model scales, performance almost al-\nways improves monotonically at every step. Absolute Gain\n\u2206highlights that the full multi-task framework is strictly\nbetter than the VL-only baseline. This demonstrates that\nrather than interference, these objectives provide comple-\nmentary supervision that enriches the shared representation.\nWe also observe distinct roles for each objective: (i) SSL\nacts as a powerful general-purpose enhancer, significantly\nlifting both semantic (+14.4% on IN-1k for ViT-L/10M)\nand spatial benchmarks (+10.4 on ADE20k).\nThis sug-\ngests that local structural regularities provide a necessary\ngeometric scaffold for global semantics. (ii) Since ground-\ning serves as a fine-grained vision-language signal, it not\nonly boosts spatial tasks but also provides benefits to global\nmultimodal understanding, such as the +3.8 R@1 gain in\nCOCO I\u2192T retrieval. (iii) Adding depth supervision con-\nsistently yields the final refinement for pixel-level tasks, im-\nproving NYUv2 by up to 0.051 in the ViT-L/50M setting.\nWhile the gains exhibit diminishing marginal returns as\nthe representation nears saturation, the large initial jump\nfrom SSL should be interpreted with caution. These rela-\ntive improvements are partially a consequence of the chosen\nexpansion order and the inherent spatial sparsity of the VL-\nonly baseline. Nevertheless, the consistent positive deltas\nverify the robustness of the joint optimization.\nQ2: Synergy vs. Interference among Objectives.\nWe further investigate how heterogeneous objectives in-\nteract during joint optimization. To quantify their comple-\nmentarity, we define a Task Synergy metric as:\nSynergy(A, B) = \u2206AB \u2212max(\u2206A, \u2206B)\nmax(\u2206A, \u2206B)\n,\n(4)\nwhere \u2206A and \u2206B denote the individual performance gains\nover the VL baseline, and \u2206AB is the gain achieved by\ntheir combination. Intuitively, a positive score indicates that\nthe tasks are mutually beneficial, providing gains beyond\nthose attainable by the superior individual task alone.\nAs shown in Table 4, all task pairs exhibit positive aver-\nage synergy across data and model scales, typically ranging\nfrom 20% to 50%. This empirically confirms that the ob-\njectives are fundamentally complementary rather than in-\nterfering with MTV. Task pairs involving grounding show\nparticularly robust synergy, reinforcing our argument that\nregion\u2013text alignment serves as a critical bridge between\nhigh-level semantics and fine-grained spatial structures.\n6\nTable 3. Task Scaling Effects across model sizes and data scales. (Numbers in parentheses) indicate the incremental increase over the previous\nrow, while Absolute Gain \u2206denotes the overall performance improvement of full multi-task training over the VL-only baseline.\nViT\nData\nTraining Tasks\nIN-1k\nCOCO\nVQA\nADE20k\nNAVI\nSPair\nNYUv2\nAcc.\nT\u2192I\nI\u2192T\nScore\nmIoU\nRecall\nRecall\nRMSE \u2193\nB/16\n10M\nVL\n36.2\n14.8\n21.9\n41.1\n27.5\n39.5\n17.0\n0.643\nVL + SSL\n43.7 (+7.5)\n19.7 (+4.9)\n28.6 (+6.7)\n43.1 (+2.0)\n36.2 (+8.7)\n41.5 (+2.0)\n21.1 (+4.1)\n0.568 (-.075)\nVL + SSL + Ground.\n49.0 (+5.3)\n23.5 (+3.8)\n33.9 (+5.3)\n43.2 (+0.1)\n39.5 (+3.3)\n43.3 (+1.8)\n22.4 (+1.3)\n0.537 (-.031)\nVL + SSL + Ground. + Depth\n49.7 (+0.7)\n23.9 (+0.4)\n34.1 (+0.2)\n43.4 (+0.2)\n41.7 (+2.2)\n43.6 (+0.3)\n22.8 (+0.4)\n0.512 (-.025)\nAbsolute Gain \u2206\n+13.5\n+9.1\n+12.2\n+2.3\n+14.2\n+4.1\n+5.8\n-0.131\nB/16\n50M\nVL\n55.7\n28.2\n43.0\n42.2\n32.2\n39.8\n18.0\n0.604\nVL + SSL\n61.4 (+5.7)\n32.4 (+4.2)\n48.0 (+5.0)\n44.7 (+2.5)\n39.9 (+7.7)\n40.4 (+0.6)\n22.7 (+4.7)\n0.556 (-.048)\nVL + SSL + Ground.\n62.9 (+1.5)\n34.7 (+2.3)\n50.7 (+2.7)\n45.1 (+0.4)\n43.0 (+3.1)\n43.8 (+3.4)\n25.4 (+2.7)\n0.513 (-.043)\nVL + SSL + Ground. + Depth\n62.9\n35.0 (+0.3)\n50.5 (-0.2)\n45.8 (+0.7)\n43.7 (+0.7)\n44.1 (+0.3)\n24.5 (-0.9)\n0.482 (-.031)\nAbsolute Gain \u2206\n+7.2\n+6.8\n+7.5\n+3.6\n+11.5\n+4.3\n+6.5\n-0.122\nL/16\n10M\nVL\n35.0\n14.9\n21.6\n39.0\n30.0\n40.1\n18.1\n0.630\nVL + SSL\n49.4 (+14.4)\n23.7 (+8.8)\n34.0 (+12.4)\n42.1 (+3.1)\n40.4 (+10.4)\n43.4 (+3.3)\n25.6 (+7.5)\n0.535 (-.095)\nVL + SSL + Ground.\n53.4 (+4.0)\n25.6 (+1.9)\n37.8 (+3.8)\n43.7 (+1.6)\n43.6 (+3.2)\n44.3 (+0.9)\n27.0 (+1.4)\n0.498 (-.037)\nVL + SSL + Ground. + Depth\n54.3 (+0.9)\n26.6 (+1.0)\n38.6 (+0.8)\n44.3 (+0.6)\n45.6 (+2.0)\n46.2 (+1.9)\n27.5 (+0.5)\n0.455 (-.043)\nAbsolute Gain \u2206\n+19.3\n+11.7\n+17.0\n+5.3\n+15.6\n+6.1\n+9.4\n-0.175\nL/16\n50M\nVL\n60.8\n31.1\n46.3\n43.3\n37.3\n39.9\n19.8\n0.575\nVL + SSL\n65.4 (+4.6)\n34.5 (+3.4)\n48.0 (+1.7)\n44.9 (+1.6)\n45.5 (+8.2)\n43.2 (+3.3)\n27.3 (+7.5)\n0.501 (-.074)\nVL + SSL + Ground.\n67.6 (+2.2)\n37.4 (+2.9)\n51.5 (+3.5)\n45.9 (+1.0)\n47.2 (+1.7)\n43.7 (+0.5)\n28.1 (+0.8)\n0.472 (-.029)\nVL + SSL + Ground. + Depth\n67.9 (+0.3)\n37.3 (-0.1)\n52.3 (+0.8)\n45.9\n49.1 (+1.9)\n45.9 (+2.2)\n29.7 (+1.6)\n0.421 (-.051)\nAbsolute Gain \u2206\n+7.1\n+6.2\n+6.0\n+2.6\n+11.8\n+6.0\n+9.9\n-0.154\nTable 4. Task Synergy (%) computed as Equation 4, measuring the relative gain from combining two tasks beyond the larger of their\nindividual improvements over the VL-only baseline. \u22c6COCO is short for COCO I\u2192T retrieval.\nViT\nData\nBaseline\nTaskA\nTaskB\nSynergy (%)\nIN-1k\nCOCO\u22c6\nADE20k\nNAVI\nSPair\nNYUv2\nAverage\nB/16\n10M\nVL\nSSL\nGround.\n62.0\n64.4\n37.9\n72.7\n31.7\n40.8\n51.7\nSSL\nDepth\n50.7\n41.8\n31.0\n-10.0\n26.8\n56.6\n32.9\nDepth\nGround.\n22.8\n27.4\n33.8\n36.4\n51.5\n56.2\n38.1\nB/16\n50M\nVL\nSSL\nGround.\n26.3\n54.0\n40.3\n100.0\n57.4\n44.4\n53.7\nSSL\nDepth\n8.8\n4.0\n27.3\n75.0\n61.7\n29.0\n34.3\nDepth\nGround.\n2.6\n26.3\n40.7\n87.5\n78.8\n19.4\n42.6\nL/16\n10M\nVL\nSSL\nGround.\n27.8\n30.6\n30.8\n27.3\n18.7\n38.9\n29.0\nSSL\nDepth\n17.4\n21.8\n27.9\n24.2\n24.0\n53.6\n28.1\nDepth\nGround.\n9.6\n3.5\n32.0\n42.9\n0.0\n12.5\n16.7\nL/16\n50M\nVL\nSSL\nGround.\n47.8\n33.8\n30.7\n21.8\n10.7\n39.2\n30.7\nSSL\nDepth\n43.5\n92.0\n19.5\n9.1\n-34.7\n4.6\n22.3\nDepth\nGround.\n45.5\n33.3\n33.3\n29.4\n17.9\n18.5\n29.7\nWe also observe distinct scaling trends between model\nand data scales: For the B/16 model, increasing data from\n10M to 50M samples leads to more stable and stronger syn-\nergy. While synergy remains positive for the larger L/16\nmodel, the relative scores are generally lower compared to\nB/16. This is expected as higher-capacity VL baselines\nnaturally leave less relative headroom for additional super-\nvision, even as absolute performance continues to rise.\nQ3: Scaling Effects across Data and Model Capacity.\nWe finally investigate how MTV scales with data size\nand model capacity. As illustrated in Figure 3, scaling the\ndata yields broadly monotonic improvements. Notably, our\nViT-Base model trained on only 100M samples consis-\ntently outperforms the CLIP-Base baseline, which was\ntrained on 400M samples. This demonstrates that multi-\ntask supervision can be more data-efficient than pure VL\npretraining for building versatile visual encoders.\nHowever, the scaling behavior is not uniform across\ntasks.\nGeometric and semantic correspondence saturates\nand begins to decline once the number of seen samples\nexceeds 1B. This suggests that correspondence tasks may\nnot benefit from naive data scaling alone and likely require\nmore targeted architectural designs or supervision.\nMoreover, model scaling from B/16 to L/16 (Table 3,\n7\nFigure 3. Data scaling behavior of multi-task visual pretraining. ViT-Base models are trained with 10M, 50M, and 100M samples\nunder our multi-task setting. CLIP-Base [33] is shown as gray dashed line. Lower \u2193is better for NYUv2; higher \u2191is better elsewhere.\nTable 5. Comprehensive comparisons with large-scale VLMs.\nViT\nModel\nData\nIN-1k\nCOCO\nVQA\nADE20K\nNAVI\nSPair\nNYUv2\nAcc.\nT\u2192I\nI\u2192T\nScore\nmIoU\nRecall\nRecall\nRMSE \u2193\nB/16\nCLIP [33]\n400M\n68.3\n33.1\n52.4\n44.8\n42.2\n36.9\n15.9\n0.603\nSigLIP [49]\n10B\n76.2\n47.2\n64.5\n47.7\n45.1\n38.5\n17.3\n0.615\nSigLIP2 [40]\n10B\n78.2\n52.1\n68.9\n48.1\n46.0\n38.0\n20.8\n0.562\nMTV\n100M\n69.5\n41.2\n57.1\n45.6\n44.6\n43.8\n28.4\n0.469\nL/14\nCLIP [33]\n400M\n75.5\n36.5\n56.3\n46.5\n46.2\n36.0\n20.5\n0.588\nL/16\nSigLIP [49]\n10B\n80.5\n51.2\n69.6\n48.3\n47.7\n38.8\n19.7\n0.550\nSigLIP2 [40]\n10B\n82.5\n54.7\n71.5\n52.5\n51.6\n43.2\n29.9\n0.484\nMTV\n100M\n75.2\n43.0\n58.1\n47.6\n48.2\n45.0\n30.1\n0.414\nTable 5) improves performance across tasks under the same\ndata scale, narrowing the gap to larger VL-only baselines\nwhile preserving strong spatial understanding."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper \"Revisiting Multi-Task Visual Representation Learning\"\n\n#### Main Contributions:\nThe paper introduces MTV (Multi-task Visual Pretraining), a framework that integrates vision-language contrastive learning, self-supervised methods, and dense spatial supervision into a unified multi-task visual representation learning system. The goal is to learn a shared backbone across these paradigms to achieve \"best-of-both-worlds\" performance\u2014enhanced fine-grained spatial reasoning without compromising global semantic understanding.\n\n#### Methodology:\n1. **Shared Backbone**: MTV employs a shared Vision Transformer (ViT) backbone for vision-language contrastive learning, self-supervised learning, and dense spatial supervision.\n2. **Synthetic Pseudo-Labeled Data**: To overcome the scarcity of human-labeled data, the authors leverage high-capacity \"expert\" models like Depth Anything V2 and OWLv2 to synthesize dense, structured pseudo-labels at scale for tasks requiring fine-grained spatial information.\n3. **Unified Training Framework**: MTV integrates these diverse supervision paradigms into a single training framework, allowing for joint optimization across multiple objectives.\n\n#### Key Findings:\n1. **Performance Enhancement**: The authors demonstrate that MTV achieves significantly better performance on various downstream visual and vision-language tasks compared to pure VL or SSL models.\n2. **Marginal Gains of Objectives**: Across different data and model scales, the addition of each objective (vision-language contrastive learning, self-supervised learning, dense spatial supervision) consistently improves overall performance, with a notable \"best-of-both-worlds\" effect.\n3. **Synergy vs. Interference Analysis**: The paper provides detailed analysis on how different objectives interact within the MTV framework, revealing that they complement each other rather than interfere.\n\n#### Conclusions:\nThe authors conclude that multi-task visual pretraining, when augmented with high-quality pseudo-supervision from expert models, is a scalable and effective path toward building versatile visual encoders capable of capturing fine-grained spatial information alongside global semantic understanding. The framework not only outperforms pure VL or SSL approaches but also demonstrates robust scaling behavior across varying data and model scales.\n\n### Detailed Breakdown:\n\n#### 1. Introduction:\nThe paper identifies the bifurcated nature of current visual representation learning paradigms, where vision-language models excel at capturing global semantics but lack spatial precision, while self-supervised methods capture intricate local structures but struggle with high-level semantic context. MTV aims to integrate these paradigms into a principled multi-task framework.\n\n#### 2. Related Work:\nThe authors review existing work on self-supervised learning (SSL), vision-language contrastive learning (VL), and self-training (ST). They highlight the limitations of current approaches, particularly in handling fine-grained spatial information.\n\n#### 3. Preliminary:\nThe paper introduces notations and key components used throughout, including a shared Vision Transformer backbone for multi-layer patch feature extraction. The authors also discuss different types of data annotations that supervise the framework.\n\n#### 4. Multi-task Visual Representation Learning (MTV):\n- **Global Semantic Supervision**: MTV uses vision-language contrastive learning to align image and text embeddings at instance level.\n- **Self-Supervised Objectives**: Two self-supervised tasks are introduced: local-to-global self-distillation for spatial consistency, and masked feature prediction for capturing fine-grained spatial dependencies.\n- **Dense Structured Supervision**: The authors incorporate dense structured supervision by distilling knowledge from expert models into the shared encoder. This includes region-level grounding for object-level semantics and per-pixel depth regression for geometry.\n\n#### 5. Experiments:\nThe paper evaluates MTV on a variety of downstream tasks, including zero-shot image classification, retrieval, visual question answering (VQA), semantic segmentation, geometric correspondence, and monocular depth estimation. The authors provide detailed training configurations and evaluation metrics.\n\n#### 6. Ablations and Analysis:\n- **Marginal Gains and Task Contributions**: MTV consistently improves performance across different tasks, with SSL acting as a powerful general-purpose enhancer.\n- **Synergy vs. Interference Among Objectives**: All task pairs exhibit positive synergy, suggesting complementary rather than interfering objectives within the framework.\n\n#### 7. Conclusions:\nThe authors conclude that MTV is a scalable and effective approach for multi-task visual representation learning, capable of capturing fine-grained spatial information alongside global semantic understanding, without compromising either aspect."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical\nFederated Learning over Resource-Constrained Edge Networks\nXiaohong Yang, Tong Xie, Minghui Liwang, Chikai Shang,\nYang Lu, Zhenzhen Jiao, Liqun Fu, Seyyedali Hosseinalipour\nAbstract\nTraining large language models (LLMs) at the net-\nwork edge faces fundamental challenges arising\nfrom device resource constraints, severe data het-\nerogeneity, and heightened privacy risks. To ad-\ndress these, we propose ELSA (Efficient LLM-\ncentric Split Aggregation), a novel framework that\nsystematically integrates split learning (SL) and hi-\nerarchical federated learning (HFL) for distributed\nLLM fine-tuning over resource-constrained edge\nnetworks.\nELSA introduces three key innova-\ntions. First, it employs a task-agnostic, behavior-\naware client clustering mechanism that constructs\nsemantic fingerprints using public probe inputs\nand symmetric KL divergence, further enhanced\nby prediction-consistency-based trust scoring and\nlatency-aware edge assignment to jointly address\ndata heterogeneity, client unreliability, and com-\nmunication constraints. Second, it splits the LLM\ninto three parts across clients and edge servers,\nwith the cloud used only for adapter aggregation,\nenabling an effective balance between on-device\ncomputation cost and global convergence stability.\nThird, it incorporates a lightweight communica-\ntion scheme based on computational sketches com-\nbined with semantic subspace orthogonal perturba-\ntion (SS-OP) to reduce communication overhead\nwhile mitigating privacy leakage during model ex-\nchanges.\nExperiments across diverse NLP tasks\ndemonstrate that ELSA consistently outperforms\nstate-of-the-art methods in terms of adaptability,\nconvergence behavior, and robustness, establishing\na scalable and privacy-aware solution for edge-side\nLLM fine-tuning under resource constraints.\n1\nIntroduction\nLarge language models (LLMs) have demonstrated remark-\nable performance across diverse language tasks, and are\ntypically adapted to real-world applications via fine-tuning\n[Luo et al., 2025; Zhang et al., 2025b; Liu and Zhao,\n2026].\nNevertheless, deploying LLM fine-tuning directly\non resource-constrained edge devices faces fundamental lim-\nitations in compute, memory, communication, and energy\n6\n4\n7\n\u00d7 Block\nEmbedding\n\u00d7 Block\nUnsketch\n\u00d7 Block\nCloud\n...\n...\n...\n\u2460\n\u2462\n\u2461\n...\n\u2460\u2462\n\u2461\nGlobal Aggregation\nSketch\nSketch\n\u2461\nSketch\nClient\nClient\nEdge\nEdge\nCloud\nCloud Cloud\nCloud\nCluster\nCluster\n11\nCluster\n11\n\u2460\n\u2462\n\u2460\u2461\u2462Train different parts of the model\nModel\nparameters\nModel\nparameters\nChannel\nChannel\nEdge\nEdge\nUnsketch\n10\n11\n12\n9\n3\n2\n5\n8\n1\nSS-OP\nSS-OP\nLoRA\nLoRA\nX\nX\nX\nUntrustworthy\nX\nFigure 1: A schematic of our proposed ELSA: clients and edge\nservers collaboratively fine-tuning LLMs via LoRA, with activations\ncompressed during transmission; the cloud globally aggregates the\nLoRA parameters.\n[Farr et al., 2025; Yang et al., 2025a]. Moreover, data col-\nlected at the edge is inherently non-independent and iden-\ntically distributed (non-IID), which exacerbates local model\nbias across edge devices. Together, these challenges highlight\nthe need for a principled, architecture-aware distributed learn-\ning framework capable of enabling resource-efficient and ro-\nbust LLM fine-tuning across edge devices [Liu et al., 2022;\nXu et al., 2023; Mai et al., 2024].\nFederated learning (FL) offers a natural solution to the\naforementioned challenges by enabling collaborative model\ntraining across geo-distributed edge devices with non-IID\ndata [Yang et al., 2025b; Chen et al., 2025a]. However, as\nthe scale of the system grows, requiring all edge devices to\ndirectly communicate with a single server can lead to se-\nvere congestion over backhaul links and impose substantial\ncommunication overhead. To alleviate this bottleneck, hier-\narchical federated learning (HFL) has emerged as an exten-\nsion of FL [Chen et al., 2025c; Lu et al., 2024; Wu et al.,\n2024]. While HFL improves scalability and reduces back-\nhaul load, it largely assumes that full model updates can still\nbe locally obtained and transmitted, an assumption that be-\ncomes increasingly untenable for LLMs due to their massive\nparameter sizes and memory footprints.\nThese limitations\nmotivate the incorporation of split learning (SL), which parti-\ntions the model across devices and servers, enabling edge de-\nvices to train only lightweight model segments locally while\narXiv:2601.13824v1  [cs.LG]  20 Jan 2026\noffloading deeper, more resource-intensive components to\ncomputationally-capable network nodes (i.e., edge and cloud\nservers) [Lin et al., 2024; Kim et al., 2024]. Although the\ncombination of HFL and SL is particularly promising for\nscalable and resource-efficient LLM fine-tuning at the net-\nwork edge, a principled framework that jointly integrates\nthese two paradigms for LLMs remains largely unexplored,\nmotivating this work. Specifically, we propose ELSA (Effi-\ncient LLM-centric Split Aggregation), which aims to system-\natically investigate the potential of jointly integrating SL and\nHFL for LLM fine-tuning, forming a novel hybrid SL-HFL\nframework, entailing the following key contributions:\n\u2022 We design a task-agnostic, behavior-aware hierarchical\nclustering mechanism. This strategy is further enhanced\nwith (i) a trustworthiness score based on prediction con-\nsistency (to filter poisoned or noisy clients), and (ii) a\nlatency-aware edge server assignment policy. In addi-\ntion, we develop an SL-aware model segmentation strat-\negy that explicitly leverages the heterogeneous capabili-\nties of devices and edge servers.\n\u2022 We introduce a model compression scheme based on\ncomputational sketches.\nBuilding on this design, we\nincorporate semantic subspace orthogonal perturbation\n(SS-OP) to preserve data privacy throughout the train-\ning process.\n\u2022 Extensive evaluations on natural language processing\n(NLP) tasks, including text classification and informa-\ntion extraction, demonstrate that ELSA consistently out-\nperforms state-of-the-art benchmarks.\n2\nRelate Work\n2.1\nFL-driven Fine-Tuning of LLMs over Edge\nNetworks\nFL has been recently explored as a promising paradigm for\nLLM fine-tuning in network edge environments. For exam-\nple, [Qiao et al., 2025] introduced an adaptive asynchronous\naccelerated FL for LLMs to enhance resource utilization effi-\nciency and model accuracy in edge computing environments.\n[Liang et al., 2023] designed FL-empowered collaborative\ntraining for bidirectional encoder representations from trans-\nformers (BERT) models and multi-scale convolutional neu-\nral networks (CNNs).\nAlso, [Ding and Hu, 2024] facili-\ntated multi-user collaborative training of LLMs by integrat-\ning FL with model parallelism strategies, while [Zhang et al.,\n2025a] proposed a lightweight large-model-based joint multi-\ntask multi-domain learning framework.\nMoreover, [Tang\net al., 2025] presented an FL-driven LLM fine-tuning ma-\nlicious traffic detection method using BERT, and [Chen et\nal., 2024] introduced a collaborative training framework for\nTransformer block classification based on FL.\nThe above works often overlook scalability bottlenecks in\nlarge edge networks as they rely on direct client-to-server\ncommunication that can cause congestion on backhaul links,\nexacerbated by heterogeneity and resource limits. While HFL\nalleviates this via intermediate aggregation for narrow models\n[Fang et al., 2024; Sai et al., 2025; Zhang et al., 2025a], its\nadaptation to LLM fine-tuning remains underexplored. Fur-\nther, the massive size of LLMs demands a redesigned HFL\nframework, a gap that this work aims to address through the\nintegration of SL.\n2.2\nReview of the Use of SL over Edge Networks\nTo address the challenges of LLM fine-tuning at the network\nedge under device resource constraints, two prevalent strate-\ngies are commonly adopted: (i) fine-tuning only a subset of\nLLM parameters using parameter-efficient methods such as\nlow-rank adaptation (LoRA); and (ii) partitioning the model\ninto multiple segments and distributing the training process\nacross multiple computing nodes. Both strategies have re-\ncently attracted significant attention in the literature. For in-\nstance, [Tian and Yuan, 2025] proposed to partitioning the\nLLM into two segments for client-server collaborative train-\ning.\n[Ma et al., 2025] proposed a fine-tuning framework\nfor LLMs by strategically partitioning LLMs into device-side\nfrozen layers and server-side, LoRA-enabled fine-tuning lay-\ners. [Wang et al., 2023] introduced a lightweight mask-based\nsegmentation learning framework for the collaborative fine-\ntuning of LLMs. [Cheng et al., 2023] presented a distributed\nhybrid federated segmentation learning framework that lever-\nages device-to-device (D2D) communications to aggregate\nedge resources for model training. [Tian et al., 2022] de-\nsigned a segmented learning framework to facilitate the col-\nlaborative training of BERT models.\nAlthough the above studies have advanced LLM fine-\ntuning, they have largely neglected the communication costs\nincurred during conventional FL: they do not consider LLM\nfine-tuning under HFL architectures, which entail a better\nscalability and backhaul link load. In addition, privacy vul-\nnerabilities arising from the joint use of HFL and SL in LLM\nfine-tuning, particularly those related to information leakage\nthrough shared model updates, remain largely unexplored.\n3\nProposed Method\n3.1\nProblem Formulation\nWe consider distributed learning through an HFL architec-\nture, where the system consists of a cloud server, multi-\nple edge servers K = {1, ..., k, ..., K}, and a set of clients\nN = {1, ..., n, ..., N}, as illustrated in the upper right por-\ntion of Fig. 1. Each client n owns a local dataset Dn =\n{(xj, yj)|1 \u2264j \u2264|Dn|}, where xj and yj refer to the feature\nvector and the label of the jth local data point, respectively.\nSubsequently, given a pre-trained LLM with fixed backbone\nparameters wLLM and the trainable fine-tuning parameter \u03b8,\nthe loss function for a data sample (xj, yj) is defined via the\ntask specific loss (e.g., cross entropy for classification):\nf\n\u0000xj, yj; wLLM, \u03b8\n\u0001\n= Ltask\n\u0000\u03c8\n\u0000xj; wLLM, \u03b8\n\u0001\n, yj\n\u0001\n(1)\nwhere \u03c8(\u00b7) is the representation output of LLM. Collectively,\nthe optimization objective to minimize the global loss func-\ntion across all participating clients, is given by:\nargmin\n\u03b8\neF(\u03b8) =\nN\nX\nn=1\n|Dn|\n|D| Fn(\u03b8),\n(2)\nwhere |D| = PN\nn=1 |Dn|, and\nFn(\u03b8) =\n1\n|Dn|\nX\n(xj,yj)\u2208Dn\nf(xj, yj; wLLM, \u03b8).\n(3)\n3.2\nHFL with Tripartite Split Training\nIn the following, we first introduce a client clustering strategy\nbased on Kullback\u2013Leibler divergence (KLD) to address data\nheterogeneity and unreliable trustworthiness and communi-\ncations across clients. Then, we detail the end-to-end work-\nflow of LLM partitioning and collaborative training across\nclients. Finally, we present a compression framework aimed\nat simultaneously enhancing communication efficiency and\nsafe guarding the data privacy during training.\nBehavior-aware Hierarchical Clustering\nPrior FL/HFL methods cluster clients via KL divergence on\nlabel distributions [Rahad et al., 2025; Yang et al., 2025c]\nto mitigate the adverse effects of non-IID data, under the\nassumption that similar class proportions imply compatible\noptimization landscapes, a reasonable approximation for vi-\nsion tasks with CNNs but ill-suited for LLMs. In distributed\nLLM fine-tuning, (i) downstream tasks lack a shared label\nspace (e.g., classification vs. summarization), and (ii) het-\nerogeneity arises from semantic/behavioral divergence, how\nlocal models interpret identical inputs, not just class imbal-\nance.\nMoreover, real-world HFL must handle unreliable\nclients (e.g., poisoned data) and unstable connectivity. To\naddress these gaps, our framework combines probe-based be-\nhavioral fingerprints, prediction-consistency trust scores, and\nlatency-aware client\u2013server assignment to enable robust, scal-\nable HFL for LLMs.\nSubsequently, we formalize our ap-\nproach in four steps:\nStep 1: Construction of a Public Probe Set. We construct a\nlightweight public probe set P = {x(1), ..., x(Q)} from open\nbenchmarks (e.g., GLUE [Wang et al., 2019]), distributed to\nall clients as a common reference for privacy-preserving be-\nhavioral evaluation.\nStep 2: Task-Agnostic Behavioral Fingerprint Extraction.\nFor each client n, we feed every probe input x(j) \u2208P into its\nlocally fine-tuned LLM (e.g., BERT) and extract the contex-\ntual representation of the [CLS] token, a standard sentence-\nlevel embedding used across transformer-based LLMs, de-\nnoted by Tj\nn \u2208RDhidden, where Dhidden represents the hid-\nden dimension. To obtain a unified-dimension, task-agnostic\nprofile that is invariant to downstream task head structures,\nwe model the client\u2019s behavioral fingerprint as a multivari-\nate Gaussian distribution: Rn = e\nN(\u00b5n, \u03a3n), where \u00b5n =\n1\nQ\nPQ\nj=1 T(j)\nn and \u03a3n = 1\nQ\nPQ\nj=1(T(j)\nn \u2212\u00b5n)(T(j)\nn \u2212\u00b5n)\u22a4\nare the empirical mean and covariance of the client\u2019s [CLS]\nembeddings over the probe set.\nStep 3: Symmetric KL Divergence for Behavioral Dis-\ncrepancy.\nWe quantify the data heterogeneity between\nclients n and n\u2032 via the symmetrized KL divergence between\ntheir Gaussian fingerprints:\nR(n, n\u2032) = KL(Rn\u2225R\u2032\nn) + KL(R\u2032\nn\u2225Rn),\n(4)\n1\n2\n11\n13\n9\n17\n12\n10\n3\n19\n7\n6\n8\n14\n15\n16\n18\n4\n5\n20\nClient ID: 1-20\n1\n2\n11\n13\n9\n17\n12\n10\n3\n19\n7\n6\n8\n14\n15\n16\n18\n4\n5\n20\nClient ID: 1-20\n0\n5\n10\n15\n20\n25\n30\n(a)\n0\n1\n2\n3\n4\n5\n6\n7\n8\nX Coordinate\n0\n1\n2\n3\n4\n5\n6\n7\n8\nY Coordinate\nEdge 0\nEdge 1\nEdge 2\nEdge 3\nX\nC0\nC1\nX\nC2\nC3\nC4\nC5\nC6\nXC7\nC8\nC9\nC10\nC11\nC12\nC13\nX\nC14\nC15\nXC16\nX\nC17\nC18\nC19\nEdge Server\nNormal\nDiscarded\nThreshold max\n(b)\nFigure 2: Client behavioral heterogeneity and clustering outcome in\na 20-client network. Left: 20 \u00d7 20 pairwise KL divergence matrix\nR(n, n\u2032) across all clients. Right: final client-server association\nmapping.\nwhere the closed-form KL between multivariate Gaussians is\ngiven by\nKL( e\nNn\u2225e\nNn\u2032) = 1\n2\n\u0014\ntr(\u03a3\u22121\nn\u2032 \u03a3n) \u2212Dhidden + ln |\u03a3n\u2032|\n|\u03a3n|\n+ (\u00b5n\u2032 \u2212\u00b5n)\u22a4\u03a3\u22121\nn\u2032 (\u00b5n\u2032 \u2212\u00b5n)\n\u0015\n.\n(5)\nStep 4: Trust and Communication-Aware Client Cluster-\ning. We integrate trustworthiness and communication feasi-\nbility into a unified client clustering framework. Specifically,\nwe first compute a trustworthiness score for each client n:\nwtrust\nn\n= exp\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\u22121\nQ\nQ\nX\nj=1\n1\n\u2225T(j)\nn \u22252\n|\n{z\n}\ninverse confidence\n\u2212\n1\nN \u22121\nX\nn\u2032\u0338=n\nR(n, n\u2032)\n|\n{z\n}\nmean behavioral divergence Rn\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n,\nwhere\n1\n\u2225T(j)\nn \u22252 is the inverse confidence of its [CLS] embed-\ndings across P. Intuitively, a client with low prediction en-\ntropy (high confidence) and low divergence from peers (con-\nsistent behavior) receives a high trust score. We then define\nthe communication-feasible edge server set for each client n:\nEn = {k | \u03c4n,k \u2264\u03c4max, k \u2208K}, where \u03c4n,k is the round-\ntrip latency between client n and edge server k, and \u03c4max is\na system-defined threshold (e.g., 200 ms). We finally con-\nstruct a communication-constrained client partition through\na three-stage process: (i) For each edge server k, we define\nits candidate client set as Ck = {n | k \u2208En}. (ii) Within each\nCk, perform spectral clustering using a trust-weighted affinity\nmatrix:\nA(k)\nn,n\u2032 = wtrust\nn\n\u00b7 wtrust\nn\u2032\n\u00b7 exp (\u2212\u03b3 \u00b7 R(n, n\u2032)) ,\n\u2200n, n\u2032 \u2208Ck,\nwhere \u03b3 > 0 controls the sensitivity to behavioral divergence.\n(iii) For each resulting cluster Nk \u2286Ck, compute its average\ntrust score \u00afwtrust\nk\n=\n1\n|Nk|\nP\nn\u2208Nk wtrust\nn\n. If wk < wmin (a trust\nthreshold), either merge Nk into the nearest high-trust cluster.\nFinally, the final grouping {Nk}K\nk=1 ensures clusters are be-\nhaviorally coherent, robust to polluted data, and assigned to\nreliably connected edge servers, enabling stable and deploy-\nable hierarchical aggregation.\nTo facilitate understanding, in Fig. 2, we consider a net-\nwork with 20 clients with the local dataset of the clients\nPart 1. Client-side\n(Pre-computation)\n[Input Token IDs]\n= hidden state\n(output of Block )\n[Compress(      ) + Send to Server]\n[Embedding Layer]         Frozen\n[Embedding Layer]         Frozen\n(         Blocks)\n...\nTransformer Block\n[MLP]         Frozen\n[MLP]         Frozen\nSelf-Attention + LoRA(         )\nFrozen\nSelf-Attention + LoRA(           )\nFrozen\n[MLP]         Frozen\n[MLP]         Frozen\nPart 2. Server-side\n(Intermediate)\nSelf-Attention + LoRA(             )\nFrozen\n[MLP]         Frozen\n[MLP]         Frozen\n= hidden state\n(output of Block )\nPart 3. Client-side\n(Post-computation & Task Head)\nSelf-Attention + LoRA(             )\nFrozen\n[Task-Specific Head]\nTrainable(e.g., Linear/MLP)\n[Task-Specific Head]\nTrainable(e.g., Linear/MLP)\nLoss\nLoss\n[Add] & [LayerNorm]\n[Add] & [LayerNorm]\n[Add] & [LayerNorm]\n[Add] & [LayerNorm]\nTransformer Block\n[Add] & [LayerNorm]\n[Add] & [LayerNorm]\n[Add] & [LayerNorm]\n[Add] & [LayerNorm]\n[Compress(            ) + Send\nto Client]\n[Pooler Layer]         Frozen\n[Pooler Layer]         Frozen\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nLogits\nTransformer Block 1\nTransformer Block\nFrozen\nSelf-Attention +\nLoRA(                      )\nSelf-Attention + LoRA(            )\nFrozen\n...\n(           Blocks)\n(           Blocks)\nTransformer Block\n[MLP]         Frozen\n[MLP]         Frozen\n[Add] & [LayerNorm]\n[Add] & [LayerNorm]\nTransformer Block\n[MLP]         Frozen\n[MLP]         Frozen\n[Add] & [LayerNorm]\n[Add] & [LayerNorm]\n(         Blocks)\n(         Blocks)\n[MLP]         Frozen\n[MLP]         Frozen\n...\n+\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n+\nFigure 3: Breakdown of ELSA: client trains Part 1, sends com-\npressed activations to edge for Part 2, which then feeds into Part 3 on\nclient. Gradients flow backward symmetrically (\u2207eHdown\nn\n\u2212\u2192Edge,\n\u2207eHup\nn \u2212\u2192Client), completing one round.\nare formed via distributing the SQuAD dataset using Dirich-\nlet distribution with parameter \u02c6\u03b1 = 0.1. Fig. 2(a) shows a\nheatmap of pairwise KLD (R(n, n\u2032)) visualizing data dis-\ntribution shifts; Fig. 2(b) displays the final client clustering\n(client\u2013server assignment). Only in-range, high-trust clients\n(green circles, \u2018Normal\u2019) participate in edge-level aggrega-\ntion; others (marked \u2018X\u2019) are excluded as untrustworthy or\nout-of-range.\nModel Splitting and Collaborative Training\nThe tripartite model segmentation in ELSA, where both the\ninitial (Part 1) and final (Part 3) segments reside on the client,\nand the intermediate transformer blocks (Part 2) are offloaded\nto the edge server, is designed to jointly ensure privacy, label\nconfidentiality, and computational feasibility. By keeping the\nembedding layer and task-specific head on-device, sensitive\ndata (input tokens, labels, and predictions) never leave the\nclient, eliminating label leakage and mitigating reconstruc-\ntion attacks on embeddings or logits. In Fig. 3, we parti-\ntion a LLM architecture (e.g., BERT) with M transformer\nblocks into three parts. Only low-rank adapter parameters\n\u03b8 = [\u03b81, \u03b82, \u03b83] (i.e., (A1\n1, B1\n1), ..., (AM\n3 , BM\n3 )) forming the\nadapters, where \u2225\u03b8\u2225\u2208RM(M \u226aD) within each attention\nmodule and the lightweight task head\u2014are trainable, ensur-\ning minimal communication and memory overhead.\nEach client n \u2208Nk within the coverage edge server k,\nfeed-forwards its data through Part 1 of its model, generating\na hidden layer state, defined as\nHup\nn =\nh\nhup\nn,1, . . . , hup\nn,d, . . . , hup\nn,D\ni\n\u2208RD\nfembed(x;\u03b81)\u2192f1\u223cp,\n(6)\nwhere hup\nn,d is the d-th component of Hup\nn , fembed(x) denotes\nthe embedding layer of the LLM, and f1\u223cp represents the se-\nquential feed-forward through transformer block 1 to block\np at the client. To mitigate privacy risks and communica-\ntion cost, the client compresses Hup\nn via sketching and sends\nthe compressed representation (i.e., eHup\nn ) along with attention\nmask An to the edge server. The server reconstructs eHup\nn and\nprocesses it through Part 2, defined as\nHdown\nn\n=\nh\nhdown\nn,1 , . . . , hdown\nn,d , . . . , hdown\nn,D\ni\n\u2208RD\nfp+1\u223cp+q( e\nHup\nn ,An;\u03b82), (7)\nwhere hdown\nn,d is the d-th component of Hdown\nn\nand fp+1\u223cp+q(.)\ndenotes sequential feed-forward through transformer blocks\np to p + q. Then, Hdown\nn\nwill be compressed into eHdown\nn\nand\nsent to the client. Next, the latest o model segments on client-\nside perform computations to obtain logits Z and loss value,\nwhich triggers gradient backpropagation; mathematically,\nZ = fp+q+1\u223cM( eHdown\nn\n, An; \u03b83),\n(8)\nL = Floss(fout(Z, y)),\n(9)\nwhere\nfp+q+1\u223cM(.)\ndenotes\nsequential\nfeed-forward\nthrough transformer blocks p + q to p + q + o, fout(.) is the\noutput layer of the model, y is the targeting sentences and\nFloss(.) denotes the loss function.\nWhile client\u2013edge collaboration improves training effi-\nciency, the restricted representational power of isolated lo-\ncal updates can compromise global model expressiveness.\nTo mitigate this, we fix \u03f1 collaborative training rounds each\nedge server k uploads its refined adapter parameters to the\ncloud for global aggregation, weighted by (i) the internal\nbehavioral coherence of its client group, and (ii) the aggre-\ngated trustworthiness of the participating clients.\nSpecif-\nically, we define:\n\u03b1k\n=\n1\n1+R k \u00d7 \u00afwtrust\nk\n, where Rk\n=\n1\n|Nk|(|Nk|\u22121)\nP\nn\u0338=n\u2032\u2208Nk R(n, n\u2032) is the average pairwise sym-\nmetric KLD among clients in Nk. We normalize the weights\nas e\u03b1k =\n\u03b1k\nPK\nk=1 \u03b1k and compute the global model: \u03b8g =\nPK\nk=1 e\u03b1k\u03b8g,k, where \u03b8g,k denotes the adapter parameters\nfrom edge server k after its local collaborative training in the\ngth round. Training terminates when consecutive global mod-\nels satisfy \u2225\u03b8g \u2212\u03b8g\u22121\u22252 \u2264\u03be, with \u03be > 0 a small convergence\nthreshold (e.g., G).\nLayered Compression for Secure and Efficient\nCommunications\nTo jointly reduce communication overhead and prevent pri-\nvacy leakage during client\u2013edge exchanges, we propose a\nsemantic subspace orthogonal perturbation (SS-OP) mecha-\nnism based on computational sketches, designed to preserve\nsemantic fidelity in model updates. Each client n first col-\nlects a matrix Jn \u2208RQ\u00d7Dhidden, denoting the matrix of Q\n(e.g., [CLS] vectors) from recent probe or training inputs.\nInstead of applying a global orthogonal transform, it identi-\nfies its dominant semantic subspace via truncated SVD (or\npower iteration) to extract the top-r principal components.\nUn = Topr\n\u0000SVD(Jn)\n\u0001\n\u2208RDhidden\u00d7r,\n(10)\nwhere U\u22a4\nn Un = Ir spans the most informative semantic di-\nrections. Next, a client-specific random orthogonal matrix\nVn \u2208Rr\u00d7r is generated by QR decomposition of a pseudo-\nrandom Gaussian matrix seeded from the client ID and a pre-\nshared salt: Vn = QR\n\u0000\u03a6(n)\n\u0001\n, where [\u03a6(n)]i,j \u223ce\nN(0, 1)\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nTest accuracy (%)\nFedProx\nFedAMS\nRaSA\nFedCAda\nRoFed\nFedAvg\nELSA\nFedAvg(Random)\n(a) Trec\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nTest accuracy (%)\nFedProx\nFedAMS\nRaSA\nFedCAda\nRoFed\nFedAvg\nELSA\nFedAvg(Random)\n(b) AG News\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nTest accuracy (%)\nFedProx\nFedAMS\nRaSA\nFedCAda\nRoFed\nFedAvg\nELSA\nFedAvg(Random)\n(c) Emotion\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nTest accuracy (%)\nFedProx\nFedAMS\nRaSA\nFedCAda\nRoFed\nFedAvg\nELSA\nFedAvg(Random)\n(d) Banking\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nTest accuracy (%)\nFedProx\nFedAMS\nRaSA\nFedCAda\nRoFed\nFedAvg\nELSA\nFedAvg(Random)\n(e) Trec\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nTest accuracy (%)\nFedProx\nFedAMS\nRaSA\nFedCAda\nRoFed\nFedAvg\nELSA\nFedAvg(Random)\n(f) AG News\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nTest accuracy (%)\nFedProx\nFedAMS\nRaSA\nFedCAda\nRoFed\nFedAvg\nELSA\nFedAvg(Random)\n(g) Emotion\n20\n40\n60\n80\n100\nEpoch\n0\n20\n40\n60\n80\n100\nTest accuracy (%)\nFedProx\nFedAMS\nRaSA\nFedCAda\nRoFed\nFedAvg\nELSA\nFedAvg(Random)\n(h) Banking\nFigure 4: Test performance comparison across TC datasets under two levels of data heterogeneity: (a)\u2013(d) correspond to \u02c6\u03b1 = 0.1, while\n(e)\u2013(h) use \u02c6\u03b1 = 0.2.\nMethod\nData Setting \u02c6\u03b1 = 0.1\nData Setting \u02c6\u03b1 = 0.2\nRTE\nCB\nMultiRC\nSQuAD\nRTE\nCB\nMultiRC\nSQuAD\nAcc.\nAcc.\nF1/EM\nF1/EM\nAcc.\nAcc.\nF1/EM\nF1/EM\nFedAvg\n79.21\n80.23\n79.61/50.17\n76.92/87.06\n79.91\n81.03\n79.96/52.03\n77.22/87.72\nFedProx\n78.92\n82.14\n80.68/49.19\n78.33/87.31\n79.54\n83.62\n81.33/50.02\n79.01/87.70\nFedAMS\n80.13\n79.82\n81.44/51.33\n77.66/86.21\n81.22\n82.38\n81.76/52.05\n78.02/87.05\nRaSA\n78.92\n81.24\n79.55/51.27\n78.38/86.79\n80.53\n83.24\n80.62/52.08\n78.99/86.95\nFedCAda\n79.23\n82.66\n79.81/51.39\n78.52/86.45\n81.31\n84.23\n80.12/52.41\n80.03/87.29\nRoFed\n79.18\n82.71\n80.22/51.09\n78.50/86.55\n82.33\n83.19\n82.19/52.32\n79.33/87.16\nFedAvg (Random)\n78.48\n79.26\n78.23/48.29\n75.33/85.66\n79.03\n79.77\n79.11/49.06\n76.52/85.98\nELSA (Ours)\n80.93\n83.93\n81.78/52.06\n79.24/87.17\n83.13\n84.99\n82.72/53.16\n80.44/88.04\nGain\n+0.80\n+1.22\n+0.34/+0.67\n+0.72/-0.14\n+0.82\n+0.76\n+0.63/+0.75\n+0.41/+0.32\nNote: Values are formatted as \u2018performance / improve vs. best baseline\u2019.\nTable 1: Performance comparison on four NLP tasks under different \u02c6\u03b1 (0.1 and 0.2). Acc. (%) is reported for RTE and CB, while F1/EM is\nreported for MultiRC and SQuAD.\nwith seedn = Hash(s \u2225n), and s is a pre-shared salt value.\nThe full perturbation matrix is then constructed as:\nQn = UnVnU\u22a4\nn + (IDhidden \u2212UnU\u22a4\nn ).\n(11)\nThis matrix is orthogonal (Q\u22a4\nn Qn = IDhidden) and performs\na random rotation only within the semantic subspace, leav-\ning the orthogonal complement unchanged. This design en-\nsures privacy (via secret-seeded orthogonal perturbation), ex-\nact gradient flow (due to orthogonality), and structural com-\npatibility (by preserving inactive subspace dimensions). The\nuploaded hidden state is then computed as Hup\nn = QnHup\nn ,\nwhich is compressed into a Y \u00d7 Z dimensional sketch ma-\ntrix U using Y independent hash functions, where Z \u226aD.\nSpecifically, each sketch element can be calculated as:\nUn,[j,k] =\nX\nd:hn,j(d)=u\nsignj(d) \u00b7 hup\nn,d,\n(12)\nwhere u \u2208{1, 2, ..., Z} represents the column index of sketch\nmatrix U and signj:1\u2264j\u2264Y : D \u2192{+1, \u22121} denotes Y pair-\nwise independent sign hash functions. Besides, we define the\ncompression ratio throughout the entire transmission process\nas \u03c1 = D/(Y Z). At the edge, we recover the original hidden\nMethod\nAG News\nBanking\nEmotion\nTrec\nRTE\nCB\nMultiRC\nSQuAD\nVanilla Model\n10.51\n6.01\n7.51\n3.38\n14.41\n18.02\n39.64\n36.93\nFedProx\n3.65\n1.97\n2.29\n1.04\n4.65\n5.72\n13.73\n13.08\nFedAMS\n3.57\n1.89\n2.47\n1.00\n4.58\n6.43\n14.58\n14.16\nRaSA\n3.43\n1.93\n2.43\n0.98\n4.79\n6.58\n15.73\n13.73\nFedCAda\n3.43\n2.32\n2.36\n0.95\n4.72\n6.15\n14.30\n13.30\nRoFed\n3.36\n1.89\n2.32\n0.97\n4.43\n6.01\n13.44\n13.08\nFedAvg\n3.79\n2.14\n2.32\n1.07\n4.58\n6.29\n14.87\n13.94\nFedAvg (Random)\n3.93\n2.22\n2.50\n1.09\n5.00\n7.15\n16.87\n14.58\nELSA (Ours)\n3.22/-0.14\n1.86/-0.03\n2.25/-0.04\n0.89/-0.06\n4.29/-0.14\n5.43/-0.29\n13.15/-0.29\n12.23/-0.85\nNote: Values are formatted as \u2018time / reduction vs. best baseline\u2019.\nTable 2: Comparison of the total communication time to reach target\nperformance across different methods (\u00d7103 s).\nlayer information based on the received Un,[j,k] as follows:\neHup\nn = Median1\u2264j\u2264Y {signj(d) \u00b7 Un,[j,hn,j(d)]},\n(13)\nwhere Median(.) is the median of all approximated values.\nCommunication Modeling\nIn ELSA, each global iteration involves \u03f1 client\u2013edge col-\nlaborative rounds and one edge-to-cloud LoRA upload (i.e.,\n|\u03b8LoRA|). The total communication overhead in the gth) global\nround is\nCg = 2\u03f1\u03b6\u00b5Dhidden\n\u03c1\nK\nX\nk=1\nX\nn\u2208Nk\nBn\n|\n{z\n}\nClient \u2194Edge\n+ K \u00b7 |\u03b8LoRA|\n|\n{z\n}\nEdge \u2192Cloud\n,\n(14)\nwhere \u03b6 is the byte size per floating-point parameter (e.g., 4\nfor FP32), \u00b5 the input sequence length, Bn the average mini-\nbatch size of client n. Furthermore, the per-client communi-\ncation time is Tg,n = 2\u03f1Bn\u00b5\u03b6Dhidden/\u03c1\nBeff\n, where Beff represents\nthe effective bandwidth (in bytes/second). Subsequently, the\ntotal communication time until convergence is given by\nTtotal \u2248G \u00d7\nmax\nn\u2208Nk, k\u2208K(Tg,n),\n(15)\nwhere G is the number of global iterations (epochs) required\nto satisfy the convergence criterion.\n3.3\nConvergence Analysis of ELSA\nTheorem 1. Let \u03b7 =\n1\nL\n\u221a\nG, the sequence of global adapter\nparameters {\u03b8g}G\u22121\ng=0 generated by ELSA satisfies:\n1\nG\nPG\u22121\ng=0 E\n\u0014\r\r\r\u2207eF(\u03b8g)\n2\u0015\n\u22644L( eF(\u03b80) \u2212eF \u2217)\n\u221a\nG\n|\n{z\n}\noptimization error\n+\n\u03c32\nlocal\n\u221a\nG\n| {z }\nvanishing noise\n+\n\u03c32\n2\n|{z}\nnon-IID bias\n,\nwhere eF \u2217= min\u03b8 eF(\u03b8) denotes the optimal value of the\nglobal objective, and please see proof in Appendix.\n4\nExperiments and Evaluations\nIn the following, we aim to we evaluate the proposed ELSA\nframework with respect to two key research questions:\n\u2022 How does ELSA perform compared to existing methods?\n\u2022 How do various hyperparameters affect ELSA\u2019s perfor-\nmance, and how can their optimal values be determined?\n4.1\nExperimental Setup\nDatasets: We consider eight diverse datasets spanning vari-\nous NLP tasks: (i) Text Classification (TC): Trec, AG News,\nEmotion, and Banking77, covering question, news, sentiment,\nand fine-grained intent classification [Li and Roth, 2002;\nZhang et al., 2015; Oluwatobi and Erik, 2020]; (ii) Natu-\nral Language Inference (NLI): RTE and CB, which involve\npremise-hypothesis entailment; MultiRC and SQuAD, rep-\nresenting multi-sentence reading comprehension and extrac-\ntive question answering tasks respectively [Wang et al., 2019;\nRajpurkar et al., 2016].\nBaselines: We evaluate and compare the performance of Fed-\nProx [Li et al., 2020], FedAMS [Wang et al., 2022], RaSA\n[Wang et al., 2025], FedCAda [Zhou et al., 2025], RoFed\n[Wang et al., 2026] and FedAvg [McMahan et al., 2017]. Ad-\nditionally, we also study the performance of FedAvg under\nrandomly selected client conditions (i.e., FedAvg(Random)).\nParameters settings:\nWe deploy 20 clients and 4 edge\nservers in an 8 km \u00d7 8 km area (Fig. 2(b)), and 4 clients\nare randomly selected to inject mislabeled samples, simu-\nlating unreliable data. We adopt BERT-base-uncased as the\nLLM architecture (12 Transformer blocks, a hidden dimen-\nsion of 768, and 12 attention heads, totaling approximately\n110 million parameters [Tang et al., 2025; Tian et al., 2022]).\nData heterogeneity combines: Quantity Skew: each client\nn allocates a local subset Dn, with the size |Dn| propor-\ntional to a performance factor \u03c7n =\nn+1\n\u2126k , where \u2126k de-\nnotes the overall samples related to edge k; Label Distribu-\ntion Skew: class imbalance drawn from Dirichlet distribution\nDir(\u02c6\u03b1) with \u02c6\u03b1 ={0.1, 0.2} [Gao et al., 2022].\nAG News Banking Emotion Trec RTE CB\nMultiRC\nSQuAD\nAvg.\nAcc.\nAcc.y\nAcc.\nAcc. Acc. Acc.\nF1/EM\nF1/EM\n\u2014\n\u03c1 = 2.1\n92.33\n86.34\n85.22\n83.33 84.24 84.18 82.45/53.12 85.66/87.88 \u2014\nCo. Be.\n1.67\n1.81\n1.66\n1.82 1.65 1.26\n1.46\n1.29\n1.63\n\u03c1 = 3.3\n91.08\n84.21\n85.02\n83.08 83.18 84.03 81.87/53.01 84.13/87.48 \u2014\nCo. Be.\n2.98\n2.68\n2.77\n2.68 2.73 2.01\n2.45\n2.26\n2.57\n\u03c1 = 6.4\n86.33\n81.03\n81.11\n81.15 80.25 81.36 81.37/51.71 77.13/86.52 \u2014\nCo. Be.\n3.65\n3.84\n3.39\n4.85 4.46 3.28\n3.86\n3.12\n3.81\n\u03c1 = 8.4\n83.22\n75.46\n78.22\n74.66 75.97 73.66 73.34/47.22 76.22/82.11 \u2014\nCo. Be.\n3.39\n4.27\n4.33\n5.56 3.45 2.24\n2.46\n3.80\n3.69\n\u03c1 = 11.8\n78.03\n70.31\n72.84\n71.81 72.25 68.34 68.19/43.18 72.55/80.06 \u2014\nCo. Be.\n3.98\n4.57\n4.72\n5.82 4.98 5.90\n3.77\n3.87\n4.70\nTable 3: Sensitivity of ELSA to compression ratio \u03c1: performance\nand communication benefit.\n4.2\nEvaluation of Learning Efficiency and Model\nPerformance\nTo evaluate training performance, we prioritize convergence\nrate on TC datasets to assess communication efficiency, and\nsteady-state metrics (Acc., F1, EM) on NLI tasks that require\ncomplex semantic reasoning. This ensures that we can cap-\nture both learning efficiency and final representational capac-\nity [Tang et al., 2025; Wang et al., 2026]. As shown in Fig. 4,\nunder quantity-based heterogeneity, ELSA exhibits superior\nconvergence across all TC datasets. Though its initial speed\nis occasionally slower (e.g., Fig. 4(a), (d), (e)), it consistently\nreaches a higher final accuracy, indicating better optimiza-\ntion stability. Table 1 shows ELSA achieves state-of-the-art\nsteady-state performance on NLI tasks (e.g., 80.93% on RTE\nand 81.78% on MultiRC, lagging only slightly behind Fed-\nProx on SQuAD (-0.14% EM)). This consistent gain across\ntasks and heterogeneity levels underscores the robustness of\nELSA\u2019s dual-stage semantic consistency framework in prac-\ntical edge settings. During clustering, behavior-aware finger-\nprints capture clients\u2019 update patterns to group semantically\nsimilar participants, while during aggregation, coherence-\nand trust-weighted fusion dynamically calibrates contribu-\ntions based on both feature alignment and historical reliabil-\nity. Additionally, the observed performance gap between Fe-\ndAvg and FedAvg (Random) across datasets stems from the\nlatter\u2019s vulnerability to unverified clients, further validating\nthe critical necessity of our trust-aware client clustering and\naggregation strategy.\n4.3\nEvaluation of Communication Costs and\nTransmission Efficiency\nIn the following, we evaluate ELSA by analyzing its\ncommunication-utility trade-offs, architectural advantages in\nmodel segmentation, and the effectiveness of the \u201cSS-OP +\nSketch\u201d mechanism in balancing privacy and representation\nquality, focusing on a setting with \u02c6\u03b1 = 0.1, Beff \u2208[50,100]\nMbps, p = 6, q = 4, o = 2.\nAnalysis of System Utility and Communication Efficiency.\nFig. 5 compares ELSA with an uncompressed Vanilla Model\n(performance upper bound).\nDespite using only 26\u201331%\nof the communication volume (3.26\u00d7\u20133.78\u00d7 less), ELSA\nachieves competitive performance (e.g., 83.93% on CB and\n81.78% on MultiRC), demonstrating that our sketch-based\ncompression preserves essential updates with high fidelity.\nFurthermore, Table 2 shows ELSA consistently achieves the\nlowest total communication time to convergence across eight\nComm. Benefit\n82.20\n91.82\n83.90\n88.35\n83.55\n87.18\n89.58\n93.45\n\u00d73.78\n\u00d73.33\n\u00d73.30\n\u00d73.26\nTrec\nEmotion\nBanking\nAG_News\nDatasets\n70\n75\n80\n85\n90\n95\n100\nTest accuracy (%)\nELSA\nVanilla Model\n(a) TC tasks\nComm. Benefit\n80.93\n85.33\n81.78\n84.55\n79.24\n86.22\n83.93\n90.30\n\u00d73.64\n\u00d73.15\n\u00d72.84\n\u00d72.40\nRTE\nMultiRC\nSQuAD\nCB\nDatasets\n70\n75\n80\n85\n90\n95\n100\nTest indicator (%)\nELSA\nVanilla Model\n(b) NLI tasks\nFigure 5: Evaluation performance and communication reduction of\nELSA versus uncompressed schemes on various datasets (Indicator\nare Acc. (RTE, CB) and F1 (MultiRC, SQuAD), compression ratio\n\u03c1 = 4.2).\ndatasets.\nIt reduces time by 69.3\u201373.7% vs.\nVanilla and\n6.05\u201312.64% vs.\nstrong baselines (e.g., FedProx, RaSA),\nwith the largest gains on complex tasks like SQuAD (e.g.,\n0.85\u00d7103 s faster than RoFed), which indicates that, even un-\nder compression, ELSA\u2019s behavior-aware hierarchical clus-\ntering scheme maintains superior generalization and robust-\nness compared to other methods. Next, in Table 3, we ana-\nlyze ELSA\u2019s sensitivity to compression ratio \u03c1. Performance\npeaks at low \u03c1 (e.g., 92.33% on AG News at \u03c1 = 2.1 ) but\ndegrades sharply at high \u03c1 (e.g., 78.03% at \u03c1 = 11.8). Com-\nmunication benefit grows with \u03c1 (up to 4.70\u00d7), yet excessive\ncompression distorts semantic representations, especially on\nNLI/QA tasks. We find that \u03c1 \u2208[2.1, 4.2] offers the best\ntrade-off, delivering 1.63\u00d7\u20132.57\u00d7 communication savings\nwith near-optimal accuracy, while larger \u03c1 should be reserved\nfor extreme bandwidth constraints.\nImpact of model segmentation.\nWe conduct a series of\nsensitivity analyses focusing on model segmentation depth\nand the effectiveness of our hierarchical aggregation com-\npared to vanilla SL."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical Federated Learning over Resource-Constrained Edge Networks**\n\n**Main Contributions**\nThe paper introduces ELSA (Efficient LLM-centric Split Aggregation), a novel framework that integrates split learning (SL) and hierarchical federated learning (HFL). Key innovations include:\n1. A task-agnostic, behavior-aware client clustering mechanism using public probe inputs to construct semantic fingerprints with symmetric KL divergence.\n2. An effective model partition strategy across clients and edge servers, enabling lightweight training on devices while offloading deeper components to edge servers for efficient global aggregation.\n3. A communication scheme based on computational sketches combined with SS-OP (Semantic Subspace Orthogonal Perturbation) to reduce overhead while mitigating privacy leakage.\n\n**Methodology**\nELSA operates within an HFL architecture where clients and edge servers collaborate in fine-tuning LLMs via LoRA, compressing activations during transmission. The cloud aggregates the LoRA parameters globally. ELSA uses a lightweight public probe set for fingerprint construction based on KL divergence, ensuring data privacy throughout training.\n\n**Key Findings**\nExperiments across diverse NLP tasks demonstrated that ELSA consistently outperforms state-of-the-art methods in terms of adaptability, convergence behavior, and robustness. Specifically:\n- It achieves superior performance (e.g., 83.93% on CB) compared to existing methods like FedProx.\n- The framework is scalable and robust under varying data heterogeneity levels.\n\n**Conclusions**\nELSA provides a scalable and privacy-aware solution for edge-side LLM fine-tuning, addressing fundamental challenges in resource-constrained environments. Its combination of SL and HFL ensures efficient global aggregation while preserving model integrity through careful client clustering and communication schemes."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural\nReasoning in LLMs\nMohsinul Kabir1,2, Tasnim Ahmed3, Md Mezbaur Rahman4,\nShaoxiong Ji5,6, Hassan Alhuzali7, Sophia Ananiadou1,2\n1Department of Computer Science, National Centre for Text Mining,\nThe University of Manchester\n2ELLIS Manchester\n3School of Computing, Queen\u2019s University, Ontario, Canada\n4Computer Science, University of Illinois Chicago\n5ELLIS Institute Finland\n6University of Turku\n7Department of Computer Science and Artificial Intelligence,\nUmm Al-Qura University, Makkah, Saudi Arabia\nCorrespondence: {mdmohsinul.kabir, sophia.ananiadou}@manchester.ac.uk\nAbstract\nCross-cultural competence in large language\nmodels (LLMs) requires the ability to iden-\ntify Culture-Specific Items (CSIs) and to adapt\nthem appropriately across cultural contexts.\nProgress in evaluating this capability has been\nconstrained by the scarcity of high-quality CSI-\nannotated corpora with parallel cross-cultural\nsentence pairs. To address this limitation, we\nintroduce XCR-Bench, a Cross(X)-Cultural\nReasoning Benchmark consisting of 4.9k par-\nallel sentences and 1, 098 unique CSIs, span-\nning three distinct reasoning tasks with corre-\nsponding evaluation metrics. Our corpus inte-\ngrates Newmark\u2019s CSI framework with Hall\u2019s\nTriad of Culture, enabling systematic analysis\nof cultural reasoning beyond surface-level ar-\ntifacts and into semi-visible and invisible cul-\ntural elements such as social norms, beliefs,\nand values. Our findings show that state-of-\nthe-art LLMs exhibit consistent weaknesses in\nidentifying and adapting CSIs related to social\netiquette and cultural reference. Additionally,\nwe find evidence that LLMs encode regional\nand ethno-religious biases even within a sin-\ngle linguistic setting during cultural adaptation.\nWe release our corpus and code to facilitate fu-\nture research on cross-cultural NLP.1\n1\nIntroduction\nLarge Language Models (LLMs) have been\nwidely documented to exhibit a pronounced Anglo-\nAmerican, or Western cultural bias in their outputs,\npredominantly reflecting values and perspectives\nassociated with the US and UK (Johnson et al.,\n2022; Dunn et al., 2024; Atari et al., 2023). This\n1Code & Corpus: github.com/mohsinulkabir14/xcr_bench\nbias is often attributed to the West-centric com-\nposition of both their training data and the anno-\ntators involved in their development, making it a\npredictable artifact of their training process (Kabir\net al., 2025c; Navigli et al., 2023). However, recent\nwork has challenged a common implicit assump-\ntion in this line of research: the presence of cultural\nbias does not necessarily imply strong performance\nor competence within the corresponding cultural\ncontext (Saha et al., 2025). This observation raises\na more fundamental question: how should \u201ccul-\nture\u201dbe meaningfully measured in LLMs?\nMoving beyond performance within predomi-\nnantly Western cultural settings, a more challeng-\ning and practically consequential problem is eval-\nuating the cross-cultural competence of LLMs.\nEvaluating cross-cultural competence requires nav-\nigating differences in common ground and about-\nness that go beyond surface-level linguistic or\nstylistic variation (Hershcovich et al., 2022). Ex-\nisting studies largely adopt intrinsic evaluation, fo-\ncusing on isolated assessments of cultural knowl-\nedge, while extrinsic evaluation better captures\nhow cultural competence manifests in realistic\nuser-centric interactions (Bhatt and Diaz, 2024). A\nfoundational concept in this area is the Culture-\nSpecific Items (CSIs), introduced by Newmark\n(1988), which encompass cultural factors ranging\nfrom material artefacts (e.g., food, clothing) to so-\ncial and ideological constructs (e.g., institutions,\nvalues) and are crucial for cross-cultural communi-\ncation. Building on this notion, subsequent work\nmodels culture as measurable facets and evalu-\nates LLMs by probing these facets as proxies for\ncultural awareness (Adilazuarda et al., 2024; Rao\n1\narXiv:2601.14063v1  [cs.CL]  20 Jan 2026\net al., 2024).\nHowever, existing evaluations of\ncross-cultural awareness largely rely on machine\ntranslation\u2013based settings (Yao et al., 2024) or iso-\nlated cultural substitutions such as dialog or recipe\n(Singh et al., 2024; Cao et al., 2024), primarily\ndue to the high cost of constructing realistic cross-\nculture CSI pairs.\nA meaningful evaluation of\ncross-cultural competence in LLMs requires as-\nsessing their ability to reason about and adapt CSIs\nacross diverse realistic cultural contexts - a set-\nting that remains largely underexplored (Guo et al.,\n2025).\nAnother line of work draws on Hall\u2019s Iceberg\n(Triad) Model of Culture (Hall, 1976), which\ndistinguishes between Visible (technical), Semi-\nvisible (formal), and Invisible (informal) cultural\nlevels. While the visible level captures observable\nbehaviors, the semi-visible and invisible levels cor-\nrespond to the underlying norms, values, and be-\nliefs that structure habits and behavior in culture.\nDespite their conceptual relevance, a clear theoret-\nical and empirical link between Newmark\u2019s notion\nof Culture-Specific Items (CSIs) and Hall\u2019s cul-\ntural levels remains absent. Specifically, there is\nno established typology or annotated resource that\nsituates CSIs within Hall\u2019s triadic framework. As\na result, existing evaluations cannot assess whether\nLLMs are capable of detecting and adapting CSIs\nacross different cultural depths, nor whether mod-\nels move beyond surface-level cultural cues to en-\ngage with underlying values and beliefs.\nIn this study, we argue that cultural compe-\ntence in LLMs extends beyond translation to the\nability to recognize, interpret, and adapt Culture-\nSpecific Items (CSIs) across cultures. We present a\ndataset of Western (US/UK) culture-specific items\n(CSIs) in realistic scenarios. It includes detailed\nannotations such as CSI categories, CSI-Hall map-\npings, and Hall\u2019s cultural levels, with a focus\non the semi-visible and invisible layers of Hall\u2019s\ntriad. Based on these annotations, we define two\ntasks, CSI Identification and CSI Prediction, to as-\nsess LLMs\u2019 reasoning of Western cultural knowl-\nedge.\nWe further construct culturally adaptable\nCSI pairs for Chinese, Arabic, and Bengali, with\nBengali additionally divided into West Bengal (In-\ndia) and Bangladeshi variants to capture regional\nvariation. This enables a third task, CSI Adapta-\ntion, focused explicitly on cross-cultural reason-\ning. We propose task-specific evaluation metrics\nand release this resource as the Cross(X)-Cultural\nReasoning Benchmark (XCR-Bench). All three\ntasks operationalize cultural reasoning, as they re-\nquire models to follow a cognitive process to in-\nterpret cultural cues, evaluate contextual appropri-\nateness, and justify culturally grounded decisions\nrather than performing surface-level pattern match-\ning (Huang and Chang, 2023).\nUsing XCR-Bench, we conduct a systematic\nevaluation of state-of-the-art LLMs. While mod-\nels demonstrate relatively strong performance in\npredicting Western cultural facets, they exhibit sub-\nstantially weaker performance in identifying CSIs\nand adapting them across cultural contexts. More-\nover, our results reveal pronounced regional bias\nwithin a single language, with models showing un-\neven adaptation performance across Bengali cul-\ntural variants. Together, these findings highlight\npersistent limitations in LLMs\u2019 cross-cultural rea-\nsoning abilities.\nIn summary, our contributions are as follows:\n\u2022 We connect Newmark\u2019s concept of Culture-\nSpecific Items (CSIs) with Hall\u2019s Triad of cul-\nture by creating and publicly releasing a real-\nistic, culturally diverse corpus. This corpus\nfeatures Western-source CSIs that are manu-\nally annotated with adaptable cross-cultural\nCSI pairs for four target cultures; these are\nmapped to Newmark\u2019s CSI categories and\nHall\u2019s cultural levels, focusing especially on\nthe Semi-visible and Invisible layers.\n\u2022 We formulate cultural competence in LLMs\nas a cross-cultural reasoning problem and\nevaluate it through three unique evaluation\ntasks with tailored metrics in both intra-\nlingual and inter-lingual settings, going be-\nyond conventional machine translation\u2013based\nevaluation.\n\u2022 We provide a thorough empirical analysis of\nLLMs\u2019 cultural proficiency and bias, inves-\ntigating their capabilities in causal reason-\ning, adaptability, and generalizing culture-\nspecific content across diverse contexts.\n2\nXCR-Bench Corpus\nDue to the nuanced nature of CSIs, we incorporate\nhuman annotation and validation throughout XCR-\nBench construction. The process comprises two\nphases: (1) building the base corpus with Western\nCSIs annotated by CSI categories and Hall\u2019s cul-\ntural levels; and (2) annotating adaptable CSI pairs\nfor four target cultures (Chinese, Arabic, Bengali\nfrom West Bengal and Bangladesh) to form a par-\n2\nCSI & Cultural Context\nExtraction\nCANDLE\nCULTURAL\nATLAS\nSentence Generation\nCSI Category & Hall's\nLevel Annotation\nCSI Extracted:Wedding Ring.\nCultural Context:A wedding ring is a finger ring\nindicating that its wearer is married, a practice\noriginally used in ancient Rome during marriage\nRepresentative:Wearing a wedding ring is a\ncommon symbol of marriage in Western cultures.\nWIKIPEDIA\nRepresentative\nCultural Concept\nCultural Context\nCSI\nNewmark\n(1988)\nWhat is the category of the elected\nsentence?\nNewmark\n(1988)\nTaxonomy from\nLiu el al. (2025)\nSocial\nTradition\nHall's Iceberg\nModel (1976)\nCustoms:\nSemi-Visible\nSentence 1\nSentence 2\n(Best)\nSentence 3\nPrompt:Given the following cultural context, write a\nrealistic sentence that naturally incorporates the CSI\nas\u00a0it would be used in everyday communication.\nInput: {CSI,\u00a0Cultural\u00a0Context}\nElected Sentence:She never takes off her <CSI>\nwedding ring </CSI>, a symbol of the love and\ncommitment she shares with her husband.\nHuman Evaluation\nWhat is the Hall's Level of the\nelected sentence?\nGPT-4o\nClaude 3.7\nSonnet\nDeepSeek-R1\nFigure 1: XCR-Bench corpus creation and annotation pipeline. (1) Culture-specific items (CSIs) and their contexts\nare extracted from well-known cultural databases. (2) Sentences are generated using LLMs and filtered for accuracy\nand fluency. (3) Each sentence is annotated with a CSI category and mapped to Hall\u2019s cultural levels.\nallel corpus.\n2.1\nConstruction of Base XCR-Bench Corpus\nFigure 1 provides an overview of the base XCR-\nBench construction pipeline. It includes: (1) CSI\nand cultural context Extraction, (2) sentence gen-\neration with the extracted CSI, and (3) annotating\nthe CSI to produce the category and Hall\u2019s level.\nCSI & Cultural Context Extraction\nThe first step in corpus construction involves identi-\nfying and extracting Culture-Specific Items (CSIs)\nrepresentative of US and UK cultures.\nTo this\nend, we leverage two structured cultural knowl-\nedge resources: CANDLE2 and Cultural Atlas3 .\nCANDLE provides high-quality cultural common-\nsense knowledge (CCSK), comprising 1.1M asser-\ntions organized into 60K coherent clusters, while\nCultural Atlas offers curated descriptions of multi-\ncultural norms derived from extensive global com-\nmunity interviews and rigorous validation (Rao\net al., 2024).\nBoth resources present cultural\nknowledge in factual form (e.g., CANDLE states\na Representative cultural concept as \u201cIn the USA,\npeople wear their wedding rings on the ring fin-\nger of their left hand\u201d), with Cultural Atlas fur-\nther organizing information into thematic sections\nsuch as Core Concepts, Communication, and Busi-\nness Culture. We crawl cultural facts from these\nplatforms, primarily focusing on the US and UK,\nsupplemented by a select few highly relevant ex-\namples from Europe. We then manually extract\n2https://commonsense.scads.ai/candle/\n3https://culturalatlas.sbs.com.au/\nCSIs by mapping these statements to the CSI cat-\negories proposed by Newmark (1988) (described\nin Appendix B.1).\nFor each extracted CSI, we\nthen retrieve a high-level cultural definition from\nWikipedia to contextualize its usage. This process\nyields a set of CSIs paired with their corresponding\ncultural contexts.\nSentence Generation incorporating CSIs\nIn the second step, we generate realistic sentences\nreflecting everyday communication by prompting\nGPT-4o, Claude-3.7-Sonnet, and DeepSeek-R1\nwith each CSI and its corresponding cultural con-\ntext, as illustrated in Step 2 of Figure 1.\nEach\nmodel produces a candidate sentence, resulting in\nthree alternatives per CSI. Two human annotators\nthen evaluate these candidates based on three crite-\nria: accuracy, fluency, and realism, and select the\nhighest-scoring one. We explicitly emphasize re-\nalism in our prompt and selection, as real-world\ncross-cultural contexts are inherently complex and\nessential for assessing cross-cultural competence\n(Rauba et al., 2024). This process yields a set of\nculturally grounded, naturalistic sentences aligned\nwith the original CSI-context pairs. For experimen-\ntal convenience, the selected sentence is annotated\nby enclosing the CSI within <CSI> and </CSI>\ntags.\nCSI Category and Hall\u2019s Level Annotation\nFinally, we categorize each CSI based on its cul-\ntural context and the generated sentence from\nSteps 1 and 2, rather than at extraction time, as\na CSI\u2019s category under Newmark\u2019s taxonomy can\nshift with contextual usage. For instance, jeans\n3\nfalls under Material Culture (Category 2) in \u201cHe\nwore a pair of <CSI>jeans</CSI> with a white\nT-shirt\u201d, but under Social Culture (Category 3) in\n\u201cWearing <CSI>jeans</CSI> to the office on\nFriday felt appropriate because of the company\u2019s\ncasual dress norm,\u201d where it reflects workplace\nnorms rather than mere attire. Although we use\nNewmark\u2019s CSI categories as a foundational frame-\nwork, their broad scope often merges diverse cul-\ntural elements. To address this, we incorporate the\ncultural taxonomy from Liu et al. (2025), mapping\nits ideational and social dimensions to Newmark\u2019s\nCategories 3 (Social Culture) and 4 (Organizations,\nCustoms, and Ideas), respectively. Annotations at\nthis stage achieve a Cohen\u2019s kappa of 0.68, indi-\ncating substantial agreement between the two an-\nnotators.\nWe further annotate each CSI with its cor-\nresponding Hall cultural level, independently of\nNewmark\u2019s categorization:\nfor instance, food-\nrelated CSIs may encode formal or informal cul-\ntural meanings without functioning as material\nartefacts (e.g., Social Tradition:\n\u201cLet\u2019s grab a\n<CSI>coffee</CSI> and catch up sometime!\u201d).\nThis context-sensitive treatment aligns with Sko-\npos Theory (Sch\u00e4ffner, 1997), which argues that\nthe function of an element in the target commu-\nnicative situation, rather than its surface form, de-\ntermines its cultural role. Accordingly, we assign\neach CSI one of Hall\u2019s three levels of cultural vis-\nibility (e.g. Visible, Semi-visible, Invisible). The\nmapping of cultural elements with Hall\u2019s Triad is\nprovided in Table 8. Annotation of Hall\u2019s cultural\nelements yields a Cohen\u2019s kappa of 0.64 between\nthe two annotators.\n2.2\nConstruction of the Parallel Corpus\nEach instance in our dataset is annotated with cul-\nturally adapted counterparts in four target cultures:\nChinese, Arabic, and Bengali (two variants). The\nobjective is to construct culturally equivalent in-\nstances of the Western CSIs from the base XCR-\nBench corpus.\nBecause cultural equivalence is\ninherently subjective and cannot be treated as a\ncategorical labeling problem, we adopt an adjudi-\ncated annotation protocol (Klie et al., 2024). For\nexample, the CSI long drive in the sentence \u201cThe\nfamily is planning a <CSI>long drive</CSI> to re-\nlax after a hectic week\u201d may be adapted in Ben-\ngali as rickshaw rides and street food, or retained\nas long drive, reflecting contemporary urban prac-\ntices. Both adaptations are culturally valid. Rather\nthan treating such variation as disagreement, we in-\nterpret it as cultural pluralism (Berry, 1974). Con-\nsequently, conventional agreement-based metrics\n(e.g., IAA) are not appropriate for this setting.\nTwo native annotators per target culture inde-\npendently produce adaptations following detailed\nguidelines, with disagreements resolved through\ndiscussion and expert moderation. An expert an-\nnotator with cultural and anthropological expertise\noversees the process and provides final adjudica-\ntion. Full details of the annotation procedure and\nannotator qualifications are provided in Section D.\nWe follow Newmark\u2019s framework for cultural\nequivalence (Newmark, 1988) and instruct annota-\ntors to identify one of four equivalence types when\nadapting CSIs to their native culture: (1) a direct\nequivalent, where available; (2) a functionally sim-\nilar equivalent; (3) a neutral expression when no\ncultural equivalent exists; or (4) a Non-transferable\nlabel when the CSI is culturally inappropriate\nor conflicts with local norms (e.g., \u201cThey met\nthrough a <CSI>dating app</CSI>\u201d, which\nmay be non-transferable in certain Arab cultural\ncontexts). In addition to detailed annotation guide-\nlines (demonstrated in Appendix D.1), annotators\nare provided with relevant cultural information on\nrituals and traditions extracted from CANDLE\nand Cultural Atlas. Annotators are further encour-\naged to modify the sentence structure as necessary\nto ensure that both intra- and inter-lingual adapta-\ntions remain coherent and culturally appropriate in\nthe target context.\n2.3\nDataset Analysis\nThe XCR-Bench corpus comprises 4,136 paral-\nlel sentences spanning 4 distinct cultures and 2 re-\ngional variants. It contains 1,098 unique Culture-\nSpecific Items (CSIs), distributed across 7 CSI\ncategories. Table 6 situates XCR-Bench in rela-\ntion to existing datasets for culture-aware bench-\nmarking. Most prior datasets in this area primar-\nily provide translation pairs, with CAMT (Yao\net al., 2024) being the closest in scope, as it in-\ncludes cross-cultural CSI pairs for a subset of\nits data. In contrast, XCR-Bench supports both\nreasoning-oriented and culturally aware adaptation\nevaluations by offering paired intra-lingual and\ninter-lingual annotations. Notably, XCR-Bench\nis the first dataset to provide extensive annotations\naligned with Hall\u2019s triadic model of culture, with\nexplicit emphasis on the Semi-visible and Invisi-\nble layers that encode beliefs, values, and social\n4\nnorms. Combined with our three task formulations,\nthis design enables the evaluation of cultural rea-\nsoning as a cognitive process involving evidence-\nbased judgment over culturally situated knowledge,\nrather than translation-focused competence alone.\nDetailed properties of our dataset are provided in\nAppendix E.\n3\nCross-Cultural Reasoning Evaluation\n3.1\nCSI Identification\nFor the CSI Identification task, we remove the\n<CSI> tags from the base XCR-Bench sentences\nand prompt LLMs to identify the term(s) that are\nspecific to Western (specifically US/UK) culture\nand salient for cross-cultural communication. To\nensure a consistent interpretation of the task, we\nprovide the definition of Culture-Specific Items\n(CSIs as defined by Newmark (1988)) within the\nprompt during inference.\n3.2\nCSI Prediction\nFor the CSI Prediction task, we replace the CSI\nspans in the base XCR-Bench sentences with a\n[MASK] token enclosed within <CSI> tags, and\nprompt models to predict culturally appropriate\nterms that reflect Western (US/UK) cultural usage.\nAs in the CSI Identification task, we include a brief\ndefinition of CSIs in the prompt to ensure a consis-\ntent interpretation of the task.\n3.3\nCSI Adaptation\nIn the CSI Adaptation task, we present LLMs with\nthe base XCR-Bench sentences containing explic-\nitly marked <CSI> spans and prompt them to\nadapt these CSIs to Arabic, Bengali, or Chinese\ncultural contexts. For each instance, models are re-\nquired to produce both intra-lingual (English) and\ninter-lingual (target-language) adaptations. In ad-\ndition, we ask models to explicitly indicate which\nof Newmark\u2019s twelve cultural adaptation strategies\n(e.g., transference, equivalence, etc. full details in\nAppendix C.1) they employ when adapting each\nCSI.\nTogether, these three tasks operationalize differ-\nent levels of cultural reasoning in LLMs, ranging\nfrom interpretive identification and contextual in-\nference to cross-cultural abstraction and decision-\nmaking. Our prompts used for each task are pre-\nsented in Appendix H.\n3.4\nEvaluation Metrics\nWe evaluate model performance on the two tasks:\nCSI Identification and Prediction, using a combina-\ntion of strict (\u201chard\u201d) and lenient (\u201csoft\u201d) metrics\ndesigned to capture different aspects of correctness.\nThe hard metrics penalize any deviation from the\nground truth, while the soft metrics provide graded\ncredit for semantically or superficially similar out-\nputs, which is crucial for culturally nuanced tasks.\nIn Cultural Adaptation task, direct string match-\ning is insufficient, as valid adaptations may use\ncontextually congruent alternatives rather than lit-\neral match. We therefore evaluate at two comple-\nmentary levels using BERTScore: CSIbert assesses\nthe semantic quality of individual adapted terms\n(within <CSI> tags), while SENTbert measures\noverall meaning preservation of the entire adapted\nsentence. A summary of all metrics is provided in\nTable 1 with detailed definitions and computation\nprovided in Appendix F.\n3.5\nModels and Prompting Technique\nWe evaluate a diverse set of recent multilingual\nLLMs, including both proprietary and open-source\nmodels:\nGPT-4o, Qwen-2.5-Max, Gemini-2.0-\nFlash, Claude-3.7-Sonnet, DeepSeek-R1 (671B),\nLLaMA-3.3 (70B), OLMo-2-32B, and DeepSeek-\nR1-Distill-Qwen-32B. All models are evaluated\nacross the full suite of tasks. For prompting, we\nadopt part of the Expert Prompting strategy pro-\nposed by Xu et al. (2023), which has been shown to\nelicit higher-quality responses than vanilla prompt-\ning.\n4\nResults & Analysis\nThe CSI Identification and Prediction tasks are\nboth designed to assess LLMs\u2019 reasoning over\nWestern cultural knowledge and their ability to dis-\ntinguish culture-specific elements. We therefore\nanalyze and compare the results of these two tasks\ntogether.\n4.1\nCSI Identification & Prediction\nLLMs perform substantially better on CSI Pre-\ndiction than on CSI Identification. Table 2 re-\nports results for both tasks under hard and soft\nevaluation metrics. Overall, most models achieve\nrelatively strong performance on the CSI Predic-\ntion task, while exhibiting notably weaker perfor-\nmance on CSI Identification. We hypothesize that\nthis disparity arises because CSI Prediction closely\n5\nTask\nMetric\nDescription\nFormula/Details\nCSI Identification\nHard Identification of CSI (HI-CSI)\nStrict matching using exact string equal-\nity.\nsimexact(g, m) = I[g = m], where\ng and m denote gold and model-\npredicted CSI spans.\nSoft Identification of CSI (SI-CSI)\nAccounts for minor variations via nor-\nmalized Levenshtein similarity, with\noptimal alignment using the Hungarian\nalgorithm.\ns(g, m) = 1 \u2212\ndLev(g,m)\nmax(|g|,|m|); sentence-\nlevel soft F1 based on alignment.\nCSI Prediction\nHard Prediction of CSI (HP-CSI)\nCase-insensitive exact match between\ngold and predicted spans. Span refers\nto text inside the enclosed <CSI> and\n</CSI> tags.\nsimexact(g, p)\n=\nI[lower(g)\n=\nlower(p)], where g and p denote gold\nand predicted spans.\nSoft Prediction of CSI (SP-CSI)\nCosine similarity between Sentence-\nBERT\nembeddings\nfor\npara-\nphrases/synonyms.\nsimsem(g, p) = cos\n(\ne(g), e(p)\n)\n, using\nall-mpnet-base-v2 (Song et al., 2020).\nCSI Adaptation\nCSI-Level Semantic Similarity (CSIbert)\nCSI-level BERTScore F1 for aligned\nCSI spans.\nCSIbert = 1\nn\n\u2211n\ni=1 F1bert(gi, oi), where\nG = {gi} and O = {oi} are gold\nand predicted CSI spans extracted from\n<CSI> tags, respectively.\nSentence-Level Semantic Similarity (SENTbert)\nAssesses\nsentence-level\nmeaning\npreservation using BERTScore F1.\nSENTbert\n=\nF1bert\n(\nsout, sgt)\n, where\nsout and sgt are model-generated and\nground-truth sentences.\nTable 1: Evaluation metrics for the three cross-cultural reasoning tasks: CSI Identification, Prediction, and Adap-\ntation. Hard metrics enforce strict correctness, while soft metrics provide graded credit for semantically or superfi-\ncially similar outputs.\nCultural\nReference\nCultural\nStereotype\nCultural\nValue\nSocial\nEtiquette\nSocial\nTradition\nSports\nTradition\nWorkplace\nCulture\nGPT-4o\nQwen-2.5-Max\nGemini-2.0-Flash\nClaude-3.7-Sonnet\nDeepseek-R1\nLlama-3.3\nOLMo-2-32B\nDeepSeek-R1-\nDistill-Qwen-32B\n56.9\n76.9\n55.9\n42.6\n60.9\n67.6\n64.6\n57.1\n75.7\n56.9\n48.6\n63.1\n67.9\n64.3\n59.5\n84.0\n54.9\n38.3\n61.2\n66.9\n59.7\n59.7\n80.9\n58.6\n43.8\n64.1\n70.5\n67.8\n60.0\n75.8\n59.7\n45.9\n63.7\n62.1\n65.9\n51.5\n63.4\n46.2\n39.2\n49.3\n58.9\n50.1\n54.5\n76.5\n61.5\n40.5\n62.1\n61.8\n68.6\n52.1\n70.2\n49.0\n46.2\n58.1\n56.9\n60.1\n40\n50\n60\n70\n80\nScore\n(a) CSI Identification (SI-CSI)\nCultural\nReference\nCultural\nStereotype\nCultural\nValue\nSocial\nEtiquette\nSocial\nTradition\nSports\nTradition\nWorkplace\nCulture\nGPT-4o\nQwen-2.5-Max\nGemini-2.0-Flash\nClaude-3.7-Sonnet\nDeepseek-R1\nLlama-3.3\nOLMo-2-32B\nDeepSeek-R1-\nDistill-Qwen-32B\n82.5\n78.0\n91.4\n74.4\n81.1\n61.3\n75.0\n82.2\n80.7\n76.9\n79.8\n80.9\n62.5\n74.2\n79.8\n76.1\n84.5\n77.0\n82.5\n69.3\n74.8\n85.2\n78.3\n86.4\n81.6\n85.5\n66.4\n72.5\n84.2\n81.4\n89.8\n79.2\n84.7\n68.9\n76.6\n79.7\n78.9\n75.5\n59.0\n73.0\n57.9\n63.7\n75.3\n66.8\n72.5\n60.0\n74.3\n43.6\n63.3\n71.7\n62.2\n64.9\n68.9\n75.7\n56.7\n59.7\n50\n60\n70\n80\n90\nScore\n(b) CSI Prediction (SP-CSI)\nFigure 2: Performance of LLMs across CSI categories for CSI Identification and Prediction tasks. For clarity, only\nsoft evaluation metrics are visualized. The results reveal consistent difficulties in CSI Identification across CSI\ncategories.\nresembles a mask-filling task.\nSince LLMs are\ntrained in an autoregressive manner and have likely\nencountered many Western CSIs during training,\nthey can often infer plausible candidates for the\nmasked span. In contrast, CSI Identification re-\nquires explicit reasoning about which elements\nin a sentence are culturally specific and salient\nfor cross-cultural communication, a substantially\nmore demanding capability on which all models\nstruggle. To test this hypothesis, we run an abla-\ntion on CSI Identification using the non-reasoning\ncounterpart of DeepSeek-R1.\nSpecifically, we\nevaluate DeepSeek-V3.2 in its non-thinking mode\non the CSI Identification task.\nIt attains only\n52.29% under the hard metric and 59.94% under\nthe soft metric- both markedly below the reasoning-\nenabled DeepSeek-R1. This controlled compari-\nson further supports our claim that CSI Identifica-\ntion requires explicit cultural reasoning and judg-\nment.\nModel\nCSI Identification\nCSI Prediction\nHI-CSI\nSI-CSI\nHP-CSI\nSP-CSI\nGPT-4o\n50.94\n57.74\n43.70\n78.47\nQwen-2.5-Max\n54.99\n59.75\n43.70\n79.30\nGemini-2.0-Flash\n48.73\n57.15\n42.94\n78.96\nClaude-3.7-Sonnet\n47.95\n60.47\n46.76\n81.76\nDeepseek-R1\n59.05\n60.01\n47.33\n81.68\nLlama-3.3\n52.62\n48.76\n34.54\n69.90\nOLMo-2-32B\n44.91\n57.71\n31.87\n68.30\nDeepSeek-R1-Distill-Qwen-32B\n51.34\n55.14\n29.96\n69.49\nTable 2: CSI Identification and Prediction performance\nacross models. Highest and second-highest scores are\nbolded and underlined, respectively. Values are percent-\nages; higher is better.\nAmong the evaluated models,\nClaude-3.7-\n6\nArabic\nChinese\nBengali (West Bengal)\nBengali (Bangladesh)\nModel\nIntra-lingual\nInter-lingual\nIntra-lingual\nInter-lingual\nIntra-lingual\nInter-lingual\nIntra-lingual\nInter-lingual\nSENTbert\nCSIbert\nSENTbert\nCSIbert\nSENTbert\nCSIbert\nSENTbert\nCSIbert\nSENTbert\nCSIbert\nSENTbert\nCSIbert\nSENTbert\nCSIbert\nSENTbert\nCSIbert\nDeepseek R1\n69.02\n28.75\n81.09\n65.86\n72.46\n33.93\n83.47\n71.34\n84.40\n37.23\n74.40\n48.44\n83.54\n35.40\n74.13\n47.41\nGemini-2.0-Flash\n68.91\n27.78\n80.78\n67.09\n70.65\n32.74\n83.08\n70.33\n76.84\n26.44\n70.30\n44.20\n76.33\n25.94\n70.14\n43.13\nGPT-4o\n72.76\n35.21\n81.08\n67.94\n74.66\n38.85\n83.81\n72.42\n81.69\n35.46\n70.49\n45.06\n80.56\n32.42\n70.16\n44.14\nLlama-3.3\n68.95\n28.93\n80.90\n66.25\n72.74\n34.60\n83.59\n72.10\n84.32\n37.07\n74.40\n48.27\n83.52\n35.17\n74.12\n47.19\nQwen-2.5-Max\n70.54\n33.21\n80.51\n67.66\n67.44\n34.67\n83.75\n72.74\n64.68\n25.42\n70.60\n46.59\n64.02\n23.26\n70.42\n44.54\nOLMo-2-32B\n69.01\n28.96\n81.09\n66.26\n72.71\n34.85\n83.56\n72.03\n76.11\n23.61\n70.80\n43.32\n75.27\n21.93\n70.61\n42.02\nClaude-3.7-Sonnet\n72.65\n33.47\n81.64\n68.69\n73.88\n36.88\n83.91\n72.07\n82.49\n34.10\n73.01\n46.01\n80.92\n30.67\n72.24\n42.86\nD-R1-D-Qwen-32B\n68.85\n23.96\n75.96\n59.49\n74.15\n34.48\n83.42\n69.73\n78.16\n28.25\n62.27\n36.54\n77.10\n26.13\n62.14\n35.46\nAverage\n70.08\n30.03\n80.38\n66.16\n72.34\n35.12\n83.58\n71.59\n78.59\n30.95\n70.78\n44.80\n77.66\n28.87\n70.50\n43.34\nTable 3: CSI adaptation performance (%) across models measured using CSIbert and SENTbert metrics. The CSIbert\nmetric is emphasized, as CSI adaptation typically involves localized substitutions that leave the overall sentence\nstructure largely unchanged, resulting in uniformly higher SENTbert scores. Best results per column are shown in\nbold, and second-best results are underlined.\nSonnet and DeepSeek-R1 consistently outperform\nothers across both hard and soft metrics in Table 2,\nindicating comparatively stronger cultural reason-\ning abilities in identifying and predicting Western\nCSIs.\nAll models show consistent performance\nacross CSI categories, excelling in some but\nstruggling in others, revealing systematic vul-\nnerabilities in specific cultural facets. Figure 2\nillustrates these category-wise trends across LLMs.\nIn the CSI Identification task (Figure 2a), models\nconsistently struggle with categories such as Social\nEtiquette and Cultural Reference, while achiev-\ning strong performance in Cultural Stereotype. In\ncontrast, the CSI Prediction task (Figure 2b) re-\nveals a reciprocal pattern: LLMs perform notably\nwell when predicting CSIs from Cultural Refer-\nence, but show weaker performance in categories\nsuch as Sports Tradition.\nThese results suggest\nthat LLMs\u2019 cultural reasoning abilities are highly\ncategory-dependent, with strengths in certain cul-\ntural dimensions not generalizing to others.\nLLM performance systematically declines\nfrom Visible to Invisible levels of Hall\u2019s cultural\ntriad for both CSI Identification and Prediction,\nexhibited in Figures 5 and 6. To further analyze\nthis behavior, Table 9 reports per-category perfor-\nmance across Hall\u2019s cultural levels. The results\nshow that LLMs exhibit substantially lower perfor-\nmance on semi-visible and invisible elements, in-\ncluding Appropriacy and Orientations. Together,\nthese results indicate that LLMs are more effective\nat handling surface-level cultural cues than deeper:\nvalue and norm-driven aspects of culture.\n4.2\nCultural Adaptation\nThe CSI Adaptation task evaluates the ability of\nLLMs to adapt culture-specific content through\ncross-cultural reasoning in both intra-lingual and\ninter-lingual settings. Table 3 reports the full re-\nsults.\nWe place primary emphasis on the hard\nmetric, as CSI adaptation typically involves local-\nized substitutions that preserve the overall sentence\nstructure, leading to consistently high SENTbert\nscores that are less discriminative in this setting.\nOverall, LLMs achieve higher adaptation perfor-\nmance for Chinese cultural content, followed by\nArabic, while performance is comparatively lower\nfor Bengali across models and CSI categories.\nLLMs consistently exhibit stronger CSI adap-\ntation performance in inter-lingual than in\nintra-lingual settings.\nAcross models, moving\nfrom intra-lingual to inter-lingual adaptation yields\nsubstantial gains in CSIbert metric, with average\nimprovements of approximately 51%\u201355% for Chi-\nnese and Arabic, and 30%\u201335% for Bengali. This\npattern suggests that while LLMs are relatively ef-\nfective at adapting Western cultural content when\noperating directly within the target language, they\nencounter greater difficulty when adapting non-\nWestern cultural concepts expressed in English.\nOne contributing factor may be the handling of\nculture-specific transliterations in intra-lingual set-\ntings, such as ganbei (\u2018cheers\u2019 in Chinese), bisht\n(a traditional Arabic garment), or pahela baishakh\n(the Bengali New Year).\nTo better understand\nwhere these gains arise, we analyze performance\ndifferences at the CSI category level. Figures 7\nand 8 show that inter-lingual adaptation yields the\nlargest improvements in the Cultural Stereotype\ncategory and the smallest gains in Cultural Refer-\nence. This pattern suggests that LLMs more read-\n7\nily adapt stereotypical or conventionalized cultural\nknowledge in the target language, whereas cultur-\nally grounded references tied to specific contexts\nor entities remain more challenging.\nA similar pattern emerges when results are ana-\nlyzed through Hall\u2019s triadic model of culture (Fig-\nures 9 and 10). Across all cultures, inter-lingual\nadaptation consistently outperforms intra-lingual\nadaptation, accompanied by a clear performance\ndecline from Visible to Invisible cultural levels.\nThis decline, however, is less pronounced for the\ntwo Bengali cultural settings (Figure 10).\nAdaptation Procedure.\nDuring CSI adapta-\ntion, we explicitly prompt LLMs to indicate which\nof the twelve adaptation strategies defined in Ap-\npendix C.1 is employed.\nIn practice, models\nfrequently select a combination of two strategies\n(Couplet), suggesting that CSI adaptation often re-\nquires balancing cultural fidelity and interpretabil-\nity. Among single strategies, Cultural Equivalent\n(replacing a CSI with a culturally analogous term\nfrom the target culture) emerges as the most com-\nmonly used approach across models. To assess the\neffectiveness of individual adaptation strategies,\nwe compute CSIbert scores for each unique method\nand aggregate performance across instances. This\nanalysis enables identification of the most and least\neffective adaptation strategies for CSI adaptation,\nas summarized in Table 10.\nRegional Bias in Adaptation. Bengali consti-\ntutes a particularly informative case for evaluating\nregional and religious bias in cultural adaptation.\nDemographically, Bengali speakers are predomi-\nnantly Muslim (approximately 71%) with a sub-\nstantial Hindu minority (approximately 28%), and\nare distributed across two postcolonial national\ncontexts: Bangladesh (approximately 59% of Ben-\ngali speakers) and the Indian state of West Bengal\n(approximately 38%) (Bangladesh Statistics Bu-\nreau, 2022). These two populations differ not only\nin nationality and religion but also in dialectal us-\nage, commonly referred to as Bangal (Bangladesh)\nand Ghoti (West Bengal)-as well as in culturally\nsalient terminology, festivals, food practices, etc.\nTo assess whether such regional and religious\nvariation influences LLM-based cultural adapta-\ntion, we evaluate Bengali CSIs against both West\nBengal and Bangladesh cultural corpora and com-\npare adaptation performance across settings. De-\nspite Bangladesh representing the larger popula-\ntion share and Muslim Bengalis constituting a clear\nmajority overall, some models consistently achieve\nModel\nIntra-lingual\n(SENTBERT / CSIBERT)\nInter-lingual\n(SENTBERT / CSIBERT)\nDeepSeek-R1\n0.003/0.005\n\u22120.0002/ \u22120.003\nGemini-2.0-Flash\n0.002/0.003\n0.001/0.012\u2217\u2217\nGPT-4o\n0.005/0.023\u2217\n\u22120.0002/ \u22120.001\nLlama-3.3\n0.003/0.006\n\u22120.0002/ \u22120.004\nOLMo-2-32B\n0.003/0.005\n\u22120.0002/0.009\nQwen-2.5-Max\n0.001/0.012\n0.002/0.030\u2217\u2217\nClaude-3.7-Sonnet\n0.009/0.020\u2217\n0.004/0.026\u2217\nDeepSeek-R1-Distill-Qwen-32B\n0.005/0.005\n0.000/0."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs\"\n\n#### Main Contributions:\n1. **Introduction of XCR-Bench**: The paper introduces XCR-Bench, a benchmark designed to evaluate cross-cultural reasoning capabilities in Large Language Models (LLMs). It consists of 4900 parallel sentences and 1098 unique Culture-Specific Items (CSIs) across three distinct tasks: CSI Identification, Prediction, and Adaptation.\n\n2. **Corpus Development**:\n   - XCR-Bench integrates Newmark\u2019s CSI framework with Hall\u2019s Triad of Culture to systematically analyze cultural reasoning beyond surface-level artifacts.\n   - The corpus includes 4900 parallel sentences from Western (US/UK) culture, annotated with detailed CSI categories and mappings to Hall's cultural levels.\n\n3. **Methodology**:\n   - XCR-Bench uses a multi-step process for corpus construction: \n     - Extracting CSIs using structured knowledge resources like CANDLE and Cultural Atlas.\n     - Generating realistic sentences incorporating the extracted CSIs.\n     - Annotating CSI categories, mappings to Hall's cultural levels, and evaluating human-generated adaptations.\n\n4. **Evaluation Metrics**:\n   - XCR-Bench employs a combination of hard (strict) and soft (flexible) evaluation metrics for tasks such as CSI Identification and Prediction.\n   - For adaptation tasks like CSI Adaptation, it uses BERTScore-based metrics to evaluate semantic similarity across different levels of cultural reasoning.\n\n5. **Key Findings**:\n   - **LLM Performance**: Models generally perform better in predicting Western cultural facets than identifying CSIs (CSIs are more salient for cross-cultural communication).\n   - **Regional Bias**: LLMs exhibit significant regional biases within a single linguistic setting, particularly when adapting non-Western cultural content.\n   - **Cultural Depth**: LLMs struggle with CSI adaptation at deeper levels of Hall\u2019s Triad, including Appropriateness and Orientations.\n\n6. **Conclusion**:\n   - XCR-Bench provides a comprehensive framework for evaluating cross-cultural reasoning in LLMs, addressing the limitations of existing benchmarks by incorporating diverse cultural contexts.\n   - The findings highlight persistent weaknesses in LLMs\u2019 ability to identify and adapt CSIs related to social etiquette and cultural references, as well as regional biases within single linguistic settings.\n\n### Summary of XCR-Bench Corpus:\n- **Construction Process**: \n  - **CSI Extraction**: Leveraging structured knowledge resources (CANDLE, Cultural Atlas) for CSI extraction.\n  - **Sentence Generation**: Using LLMs to generate realistic sentences incorporating the extracted CSIs.\n  - **Annotation**: Annotating each sentence with CSI categories and mappings to Hall's cultural levels.\n\n### Evaluation Metrics:\n- **CSI Identification & Prediction**:\n  - Hard metrics (exact string matching) vs. Soft metrics (normalized Levenshtein similarity).\n- **CSI Adaptation**:\n  - Evaluates semantic similarity across different levels of culture using BERTScore-based metrics.\n\n### Key Findings from the Analysis:\n- **Performance Variability**: Models generally perform better in predicting than identifying CSIs, indicating a stronger focus on salient cultural elements for communication.\n- **Regional Bias**: LLMs show significant regional biases when adapting non-Western cultural content, particularly within Bengali (West Bengal and Bangladesh) contexts.\n- **Cultural Depth Issues**: LLMs struggle with CSI adaptation at deeper levels of Hall\u2019s Triad, including Appropriateness and Orientations.\n\n### Conclusion:\nXCR-Bench offers a robust framework for evaluating cross-cultural reasoning in LLMs. It highlights the need for models to improve their ability to identify and adapt CSIs related to social etiquette and cultural references, as well as reduce regional biases within single linguistic settings. The release of this resource will facilitate future research on cross-cultural Natural Language Processing (NLP)."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "DAME: DURATION-AWARE MATRYOSHKA EMBEDDING FOR DURATION-ROBUST\nSPEAKER VERIFICATION\nYoungmoon Jung*, Joon-Young Yang*, Ju-ho Kim, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho\nAI Solution Team, Samsung Research, Seoul, South Korea\nABSTRACT\nShort-utterance speaker verification remains challenging due to lim-\nited speaker-discriminative cues in short speech segments. While\nexisting methods focus on enhancing speaker encoders, the em-\nbedding learning strategy still forces a single fixed-dimensional\nrepresentation reused for utterances of any length, leaving capacity\nmisaligned with the information available at different durations.\nWe propose Duration-Aware Matryoshka Embedding (DAME), a\nmodel-agnostic framework that builds a nested hierarchy of sub-\nembeddings aligned to utterance durations:\nlower-dimensional\nrepresentations capture compact speaker traits from short utter-\nances, while higher dimensions encode richer details from longer\nspeech. DAME supports both training from scratch and fine-tuning,\nand serves as a direct alternative to conventional large-margin fine-\ntuning, consistently improving performance across durations. On the\nVoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently\nreduces the equal error rate on 1-s and other short-duration trials,\nwhile maintaining full-length performance with no additional infer-\nence cost. These gains generalize across various speaker encoder\narchitectures under both general training and fine-tuning setups.\nIndex Terms\u2014 Short-duration speaker verification, multi-scale\naggregation, matryoshka representation learning\n1. INTRODUCTION\nSpeaker verification (SV) aims to validate a claimed identity based\non the speaker\u2019s voice. Although recent advances in speaker en-\ncoders and training strategies have substantially improved perfor-\nmance on full-length utterances [1\u201311], short speech segments still\npose a major challenge, due to limited phonetic and acoustic vari-\nability. To address this, prior studies have proposed robust mod-\neling strategies under duration constraints.\nAmong them, multi-\nscale feature aggregation (MFA) combines outputs from different\nencoder layers to build more generalizable frame-level representa-\ntions [3\u20135,8,9]. While effective, MFA strictly operates at the feature\nlevel and still trains a fixed-dimensional embedding, which may not\nadapt to the varying amounts of speaker information available from\nutterances of different lengths. Notably, prior studies [12, 13] have\nshown that lower-dimensional embeddings are effective for short-\nutterance SV, while higher-dimensional embeddings better capture\nlong-duration speaker traits, implying the inefficiency of fixed-size\nembedding learning against variable-length inputs.\nMotivated by these observations, we introduce Duration-Aware\nMatryoshka Embedding (DAME), a model-agnostic framework that\nshifts the focus from feature aggregation to embedding extraction.\nTo address the short-duration SV problem, DAME constructs a\nnested hierarchy of speaker embeddings aligned to utterance dura-\ntions. During training, input utterances are truncated to different\n*Equal contribution.\nlengths and then processed through a shared speaker encoder. Sub-\nsequently, the sub-dimensions of the embeddings are trained such\nthat lower- and higher-dimensional sub-embeddings are supervised\nto discriminate speakers using short- and longer-duration utterances,\nrespectively.\nBy aligning the embedding sub-dimensions in pro-\nportion to the information available in the input utterances, this\nduration-aware supervision strategy encourages lower dimensions\nto capture compact speaker traits in short utterances while higher\ndimensions encode richer speaker information in longer utterances.\nBecause the proposed DAME framework is model-agnostic,\nit can also be applied to fine-tune pretrained speaker encoders\nwith various architectures. Conventional large-margin fine-tuning\n(LMFT) [14] is known to bias optimization toward long utterances\nand often degrades SV performance on short utterances [6, 7]. Re-\ncent remedies like adaptive LMFT (ALMFT) [6] and the ECAPA2\n[7] training strategy mitigate this issue through margin scheduling\nand variable-duration training. Building on these advances, DAME\noffers a more direct solution at the embedding level. It replaces the\nsingle fine-tuning objective with multiple duration-supervised prefix\nlosses on the nested embeddings, coupled with a duration-aligned,\nprefix-specific margin scheduling and variable-length mini-batches.\nIn our experiments, consistent gains are observed across three pop-\nular speaker encoders, ResNet34 [15], ECAPA-TDNN [16], and\nERes2NetV2 [9], demonstrating the generality and effectiveness of\nour method.\n2. RELATED WORK\nMatryoshka Representation Learning. Matryoshka Representa-\ntion Learning (MRL) [17] was initially proposed in NLP to produce\na high-dimensional embedding whose leading prefixes (e.g., first 32,\n64, and 128 dimensions) could serve as sub-embeddings, enabling\ndynamic adjustment of embedding size without extra inference cost.\nIn SV, M-Vec [10] preserved SV performance with compact prefixes,\nand DSM-GS [11] improved training via dimension-aware margins\nand gradient scaling, yet these works mainly target efficiency rather\nthan duration robustness. We instead repurpose MRL for duration-\naware learning. Motivated by evidence that short utterances prefer\ncompact embeddings while long utterances benefit from higher di-\nmensions [12, 13], DAME aligns each duration scale to a matched-\ncapacity sub-embedding.\nMulti-scale Feature Aggregation. MFA has been widely adopted to\nimprove short-utterance SV robustness at the feature level. FPM [3],\nERes2NetV2 [9], and FA-ExU-Net [8] enhance encoder designs to\nintegrate features across different temporal scales. However, these\nmethods still train a fixed-dimensional embedding regardless of the\ninput utterance length. In contrast, the proposed DAME approach\nis independent of the speaker encoder architecture and instead re-\nstructures the embedding such that the sub-dimensions are aligned\narXiv:2601.13999v1  [eess.AS]  20 Jan 2026\n\ud835\udc621\n(\ud835\udc56)\nSpeaker Encoder \ud835\udc53(\u2219)\n\ud835\udc671\n(\ud835\udc56)\n\ud835\udc671,1\n(\ud835\udc56)\n\u21121,1\n(\ud835\udc56)\n\u21122,2\n(\ud835\udc56)\n\u21123,3\n(\ud835\udc56)\n\ud835\udc622\n(\ud835\udc56)\n\ud835\udc623\n(\ud835\udc56)\n\ud835\udc672,2\n(\ud835\udc56)\n\ud835\udc673,3\n(\ud835\udc56)\n\ud835\udc672\n(\ud835\udc56)\n\ud835\udc673\n(\ud835\udc56)\n\u21131\n\u21132\n\u21133\nFig. 1.\nDAME overview (HW, J=K=3):\none-to-one dura-\ntion\u2013prefix alignment; each chunk supervises its matched sub-\nembedding via a large-margin loss.\nto the amount of speaker information available from the input utter-\nance. This makes the proposed DAME framework orthogonal and\ncomplementary to the conventional MFA methods.\nLarge-Margin Fine-Tuning. Conventionally, the LMFT [14] strat-\negy has been widely used to boost SV performance, but recent stud-\nies such as ALMFT [6] and ECAPA2 [7] reported that LMFT can de-\ngrade short-utterance SV performance. This was because the LMFT\nprocess focused on the optimization of model parameters toward\nlong utterances with a larger margin. To address this problem, a\nvariable-duration LMFT method was presented in [6,7], which com-\nposed training mini-batches with utterances of variable durations\nfrom shorter to longer ones. Specifically, a duration-based ALMFT\n(D-ALMFT) strategy was proposed in [6], which adaptively intro-\nduced smaller and larger margins for shorter and longer utterances,\nrespectively.\n3. PROPOSED METHOD\n3.1. Overview and Notation\nDuring training, let y(i) denote the speaker label for the i-th train-\ning instance, and let T = {\u21131, . . . , \u2113J} be the set of target chunk\ndurations with \u21131 < \u00b7 \u00b7 \u00b7 < \u2113J. For each instance i, we construct J\nspeech chunks {u(i)\nj }J\nj=1 by truncating or cropping different utter-\nances of the same speaker to durations \u2113j \u2208T . Each chunk is passed\nthrough a shared encoder f(\u00b7) to produce a full speaker embedding\nz(i)\nj\n= f(u(i)\nj ) \u2208RD, where D is the full embedding dimen-\nsion. Given a set of prefix (nesting) dimensions D = {d1, . . . , dK}\nwith d1 < \u00b7 \u00b7 \u00b7 < dK and dK = D, we define the prefix (sub-\n)embeddings z(i)\nj,k \u2208Rdk as the leading dk components of z(i)\nj .\n3.2. DAME Loss\nIn the proposed DAME framework, each prefix embedding z(i)\nj,k is\nsupervised by a large-margin classifier (SphereFace2 [18]). The per-\nprefix loss is defined as\nL(i)\nj,k = Lcls\n\u0000z(i)\nj,k, y(i); s, mdk\n\u0001\n.\n(1)\nHere, s = 30 is the fixed logit scale, and mdk is the angular margin\nfor the prefix dimension dk. The multi-prefix loss aggregates the\nper-prefix losses using duration\u2013dimension alignment weights cj,k:\nL(i)\nj,D =\nK\nX\nk=1\ncj,k L(i)\nj,k.\n(2)\nFinally, the DAME loss aggregates multi-prefix losses for all\ncropped chunks of all utterances in a mini-batch:\nLT ,D = 1\nI\nI\nX\ni=1\n\u03b1 L(i)\nJ,D + 1 \u2212\u03b1\nJ \u22121\nJ\u22121\nX\nj=1\nL(i)\nj,D\n!\n,\n(3)\nwhere I is the mini-batch size, and \u03b1 is a weighting factor be-\ntween the longest chunk (j\n=\nJ) and the remaining chunks\n(j=1, . . . , J\u22121).\nAs explained briefly in Section 1, the DAME approach aims to\nalign lower- and higher-dimensional sub-embeddings with shorter\nand longer chunks, respectively.\nTo achieve this, the duration\u2013\ndimension alignment weights, cj,k, in Eq. (2) are defined as follows:\ncj,k =\n(\n1,\nbj\u22121 < k \u2264bj,\n\u03b3k,\notherwise.\n, bj =\nj\nj \u00b7 K\nJ\nk\n, b0 = 0,\n(4)\nwhere j \u2208{1, . . . , J} and k \u2208{1, . . . , K}. This definition yields\na disjoint partition of {1, . . . , K} with no overlaps or gaps. Using\nEq. (4), cj,k=1 for prefixes in the j-th band (bj\u22121, bj] (i.e., those\nnear the scaled diagonal k\u2248jK\nJ ), and cj,k=\u03b3k otherwise.\nTwo weighting schemes are considered in Eq. (4).\nIn the\nhard weighting (HW) case, we set J=K and \u03b3k=0, enforcing a\nstrict one-to-one correspondence between the cropped durations\n{\u21131, . . . , \u2113J} and the prefix dimensions {d1, . . . , dK}, such that\neach \u2113j supervises only dj. In the soft weighting (SW) case, we\nset J<K and \u03b3k=2\u2212(K\u2212k+1), so that 0<\u03b3k\u22641\n2 and \u03b3k increases\nmonotonically with k. This setup keeps the near-diagonal band at 1,\nwhile softly down-weighting entries outside this band as the prefix\ndimension dk decreases, thus promoting gradual cross-dimension\ntransfer. With these weighting schemes, the proposed DAME strat-\negy (Eqs. (1)\u2013(4)) promotes a hierarchical supervision between the\npredefined duration and dimension sets, thereby encouraging lower\nprefixes to learn compact cues from short utterances and higher pre-\nfixes to encode richer information from longer utterances. Figure 1\nillustrates the HW case with J=K=3.\n3.3. Training Regime\nThe proposed DAME framework can be applied either to train a\nspeaker encoder from scratch or to fine-tune a pretrained model. We\nrefer to these two schemes as DAME general training (DAME-GT)\nand DAME fine-tuning (DAME-FT), respectively. For DAME-GT,\nfollowing the original MRL formulation [17], each prefix dimen-\nsion is assigned a separate classification head (i.e., weight matrix)\nWk \u2208Rdk\u00d7C, where the column size C denotes the number of\ntraining speakers. Because the weights are not shared across the\nprefix dimensions, the training allows each sub-embedding to con-\ntribute better to the objective assigned to it. For DAME-FT, we adopt\nthe efficient MRL-E weight-tying strategy proposed in [17]. We first\ntrain a speaker encoder using the standard approach with a single\nclassification head W \u2208RD\u00d7C. Then, the classification head of a\npretrained speaker encoder is shared among the different prefix di-\nmensions by simply taking the leading dk rows of the weight matrix.\nThis weight-tying scheme improves training efficiency as it avoids\ntraining new heads for the prefix dimensions from scratch.\nTable 1. DAME configuration for each encoder architecture under general training and fine-tuning setups.\nWeighting Scheme\nEncoder Architecture\nNesting Dimension Set (D)\nDuration Set (T )\n{mdk }init\n{mdk }final\nSW\nResNet34\n{32, 64, 128, 256}\n{1.0, 2.0}\n{0.0, 0.0, 0.0, 0.0}\n{0.0, 0.1, 0.2, 0.2}\nECAPA-TDNN\n{24, 48, 96, 192}\n{1.0, 2.0}\n{0.0, 0.0, 0.0, 0.0}\n{0.0, 0.0, 0.1, 0.2}\nERes2NetV2\n{24, 48, 96, 192}\n{1.0, 2.0}\n{0.0, 0.0, 0.0, 0.0}\n{0.0, 0.0, 0.1, 0.2}\nHW\nResNet34\n{64, 128, 256}\n{1.0, 2.0, 6.0}\n{0.0, 0.2, 0.5}\n{0.0, 0.2, 0.5}\nECAPA-TDNN\n{48, 96, 192}\n{1.0, 2.0, 6.0}\n{0.0, 0.2, 0.5}\n{0.0, 0.2, 0.5}\nERes2NetV2\n{48, 96, 192}\n{1.0, 2.0, 3.0}\n{0.0, 0.2, 0.4}\n{0.0, 0.2, 0.4}\n4. EXPERIMENTAL SETUP\n4.1. Model Training\nAll experiments used the WeSpeaker toolkit [19] on a single\nNVIDIA H100 GPU. We evaluated three encoders in their origi-\nnal configurations: ResNet34 (D=256, 6.4M) [15], ECAPA-TDNN\n(D=192, 15.2M) [16], and ERes2NetV2 (D=192, 34.8M) [9].\nBaselines followed WeSpeaker recipes with 2 s crops, SphereFace2\nloss [18] with a final margin of 0.2, SGD with an initial learning rate\n(LR) of 0.01, and the Triangular2 scheduler [20].\nFor the proposed DAME approach, we adopted the HW and SW\nschemes described in Sec. 3.2. Table 1 lists the nesting dimensions\nD, the duration sets T , and the per-prefix angular margins for each\nprefix dimension dk, reported as initial and final values {mdk}init\nand {mdk}final. These hyperparameters were determined by the fol-\nlowing principles: (i) for HW, we reuse the conventional LMFT-style\ndurations and margins [6,7]; (ii) for SW, we extend the standard 2-s\ncrop with an additional 1-s crop and assign a smaller angular margin\nto its corresponding low-dimensional prefixes (e.g., m=0.1 for 1-s),\nreflecting that shorter segments contain less speaker evidence and\nthus benefit from a softer constraint. Under the alignment weights\nin Eq. (4), this margin design couples duration and prefix dimen-\nsion through the loss weighting: although mdk is dimension-specific\n(per-prefix), the losses for a chunk of duration \u2113j are weighted to-\nward its matched prefixes, yielding duration-aligned effective mar-\ngins (smaller for shorter crops, larger for longer crops). The heuristic\nof assigning smaller margins to lower-dimensional prefixes is moti-\nvated by dimension-aware margin scheduling in prior work [11]. In\nall cases, we choose (D, T ) to fit the GPU memory budget and keep\nthe number of distinct settings minimal.\nFor DAME-GT, models were trained from scratch with Eq. (3);\n\u03b1 was initially set to 1.0 and then linearly decayed to 0.5 by epoch\n50, and margins for each prefix dk were warmed up exponentially\nfrom 0.0 to mdk during epochs 30\u201340. For DAME-FT, pretrained\nbaselines were fine-tuned for 30 epochs using the DAME loss in\nEq. (3) with fixed \u03b1=0.5 and fixed final margins mdk (Table 1); the\nLR decayed exponentially from 10\u22124 to 10\u22125.\nAll models were trained on VoxCeleb2-dev [21] at 16 kHz.\nWe applied on-the-fly data augmentation using MUSAN [22] and\nsimulated room impulse responses and extracted 80-dim log Mel-\nfilterbanks (25 ms window, 10 ms shift) with utterance-level mean\nsubtraction. Mini-batches contained 128 speakers for ResNet34 and\nECAPA-TDNN, and 64 for ERes2NetV2. For DAME training, the\nJ chunks (j=1, . . . , J) were drawn from different utterances of the\nsame speaker.\n4.2. Evaluation Protocols\nEvaluation was conducted on four test subsets:\nVoxCeleb1-O,\nVoxCeleb1-E, VoxCeleb1-H [1], and VOiCES [23] corpus.\nWe\nfollowed the protocol in [3] and considered multiple trial conditions\nTable 2. General training results on VoxCeleb1 (EER, %).\nEncoder\nMethod\nVoxCeleb1-O\nVoxCeleb1-E\nVoxCeleb1-H\nf\u2013f\ns\u2013avg 5s\u20131s\nf\u2013f\ns\u2013avg 5s\u20131s\nf\u2013f\ns\u2013avg 5s\u20131s\nResNet34\nBaseline\n1.02\n2.24\n4.33\n1.12\n2.27\n4.25\n2.03\n3.77\n6.57\n+VLT\n1.02\n1.89\n3.35\n1.11\n1.98\n3.41\n2.11\n3.48\n5.62\nMRL\n0.99\n2.32\n4.39\n1.14\n2.29\n4.26\n2.11\n3.87\n6.65\nDAME-GT-SW 0.92\n1.82\n3.29\n1.16\n1.93\n3.40\n2.11\n3.46\n5.59\nDAME-GT-HW 0.98\n2.11\n3.89\n1.15\n2.19\n3.92\n2.10\n3.71\n6.13\nECAPA-TDNN\nBaseline\n1.08\n2.54\n4.84\n1.13\n2.44\n4.68\n2.00\n3.93\n6.95\n+VLT\n1.00\n2.15\n3.97\n1.18\n2.20\n3.92\n2.19\n3.83\n6.34\nMRL\n1.03\n2.41\n4.69\n1.19\n2.46\n4.66\n2.20\n4.16\n7.26\nDAME-GT-SW 1.08\n2.06\n3.66\n1.19\n2.13\n3.69\n2.16\n3.80\n6.19\nDAME-GT-HW 1.09\n2.29\n4.21\n1.15\n2.27\n4.18\n2.08\n3.88\n6.72\nERes2NetV2\nBaseline\n0.91\n2.04\n3.89\n1.15\n2.26\n4.11\n2.09\n3.81\n6.40\n+VLT\n0.93\n1.91\n3.38\n1.11\n1.98\n3.35\n2.09\n3.48\n5.53\nMRL\n0.89\n1.97\n3.77\n1.00\n2.01\n3.70\n1.87\n3.45\n5.91\nDAME-GT-SW 0.91\n1.82\n3.26\n1.06\n1.94\n3.32\n2.04\n3.44\n5.59\nDAME-GT-HW 0.93\n1.90\n3.55\n1.04\n2.05\n3.44\n1.98\n3.55\n5.81\nTable 3. Fine-tuning results on VoxCeleb1 (EER, %).\nEncoder\nMethod\nVoxCeleb1-O\nVoxCeleb1-E\nVoxCeleb1-H\nf\u2013f\ns\u2013avg 5s\u20131s\nf\u2013f\ns\u2013avg 5s\u20131s\nf\u2013f\ns\u2013avg 5s\u20131s\nResNet34\nPretrained\n1.02\n2.24\n4.33\n1.12\n2.27\n4.25\n2.03\n3.77\n6.57\nLMFT\n0.95\n2.35\n4.59\n1.01\n2.33\n4.45\n2.01\n4.08\n7.18\nD-ALMFT\n1.00\n2.04\n4.10\n1.18\n2.15\n4.06\n1.85\n3.70\n6.50\nDAME-FT-SW 1.47\n2.35\n3.96\n1.49\n2.36\n3.91\n2.71\n4.25\n6.89\nDAME-FT-HW 0.92\n2.00\n4.19\n0.97\n2.11\n4.03\n1.79\n3.65\n6.45\nECAPA-TDNN\nPretrained\n1.08\n2.54\n4.84\n1.13\n2.44\n4.68\n2.00\n3.93\n6.95\nLMFT\n0.94\n2.80\n5.67\n1.00\n2.70\n5.58\n1.91\n4.31\n8.10\nD-ALMFT\n1.05\n2.28\n4.30\n1.18\n2.31\n4.17\n2.15\n4.01\n6.90\nDAME-FT-SW 1.13\n2.43\n4.51\n1.18\n2.32\n4.27\n2.13\n3.92\n6.78\nDAME-FT-HW 1.00\n2.27\n4.39\n1.14\n2.30\n4.22\n2.10\n4.06\n6.80\nERes2NetV2\nPretrained\n0.91\n2.04\n3.89\n1.15\n2.26\n4.11\n2.09\n3.81\n6.40\nLMFT\n0.84\n2.18\n4.18\n1.03\n2.25\n4.12\n2.04\n3.95\n6.79\nD-ALMFT\n0.90\n2.08\n3.91\n1.14\n2.16\n3.89\n2.02\n3.56\n6.10\nDAME-FT-SW 1.33\n2.34\n4.14\n1.46\n2.36\n4.12\n2.56\n4.17\n7.20\nDAME-FT-HW 0.83\n1.97\n3.78\n1.06\n2.09\n3.77\n1.98\n3.61\n6.07\nwith varied enrollment\u2013test durations: full\u2013full (\u2018f\u2013f\u2019), 5s\u20135s, 5s\u2013\n3s, 5s\u20132s, and 5s\u20131s. This setup was intended to reflect practical\nscenarios in which an utterance of sufficient duration is typically\nrequired for speaker enrollment, but a short phrase is used in test\ntime. All systems\u2014including MRL and DAME variants\u2014use the\nfull embedding at test time (dK=D); thus the inference procedure\nand cost are unchanged. We report equal error rate (EER) and, due\nto space, the macro-average over {5s\u20135s, 5s\u20133s, 5s\u20132s, 5s\u20131s} as\n\u2018s\u2013avg\u2019.\n5. RESULTS\n5.1. Results on VoxCeleb1\nTable 2 compares DAME-GT with baselines on the VoxCeleb1 test\nsets, including a variable-length training (VLT) baseline (identical\nto the baseline but trained with T ={1.0, 2.0}) and a plain MRL\nvariant [10]. While f\u2013f was broadly comparable across methods,\nboth VLT and DAME-GT significantly outperformed the baseline\nand MRL on s\u2013avg and 5s\u20131s, attributable to short-duration su-\nf\u2013f\ns\u2013avg\n5s\u20131s\n\u221210\n0\n10\n20\n30\nRelative EER reduction (%)\nprefix d=48\nprefix d=96\nprefix d=192\nFig. 2. Relative EER reduction (%) vs. ECAPA-TDNN baseline\n(D=192) on VoxCeleb1-O for one DAME-GT-SW model at prefixes\nd \u2208{48, 96, 192}; results are shown for f\u2013f, s\u2013avg, and 5s\u20131s.\npervision.\nAlthough VLT and DAME-GT-SW were competitive,\nthe latter exhibited 4.2%\u20137.8% lower EERs relative to the former\non short-duration trials (s-avg and 5s\u20131s, respectively) with the\nECAPA-TDNN encoder on VoxCeleb1-O. Between SW and HW\nschemes, the DAME-GT-SW consistently outperformed the DAME-\nGT-HW on both s\u2013avg and 5s\u20131s conditions.\nTable 3 summarizes fine-tuning results. We compared LMFT [14]\nand D-ALMFT [6] as conventional fine-tuning baselines: LMFT\nused 6-s utterances with a fixed margin of 0.5, while D-ALMFT re-\nplaced the DAME-FT-HW nesting dimension set with D={D, D, D}.\nLMFT slightly improved the baseline system on the f\u2013f condition but\nexhibited overall EER increases on both s\u2013avg and 5s\u20131s conditions.\nIn contrast, both D-ALMFT and DAME-FT-HW generally improved\nthe baseline on the overall test conditions, with the latter consistently\noutperforming the former with ResNet34 and ERes2NetV2 archi-\ntectures. DAME-FT-SW occasionally achieved the lowest EER on\nthe 5s\u20131s condition, but tended to hurt the f\u2013f performance.\n5.2. Prefix analysis\nFigure 2 shows the relative EER reduction of the DAME-GT-SW\nmodel compared to the ECAPA-TDNN baseline across prefixes\nd \u2208{48, 96, 192}. With the smallest prefix (d = 48), the DAME-\nGT-SW was superior to the baseline on the short-duration trials\n(s\u2013avg +3.5%, 5s\u20131s +9.1%), but was inferior on the f\u2013f condition\n(\u221216.3%). However, as the prefix dimension increased (d = 96 or\nd = 192), the performance gap observed on the f\u2013f condition was\nsignificantly reduced, and the short-duration gains were enlarged.\nThese trends suggest that (i) short-duration cues were concentrated\nin leading dimensions via the DAME\u2019s intended capacity\u2013duration\nalignment, and (ii) higher dimensions were crucial for long utter-\nances, thus supporting the use of a full embedding at test time.\n5.3. Domain robustness on VOiCES\nTable 4 summarizes the results on the VOiCES dataset, which con-\ntains far-field speech recorded in noisy and reverberant room envi-\nronments. Given the severe domain mismatch between training and\ntest, these results assess the systems\u2019 domain robustness. In the gen-\neral training (GT) regime, the VLT generally improved the baseline\non both s\u2013avg and 5s\u20131s conditions, but with substantial EER in-\ncreases on the f\u2013f condition. Similarly, the DAME-GT-SW degraded\nthe baseline on the f\u2013f condition, but with much smaller EER in-\ncreases compared to the VLT with ECAPA-TDNN and ERes2NetV2\nmodels. In contrast, the DAME-GT-SW consistently outperformed\nall other systems on both s\u2013avg and 5s\u20131s conditions across all en-\ncoder architectures.\nIn the fine-tuning (FT) regime, all methods improved the base-\nline on the f\u2013f condition, except for the DAME-FT-SW, which used\nTable 4. VOiCES under domain mismatch (EER %).\nRegime Encoder\nMethod\nf\u2013f\ns\u2013avg 5s\u20131s\nGT\nResNet34\nBaseline\n5.30 11.02 17.50\n+VLT\n5.78 10.59 15.63\nMRL\n5.52 10.87 16.91\nDAME-GT-SW\n5.75 10.55 15.60\nDAME-GT-HW 5.98 11.37 16.31\nECAPA-TDNN\nBaseline\n4.84 11.18 18.53\n+VLT\n5.96 11.25 17.09\nMRL\n5.23 11.40 18.33\nDAME-GT-SW\n5.24 10.79 16.34\nDAME-GT-HW 5.67 11.09 17.33\nERes2NetV2\nBaseline\n5.21 10.99 17.09\n+VLT\n5.76 10.81 16.09\nMRL\n5.02 10.61 16.62\nDAME-GT-SW\n5.31 10.25 15.53\nDAME-GT-HW 5.44 10.53 15.91\nFT\nResNet34\nPretrained\n5.30 11.02 17.50\nLMFT\n4.97 11.49 18.63\nD-ALMFT\n5.02 11.00 17.58\nDAME-FT-SW\n6.11 11.01 16.43\nDAME-FT-HW\n4.21 10.90 17.76\nECAPA-TDNN\nPretrained\n4.84 11.18 18.53\nLMFT\n3.93 13.46 22.67\nD-ALMFT\n4.26 11.16 18.61\nDAME-FT-SW\n5.10 10.82 17.40\nDAME-FT-HW\n4.18 10.61 17.12\nERes2NetV2\nPretrained\n5.21 10.99 17.09\nLMFT\n4.96 11.22 17.88\nD-ALMFT\n5.01 10.85 17.04\nDAME-FT-SW\n5.63 11.39 17.21\nDAME-FT-HW\n4.30 10.61 16.83\nonly short (\u22642 s) utterances during fine-tuning. Similar to that\nobserved in Table 3, the LMFT still degraded the SV performance\non the short-duration trials. In contrast, the DAME-FT-HW gen-\nerally improved the baseline on both s\u2013avg and 5s\u20131s conditions\nacross various encoder architectures, while consistently outperform-\ning the D-ALMFT method (except for the 5s\u20131s condition with the\nResNet34). This suggests that the proposed duration-aligned pre-\nfix dimension supervision strategy improves generalizability over\nvarying training utterance durations under domain mismatch, un-\nlike the conventional D-ALMFT that fits the entire embedding di-\nmensions toward both short- and long-duration utterances during the\nfine-tuning procedure.\nTo summarize, the results on VOiCES demonstrate that DAME\u2019s\nbenefits are not limited to the in-domain VoxCeleb data.\nBoth\nDAME-GT-SW for general training and DAME-FT-HW for fine-\ntuning served as reliable strategies for enhancing short-duration SV\nrobustness under significant domain mismatch, all without altering\nthe speaker encoder architecture or increasing inference cost.\n6. CONCLUSIONS\nWe introduced DAME, a model-agnostic framework that aligns\nembedding capacity with utterance length by training nested prefix\nembeddings with duration-supervised losses. Operating at the em-\nbedding level and orthogonal to MFA, DAME was applied via two\nefficient schemes, soft-weighted general training (DAME-GT-SW)\nand hard-weighted fine-tuning (DAME-FT-HW), without increasing\ninference cost. Experiments across three encoders on VoxCeleb1-\nO/E/H and the mismatched VOiCES dataset consistently showed\nimproved short-duration EER while preserving full-length perfor-\nmance. Notably, DAME-GT-SW surpassed standard MRL variants,\nand DAME-FT-HW served as a superior alternative to LMFT for\nshort-duration trials. Future work includes adaptive test-time prefix\nselection and joint duration\u2013margin scheduling.\n7. REFERENCES\n[1] A. Nagrani, J. S. Chung, and A. Zisserman,\n\u201cVoxceleb: A\nlarge-scale speaker identification dataset,\u201d\nin Proc. Inter-\nspeech, 2017, pp. 2616\u20132620.\n[2] S. M. Kye, Y Jung, H B Lee, S J Hwang, and H Kim, \u201cMeta-\nlearning for short utterance speaker recognition with imbalance\nlength pairs,\u201d in Proc. Interspeech, 2020, pp. 2982\u20132986.\n[3] Y. Jung, S. M. Kye, Y. Choi, M. Jung, and H. Kim, \u201cImprov-\ning multi-scale aggregation using feature pyramid module for\nrobust speaker verification of variable-duration utterances,\u201d in\nProc. Interspeech, 2020, pp. 1501\u20131505.\n[4] S. H. Mun, J. Jung, M. H. Han, and N. S. Kim, \u201cFrequency and\nmulti-scale selective kernel attention for speaker verification,\u201d\nin Proc. IEEE Spoken Language Technology Workshop (SLT),\n2022, pp. 548\u2013554.\n[5] Y. Chen, S. Zheng, H. Wang, L. Cheng, Q. Chen, and J. Qi,\n\u201cAn enhanced res2net with local and global feature fusion for\nspeaker verification,\u201d arXiv preprint arXiv:2305.12838, 2023.\n[6] L. Zhang, Z. Chen, and Y. Qian,\n\u201cAdaptive large margin\nfine-tuning for robust speaker verification,\u201d in Proc. Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2023, pp. 1\u20135.\n[7] J. Thienpondt and K. Demuynck, \u201cECAPA2: A hybrid neural\nnetwork architecture and training strategy for robust speaker\nembeddings,\u201d in Proc. Automatic Speech Recognition and Un-\nderstanding Workshop (ASRU), 2023.\n[8] J. h. Kim, J. Heo, H. s. Shin, C. y. Lim, and H. J. Yu, \u201cFA-\nExU-Net: The simultaneous training of an embedding extrac-\ntor and enhancement model for a speaker verification system\nrobust to short noisy utterances,\u201d IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, vol. 32, pp. 2269\u2013\n2282, 2024.\n[9] Y. Chen, S. Zheng, H. Wang, L. Cheng, Q. Chen, S. Zhang,\nand J. Li, \u201cERes2NetV2: Boosting short-duration speaker ver-\nification performance with computational efficiency,\u201d in Proc.\nInterspeech, 2024, pp. 3245\u20133249.\n[10] S. Wang, P. Zhu, and H. Li,\n\u201cM-Vec:\nMatryoshka\nspeaker embeddings with flexible dimensions,\u201d arXiv preprint\narXiv:2409.15782, 2024.\n[11] S. Park and H. S. Kim,\n\u201cDimension-specific margins\nand element-wise gradient scaling for enhanced matryoshka\nspeaker embedding,\u201d IEEE Access, vol. 13, pp. 45473\u201345487,\n2025.\n[12] A. Kanagasundaram, S. Sridharan, G. Sriram, S. Prachi, and\nC. Fookes, \u201cA study of x-vector based speaker recognition on\nshort utterances,\u201d in Proc. Interspeech, 2019, pp. 2943\u20132947.\n[13] S. Zheng, Y. Lei, and H. Suo, \u201cPhonetically-aware coupled net-\nwork for short duration text-independent speaker verification,\u201d\nin Proc. Interspeech, 2020, pp. 926\u2013930.\n[14] J. Thienpondt, B. Desplanques, and K. Demuynck, \u201cThe ID-\nLab VoxSRC-20 submission: Large margin fine-tuning and\nquality-aware score calibration in DNN based speaker verifica-\ntion,\u201d in Proc. International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2021, pp. 5814\u20135818.\n[15] J. S. Chung, J. Huh, S. Mun, M. Lee, H. S. Heo, S. Choe,\nC. Ham, S. Jung, B-J Lee, and I. Han, \u201cIn defence of metric\nlearning for speaker recognition,\u201d in Proc. Interspeech, 2020,\npp. 2977\u20132981.\n[16] B. Desplanques, J. Thienpondt, and K. Demuynck, \u201cECAPA-\nTDNN: emphasized channel attention, propagation and aggre-\ngation in TDNN based speaker verification,\u201d in Proc. Inter-\nspeech, 2020, pp. 3830\u20133834.\n[17] A. Kusupati et al., \u201cMatryoshka representation learning,\u201d in\nProc. the 36th Conference on Neural Information Processing\nSystems (NeurIPS), 2022.\n[18] B. Han, Z. Chen, and Y. Qian, \u201cExploring binary classification\nloss for speaker verification,\u201d in Proc. IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP),\n2023, pp. 1\u20135.\n[19] H. Wang et al., \u201cWespeaker: A research and production ori-\nented speaker embedding learning toolkit,\u201d in Proc. Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2023, pp. 1\u20135.\n[20] L. N. Smith, \u201cCyclical learning rates for training neural net-\nworks,\u201d in Proc. IEEE WACV, 2017, pp. 464\u2013472.\n[21] J. S. Chung, A. Nagrani, , and A. Zisserman,\n\u201cVoxceleb2:\nDeep speaker recognition,\u201d\nin Proc. Interspeech, 2018, pp.\n1086\u20131090.\n[22] D. Snyder, G. Chen, and D. Povey, \u201cMusan: A music, speech,\nand noise corpus,\u201d arXiv:1510.08484v1, 2015.\n[23] M. K. Nandwana, J. V. Hout, C. Richey, M. McLaren, M. A.\nBarrios, and A. Lawson, \u201cThe VOiCES from a distance chal-\nlenge 2019,\u201d in Proc. Interspeech, 2019, pp. 2438\u20132442."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper\n\n#### Main Contributions\nThis paper introduces DAME (Duration-Aware Matryoshka Embedding), a novel framework designed to address the challenges posed by short utterance speaker verification. The main contributions are:\n1. **Model-Agnostic Approach**: DAME is a model-agnostic framework that aligns embedding capacity with utterance length through duration-supervised losses, enabling it to be applied across various speaker encoder architectures.\n2. **Nested Embedding Hierarchy**: It constructs a nested hierarchy of sub-embeddings aligned to utterance durations, allowing for the capture of both short and long-term speaker traits in different dimensions.\n3. **Efficient Training Methods**: DAME supports training from scratch as well as fine-tuning on pretrained models, offering flexibility without increasing inference cost.\n\n#### Methodology\n**Overview and Notation (Section 3.1)**: The framework is applied to either train a speaker encoder from scratch or fine-tune a pretrained model. Each prefix embedding is supervised by a large-margin classifier, with the per-prefix loss defined in Eq. (1). \n- **DAME Loss (Section 3.2)**: This involves aggregating multi-prefix losses using duration\u2013dimension alignment weights cj,k as described in Eq. (2), and finally aggregating these to form the DAME loss in Eq. (3).\n- **Training Regime (Section 3.3)**: The framework can be applied either for general training or fine-tuning, with different weight schemes considered (Hard Weighting vs Soft Weighting).\n\n#### Key Findings\n**Experimental Setup and Results (Sections 4 & 5)**:\n1. **VoxCeleb1 Evaluation**: \n   - DAME-GT-SW outperformed baselines on VoxCeleb1-O, E, H datasets by reducing the equal error rate (EER) significantly for short-duration trials compared to the ECAPA-TDNN baseline.\n   - The DAME framework consistently improved performance across different utterance lengths and architectures without increasing inference cost.\n\n2. **Fine-tuning Results**:\n   - DAME-FT-HW outperformed LMFT on VoxCeleb1-O, E, H datasets for short-duration trials (5s\u20131s).\n   - The DAME framework showed consistent improvements across various encoder architectures in fine-tuning scenarios.\n   \n3. **Domain Robustness Evaluation**:\n   - On the VOiCES dataset with domain mismatch between training and testing environments, DAME-GT-SW outperformed all other systems on both s\u2013avg and 5s\u20131s conditions.\n\n#### Conclusions\nDAME offers a robust framework for short-duration speaker verification that is model-agnostic, efficient in terms of inference cost, and adaptable across various architectures. The results demonstrate its effectiveness under significant domain mismatch scenarios without altering the speaker encoder architecture or increasing inference time. Future work includes exploring adaptive test-time prefix selection and joint duration\u2013margin scheduling.\n\n### Key Takeaways\n1. **Duration-Aware Learning**: DAME effectively captures short-term and long-term speaker traits through a nested hierarchy of sub-embeddings, aligning with utterance durations.\n2. **Efficiency and Flexibility**: The framework supports both training from scratch and fine-tuning on pretrained models without increasing inference cost.\n3. **Versatility Across Architectures**: DAME can be applied to various architectures, making it a versatile solution for speaker verification tasks.\n\nThis summary encapsulates the main points of the paper's methodology, key findings, and conclusions, providing an overview of its contributions and impact in the field of short-duration speaker verification."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Efficient Coordination with the System-Level Shared State: An\nEmbodied-AI Native Modular Framework\nYixuan Denga,c (First Author), Tongrun Wub (co-First Author), Donghao Wuc,e, Zeyu Weif,\nJiayuan Wangc, Zhenglong Suna,b,c, Yuqing Tangd and Xiaoqiang Jia,b,c (Corresponding Author)\naSchool of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, 2001 Longxiang Boulevard, Shenzhen, China\nbSchool of Artificial Intelligence, The Chinese University of Hong Kong, Shenzhen, 2001 Longxiang Boulevard, Shenzhen, China\ncShenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China\ndInternational Digital Economy Academy, Shenzhen-Hong Kong Collaborative Innovation Center, Shenzhen, China\neSchool of Data Science, The Chinese University of Hong Kong, Shenzhen, 2001 Longxiang Boulevard, Shenzhen, China\nfThe School of Computer Science, The University of Sydney, Sydney, Australia\nA R T I C L E I N F O\nKeywords:\nEmbodied AI\nComputing Architecture\nRobustness\nA B S T R A C T\nAs Embodied AI systems move from research prototypes to real world deployments, they tend to\nevolve rapidly while remaining reliable under workload changes and partial failures. In practice,\nmany deployments are only partially decoupled: middleware moves messages, but shared context\nand feedback semantics are implicit, causing interface drift, cross-module interference, and brittle\nrecovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness\nexplicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract\nfor the standardized shared state, from (ii) a communication bus for many-to-many dissemination\nand feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop\nfeasibility on a de-identified workflow instantiation, characterize latency distributions under varying\npayload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes\nand restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into\nexplicit contracts, enabling controlled degradation under load and self-healing recovery for scalable\ndeployment of closed-loop AI systems.\n1. Introduction\nEmbodied AI has progressed rapidly across autonomous\ndriving, robotics, and aerial autonomy. In driving, unified\nend-to-end frameworks increasingly model the full stack\nfrom perception to planning (e.g., UniAD, CVPR 2023) [1];\nin robotics, learning-based controllers that execute reliably\non real hardware have advanced quickly (e.g., diffusion\npolicy, RSS 2023) [2]; and in aerial autonomy, highly dy-\nnamic real-world systems have reached (and in some settings\nrivaled) top human performance (e.g., the swift drone racing\nsystem, Nature 2023) [3]. This fast-moving landscape places\ngrowing pressure on the system layer: practical deployments\nmust support rapid iteration, where heterogeneous compo-\nnents evolve and recombine, while still providing depend-\nable runtime behavior as task demands expand.\nTo keep pace with iteration pressure, the system layer\nmust provide decoupling as a default engineering property.\nIn practice, embodied deployments rarely evolve as a cleanly\nseparated pipeline; instead, perception, state maintenance,\ndecision making, tool use, and execution control co-evolve\nat different cadences and are repeatedly recomposed under\nchanging tasks and hardware. Decoupling matters because it\nallows components to be replaced independently, constrains\nhow load spikes and faults propagate across modules, and\nreduces integration overhead.\n\u2217Corresponding author\njixiaoqiang@cuhk.edu.cn (X. Ji)\nORCID(s): 0000-0002-8556-3579 (X. Ji)\nHowever, many deployed stacks remain only partially\ndecoupled. Even when publish\u2013subscribe middleware is\nused, interface contracts are often implicit (e.g., undocu-\nmented message semantics, ad-hoc topic conventions, du-\nplicated context), and coordination logic tends to leak across\nmodule boundaries as the system grows. As a result, iteration\nfrequently induces interface drift and hidden coupling, and\nintegration cost can dominate development cycles [4, 5].\nThis motivates treating decoupling not merely as a middle-\nware choice, but as a system-level design commitment.\nThis motivation underlies the long-standing use of mod-\nular middleware and componentized architectures in robotics,\nranging from earlier robot platforms and middleware (e.g.,\nYARP and Orocos) [6, 7] to lightweight robotics messaging\nsystems (e.g., LCM) [8], to modern deployments such as\nROS 2 [4], as well as microservice-based control architec-\ntures that emphasize reuse and interruptible concurrency in\nsafety-critical mobile robots [9]. These systems also reflect\ngeneral publish\u2013subscribe principles that decouple produc-\ners and consumers [5]. We therefore view decoupling-\nenabled modularity as an engineering amplifier: it does not\nprescribe a learning paradigm (end-to-end or otherwise),\nbut it can substantially improve maintainability, evolvability,\nand robustness in deployment.\nBeyond evolvability, real-world physical environments\nimpose a second constraint: robustness under long-horizon\noperation and deployment-time disturbances. In real-world\nphysical environments, embodied agents must operate for\nlong durations under imperfect sensing and actuation, while\nDeng et al.: Preprint submitted to Elsevier\nPage 1 of 12\narXiv:2601.13945v1  [cs.RO]  20 Jan 2026\nEfficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework\ncontinuously interacting with humans, other agents, and\nchanging environments. These settings inevitably introduce\ndistribution shifts, long-tail events, and uncertainty, raising\nthe bar for system-level safety and robustness. Accordingly,\nrecent work increasingly treats safety and robustness as first-\nclass goals, including unified views of runtime assurance\nand safety filters [10], safe motion planning under uncer-\ntainty in traffic [11], safe and platform-aware navigation\nmodeling in complex terrain [12], broader discussions of\nrobustness challenges for autonomous vehicles [13], and\nscalable learning-based safety filters in practice [14]. How-\never, integrating these ideas into a reusable and evolv-\nable systems framework remains an important engineering\nchallenge as embodied AI scales to diverse deployments:\ninterfaces drift as modules evolve, shared context becomes\nfragmented across components, and recovery and degrada-\ntion behaviors are often not made explicit at the system\nlevel [4, 5]. In many deployments, recovery, degradation,\nand fault-containment behaviors are not made explicit at\nthe system layer and instead emerge from ad-hoc glue code\nand local conventions. This implicitness makes robustness\nfragile under scale: when components are tightly coupled\nthrough shared-but-undefined context, anomalies and load\nfluctuations can amplify across the stack [4, 5].\nMotivated by these requirements, we introduce AN-\nCHOR, a modular framework for embodied AI systems that\nprovides stable engineering support amid rapid iteration and\nreal-world deployment. ANCHOR does not assume an end-\nto-end or a staged realization; instead, it offers clear system\nabstractions and interface semantics along the sensing-to-\nexecution chain, enabling key capabilities to be replaced,\nextended, and composed independently. Concretely, AN-\nCHOR organizes the system around two system-level, stable\ninterfaces: (i) Canonical Records, a shared and evolvable\nrepresentation of normalized observations and system con-\ntext, and (ii) Communication Bus that supports many-to-\nmany dissemination, concurrency-aware delivery, and feed-\nback interactions for closed-loop execution [5]. By making\nshared state and inter-component communication explicit\nsystem mechanisms (rather than implicit, ad-hoc glue), AN-\nCHOR aims to provide clearer operational behavior under\nload, partial failures, and restarts, while keeping pace with\nevolving task demands and model capabilities.\nThe main contributions of this work are as follows:\n1. A modular framework viewpoint for embodied de-\nployments. We formulate the systems-layer need for\nan evolvable framework that supports rapid iteration,\nheterogeneous component recomposition, and robust\nruntime operation in real deployments [4, 5].\n2. Canonical Records as an explicit shared-state in-\nterface. We introduce canonical records as a stable\nand evolvable interface for normalized observations\nand shared system context, intended to reduce inter-\nface drift and implicit coupling across components.\n3. A high-concurrency inter-component communica-\ntion bus. We design and implement a high-concurrency\ncommunication mechanism based on multi-producer\nmulti-consumer (MPMC) dissemination and concur-\nrency governance, enabling multiple command sources,\nmultiple executors, and multiple observers, and sup-\nporting feedback interactions for closed-loop execu-\ntion [5].\n4. Evidence of modularity and robustness in a closed-\nloop framework. We provide empirical evidence that\nANCHOR supports modular recomposition via ex-\nplicit contracts, controlled degradation under load,\nand automatic recovery after hard crashes and restarts,\nvalidating the system-level properties targeted by our\ndesign.\nThe rest of this paper is organized as follows. Sec-\ntion 2 reviews related work. Section 3 presents the design\nof ANCHOR and provides an overall system view. Section 4\npresents empirical evaluation results. Section 5 analyzes key\nmechanisms and critical paths. Section 6 discusses limita-\ntions and future directions. Finally, Section 7 concludes the\npaper.\n2. Related Work\nWe review prior work from three perspectives that mo-\ntivate ANCHOR: (i) embodied system architectures and\nmiddleware, (ii) robustness under deployment constraints,\nand (iii) extensibility for rapid iteration. And we distinguish\nwhat is explicitly defined and supported as a system-level\nprimitive from what is left to downstream integration choices\nor external extensions.\n2.1. Embodied AI system architectures\nWhen embodied AI systems transition from research\nprototypes to real deployments, their architectures are rarely\nrebuilt from scratch. Instead, they typically build on mature\nmiddleware and tooling ecosystems, organizing perception,\nplanning, and control as composable components integrated\nthrough message-based communication. This line of work\nprimarily targets engineering concerns such as integration,\nreuse, and coordination, rather than prescribing whether\nhigher-level algorithms must be end-to-end or staged.\nROS provides a widely adopted communications layer\nand tooling ecosystem to connect functional modules across\nheterogeneous compute nodes [15], and ROS 2 re-architects\nthe stack toward scalability and deployment readiness with\ndocumented real applications [4]. Beyond ROS, modern\nembodied architectures exhibit recurring trade-offs: they\npromote modularity and reuse while balancing concurrency,\nreal-time constraints, and production readiness against over-\nall complexity and maintainability. For example, XBot2\nemphasizes multi-threaded, mixed real-time (RT) and non-\nRT execution within a single system, together with hardware\nabstraction and pluggable components for cross-platform\nreuse [16]. In parallel, software-architecture research sys-\ntematizes how architecture-based self-adaptation and recon-\nfiguration address runtime uncertainty and environmental\nchanges [17]. Despite these ecosystems, decoupling is often\nincomplete at the system level. Middleware enables message\nDeng et al.: Preprint submitted to Elsevier\nPage 2 of 12\nEfficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework\nTable 1\nExplicit design coverage of system-level mechanisms. \u2713: explicitly provided as a system-level primitive with defined seman-\ntics/interfaces; \u00d7: not a primary system-level primitive (even if achievable via extensions).\nWork\nMPMC\npub\u2013sub\nFeedback\nprimitives\nQoS /\nbackpressure\nObservability\n& replay\nCanonical\nshared state\nROS/ROS2\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\nBehavior Trees (pattern)\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nMicroservice control architectures\n\u00d7\n\u2713\n\u2713\n\u2713\n\u00d7\nANCHOR (this work)\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nexchange, but higher-level interface contracts\u2014such as the\nsemantics and lifecycle of shared context, and how feedback\nis incorporated across components\u2014are frequently left to\ndownstream integration choices. As systems scale, these im-\nplicit contracts become a common source of hidden coupling\nand unpredictable cross-module interactions.\nRecent real-time systems studies further show that even\nwithin ROS 2-style middleware, communication paths and\nscheduling interactions can materially shape end-to-end tim-\ning behavior [18]. Complementary modeling work high-\nlights that inter-process communication introduces nontriv-\nial delay components that require explicit reasoning [19].\nMoreover, predictability can be sensitive to cross-layer prior-\nity effects when multiple components contend for shared re-\nsources [20]. Executor policies and shared-resource schedul-\ning in multi-threaded ROS 2 can further alter end-to-end be-\nhavior, effectively becoming a hidden coupling point across\notherwise modular components [21]. Together, these obser-\nvations suggest that concurrency governance and delivery\nsemantics should be treated as first-order system concerns\nrather than incidental configuration.\nAs embodied tasks become longer-horizon, more collab-\norative, and faster-evolving, architectural concerns increas-\ningly shift from merely \u201cwiring modules to run\u201d toward\nenabling concurrent coordination among components and\ncontinual sensing\u2013adjustment during execution. Surveys on\nintegrating foundation/large language models into robotics\nsystems similarly organize the landscape around compo-\nnent views (e.g., communication, perception, planning, and\ncontrol) and emphasize filtering/correction mechanisms for\nstable real-world execution [22]. Motivated by this shift\nand without presupposing any learning paradigm, ANCHOR\nadopts a modular-framework perspective: clear component\nboundaries and communication organization serve as the\nsubstrate for concurrent coordination and feedback-driven\ninteraction, supporting rapid iteration and extensible capa-\nbility growth [23].\n2.2. Robustness under deployment constraints\nIn real deployments, embodied systems must not only\ncomplete tasks but also maintain stable and safe execution\nunder uncertainty and dynamic interactions, making robust-\nness and safety first-class requirements. On the algorith-\nmic side, runtime assurance and safety-filtering paradigms\nmonitor and (when necessary) intervene on nominal policies\nduring execution [10]. On the systems and architecture side,\nrecent efforts emphasize engineering mechanisms such as\ninterruptibility, concurrency, and fail-safety, and explore\ndistributed organizational structures to mitigate brittleness\nfrom centralized logic [9].\nMany approaches further introduce explicit and inter-\npretable execution structures\u2014including hierarchical state\nmachines, behavior trees, and crash-only recovery-style\ndesigns\u2014to support online monitoring, rapid interruption,\nand error recovery, while decomposing complex tasks into\nreusable units [24, 25, 26]. At the architecture level, organiz-\ning robot functionality into finer-grained services/components\nand reducing coupling to centralized logic can improve\nconcurrent coordination and contain fault propagation under\nanomalies [9]. Behavior trees are widely used in practice\ndue to their modular conditions/actions and natural support\nfor fallback and retries; recent work also systematizes BT\nproperties and evaluation metrics, highlighting the need for\nconsistent measurements [27].\nThese directions provide important foundations for ro-\nbust execution via execution structure, monitoring, and re-\ncovery. However, in real deployments robustness is often\nconstrained by systems-layer coupling: when many mod-\nules run concurrently, information fans out to multiple con-\nsumers, and feedback must be incorporated online, robust-\nness depends on how coordination is realized across com-\nponents. Without an explicit shared representation of state\nand context, interface drift and implicit coupling can be\namplified by iteration and load fluctuations, increasing fault\npropagation and debugging costs.\nRecent analyses of executor behavior in ROS 2 highlight\nthat runtime scheduling choices can measurably affect end-\nto-end responsiveness in component pipelines [28]. Sepa-\nrately, timing analysis of processing chains with data re-\nfreshing shows that seemingly local design decisions can\npropagate into system-level latency and stability proper-\nties [29]. Together, these observations reinforce a trend to-\nward making coordination, feedback, and fault-containment\nexplicit as system mechanisms rather than emergent behav-\nior from glue code. ANCHOR targets this systems-layer gap\nby aligning (i) a unified yet evolvable shared representa-\ntion (canonical records) and (ii) concurrency-friendly inter-\ncomponent communication and feedback as explicit system\nmechanisms.\nDeng et al.: Preprint submitted to Elsevier\nPage 3 of 12\nEfficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework\n2.3. Extensibility for rapid iteration\nRapid iteration in embodied systems often occurs at\nthe level of capability composition: sensing modalities are\nadded or replaced, models and policies are updated fre-\nquently, and execution stacks evolve with changing hardware\nform factors and task demands. Deployment-oriented sys-\ntems therefore must allow components to iterate at different\ncadences and minimize full-stack rewiring and interface\nrefactoring; otherwise, integration overhead quickly domi-\nnates development cycles [4].\nIn practice, extensibility is accumulated through infras-\ntructure at different layers. Mixed RT/non-RT middleware\nemphasizes pluginized components and hardware abstrac-\ntion to reduce coupling when integrating new devices and\ncontrol modules [16], while mature open-source planning/\nmanipulation stacks often rely on community-driven exten-\nsions to close capability gaps and evolve via incremental\nadditions [30]. As embodied systems move toward heteroge-\nneous integration and multi-agent collaboration, extensibil-\nity becomes an organizational capability across platforms,\nvendors, and subsystems: fleet and infrastructure manage-\nment frameworks highlight standardized interfaces and\nadapter-based integration to append new robots and de-\nvices without rewriting the entire system [31]. Despite these\nbuilding blocks, the literature consistently notes that rapid\niteration remains constrained by interface drift, dependency\ngrowth, and runtime uncertainty; consequently, architecture-\nlevel mechanisms for adaptation and evolvability remain\nactively studied yet not fully converged [17]. Empirical com-\nparisons also suggest that middleware backends can exhibit\ndistinct latency and robustness behaviors under dynamic\nnetwork conditions, turning backend selection and tuning\ninto a recurring integration decision as systems scale [32].\nRecent work on workflow-oriented and context-aware\norchestration in ROS-based deployments emphasizes that\nintegration increasingly involves explicit workflow logic\nand shared context management, rather than simple point-\nto-point connections [33]. Related frameworks for hybrid\nedge\u2013cloud robot-assisted operations likewise highlight the\npractical need to manage shared context and task assignment\nacross distributed resources [34]. These trends amplify the\nvalue of stable interfaces for shared context and coordination\nas systems evolve and scale. From this perspective, AN-\nCHOR is positioned as a modular framework: it empha-\nsizes clear component boundaries and an evolvable shared\nrepresentation (Canonical Records), and uses concurrency-\nfriendly inter-component communication to sustain multi-\ncomponent coordination, turning repeated refactoring into\ncontrolled, incremental evolution.\n2.4. Operational definition of first-class\nmechanisms\nTo make comparisons concrete (Table 1), we use first-\nclass mechanism in an operational sense: a concern is first-\nclass if it is explicitly defined and supported by the sys-\ntem layer (with clear semantics, interfaces, and lifecycle),\nrather than being left to ad-hoc downstream integration or\nachievable only via external extensions. This definition dis-\ntinguishes what a framework provides as a direct primitive\nfrom what may be possible but is not a primary design\ncommitment.\nTo make this landscape concrete, Table 1 summarizes\nwhether representative systems and patterns explicitly define\nseveral modularity and robustness concerns as first-class\nmechanisms (per the above definition). We emphasize that\nthis checklist reflects explicit design emphasis rather than\nabsolute capability.\nTable 1 highlights that prior systems often provide strong\nsupport for communication ecosystems, QoS and opera-\ntional tooling, or architectural isolation at specific layers [15,\n4, 9]. ANCHOR builds on these foundations and aims to\nfurther elevate a canonical shared-state contract (canonical\nrecords) as an explicit system-level mechanism, aligning\nshared context with concurrency-friendly dissemination and\nfeedback to support robust coordination under deployment\nconstraints.\n3. ANCHOR Design\n3.1. System Overview\nANCHOR is a modular framework for embodied AI\nsystems designed to support rapid iteration and robust run-\ntime operation. Rather than committing to an end-to-end or\nstaged realization, ANCHOR provides stable system-level\nabstractions and interface semantics along the sensing-to-\nexecution chain, so heterogeneous components can evolve\nindependently while remaining composable in deployment.\nAt high level, ANCHOR organizes the system around\ntwo explicit primitives: canonical records, which provide a\nshared and evolvable representation for normalized observa-\ntions and system context, and a communication bus, which\nprovides many-to-many dissemination with concurrency-\naware delivery and explicit support for feedback. This sepa-\nration makes both shared context and inter-component coor-\ndination explicit at the system layer, avoiding ad-hoc \u201cglue\u201d\nlogic scattered across components.\nA typical closed-loop runtime flow (Fig. 1) proceeds\nas follows. Upstream modules ingest raw signals and write\nnormalized outputs into canonical records. Inference mod-\nules read the current context from canonical records and\npublish action/control messages to the communication bus.\nExecution modules subscribe to these messages, carry out\ncorresponding actions, and publish status/events back to the\nbus. Designated subscribers absorb feedback and materialize\nit as state updates in canonical records, closing the loop\nwhile keeping component boundaries intact. Because cross-\ncomponent context and coordination are mediated by stable\nprimitives, the system supports recomposition with reduced\nrewiring, and its operational boundaries under concurrency\ncan be reasoned about more directly.\n3.2. Design Goals\nANCHOR is guided by two primary goals: modularity\nand robustness under deployment constraints. We make\nDeng et al.: Preprint submitted to Elsevier\nPage 4 of 12\nEfficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework\nFigure 1: ANCHOR system overview. Upstream ingestion normalizes heterogeneous inputs into Canonical Records; inference\nconsumes records and publishes commands/actions to the communication bus; execution subscribes and produces status/events;\nfeedback is materialized back into Canonical Records, forming an explicit closed loop without a centralized monolithic controller.\nthese goals concrete in terms of what the system layer must\nprovide.\nModularity. Embodied deployments evolve quickly:\nsensors, models, and execution stacks change at different\ncadences. The system layer should therefore provide stable\ninterfaces that (i) localize changes within a component,\n(ii) minimize cross-module rewiring when components are\nadded or replaced, and (iii) reduce implicit coupling caused\nby undocumented message contracts or fragmented shared\ncontext. ANCHOR pursues modularity by explicitly sepa-\nrating shared context (canonical records) from coordination\n(communication bus), and by defining clear semantics for\nboth.\nRobustness. Real-world operation introduces uncer-\ntainty, load fluctuations, and partial failures. Robustness in\nour setting is a property of runtime behavior under stress:\nthe system should degrade in a controlled manner rather\nthan exhibiting cascading failures, and it should recover\ncleanly after transient disconnections or restarts. ANCHOR\npursues robustness through concurrency-friendly dissemi-\nnation (MPMC), explicit feedback pathways, and runtime\nmechanisms such as bounded buffering, prioritization/QoS\nknobs, and liveness monitoring for failure detection and\nrecovery.\n3.3. Canonical Records\nCanonical records provide an explicit shared-state inter-\nface for cross-component context. The key motivation is to\nreduce interface drift and implicit coupling when multiple\ncomponents must consume or update overlapping state.\nStandardized, evolvable representation. Instead of\npassing heterogeneous raw payloads through chains of point-\nto-point adapters, upstream modules normalize observations\ninto a canonical schema. This schema acts as a contract:\nfields have explicit semantics and update conventions, en-\nabling independently developed components to interoperate\nwith fewer hidden assumptions. As the system evolves, the\nschema can be extended in a controlled manner, preserving\ncompatibility where needed.\nEfficient sharing and persistence. Canonical records\nare implemented on a shared memory region backed by\nmemmap, enabling efficient read/write access across processes\nwhile also supporting persistence for logging and replay. On\ntop of memmap, ANCHOR adopts a uniform array-oriented\nlayout contract, so heterogeneous components can access\nshared context without exposing internal implementation\ndetails. This design reduces redundant copies and avoids\nforcing every interaction to go through the communication\nbus, while keeping state exchange explicit and inspectable at\nthe system boundary.\nExplicit responsibilities in closed-loop execution.\nCanonical records are where normalized context is accumu-\nlated and where feedback is materialized as state updates\n(Fig. 2). Inference modules treat Canonical records as read-\nonly context for decision making, while write paths are\nexplicitly associated with ingestion and feedback handlers.\nDeng et al.: Preprint submitted to Elsevier\nPage 5 of 12\nEfficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework\nFigure 2: Canonical Records as an explicit shared-state contract. A memmap-backed region stores normalized observations and\nsystem context with an agreed layout and update conventions, enabling cross-process sharing, persistence, and controlled schema\nevolution.\nThis separation preserves clear responsibilities and simpli-\nfies debugging under concurrency.\n3.4. Communication Bus\nWhile Canonical records anchor shared context, the\ncommunication bus anchors runtime coordination. It adopts\na publish\u2013subscribe abstraction to decouple producers and\nconsumers, and is designed to support many-to-many (MPMC)\ndissemination as a first-class pattern.\nMessages and interaction patterns. The bus carries\ntwo primary message classes: (i) commands/actions pub-\nlished by inference (or other decision-making) modules,\nand (ii) status/events published by execution modules. Both\nmessage classes follow the same dissemination mechanism,\nwhich makes feedback an explicit part of the runtime inter-\naction model rather than an afterthought.\nRoles and routing. To support both intra-cluster and\ncross-cluster coordination, the bus organizes routing into\nthree roles: nodes (publishers/subscribers), a master that\nmaintains subscriptions and forwards messages within a\ncluster, and gateways that bridge traffic across clusters and\nre-inject messages into target clusters. This structure pre-\nserves local publish\u2013subscribe semantics while enabling\nscale-out beyond a single node group.\nTopic structure and basic QoS. Topics follow a hi-\nerarchical structure such as /channel/region/(nodeId)/prio.\nThe region field controls dissemination scope (e.g., local\nvs cross-cluster), nodeId supports directed delivery when\nneeded, and prio provides a basic priority signal that can be\nmapped from message types. This organization helps keep\naddressing explicit and supports priority-aware handling\nunder load.\nHigh-load handling. To reduce overhead under high\nthroughput, nodes use shared-memory caches and asyn-\nchronous sending, and the bus supports batching for efficient\ntransmission. To avoid excessive delays at low traffic, a\nmaximum residence time bounds how long messages may\nwait for batching. Together, these mechanisms aim to keep\ndelivery behavior controlled across a wide range of load\nregimes.\nLiveness and recovery. For transient disconnects and\nprocess failures, the bus uses liveness monitoring (e.g.,\nheartbeats) to detect outages and trigger reconnection. Re-\nconnection includes re-registering identity and subscriptions\nso routes can be reconstructed without manual intervention.\nThese mechanisms provide a systems-layer basis for recov-\nery and reduce the chance that a single component failure\npermanently breaks closed-loop operation.\nRuntime subscription management. In deployment,\nsubscribers may need to adjust what they consume based\non task phase and resource constraints. The bus supports\nruntime subscription management so nodes can change sub-\nscriptions without global rewiring, enabling the system to\nadapt component coordination under evolving workloads.\n3.5. Modularity and Robustness in Practice\nANCHOR combines canonical records and the com-\nmunication bus to realize the design goals of modular-\nity and robustness in practice. Canonical records stabilize\ncross-component context by making shared state explicit\nand evolvable, which localizes change and reduces inter-\nface drift as components iterate. The communication bus\ncomplements this by governing concurrent coordination and\nfeedback through explicit dissemination semantics, keeping\ninteractions decoupled and inspectable at runtime.\nFrom a modularity perspective, explicit shared context\nand publish\u2013subscribe coordination reduce repeated inte-\ngration work and global rewiring when components are\nadded, replaced, or composed. From a robustness perspec-\ntive, bounded buffering and prioritization provide controlled\nDeng et al.: Preprint submitted to Elsevier\nPage 6 of 12\nEfficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework\nFigure 3: Communication bus roles and routing. A master maintains subscriptions and forwards messages within a cluster; gateways\nbridge cross-cluster traffic; nodes publish/subscribe to commands/actions and status/events for many-to-many coordination and\nfeedback.\nbehavior under overload, while liveness monitoring and\nreconnection logic enable clean recovery from transient\nfailures or restarts. We later characterize these runtime prop-\nerties empirically through latency distributions and recovery\nbehavior under load and failures.\n4. Empirical Evaluation\n4.1. Overview and Scope\nThis section empirically characterizes the runtime be-\nhavior of ANCHOR using an event-driven closed-loop au-\ntomation workflow. Multiple upstream producers ingest het-\nerogeneous observation streams and periodically perform\nwindow-based preprocessing and aggregation, writing nor-\nmalized state into canonical records as a shared contract\nacross components. A policy inference module consumes\nthe records and publishes action commands to a message\nbus; downstream executors subscribe to these commands,\ninteract with remote service endpoints, and publish runtime\nstatus and outcome events back to the bus. Selected feedback\nsignals are integrated into canonical records and archived for\nreplay, enabling traceability across iterations.\nOur goal is to validate end-to-end feasibility and charac-\nterize runtime properties of the messaging layer under con-\ncurrency and transient disruptions, rather than optimizing\ntask-specific performance. To avoid domain-specific disclo-\nsure, we abstract application details while preserving the\nsystem structure, interfaces, and representative workloads\nrelevant to ANCHOR.\nWe evaluate ANCHOR along two axes. First, we de-\nscribe and validate a complete closed-loop execution pattern\u2014\nfrom upstream aggregation intocanonical records, to com-\nmand publication, to downstream execution and feedback\nmaterialization\u2014to establish feasibility and traceability.\nSecond, we stress the message bus under increasing load and\nunder transient disconnect/reconnect events, reporting dis-\ntributional latency (ECDF and tail percentiles) and recovery\nbehavior to characterize robustness at runtime.\n4.2. Implementation and Measurement Setup\nCurrent testbed. All experiments are conducted on a\nsingle physical machine in our current test environment\n(CPU: AMD EPYC 9754 with 16 CPU cores available to the\nexperimental environment; memory: 128 GB DRAM; GPU:\nNVIDIA RTX 3090 with 24 GB memory for inference-\nrelated runs; OS: Ubuntu 22.04). The reported numbers\nshould be interpreted as an empirical characterization under\nthis testbed and the current implementation, rather than a\nhardware-agnostic performance bound.\nWorkflow. The evaluation pipeline is instantiated as a set\nof independent component processes connected via canon-\nical records and message bus. Upstream producers ingest\nheterogeneous observation streams and perform periodic\nwindow-based aggregation, writing the aggregated state\ninto canonical records. The policy module reads canoni-\ncal records and publishes action commands; one or more\nexecutor processes subscribe to these commands, invoke\nremote endpoints, and publish runtime status and outcome\nevents. For replay and debugging, commands and events are\nrecorded during execution.\nMetrics. We report message-processing latency in a\ndistributional form by plotting empirical cumulative distri-\nbution functions (ECDFs) and summarizing key percentiles\n(P50, P90, and P99). For transient disconnect/reconnect\ntests, we additionally report detection time and reconnection\ntime. Unless otherwise stated, we reuse the same setup and\nmeasurement conventions throughout this section.\nDeng et al.: Preprint submitted to Elsevier\nPage 7 of 12\nEfficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework\n(a) Payload=128B, Rate=1000 msg/s.\n(b) Payload=128B, Rate=5000 msg/s.\n(c) Payload=1024B, Rate=1000 msg/s.\n(d) Payload=1024B, Rate=5000 msg/s.\nFigure 4: Message-bus delivery latency ECDFs under a 1\u00d71 setup for four (payload size, publish rate) configurations.\n4.3."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**Summary of the Paper \"Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework\"**\n\nThis paper introduces ANCHOR, a modular framework designed to support rapid iteration and robust runtime operation in embodied AI systems. The main contributions are:\n\n1. **Modular Framework Viewpoint**: ANCHOR provides stable engineering support for rapid iteration and heterogeneous component recomposition while ensuring robust runtime behavior under real-world deployment constraints.\n\n2. **Canonical Records as Explicit Shared-State Interface**: Canonical records offer a shared, evolvable representation of normalized observations and system context to reduce interface drift and implicit coupling across components.\n\n3. **High-Concurrency Inter-Component Communication Bus**: ANCHOR introduces a high-concurrency communication mechanism based on multi-producer multi-consumer (MPMC) dissemination for many-to-many coordination, concurrency-aware delivery, and feedback interactions.\n\n4. **Empirical Evidence of Modularity and Robustness**: The authors validate the system-level properties targeted by their design through empirical evidence that ANCHOR supports modular recomposition via explicit contracts, controlled degradation under load, and automatic recovery after hard crashes and restarts.\n\n**Methodology**:\n\n- **System Architecture**: ANCHOR organizes the system around two key primitives: Canonical Records (a shared, evolvable representation of normalized observations and system context) and a Communication Bus (for many-to-many dissemination with concurrency-aware delivery and explicit feedback support).\n\n- **Design Goals**: The framework aims to provide stable interfaces that localize changes within components, minimize cross-module rewiring when components are added or replaced, and reduce implicit coupling caused by undocumented message contracts or fragmented shared contexts. Robustness is achieved through concurrency-friendly dissemination (MPMC), explicit feedback pathways, and runtime mechanisms like bounded buffering, prioritization, and liveness monitoring.\n\n**Key Findings**:\n\n- **Modularity**: ANCHOR supports modular recomposition via explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.\n  \n- **Robustness**: The framework demonstrates robust behavior under varying payload sizes and publish rates, with automatic stream resumption after hard crashes and restarts even when shared-memory loss occurs.\n\n**Conclusions**:\n\nANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems. The framework provides a clear design viewpoint that emphasizes component boundaries and communication organization to support rapid iteration and extensible capability growth while maintaining robustness and modularity at the system level.\n\n**Limitations and Future Work**:\n\n- **Interface Drift**: ANCHOR assumes shared context is explicitly defined, which may not always be feasible in all deployments.\n  \n- **Extensibility**: While ANCHOR supports different cadences of component evolution, it does not address how to handle unexpected or emergent components.\n\nIn summary, ANCHOR offers a robust and modular framework for embodied AI systems that can support rapid iteration and ensure reliable operation under real-world deployment constraints."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Soft Tail-dropping for Adaptive Visual Tokenization\nZeyuan Chen1\nKai Zhang2\nZhuowen Tu1\nYuanjun Xiong2\n1UC San Diego\n2Adobe\nToken Count\nPSNR\n(a) Adaptive Reconstruction by STAT\n(b) Text-conditional AR Generation\n(c) Class-conditional AR Generation\nFID\nTiTok-S\nLlamaGen\nOne-D-Piece\nLlamaGen\nSTAT (Ours)\nAverage Token Count\nSimple\nMedium\nHard\nFigure 1. (a) STAT adaptively allocates token counts based on image complexity (the outlined figures), using fewer tokens for simple\nimages and more for complex ones. When paired with a vanilla autoregressive model, STAT enables (b) high-quality text-conditional\nimage generation and (c) the best FID in class-conditional generation on ImageNet among visual tokenizers.\nAbstract\nWe present Soft Tail-dropping Adaptive Tokenizer (STAT),\na 1D discrete visual tokenizer that adaptively chooses the\nnumber of output tokens per image according to its struc-\ntural complexity and level of detail. STAT encodes an im-\nage into a sequence of discrete codes together with per-\ntoken keep probabilities. Beyond standard autoencoder ob-\njectives, we regularize these keep probabilities to be mono-\ntonically decreasing along the sequence and explicitly align\ntheir distribution with an image-level complexity measure.\nAs a result, STAT produces length-adaptive 1D visual tokens\nthat are naturally compatible with causal 1D autoregressive\n(AR) visual generative models.\nOn ImageNet-1k, equip-\nping vanilla causal AR models with STAT yields competi-\ntive or superior visual generation quality compared to other\nprobabilistic model families, while also exhibiting favorable\nscaling behavior that has been elusive in prior vanilla AR\nvisual generation attempts.\n1. Introduction\nAutoregressive models have demonstrated remarkable suc-\ncess across modalities - powering large language models\n(LLMs) [43, 58] in text and achieving strong performance\nin visual generation for both images and videos [6], as\nwell as joint modeling of language, vision, and beyond [8\u2013\n10]. Being the cornerstone of this success, modern text to-\nkenizers [44, 53] demonstrate the intriguing property that\nits representation cardinality is dependent on the content\nit is representating.\nThis property of content-dependent\ncompression, is unsurprisingly also the hallmark of mod-\nern image and visual compression approaches. However,\ntokenizers for popular visual generation models [5, 42, 56]\nstill anchor the visual tokens on rigid 2D structure and\nenforce fixed compression ratio regardless of the image\nand video content. Consequently, simple images may be\nover-represented, while complex scenes may remain under-\nencoded, which in turn challenges the capability of the gen-\nerative models. Despite numerous efforts [2, 27, 40, 48, 68]\nto decouple the representation cardinality of visual tok-\nenizer from the input resolution, we have not achieved\narXiv:2601.14246v1  [cs.CV]  20 Jan 2026\nadaptive alignment of output token cardinality with the\nvisual complexity.\nWe also have not observed empiri-\ncally the compatibility of content-adaptive visual tokeniz-\ners with generative models, especially autoregressive gen-\nerative models, which is well suited for tokenizers with this\nproperty [6, 43].\nIn this work, we propose STAT, which uses soft tail-\ndropping for adaptive visual tokenization. In detail, as a to-\nkenizer, STAT is able to encode a given image into discrete\nlatent tokens, decide the token counts used for a given im-\nage, and use the corresponding token counts for decoding.\nConcretely, STAT introduces a probabilistic token dropout\nmodule that predicts keep probabilities for each token po-\nsition, followed by per-token Bernoulli sampling to decide\nwhether a token is kept or dropped. To guide the learning\nprocess for the dropout probabilities, we apply three pri-\nors: (1) the probability profile should be monotonically de-\ncreasing, enforcing a tail-shaped dropping pattern; (2) the\nsum of predicted probabilities should correlate with image\ncomplexity, assigning fewer tokens to simple images and\nmore to complex ones; and (3) the model should attempt to\nuse fewer tokens to increase the compression ratio. These\npriors induce a model providing adaptive compression over\nimages, which is demonstrated to have fewer average token\ncount but better reconstruction.\nMore importantly, the adaptivity of STAT facilitates ease\nof modeling for generative models. When integrated with a\nvanilla autoregressive model that performs next token pre-\ndiction, STAT enables the model to reach state-of-the-art\ngeneration quality, comparable to or surpassing diffusion-\nbased models and other autoregressive models deviating\nfrom the simple next token prediction paradigm.\nTo summarize, our contributions are:\n\u2022 We propose STAT, an adaptive visual tokenizer that dy-\nnamically determines the compression ratio with different\ntoken counts based on image complexity.\n\u2022 We introduce to learn a probabilistic soft tail-dropping\nstrategy that enables the tokenizer to adaptively allocate\ntoken counts to different images, leading to improved ro-\nbustness in generative modeling.\n\u2022 STAT achieves the state-of-the-art reconstruction perfor-\nmance with fewer average tokens used. Integrating STAT\ninto a vanilla autoregressive framework yields a strong\ngenerative model comparable with other state-of-the-art\ngenerative models.\n2. Related Work\n2.1. Image Tokenization\nImage tokenization is one important step in modern visual\ngenerative models [5, 31, 49], where the images are mapped\nfrom the pixel space into a compressed high-dimensional\nlatent space, either continuous or discrete, to avoid pro-\nhibitive computation in pixel-space and smoothen the distri-\nbution to be modeled. Variational Autoencoder (VAE) [29]\nis one representative continuous tokenizer that maps images\ninto a continuous latent space. Vector-quantized autoen-\ncoder (VQ-VAE) [60] pioneers discrete visual tokenization.\nIt consists of an encoder, a quantizer with a codebook at\nthe bottleneck, and a decoder. The quantizer maps the en-\ncoded continuous embeddings into codebook embeddings\nby nearest neighbor search, and the discretized embeddings\nare fed to the decoder for reconstruction. Improvements to\nVQ-VAEs have been widely explored, including architec-\nture [7, 31, 46, 65], training objectives [16, 45], and quanti-\nzation techniques [15, 39, 66, 71].\nVQ tokenizers typically maintain a 2D spatial structure,\nwhere the downsampled 2D features at the bottleneck are\ndiscretized. To decouple the token number from input di-\nmension, 1D tokenizers like TiTok [27, 68] propose to con-\ncatenate 2D patch embeddings with 1D latent tokens as the\nmodel input and distill information from patch embeddings\ninto 1D latent tokens, which are then quantized. Flexible\ntokenizers [2, 40, 64] are subsequently developed to learn\nordered 1D token representation with nested dropout [48],\nallowing representing images with a varying number of to-\nkens. These methods still employs a predetermined token\ncount in their inference. STAT learns to reconstruct images\nwith content-adaptive token counts, representing an image\nwith the number of tokens aligned with its complexity.\nAmong emerging works investigating adaptive tokeniz-\ners, ALIT [14] proposes to iteratively update the 1D token\nsequences to identify smallest number of tokens for a given\nimage, but it requires multiple forward passes. One con-\ncurrent work [13] uses post-hoc heuristics based on recon-\nstruction of the full-length tokens to determine the token\ncount after the encoding. In contrast, STAT is single-pass\nand aligns token cardinality prediction with image com-\nplexity via an explicit objective, rather than treating token\ncount as a fixed hyperparameter or a heuristic byproduct.\nWe also demonstrate that STAT is particularly suitable for\nvanilla autoregressive models, achieving compelling gener-\nation quality that scales favorably with the capacity of gen-\nerative models.\n2.2. Image Generation\nGenerative adversarial network (GAN) [20] is one repre-\nsentative framework for early image geneartion. Many sub-\nsequent works [3, 26, 51] have been proposed to advance\nGANs in both fidelity and diversity. More recently, dif-\nfusion models [4, 25, 38, 42, 49, 54, 69] have become a\npopular choice for image generation due to their excellent\ngeneration quality and stable training.\nWith the development of VQ-VAE and other discrete to-\nkenizers, discrete generative models have also gained atten-\ntion in recent years, which can be mostly divided into two\n126\n531\n0\n1113 3432\n11\nViT Decoder\nViT Encoder\nImage Patches\n...\nLatent Tokens\n...\nOutput Tokens\nInput Image\nReconstruction\n...\n...\nKeep Probability\nToken Pos.\n...\n...\n126\n531\n0\n1113\n3432\n11\nKeep Prob\nToken Pos\n...\n1.0\n0.9\n0.8\n0.2\n0.1\n0.05\nSampled\nThreshold\n...\n...\nCausal AR Model\nCausal AR training\n(a) Overview of STAT\n(b) Image Generation with STAT\n126\n531\n1113\n0\n879\n531\nEoS\nEoS\nVQ & Prob Head\n...\n126\n531\n0\n1113 3432\n11\nDiscrete tokens\n&\nKeep Probabilities\nContent-adaptive Loss \u2112!\"#$%#$\nKeep Probability\nDecreasing Importance Prior \u2112&%!'%()*#+\n\u2112!\"#$\"%&'()\nComplexity\nToken Count\nToken Pos.\n\u2112#*(+\"(+\n...\n0\nFigure 2. (a) Overview of STAT. A ViT encoder followed by a vector-quantizer (VQ) layer produces discrete latent tokens along with\na keep probability for each token position. Probabilistic token dropping is applied to obtain a masked latent sequence for reconstruction,\nwhile two regularization losses enforce content-adaptive allocation and a decreasing importance prior. (b) Image generation with STAT.\nA special End-of-Sequence (EoS) token enables adaptive-length autoregressive generation, where the EoS position is determined by a\nthreshold with the keep-probability profile predicted by STAT.\ncategories. BERT-style models [5, 32, 66] that typically\nuse an Transformer encoder architecture with bi-directional\nattention and sample tokens in random order to gradually\nunmask all tokens in generation. Another category, GPT-\nstyle autoregressive generative models [16, 31, 56, 65] are\nbased on decoder-only architectures with casual attention.\nIt features easy integreation with other modalities such as\nlanaguage. LlamaGen [56] is one representative work in\nthis category, which demonstrates that with powerful image\ntokenizers and scalable model architectures, autoregressive\nmodel can reach competitive performance with diffusion\nmodels, albeit still hitting scaling bottleneck with respect\nto the model capacity.\nRecently there have been numerous efforts in explor-\ning new autoregressive-like pipelines for visual generation.\nVAR [57] models the autoregressive learning on images\nas next-scale prediction to form a coarse-to-fine generation\nprocess. MAR [33] introduces a diffusion loss to BERT-\nstyle generative models to allow using continuous tokeniz-\ners. RandAR [41] and xAR [47] augment next token predic-\ntion with different variations to strengthen decoder-only au-\ntoregressive visual generation. We note that these methods,\nwhile achieving strong generation performance, require ad-\nditional effort [10, 21, 72] to unite with LLMs.\n3. Method\nOur goal is to build a visual tokenizer that allocates content-\nadaptive numbers of latent tokens.\nThis requires mov-\ning beyond fixed 2D image grids and learning a 1D la-\ntent sequences whose lengths are determined adaptively by\nthe input complexity. STAT addresses this through a two-\nstage training pipeline. First, the tokenizer learns prefix-\nrobust reconstruction via randomized tail truncation, pro-\nducing an ordered latent sequence.\nSecond, it becomes\ncontent-adaptive by predicting per-token keep probabilities\nand learning a soft tail-dropping policy driven by visual\ncomplexity. This enables STAT to assign more tokens to\ncomplex images and fewer to simple ones, and naturally\nsupports 1D auto-regressive (AR) visual generation.\n3.1. Architecture\nSTAT adopts a 1D Transformer-based tokenizer that decou-\nples tokens from fixed 2D grids, enabling decoding with\nvariable-length latents and adaptive token allocation.\nGiven an image x \u2208RB\u00d7C\u00d7H\u00d7W , a patch embedding\nlayer maps it to P \u2208RB\u00d7N\u00d7D, where N = H/f \u00b7W/f for\ndownsampling factor f. We concatenate P with L learnable\nlatent tokens L \u2208RB\u00d7L\u00d7D and encode the full sequence:\n[zp, zl] = Enc([P, L]).\n(1)\nwhere zp and zl are output patch tokens and latent tokens.\nWe only retain zl and the vector-quantizer maps each latent\ntoken from zl to the nearest codebook entry, yielding dis-\ncrete latents zq. The decoder reconstructs the image from\nzq and a set of learnable output tokens O:\n\u02c6x = Dec([zq, O]).\n(2)\nThis architecture distills any input image into a 1D token\nsequence, making STAT compatible with both flexible pre-\nfix reconstruction (Sec. 3.2.1) and content-adaptive token\nallocation (Sec. 3.2.2).\n3.2. Adaptive Visual Tokenizer\nSTAT is trained in two stages. In the first stage (Sec. 3.2.1),\nthe tokenizer learns to reconstruct images from flexible pre-\nfix lengths, but the selection of token count is externally\nrandomized and not conditioned on content. In the second\nstage (Sec. 3.2.2), STAT becomes content-adaptive, learn-\ning to allocate token counts based on image complexity\nthrough a probabilistic soft tail-dropping mechanism.\n3.2.1. Flexible Prefix Reconstruction\nBefore learning content adaptivity, we first train the tok-\nenizer to reconstruct images from any prefix of its 1D latent\nsequence. Following prior work [2, 40], STAT is trained\nwith hard tail-dropping: in each iteration, a keep length\nK \u223cU(Lmin, Lmax) is sampled from a uniform distribution\nand the first K quantized tokens are used for reconstruction:\n\u02c6x = Dec([zq \u2299m, O]),\nmi = 1[i < K].\nRandom prefix truncation induces an ordered token rep-\nresentation in which early tokens encode coarse global\nstructure, while later tokens refine details.This stage enables\nSTAT to reconstruct images from prefixes of latent token se-\nquences, but does not teach the tokenizer to determine how\nmany tokens each image should use.\n3.2.2. Content-Adaptive Token Allocation\nTo make token counts adaptive to visual complexity, STAT\nlearns to predict a per-token keep-probability profile for\neach image and applies probabilistic soft tail-dropping.\nPer-token Keep Probabilities. We design our encoder to\noutput latent tokens zl \u2208RB\u00d7L\u00d7D along with their per-\ntoken keep probability by:\npj,i = \u03c3\n\u0000g\u03b8(zl[j, i])\n\u0001\n.\n(3)\nwhere j indexes the batch dimension and i indexes the to-\nken position. g\u03b8 is a position-aware multilayer perception\n(MLP), and \u03c3 denotes the sigmoid function. The result-\ning probabilities form a token importance profile over po-\nsitions. Each token is stochastically retained via indepen-\ndent Bernoulli sampling mj,i \u223cBernoulli(pj,i), and re-\nconstruction is performed from the masked discrete latent\nsequence zq,j \u2299mj, where mj = [mj,1, mj\u22122, . . . , mj,L]\nis the dropping mask for the image xj. Since Bernoulli sam-\npling is non-differentiable, we adopt a straight-through es-\ntimator (STE) to allow gradient flow during training.\nWe can also compute the expected number of retained\n...\n...\n126\n531\n0\n1113 3432\n11\nKeep the first \ud835\udc3etokens\n\ud835\udc3e= Uniform(\ud835\udc41!\"#, \ud835\udc41$%&$)\n...\n126\n531\n0\n1113 3432\n11\n...\n126\n531\n0\n1113 3432\n11\nBernoulli Sampling\n...\n126\n531\n0\n1113 3432\n11\nStage 1: Hard tail-dropping\nStage 2: Soft tail-dropping\nFigure 3. Token dropping strategies in two training stages.\ntokens for image xj by:\nTj =\nL\u22121\nX\ni=0\npj,i,\n(4)\nso token allocation becomes a differentiable function of the\ncontinuous importance landscape rather than a discrete se-\nlection problem, hence the name \u2018soft tail-dropping\u2019.\nContent-adaptive prior. To allocate tokens based on visual\ncomplexity, we impose a prior that encourages the expected\ntoken count to increase with perceptual difficulty. For each\nimage xj, we compute its perceptual reconstruction error\nLperc,j (e.g., LPIPS) as a proxy for visual complexity, and\ncorrelate this quantity with the expected token count Tj.\nAcross all samples in a batch, we optimize\nLcontent =\n\u00001 \u2212corr(Lperc, T)\n\u00012,\n(5)\nwhich encourages a strong positive correlation between the\ncomplexity proxy Lperc and the allocated token count T.\nLcontent acts as a content-adaptive prior on token counts,\nnudging the tokenizer to assign more tokens to harder im-\nages and fewer tokens to simpler ones. By contrasting all\nsamples in the batch, this loss encourages the model to\nproperly allocate tokens without the need to normalize the\ncomplexity proxy precisely. In practice, we compute the\ncorrelation using the Pearson correlation coefficient.\nDecreasing Importance Prior. To match the causal struc-\nture prior required for autoregressive generation, we regu-\nlarize the importance profile toward a decreasing form by\npenalizing upward steps:\nLdecrease =\nL\u22121\nX\ni=1\nmax\n\u00000, pj,i \u2212pj,i\u22121\n\u0001\n.\n(6)\nThis encourages a soft monotonic decay along positions,\nstabilizes variable-length decoding, and produces consis-\ntent, content-dependent early-stop behavior when the tok-\nenizer is used in AR models for generation.\nKL Sparsity Prior. The content adaptive loss shapes rela-\ntive allocation but does not enforce a global token budget.\nWe impose a Bernoulli sparsity prior by regularizing the av-\nerage keep probability:\n\u00afpj = 1\nL\nL\u22121\nX\ni=0\npj,i\n(7)\ntoward a target preset sparsity p\u2217using KL divergence:\nLsparse = KL(Bern(p\u2217) \u2225Bern(\u00afpj)) .\n(8)\nOverall Objective The adaptive tokenizer is trained with\nthe composite loss\nL = Lrecon + LGAN + LVQ + \u03bbcontentLcontent\n+\u03bbdecreaseLdecrease + \u03bbsparseLsparse.\n(9)\nThis yields a tokenizer that is content-adaptive, con-\ntrastively aligned with visual complexity,\nstructurally\nmonotonic for AR usage, and globally sparse under a con-\ntrollable token budget. Further details on all loss terms and\nthe corresponding weight settings are provided in the sup-\nplementary material.\n3.3. Visual Generation with Adaptive Tokenizer\nThe primary purpose of visual tokenizers is to provide an ef-\nfective representation space for generative models. To eval-\nuate this capacity, we integrate STAT with a vanilla autore-\ngressive (AR) model and train the AR model on the discrete\nlatent tokens produced by STAT.\n1-D autoregressive modeling. Given an image x, STAT\nproduces a quantized latent sequence q = (q0, . . . , qL\u22121).\nThe AR model is then trained to model the distribution over\nthis sequence as:\np\u03d5(zq) =\nL\nY\nt=0\np\u03d5(zt | z<t),\n(10)\nwhere p\u03d5(\u00b7) is the probability distribution parameterized by\nthe AR model, and \u03d5 represents its learnable parameters.\nEoS from keep probabilities. Different from traditional\nfixed-length tokenizers, STAT encodes each image together\nwith a keep-probability profile p = (p0, . . . , pL\u22121), which\nis supposed to be nearly monotonic via Ldecrease during the\ntraining of STAT (Sec. 3.2.2). This structure allows the AR\nmodel to operate in an adaptive manner using a special end-\nof-sequence (EoS) token. During training, we randomly\nsample a probability threshold \u03c4 and define the EoS posi-\ntion as:\nk = min{i | pi < \u03c4}.\n(11)\nThe\nAR\nmodel\nis\nthen\nsupervised\non\nthe\nprefix\n(z0, . . . , zk\u22121) followed by the EoS token. The monotonic-\nity of p ensures that the thresholding is stable and naturally\naligned with the soft tail-dropping behavior of STAT.\n4. Experiments\nIn this section, we evaluate the reconstruction capacity\nof STAT, and explore its integration with an autoregres-\nsive (AR) generation model, assessing its effectiveness in\nboth class-conditional and text-conditional image genera-\ntion tasks. We show that STAT achieves higher reconstruc-\ntion quality while using fewer tokens than existing visual to-\nkenizers, demonstrating its ability to perform content-aware\ncompression. For image generation, we demonstrate that a\nvanilla AR model combined with STAT attains competitive\nperformance against both diffusion-based approaches and\nAR models that rely on complex training pipelines. Finally,\nwe extend our study to the video domain and validate the\neffectiveness of the soft probabilistic tail-dropping mecha-\nnism for adaptive video tokenization.\nTraining setup.\nWe train STAT on the ImageNet-1k [11]\ntraining set at a resolution of 256 \u00d7 256 using random-crop\naugmentation. The patch-embedding downsampling factor\nis set to f = 16, and the vector-quantizer uses a codebook\nof size 4096 with a code dimension of 12. The latent se-\nquence length is set to 256.\nIn the first stage, the keep\nlength is sampled from a uniform distribution with bounds\nLmin = 160 and Lmax = 256. At inference time, we use\na default probability threshold of 0.5, retaining all tokens\nwhose predicted keep probabilities exceed this threshold\nand dropping the rest. For additional hyperparameter set-\ntings and details of AR model training and inference, please\nrefer to the supplementary material.\nMetrics.\nWe report reconstruction FID (rFID) [23] and\nPSNR for the image reconstruction task. For image gen-\neration, we evaluate generation FID (gFID) [23], Inception\nScore (IS) [50], and Precision/Recall [30]. For video re-\nconstruction, we measure reconstruction FVD [59] (rFVD),\nPSNR, and LPIPS [70] to assess the quality.\n4.1. Image Reconstruction\nThe evaluation for image reconstruction is conducted on the\nImageNet-1K validation set at a resolution of 256\u00d7256. We\ncompare STAT with representative 2D and 1D tokenizers.\nAs shown in Tab. 1, STAT achieves the best rFID among\nall tokenizers while using only 90% of the token count\ncompared to other tokenizers.\nThis highlights the bene-\nfits from the adaptive token allocation strategy learned by\nSTAT, which maximizes the representational efficiency of\nthe available tokens. Moreover, when allowing more token\nbudgets for reconstruction by either adjusting the probabil-\nity threshold in inference or manually adding the budgets,\nSTAT shows further improvements in rFID, albeit with a\nmarginal decrease in PSNR. We hypothesize that this trade-\noff emerges because more tokens encourage the model to\nprioritize perceptual realism over pixel-level fidelity.\nIn Fig. 4, we show the keep-probability profile for dif-\nferent images. We show that STAT successfully learns a\nToken Count: 175\nJPEG Size: 63 KB\nToken Count: 195\nJPEG Size: 83 KB\nToken Count: 222\nJPEG Size: 124 KB\nToken Count: 237\nJPEG Size: 145 KB\nToken Count: 256\nJPEG Size: 164 KB\nToken Count: 208\nJPEG Size: 104 KB\nKeep Probability\nToken Position\nFigure 4. Keep-probability curves and corresponding reconstructions across samples of varying complexity. More complex images receive\nhigher token counts, and the learned allocation correlates strongly with the JPEG file sizes of the inputs.\ncomplexity-aware token allocation strategy, which is further\nvalidated by the high correlation between the allocated to-\nken counts and the JPEG sizes for storing the images.\nTable 1. Comparison on ImageNet-1k image reconstruction.\nWe compare with 2D image tokenizers including Taming VQ-\nGAN [16], MaskGIT VQ-GAN [5], Open MAGVIT-v2 [36], and\nthe tokenizer in LlamaGen [56]; 1D image tokenizers including\nTiTok [68], FlexTok [2], and One-D-Piece [40]. All evaluations\nare performed on the ImageNet-1k validation set at a resolution of\n256\u00d7256. STAT achieves the best rFID while using 10% fewer\ntokens compared with other tokenizers. We additionally report\nresults using a probability threshold of 0.01 at inference. \u201c(+X\nTokens)\u201d denotes manually adding X extra tokens to all images at\ninference. These variants of STAT further improves to an rFID of\n0.88 when using an average of 240 tokens.\nType\nTokenizer\n#Tokens\nCodebook Size\nrFID \u2193\nPSNR \u2191\n2D Tokenizer\nTaming VQ-GAN [16]\n256\n16384\n4.98\n19.40\nMaskGIT VQ-GAN [5]\n256\n1024\n2.28\n-\nOpen MAGVIT-v2 [36]\n256\n262144\n1.17\n22.64\nLlamaGen [56]\n256\n4096\n3.02\n19.99\nLlamaGen [56]\n256\n16384\n2.19\n20.79\n1D Tokenizer\nTiTok-L [68]\n32\n4096\n2.21\n15.96\nTiTok-B [68]\n64\n4096\n1.70\n17.13\nTiTok-S [68]\n128\n4096\n1.71\n17.80\nFlexTok [2]\n256\n64000\n1.08\n17.70\nOne-D-Piece-L [40]\n256\n4096\n1.08\n19.04\n1D Tokenizer\nSTAT-Fixcount\n220\n4096\n1.15\n20.35\nSTAT-Harddrop\n222\n4096\n1.15\n20.30\n1D Tokenizer\nSTAT\n220\n4096\n1.15\n20.22\nSTAT(Threshold 0.01)\n230\n4096\n0.99\n20.25\nSTAT(Threshold 0.01; +10 Tokens)\n240\n4096\n0.88\n20.02\n4.2. Image Generation\nGiven the strong reconstruction capability of STAT, we in-\nvestigate whether this translates into improved generation\nperformance. To this end, we integrate STAT into a vanilla\nautoregressive image generation framework, using the same\nbackbone as LlamaGen [56]. An End-of-Sequence (EoS)\ntoken is used to allow the AR model to dynamically deter-\nmine the effective token length during inference.\nAs summarized in Tab. 2, the vanilla AR model equipped\nwith STAT achieves generation quality that is comparable\nto state-of-the-art AR and diffusion-based models, despite\nits minimalist design. This demonstrates that STAT is well\nsuited for AR generative modeling, improving both effi-\nciency and flexibility without requiring elaborate architec-\ntural heuristics. These results suggest that STAT not only\nw/ \u2112!\"#$\"%&\"\nw/o \u2112!\"#$\"%&\"\nw/ \u2112#'()\"()\nw/o \u2112#'()\"()\nKeep Probability\nToken Position\nToken Count Range\nFigure 5. Ablation Study on the two loss terms for decreasing\nimportance prior and the content-adaptive prior. The decreasing\nimportance prior is necessary for learning the monotonic structure\nfor AR generation, while the content-adaptive prior is crucial for\nsuccessfully learning token allocation strategy.\nexcels at reconstruction, but also enhances generation in a\nsimple AR setup, underscoring its potential as a strong tok-\nenizer for generation and unified multimodal models.\nIn Tab. 3, we compare different visual tokenizers within\nthe same AR generation framework, including the improved\nVQ tokenizer from LlamaGen, TiTok-S, and One-D-Piece-\nL. STAT delivers the best generation performance, surpass-\ning all baseline tokenizers by a substantial margin. These\nresults further confirm the effectiveness of STAT in enhanc-\ning autoregressive image generation.\n4.3. Ablation Study\nWe investigate the effectiveness of different design compo-\nnents in the STAT pipeline, evaluating their impact on re-\nconstruction and generation quality.\nAdaptive Token Allocation. To assess whether the learned\nadaptivity contributes to performance gains, we include an\nablation in which the tokenizer uses a fixed token count for\nall images. This count is set to the average number of tokens\nused by STAT in inference when the probability threshold\nis 0.5. We denote this variant as STAT-Fixcount.\nThe reconstruction comparison is shown in Tab. 1. While\nSTAT-Fixcount achieves similar rFID and slightly higher\nPSNR, the adaptive version offers greater flexibility, en-\nabling it to leverage additional token budgets to obtain im-\nTable 2. Comparison on ImageNet-1k class-conditional image generation. We compare the AR generative model built on STAT\nwith a broad range of generative models, including: GAN-based approaches such as BigGAN [3], GigaGAN [26], and StyleGAN [51];\ndiffusion-based models including ADM [12], LDM [49], DiT [42], and SiT [37]; autoregressive models with bi-directional attention\nincluding MaskGIT [5], MAGVIT-v2 [66], MAR [33], and TiTok [68]; autoregressive models with causal attention including VQGAN [16],\nRQTransformer [31], Open-MAGVIT-v2 [36], VAR [57], SAR [35], RAR [67], RandAR [41], and LlamaGen [56]. All evaluations are\nconducted on the ImageNet-1k validation set at a resolution of 256\u00d7256. By simply replacing the original image tokenizer in LlamaGen\nwith our proposed STAT, the 3B model achieves a gFID of 1.77, improving upon the LlamaGen baseline by 0.41 while using less than half\nthe number of tokens. The performance of our AR model is also comparable to diffusion-based models and other AR models that rely on\nmore advanced architectures and training strategies.\nType\nModel\n#Para.\ngFID\u2193\nIS\u2191\nPrecision\u2191\nRecall\u2191\n#Tokens\nGAN\nBigGAN [3]\n112M\n6.95\n224.5\n0.89\n0.38\n1\nGigaGAN [26]\n569M\n3.45\n225.5\n0.84\n0.61\n1\nStyleGAN-XL [51]\n166M\n2.30\n265.1\n0.78\n0.53\n1\nDiffusion\nADM [12]\n554M\n4.59\n186.7\n0.82\n0.53\n250\nLDM-4 [49]\n400M\n3.60\n247.7\n\u2013\n\u2013\n250\nDiT-XL [42]\n675M\n2.27\n278.2\n0.83\n0.57\n250\nSiT-XL [37]\n675M\n2.06\n270.3\n0.82\n0.59\n250\nBi-directional AR\nMaskGIT-re [5]\n227M\n4.02\n355.6\n\u2013\n\u2013\n256\nMAGVIT-v2 [66]\n307M\n1.78\n319.4\n\u2013\n\u2013\n256\nMAR-L [33]\n479M\n1.98\n290.3\n\u2013\n\u2013\n64\nMAR-H [33]\n943M\n1.55\n303.7\n0.81\n0.62\n256\nTiTok-S-128 [68]\n287M\n1.97\n281.8\n\u2013\n\u2013\n128\nAdvanced Causal AR\nVAR [57]\n600M\n2.57\n302.6\n0.83\n0.56\n680\nVAR [57]\n2.0B\n1.92\n350.2\n0.82\n0.59\n680\nSAR-XL [35]\n893M\n2.76\n273.8\n0.84\n0.55\n256\nRAR-XXL [67]\n955M\n1.50\n306.9\n0.80\n0.62\n256\nRAR-XL [67]\n1.5B\n1.48\n326.0\n0.80\n0.63\n256\nRandAR-XL [41]\n775M\n2.25\n317.8\n0.80\n0.60\n256\nRandAR-XXL [41]\n1.4B\n2.15\n322.0\n0.79\n0.62\n256\nVanilla Causal AR\nVQGAN [16]\n1.4B\n5.20\n280.3\n\u2013\n\u2013\n256\nRQTran.-re [31]\n3.8B\n3.80\n323.7\n\u2013\n\u2013\n256\nOpen-MAGVIT-v2-XL [36]\n1.5B\n2.33\n271.8\n0.84\n0.54\n256\nLlamaGen-XL-256 [56]\n775M\n3.39\n227.1\n0.81\n0.54\n256\nLlamaGen-XXL-256 [56]\n1.4B\n3.09\n253.6\n0.82\n0.53\n576\nLlamaGen-3B-256 [56]\n3.1B\n3.06\n279.7\n0.84\n0.54\n576\nLlamaGen-XL-576 [56]\n775M\n2.62\n244.1\n0.80\n0.57\n576\nLlamaGen-XXL-576 [56]\n1.4B\n2.34\n253.9\n0.80\n0.59\n576\nLlamaGen-3B-576 [56]\n3.1B\n2.18\n263.3\n0.81\n0.58\n576\nVanilla Causal AR\nLlamaGen-STAT-XL\n775M\n2.36\n244.0\n0.78\n0.62\n223\nLlamaGen-STAT-XXL\n1.4B\n1.91\n290.2\n0.79\n0.63\n227\nLlamaGen-STAT-3B\n3.1B\n1.75\n300.6\n0.80\n0.64\n229\nproved rFID. For image generation results in Tab. 3), learn-\ning adaptivity leads to substantial performance gains. We\nattribute this improvement to that adaptive tokenization al-\nlows the AR model to jointly explore the latent code space\nand the token-count dimension, ultimately yielding higher-\nquality generations.\nPer-Token v.s. Per-Sample Dropping. In STAT, token\ndropping is model as a per-token sampling process, and the\nproposed regularization terms shape the keep-probability\nprofile into a soft tail-dropping pattern. To better under-\nstand its effect, we implement a hard token dropping vari-\nant, dubbed STAT-Harddrop, in which tokens are always\ntruncated from the tail, and the token count is determined\nby the sum of the predicted keep probabilities across all to-\nken positions.\nAs shown in Tab. 1, the two mechanisms show compa-\nrable performance, with STAT-Harddrop achieving slightly\nhigher PSNR possibly due to the hard tail dropping pat-\ntern is easier for decoding. However, in image generation,\nSTAT notably outperforms its harddrop counterpart. We at-\nTable 3.\nImageNet-1k image class-conditional image gener-\nation performance on LlamaGen with different tokenizers.\nWe train LlamaGen, a vanilla transformer-based autoregressive\nmodel, on different tokenizers including the VQ tokenizer in [56],\nTiTok [68], One-D-Piece [40], and STAT. All experiments are con-\nducted on LlamaGen-XL (775M) with the same training settings.\nThe model trained with STAT outperforms other tokenizers by a\nlarge margin. We further evaluate a variant that replaces the orig-\ninal soft probabilistic dropping strategy with a deterministic tail-\ndrop, and the results demonstrate that the soft probabilistic drop-\nping mechanism is essential for achieving high-quality generation.\nTokenizer\n#Tokens\nCodebook Size\ngFID \u2193\nIS \u2191\nPre. \u2191\nRec. \u2191\nLlamaGen [56]\n256\n16384\n3.39\n227.1\n0.81\n0.54\nLlamaGen [56]\n576\n16384\n2.62\n244.1\n0.80\n0.57\nTiTok-S [68]\n128\n4096\n4.90\n191.7\n0.77\n0.56\nOne-D-Piece-L [40]\n256\n4096\n2.99\n235.1\n0.81\n0.59\nSTAT-FixCount\n220\n4096\n2.73\n234.2\n0.77\n0.62\nSTAT-Harddrop\n223\n4096\n2.67\n231.2\n0.76\n0.63\nSTAT-Fixthreshold\n221\n4096\n2.49\n240.7\n0.79\n0.61\nSTAT\n223\n4096\n2.36\n244.0\n0.78\n0.62\ntribute this to the stochasticity introduced by per-token sam-\npling where early tokens are also possibly dropped. This\nstochastic perturbation improves robustness to imperfect to-\nkens and effectively mitigates the exposure bias commonly\nobserved in autoregressive inference.\nPosition of the End-of-Sequence (EoS) Token.\nFor the\nAR generation model, we examine whether sampling the\nEoS threshold and allowing its position to vary across dif-\nferent passes of the same images could lead to better per-\nformance than using a fixed threshold that enforces a static\nEoS position for each image. As in Tab. 3 between STAT\nand STAT-Fixthreshold, sampling the EoS threshold leads\nto a noticeable improvement in generation. The observa-\ntion shows that the varying EoS positions could work as an\naugmentation for further improvement in generation.\nRegularization Losses. We test the effectiveness of two\nloss terms: the decreasing loss and content-adaptive loss.\nAs shown in Fig. 5, without the content-adaptive loss, the\ntokenizer is unable to learn effective token allocation strate-\ngies and learns only trivial solution where all images use\nalmost the same token count. The decreasing loss is essen-\ntial for the tokenizer to learn the soft tail dropping pattern\nand the structure prior of causality, which is necessary for\nAR generation training.\n4.4. Text-conditional Image Generation\nWe evaluate STAT on text-conditional image generation by\ncomparing it with the enhanced VQ tokenizer from Llama-\nGen [56]. Both tokenizers are trained on ImageNet, and we\ntrain separate vanilla autoregressive models with the two to-\nkenizers on a subset of LAION-2B [52] for one epoch at the\nresolution 256\u00d7256. We measure the text-conditional gen-\neration quality on the GenEval [19] benchmark.\nTable 4. Comparison between STAT and VQGAN from Llam-\naGen on Text-conditional generation results on GenEval.\nTokenizer\nS. Obj.\nT. Obj.\nCount.\nColors\nPosition\nC. Attri.\nOverall\nLlamaGen\n0.91\n0.18\n0.19\n0.57\n0.04\n0.01\n0.32\nSTAT\n0.99\n0.36\n0.40\n0.81\n0.10\n0.06\n0.45\nTable 5.\nComparison on UCF-101 video reconstruction.\nWe compare STAT-Video with other video tokenizers includ-\ning TATS [18], OmniTokenizer [63], ElasticTok [64], Cosmos-\nTokenizer-DV [1], LARP [62], and AdapTok [34]. Following the\nevaluation protocol of AdapTok [34], all methods are evaluated on\n16-frame video clips at a resolution of 128\u00d7128. Our approach\nachieves the best PSNR and LPIPS while delivering competitive\nFVD with other state-of-the-art video tokenizers. \u201c(+ X Tokens)\u201d\ndenotes manually adding X extra tokens to all videos at inference.\n\u2020 indicates the baseline was trained for an additional 75 epochs.\nTokenizer\n#Tokens\nCodebook Size\nrFVD \u2193\nPSNR \u2191\nLPIPS \u2193\nTATS [18]\n1024\n16384\n157\n24.22\n0.206\nOmniTokenizer [63]\n1280\n8192\n62\n27.76\n0.112\nElasticTok [64]\n1022\n64000\n230\n-\n-\nCosmos-Tokenizer-DV [1]\n1280\n64000\n140\n-\n-\nLARP [62]\n1024\n8192\n24\n-\n-\nLARP\u2020 [62]\n1024\n8192\n20\n28.71\n0.075\nAdapTok [34]\n1024\n8192\n36\n25.72\n0.114\nSTAT-Video\n997\n8192\n29\n28.77\n0.064\nSTAT-Video (+100 Tokens)\n1097\n8192\n26\n29.08\n0.062\nSTAT-Video (+200 Tokens)\n1197\n8192\n23\n29.27\n0.060\nSTAT-Video (+300 Tokens)\n1297\n8192\n22\n29.40\n0.059\nAs shown in Tab. 4, STAT consistently outperforms the\nVQ model across all metrics, demonstrating stronger gener-\nalization for text-conditional generation.\n4.5. Video Reconstruction\nWe extend our soft tail-dropping strategy to the video do-\nmain by applying STAT to 16-frame video clips. As shown\nin Tab."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**Summary of \"Soft Tail-dropping for Adaptive Visual Tokenization\"**\n\nThis paper introduces Soft Tail-dropping Adaptive Tokenizer (STAT), a novel visual tokenizer designed to adaptively allocate token counts based on image complexity and detail, enabling more efficient representation and better generation quality. \n\n### Main Contributions\n\n1. **Adaptive Token Allocation**: STAT dynamically determines the number of tokens per image according to its structural complexity and level of detail, allowing for content-adaptive compression.\n\n2. **Probabilistic Soft Tail-Dropping (PSTD)**: The tokenizer employs a probabilistic token dropout mechanism that predicts keep probabilities for each token position. This enables adaptive allocation of token counts without requiring fixed hyperparameters or heuristic adjustments.\n\n3. **Content-Adaptive Loss**: STAT incorporates content-adaptive prior losses to encourage the model to use fewer tokens for complex images and more for simple ones, aligning with image complexity measures like perceptual loss (LPIPS).\n\n4. **Vanilla Autoregressive Integration**: When integrated into a vanilla autoregressive model, STAT yields competitive or superior visual generation quality compared to other probabilistic models, including diffusion-based methods.\n\n### Methodology\n\n**Architecture and Training Pipeline:**\n- STAT uses a 1D Transformer-based tokenizer that decouples tokens from fixed 2D grids. It encodes images into a sequence of discrete latent tokens along with per-token keep probabilities.\n- In the first stage, STAT learns prefix robust reconstruction via randomized tail truncation to produce an ordered latent sequence.\n- In the second stage, it becomes content-adaptive by predicting per-token keep probabilities and learning a soft tail-dropping policy driven by visual complexity.\n\n**Loss Functions:**\n- **Reconstruction Loss (Lrecon)**: Encourages the model to reconstruct images accurately from latent tokens.\n- **GAN Loss (LGAN)**: Ensures the generated images are realistic.\n- **VQ Loss (LVQ)**: Encourages the use of a discrete codebook for efficient compression.\n- **Content-Adaptive Prior Loss (Lcontent)**: Aligns token counts with image complexity, encouraging more tokens for complex scenes and fewer for simple ones.\n- **Decreasing Importance Prior Loss (Ldecrease)**: Ensures a monotonic decay in importance profiles to facilitate autoregressive generation.\n\n### Key Findings\n\n1. **Image Reconstruction**: STAT achieves state-of-the-art reconstruction performance on the ImageNet-1k dataset, using only 90% of the token count compared to other tokenizers while maintaining competitive PSNR values.\n2. **Image Generation**: When integrated with a vanilla autoregressive model, STAT outperforms existing visual tokenizers in both class-conditional and text-conditional image generation tasks, demonstrating strong generative capabilities without requiring complex architectural adjustments.\n\n### Conclusions\n\nThe paper presents STAT as an effective solution for adaptive visual tokenization, offering improved compression efficiency and robustness compared to traditional fixed-token approaches. The proposed probabilistic soft tail-dropping mechanism enables content-adaptive allocation of tokens, leading to better generation quality across various tasks including image and video reconstruction."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "FASTGHA: GENERALIZED FEW-SHOT 3D GAUSSIAN\nHEAD AVATARS WITH REAL-TIME ANIMATION\nXinya Ji1,2, Sebastian Weiss3, Manuel Kansy3, Jacek Naruniec3, Xun Cao1,\nBarbara Solenthaler2, Derek Bradley3\n1Nanjing University, 2ETH Z\u00a8urich, 3DisneyResearch|Studios\nABSTRACT\nDespite recent progress in 3D Gaussian-based head avatar modeling, efficiently\ngenerating high fidelity avatars remains a challenge. Current methods typically\nrely on extensive multi-view capture setups or monocular videos with per-identity\noptimization during inference, limiting their scalability and ease of use on un-\nseen subjects. To overcome these efficiency drawbacks, we propose FastGHA,\na feed-forward method to generate high-quality Gaussian head avatars from only\na few input images while supporting real-time animation. Our approach directly\nlearns a per-pixel Gaussian representation from the input images, and aggregates\nmulti-view information using a transformer-based encoder that fuses image fea-\ntures from both DINOv3 and Stable Diffusion VAE. For real-time animation, we\nextend the explicit Gaussian representations with per-Gaussian features and intro-\nduce a lightweight MLP-based dynamic network to predict 3D Gaussian defor-\nmations from expression codes. Furthermore, to enhance geometric smoothness\nof the 3D head, we employ point maps from a pre-trained large reconstruction\nmodel as geometry supervision. Experiments show that our approach significantly\noutperforms existing methods in both rendering quality and inference efficiency,\nwhile supporting real-time dynamic avatar animation.\n1\nINTRODUCTION\nOver the last few years the field of digital avatars has seen tremendous growth, fueled primarily by\nthe advent of neural rendering (Mildenhall et al., 2021; Lombardi et al., 2019) and 3D Gaussian\nSplats (3DGS) (Kerbl et al., 2023) in particular. The simple parameterization, tractable reconstruc-\ntion, and real-time novel view rendering of 3DGS provides a suitable representation for photoreal\ndigital human avatars. For this reason, Gaussian-based avatars have been extensively explored in\nseveral recent works (Kirschstein et al., 2025; Gong et al., 2025; He et al., 2025; Chu & Harada,\n2024).\nInitial digital avatar methods require synchronized multi-view video or long monocular data for the\nsubject to be reconstructed (Hong et al., 2022; Qian et al., 2024; Giebenhain et al., 2024; Wang et al.,\n2025a; Zielonka et al., 2025; Ma et al., 2024). While this approach proved the ability to represent\ndigital avatars using Gaussian-based representations, this avenue is clearly not scalable. Some works\nreduce the data requirements by synthesizing novel views first and then optimizing a Gaussian avatar\non these synthetic observations (Taubner et al., 2025; Zhou et al., 2025; Tang et al., 2025; Yin et al.,\n2025; Gu et al., 2025). While this alleviates the need for dense captures, it still requires long per-\nidentity reconstruction. Another line of research involves training generative models with 3D head\npriors, but still requires fitting an identity code for each new person at inference (Zheng et al., 2025;\nSun et al., 2023; Tang et al., 2023; Xu et al., 2025). The most efficient and attractive approach that\nhas emerged so far is to build the avatar in a feed-forward way given only a small set of images\nof the subject. While this requires pre-training a generalized model on a large corpus of human\nface data, it enables instantaneous reconstruction and novel-view rendering of unseen subjects at\ninference time. Inspired from the success of large reconstruction models (Xu et al., 2024a), recent\nwork such as Avat3r (Kirschstein et al., 2025) and Facelift (Lyu et al., 2024) leverage a feed-forward\nnetwork to fuse sparse input views into 3D Gaussian head representations. However, these methods\neither lack support for controllable facial animation or suffer from slow animation speed and limited\nreconstruction fidelity.\n1\narXiv:2601.13837v1  [cs.CV]  20 Jan 2026\nFigure 1: We present FastGHA, a feed-forward method that generates Gaussian head avatars from\nfew-shot input images and animates them in real time.\nIn this work, we propose FastGHA, a new architecture for generalized few-shot 3D Gaussian head\navatar reconstruction that allows real-time animation. Our network is a feed-forward approach that\ntakes only a few images of a subject from well-distributed views and arbitrary expressions and learns\na high quality animatable avatar. We infer a per-pixel Gaussian representation, enabling the preser-\nvation of fine-grained head details. To achieve high-fidelity and efficient animation, we adopt a\ntwo-stage pipeline to first reconstruct a canonical Gaussian head from the input images, followed by\na lightweight deformation network to animate the Gaussians based on descriptive expression codes.\nIn this way, our model learns to disentangle the different facial expressions in the input images by\neffectively mapping the images to a neutral canonical Gaussian head before applying animation. For\nhigh-quality reconstruction, we leverage features extracted from a pre-trained diffusion VAE (Rom-\nbach et al., 2022) and DINOv3 (Sim\u00b4eoni et al., 2025), and introduce a transformer-based feed-\nforward module to fuse multi-view information from these features. For real-time animation, we\nextend the explicit Gaussian representation with per-Gaussian features and introduce a lightweight\nMLP-based dynamic network to predict fine-scale Gaussian deformations from expression codes.\nFurthermore, to enhance geometric smoothness of the 3D head and the robustness of our model, we\nemploy point maps predicted from a pre-trained transformer (Wang et al., 2025b) as geometry su-\npervision. We train the network on large scale multi-view head video datasets in an end-to-end way.\nOur method allows real-time animation of Gaussian head avatars created in a feed-forward manner,\nwith reconstruction in less than one second. Extensive experiments also show that FastGHA out-\nperforms Avat3r and other state-of-the-art approaches in both reconstruction fidelity and animation\nefficiency.\nSpecifically, we offer the following contributions:\n\u2022 We propose FastGHA, a framework for generalizable 3D Gaussian head avatar reconstruc-\ntion from few-shot images with measurable quality improvement over current state-of-the-\nart methods;\n\u2022 We achieve real-time animation by introducing a lightweight MLP conditioned on extended\nper-Gaussian features to learn fast pixel-wise 3D Gaussian deformations;\n\u2022 We formulate a geometry prior from VGGT as a regularization loss during training to im-\nprove the 3D head consistency and robustness.\n2\nRELATED WORK\nAnimatable 3D Head Avatars. Generating animatable 3D head avatars from a few input images is\na challeging task that typically requires accurate head geometry reconstruction and a robust method\nfor capturing dynamic expressions. Early works (Khakhulin et al., 2022; Xu et al., 2020) rely on\nparametric face models such as 3DMM to reconstruct and animate human heads. These methods\nare limited by rendering quality and fail to produce realistic animation. With the advent of Neural\n2\nRadiance Fields (NeRFs) and subequent 3D Gaussian Splatting (3DGS), high-quality avatar recon-\nstruction with real-time rendering became feasible. Most approaches (Gafni et al., 2021; Gao et al.,\n2022; Hong et al., 2022; Ki et al., 2024; Ma et al., 2024; Xu et al., 2024b; Qian et al., 2024; Gieben-\nhain et al., 2024; Dhamo et al., 2024; Tretschk et al., 2021; Wang et al., 2025a; Zielonka et al.,\n2025) require expensive training on a long monocular or multi-view video sequences for each sub-\nject, making them impractical in many scenarios. Some methods (Taubner et al., 2025; Zhou et al.,\n2025; Tang et al., 2025; Yin et al., 2025; Gu et al., 2025) bypass the extensive data requirement,\nby first generating novel-views from a limited set of images, and then optimizing a Gaussian avatar\nusing these synthetic observations. This approach, however, does not solve the long optimization\ntime for new characters. Another line of research (Zheng et al., 2025; Sun et al., 2023; Tang et al.,\n2023; Xu et al., 2025) leverages large-scale video datasets to learn 3D head priors by training gener-\native avatar models. During inference time, an identity code is fitted to a new, unseen person. This\nprocess is time-consuming, thus restricting the set of possible application scenarios. Recently, sev-\neral works (Kirschstein et al., 2025; Gong et al., 2025; He et al., 2025; Chu & Harada, 2024; Zhao\net al., 2024; Chu et al., 2024) explored feedforward avatar reconstruction in a few- or single-shot\nsetting. Most single-shot methods trained on large monocular video datasets (Gong et al., 2025; He\net al., 2025; Gafni et al., 2021) lack 3D consistency and focus primarily on the frontal view. Most\nrecently, Avat3r (Kirschstein et al., 2025) introduced an animatable, feedforward Gaussian recon-\nstruction model based on four arbitrary views. However, the inaccurate geometry predicted by a\ngeneral reconstruction model is directly injected into the network through skip-connections, which\nnegatively affects the reconstruction fidelity. Moreover, they use cross attention blocks to model\nfacial animation from expression codes, resulting in more computation and slow rendering speed.\nSpecifically, it is not possible to achieve real-time novel animation with their architecture. In this\npaper, we employ a similar feedforward paradigm as Avat3r (Kirschstein et al., 2025) and support an\narbitrary number of input views. We use a large reconstruction model, VGGT (Wang et al., 2025b),\nas a geometry prior during training, avoiding the error propagation during inference. We also de-\nsign a lightweight MLP to model Gaussian deformations from expression codes, enabling real-time\nanimation with high fidelity.\n2D Facial Animation. In recent years, the development of deep generative models has led to rapid\nadvancements of photoreal 2D facial animation methods. Early work (Qiao et al., 2018; Siarohin\net al., 2019; Wang et al., 2021; Tewari et al., 2020) take GANs (Goodfellow et al., 2020; Karras\net al., 2020; 2019) as the backbone for motion generation. Though computationally efficient, these\nmethods lack diversity and suffer from poor generalization to occluded areas and restricted expres-\nsiveness for intricate facial dynamics. Recently, diffusion-based generative models (Song et al.,\n2021a;b; Wang et al., 2024a) have demonstrated remarkable capabilities and superior quality in\nmany image generation tasks. Large pre-trained models, such as Stable Diffusion (Rombach et al.,\n2022), have spurred numerous applications leveraging their robust model priors. Early works (Guo\net al., 2024; Hu, 2024; Xu et al., 2024c; Guan et al., 2024) explored the animation task by extending\nthe pre-trained model from image generation to video generation by introducing temporal compo-\nnents. DreamPose (Karras et al., 2023) introduces a dual clip-image encoder to animate a person\ngiven a set of input poses. Other methods (Guan et al., 2024; Xu et al., 2024c; Hu, 2024) resort to\na ReferenceNet to generate full body animation based on the rough joint estimates. Video diffusion\nmodels (Ho et al., 2022) now emerged as a backbone for high-quality facial animation, with explicit\nmodeling of temporal correlations to produce smooth and photorealistic videos from prompts or\ngiven driving frames. However, these methods (Cui et al., 2025; Blattmann et al., 2023; Yu et al.,\n2025) come with high computational cost, limiting their efficiency in real-time scenarios. Moreover,\nsince they operate in the 2D domain without explicit 3D constraints, these methods struggle with\ngeometric artifacts under large view changes. In contrast, our approach reconstructs an explicit 3D\nGaussian avatar, which enables controllable animation under arbitrary viewpoints and expressions\nin real time.\n3\nFASTGHA METHOD\nAs illustrated in Figure 2, the goal of our model is to generate a high-fidelity and animatable head\navatar from few-shot input views of a target subject using dynamic 3D Gaussians controlled by\nexpression codes (Section 3.1). We first reconstruct a canonical Gaussian point cloud from the\ninput images and their camera parameters. Here, \u201ccanonical\u201d means an un-posed Gaussian head\nwith little to no facial expression, although we do not supervise this intermediate output. Note\n3\n: Per Gaussian Feature\n: Gaussian Attributes\nInput Images\nMulti-view\nTransformer\nPer-Pixel Gaussian\nRendered Frames\nRendering\nCanonical Gaussian\nCalib./Esti. Cams\nVAE\nEncoder\nDINO\nEncoder\nc\nExpression Code\nGaussian Decoder\nGaussian Reconstruction\nAnimation / Rendering\nDeform.\nMLP\nc\n: Concatenation\nAnimated Gaussian\nFigure 2: Overview of our method. Given a few input images with arbitrary views and expressions,\nwe first extract multi-view features with pre-trained models and then train a multi-view transformer\nnetwork that projects these features into 3D to reconstruct a canonical Gaussian head avatar. To\nenable real-time animation, we introduce a lightweight MLP that deforms the Gaussians according\nto the expression code.\nthat the input images can portray any camera view or facial expression, as long as the views are\nwell-distributed, without requiring a controlled multi-view capture system. Then, we introduce\na lightweight multi-layer perceptron (MLP), which deforms the canonical Gaussians according to\nFLAME expression codes (Li et al., 2017) to handle the facial dynamics in real time. To achieve\nthis feed-forward prediction, we train our model on a large multi-view and multi-expression dataset\nusing a combination of photometric loss and a geometric loss that employs a large foundation model\nVGGT (Wang et al., 2025b), see Section 3.2.\n3.1\nANIMATABLE GAUSSIAN HEAD AVATARS\n3D Gaussian Representation.\nIn 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023), the scene is\nrepresented by a set of Gaussian ellipsoids G, each defined by its position X, multi-channel color C,\nrotation Q, scale S and opacity \u03b1. Given camera parameters \u00b5, these Gaussians can be rasterized\nand rendered (collectively denoted as R) into novel-view images I. This process can be described\nas:\nG = {X, C, Q, S, \u03b1},\nI = R(G, \u00b5).\n(1)\nIn order to faciliate the learning of Gaussian dynamics driven by expression codes, we augment the\ndefault Gaussian representation with a per-Gaussian feature f \u2208RN\u00d732:\nGf = {X, C, Q, S, \u03b1} \u222a{f}.\n(2)\nThese additional features encode high-level semantic information that guide the update of Gaussian\nattributes during animation and are used below to deform the Gaussians for novel expressions.\nFeature Extraction and Reconstruction.\nDifferent from person-specific optimization-based ap-\nproaches (Xu et al., 2024b; Giebenhain et al., 2024; Dhamo et al., 2024), our model aims for feed-\nforward reconstruction, enabling fast head avatar reconstruction. Thus, instead of using template\nGaussians rigged to a 3DMM (Li et al., 2017; Gerig et al., 2018) face template, we draw inspira-\ntion from the recent success of large reconstruction models (LRM) (Xu et al., 2024a; Zhang et al.,\n2024b; Tang et al., 2024; Kirschstein et al., 2025) and directly predict Gaussian primitives for each\npixel of the input images, see Figure 2\u2013Reconstruction. Following the paradigm of a LRM, we first\nextract per-view features using pre-trained models, then apply a Vision Transformer (ViT) to capture\nmulti-view correspondence and a decoder to regress Gaussian attributes.\nSpecifically, we adopt the VAE from SD-Turbo (Sauer et al., 2024) for color features and DI-\nNOv3 (Sim\u00b4eoni et al., 2025) for semantic features to encode RGB images Iin \u2208RV \u00d73\u00d7H\u00d7W\ninto low-dimensional latent features, where V represents the number of input images. Camera poses\nare represented as Pl\u00a8ucker ray maps following Zhang et al. (2024a;b), which takes the form of\nimage-like 2D data. The pre-trained encoder weights are frozen during training to ensure stable\nand semantically meaningful features. For DINOv3, we apply extra convolutional layers to fuse the\n4\nfeatures derived from different layers and obtain a single feature map Fdino that matches the spatial\ndimension of the VAE feature map Fenc. We refer to the supplementary material for details on the\ndifferent feature layers. Then, we concatenate the image features and the Pl\u00a8ucker ray coordinates\nmap Pcam:\nV = [Fdino; Fenc; Pcam] \u2208RV \u00d7(Cd+Ce+6)\u00d7Hf \u00d7Wf ,\n(3)\nwhere [\u00b7; \u00b7] denotes channel-wise concatenation, Cd, Ce denote the channel dimensions of the DI-\nNOv3 and VAE feature maps respectively, and Hf = H/8, Wf = W/8. The concatenated latent V\nis fed into a Multi-view Transformer consisting of multiple self-attention layers to integrate cross-\nview cues and model geometric correspondences. Here, we follow the implementation of GRM (Xu\net al., 2024a), where each self-attention layer operates on V \u00d7Hf \u00d7Wf tokens and outputs a feature\nvector with the same length. The resulting latent tokens \u03c4 are then decoded via a modified SD-Turbo\nVAE decoder, where the input and output channels are expanded to accommodate \u03c4 and regress the\nGaussian attributes while keeping the pre-trained inner layers as initialization. During training, we\nfine-tune the entire decoder. The decoder outputs a set of Gaussian image maps that contain per-\npixel 3D Gaussian parameters. We regress the map for each input view, where the 3D Gaussians\nrepresent the head in a learned canonical expression. By design, the canonical head is devoid of ex-\npression and thus does not correspond to any of the input expressions, allowing animation to happen\ndownstream. These maps are fused into a single canonical 3D Gaussian head, Gc\nf.\nGaussian Head Animation.\nTo achieve real-time animation of pixel-wise Gaussians, we lever-\nage a Multi-layer-Perceptron (MLP) D to model the facial dynamics based on FLAME expression\nparameters. Specifically, D takes a FLAME expression code zexp of the desired target expression\nalong with the canonical Gaussian head Gc\nf including learned per-Gaussian features and predicts\noffsets \u03b4z for the Gaussian attributes:\n\u03b4z = D(Gc\nf, zexp).\n(4)\nSpecifically, we predict expression-specific offsets to the position X and color C of the Gaussians.\nNote that D acts independently on each Gaussian point, enabling efficient and parallelizable defor-\nmation. The deformed Gaussians can then be rendered to get the final image Iout, alpha mask Mout\nand depth map Dout (used for supervision) at arbitrary camera views through the differentiable\nrasterizer R.\n3.2\nLOSS FUNCTION\nWe supervise the training with photometric losses on a set of novel view images (see Section 4.1 for\ndetails). Our losses include an RGB loss and a SSIM loss:\nLRGB = ||Iout \u2212Igt||1,\nLSSIM = SSIM(Iout, Igt).\n(5)\nWe also use a perceptual loss (Zhang et al., 2018b; Johnson et al., 2016) as well as a silhouette loss\nto encourage the 3D shape consistency:\nLperc = V GG(Iout, Igt),\nLsil = ||Mout \u2212Mgt||1.\n(6)\nRecent work (Kirschstein et al., 2025; Shi et al., 2025; Jiang et al., 2025) has shown the benefit\nof including a geometry prior predicted by a 3D geometry model (Wang et al., 2025b; 2024b) for\nGaussian-based reconstruction. Therefore, we introduce a geometric loss derived from a pre-trained\nVGGT (Wang et al., 2025b) backbone. Different from Avat3r (Kirschstein et al., 2025), which\ndirectly takes the predicted point map as input, we incorporate the geometry prior as a regularization\nloss during training.This approach alleviates the requirement of the network to explicitly compensate\nfor discontinuities and artifacts in the prior. To ensure the consistency between the geometry prior\nand our Gaussian avatar, we first align the VGGT point cloud with the ground truth 3D face mesh\n(more details are presented in the supplementary document). Then, the aligned points are reprojected\nby camera \u00b5 to infer a depth map, which serves as the ground-truth Dgt. The geometric loss is\nformulated as:\nLgeo = ||Dout \u2212Dgt||1.\n(7)\n5\nTable 1: Quantitative comparison with state-of-the-art methods on Ava-256 and Nersemble datasets.\nMethod\nAva-256\nNeRSemble\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nCSIM \u2191\nAKD \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nCSIM \u2191\nAKD \u2193\nInvertAvatar\n14.2\n0.36\n0.55\n0.29\n15.8\n15.1\n0.47\n0.46\n0.45\n8.2\nGPAvatar\n19.1\n0.70\n0.32\n0.26\n6.9\n20.8\n0.76\n0.30\n0.54\n6.3\nAvat3r\n20.7\n0.71\n0.33\n0.59\n4.8\n-\n-\n-\n-\n-\nOurs (Ava-256)\n22.2\n0.76\n0.24\n0.71\n4.8\n22.3\n0.78\n0.27\n0.70\n4.8\nOurs (both)\n22.5\n0.77\n0.23\n0.73\n4.8\n24.0\n0.81\n0.24\n0.77\n4.4\nOverall, the total loss function is formulated as:\nL = LRGB + \u03bbSSIMLSSIM + \u03bbpercLperc + \u03bbsilLsil + \u03bbgeoLgeo,\n(8)\nwhere \u03bb denoting the weights of each term, and are set as follows: \u03bbSSIM = 1, \u03bbsil = 1, \u03bbperc =\n0.5 and \u03bbgeo = 0.5.\n4\nEXPERIMENTS\nIn the following, we start by describing our training data, implementation details and comparison\nbaselines in Section 4.1. We then demonstrate our results, comparisons and analysis in Section 4.2.\nAblation studies that justify our main design decisions are given in Section 4.3, and we demonstrate\napplications of our method in Section 4.4.\n4.1\nEXPERIMENTAL SETUPS\nDataset. Our model is trained on a combination of two multi-view video datasets, Ava-256 (Mar-\ntinez et al., 2024) and Nersemble (Kirschstein et al., 2023). The Ava-256 dataset contains 256\nsubjects captured with 80 cameras, from which we use only 40 cameras with frontal viewpoints\nfor training. The Nersemble dataset has a total of 425 subjects with 16 cameras. For each video\nsequence, we first crop the image to 512 \u00d7 512 resolution, remove the background (Lin et al., 2021)\nand then employ off-the-shelf head tracking tools (Zielonka et al., 2022; Qian et al., 2024) to obtain\nFLAME expression weights Li et al. (2017) with jaw and eye poses as the per-frame expression\ncodes zexp. Similar to Kirschstein et al. (2025), in each training iteration, we randomly sample four\nimages of the same subject with different expressions and camera poses as input to the network. We\nadditionally sample eight supervision images with the same expression, which is potentially differ-\nent from the four input images. From these eight supervision images, four match the input camera\nviews and four are from novel views, to provide multi-view and novel-expression supervision.\nImplementation details. We train the network using the ADAM optimizer (Kingma & Ba, 2014)\nwith a learning rate of 5e\u22125, except for the MLP D, where we use a higher learning rate of 5e\u22124.\nThe training takes roughly four days for 400k steps on four H800 GPUs with a batch size of 1 on\neach GPU. During inference, the first reconstruction part of our network only needs to be performed\nonce for a given set of input images and takes less than one second. After that, our method achieves\nreal-time animation and rendering on a single H800 GPU.\nBaselines.\nWe compare our approach with other state-of-the-art methods including InvertA-\nvatar (Zhao et al., 2024), GPAvatar (Chu et al., 2024) and Avat3r (Kirschstein et al., 2025), which\nall reconstruct 3D head avatars from few-shot input images in a single forward pass. InvertAvatar\nuses 3D GAN inversion for head avatar reconstruction. GPAvatar is based on the tri-plane represen-\ntation (Chan et al., 2022) and uses a FLAME point cloud as prior to sample 3D points for animation.\nAvat3r is the most related to our work, as it directly regresses pixel-wise 3D Gaussians from the\nimages. However, Avat3r incorporates the expression parameters directly into the cross-attention\nmodel for reconstruction, which comes at the cost of a slow animation speed of only 8 fps.\nMetrics. To measure the image quality, we employ Peak Signal-to-Noise Ratio (PSNR), Struc-\ntural Similarity Index (SSIM) (Wang et al., 2004) and Learned Perceptual Image Patch Similarity\n(LPIPS) (Zhang et al., 2018a) between the synthesized images and the ground truth images. We then\nuse ArcFace (Deng et al., 2019) to compute the cosine similarity of the identity features for identity\n6\nFigure 3: Qualitative reconstruction comparison on Ava-256 held-out subjects.\nFigure 4: Qualitative reconstruction comparison on Nersemble held-out subjects.\nsimilarity (CSIM). Furthermore, we leverage Average Keypoint Distance (AKD) based on a facial\nlandmark detector (Jin et al., 2021) to evaluate the reconstruction and motion accuracy.\n4.2\nMAIN RESULTS\nQuantitative Comparisons. We quantitatively evaluate our FastGHA method on a reconstruction\ntask using held-out data from both the Ava-256 and Nersemble datasets. Results are shown in Tab 1,\ncomparing our method to the baselines across all metrics. Note that, for both InvertAvatar and\nGPAvatar, we use the official released code for comparison. For Avat3r, the code is not yet released\nbut we thank the authors for providing their results on Ava-256 for us to compare to (unfortunately,\ntheir results on Nersemble data were not available).\nThe held-out data for the Ava-256 comparison includes 1000 different expressions from 12 subjects\nfrom different viewpoints. For Nersemble, we sampled 600 images from 4 subjects. We evaluate\nall methods at 512 \u00d7 512 pixels resolution, and for all results, we use four input images. For a fair\ncomparison to Avat3r, which only used Ava-256 for training, we also train one version of our model\non only the Ava-256 dataset (without Nersemble) and then show two separate results, one with only\nAva-256 training data and one with both Ava-256 and Nersemble training data (labelled \u201cboth\u201d in\nthe table and figures) to compare against the other methods.\nAs shown in the table, our FastGHA method outperforms the baseline state-of-the-art approaches\nwith a large gap in reconstruction quality (PSNR, SSIM, LPIPS) and identity preservation (CSIM),\nwith equal or better performance on facial keypoint similarity (AKD). With both Ava-256 and\n7\nFigure 5: Analysis on the different number of input views. Two inputs means the first two in the top\nrow, three inputs means the first three, four inputs adds the first image in the bottom row, and so on.\nTable 2: Running time with different number of input images of our method. All results exclude the\ntime for obtaining camera and expression parameters that can be calculated in advance.\nNumber of Input\n2\n3\n4\n5\n6\nReconstruction Time (s)\n0.68\n0.83\n0.98\n1.15\n1.40\nAnimation Speed (FPS)\n128\n83\n62\n48\n32\nNersemble datasets in training, our method performs better than when training on Ava-256 alone,\ndemonstrating that we can improve generalization by training on more data.\nQualitative Comparisons. We also show visual comparisons of held-out subject reconstructions\nusing FastGHA and the baseline methods. Three subjects from the Ava-256 test set are shown in\nFigure 3, and three from Nersemble are shown in Figure 4. Note, again, that we do not have Avat3r\nresults for the Nersemble test set.\nThis qualitative comparison shows that InvertAvatar and GPAvatar are unable to produce high fi-\ndelity results. Among the previous methods, only Avat3r can recover plausible photoreal avatars;\nhowever, it struggles with the identity recovery and does not match the target expression as well as\nour method. Important details like the hairstyle, skin texture and jewelry (earrings and nose rings)\nare better recovered by our FastGHA.\nAnalysis on the Number of Input Views. Although our method is trained and mostly evaluated\nwith four input views, our architecture supports any number of input images. Figure 5 illustrates the\nreconstruction quality for a variety of input views, from two to six. Note how the overall reconstruc-\ntion of the avatar is agnostic to the exact number of inputs, and only fine-scale details are missing\nwhen we have only sparser views. In particular, using the first two input images only, the subject\u2019s\nleft-side of the face is unseen, and details are hypothesized by our method. Already by adding the\nthird input view, left-side details are recovered.\nWe also measured the reconstruction time and real-time animation speed given the different number\nof inputs, as shown in Table 2. Because we infer per-pixel Gaussian attributes, the number of\nGaussians and thus the reconstruction time both increase rather linearly with the number of input\nviews, and the animation speed decreases accordingly, although still within real-time frame-rates\neven for six input images.\nAnalysis of Input Expressions. Since our model is trained with arbitrary input views and expres-\nsions, we analyze the impact of different input expressions on the reconstructed canonical Gaussian\nand animated Gaussian head in Figure 6. There are slight differences in the animated results caused\nby the variation in input expressions. Nevertheless, the model consistently produces accurate expres-\nsions and stable animations even when the four input views have significantly different expressions.\nMoreover, even though our method is trained end-to-end without explicit supervision on the canoni-\ncal Gaussian, we observe that our model is able to disentangle identity and expression, by regressing\nthe Gaussian avatar in a learned canonical representation. Specifically, when the four input images\nhave different expressions, the model tends to learn a \u201cmean\u201d canonical Gaussian that averages the\nvariations. While when all input views share similar expressions, the canonical Gaussian would\npreserve that expression to some extent but tends to be close to a neutral face.\n8\nFigure 6: Visualization of the canonical and animated Gaussian head with different input expres-\nsions.\nFigure 7: Ablation study showing the importance of each of our design decisions.\n4.3\nABLATION STUDY\nWe perform quantitative and qualitative ablations on several of our design decisions. The ablations\nare performed on the Ava-256 dataset. Note that the test set used for the ablation study differs from\nthe one used in the main results, where the input images all share the same expressions as in Avat3r.\nThe ablations are conducted on a separate test set where input images are with different expressions,\nwhich aligns with our training paradigm. Quantitative results are shown in Table 3, with qualitative\nvisual comparisons given in Figure 7 and the supplemental video. Specifically, we evaluate the\nfollowing alternative strategies for our network and training loss:\nw/o VAE weights. In this version we do not use a pre-trained SD-Turbo VAE for our (frozen)\nencoder and (fine-tuned) decoder, but instead train the encoder and decoder from scratch using the\nsame architecture. This experiment is inspired by Avat3r, who train their own encoder/decoder.\nSuperior rendering quality is achieved by including the pre-trained weights as we do in our method.\nw/o Lgeo. Here we show that without Lgeo based on VGGT in our training loss function, we have\nworse 3D consistency and artifacts in the reconstruction.\nw/o per-Gaussian features. Removing the learned per-Gaussian features from our network largely\naffects the animation accuracy (please refer to the supplemental video).\nSapiens features. We also trained a version of our network using Sapiens features (Khirodkar et al.,\n2024) in place of DINOv3, following Avat3r. Using the DINOv3 features provides better results.\n4.4\nAPPLICATIONS\nExtreme Poses and Expressions. We first showcase the capability of our method by animating and\nrendering the head avatar in extreme expressions and large view angles in Figure 8. In particular, we\nrender novel views reaching 90\u00b0, even though such poses lie far outside the distribution of the input\nimages.\n9\nTable 3: Ablation results on the Ava-256 dataset.\nMethod\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nCSIM \u2191\nAKD \u2193\nw/o VAE weights\n20.789\n0.738\n0.251\n0.681\n5.487\nw/o Lgeo\n21.132\n0.741\n0.239\n0.687\n5.049\nw/o per-Gaussian features\n21.053\n0.740\n0.245\n0.690\n5.216\nSapiens features\n21.081\n0.741\n0.241\n0.689\n5.021\nOurs\n21.274\n0.745\n0.237\n0.704\n4.996\nFigure 8: Animation under large poses and expressions.\nIn-the-wild Results. We end demonstrating our method on the practical application of reconstruct-\ning 3D Gaussian avatars from candid photos in-the-wild. Different examples are shown in Figure 1.\nOur model supports portrait images captured from phone scans, monocular video frames, and images\ngenerated from a multi-view diffusion model. Even though the input images are outside the distri-\nbution of lighting and camera angles from the in-studio training datasets, our method has learned to\ngeneralize and creates realistic avatars that can be animated and rendered from novel views.\n5\nDISCUSSION\nIn this work we present FastGHA, a new feed-forward method to build a high-quality Gaussian\navatar from a sparse number of input views. Our experiments have shown that our novel network\ndesign improves over both quality and efficiency compared to current state-of-the-art methods. In\nparticular, FastGHA allows avatar reconstruction in under one second, followed by real-time anima-\ntion and rendering, and can be applied to in-the-wild images.\nLimitations. Our method shares similar limitations as Avat3r, in that we rely on accurate camera\nparameters as input, and our reconstructed avatars retain the same shading as the input images.\nAdditionally, for our expression code parameterization we chose the FLAME expression space, as it\noffers intuitive artistic control over the resulting avatars. Unfortunately it does not allow to control\nthe tongue motion. Other expression parameterizations like the one from Ava-256 could be used,\nbut are less easily controllable for novel animation.\nEthical Considerations. Given only a small number of images, our method allows users to generate\nsynthetic animations of a person. This technology should only be used with the consent of the\nsubject. We do not support any unethical use of our method.\n6\nREPRODUCIBILITY STATEMENT\nWe strive to provide all the details required to reproduce our approach, including the architecture\noverview (Section 3.1), loss function details (Section 3.2), ablations on alternate variations (Sec-\ntion 4.3), and fine-level architecture details (in supplemental document).\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "**Summary of the Paper \"FASTGHA: GENERALIZED FEW-SHOT 3D GAUSSIAN HEAD AVATARS WITH REAL-TIME ANIMATION\"**\n\n### Main Contributions\n- **FastGHA Framework**: Proposes a novel feed-forward architecture for generating high-quality Gaussian head avatars from few-shot input images, enabling real-time animation.\n- **Efficient Animation**: Introduces lightweight MLP-based networks to predict 3D Gaussian deformations from expression codes, supporting fast and efficient avatar animation.\n- **Geometry Supervision**: Employs point maps predicted from a pre-trained large-scale reconstruction model as geometry supervision to enhance geometric smoothness of the 3D head.\n\n### Methodology\n1. **Input Images and Multi-view Features**: The approach leverages multi-view features extracted using pretrained models (VAE, DINOv3) for input images.\n2. **Canonical Gaussian Reconstruction**: A canonical Gaussian representation is learned from input images to facilitate real-time animation by mapping inputs to a neutral Gaussian head before applying deformations.\n3. **Expression Code-Based Deformation**: A lightweight MLP network deforms the canonical Gaussians based on expression codes, enabling real-time and high-fidelity animation.\n4. **Geometry Prior**: Uses point maps from a pre-trained large-scale reconstruction model as geometry supervision to improve 3D consistency and robustness of the avatar.\n\n### Key Findings\n- **Performance Improvement**: Experiments show that FastGHA significantly outperforms existing methods in both rendering quality (PSNR, SSIM, LPIPS) and inference efficiency.\n- **Real-Time Animation**: The model can generate avatars with real-time animation capabilities, achieving speeds up to 128 FPS on a single H800 GPU.\n\n### Conclusions\n- FastGHA offers a novel framework for generating high-fidelity Gaussian head avatars from few-shot input images and supports real-time animation. It achieves superior performance in terms of quality and efficiency compared to existing methods.\n- The approach is scalable, allowing for fast reconstruction times (less than one second) and efficient inference, making it suitable for practical applications such as creating realistic avatars from uncontrolled data.\n\n### Limitations\n- **Camera Parameters**: Relies on accurate camera parameters input, which may limit its applicability in certain scenarios.\n- **Expression Control**: The method uses a specific expression space (FLAME), limiting the control over tongue motion and novel animations.\n\nThis summary encapsulates the main contributions, methodology, key findings, and conclusions of the paper \"FASTGHA: GENERALIZED FEW-SHOT 3D GAUSSIAN HEAD AVATARS WITH REAL-TIME ANIMATION\"."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "DISCRIMINANT LEARNING-BASED COLORSPACE FOR BLADE SEGMENTATION\nRa\u00a8ul P\u00b4erez-Gonzalo1,2, Andreas Espersen2 and Antonio Agudo1\n1Institut de Rob`otica i Inform`atica Industrial, CSIC-UPC, Barcelona, Spain\n2Wind Power LAB, Copenhagen, Denmark\nABSTRACT\nSuboptimal color representation often hinders accurate image\nsegmentation, yet many modern algorithms neglect this crit-\nical preprocessing step. This work presents a novel multi-\ndimensional nonlinear discriminant analysis algorithm, Col-\norspace Discriminant Analysis (CSDA), for improved seg-\nmentation. Extending Linear Discriminant Analysis into a\ndeep learning context, CSDA customizes color representation\nby maximizing multidimensional signed inter-class separabil-\nity while minimizing intra-class variability through a general-\nized discriminative loss. To ensure stable training, we intro-\nduce three alternative losses that enable end-to-end optimiza-\ntion of both the discriminative colorspace and segmentation\nprocess. Experiments on wind turbine blade data demonstrate\nsignificant accuracy gains, emphasizing the importance of tai-\nlored preprocessing in domain-specific segmentation.\nIndex Terms\u2014 Image Preprocessing, Colorspace Opti-\nmization, Deep Discriminant Analysis, Blade Segmentation.\n1. INTRODUCTION\nImage segmentation partitions an image into homogeneous\nregions and represents a fundamental problem in image pro-\ncessing. Among visual features, color is crucial in achieving\ncomprehensive image representation and reliable segmenta-\ntion [1, 2]. Consequently, selecting an appropriate colorspace\nis vital to achieve high segmentation performance.\nOver the past decade, deep-learning techniques have\ndriven significant progress in image segmentation [3, 4],\nincluding domain-specific industry solutions [5]. Most ap-\nproaches rely on encoder\u2013decoder architectures [6, 7] and at-\ntention mechanisms for enhanced performance [8, 9]. Recent\nmodels target efficiency in resource-constrained settings [10,\n11], while zero-shot methods such as SAM [12] reflect the\nversatility of transformer-based segmentation [13, 14].\nDespite these advances, segmentation algorithms rarely\noptimize the underlying colorspace for domain-specific\nneeds.\nTraditional spaces such as RGB are designed for\ngeneral-purpose imaging [15] and may fail to capture subtle\nThis work has been partially supported by the project GRAVATAR\nPID2023-151184OB-I00 funded by MCIU/AEI/10.13039/501100011033\nand ERDF, UE; and by GreenVAR project of the Fundaci\u00b4on Ram\u00b4on Areces.\nFig. 1: CSDA framework. Joint optimization of colorspace\nand segmentation via discriminant and probability losses.\ncues critical in specialized contexts. Moreover, hybrid spaces\nthat combine components of standard colorspaces [16, 15, 17]\nalso lack class-specific discriminability.\nThis paper addresses this gap by introducing a domain-\nspecific colorspace tailored for the target application. With-\nout loss of generality, we focus on wind turbine blade seg-\nmentation, where accurately isolating blades from the back-\nground is key for downstream defect detection and assess-\nment tasks [18, 19, 20]. This focus is motivated by typical\nfailure cases, where shadowed blade regions appear with in-\ntensities opposite to those illuminated, leading to segmenta-\ntion errors [5]. Inspired by prior work in face recognition\nthat uses class-based color information for discriminant anal-\nysis [21, 22, 23], we build on Linear Discriminant Analy-\nsis (LDA) [24] to develop ColorSpace Discriminant Analysis\n(CSDA). CSDA is a learnable nonlinear transformation that\nintegrates discriminant colorspace learning into an end-to-end\nsegmentation framework, outlined in Fig. 1. A neural network\njointly optimizes colorspace representation and segmentation,\nenhancing class separability in the learned embedding.\nCompared with existing face recognition methods, CSDA\nproposes a multidimensional, nonlinear setting via deep dis-\ncriminant analysis (DDA) [25, 26]. Our formulation intro-\nduces a signed between-class variance matrix and avoids\nmatrix inversion through three alternative CSDA losses, im-\nproving training stability and performance.\nExperimental\nevidence demonstrates that CSDA offers a robust and in-\nterpretable solution for colorspace optimization, achieving\nsuperior performance over current state-of-the-art baselines.\n2. COLORSPACE DISCRIMINANT ANALYSIS\nWe propose a model that transforms input images into a new\ncolorspace where blade and non-blade classes are maximally\nseparable. Our approach builds on LDA [24], which seeks a\nlinear projection that maximizes class separability based on\nthe Fisher criterion: the ratio of between-class sb to within-\narXiv:2601.13816v1  [cs.CV]  20 Jan 2026\nclass variance sw. In the two-class case, Ci with i \u2208{0, 1},\nthis is defined as:\nmax sb\nsw\n= (\u00b5C0 \u2212\u00b5C1)2\ns2\nC0 + s2\nC1\n,\n(1)\nwhere \u00b5Ci and s2\nCi denote the class mean and variance, re-\nspectively. We extend this principle to a multidimensional set-\nting and further enhance discriminative power through non-\nlinear transformations optimized via gradient descent. The\nresulting CSDA framework integrates these ideas into an end-\nto-end model.\n2.1. Multidimensional Discriminant Analysis\nLDA transforms the original feature space into a lower-\ndimensional one that emphasizes class separability, facilitat-\ning the classification of the classes. The dimensionality of\nthe projected space is at most C \u22121, where C is the number\nof classes. In a binary classification, this results in a single\nprojection line. However, this reduction in dimensionality\nmay discard useful discriminative information.\nTo address this, we propose an extended multidimensional\nLDA criterion that maximizes the trace of the between-class\nscatter Sb relative to the within-class scatter Sw:\nmax Tr\n\u0000S\u22121\nw Sb\n\u0001\nwith\n(\nSb = (\u00b5C0 \u2212\u00b5C1)(\u00b5C0 \u2212\u00b5C1)\u22a4\nSw = S2\nC0 + S2\nC1\n,\nwhere \u00b5Ci and S2\nCi denote the mean vector and covariance\nmatrix of class Ci, respectively. This criterion seeks to en-\nhance class separability in a multidimensional space, retain-\ning discriminative information for classification.\n2.2. Extending LDA to a Nonlinear Setting\nThe multidimensional Fisher criterion from Eq. (2) is invari-\nant to any nonsingular linear transformation W.\nThus, if\nclasses are not separable in the original space, linear map-\npings cannot improve class separability, due to the trace re-\nmains unchanged under such transformations:\nTr\n\u0000(W\u22a4SwW)\u22121(W\u22a4SbW)\n\u0001 = Tr\n\u0000(W\u22121S\u22121\nw (W\u22a4)\u22121W\u22a4SbW\n\u0001\n= Tr\n\u0000W\u22121S\u22121\nw SbW\n\u0001 = Tr\n\u0000WW\u22121S\u22121\nw Sb\n\u0001 = Tr\n\u0000S\u22121\nw Sb\n\u0001 . (2)\nThis motivates the need for nonlinear approaches, which\ncan enhance discriminative power while preserving multidi-\nmensionality. In this setting, analytical optimization is in-\ntractable, so we resort to gradient-based methods. However,\ndirectly optimizing the Fisher criterion (Eq. (2)) can lead to\ninstability: the gradient of the between-class variance \u2207Sb\nmay alternate in sign across training steps, as it depends on the\nclass means\u2019 relative positions. Letting u := (\u00b5C0 \u2212\u00b5C1),\nthe sign of \u2207Sb is then given by:\n\u2207(uu\u22a4) = (\u2207u)u\u22a4+ u(\u2207u\u22a4)\n(\n< 0 if u < 0; \u00b5C1 > \u00b5C0\n> 0 if u > 0; \u00b5C0 > \u00b5C1\n. (3)\nTo address inconsistent training updates, we adopt a\nsigned between-class variance to ensure consistent gradient\ndirectionality. Let Dsign(u) be a matrix whose diagonal cap-\ntures the sign of each component of u, denoted by sign(uj),\nand whose off-diagonal elements are one, then we obtain an\nadapted discriminant function to improve class separability\nin a nonlinear, multidimensional setting:\nmax Tr\n\u0000S\u22121\nw Dsign(\u00b5C0 \u2212\u00b5C1) \u2299Sb\n\u0001\n, with\n(4)\nDsign(u) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nsign(u1)\n1\n\u00b7 \u00b7 \u00b7\n1\n1\nsign(u2)\n\u00b7 \u00b7 \u00b7\n1\n...\n...\n...\n...\n1\n1\n\u00b7 \u00b7 \u00b7\nsign(udCS)\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8,\nwhere dCS is the dimensionality of the projected colorspace.\nThe signed between-class variance is computed via the\nHadamard product \u2299of Dsign(u) and Sb. This new criterion\nintroduces directionality by encoding class order information\ninto the loss, ensuring a consistent mapping in which, for each\nfeature dimension j \u2208{1, . . . , dCS}, samples from C0 are\nmapped to higher values than those from C1, i.e., y0j > y1j.\n2.3. Colorspace Discriminant Analysis Losses\nTo optimize the colorspace, we propose three alternative loss\nfunctions derived from Eq. (4) that avoid the instability in-\ntroduced by matrix inversion. These losses allow direct op-\ntimization of the colorspace for improved class separability\nin a nonlinear deep network setting, ensuring that the learned\nfeature space maximizes discriminative power across classes.\nCSDA Loss. The first loss adapts Eq. (4) into a mini-\nmization form by retaining both between- and within-class\nvariance in the numerator, avoiding matrix inversion. The\nsigned between-class variance enforces an ordering in which\npositive-class components are consistently mapped above\nnegative-class components, i.e., y1j > y0j:\nLCSDA = Tr\n\u0000SwDsign(\u00b5C0 \u2212\u00b5C1) \u2299Sb\n\u0001\n.\n(5)\nLogarithmic CSDA Loss. An analogous procedure is ap-\nplied to the separability measure Tr(Sb)\nTr(Sw) [27]. We adopt the\nsigned between-class variance and convert this ratio into an\nadditive form via logarithms. Since this signed matrix has\ndCS diagonal terms within [\u22121, 1], we shift the argument by\ndCS + \u03f5 to keep the logarithm well defined. Finally, a scalar\n\u03bbF controls the trade-off between both variance terms as:\nL(ln)\nCSDA = ln\n\u0000dCS + \u03f5 + Tr\n\u0000Dsign(\u00b5C0 \u2212\u00b5C1) \u2299Sb\n\u0001\u0001\n+ \u03bbF ln (dCS + \u03f5 + Tr (Sw)) .\n(6)\nDelta CSDA Loss. Instead of minimizing a ratio, an-\nother effective separability criterion is to optimize the sum\nof between-class separation and within-class compactness\nTr(Sb) + Tr(Sw) [27], providing a simple but effective loss:\nL(\u2206)\nCSDA = Tr\n\u0000Dsign(\u00b5C0 \u2212\u00b5C1) \u2299Sb\n\u0001\n+ \u03bbF Tr (Sw) .\n(7)\ndCS = 1\ndCS = 4\ndCS = 2\ndCS = 5\ndCS = 3\ndCS = 6\nFig. 2: Visualization of colorspace transformations. On\nthe left side, RGB input (top) and ground-truth mask (bot-\ntom). On the right, the transformed image at various dCS for\nCSDA(\u2206) with its corresponding estimated mask on its right;\naccuracy, F1, and mIoU on top. See Sec. 3.1 for details.\nFig. 3: Segmentation performance across model variants\nand colorspace dimensions.\nDetailed descriptions of the\nmodel variants are provided in Sec. 3.2.\n2.4. Model Design\nBoth colorspace and segmentation models use a U-Net [28]\nand are trained end-to-end (see Fig. 1). The colorspace model\ntransforms an RGB image into an optimized representation\nY that serves as input to the segmentation branch, which pre-\ndicts the mask \u02c6M given the ground truth M.\nThe total loss LT = LP + \u03bbP LDDA combines our pro-\nposed discriminant losses LDDA from Sec. 2.3 with a proba-\nbilistic segmentation loss LP , balanced by \u03bbP . Specifically,\nwe adopt the focal loss [29] for LP , which emphasizes low-\nconfidence predictions through exponential weighting.\n2.5. Implementation Details\nInput images are resized to 256 \u00d7 256, min-max normalized,\nand augmented with flipping and cropping. We use Adam\noptimizer with a learning rate of 10\u22124 and batch size of 8.\nOur custom scheduler decreases the learning rate after three\nepochs without validation improvement. Pixel probabilities\n\u02c6m are generated using the sigmoid function. We set \u03f5 = 10\u22128\nin Eq. (6) for stable training. Balancing factors are \u03bbP = 1.3\nfor focal and CSDA losses, and \u03bbF = 0.5 for variance terms.\n3. EXPERIMENTAL RESULTS\nWe assess CSDA via an analysis of the learned colorspaces,\nan ablation on its color preprocessing, and quantitative and\nqualitative evaluations across different windfarms. Our data\nis comprised of images captured during drone turbine inspec-\ntion following [5]. To emulate evaluation on newly acquired\nimages, the test set is sourced from different windfarms.\nFig. 4: Qualitative comparison on test images for DDA,\nFocal and CSDA. First two columns:\ninput image and\nground-truth mask M. Next: estimated segmentation masks\nfrom DDA(\u2206), DDA(ln), Focal, and CSDA(\u2206) with accuracy,\nF1, and mIoU shown above.\n3.1. Visual Exploration of Colorspaces\nFigure 2 illustrates the colorspace transformation across di-\nmensions dCS for CSDA(\u2206). For dCS = 1, the transformed\nimage Y is shown in grayscale; for dCS = 2, as a partial RGB\nimage with a zero-filled channel; and for dCS = 3, as a full\nRGB image. Higher dimensions are visualized by grouping\nchannels into RGB triplets and padding with zeros if needed:\ndCS = 4 yields two partial RGB images, dCS = 5 one full\nand one partial RGB, and dCS = 6 two full RGB images.\nAt low colorspace dimensions, CSDA enhances contrast\nby brightening either the blade or background (e.g., at dCS =\n2, the landscape is bright while the blade is dark). Increas-\ning dimensionality enriches the color palette: at dCS = 3,\nblades appear green against a pink background; at dCS = 6,\nthe background spans orange to green while blades remain\ngray. The model also reduces within-class variance, produc-\ning more uniform blade regions (e.g., the blade becomes con-\nsistently green at dCS = 3). Overall, CSDA adapts images\ninto discriminative palettes where blade and background in-\ntensities are more clearly separated.\n3.2. Ablation Study\nWe show that applying discriminant analysis for colorspace\npreprocessing before segmentation improves accuracy over\nusing either discriminant analysis or segmentation alone. We\ncompare against three baselines: (1) Deep Discriminant Anal-\nysis (DDA): maps RGB input to a 1D feature map (dCS = 1)\nusing only the discriminant loss (LT = LDDA), with sigmoid\noutputs thresholded for validation accuracy; (2) Focal Model:\na standard segmentation model trained directly on RGB im-\nages with focal loss (LT = LP ); and (3) Two U-Net Model:\nCSDA\u2019s architecture but trained only with focal loss.\nFigures 3 and 4 exhibit that combining discriminant anal-\nysis with segmentation consistently boosts performance in the\nMethod\nAccuracy\nPrecision\nRecall\nF1\nmIoU\nIoUC0\nIoUC1\n[%]\n[%]\n[%]\n[%]\n[%]\n[%]\n[%]\nDeepLabv3+ [3]\n94.14\n96.36\n87.38\n89.03\n87.47\n90.31\n84.62\nSW [6]\n93.48\n93.57\n91.71\n91.37\n87.44\n88.64\n86.23\nResNeSt [9]\n94.23\n96.84\n91.47\n92.77\n89.63\n90.40\n88.86\nU-NetFormer [7]\n96.20\n97.31\n93.51\n94.42\n91.75\n92.53\n90.96\nSAM [12]\n94.36\n97.29\n91.22\n92.60\n91.66\n92.31\n91.01\nCLIPSeg [13]\n82.70\n77.02\n75.52\n74.29\n75.09\n80.16\n70.02\nDiffSeg [14]\n96.37\n82.08\n89.74\n85.73\n86.40\n91.66\n81.13\nEfficientFormer [11]\n96.42\n95.47\n93.63\n94.55\n93.51\n94.02\n92.99\nMobileViT [10]\n96.14\n95.44\n93.33\n94.38\n93.47\n94.06\n92.88\nMask2Former [8]\n96.68\n95.63\n93.89\n94.76\n93.72\n94.49\n92.93\nBU-Net [5]\n97.39\n99.42\n93.35\n95.73\n93.80\n94.70\n92.90\nCSDA\n96.98\n96.78\n96.65\n96.13\n93.86\n94.21\n93.51\nCSDA(ln)\n97.40\n95.72\n97.38\n96.17\n93.72\n94.25\n93.18\nCSDA(\u2206)\n97.34\n95.97\n97.30\n96.20\n93.94\n94.51\n93.38\nTable 1: Quantitative comparison on blade segmentation.\nblade domain. Quantitatively, as shown in Fig. 3, CSDA out-\nperforms both DDA and Focal baselines across all metrics, in-\ndependent of the chosen loss. The gain is not architectural, as\nstacking U-Nets performs worst, but stems from effective dis-\ncriminant preprocessing. Qualitative results (Fig. 4) confirm\nthat CSDA better preserves blade contours and reduces back-\nground misclassifications. Finally, Figure 3 shows CSDA is\nrobust to the choice of dCS; we select dCS = 4 for subsequent\nexperiments due to its consistently strong F1 and mIoU.\n3.3. Quantitative Evaluation\nTable 1 compares CSDA models from Sec. 2.3 against com-\npeting segmentation models. CSDA(ln) and CSDA(\u2206) show\nslight improvements over the standard CSDA, benefiting from\nadjustable class variances via \u03bbF . Moreover, they achieve\ntop-tier performance, maintaining balanced precision and re-\ncall. CSDA(ln) matches the highest accuracy alongside BU-\nNet [5], and CSDA(\u2206) achieves the best F1-score and mIoU-\nmetrics that offer robust and consistent evaluation across hy-\nperparameters like dCS, as discussed in Sec. 3.2.\nComputationally, CSDA remains efficient at inference\ntime, as the discriminant losses are only computed during\ntraining.\nWhile CSDA stacks two U-Nets, each U-Net is\nlightweight, with a single model requiring just 0.024s per im-\nage on an NVIDIA RTX 3080 Ti and Intel i9. Thus, CSDA\u2019s\ninference time requires only 0.048s, significantly faster than\nBU-Net [5], the next-best performer, which takes 0.089s.\n3.4. Qualitative Analysis\nWe illustrate the effectiveness of CSDA(\u2206) with dCS = 4 in\nenhancing segmentation. Figure 5 shows the RGB input and\nground-truth mask, followed by the colorspace transforma-\ntion Y and the predicted segmentation. Y is visualized both\nas partial RGB images (using two colorspace channels plus a\nzero channel) and as individual grayscale channels.\nThe discriminative power of the CSDA transformation is\nevident: the blade area becomes more homogeneous due to\nwithin-class variance is reduced, while between-class vari-\nance increases, helping separate the blade from the back-\nground. For example, in the first channel of the third instance,\nCSDA effectively maps the orange spot on the blade to sim-\nFig. 5: Qualitative results for CSDA(\u2206) with dCS = 4.\nFirst two columns: input image and ground-truth mask M.\nThird to fourth column: transformed colorspace image, see\nSec. 3.1. Fifth column: estimated mask. Last columns: indi-\nvidual transformed colorspace channels.\nFig. 6: Boxplot of the segmentation results across diverse\nwindfarms of the test set for CSDA(\u2206) with dCS = 4.\nilar intensities as the rest of the blade, aiding segmentation.\nIn more complex scenes, such as the last example where\nthe blade and a background turbine share similar RGB colors,\nCSDA still isolates the foreground blade accurately, capturing\nthe blade tip, something BU-Net fails to achieve [5].\n3.5. Generalization across Windfarms\nTo assess the robustness of CSDA(\u2206) with dCS = 4, we eval-\nuate its performance across multiple windfarm inspections in-\ncluded exclusively in the test set. Figure 6 illustrates a con-\nsistently high performance across all windfarms, indicating\nstrong generalization to images in our real-world setting.\n4. CONCLUSION\nThis paper presents CSDA, a novel colorspace transforma-\ntion optimized for image segmentation preprocessing. CSDA\nextends LDA by jointly optimizing a multidimensional non-\nlinear colorspace transformation with the segmentation objec-\ntive, emphasizing discriminative color features relevant to the\ntarget domain. To our knowledge, CSDA is the first approach\nto integrate nonlinear discriminant analysis into a learnable\ncolorspace transformation for segmentation. Experimental re-\nsults on a real industry problem demonstrate the effectiveness\nof colorspace optimization in domain-specific segmentation.\nSignificant improvements in wind turbine blade segmentation\nare observed compared to optimizing either segmentation loss\nor discriminant analysis alone. These results underscore the\nvalue of task-specific preprocessing and establish CSDA as a\nrobust tool for domain-adapted segmentation tasks.\n5. REFERENCES\n[1] P. Gupta, S. Saxena, S. Singh, S. Dhami, and V. Singh,\n\u201cColor image segmentation: A state of the art survey,\u201d\nIJCIR, vol. 8, no. 1, pp. 17\u201326, 2012.\n[2] J. Taipalmaa, N. Passalis, and J. Raitoharju, \u201cDifferent\ncolor spaces in deep learning-based water segmentation\nfor autonomous marine operations,\u201d in ICIP, 2020.\n[3] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and\nH. Adam, \u201cEncoder-decoder with atrous separable con-\nvolution for semantic image segmentation,\u201d in ECCV,\n2018.\n[4] E. Sahragard, H. Farsi, and S. Mohamadzadeh,\n\u201cAd-\nvancing semantic segmentation: Enhanced unet algo-\nrithm with attention mechanism and deformable convo-\nlution,\u201d PLOS One, vol. 20, no. 1, pp. 1\u201325, 2025.\n[5] R. P\u00b4erez-Gonzalo, A. Espersen, and A. Agudo, \u201cRobust\nwind turbine blade segmentation from RGB images in\nthe wild,\u201d in ICIP, 2023.\n[6] X. Pan, X. Zhan, J. Shi, X. Tang, and P. Luo, \u201cSwitch-\nable whitening for deep representation learning,\u201d\nin\nICCV, 2019.\n[7] L. Wang, R. Li, C. Zhang, S. Fang, C. Duan, X. Meng,\nand P. M. Atkinson, \u201cUnetformer: A unet-like trans-\nformer for efficient semantic segmentation of remote\nsensing urban scene imagery,\u201d JPRS, vol. 190, pp. 196\u2013\n214, 2022.\n[8] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and\nR. Girdhar,\n\u201cMasked-attention mask transformer for\nuniversal image segmentation,\u201d in CVPR, 2022.\n[9] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang,\nY. Sun, T. He, J. Mueller, R. Manmatha, M. Li, and\nA. Smola,\n\u201cResNeSt: Split-attention networks,\u201d\nin\nCVPRW, 2022.\n[10] S. Mehta and M. Rastegari,\n\u201cMobileViT: Light-\nweight, general-purpose, and mobile-friendly vision\ntransformer,\u201d in ICLR, 2022.\n[11] Y. Li et al.,\n\u201cRethinking vision transformers for mo-\nbilenet size and speed,\u201d in ICCV, 2023.\n[12] A. Kirillov et al., \u201cSegment anything,\u201d in ICCV, 2023,\npp. 4015\u20134026.\n[13] T. L\u00a8uddecke and A. Ecker, \u201cImage segmentation using\ntext and image prompts,\u201d in CVPR, 2022.\n[14] J. Tian,\nL. Aggarwal,\nA. Colaco,\nZ. Kira,\nand\nM. Gonzalez-Franco, \u201cDiffuse attend and segment: Un-\nsupervised zero-shot segmentation using stable diffu-\nsion,\u201d in CVPR, 2024.\n[15] G. Farid, C. Jair, L. Asdr\u00b4ubal, and R. Lisbeth, \u201cSeg-\nmentation of images by color features: A survey,\u201d Neu-\nrocomputing, vol. 292, pp. 1\u201327, 2018.\n[16] J. Yang, C. Liu, and L. Zhang, \u201cColor space normal-\nization: Enhancing the discriminating power of color\nspaces for face recognition,\u201d\nPR, vol. 43, no. 4, pp.\n1454\u20131466, 2010.\n[17] S. N. Gowda and C. Yuan, \u201cColornet: Investigating the\nimportance of color spaces for image classification,\u201d in\nACCV, 2019.\n[18] R. P\u00b4erez-Gonzalo, A. Espersen, and A. Agudo, \u201cGen-\neralized nested latent variable models for lossy coding\napplied to wind turbine scenarios,\u201d in ICIP, 2024.\n[19] J. Sterckx, M. Vlaminck, K. De Bauw, and H. Luong,\n\u201cAccurate and robust 3d reconstruction of wind turbine\nblade leading edges from high-resolution images,\u201d Au-\ntomation in Construction, vol. 175, pp. 106153, 2025.\n[20] R. P\u00b4erez-Gonzalo,\nR. Magro,\nA. Espersen,\nand\nA. Agudo,\n\u201cUnsupervised modular adaptive region\ngrowing and regionmix classification for wind turbine\nsegmentation,\u201d in WACV, 2026.\n[21] J. Yang and C. Liu, \u201cColor image discriminant models\nand algorithms for face recognition,\u201d TNN, vol. 19, no.\n12, pp. 2088\u20132098, 2008.\n[22] S.-J. Wang, J. Yang, N. Zhang, and C.-G. Zhou, \u201cTensor\ndiscriminant color space for face recognition,\u201d TIP, vol.\n20, no. 9, pp. 2490\u20132501, 2011.\n[23] L. Ze, J. Xudong, and K. Alex, \u201cColor space construc-\ntion by optimizing luminance and chrominance compo-\nnents for face recognition,\u201d PR, vol. 83, pp. 456\u2013468,\n2018.\n[24] R. A. Fisher, \u201cThe use of multiple measurements in tax-\nonomic problems,\u201d\nAnn. Eugenics, vol. 7, no. 2, pp.\n179\u2013188, 1936.\n[25] L. Li, M. Doroslova\u02c7cki, and M. H. Loew, \u201cDiscriminant\nanalysis deep neural networks,\u201d in CISS, 2019, pp. 1\u20136.\n[26] C.-C. Chang, \u201cFisher\u2019s linear discriminant analysis with\nspace-folding operations,\u201d\nPAMI, vol. 45, no. 7, pp.\n9233\u20139240, 2023.\n[27] K. Fukunaga, Introduction to statistical pattern recog-\nnition, Elsevier, 2013.\n[28] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-Net: Con-\nvolutional networks for biomedical image segmenta-\ntion,\u201d in MICCAI, 2015.\n[29] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00b4ar,\n\u201cFocal loss for dense object detection,\u201d in ICCV, 2017."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Academic Paper\n\n#### Main Contributions\nThis paper introduces a novel multi-dimensional nonlinear discriminant analysis algorithm called Colorspace Discriminant Analysis (CSDA) for improved image segmentation. The key contributions are:\n- **Extending LDA to Deep Learning Context**: CSDA leverages deep learning techniques by extending Linear Discriminant Analysis (LDA) into a multidimensional setting, enhancing the ability of color representation in specialized contexts.\n- **Alternative Loss Functions**: Three alternative loss functions have been introduced to ensure stable training and improve discriminative power. These include the Logarithmic CSDA Loss, Delta CSDA Loss, and CSDA Loss.\n- **Model Design Integration**: The model integrates these losses into an end-to-end segmentation framework using a U-Net architecture.\n\n#### Methodology\nThe methodology involves:\n1. **Color Representation Customization**: CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through generalized discriminative loss.\n2. **Model Architecture**: The model is composed of two parts: the colorspace transformation part, which converts an RGB image into a new optimized representation Y; and the segmentation branch, which predicts masks based on ground truth M.\n3. **Loss Functions**: Three alternative losses are introduced to optimize color space learning:\n   - CSDA Loss (minimizes both between- and within-class variance)\n   - Logarithmic CSDA Loss (converts ratio into additive form via logarithms)\n   - Delta CSDA Loss (optimizes sum of between-class separation and within-class compactness)\n\n#### Key Findings\nThe experiments demonstrate significant accuracy gains in wind turbine blade segmentation. The key findings are:\n- **Improved Segmentation Performance**: CSDA outperforms both DDA and Focal baselines across all metrics, independent of the chosen loss function.\n- **Robust Generalization**: CSDA achieves top-tier performance with balanced precision and recall, matching BU-Net in accuracy and achieving the best F1-score and mIoU metrics.\n- **Efficient Inference Time**: The model remains efficient at inference time due to only computing discriminant losses during training.\n\n#### Conclusions\nThe paper concludes that CSDA is a robust approach for domain-specific segmentation tasks. It integrates nonlinear discriminant analysis into learnable color space transformations, offering significant improvements over existing methods in wind turbine blade segmentation compared to optimizing either segmentation loss or discriminant analysis alone. The results underscore the value of task-specific preprocessing and establish CSDA as a reliable tool for domain-adapted segmentation.\n\n### Detailed Breakdown\n\n#### Introduction\nThe paper addresses the critical issue of suboptimal color representation hindering accurate image segmentation, particularly in specialized domains like wind turbine blade segmentation. Traditional spaces such as RGB fail to capture subtle cues necessary for accurate classification. The authors propose CSDA, a novel approach that extends LDA into a deep learning context by maximizing multidimensional inter-class separability while minimizing intra-class variability.\n\n#### Colorspace Discriminant Analysis\nThe core of the paper is the development and application of CSDA:\n1. **Multidimensional Fisher Criterion**: Extends LDA to a multidimensional setting, enhancing class separability in high-dimensional spaces.\n2. **Signed Between-Class Variance**: Introduces a signed between-class variance matrix to ensure consistent gradient directionality across training steps.\n3. **Alternative Loss Functions**: Proposes three alternative loss functions (CSDA Loss, Logarithmic CSDA Loss, Delta CSDA Loss) to stabilize and optimize the learning process.\n\n#### Model Design\nThe model architecture is designed as follows:\n- **Colorspace Transformation**: Converts RGB images into a new optimized representation Y.\n- **Segmentation Prediction**: Predicts masks based on ground truth M using an end-to-end U-Net framework.\n- **Loss Function Integration**: Combines proposed discriminant losses with a probabilistic segmentation loss to balance the model's performance.\n\n#### Experimental Results\nThe experiments validate CSDA\u2019s effectiveness:\n- **Visual Exploration of Colorspaces**: Demonstrates how different dimensions affect color representation and separability.\n- **Quantitative Evaluation**: Shows that CSDA outperforms existing methods across various metrics, including accuracy, precision, recall, F1-score, and mIoU.\n- **Qualitative Analysis**: Confirms the model\u2019s ability to preserve blade contours and reduce background misclassifications.\n\n#### Conclusion\nThe paper concludes by emphasizing the importance of task-specific preprocessing in specialized domains. CSDA is presented as a robust tool for domain-adapted segmentation tasks, offering significant improvements over existing methods in wind turbine blade segmentation. The results underscore the value of tailored preprocessing steps and highlight the potential of nonlinear discriminant analysis within learnable color space transformations.\n\n### References\nThe paper references numerous related works on image segmentation, deep learning, and face recognition techniques, providing a comprehensive context for CSDA's development and validation."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "PAC-Private Responses with Adversarial Composition\nXiaochen Zhu\nMIT\nCambridge, MA, USA\nxczhu@mit.edu\nMayuri Sridhar\nMIT\nCambridge, MA, USA\nmayuri@mit.edu\nSrinivas Devadas\nMIT\nCambridge, MA, USA\ndevadas@mit.edu\nAbstract\nModern machine learning models are increasingly deployed behind\nAPIs. This renders standard weight-privatization methods (e.g. DP-\nSGD) unnecessarily noisy at the cost of utility. While model weights\nmay vary significantly across training datasets, model responses to\nspecific inputs are much lower dimensional and more stable. This\nmotivates enforcing privacy guarantees directly on model outputs.\nWe approach this under PAC privacy, which provides instance-\nbased privacy guarantees for arbitrary black-box functions by con-\ntrolling mutual information (MI). Importantly, PAC privacy explic-\nitly rewards output stability with reduced noise levels. However, a\ncentral challenge remains: response privacy requires composing a\nlarge number of adaptively chosen, potentially adversarial queries\nissued by untrusted users, where existing composition results on\nPAC privacy are inadequate. We introduce a new algorithm that\nachieves adversarial composition via adaptive noise calibration\nand prove that mutual information guarantees accumulate linearly\nunder adaptive and adversarial querying.\nExperiments across tabular, vision, and NLP tasks show that our\nmethod achieves high utility at extremely small per-query privacy\nbudgets. On CIFAR-10, we achieve 87.79% accuracy with a per-step\nMI budget of 2\u221232. This enables serving one million queries while\nprovably bounding membership inference attack (MIA) success\nrates to 51.08% \u2013 the same guarantee of (0.04, 10\u22125)-DP. Further-\nmore, we show that private responses can be used to label public\ndata to distill a publishable privacy-preserving model; using an Im-\nageNet subset as a public dataset, our model distilled from 210,000\nresponses achieves 91.86% accuracy on CIFAR-10 with MIA success\nupper-bounded by 50.49%, which is comparable to (0.02, 10\u22125)-DP.\nKeywords\nPAC privacy, machine learning privacy, adversarial composition,\nadaptive composition, membership inference attacks\n1\nIntroduction\nMachine learning (ML) is increasingly deployed across industries\nfor a wide range of applications. This widespread adoption, however,\nhas raised significant privacy concerns, as models are often trained\non sensitive personal information, such as medical records, financial\ntransactions, and private communications [29, 31, 34]. Studies have\ndemonstrated that ML models, particularly over-parameterized\nneural networks, inadvertently memorize information about their\ntraining datasets; this information can be leaked, most notably\nthrough membership inference attacks (MIAs), where an adversary\ninfers whether a specific individual\u2019s data was used to train the\nmodel [4, 35].\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nDifferential privacy (DP) [13, 15] has emerged as the de facto\nstandard to address these concerns. In ML, this is typically imple-\nmented via private training algorithms like DP-SGD [1, 37]. By\ninjecting noise to model gradients during training, this approach\nyields private model weights that can be safely published under\nDP guarantees. However, despite its theoretical effectiveness, DP-\ntrained models often face a harsh privacy-utility tradeoff. Achieving\nacceptable model performance often necessitates a large privacy\nbudget, which weakens the security guarantee [9, 19, 20].\nIn practice, however, full model weights are rarely published.\nInstead, modern models are increasingly deployed as black-box\nservices behind remote APIs, where users interact with the system\nsolely via queries and responses [17]. This reality motivates our shift\nin focus: we propose to directly privatize model responses rather\nthan weights. This approach offers three key advantages. First,\nthe response space is typically much lower-dimensional than the\nparameter space. For instance, image classifiers trained on ImageNet\nmay have millions of parameters, but they only produce confidence\nscores over 1,000 classes [22]. This significantly reduces the scale of\nnoise required for privacy. Secondly, model responses are far more\nstable than model weights. Small variations in training data can\ncause substantial changes in the learned weights, but often with\nminimal differences in their outputs [3, 6]. Finally, privatizing model\nresponses offers black-box compatibility across model architectures,\nwithout requiring access to the training dynamics such as gradients.\nApplying DP to model responses remains challenging. DP mech-\nanisms calibrate noise to the sensitivity of the function, i.e., the\nmaximum change in output over adjacent datasets. In general, tight\ncalculation of sensitivity is NP-hard [48]. While strong stability\nassumptions can be established for simple or convex learners to\nbound sensitivity [14], bounding the prediction shift caused by a sin-\ngle training record is generally infeasible for complex non-convex\nmodels. Consequently, mechanisms are forced to be pessimistic, as-\nsuming sensitivity as high as the full diameter of the output space.\nThis results in excessive noise that fails to exploit the inherent\nstability of model responses.\nProbably approximately correct (PAC) privacy [44, 45] offers a\ncompelling alternative to enable response privacy. As an instance-\ndependent framework, PAC privacy is able to measure the underly-\ning stability of an arbitrary data processing function via black-box\nsimulations. As a result, PAC privacy rewards stability automatically:\nfunctions with stable outputs under the input distribution require\nsignificantly less noise to privatize. However, enforcing PAC privacy\non a sequence of responses still presents a fundamental theoreti-\ncal challenge: adversarial composition. Unlike the non-adaptive\nsettings, untrusted users can act as adaptive adversaries, selecting\nqueries based on the interaction history to maximize information\nleakage. Standard composition theorems [44] fail under such adap-\ntivity, while existing adversarial composition [43] performs poorly:\narXiv:2601.14033v1  [cs.LG]  20 Jan 2026\nXiaochen Zhu, Mayuri Sridhar, and Srinivas Devadas\neither accumulating privacy leakage quadratically in the number of\nreleases, or reverting to DP-like input-independent excessive noise\nat the cost of utility, as we demonstrate in Section 3.2.\nOur Contributions. To address this gap, we introduce a general\nframework for posterior-aware adversarial composition. Our core\ninsight is that, to maintain tight privacy accounting against an\nadaptive adversary, the curator itself must adapt to the adversary\u2019s\nevolving knowledge. We propose an algorithm with adaptive noise\ncalibration, where the curator maintains a belief state representing\nthe posterior distribution of the secret given the interaction history.\nThe curator dynamically calibrates the noise based on this posterior\ndistribution. Under this algorithm, we prove a new adversarial\ncomposition theorem that achieves a linear accumulation of mutual\ninformation. This result overcomes the limitations of prior work,\npreserving the instance-based utility advantages of PAC privacy\nwith linear composition under adaptive adversarial strategies.\nThis theoretical result enables the first application of PAC pri-\nvacy to modern ML models. In particular, we release PAC-private\nML responses for model-agnostic classification tasks, while prov-\nably limiting MIA success on the sensitive training data. Extensive\nempirical results across tabular, vision and NLP tasks demonstrate\nthat our method achieves competitive accuracy while supporting\nmillions of queries under tight privacy guarantees. For example,\non CIFAR-10, our method achieves an average test accuracy of\n87.79% while provably limiting MIA success rate under 51.08% af-\nter one million queries, which is the same guarantee provided by\n(0.04, 10\u22125)-DP. Further, to bridge the gap between finite privacy\nbudgets and the need for unlimited inference, we propose a private\nmodel distillation protocol; that is, we use the private responses\nto label a public auxiliary dataset and train a student model. To\nmitigate the label noise introduced by privatization, we develop\na confidence filtering protocol based on hypothesis testing that\nprovably bounds the probability of retaining mislabeled data. This\nenables the release of a distilled model that achieves high utility\nwhile inheriting the rigorous privacy guarantees of the PAC-private\nresponses. As a proof of concept, we run distillation with auxiliary\naccess to CINIC-10 [7], achieving 91.86% accuracy on CIFAR-10 un-\nder a strong PAC privacy guarantee comparable to (0.02, 10\u22125)-DP.\nPaper Organization. The remainder of this paper is organized\nas follows. Section 2 provides the necessary background on the\nPAC privacy framework. Section 3 presents our main theoretical\ncontribution: the posterior-aware composition framework and the\nderivation of the linear adversarial composition theorem. In Sec-\ntion 4, we instantiate this framework to release PAC-private ML re-\nsponses, detailing the threat model, mechanism design, and efficient\nimplementation. Section 5 describes our private model distillation\nprotocol with confidence filtering. Section 6 presents our extensive\nempirical evaluation across multiple modalities. Section 7 conducts\na comprehensive literature review on related work, including dif-\nferential privacy, and Section 8 concludes the paper.\n2\nBackground\n2.1\nPAC Privacy\nPAC privacy [38, 43\u201347] is a recently proposed framework that\nprovides instance-specific privacy guarantees. It leverages entropy\nin private data, and allows automatic measurement and control of\nprivacy leakage for arbitrary processing functions in a black-box\nmanner. Formally, it is defined as follows:\nDefinition 2.1 ((\ud835\udeff, \ud835\udf0c, \ud835\udc43\ud835\udc46)-PAC Privacy [44]). Let S be the domain\nof the sensitive input. Given a possibly randomized function \ud835\udc40:\nS \u2192R, probability distribution \ud835\udc43\ud835\udc46over S, and a binary-valued\nattack success criterion \ud835\udf0c: S \u00d7 S \u2192{0, 1}, we say that \ud835\udc40satisfies\n(\ud835\udeff, \ud835\udf0c, \ud835\udc43\ud835\udc46)-PAC privacy if for any informed adversary \ud835\udc34: R \u2192S,\nwho knows (\ud835\udc43\ud835\udc46, \ud835\udc40), takes \ud835\udc45= \ud835\udc40(\ud835\udc46) \u2208R as input, and outputs an\nestimate \u02c6\ud835\udc46of \ud835\udc46, its success rate measured by the attack criterion \ud835\udf0c,\ndenoted by 1 \u2212\ud835\udeff\ud835\udc34, is at most 1 \u2212\ud835\udeff, i.e.,\n1 \u2212\ud835\udeff\ud835\udc34:= Pr\ud835\udc46\u223c\ud835\udc43\ud835\udc46, \ud835\udc45\u2190\ud835\udc40(\ud835\udc46), \u02c6\ud835\udc46\u2190\ud835\udc34(\ud835\udc45) [\ud835\udf0c( \u02c6\ud835\udc46,\ud835\udc46) = 1] \u22641 \u2212\ud835\udeff,\n(1)\nwhere the probability is taken over the randomness of \ud835\udc46, \ud835\udc40, and \ud835\udc34.\nRemark 2.2. Definition 2.1 guarantees that the adversary cannot\nwin the following security game with probability more than 1 \u2212\ud835\udeff:\n\u201cThe data curator draws data \ud835\udc46\u223c\ud835\udc43\ud835\udc46and sends \ud835\udc45= \ud835\udc40(\ud835\udc46) to the\nadversary. The adversary, knows \ud835\udc43\ud835\udc46, \ud835\udc40, but not the secret \ud835\udc46, returns\nan estimate \u02c6\ud835\udc46. The adversary wins if \ud835\udf0c( \u02c6\ud835\udc46,\ud835\udc46) = 1.\u201d This guarantee\nis defined under a strong adversary who knows \ud835\udc43\ud835\udc46and \ud835\udc40and is\ncomputationally unbounded. The same bound naturally extends to\na realistic adversary who does not know \ud835\udc43\ud835\udc46. In machine learning,\n\ud835\udc43\ud835\udc46is typically the distribution from which the training dataset is\ndrawn, and \ud835\udc40(\ud835\udc46) is the model trained on \ud835\udc46or its responses.\nWhile Definition 2.1 bounds the absolute probability of the ad-\nversary\u2019s success, it is often easier to quantify and control the\nimprovement in the adversary\u2019s performance relative to their prior\nknowledge. We define this improvement as posterior advantage.\nDefinition 2.3 (Posterior Advantage [44]). For any adversary \ud835\udc34as\ndefined in Definition 2.1 who has success rate (1 \u2212\ud835\udeff\ud835\udc34) under the\nattack criterion \ud835\udf0c, its posterior advantage under \ud835\udc53-divergence is\n\u0394\ud835\udc53\ud835\udeff\ud835\udc34:= \ud835\udc37\ud835\udc53(Bern(\ud835\udeff\ud835\udc34) \u2225Bern(\ud835\udeff0)) = \ud835\udeff0\ud835\udc53\n\u0012\ud835\udeff\ud835\udc34\n\ud835\udeff0\n\u0013\n+(1\u2212\ud835\udeff0)\ud835\udc53\n\u0012 1 \u2212\ud835\udeff\ud835\udc34\n1 \u2212\ud835\udeff0\n\u0013\nwhere \ud835\udc37\ud835\udc53is some \ud835\udc53-divergence and 1\u2212\ud835\udeff0 is the optimal prior success\nrate for an a priori adversary who only knows \ud835\udc43\ud835\udc46and \ud835\udc40without\nobserving \ud835\udc40(\ud835\udc46), i.e., 1 \u2212\ud835\udeff0 := max\ud835\udc44Pr\ud835\udc46\u223c\ud835\udc43\ud835\udc46, \u02c6\ud835\udc46\u223c\ud835\udc44[\ud835\udf0c( \u02c6\ud835\udc46,\ud835\udc46) = 1].\nGiven the distribution \ud835\udc43\ud835\udc46and attack criterion \ud835\udf0c, the optimal\nprior success rate (1 \u2212\ud835\udeff0) is a known constant. For example, in\nmembership inference attacks where \ud835\udc43\ud835\udc46is specified such that each\ndata point is included in the training set with 50% probability, the\noptimal prior success rate is exactly 50%. Since the prior is fixed,\nwe can limit the adversary\u2019s final success rate (1 \u2212\ud835\udeff\ud835\udc34) simply by\ncontrolling their posterior advantage \u0394\ud835\udc53\ud835\udeff\ud835\udc34. The foundation of PAC\nprivacy lies in the connection between an arbitrary adversary\u2019s\nposterior advantage and the mutual information (MI) between the\ninput and the output, as established in the following theorem.\nTheorem 2.4 ([44]). For any adversary \ud835\udc34, its posterior advantage\nsatisfies \u0394\ud835\udc53\ud835\udeff\ud835\udc34\u2264inf\ud835\udc43\ud835\udc4a\ud835\udc37\ud835\udc53(\ud835\udc43\ud835\udc46,\ud835\udc40(\ud835\udc46) \u2225\ud835\udc43\ud835\udc46\u2297\ud835\udc43\ud835\udc4a), where \ud835\udc43\ud835\udc4ais the\ndistribution of an arbitrary random variable \ud835\udc4awith support R. In\nparticular, when \ud835\udc37\ud835\udc53is the KL-divergence and \ud835\udc43\ud835\udc4a= \ud835\udc43\ud835\udc40(\ud835\udc46), we have\n\u0394KL\ud835\udeff\ud835\udc34= (1 \u2212\ud835\udeff\ud835\udc34) log 1 \u2212\ud835\udeff\ud835\udc34\n1 \u2212\ud835\udeff0\n+ \ud835\udeff\ud835\udc34log \ud835\udeff\ud835\udc34\n\ud835\udeff0\n\u2264\ud835\udc3c(\ud835\udc46; \ud835\udc40(\ud835\udc46)),\n(2)\nwhere \ud835\udc3c(\u00b7 ; \u00b7) is the mutual information between two random variables.\nPAC-Private Responses with Adversarial Composition\nWe note that this upper bound is attack-agnostic: the mutual\ninformation \ud835\udc3c(\ud835\udc46; \ud835\udc40(\ud835\udc46)) depends only on \ud835\udc43\ud835\udc46and \ud835\udc40, and bounds the\nadvantage for any attack criterion \ud835\udf0c. Hence, one can privatize an\nalgorithm against arbitrary adversaries simply by setting a mutual\ninformation budget \ud835\udc35and enforcing \ud835\udc3c(\ud835\udc46; \ud835\udc40(\ud835\udc46)) \u2264\ud835\udc35. Then, concrete\nguarantees on adversarial success rate can be derived via Eq. (2).\nFor deterministic functions \ud835\udc40: S \u2192R\ud835\udc51, the following theorem\ndetermines the required scale of Gaussian noise such that the noisy\nmechanism satisfies such a MI budget, and hence, is PAC-private.\nTheorem 2.5 (Noise Determination [38, 44]). Given random\nvariable \ud835\udc46\u223c\ud835\udc43\ud835\udc46, a deterministic function \ud835\udc40: S \u2192R\ud835\udc51and a mutual\ninformation budget \ud835\udc35> 0, let Var(\ud835\udc40(\ud835\udc46)) = \ud835\udc48\u039b\ud835\udc48\u22a4be the singular\nvalue decomposition (SVD) of the covariance matrix of \ud835\udc40(\ud835\udc46), where\n\u039b = diag(\ud835\udf061, . . . , \ud835\udf06\ud835\udc51). Define diagonal matrix \u039b\ud835\udc35as\n\u039b\ud835\udc35= diag\n\u221a\ud835\udf06\ud835\udc56\n\u00cd\ud835\udc51\n\ud835\udc57=1\n\u221a\ufe01\n\ud835\udf06\ud835\udc57\n2\ud835\udc35\n: \ud835\udc56= 1, . . . ,\ud835\udc51\n!\n.\nThen, \ud835\udc3c(\ud835\udc46; \ud835\udc40(\ud835\udc46) + \ud835\udc4d) \u2264\ud835\udc35where \ud835\udc46\u223c\ud835\udc43\ud835\udc46and \ud835\udc4d\u223cN (0,\ud835\udc48\u039b\ud835\udc35\ud835\udc48\u22a4).\nRemark 2.6. Theorem 2.5 controls the MI by adding Gaussian\nnoise given the covariance matrix of the output \ud835\udc40(\ud835\udc46). This matrix\nmay be evaluated in its exact form, for example, when the secret\nspace S is tractable with size \ud835\udc5a:= |S|; indeed, for the instantiation\nconsidered in this paper, exact evaluation is feasible. The major\ncomputational cost under this case is to evaluate \ud835\udc40(\ud835\udc46) for all \ud835\udc46\u2208S.\nOnce the \ud835\udc5aoutputs are collected, we optimize noise calibration by\nperforming SVD directly on the weighted, centered \ud835\udc5a\u00d7 \ud835\udc51matrix\nin \ud835\udc42(\ud835\udc5a\ud835\udc51\u00b7 min(\ud835\udc5a,\ud835\udc51)) time, avoiding explicit covariance formation.\nThis decomposition also enables efficient noise sampling via linear\ntransformation in \ud835\udc42(\ud835\udc512) time. Consequently, the total complexity\nto calibrate and sample the noise is \ud835\udc42(\ud835\udc5a\ud835\udc36+ \ud835\udc5a\ud835\udc51\u00b7 min(\ud835\udc5a,\ud835\udc51) + \ud835\udc512),\nwhere \ud835\udc36is the cost of a single evaluation of the function \ud835\udc40. The\ntotal complexity is typically dominated by the evaluation term \ud835\udc5a\ud835\udc36.\nWhen S is computationally intractable, one can estimate the\ncovariance matrix by Monte Carlo simulation, i.e., sampling \ud835\udc46from\n\ud835\udc43\ud835\udc46and evaluating \ud835\udc40(\ud835\udc46) for \ud835\udc5a\u2032 \u226a\ud835\udc5atimes in a black-box manner.\nXiao and Devadas [44] determine the sampling complexity \ud835\udc5a\u2032 as a\nfunction of the budget \ud835\udc35and a confidence parameter \ud835\udefe, to ensure\nthat the estimated covariance is sufficiently precise. Alternatively,\nSridhar et al. [38] propose the empirical convergence of variance\nestimation as a practical criterion for sufficient sampling.\nIn both cases, the \ud835\udc5aor \ud835\udc5a\u2032 samples for noise calibration are\nindependent of the underlying realized secret \ud835\udc46. Operationally, the\ndata curator samples a secret \ud835\udc46\u223c\ud835\udc43\ud835\udc46, calibrates the noise covariance\nmatrix \u03a3 based on \ud835\udc43\ud835\udc46, \ud835\udc40, and \ud835\udc35, and releases \ud835\udc40(\ud835\udc46) + N (0, \u03a3).\n2.2\nMembership Inference Attacks\nMembership inference attacks (MIAs) have become the de facto\nstandard to empirically measure privacy leakage in ML. To provide a\nconcrete understanding of the PAC privacy guarantees, we translate\nthe MI budget \ud835\udc35into a guarantee on the MIA success rate. We\nformalize MIA to match the PAC privacy framework as follows:\nDefinition 2.7 (Membership Inference Attack [38]). Given a finite\ndata pool \ud835\udc48= {\ud835\udc621, \u00b7 \u00b7 \u00b7 ,\ud835\udc62\ud835\udc5b} and a processing mechanism \ud835\udc40, let \ud835\udc46\u223c\n\ud835\udc43\ud835\udc46be a subset of \ud835\udc48randomly selected according to distribution \ud835\udc43\ud835\udc46.\nAn informed adversary \ud835\udc34, knowing (\ud835\udc48, \ud835\udc43\ud835\udc46, \ud835\udc40) and observing \ud835\udc40(\ud835\udc46),\n2 32\n( =2.32\n\u00d710 5)\n2 26\n( =3.25\n\u00d710 4)\n2 20\n( =2.74\n\u00d710 3)\n2 14\n( =0.02)\n2 8\n( =0.18)\n2 2\n( =1.64)\nMutual Information Budget (B)\n(DP  with Matching MIA Guanrantee ( =10 5))\n0.5\n0.6\n0.7\n0.8\nMIA Success Bound\nFigure 1: Provable upper bound on the individual MIA success\nrate as a function of the MI budget B, under a uniform prior\nsuccess rate of 50%. Differential privacy parameters with\nmatching guarantees are showed in brackets for reference.\noutputs a membership estimate \u02c6\ud835\udc46\u2286\ud835\udc48. The individual membership\ninference success rate for a user \ud835\udc62\ud835\udc56is defined as the probability that\nthe adversary correctly determines their membership:\n1 \u2212\ud835\udeff\ud835\udc34,\ud835\udc56:= Pr\ud835\udc46\u223c\ud835\udc43\ud835\udc46, \u02c6\ud835\udc46\u2190\ud835\udc34(\ud835\udc40(\ud835\udc46)) [1{\ud835\udc62\ud835\udc56\u2208\ud835\udc46} = 1{\ud835\udc62\ud835\udc56\u2208\u02c6\ud835\udc46}],\nwhere 1{\ud835\udc38} is the indicator variable for event \ud835\udc38.\nThis definition allows us to treat membership inference for any\ndata point \ud835\udc62\ud835\udc56as a specific instance of the attack criterion \ud835\udf0cin\nDefinition 2.1. The bound derived in Theorem 2.4 applies directly.\nAs discussed previously, given the MI budget \ud835\udc35, Eq. (2) allows us\nto numerically solve for the upper bound on the adversary\u2019s success\nrate (1\u2212\ud835\udeff\ud835\udc34,\ud835\udc56). In particular, if \ud835\udc43\ud835\udc46is specified s.t. \u2200\ud835\udc56, Pr(\ud835\udc62\ud835\udc56\u2208\ud835\udc46) = 1/2,\ni.e., the prior success rate is 1\u2212\ud835\udeff0,\ud835\udc56= 50%, a budget of \ud835\udc35= 2\u22122 prov-\nably limits the MIA success rate to 83.78%. Tightening to \ud835\udc35= 2\u221210\nreduces this bound to 52.21%, and an extremely strict budget of\n\ud835\udc35= 2\u221232 results in a guarantee of \u224850.001%, virtually indistinguish-\nable from random guessing. These bounds apply simultaneously to\nevery individual data point in\ud835\udc48, as the prior success rate is uniform\nacross the population. For context, (\ud835\udf16,\ud835\udeff)-DP [15] also theoretically\nbounds MIA success by 1 \u2212(1 \u2212\ud835\udeff)/(1 +\ud835\udc52\ud835\udf16) [38]: (1, 10\u22125)-DP prov-\nably limits MIA success to 73.11%, while (0.1, 10\u22125)-DP reduces it\nto 52.50%. Fig. 1 illustrates the theoretical upper bounds on MIA\nsuccess rate across different MI budgets as well as their correspond-\ning DP parameters. While the privacy notions differ semantically,\nwe provide these comparable DP parameters throughout the paper\nto serve as a familiar reference for our PAC privacy guarantees. We\nprovide a more comprehensive discussion on DP in Section 7.\n3\nAdversarial Composition\nThe PAC privacy guarantees introduced in Section 2 apply to mech-\nanisms with a single release. However, if we enforce PAC privacy\nguarantees on sequential responses, each release involves a stan-\ndalone mechanism. Understanding how the privacy leakage accu-\nmulates (or equivalently, how the privacy guarantee composes)\nunder multiple releases requires rigorous composition analysis.\nStandard composition theorems for PAC privacy study non-\nadaptive settings in which the sequence of mechanisms is fixed\nin advance or independent of past outputs, and show that the MI\nguarantee accumulates linearly [44]. However, many practical use\nXiaochen Zhu, Mayuri Sridhar, and Srinivas Devadas\ncases are inherently adaptive. For example, in stochastic gradient\ndescent (SGD), each mechanism corresponds to a gradient compu-\ntation step that depends on the current model parameters, which is\na function of all previous gradient outputs. Moreover, in response\nprivacy, the sequence of queries is not only adaptive to previous\nreleases but may also be adversarial, as a user can strategically\nchoose future queries based on observed outputs to maximize in-\nformation leakage. In these cases, the sequence of mechanisms is\nneither independent nor pre-defined, but instead depends on the\nentire interaction history. This adaptivity allows later mechanisms\nto amplify information revealed by earlier outputs, even when each\nindividual release satisfies a strong per-query PAC privacy guar-\nantee. Therefore, non-adaptive composition theorems cannot be\ndirectly applied, and na\u00efvely composing per-step guarantees can\nlead to incorrect bounds on the total information leakage.\nIn this section, we formalize an adversarial composition setting\nfor PAC privacy where each mechanism may be adaptively chosen\nbased on past outputs via a strategy unknown to the data curator.\nWe then present a composition algorithm that calibrates noise\nadaptively to the interaction history and prove that the mutual\ninformation guarantee accumulates linearly despite adaptivity.\n3.1\nSetting\nFollowing Section 2, let random variable \ud835\udc46\u2208S represent the secret\ninput data. Let \ud835\udc61= 1, 2, . . . ,\ud835\udc47be discrete time steps. At initialization\n(\ud835\udc61= 0), the data curator samples the secret \ud835\udc46\u223c\ud835\udc43\ud835\udc46. At each time step\n\ud835\udc61, an adaptive adversary, who wishes to gain knowledge about \ud835\udc46,\nprovides the data curator with a processing function \ud835\udc40\ud835\udc61: S \u2192R\ud835\udc61,\nwhere R\ud835\udc61= R\ud835\udc51\ud835\udc61. In response, the data curator releases \ud835\udc45\ud835\udc61\u2208R\ud835\udc61, a\npossibly noisy version of \ud835\udc40\ud835\udc61(\ud835\udc46). We define random variable T\ud835\udc61=\n{(\ud835\udc40\ud835\udc56, \ud835\udc45\ud835\udc56)}\ud835\udc61\n\ud835\udc56=1 as the full transcript of the interaction history up to\ntime step \ud835\udc61. The transcript comprises both the adversary\u2019s chosen\nmechanisms and the data curator\u2019s releases, with T0 = \u2205.\nIn particular, the adversary may be adaptive, whose choices are\ndefined by a sequence of unknown strategies {\ud835\udc39\ud835\udc61}\ud835\udc47\n\ud835\udc61=1. At time step\n\ud835\udc61, the adversary observes the transcript T\ud835\udc61\u22121 and samples the next\nquery \ud835\udc40\ud835\udc61\u223c\ud835\udc39\ud835\udc61(T\ud835\udc61\u22121), where \ud835\udc40\ud835\udc61: S \u2192R\ud835\udc51\ud835\udc61. Given a transcript T\ud835\udc61\u22121,\nthe adversarial strategy \ud835\udc39\ud835\udc61defines a distribution over all possible\nfunctions from S to R\ud835\udc51\ud835\udc61and is unknown to the data curator.\nThe only assumption we make about the adversary is that they\nhave no side-channel access to the secret. While a strong, informed\nadversary knows the distribution \ud835\udc43\ud835\udc46of the secret, they can only gain\nknowledge about the realized \ud835\udc46by interacting with the curator and\nobserving their responses. In particular, the choice of the next query\nfunction \ud835\udc40\ud835\udc61depends on \ud835\udc46only via the transcript T\ud835\udc61\u22121. Formally,\n\ud835\udc40\ud835\udc61\u22a5\ud835\udc46| T\ud835\udc61\u22121.\n(3)\nWe emphasize that the data curator uses the identical secret input\n\ud835\udc46when evaluating \ud835\udc40\ud835\udc61at each step. This persistence is essential\nfor meaningful privacy analysis. Recall the previous MIA example\nwhere \ud835\udc43\ud835\udc46defines a distribution over subsets of \ud835\udc48s.t. \u2200\ud835\udc62\u2208\ud835\udc48\ud835\udc43(\ud835\udc62\u2208\n\ud835\udc46) = 1/2. In the context of ML, MIA adversaries are interested in\nknowing if a particular example \ud835\udc62\u2208\ud835\udc48is ever used across all the\nresponses. If the curator re-samples at each step, the probability\nthat an arbitrary \ud835\udc62is included in at least one sampled subset rapidly\napproaches 1 as \ud835\udc47increases. A prior success rate near 100% renders\nany bound on the posterior advantage vacuous. By fixing \ud835\udc46, we\nensure that the input distribution of the overall mechanism remains\n\ud835\udc43\ud835\udc46, and hence the prior MIA success rate remains constant, i.e., 50%.\nThus, any privacy loss is strictly due to leakage from responses,\nrather than increased prior success rates. While this setting is crucial\nfor meaningful privacy guarantees, it presents a greater challenge\nto bound the overall leakage compared to independent re-sampling:\nthe adversary can aggregate information from the entire transcript\nT\ud835\udc47to refine their knowledge about the single, persistent secret \ud835\udc46.\n3.2\nPosterior-Aware Composition\nOur goal is to bound the total mutual information leakage of the\noverall mechanism accumulated over \ud835\udc47steps, i.e., \ud835\udc3c(\ud835\udc46;\ud835\udc451, . . . , \ud835\udc45\ud835\udc47).\nBy Theorem 2.4, this then bounds the posterior advantage and\nconsequently, the adversarial success probability. By the chain rule\nof MI, the overall MI decomposes into the sum of conditional MIs:\n\ud835\udc3c(\ud835\udc46;\ud835\udc451, . . . , \ud835\udc45\ud835\udc47) =\n\ud835\udc47\u2211\ufe01\n\ud835\udc61=1\n\ud835\udc3c(\ud835\udc46;\ud835\udc45\ud835\udc61| \ud835\udc451, . . . , \ud835\udc45\ud835\udc61\u22121).\nHence, a natural approach is to bound each term by a per-step\nmutual information budget \ud835\udc4f\ud835\udc61. However, this is not trivial.\nOne straightforward attempt is to apply Theorem 2.5 at ev-\nery step \ud835\udc61to calibrate noise scale \u03a3\ud835\udc61for the mechanism \ud835\udc40\ud835\udc61such\nthat \ud835\udc3c(\ud835\udc46;\ud835\udc45\ud835\udc61) \u2264\ud835\udc4f\ud835\udc61where \ud835\udc45\ud835\udc61= \ud835\udc40\ud835\udc61(\ud835\udc46) + N (0, \u03a3\ud835\udc61). However, this\nis not sufficient as it fails to account for the dependencies be-\ntween releases. For instance, in a simple case of privatizing two\nadaptively chosen mechanisms, the total mutual information is\n\ud835\udc3c(\ud835\udc46;\ud835\udc451, \ud835\udc452) = \ud835\udc3c(\ud835\udc46;\ud835\udc451) +\ud835\udc3c(\ud835\udc46;\ud835\udc452 | \ud835\udc451); however, Theorem 2.5 only en-\nforces that \ud835\udc3c(\ud835\udc46;\ud835\udc451) \u2264\ud835\udc4f1 and \ud835\udc3c(\ud835\udc46;\ud835\udc452) \u2264\ud835\udc4f2 independently for \ud835\udc46\u223c\ud835\udc43\ud835\udc46.\nEven if the marginal leakage \ud835\udc3c(\ud835\udc46;\ud835\udc452) is small, the conditional leak-\nage \ud835\udc3c(\ud835\udc46;\ud835\udc452 | \ud835\udc451) remains unbounded because the adversary may\nchoose \ud835\udc402 adaptively to exploit the information revealed by \ud835\udc451.\nOne theoretical workaround [43] is to enforce a stronger worst-\ncase bound for each release, i.e., calibrate noise scale \u03a3\ud835\udc61to bound\nsup\ud835\udc44\ud835\udc3c\ud835\udc46\u223c\ud835\udc44(\ud835\udc46; \ud835\udc40\ud835\udc61(\ud835\udc46) + N (0, \u03a3\ud835\udc61)) \u2264\ud835\udc4f\u2032\n\ud835\udc61and \ud835\udc3c(\ud835\udc46; \ud835\udc40\ud835\udc61(\ud835\udc46) + N (0, \u03a3\ud835\udc61)) \u2264\n\ud835\udc4f\ud835\udc61simultaneously for each \ud835\udc61. The former constraint directly bounds\nthe conditional MI given any possible transcript, effectively enforc-\ning an input-independent, worst-case guarantee, similar to that of\nlocal differential privacy [12]. However, as detailed in Appendix A,\nthis forces a harsh trade-off: one must either accept a composition\nbound that scales quadratically in\ud835\udc47, or add excessive noise for a DP-\nlike input-independent guarantee that ignores the instance-specific\nstability. This negates the core benefits of PAC privacy.\nFundamentally, the issue of this static composition approach\ncomes from noise misspecification. From the perspective of an in-\nformed adversary, its belief on the distribution of \ud835\udc46changes over\ntime. Before any observations, an informed adversary knows \ud835\udc46is\nsampled from \ud835\udc43\ud835\udc46as its prior belief, which is usually the uniform\ndistribution over S. After observing the history T\ud835\udc61\u22121, any ratio-\nnal adversary would be tracking the posterior distribution \ud835\udc43\ud835\udc46|T\ud835\udc61\u22121\nbased on the observed transcript to refine their knowledge of the\nsecret. In the aforementioned approach, the data curator calibrates\nnoise based on \ud835\udc43\ud835\udc46at all steps. Hence, the mechanism only protects\nagainst an adversary who has observed nothing, rather than one\nwho has already learned from the interactions. To provide a tight\nguarantee on the conditional leakage \ud835\udc3c(\ud835\udc46;\ud835\udc45\ud835\udc61| T\ud835\udc61\u22121), the noise must\nPAC-Private Responses with Adversarial Composition\nS1\nS2\nS3\nS4\nSm\nSecret Space\nPS\n(a)  t=0\nS1\nS2\nS3\nS4\nSm\nSecret Space\nPS\nT = T\n(b)  t=T\nFigure 2: An informed adversary refines their knowledge of\nthe secret by tracking the posterior distribution of the secret\nconditioned on the observed transcript. In response, the cu-\nrator adaptively calibrates noise to bound the conditional MI\ngiven the current transcript, i.e., the expected KL divergence\nbetween the posterior distribution and the prior \ud835\udc43\ud835\udc46.\nbe calibrated adaptively with respect to the current posterior, ef-\nfectively responding to the adversary\u2019s knowledge. Equivalently,\nbounding the conditional MI bounds the expected KL divergence\nbetween the adversary\u2019s posterior belief on the distribution of \ud835\udc46\nand the prior \ud835\udc43\ud835\udc46, limiting the adversary\u2019s capabilities to refine their\nknowledge of the secret. This is illustrated in Fig. 2.\nTo formalize this, we first define a helper function that deter-\nmines the Gaussian noise required to bound the leakage of a specific\nprocessing function under a specific input distribution. This extends\nthe noise determination mechanism from Theorem 2.5 by treating\nthe noise scale as a function of the input distribution as well.\nDefinition 3.1 (Noise Calibration Function \u03a3). A function \u03a3 :\nP(S) \u00d7 (R\ud835\udc51)S \u00d7 R>0 \u2192R\ud835\udc51\u00d7\ud835\udc51is said to be a noise calibration\nfunction if the following bound holds for any distribution \ud835\udc43over S,\ndeterministic function \ud835\udc40: S \u2192R\ud835\udc51, and positive real number \ud835\udc35:\n\ud835\udc3c\ud835\udc46\u223c\ud835\udc43(\ud835\udc46; \ud835\udc40(\ud835\udc46) + N (0, \u03a3(\ud835\udc43, \ud835\udc40, \ud835\udc35)) \u2264\ud835\udc35.\nWe can instantiate such a function \u03a3 using Theorem 2.5.\nEquipped with this definition, we present an algorithm that al-\nlows the curator to achieve posterior-aware composition via Bayesian\nposterior updates in Algorithm 1. The mechanism explicitly main-\ntains a belief state \ud835\udc43\ud835\udc61, which represents the posterior distribution\nof the secret \ud835\udc46given the current history T\ud835\udc61. At \ud835\udc61= 0, the curator\nsamples the secret \ud835\udc46\u223c\ud835\udc43\ud835\udc46and initializes the belief state, identical to\nthe prior distribution \ud835\udc430 = \ud835\udc43\ud835\udc46. At each step, the adversary chooses\na query \ud835\udc40\ud835\udc61, via its querying strategy \ud835\udc39\ud835\udc61that is unknown to the\ncurator. Upon receiving \ud835\udc40\ud835\udc61, the curator calculates the noise covari-\nance \u03a3\ud835\udc61= \u03a3(\ud835\udc43\ud835\udc61\u22121, \ud835\udc40\ud835\udc61,\ud835\udc4f\ud835\udc61) required to satisfy the privacy budget for\nquery function \ud835\udc40\ud835\udc61under the current belief \ud835\udc43\ud835\udc61\u22121. Sampling the ad-\nditive noise from N (0, \u03a3\ud835\udc61), the curator releases the noisy response\n\ud835\udc40\ud835\udc61(\ud835\udc46) + N (0, \u03a3\ud835\udc61), and then updates the belief using Bayes\u2019 rule:\n\u2200\ud835\udc60\u2208S,\n\ud835\udc43\ud835\udc61(\ud835\udc60) \u221dexp\n\u0012\n\u22121\n2 (\ud835\udc45\ud835\udc61\u2212\ud835\udc40\ud835\udc61(\ud835\udc60))\u22a4\u03a3\u22121\n\ud835\udc61\n(\ud835\udc45\ud835\udc61\u2212\ud835\udc40\ud835\udc61(\ud835\udc60))\n\u0013\n\u00b7 \ud835\udc43\ud835\udc61\u22121(\ud835\udc60).\n(4)\nWe now analyze the privacy guarantee and computational com-\nplexity of Algorithm 1.\nAlgorithm 1 Posterior-Aware Composition\n1: Input: Prior \ud835\udc43\ud835\udc46, per-step mutual information budgets {\ud835\udc4f\ud835\udc61}\ud835\udc47\n\ud835\udc61=1,\nnoise calibration function \u03a3, horizon \ud835\udc47. The adversary addi-\ntionally holds strategies {\ud835\udc39\ud835\udc61}\ud835\udc47\n\ud835\udc61=1 unknown to the curator.\n2: Initialize:\n3:\nInitial belief \ud835\udc430 \u2190\ud835\udc43\ud835\udc46\n4:\nTranscript T0 \u2190\u2205\n5:\nSample secret \ud835\udc46\u223c\ud835\udc43\ud835\udc46\n6: for \ud835\udc61= 1, 2, . . . ,\ud835\udc47do:\n7:\nAdversary step:\n8:\nAdversary samples \ud835\udc40\ud835\udc61: S \u2192R\ud835\udc51\ud835\udc61from \ud835\udc39\ud835\udc61(T\ud835\udc61\u22121)\n9:\nAdaptive noise calibration:\n10:\n\u03a3\ud835\udc61\u2190\u03a3(\ud835\udc43\ud835\udc61\u22121, \ud835\udc40\ud835\udc61,\ud835\udc4f\ud835\udc61)\n11:\nRelease:\n12:\nRelease \ud835\udc45\ud835\udc61\u2190\ud835\udc40\ud835\udc61(\ud835\udc46) + N (0, \u03a3\ud835\udc61)\n13:\nUpdate:\n14:\nUpdate history T\ud835\udc61\u2190(T\ud835\udc61\u22121, (\ud835\udc40\ud835\udc61, \ud835\udc45\ud835\udc61))\n15:\nUpdate posterior belief conditioned on T\ud835\udc61: \u2200\ud835\udc60\u2208S,\n16:\n\ud835\udc43\ud835\udc61(\ud835\udc60) \u221dexp(\u22121\n2 (\ud835\udc45\ud835\udc61\u2212\ud835\udc40\ud835\udc61(\ud835\udc60))\u22a4\u03a3\u22121\n\ud835\udc61(\ud835\udc45\ud835\udc61\u2212\ud835\udc40\ud835\udc61(\ud835\udc60))) \u00b7\ud835\udc43\ud835\udc61\u22121(\ud835\udc60)\n17: end for\n3.2.1\nPrivacy Guarantee. We show that under Algorithm 1, the\nper-step MI bounds compose linearly to bound the overall MI. This\nmatches the non-adaptive setting [44] despite adversarial adaptivity.\nFirst, we prove that the belief state \ud835\udc43\ud835\udc61maintained by the algorithm\nvia Eq. (4) correctly tracks the true posterior of \ud835\udc46, which is the same\ndistribution maintained by an informed and optimal adversary.\nLemma 3.2 (Validity of Posterior Update). Let T\ud835\udc61be the ran-\ndom variable representing the transcript up to time \ud835\udc61generated by\nAlgorithm 1. For any time step 0 \u2264\ud835\udc61\u2264\ud835\udc47and any transcript \ud835\udf0f\ud835\udc61realiz-\nable at \ud835\udc61, when conditioned on T\ud835\udc61= \ud835\udf0f\ud835\udc61, the belief state \ud835\udc43\ud835\udc61maintained\nby the curator is the same as the posterior distribution of the secret\nmaintained by an informed and optimal adversary: \ud835\udc43\ud835\udc61= \ud835\udc43\ud835\udc46|T\ud835\udc61=\ud835\udf0f\ud835\udc61.\nProof. This is a Bayesian inference step and we can prove the\nstatement via induction. Full proof is deferred to Appendix B.1.\n\u25a1\nWe proceed to prove the main composition theorem. As the noise\nis calibrated adaptively to the true posterior, we can bound the con-\nditional MI at each step exactly by \ud835\udc4f\ud835\udc61, allowing linear composition.\nTheorem 3.3 (Adversarial Composition). Fix a prior \ud835\udc43\ud835\udc46over\nthe secret space S and a horizon \ud835\udc47\u2208N. Let (\ud835\udc451, . . . , \ud835\udc45\ud835\udc47) be the\nsequence of releases produced by Algorithm 1 run with step MI budgets\n{\ud835\udc4f\ud835\udc61}\ud835\udc47\n\ud835\udc61=1, against an arbitrary adaptive adversary (\ud835\udc39\ud835\udc61)\ud835\udc47\n\ud835\udc61=1.\nThen, the mutual information between the secret input \ud835\udc46and the\nfull sequence of \ud835\udc47releases (\ud835\udc451, . . . , \ud835\udc45\ud835\udc47) satisfies\n\ud835\udc3c(\ud835\udc46;\ud835\udc451, . . . , \ud835\udc45\ud835\udc47) \u2264\ud835\udc35\ud835\udc47:=\n\ud835\udc47\u2211\ufe01\n\ud835\udc61=1\n\ud835\udc4f\ud835\udc61.\nProof. We prove a stronger statement that \ud835\udc3c(\ud835\udc46;\ud835\udc451, . . . , \ud835\udc45\ud835\udc47) \u2264\n\ud835\udc3c(\ud835\udc46; T\ud835\udc47) \u2264\ud835\udc35\ud835\udc47. Full proof is deferred to Appendix B.2.\n\u25a1\nRemark 3.4. Theorem 3.3 also bounds the expected KL diver-\ngence between the adversary\u2019s posterior belief of the secret\u2019s dis-\ntribution and the prior distribution \ud835\udc43\ud835\udc46: E\ud835\udf0f\ud835\udc47[\ud835\udc37KL(\ud835\udc43\ud835\udc46| T\ud835\udc47=\ud835\udf0f\ud835\udc47\u2225\ud835\udc43\ud835\udc46)] =\nXiaochen Zhu, Mayuri Sridhar, and Srinivas Devadas\nTable 1: The PAC privacy guarantees on MIA success rate\nunder a 50% prior success rate after 1 \u2264\ud835\udc47\u2264106 responses\nacross different levels of per-step MI budget \ud835\udc4f. The table also\nincludes the maximum number of responses such that the\nguarantee matches that of \ud835\udf16\u22641 and 8 under DP (\ud835\udeff= 10\u22125).\n\ud835\udc4f\n2\u22124\n2\u22128\n2\u221212\n2\u221216\n2\u221220\n2\u221224\n2\u221228\n2\u221232\n\ud835\udc47= 1\n67.5\n54.4\n51.1\n50.3\n50.07\n50.02\n50.00\n50.00\n\ud835\udc47= 10\n98.7\n63.9\n53.5\n50.9\n50.22\n50.05\n50.01\n50.00\n\ud835\udc47= 102\n100\n91.0\n61.0\n52.8\n50.69\n50.17\n50.04\n50.01\n\ud835\udc47= 103\n100\n100\n83.4\n58.7\n52.18\n50.55\n50.14\n50.03\n\ud835\udc47= 104\n100\n100\n100\n76.9\n56.89\n51.73\n50.43\n50.11\n\ud835\udc47= 105\n100\n100\n100\n100\n71.48\n55.45\n51.36\n50.34\n\ud835\udc47= 106\n100\n100\n100\n100\n100\n67.09\n54.31\n51.08\n\ud835\udc47s.t. \ud835\udf16\u22641\n1\n28\n454\n7K\n116K\n2M\n30M\n477M\n\ud835\udc47s.t. \ud835\udf16\u22648\n11\n176\n3K\n45K\n724K\n12M\n185M\n3B\n\ud835\udc3c(\ud835\udc46; T\ud835\udc47) \u2264\ud835\udc35\ud835\udc47. Intuitively, this limits the adversary\u2019s power to refine\nits knowledge on \ud835\udc46and perform inference attacks on \ud835\udc46(cf. Fig. 2).\nTheorem 3.3 shows that when the noise calibration is adaptive to\nthe interaction transcript, the per-step mutual information budgets\n{\ud835\udc4f\ud835\udc61} compose linearly to a total bound of \ud835\udc35\ud835\udc47= \u00cd\ud835\udc47\n\ud835\udc61=1 \ud835\udc4f\ud835\udc61. Conse-\nquently, under a uniform budget allocation where \ud835\udc4f\ud835\udc61= \ud835\udc4ffor all \ud835\udc61,\nthe total MI after \ud835\udc47releases is upper bounded by \ud835\udc35\ud835\udc47= \ud835\udc4f\ud835\udc47.\nTo illustrate the practical implications, we return to the MIA\nexample under a uniform 50% prior success rate (cf. Section 2.2). Re-\ncall from Section 3.1 that the secret \ud835\udc46is sampled once and persistent\nacross responses, hence, the adversary\u2019s prior success rate remains\nconstant at 50% regardless of \ud835\udc47. Table 1 presents the theoretical up-\nper bound on a MIA adversary\u2019s success rate under varying budgets\n\ud835\udc4fand horizons\ud835\udc47. Table 1 also lists the maximum number of queries\nthe system can take before the guarantee reaches the equivalent\nof \ud835\udf16= 1 or 8 under DP. Under a per-step budget of \ud835\udc4f= 2\u22128, MIA\nsuccess is bounded by 54.42% after a single response (matching the\nguarantee of DP with \ud835\udf16\u22480.18), rising to 91% (\ud835\udf16\u22482.31) after 100\nresponses. With a much tighter budget of \ud835\udc4f= 2\u221232, the privacy loss\nis almost negligible: even after one million responses, MIA success is\nstill bounded by 51.08%, matching \ud835\udf16\u22480.04 under DP. In fact, under\nthis budget, the system can support up to \u2248477 million responses\nbefore reaching the same guarantee as (1, 10\u22125)-DP. This capacity\nexceeds the lifetime query volume of many real-world deployments,\neffectively rendering the privacy budget inexhaustible.\n3.2.2\nComputational Complexity. While Algorithm 1 is theoreti-\ncally valid for any secret space S, we first analyze its computational\ncomplexity under the tractable setting where \ud835\udc5a= |S| is finite. As\ndiscussed in Remark 2.6, for a single release, noise calibration and\nsampling requires \ud835\udc42(\ud835\udc5a\ud835\udc36+\ud835\udc5a\ud835\udc51\u00b7 min(\ud835\udc5a,\ud835\udc51) +\ud835\udc512) time, dominated by\nthe evaluation term \ud835\udc42(\ud835\udc5a\ud835\udc36). For applications in this paper, \ud835\udc51< \ud835\udc5a,\nand this cost becomes \ud835\udc42(\ud835\udc5a\ud835\udc36+ \ud835\udc5a\ud835\udc512). Algorithm 1 introduces an\nadditional step of posterior update for each release, which reuses\nthe eigen-decomposition obtained during noise calibration. This\nallows us to avoid explicit matrix inversion in Eq. (4) and compute\nthe likelihoods via efficient vector projections in \ud835\udc42(\ud835\udc5a\ud835\udc512) time. Con-\nsequently, the additional belief-tracking step in Algorithm 1 incurs\nno additional asymptotic overhead, while being negligible compared\nto the \ud835\udc42(\ud835\udc5a\ud835\udc36) cost for \ud835\udc5aevaluations.\nIn cases where |S| is intractable (e.g., when S is a continuous\nspace and |S| is infinite), maintaining the full posterior distribution\n\ud835\udc43\ud835\udc61is not feasible. However, we note that \ud835\udc43\ud835\udc61is only used in the\nnoise calibration step where we need to evaluate the covariance\nmatrix of the output distribution \ud835\udc40(\ud835\udc46) under \ud835\udc46\u223c\ud835\udc43\ud835\udc61\u22121. Similar\ntechniques discussed in Remark 2.6 can be employed to estimate\nsuch covariance under \ud835\udc43\ud835\udc61\u22121. As mentioned previously, for applica-\ntions exhibited in this paper, S is indeed finite and tractable, and\nthe exact evaluation of \ud835\udc43\ud835\udc61is feasible and efficient. We leave a more\ndetailed discussion on the intractable case to future work."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary\n\n#### Main Contributions\nThe paper introduces an innovative approach for enforcing privacy guarantees directly on model responses rather than weights, which is particularly relevant given that modern machine learning models are often deployed as APIs. The key contributions include:\n1. **PAC Privacy Framework for Adversarial Composition**: The authors propose a new framework to achieve adversarial composition under PAC (Probably Approximately Correct) privacy, addressing the challenge of composing multiple adaptively chosen queries issued by untrusted users.\n2. **Adaptive Noise Calibration Algorithm**: They introduce an adaptive noise calibration algorithm that dynamically adjusts the noise level based on the posterior distribution of the secret given the interaction history. This allows for linear accumulation of mutual information guarantees under adaptive and adversarial querying.\n\n#### Methodology\nThe authors formalize a PAC privacy framework tailored to model responses, leveraging entropy in private data and allowing automatic measurement and control of privacy leakage via black-box simulations. The methodology includes:\n1. **PAC Privacy Definition**: They define PAC privacy as an instance-dependent framework that rewards stability with reduced noise levels.\n2. **Mutual Information (MI) Guarantees**: By establishing the connection between an arbitrary adversary's posterior advantage and mutual information, they derive a theorem stating that the MI guarantee accumulates linearly under adaptive and adversarial querying.\n\n#### Key Findings\nThe experiments demonstrate:\n1. **High Utility at Small Privacy Budgets**: The method achieves high accuracy with extremely small per-query privacy budgets across various tasks (tabular, vision, NLP). For instance, on CIFAR-10, it achieves 87.79% accuracy with a per-step MI budget of \\(2^{-32}\\).\n2. **Competitive Performance**: The approach outperforms standard DP mechanisms like (0.04, 10\\(^{-5}\\)-DP) in terms of accuracy while maintaining similar privacy guarantees.\n3. **Private Responses for Labeling Public Data**: The authors show that private responses can be used to label public data and distill a publishable model, achieving comparable performance to DP mechanisms.\n\n#### Conclusions\nThe paper's main conclusion is the successful application of PAC privacy to modern machine learning models, providing strong guarantees on adversarial composition. This approach enables high utility while maintaining rigorous privacy protections, overcoming the limitations of traditional methods like differential privacy. The authors propose a private model distillation protocol that leverages these results to create robust, privacy-preserving models.\n\n### Summary Breakdown\n\n#### Introduction\nThe introduction highlights the increasing deployment of machine learning (ML) models behind APIs and the associated privacy concerns, particularly in scenarios where full model weights are not published. Traditional DP mechanisms often result in a harsh trade-off between utility and privacy. The paper proposes enforcing PAC privacy directly on model responses to address these issues.\n\n#### Background\nThe background section introduces PAC privacy as an instance-dependent framework that provides automatic measurement and control of privacy leakage via black-box simulations, with the key theorem stating that mutual information (MI) guarantees accumulate linearly under adaptive and adversarial querying. The authors also discuss membership inference attacks (MIA), which are used to empirically measure privacy leakage in ML.\n\n#### Adversarial Composition\nThe paper introduces a new framework for adversarial composition under PAC privacy, where each mechanism may be adaptively chosen based on past outputs via an unknown strategy to the data curator. They present a novel algorithm that allows the curator to achieve posterior-aware composition by dynamically calibrating noise with respect to the current posterior distribution.\n\n#### Key Theorem and Algorithm\nThe authors prove a new adversarial composition theorem, showing that mutual information guarantees accumulate linearly under adaptive and adversarial querying. This result overcomes limitations of prior work while preserving instance-based utility advantages of PAC privacy under adaptive adversarial strategies.\n\n#### Experimental Results\nExperiments across tabular, vision, and NLP tasks demonstrate the method's effectiveness in achieving high accuracy with extremely small per-query privacy budgets. For example, on CIFAR-10, it achieves 87.79% accuracy with a per-step MI budget of \\(2^{-32}\\). The authors also show that private responses can be used to label public data and distill a publishable model while maintaining similar privacy guarantees.\n\n#### Conclusions\nThe paper concludes by highlighting the successful application of PAC privacy to modern ML models, providing strong guarantees on adversarial composition. This approach enables high utility while maintaining rigorous privacy protections, overcoming limitations of traditional methods like differential privacy. The authors propose a private model distillation protocol that leverages these results to create robust, privacy-preserving models.\n\n### Conclusion\nThe paper presents a comprehensive framework for enforcing PAC privacy directly on model responses, addressing the challenges posed by modern machine learning deployments. Through rigorous theoretical analysis and empirical validation, it demonstrates the feasibility of achieving high utility while maintaining stringent privacy guarantees."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "On the Role of Rotation Equivariance in\nMonocular 3D Human Pose Estimation\nPavlo Melnyk1, Cuong Le1, Urs Waldmann1,\nPer-Erik Forss\u00e9n1, and Bastian Wandt2\n1 Computer Vision Laboratory, Link\u00f6ping University, Link\u00f6ping 58331, Sweden\n2 Independent Researcher\npavlo.melnyk@liu.se\nAbstract. Estimating 3D from 2D is one of the central tasks in com-\nputer vision. In this work, we consider the monocular setting, i.e. single-\nview input, for 3D human pose estimation (HPE). Here, the task is to\npredict a 3D point set of human skeletal joints from a single 2D input\nimage. While by definition this is an ill-posed problem, recent work has\npresented methods that solve it with up to several-centimetre error. Typ-\nically, these methods employ a two-step approach, where the first step is\nto detect the 2D skeletal joints in the input image, followed by the step of\n2D-to-3D lifting. We find that common lifting models fail when encoun-\ntering a rotated input. We argue that learning a single human pose along\nwith its in-plane rotations is considerably easier and more geometrically\ngrounded than directly learning a point-to-point mapping. Furthermore,\nour intuition is that endowing the model with the notion of rotation\nequivariance without explicitly constraining its parameter space should\nlead to a more straightforward learning process than one with equivari-\nance by design. Utilising the common HPE benchmarks, we confirm that\nthe 2D rotation equivariance per se improves the model performance on\nhuman poses akin to rotations in the image plane, and can be efficiently\nand straightforwardly learned by augmentation, outperforming state-of-\nthe-art equivariant-by-design methods.\nKeywords: Equivariance \u00b7 Human pose estimation \u00b7 2D-3D lifting\n1\nIntroduction\nPredicting 3D structures from 2D images is a fundamental task in computer\nvision. The problem, however, becomes ill-posed when the setting is constrained\nby single-view (i.e. monocular) estimation and unknown camera parameters. In\nthis paper, we consider this setting within the task of monocular 3D human pose\nestimation (HPE), where the goal is to estimate 3D human skeletal joints from\na single 2D image.\nWhile there exist methods that solve this task in an end-to-end manner, a\ncommon approach is two-stage: 1) detect 2D keypoints from the input image, e.g.\nby using a pre-trained detector such as HRNet [4], and 2) lift the detected 2D\narXiv:2601.13913v1  [cs.CV]  20 Jan 2026\n2\nP. Melnyk et al.\nFig. 1. Teaser: Learning rotation equivariance improves the performance of a 2D\u21923D\nlifting model in monocular human pose estimation for motions akin to rotations in\nthe image plane. In contrast, typical lifting models lack this notion of equivariance.\nFurthermore, models that are equivariant by design over-constrain the learning process;\nboth result in poor 3D pose predictions. Blue, green, and orange, respectively, encode\nthe right side, middle, and left side of the human body.\nkeypoints to 3D by means of another trained model. A geometrically consistent\n2D\u21923D lifting model must respect rotation-equivariance, i.e. as the input 2D\nkeypoints rotate in-plane, the output 3D pose must rotate accordingly about\nthe optical axis. Lifting models, however, typically do not exhibit this property,\nsince it is insufficiently represented in the data they were trained on and their\ndesign does not account for it. We conjecture that incorporating the inductive\nbias of in-plane rotation equivariance into a lifting model without constraining\nthe model parameter space should lead to a simpler learning process compared\nto one where equivariance is incorporated by design.\nTherefore, we employ the common human pose estimation datasets Hu-\nman3.6M [9] and MPII-INF-3DHP [14], as well as SportsCap [3], and investigate\nthe role of rotation equivariance for the task of monocular 2D\u21923D lifting in the\nhuman pose estimation pipeline.\nThe scope of our work and the assumptions we make are thus as follows:\n(i) the input is a single 2D image, i.e. a single frame; (ii) the 2D keypoints are\ndetected by a common off-the-shelf keypoint detector, i.e. equivariance is only\ninvestigated in lifting models; (iii) no additional features are provided in the\ninput to the lifting model, besides 2D keypoints.\nOur contributions are that we quantitatively and qualitatively demonstrate\nthat for monocular 3D HPE,\n\u2013 Typical lifting models are geometrically inconsistent, and they fail on rotated\nout-of-distribution poses\n\u2013 Rotation equivariance as an inductive bias improves the performance for\nhuman poses akin to rotations in the image plane (see Figure 1)\n\u2013 The correct rotation equivariance is efficiently and straightforwardly learned\nby augmentation, outperforming state-of-the-art methods that are equivari-\nant by design.\nEquivariance in Monocular Human Pose Estimation\n3\n2\nBackground\n2.1\nHuman Pose Estimation\nMonocular 3D human pose estimation is a challenging but vast field of research.\nMethods can be split into two categories: two-stage and end-to-end approaches.\nTwo-Stage Approaches Two-stage approaches decompose monocular 3D hu-\nman pose estimation into two stages: 2D keypoint detection and 3D pose lifting.\nThis allows the lifting module to focus completely on geometric reasoning.\nA well-established baseline in this category is the work by Martinez et al. [13],\nwhich uses a lightweight fully connected network to regress 3D joint coordinates\nfrom 2D inputs. Despite its simplicity, this method achieves strong performance\non Human3.6M [9] and clearly demonstrates that accurate 3D pose estimation is\nfeasible given reliable 2D keypoints. Due to its effectiveness and minimal archi-\ntectural assumptions, it remains a standard baseline for evaluating lifting-based\nmethods, including in our work.\nSubsequent research extends this paradigm by incorporating additional con-\nstraints and refinement strategies. Li et al. [11] propose a cascaded lifting frame-\nwork with iterative refinement and data evolution to improve robustness to noisy\n2D detections. Xu et al. [20] integrate kinematic priors directly into the lifting\nnetwork, enforcing physically plausible joint relationships to reduce depth am-\nbiguity and implausible poses. Extending lifting methods to more challenging\nviewpoints, Wang et al. [18] augment 2D keypoints with scene geometry cues,\naddressing severe occlusions and missing joints common in egocentric settings.\nMore recently, transformer-based architectures have been adopted to enhance\nlifting performance by modelling global spatial and temporal dependencies. Pose-\nFormer [21] achieves improved robustness to occlusion and temporal inconsis-\ntency compared to convolutional temporal models.\nEnd-To-End Approaches End-to-end methods aim to predict 3D human pose\ndirectly from RGB images, eliminating the explicit 2D lifting stage.\nWang et al. [19] introduce a unified framework that jointly performs detection\nand 3D pose estimation while modelling uncertainty through distribution-aware\nrepresentations, enabling robust multi-person inference in crowded scenes. Ear-\nlier work from Moon et al. [15] incorporates camera distance estimation into a\ntop-down pipeline to alleviate scale and depth ambiguities inherent in monoc-\nular images. Complementary to fully supervised end-to-end approaches, Wandt\nand Rosenhahn [17] propose RepNet, a weakly supervised adversarial reprojec-\ntion framework that learns image-to-3D pose mappings using only 2D keypoint\nsupervision.\nFor further reading and comprehensive overviews, we refer to these two survey\npapers [10,7].\n4\nP. Melnyk et al.\n2.2\nRotation Equivariance\nRotations in nD are the actions of the special orthogonal group SO(n), and can\nbe represented with n \u00d7 n matrices R such that R\u22a4R = RR\u22a4= In, with In\nbeing the identity matrix, and det R = 1.\nA function f : X \u2192Y is said to be SO(n)-equivariant, i.e. equivariant under\nnD rotations, if for every R \u2208SO(n), in the function output space, Y, there\nexists the transformation representation, \u03c1(R), such that\n\u03c1(R) f(X) = f(Rx)\nfor all R \u2208SO(n), X \u2208X \u2286Rn.\n(1)\nIf the transformation representation in the output space is identity, i.e. if for\nevery R \u2208SO(n), \u03c1(R) = In, we call a function f : X \u2192Y SO(n)-invariant:\nf(X) = f(Rx)\nfor all R \u2208SO(n), x \u2208X \u2286Rn.\n(2)\nRotation equivariance is a powerful inductive bias, and when incorporated\nappropriately, leads to improved and robust performance, e.g. in point-cloud\nclassification [5], trajectory prediction [1], molecular properties prediction [2],\nand energy and force prediction for materials science applications [12].\nIn this work, we focus specifically on equivariance under 2D rotations, i.e.\nSO(2) actions, as well as their embedding into SO(3), in the context of 2D\u21923D\nlifting for monocular 3D HPE. This is unlike the DECA method [6], which is\nan end-to-end HPE method working on depth images, equivariant to camera\nviewpoint changes (i.e. 3D rotations of the camera around the human).\nIn addition, the work by Howell et al. [8] examines the theoretical basis for in-\ncorporating rotation equivariance constraints when learning 3D representations\nfrom 2D images. Namely, it shows that only restricted symmetry actions can be\nconsistently defined between 2D input images and 3D outputs. While we do not\nbuild our work on this framework directly, our observations made in Section 4\nare consistent with their analysis.\n3\nGeometrically Consistent Lifting Model\nWe call a lifting model, f(X) : RN\u00d72 \u2192RN\u00d73, geometrically consistent if the\nfollowing equivariance property holds for any X \u2208RN\u00d72 and an in-plane rota-\ntion, R\u03b8 \u2208SO(2), about the angle \u03b8 \u2208[0, 2\u03c0):\nf(X R\u22a4\n\u03b8 ) = f(X)\n\u0014\nR\u22a4\n\u03b8 0\n0 1\n\u0015\n,\n(3)\nwhich means that any (global) in-plane rotation of the input 2D keypoints cor-\nresponds to a rotation about the optical axis of the output 3D point set (see\nFigure 2), leaving the depth-coordinate invariant.\nEnforcing exact equivariance given by Eq. (3) may be too restrictive since\nthe true 2D\u21923D mapping is often only approximately rotation-equivariant in\nEquivariance in Monocular Human Pose Estimation\n5\nImage Plane (2D Keypoints)\nLifting\nModel\nOptical axis\n3D Human Pose\n\ud835\udc45!\n\ud835\udc45!\nrotation in image plane    \u279d\nrotation about optical axis\nFig. 2. Geometric consistency in the lifting model: rotations in the image plane cor-\nrespond to the rotations about the optical axis in the camera coordinate system.\npractice. The exact equivariance can thus prevent the model from producing an\noptimal prediction.\nIn the following section, we investigate how geometrically consistent common\n2D\u21923D lifting models [13,21] are and how useful and limiting the constraint in\nEq. (3) becomes in practice by employing the following types of models:\n\u2013 Fully equivariant: Models that satisfy Eq. (3) exactly by design\n\u2013 Hybrid: Models that only produce rotation-equivariant xy-coordinates by\ndesign, with a non-invariant output depth (hybrid) and trained to produce\napproximately invariant depth (hybrid + aug)\n\u2013 Vanilla: Models that are non-equivariant (vanilla) and trained to approxi-\nmately satisfy Eq. (3) by augmentation (vanilla + aug).\n4\nExperiments and Results\n4.1\nModels\nIn this section, we outline the models we build according to the types described\nin Section 3. We strive to conduct a fair comparison and therefore construct the\nmodels with a similar number of parameters (\u223c3.7\u22124.4M) determined by, when\napplicable, a matching number of layers and feature dimensions. Our choice of\nmodels is motivated by their relevance to the scope of our work (see Section 1),\nreported performance on related tasks, and reproducibility.\nFully equivariant We utilise VN-Transformer [1], a transformer architecture\nbased on the vector neurons framework [5] for rotation-equivariant point cloud\nprocessing. In addition, we employ GotenNet [2], a state-of-the-art model for\n6\nP. Melnyk et al.\nX \u2208RN\u00d72\n2D Equivariant Model\nNon-equivariant Baseline\nxy-coordinates\nz-coordinate\n\u02c6X \u2208RN\u00d73\nFig. 3. Hybrid model architecture outline: the xy-outputs are rotation-equivariant,\nwhereas the z-coordinate prediction is non-equivariant. In the default setting (no dot-\nted line), the input is fed into both models in parallel; for the ablation study (with\nthe dotted line), the input to the non-equivariant baseline is the output of the first\nequivariant layer of the 2D equivariant model.\nrotation-equivariant molecular analysis, reconciling the expensive high-order\nequivariant interactions of concurrent methods with scalar-based approaches.\nWe use open-source implementations3. In order to use them, we need to en-\nsure that the input joints (2D) are in the same dimension as the output (3D).\nWhile we can use a constant value as the third coordinate for all input points for\nVN-Transformer (e.g. 1), this does not work for GotenNet due to the joint-wise\ndifferences used in the model, which leads to the third coordinate in the output\nalways being zero. Thus, for both models, we append every N \u00d7 2 input with a\nvector of size N sampled randomly only once when the model is instantiated.\nVanilla We use the simple ResNet-based baseline by Martinez et al. [13], as\nwell as a transformer-based baseline, PoseFormer, by Zheng et al. [21]. These\nvanilla models do not consider rotation equivariance in their 3D human pose\nestimates, thus failing when encountering a rotated out-of-distribution pose, as\nseen in the example in Fig. 1. The architecture from Martinez et al. [13] consists\nof two sequential blocks of ResNet, taking the flattened 2D pose as input vector,\nand outputs the corresponding 3D pose vector. The PoseFormer [21] models\nthe spatial and temporal correlation between human joints via the transformer\narchitecture. Originally, PoseFormer is designed to take in a sequence of 2D poses\nas input. To maintain fair comparison, we modify the architecture of PoseFormer\nto work with single-frame inputs, by reducing the length of input sequence to 1\nand decreasing the depth of the temporal transformer module to 2, resulting in\na comparable number of learnable parameters.\nHybrid In order to analyse the potential limitation of the expensiveness of fully\nequivariant models mentioned in Section 3, we combine an equivariant model,\nVN-Transformer or GotenNet, used for xy-prediction only, with the ResNet base-\nline [13] predicting the depth coordinate, as shown in Figure 3. Given the VN-\nTransformer implementation, we straightforwardly instantiate it for 2D, whereas\n3 https://github.com/lucidrains/VN-transformer and https://github.com/lucidrains/\ngotennet-pytorch, respectively.\nEquivariance in Monocular Human Pose Estimation\n7\nFig. 4. Samples from the datasets used in our experiments. SportsCap contains poses\nwith realistic full-body rotations that do not exist in the Human3.6M and the MPII-\nINF-3DHP datasets.\nthe GotenNet implementation is optimised for 3D, and we run it as-is by initial-\nising the third coordinate to 0 and extracting the first two coordinates for each\njoint in the output pose. In both cases, we reduce the number of parameters\nof the equivariant models to keep the same non-equivariant baseline ResNet, so\nthat the total number of parameters falls within the range of the other models\nin comparison.\n4.2\nDatasets\nWe first train the model on the Human3.6M dataset [9], which contains diverse\nmotion capture data in a laboratory setup. Following the standard training pro-\ntocol [13,21], we use the first five subjects (S1, S5, S6, S7, S8) for training and the\nremaining two (S9, S11) for testing. To examine the generalisation of the models,\nwe additionally evaluate them on the test set of the MPII-INF-3DHP dataset [14]\nwithout any fine-tuning. Besides the two common datasets, Human3.6M and\nMPII-INF-3DHP, that mostly contain simple standing and walking poses, we\npropose an additional evaluation on the more challenging SportsCap dataset [3]\n(without 3D ground truth) containing complex gymnastics poses with realistic\nfull-body rotations that do not exist in the Human3.6M and MPII-INF-3DHP\ndatasets (see Figure 4).\n8\nP. Melnyk et al.\nMethod\nType\nHuman3.6M\nMPII-INF-3DHP\nOriginal\nRotated\nOriginal\nRotated\nResNet [13]\nVanilla\n62.9 \u00b1 0.2 209.6 \u00b1 1.4\n144.0 \u00b1 0.7\n225.8 \u00b1 2.9\nResNet+aug\nVanilla\n64.2 \u00b1 0.1\n64.3 \u00b1 0.1\n152.3 \u00b1 1.1\n152.6 \u00b1 1.4\nPoseFormer [21]\nVanilla\n65.9 \u00b1 0.1\n232.4 \u00b1 5.2 139.9 \u00b1 0.7\n247.5 \u00b1 5.7\nPoseFormer+aug\nVanilla\n65.1 \u00b1 0.1\n65.0 \u00b1 0.2\n145.7 \u00b1 4.1\n146.2 \u00b1 1.8\nVNTransformer [1]\nFully equi.\n76.4 \u00b1 1.2\n76.4 \u00b1 1.2\n171.2 \u00b1 2.0\n171.2 \u00b1 2.0\nVNTransformer+aug Fully equi.\n76.8 \u00b1 1.0\n76.8 \u00b1 1.0\n174.5 \u00b1 9.2\n174.5 \u00b1 9.2\nGotenNet [2]\nFully equi.\n76.7 \u00b1 1.3\n76.7 \u00b1 1.3\n169.4 \u00b1 5.7\n169.4 \u00b1 5.7\nGotenNet+aug\nFully equi.\n76.8 \u00b1 1.3\n76.8 \u00b1 1.3\n171.9 \u00b1 3.0\n171.9 \u00b1 3.0\nVNTransformer\nHybrid\n64.2 \u00b1 0.1\n176.8 \u00b1 1.0\n153.2 \u00b1 2.1\n212.0 \u00b1 2.2\nVNTransformer+aug\nHybrid\n66.0 \u00b1 0.4\n66.0 \u00b1 0.3\n160.1 \u00b1 7.4\n160.2 \u00b1 6.9\nGotenNet\nHybrid\n64.3 \u00b1 0.3\n177.5 \u00b1 0.8\n143.7 \u00b1 4.3\n206.0 \u00b1 3.1\nGotenNet+aug\nHybrid\n65.8 \u00b1 0.2\n65.9 \u00b1 0.3\n151.1 \u00b1 0.6\n150.8 \u00b1 1.2\nTable 1. Quantitative results with evaluation Protocol 1 on the Original and Rotated\ntest sets of Human3.6M and MPII-INF-3DHP. The best results are highlighted in bold,\nand the second-best results are underlined.\nMetrics We use the mean per-joint position error (MPJPE) as the main eval-\nuation metric for comparisons. The MPJPE measures the average L2 distance\nbetween the estimated 3D human poses \u02c6X \u2208RN\u00d73 and ground truth X \u2208RN\u00d73\nin millimetre (mm), computed as:\nMPJPE(X, \u02c6X) = 1\nN\nN\nX\nn=1\n\u2225X \u2212\u02c6X\u22252 .\n(4)\nIn Table 1, we refer the MPJPE metric as Protocol 1. We also report\nthe Procrustes-aligned mean per-joint position error (PA-MPJPE), commonly\nknown as Protocol 2, in Table 2. The PA-MPJPE eliminates global rotation,\ntranslation, and scaling errors, focusing more on the local joint pose error be-\ntween the estimations and ground truth.\n4.3\nImplementation Details\nTraining details We use PyTorch [16] in our experiments. The inputs to the\nmodels are the 2D poses collected from an off-the-shelf estimator HRNet [4]. The\ninput 2D poses are standardised to a zero mean and a standard deviation of one.\nThe ground-truth 3D poses are in camera coordinates and root-aligned at the\norigin. All models are trained to minimise the mean squared error (MSE) between\nthe ground truth and estimated human poses, for a total of 100 epochs on the\ntraining set of the Human3.6M dataset, with a batch size of 1024 (reduced to\n512 for GotenNet due to computational constraints), Adam optimiser, an initial\nlearning rate of 10\u22123, and the ExponentialLR scheduler with \u03b3 = 0.96.\nEquivariance in Monocular Human Pose Estimation\n9\nMethod\nType\nHuman3.6M\nMPII-INF-3DHP\nOriginal\nRotated\nOriginal\nRotated\nResNet [13]\nVanilla\n44.5 \u00b1 0.1 109.1 \u00b1 0.8\n99.3 \u00b1 0.4\n134.6 \u00b1 1.0\nResNet+aug\nVanilla\n46.2 \u00b1 0.1\n46.2 \u00b1 0.1 102.6 \u00b1 2.0\n102.6 \u00b1 1.4\nPoseFormer [21]\nVanilla\n47.9 \u00b1 0.1\n127.0 \u00b1 1.6 97.4 \u00b1 1.0\n141.4 \u00b1 1.0\nPoseFormer+aug\nVanilla\n46.6 \u00b1 0.1\n46.5 \u00b1 0.2\n101.3 \u00b1 2.9 101.9 \u00b1 1.9\nVNTransformer [1]\nFully equi.\n56.2 \u00b1 0.8\n56.2 \u00b1 0.8\n124.0 \u00b1 2.8\n124.0 \u00b1 2.8\nVNTransformer+aug Fully equi.\n56.3 \u00b1 0.6\n56.3 \u00b1 0.6\n126.5 \u00b1 5.7\n126.5 \u00b1 5.7\nGotenNet [2]\nFully equi.\n55.5 \u00b1 0.7\n55.5 \u00b1 0.7\n125.5 \u00b1 5.3\n125.5 \u00b1 5.3\nGotenNet+aug\nFully equi.\n55.8 \u00b1 0.6\n55.8 \u00b1 0.6\n126.5 \u00b1 2.3\n126.5 \u00b1 2.3\nVNTransformer\nHybrid\n45.6 \u00b1 0.1\n101.2 \u00b1 0.7 106.2 \u00b1 0.8\n134.4 \u00b1 1.5\nVNTransformer+aug\nHybrid\n46.8 \u00b1 0.2\n46.8 \u00b1 0.2\n110.6 \u00b1 4.5\n110.1 \u00b1 3.6\nGotenNet\nHybrid\n45.6 \u00b1 0.1\n102.2 \u00b1 1.0 101.3 \u00b1 3.5\n130.1 \u00b1 1.0\nGotenNet+aug\nHybrid\n46.9 \u00b1 0.1\n47.0 \u00b1 0.1\n105.2 \u00b1 1.0\n104.1 \u00b1 1.1\nTable 2.\nQuantitative results with evaluation Protocol 2 on the test sets of Hu-\nman3.6M and MPII-INF-3DHP. The best results are highlighted in bold, and the\nsecond-best results are underlined.\nData augmentation When specified +aug, we perform data augmentation\nby randomly transforming the input X \u2208RN\u00d72 and the xy-coordinates of the\ncorresponding target pose X \u2208RN\u00d73 with the same rotation R\u03b8. The rotation\nangle \u03b8 is sampled from a uniform distribution during training. To evaluate the\nequivariance property, we also apply random rotations to the test data in the\nsame way, when specified Rotated.\nComputational resources All experiments are conducted on the NVIDIA\nA100 devices with 40GB of memory. To demonstrate the reproducibility, each\nmodel is evaluated on three different random seeds. The mean error and standard\ndeviation across three runs are reported for the quantitative results. The training\nand inference times are presented in Table 3.\n4.4\nResults and Discussion\nThe results presented in Tables 1 and 2 consistently demonstrate that the vanilla\nlifting models do not learn the equivariance property from the data (see the\nperformance on the Original vs. Rotated). However, we straightforwardly solve\nthis by augmentation, resulting in vanilla+aug models outperforming both fully\nequivariant and hybrid+aug models, across all the Rotated datasets. Notably,\nthe performance of the +aug models, in most cases, is slightly reduced on the\nOriginal datasets, since they do not contain poses resembling full-body rotations.\nFurthermore, the domain generalisation of the vanilla models (see Original\nMPII-INF-3DHP results) and vanilla+aug models (see Rotated MPII-INF-3DHP\nresults) is better than that of other models, suggesting that the augmentation\ndoes not incur overfitting to a specific dataset.\n10\nP. Melnyk et al.\nMethod\nType\nTraining time (1 epoch) Inference time (1 sample)\nResNet [13]\nVanilla\n11 s\n0.39 ms\nResNet+aug\nVanilla\n34 s\n0.39 ms\nPoseFormer [21]\nVanilla\n26 s\n2.23 ms\nPoseFormer+aug\nVanilla\n48 s\n2.23 ms\nVNTransformer [1]\nFully equi.\n5 m 18 s\n4.28 ms\nVNTransformer+aug Fully equi.\n5 m 18 s\n4.28 ms\nGotenNet\u2217[2]\nFully equi.\n11 m 5 s\n14.38 ms\nGotenNet+aug\u2217\nFully equi.\n11 m 7 s\n14.38 ms\nVNTransformer\nHybrid\n46 s\n4.68 ms\nVNTransformer+aug\nHybrid\n47 s\n4.68 ms\nGotenNet\nHybrid\n3 m 41 s\n14.68 ms\nGotenNet+aug\nHybrid\n3 m 43 s\n14.68 ms\nTable 3.\nTraining and inference time comparison. The most accurate models\n(vanilla+aug) have both the shortest training and inference times. \u2217The fully equivari-\nant GotenNet uses batch size 512 due to computational constraints. The best results\nare highlighted in bold, and the second-best results are underlined.\nOverall, the relation between the performance of the different model types\non the Rotated datasets can be written as\nvanilla+aug > hybrid+aug > fully equivariant > hybrid > vanilla\nconfirming our intuition that strictly imposing the equivariance constraint\nEq. (1) on a lifting model harms the task performance while guaranteeing ro-\nbustness to rotations by design. Coherently, hybrid+aug models are closer in\nperformance to that of vanilla+aug, indicating that relaxing the equivariance\nconstraint (the rotation-invariance of the depth prediction in this case) is a step\nin the right direction.\nThese observations are consistently supported by the qualitative examples\npresented in Figures 5 and 6, where vanilla+aug models demonstrate more accu-\nrate 3D pose estimations. More qualitative results are shown in Figures 7 and 8.\nRemarkably, the best performing models (vanilla+aug) are also among the\nfastest to train and have the fastest inference, as presented in Table 3. While the\naugmentation adds to the training time, in total it constitutes only an insignifi-\ncant fraction of the training time required by fully equivariant models.\nAll in all, the baseline ResNet [13] endowed with equivariance learned by aug-\nmentation clearly exhibits a favourable performance/training/inference trade-off,\nfollowed closely by PoseFormer+aug.\nHybrid model ablation We also conduct an ablation study for the hybrid\nmodels by using the first layer equivariant model features for the depth prediction\n(see Figure 3). The results presented in Table 4 suggest that no improvement\nis gained compared to the original hybrid models, despite the parameter count\nincrease caused by the growth of the dimensionality in the input to the non-\nequivariant baseline for depth-prediction."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary\n\n**Title:** On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation\n\n#### Main Contributions:\n1. **Rotation Equivariance as a Bias:** The authors argue that incorporating rotation equivariance as an inductive bias is simpler and more geometrically grounded compared to directly learning a point-to-point mapping.\n2. **Validation of Rotation Equivariance:** They demonstrate the effectiveness of rotation equivariance by showing improved performance on human poses akin to rotations in the image plane, which can be efficiently learned through augmentation.\n3. **Quantitative Evaluation:** The study evaluates various models including fully equivariant, hybrid, and vanilla architectures using metrics such as MPJPE (Mean Per-Joint Position Error) and PA-MPJPE (Procrustes-aligned Mean Per-Joint Position Error). \n4. **Augmentation Effectiveness:** Rotation-equivariant augmentation significantly improves model performance on rotated data compared to non-equivalent models.\n\n#### Methodology:\n1. **Data Sets:** The work uses Human3.6M, MPII-INF-3DHP, and SportsCap datasets for evaluation.\n2. **Models:**\n   - **Fully Equivariant Models:** Utilize VN-Transformer and GotenNet architectures that enforce exact rotation-equivariance.\n   - **Hybrid Models:** Combine equivariant models with non-equivariant baselines to predict depth coordinates.\n   - **Vanilla Models:** Use simple ResNet-based models without incorporating rotation-equivariance.\n3. **Training Details:**\n   - The models are trained using PyTorch on the Human3.6M dataset for 100 epochs, with a batch size of 1024 and Adam optimizer.\n   - Data augmentation is applied to both input poses and target poses during training.\n\n#### Key Findings:\n1. **Vanilla Models:** Do not learn rotation-equivariance from data; vanilla+aug models outperform fully equivariant and hybrid+aug models across all rotated datasets.\n2. **Augmentation Effectiveness:** Rotation-equivariant augmentation significantly improves model performance, with the best results achieved by vanilla+aug models on both original and rotated datasets.\n3. **Hybrid Models:** Perform close to vanilla+aug models, indicating that relaxing the equivariance constraint is beneficial for generalization.\n\n#### Conclusions:\n1. **Rotation Equivariance as a Bias:** Rotation-equivariance should be considered an inductive bias rather than a design constraint.\n2. **Efficient Learning of Rotation Equivariance:** The authors confirm that rotation-equivariance can be efficiently learned through augmentation without explicitly constraining the parameter space.\n3. **Model Performance and Training Inference Trade-off:** Hybrid+aug models perform closer to vanilla+aug models, suggesting that relaxing equivariance constraints is beneficial for generalization.\n\nThis work provides a comprehensive framework for understanding and leveraging rotation-equivariant properties in monocular 3D human pose estimation, highlighting the importance of augmentation techniques."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "PAtt: A Pattern Attention Network for ETA\nPrediction Using Historical Speed Profiles\nByeoungDo Kim\u2217, JunYeop Na, Kyungwook Tak, JunTae Kim, DongHyeon Kim, Duckky Kim\nNaver Corporation\n{byeoungdo.kim, junyeop.na, kyungwook.tak, jun tae.kim, donghyeon.kim67, duckky.kim}@navercorp.com\nAbstract\u2014In this paper, we propose an ETA model (Estimated\nTime of Arrival) that leverages an attention mechanism over\nhistorical road speed patterns. As autonomous driving and\nintelligent transportation systems become increasingly prevalent,\nthe need for accurate and reliable ETA estimation has grown,\nplaying a vital role in navigation, mobility planning, and traffic\nmanagement. However, predicting ETA remains a challenging\ntask due to the dynamic and complex nature of traffic flow.\nTraditional methods often combine real-time and historical\ntraffic data in simplistic ways, or rely on complex rule-based\ncomputations. While recent deep learning models have shown\npotential, they often require high computational costs and do not\neffectively capture the spatio-temporal patterns crucial for ETA\nprediction. ETA prediction inherently involves spatio-temporal\ncausality, and our proposed model addresses this by leveraging\nattention mechanisms to extract and utilize temporal features\naccumulated at each spatio-temporal point along a route. This\narchitecture enables efficient and accurate ETA estimation while\nkeeping the model lightweight and scalable. We validate our\napproach using real-world driving datasets and demonstrate\nthat our approach outperforms existing baselines by effectively\nintegrating road characteristics, real-time traffic conditions, and\nhistorical speed patterns in a task-aware manner.\nIndex Terms\u2014Estimated Time of Arrival (ETA) Prediction;\nTraffic Forecasting, Attention Mechanism, Spatio-Temporal Mod-\neling\nI. INTRODUCTION\nWith the widespread adoption of various transportation\nmeans, the advancement of intelligent systems, and the de-\nvelopment of autonomous driving, current driving practices\nhave significantly changed. Unlike the past, drivers today often\nfollow predefined routes set by navigation systems. These\nroutes are not only focused on reaching the destination but\nalso consider various factors such as the shortest travel time,\nthe most comfortable route, fuel efficiency, and optimization\nbased on traffic conditions. As a result, accurate ETA (Esti-\nmated Time of Arrival) goes beyond simply predicting arrival\ntimes. It plays a crucial role in decision-making for individual\ndrivers, as well as in logistics and transportation scheduling\nand optimization. [1]\u2013[8] ETA is a key factor in efficient\nresource allocation, cost reduction, and improving customer\nsatisfaction. However, improving the accuracy of ETA is a\nchallenging task since it is influenced by various traffic factors\nsuch as traffic volume, driving speed, and road characteristics.\nIt is essential to not only consider current traffic conditions\n\u2217Corresponding author\nbut also predict future traffic conditions and incorporate those\npredictions into ETA calculations. Therefore, ETA prediction\nmethods that are solely based on distance or current traffic\nconditions are insufficient for providing accurate estimates.\nMore precise ETA predictions require advanced techniques\nthat combine and analyze both past and present traffic data\nand predict future traffic conditions.\nGiven the significance and complexity of the problem,\nETA prediction has long been regarded as a core challenge\nwithin the domain of Intelligent Transportation Systems (ITS),\nand numerous methods have been proposed to address this\nchallenge. Previous studies on ETA prediction have explored\na variety of approaches, ranging from historical pattern-based\nmethods to deep learning models. Early methods focused\non matching past trajectories or applying statistical models\nbased on time and location features [9]\u2013[13]. With the rise\nof data availability and computational power, recent works\nhave leveraged neural networks, including RNNs, CNNs, and\nGNNs, to capture complex spatiotemporal dependencies across\nroad networks [14]\u2013[26]. These models emphasize real-time\ntraffic dynamics, spatial correlations among road segments,\nand contextual embeddings. However, challenges remain in\nterms of scalability, interpretability, and robustness under real-\nworld dynamic traffic conditions. Some models emphasize\nreal-time speed updates and traffic propagation between links,\nwhile others integrate external factors such as weather or\nevents. Despite their advancements, many approaches struggle\nwith generalization, scalability, or real-world applicability,\nhighlighting the need for more interpretable and efficient\nmodels.\nIn this paper, we propose an ETA prediction model based\non pattern speed information from driving logs. While many\nrecent approaches have primarily focused on capturing rela-\ntionships between neighbor links and congestion propagation\nbased on real-time speed information, they often employ\ncomplex methodologies such as representing the road network\nas a graph, applying GNNs with multi-step operations, or\npredefining strongly correlated distant links. In contrast, our\nproposed model focuses on the overall driving pattern across\nthe road network and effectively leverages it for ETA pre-\ndiction. Although individual driving instances are initiated by\ndiverse and often unpredictable personal factors, traffic flow,\nwhen observed collectively across a large population and over\ntime, tends to exhibit consistent and recurring patterns. These\narXiv:2601.13793v1  [cs.LG]  20 Jan 2026\npatterns arise from the aggregation of countless individual\ndriving behaviors, which are themselves influenced by both\nglobal and local factors. Global factors\u2014such as time of\nday, day of week, and broader social routines\u2014contribute to\nstatistically stable spatiotemporal distributions of traffic speed\nacross road segments. In contrast, local factors\u2014including\nregional events, construction, and accidents\u2014are irregular in\nnature and, while they can have strong localized impacts on\ntraffic, their influence on the overall traffic pattern tends to be\nlimited. Furthermore, if such local disruptions have already\noccurred, their effects are often reflected in the current states\nof adjacent links. For distant disruptions that have not yet\npropagated, the impact is typically minor, resulting in a limited\neffect on the broader traffic pattern.\nBased on this insight, if sufficient pattern information and\na reliable representation of the current state are provided, it\nis possible to achieve high ETA prediction accuracy without\nexplicitly modeling complex relationships between neighbor\nlinks. Based on this perspective, our model focuses on the\nuse of pattern speed and the inherent characteristic of the\nETA task itself rather than on representing the relationships\nbetween neighbor links. To utilize pattern speed effectively,\nwe apply an attention mechanism over temporal slots of\nthe pattern speed based on temporal embedding vectors and\nenvironmental features. Since the ETA task inherently involves\ntraversing a sequence of road links, the driving behavior on\nthe current link is causally dependent only on the previously\ntraversed links, making it a spatio-temporal causal problem.\nThis spatio-temporal causality has led prior methods to adopt\nsequential link-wise duration prediction, rather than simulta-\nneous estimation across all links.\nTo address this, we propose a novel method that accumu-\nlates and passes a time pre-projection vector along the route,\ncombined with k-cycle processing that allows the model to\nrefine its predictions iteratively. Typically, the entry time into a\ngiven link is represented as a scalar sum\u2014the sum of durations\nof previous links. However, our method encodes this time\ninformation as a vector, enabling the model to convey richer\ntemporal representations while also capturing uncertainty aris-\ning during accumulation. This vectorized temporal embedding\nallows for more informed and flexible ETA predictions, and\nrepeated refinements over multiple cycles further enhance\nprediction accuracy and robustness.\nConsequently, our model demonstrates substantially im-\nproved prediction performance and robustness, offering a\npractical and efficient solution for real-world ETA estimation\ntasks.\nII. RELATED WORKS\nTo address this challenge, a wide variety of ETA pre-\ndiction methods have been developed within the field of\nIntelligent Transportation Systems (ITS). One of the earlier\napproaches, Similarity-based travel time estimation [9], intro-\nduced a lightweight model that identifies historically similar\ntrips from origin-destination trip pairs and estimates ETA\nby adjusting historical speed data through scaling factors.\nThis approach accounts for both temporal dynamics and\ngeographic regional characteristics, and it offers the advantage\nof computational efficiency. However, its reliance on historical\npattern matching limits its generalizability to rare or unseen\nroutes and significantly diminishes its ability to adapt to real-\ntime conditions. To address these limitations, Sparse trajectory\nbased travel time estimation [10] proposed a more structured\nmodeling framework. This approach constructs a large sparse\n3D tensor incorporating road segments, time slots, and driver\nidentifiers. The tensor is then utilized to extract frequent\ntrajectory patterns, decompose travel routes into relevant sub-\npaths, and aggregate time costs across these sub-paths. This\nstructured representation enables more informed and inter-\npretable ETA predictions.\nWith recent advancements in deep learning, many neu-\nral network-based approaches have been proposed, offering\nsubstantial improvements over traditional methods. For in-\nstance, DeepTravel [15] avoids the use of explicit road net-\nwork graphs by discretizing geographic space into a two-\ndimensional N \u00d7 N grid. It models travel sequences in three\ndistinct stages\u2014starting, middle, and ending\u2014while leverag-\ning both short-term and long-term traffic features to predict\ntravel time. DCRNN [16] improves the modeling of spa-\ntiotemporal dependencies by combining diffusion convolution,\napplied over directed graphs representing the road network,\nwith GRU [27]-based temporal sequence modeling. Similarly,\nST-GCN [17] views traffic data as a graph-structured time\nseries and leverages spatial graph convolutions with temporal\ngated convolutions. This design allows the model to capture\nspatiotemporal relationships effectively while avoiding the\ncomputational complexity associated with recurrent layers.\nSTANN [18] further enhances spatiotemporal modeling by\nintroducing an encoder-decoder architecture with both spatial\nand temporal attention mechanisms, enabling more accurate\nlong-range traffic predictions.\nMore recently, GNNs [28], [29] have emerged as a powerful\nmodeling paradigm for ETA prediction, particularly due to\ntheir capacity to capture the topological structures and com-\nplex dynamics of road networks. CompactETA [22] proposes\nan architecture consisting of an asynchronous updater and a\nlightweight predictor. The updater employs a graph attention\nnetwork that runs recurrently for k steps to generate a pre-\ncomputed table of link representations, capturing multi-hop\nneighbor influences. During inference, the predictor retrieves\nrelevant representations from this table using a lightweight\nquery mechanism enhanced by positional encoding, allowing\nCompactETA to effectively model sequential travel while\nmaintaining computational efficiency. GNN-ETA [21] presents\na large-scale GNN model designed for real-world production\nenvironments. It estimates travel times over supersegments\nby integrating both real-time and historical traffic features,\nand it employs stabilization techniques such as MetaGradients\n[30] and Exponential Moving Averages (EMA) to ensure\nrobustness in real-world production environments. ConSTGAT\n[19] refines GNN-based modeling by introducing a three-\ndimensional graph attention network that jointly captures spa-\ntial and temporal correlations. Instead of treating spatial and\ntemporal dynamics separately, ConSTGAT builds a 3D graph\nwhere nodes correspond to road segments at different time\nslots, allowing attention to be applied across both space and\ntime simultaneously. After the attention-based aggregation,\nconvolutional windows are used to summarize local contextual\ninformation, which is then leveraged to predict travel times at\nboth the segment and route levels. Meanwhile, DuETA [20]\nintroduces a congestion-aware graph transformer architecture\nthat jointly models traffic congestion propagation and long-\nrange dependencies across the network. By leveraging both\na congestion-sensitive graph and a route-aware transformer,\nDuETA captures not only local interactions but also distant\ncorrelations among traffic segments. This design allows the\nmodel to remain robust under complex conditions, such as cas-\ncading traffic delays, which are challenging for conventional\napproaches to handle.\nIII. METHOD\nA. Problem Formulation\nWe formulate ETA prediction as a supervised regression\ntask, where the goal is to estimate the total duration of a given\nvehicle trajectory based on background information, road\ncontext data, and historical speed patterns. A vehicle route\nis composed of the background information and a sequence\nof connected links, denoted by:\n(R, b) = ([r1, r2, . . . , rn], b)\n(1)\nwhere b represents global environmental factors, such as the\nday of the week and the departure time, which significantly\naffect traffic patterns, and R denotes the ordered sequence of\nlinks that constitute the route.\nEach link ri is associated with a set of features consisting\nof:\nri = (ci, pi)\n(2)\nwhere ci represents the static and dynamic attributes of the link\n(e.g., road kind, number of lanes, curvature, real-time speed\nat departure), and pi denotes the pattern speed profile, which\nconsists of historical average speeds aligned by day of week\nand time of day.\nThe total travel duration d is defined as the sum of durations\nover all links:\nD =\nn\nX\ni=1\ndi\n(3)\nEach link duration di is modeled as a function of the back-\nground information b, the link features (ci, pi), and the link\nentry time ti:\ndi = flink(b, ci, pi, ti)\n(4)\nwhere the link entry time ti is computed recursively based on\nthe cumulative durations of the preceding links:\nti = t0 +\ni\u22121\nX\nk=1\ndk,\nwith\nt0 = departure time\n(5)\nBackground Features \ud835\udc1b\nLink Context Features \ud835\udc6a\nPattern Speed Features \ud835\udc0f\nPositional Embedding\nContextual Representation \ud835\udc73\nTemporal Embedding \ud835\udc86!\nCross-Attention\nMLP\nBackground Layer\nMLP\nPattern Layer\nMLP\nProjection Layer\nCumulative Sum\nMLP\nEstimation Layer\nMLP\nContext Layer\nQ\nK, V\n: concatenate\nPredicted Link Duration\n\ud835\udc51' \",\u2026 , \ud835\udc51'#\n\u00d7 \ud835\udc58\nFig. 1: The overall architecture of Model: Contextual repre-\nsentation are extracted from background, link context via MLP\nlayers, combined with temporal embedding, and processed\nthrough a cross attention with pattern speed profile and an\nestimation layer to produce temporal embedding. The proce-\ndure is repeated k times and the final temporal embedding is\nused to produce the final link-level ETA predictions.\nThe objective is to learn a function f that captures these\ndynamic interactions and outputs the estimated total duration:\n\u02c6D = f(b, R, t0)\n(6)\nwhere \u02c6D \u2208R+ is a scalar prediction representing the esti-\nmated travel time.\nThis formulation enables the model to account for the\ntemporal shift of traffic conditions along the route and to\nleverage both local road-level patterns and global temporal\ncontext for accurate ETA prediction.\nB. Structure of PAtt model\n1) Overall System: Fig. 1 illustrates the overall structure\nof the proposed PAtt (Pattern Attention) model. The model\nprocesses input features by decomposing them into three\ncomponents: (1) a global background feature vector b, (2)\nper-link context feature vectors C = [c1, c2, . . . , cn], and (3)\nper-link pattern speed profile vectors P = [p1, p2, . . . , pn]\ndefined over temporal slots.\nAt the initial cycle, the temporal embedding vectors are\ninitialized as zero vectors. The background feature vector\nb, which remains constant across all links, is provided to\neach link-level processing unit. Simultaneously, each link\ncontext feature ci is combined with b to form a contextual\nrepresentation li.\nFor each link, a pattern attention mechanism is applied.\nThe contextual representations L = [l1, l2, . . . , ln] is used\nas the query, while the pattern speed profiles P\u2014a temporal\nsequence of historical average speeds\u2014are used as both the\nkeys and values. The outputs of the attention module are con-\ncatenated with the corresponding contextual representations to\nproduce the temporal embedding vectors for each link.\nThese embeddings are intermediate representations, not\ndirect duration estimates. At each cycle, the embeddings\nare cumulatively updated along the route and then used as\ninputs for the next cycle. As the model proceeds through k\nrefinement cycles, the temporal embeddings for each link are\nprogressively refined.\nAfter k refinement cycles, the final temporal embedding\nvectors are projected into scalar values representing the es-\ntimated duration of each link. The total estimated duration is\ncomputed by summing all per-link predictions.\n2) Feature Representation: The input features of the PAtt\nmodel consist of three main components: background features,\nlink context features, and pattern speed features. Each compo-\nnent captures different aspects of temporal and spatial traffic\ninformation required for accurate duration prediction. Raw\nfeature values from different components are first preprocessed\nand then individually transformed through separate MLP lay-\ners to produce dense feature vectors suitable for downstream\nprocessing. This design allows the model to flexibly handle\nheterogeneous feature types and to learn appropriate represen-\ntations for each component.\na) Background Features (b): Global features that are\nconstant across all links within a trip. These include:\n\u2022 Day of the week\n\u2022 Time of departure (e.g., hour or minute of day)\n\u2022 Holiday indicator\nb) Link Context Features (C): Per-link features describ-\ning the physical, dynamic, and traffic-related characteristics of\neach link. These include:\n\u2022 Static attributes: road kind, number of lanes, curva-\nture, link length, speed limit\n\u2022 Real-time speed at the time of departure\n\u2022 Representative average speed\nc) Pattern Speed Features (P): A sequence of historical\naverage speeds for each link, retrieved according to future time\nslots relative to the departure time (e.g., every 20 minutes up\nto a 120-minute horizon). These historical profiles represent\ntypical traffic conditions at future moments from the departure\ntime and capture global and periodic traffic flow patterns.\nFormally, for link ri, we define:\npi = [p(1)\ni , p(2)\ni , . . . , p(T )\ni\n],\np(t)\ni\n\u2208R\n(7)\nwhere T denotes the number of future time slots.\nd) Feature Normalization and Alignment:\nNumerical\nfeatures are normalized using either min-max scaling or z-\nscore standardization, and categorical features are embedded\nvia learnable embeddings. Speed-related features are further\nprocessed through min-max normalization within predefined\nbounds, quantization, one-hot encoding, and optional embed-\nding. All features are aligned to ensure consistency across time\nand the sequential order of links.\n3) Pattern Attention Network: The Pattern Attention Net-\nwork is designed to extract temporally-aware link representa-\ntions by leveraging pattern speed profiles through an attention\nmechanism. It operates in k iterative refinement cycles, where\neach cycle updates the temporal representation of each link\nusing aggregated contextual and pattern-based information.\na) Extract Link Representation: Initially, the background\nfeature vector b and the link context feature C are concate-\nnated and passed through a multilayer perceptron (MLP) to\ngenerate a high-level link representation L:\nL = MLPbc([b; C])\n(8)\nIn the m-th cycle, this representation is concatenated with the\npre-projection temporal embedding vector e(m\u22121)\ni\nfrom the\nprevious cycle (initialized as zero in the first cycle), forming\nthe query vector q(m)\ni\n= [li; e(m\u22121)\ni\n] for attention.\nb) Pattern Attention over Temporal Slots: A soft atten-\ntion mechanism is applied over these temporal slots:\na(m)\ni\n= Attention(q(m)\ni\n, ki, vi),\nwhere ki = vi = pi (9)\nThe output a(m)\ni\nencodes how the temporal dynamics relate to\nthe current query. Through this mechanism, the model learns\nto attend to time slots that are most relevant to the current\ntraffic context.\nc) Temporal Embedding Refinement: The attention out-\nput a(m)\ni\nis concatenated with the link representation li and\npassed through another MLP to generate the updated pre-\nprojection temporal embedding:\ne(m)\ni,link = MLPemb([a(m)\ni\n; li])\n(10)\nAfter generating e(m)\ni,link for all links, they are cumulatively\nsummed over previous links up to i\u22121 to reflect the temporal\nshift in traffic:\ne(m)\ni\n= \u03a3i\u22121\nj=1e(m)\nj,link\n(11)\nThis updated embedding is then used again as part of the next\ncycle\u2019s query input.\nd) Final Projection: After k refinement cycles, the final\ntemporal embeddings e(k)\ni,link are passed through a projection\nMLP to produce the predicted duration \u02c6di for each link:\n\u02c6di = MLPproj(e(k)\ni,link)\n(12)\nThe total travel time \u02c6D is obtained by summing all per-link\npredictions:\n\u02c6D =\nn\nX\ni=1\n\u02c6di\n(13)\nC. Loss Function\nTo train the PAtt model, we define a composite loss that\naccounts for both fine-grained link-level accuracy and global\nroute-level consistency. The overall loss is composed of two\ncomponents: the link-level loss and the route-level loss.\na) Link-Level Loss: For each link ri, the model predicts\nthe travel duration \u02c6di, which is compared to the ground-truth\nduration di. We use the Smooth L1 loss (also known as Huber\nloss) to provide robustness against outliers while maintaining\nsensitivity to small errors:\nLlink = 1\nn\nn\nX\ni=1\nSmoothL1( \u02c6di, di)\n(14)\nThis loss encourages accurate per-link predictions and facili-\ntates stable training by controlling the gradient scale.\nb) Route-Level Loss: To ensure that the overall route-\nlevel prediction aligns with the true total travel duration,\nwe define an accumulation on the total predicted duration\n\u02c6D compared to the ground-truth duration D. We adopt the\nAbsolute Percentage Error (APE) as the route-level loss,\nwhile introducing a small constant Dlow = 20 (minutes) to\nstabilize the denominator and prevent instability caused by\ndisproportionately large errors on short routes.\nLroute =\n\u02c6D \u2212D\nmax(D, Dlow)\n(15)\nHere, Dlow acts as a lower bound for the denominator,\nensuring that the percentage error remains well-behaved even\nwhen D is small.\nc) Multi-Cycle Aggregation: To encourage consistent re-\nfinement across all cycles, both losses are computed not only\non the final outputs but also on the intermediate outputs from\neach refinement cycle m = 1, . . . , k. Let \u02c6d(m)\ni\ndenote the pre-\ndicted duration for link ri at cycle m, and \u02c6D(m) = Pn\ni=1 \u02c6d(m)\ni\nbe the corresponding total route duration. The overall training\nloss is the sum of all link and route losses over k cycles:\nLtotal =\nk\nX\nm=1\n\u0010\n\u03bb \u00b7 L(m)\nlink + L(m)\nroute\n\u0011\n(16)\nThis multi-cycle supervision strategy stabilizes learning and\nencourages progressive refinement of predictions throughout\nthe pattern attention iterations.\nIV. EXPERIMENTS\nA. Experimental Setup\n1) Dataset: We use a large-scale real-world dataset col-\nlected from South Korea for training and evaluation. The\ndataset includes vehicle trajectories annotated with per-link\ntravel durations, as well as dynamic traffic features such as\npattern speed profiles. The data spans the period from March\n1st, 2024, to February 28th, 2025. To ensure the robustness\nof the model, we filter out extremely short or long routes and\nretain only those with lengths between 5 km and 500 km. This\nfiltering results in a total of 294,186,507 valid routes. We use\n90% of these samples for training and the remaining 10% for\nevaluation. On average, each route contains 47 links, with a\nmean route length of 28 km and an average travel duration of\n35 minutes.\n2) Models for Comparison: We compare the proposed\nmodel against the following baselines and representative meth-\nods:\na) Rule-Based\nSpeed\nCombination\n(Baseline):\nThis\nbaseline computes ETA by combining real-time and pattern-\nbased speed profiles. For each link, a weighted average speed\nis calculated, where the weights are determined based on the\nexpected entry time into the link.\n1\n2\n3\n4\n5\n6\nNumber of Cycles\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nMAE\nAll Cycle Loss\nMin Cycle Loss\nLast Cycle Loss\n(a) MAE for each loss function by number of cycles\n1\n2\n3\n4\n5\n6\nNumber of Cycles\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nMAPE\nAll Cycle Loss\nMin Cycle Loss\nLast Cycle Loss\n(b) MAPE for each loss function by number of cycles\nFig. 2: Performance comparison of loss functions with varying\nnumber of cycles\nb) DeepTravel [15]: DeepTravel is an end-to-end ETA\nprediction model that transforms routes into grid sequences.\nIt employs a bidirectional LSTM with dual auxiliary losses\nto learn both forward and backward cumulative travel times.\nThis design enhances the model\u2019s ability to generalize across\nvariable-length routes and facilitates robust learning of tem-\nporal travel patterns.\nc) CompactETA [22]: CompactETA generates spatio-\ntemporal link representations using a graph attention network.\nIt then utilizes positional encoding to retain the sequential\ncontext of routes without relying on recurrent operations,\ninstead using precomputed link representations for efficient\ninference.\nWe exclude several recent models from direct comparison\nfor the following reasons:\n\u2022 GNN-ETA [21], DCRNN [16] and STANN [18] predict\nETA at the segment or link level and evaluate perfor-\nmance accordingly, which is not directly comparable with\nour route-level prediction setting.\n\u2022 DuETA is particularly designed for short-range urban\ndriving scenarios and is not suited for evaluating mod-\nels over the long and diverse routes considered in our\nexperiments.\nB. Experiment Results\n1) Ablation Study: In this section, we evaluate how dif-\nferent loss functions and the number of refinement cycles\nTABLE I: Comparison of MAPE across the proposed method\nand existing ETA prediction models\nMethod\nMAPE(%)\nBaseline\n12.1\nDeepTravel*\n13.30\nCompactETA*\n11.07\nDuETA*\n19.50\nPAtt\n8.78\n* Results are quoted from the corresponding references.\naffect training and performance, as illustrated in Figure 2. We\ncompare three training strategies:\n\u2022 All Cycle Loss: computes the total loss across all refine-\nment cycles\n\u2022 Min Cycle Loss: uses the lowest loss among all cycles\nfor training\n\u2022 Last Cycle Loss: uses only the last cycle\u2019s loss during\ntraining\nFor all experiments, the number of cycles for training was set\nto 4. As shown in Figure 2, the All Cycle Loss consistently\noutperforms the other loss strategies across all cycles. While\nMin Cycle Loss and Last Cycle Loss exhibit unstable perfor-\nmance, with fluctuations in both MAE and MAPE without\nclear trends as the number of cycles increases, All Cycle\nLoss demonstrates stable and gradually improving results. This\nconfirms the effectiveness of iterative refinement and supports\nour design choice of leveraging all cycles for training.\n2) Quantitative Result:\na) Performance Comparison: We compare our model\nagainst several baseline and recent methods in terms of Mean\nAbsolute Percentage Error (MAPE). While MAE and RMSE\nare also commonly used, they are highly sensitive to the\ndistribution characteristics of the dataset, making them less\nsuitable for cross-dataset comparisons.\nAs shown in Table I, our proposed model achieves the\nlowest MAPE, significantly outperforming both the traditional\nbaseline and recent deep learning-based models.\nb) Performance Across Route Lengths: We further an-\nalyze how the model performance varies according to route\nlength. Figure 3 presents the MAE and MAPE of the pro-\nposed model across different route lengths. As shown in\nFigure 3(a), the MAE increases with route length, which is\nan expected outcome since absolute travel time errors tend\nto grow proportionally with longer distances and durations.\nHowever, Figure 3(b) shows that the MAPE decreases as route\nlength increases, indicating that the model achieves relatively\nmore accurate predictions for longer routes when considering\nproportional errors. This trend demonstrates that the proposed\nmodel effectively leverages historical pattern speed profiles to\nforecast future traffic states even over longer time horizons. In\naddition, Figure 3(b) shows that the model maintains robust\nperformance even for short-distance routes, achieving a MAPE\nconsistently below 11%. Although short routes are inherently\nmore sensitive to small absolute errors (leading to higher\nMAPE), the proposed method still delivers reliable results.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nRoute Length\n2\n3\n4\n5\n6\n7\n8\nMAE\n(a) MAE according to route length\n0\n25\n50\n75\n100\n125\n150\n175\n200\nRoute Length\n6\n7\n8\n9\n10\nMAPE\n(b) MAPE according to route length\nFig. 3: Performance variation with respect to route length\nThese findings demonstrate that our model not only handles\nlong-range predictions effectively but also preserves strong\npredictive performance across diverse route lengths.\nV. CONCLUSIONS\nIn this paper, we proposed a novel ETA prediction model\nthat effectively leverages historical pattern speed profiles\nthrough a Pattern Attention mechanism. Unlike existing ap-\nproaches that heavily rely on capturing real-time neighbor link\ninteractions or complex road network structures, our method\nfocuses on extracting temporally-aware representations based\non global and local traffic patterns. Through extensive ex-\nperiments on large-scale real-world datasets, we demonstrated\nthat the proposed model achieves superior performance com-\npared to baselines, particularly in handling both short and\nlong routes. The results confirm that incorporating structured\npattern information not only improves predictive accuracy\nbut also enhances model robustness across various traffic\nconditions. Our findings highlight the importance of historical\ntraffic patterns in ETA prediction and suggest that lightweight,\npattern-driven architectures can serve as strong alternatives to\nmore complex graph-based models. In future work, we plan\nto further explore adaptive pattern extraction techniques and\ninvestigate the integration of real-time local event information\nto enhance model responsiveness.\nREFERENCES\n[1] W. Ji, S. Liu, K. Han, Y. Li, and T. Liu, \u201cThe share-a-ride problem with\nmixed ride-hailing and logistic vehicles,\u201d Transportation Research Part\nE: Logistics and Transportation Review, vol. 192, p. 103758, 2024.\n[2] S. Xue, Q. Zhang, and N. Shiwakoti, \u201cSharing a ride: A dual-service\nmodel of people and parcels sharing taxis with loose time windows of\nparcels,\u201d Systems, vol. 12, no. 8, p. 302, 2024.\n[3] W. Qi, L. Li, S. Liu, and Z.-J. M. Shen, \u201cShared mobility for last-mile\ndelivery: Design, operational prescriptions, and environmental impact,\u201d\nManufacturing & Service Operations Management, vol. 20, no. 4,\npp. 737\u2013751, 2018.\n[4] F. Fehn, R. Engelhardt, F. Dandl, K. Bogenberger, and F. Busch,\n\u201cIntegrating parcel deliveries into a ride-pooling service\u2014an agent-\nbased simulation study,\u201d Transportation Research Part A: Policy and\nPractice, vol. 169, p. 103580, 2023.\n[5] J. Teusch, J. N. Gremmel, C. Koetsier, F. T. Johora, M. Sester, D. M.\nWoisetschl\u00a8ager, and J. P. M\u00a8uller, \u201cA systematic literature review on\nmachine learning in shared mobility,\u201d IEEE Open Journal of Intelligent\nTransportation Systems, vol. 4, pp. 870\u2013899, 2023.\n[6] T. Modica, E. Tappia, C. Colicchia, and M. Melacini, \u201cIntegrating\narrival time estimation in truck scheduling: an explorative study in\ngrocery retailing,\u201d Production & Manufacturing Research, vol. 12, no. 1,\np. 2425678, 2024.\n[7] A. U. Raghunathan, D. Bergman, J. N. Hooker, T. Serra, and S. Kobori,\n\u201cSeamless multimodal transportation scheduling,\u201d INFORMS Journal on\nComputing, vol. 36, no. 2, pp. 336\u2013358, 2024.\n[8] C. Gao, F. Zhang, G. Wu, Q. Hu, Q. Ru, J. Hao, R. He, and Z. Sun, \u201cA\ndeep learning method for route and time prediction in food delivery\nservice,\u201d in Proceedings of the 27th ACM SIGKDD Conference on\nKnowledge Discovery & Data Mining, pp. 2879\u20132889, 2021.\n[9] H. Wang, X. Tang, Y.-H. Kuo, D. Kifer, and Z. Li, \u201cA simple baseline\nfor travel time estimation using large-scale trip data,\u201d ACM Transactions\non Intelligent Systems and Technology (TIST), vol. 10, no. 2, pp. 1\u201322,\n2019.\n[10] Y. Wang, Y. Zheng, and Y. Xue, \u201cTravel time estimation of a path\nusing sparse trajectories,\u201d in Proceedings of the 20th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\nKDD \u201914, (New York, NY, USA), p. 25\u201334, Association for Computing\nMachinery, 2014.\n[11] J. Li, \u201cBus arrival time prediction based on random forest,\u201d in 2017 5th\nInternational Conference on Frontiers of Manufacturing Science and\nMeasuring Technology (FMSMT 2017), pp. 867\u2013872, Atlantis Press,\n2017.\n[12] Y. Bin, Y. Zhongzhen, and Y. Baozhen, \u201cBus arrival time prediction\nusing support vector machines,\u201d Journal of Intelligent Transportation\nSystems, vol. 10, no. 4, pp. 151\u2013158, 2006.\n[13] M. Sinn, J. W. Yoon, F. Calabrese, and E. Bouillet, \u201cPredicting arrival\ntimes of buses using real-time gps measurements,\u201d in 2012 15th Interna-\ntional IEEE Conference on Intelligent Transportation Systems, pp. 1227\u2013\n1232, IEEE, 2012.\n[14] X. Hu, T. Binaykiya, E. Frank, and O. Cirit, \u201cDeepreta: an eta post-\nprocessing system at scale,\u201d arXiv preprint arXiv:2206.02127, 2022.\n[15] H. Zhang, H. qing Wu, W. Sun, and B. Zheng, \u201cDeeptravel: a neural\nnetwork based travel time estimation model with auxiliary supervision,\u201d\nin International Joint Conference on Artificial Intelligence, 2018.\n[16] Y. Li, R. Yu, C. Shahabi, and Y. Liu, \u201cDiffusion convolutional recurrent\nneural network: Data-driven traffic forecasting,\u201d in International Con-\nference on Learning Representations (ICLR \u201918), 2018.\n[17] B. Yu, H. Yin, and Z. Zhu, \u201cSpatio-temporal graph convolutional net-\nworks: a deep learning framework for traffic forecasting,\u201d in Proceedings\nof the 27th International Joint Conference on Artificial Intelligence,\npp. 3634\u20133640, 2018.\n[18] Z. He, C.-Y. Chow, and J.-D. Zhang, \u201cStann: A spatio\u2013temporal attentive\nneural network for traffic prediction,\u201d IEEE Access, vol. 7, pp. 4795\u2013\n4806, 2018.\n[19] X. Fang, J. Huang, F. Wang, L. Zeng, H. Liang, and H. Wang,\n\u201cConstgat: Contextual spatial-temporal graph attention network for travel\ntime estimation at baidu maps,\u201d in Proceedings of the 26th ACM\nSIGKDD international conference on knowledge discovery & data\nmining, pp. 2697\u20132705, 2020.\n[20] J. Huang, Z. Huang, X. Fang, S. Feng, X. Chen, J. Liu, H. Yuan, and\nH."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n**PAtt: A Pattern Attention Network for ETA Prediction Using Historical Speed Profiles**\n\n#### Main Contributions:\nThe proposed PAtt model leverages an attention mechanism over historical road speed patterns to predict Estimated Time of Arrival (ETA). It addresses the challenges posed by dynamic and complex traffic flow, which traditional methods often fail to handle effectively. The key contributions include:\n\n1. **Model Architecture**: The model is designed as a Pattern Attention Network that decomposes input features into three components: background information, per-link context features, and pattern speed profiles. These are processed through multiple layers of MLP (Multi-Layer Perceptron) networks, incorporating temporal embedding vectors for each link.\n\n2. **Attention Mechanism**: A key feature is the application of an attention mechanism over temporal slots of pattern speed profiles. This allows the model to focus on relevant time slots and capture spatio-temporal causality inherent in ETA prediction tasks.\n\n3. **Feature Representation**: The input features are decomposed into background, link context, and pattern speed components. These are processed through separate MLP layers to produce dense feature vectors suitable for downstream processing. Categorical features like categorical traffic conditions are embedded via learnable embeddings.\n\n4. **Loss Function**: A composite loss function is employed that combines both fine-grained link-level accuracy (using Smooth L1 loss) and global route-level consistency (using Absolute Percentage Error, APE). This ensures robust training while maintaining sensitivity to small errors.\n\n#### Methodology:\nThe PAtt model operates in k iterative refinement cycles. In each cycle, the temporal representation of each link is updated using aggregated contextual and pattern-based information. The final outputs are projected into scalar values representing estimated durations for each link.\n\n#### Key Findings:\n- **Performance Comparison**: The proposed PAtt model outperforms existing baselines significantly across various datasets, particularly in handling both short and long routes.\n- **Robustness Across Route Lengths**: The model demonstrates strong performance even on shorter routes, maintaining a MAPE consistently below 11%.\n- **Effectiveness of Historical Patterns**: Incorporating structured pattern information improves predictive accuracy and enhances robustness across different traffic conditions.\n\n#### Conclusions:\nThe PAtt model effectively leverages historical speed patterns through an attention mechanism to predict ETA. It outperforms existing methods, especially in handling both short and long routes. The findings underscore the importance of incorporating historical traffic patterns into ETA prediction models, suggesting that lightweight pattern-driven architectures can be strong alternatives to more complex graph-based models.\n\n### Summary:\nThe PAtt model offers a novel approach for ETA prediction by leveraging an attention mechanism over historical speed profiles. It effectively addresses the challenges posed by dynamic and complex traffic flow, demonstrating superior performance across various datasets and route lengths. This work highlights the importance of incorporating structured pattern information into ETA prediction models, suggesting that lightweight architectures can serve as strong alternatives to more complex methods."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Proceedings of Machine Learning Research vol 313:1\u201362, 2026\n37th International Conference on Algorithmic Learning Theory\nOptimal L2 Regularization\nin High-dimensional Continual Linear Regression\nGilad Karpel\nKARPELGILAD@GMAIL.COM\nDepartment of Data and Decisions Sciences, Technion\nEdward Moroshko\nEDWARD.MOROSHKO@GMAIL.COM\nSchool of Engineering, University of Edinburgh\nRan Levinstein\nRANLEVINSTEIN@GMAIL.COM\nDepartment of Computer Science, Technion\nRon Meir\nRMEIR@EE.TECHNION.AC.IL\nDaniel Soudry\nDANIEL.SOUDRY@GMAIL.COM\nDepartment of Electrical and Computing Engineering, Technion\nItay Evron\nITAY@EVRON.ME\nMeta\nEditors: Matus Telgarsky and Jonathan Ullman\nAbstract\nWe study generalization in an overparameterized continual linear regression setting, where a model\nis trained with L2 (isotropic) regularization across a sequence of tasks. We derive a closed-form\nexpression for the expected generalization loss in the high-dimensional regime that holds for arbi-\ntrary linear teachers. We demonstrate that isotropic regularization mitigates label noise under both\nsingle-teacher and multiple i.i.d. teacher settings, whereas prior work accommodating multiple\nteachers either did not employ regularization or used memory-demanding methods. Furthermore,\nwe prove that the optimal fixed regularization strength scales nearly linearly with the number of\ntasks T, specifically as T/ ln T. To our knowledge, this is the first such result in theoretical contin-\nual learning. Finally, we validate our theoretical findings through experiments on linear regression\nand neural networks, illustrating how this scaling law affects generalization and offering a practical\nrecipe for the design of continual learning systems.\nKeywords: Continual learning, Lifelong learning, Regularization methods, Regularization strength\n1. Introduction\nIn the era of large foundation models (e.g., LLMs), the ability to learn tasks sequentially without\ncatastrophic forgetting (McCloskey and Cohen, 1989) or retraining from scratch has become in-\ncreasingly critical (see surveys in Wang et al., 2024; van de Ven et al., 2024). Continual learning\nnot only saves substantial computational resources but is also essential in settings where previous\ndata cannot be retained, e.g., under privacy or storage constraints.\nWe focus on continual linear regression, a fundamental analytical setting that has proven useful\nfor illuminating diverse aspects of continual learning systems, such as the effects of task similarity\n(Lin et al., 2023; Hiratani, 2024; Tsipory et al., 2025), task recurrence (Evron et al., 2022; Kong\net al., 2023), overparameterization (Goldfarb et al., 2024), algorithms (Doan et al., 2021; Peng et al.,\n2023), regularization (Li et al., 2023; Levinstein et al., 2025), and trade-offs such as memory versus\ngeneralization (Li et al., 2025). Similar linear settings are also used to study continual classification\n(e.g., Evron et al., 2023; Jung et al., 2025).\n\u00a9 2026 G. Karpel, E. Moroshko, R. Levinstein, R. Meir, D. Soudry & I. Evron.\narXiv:2601.13844v1  [cs.LG]  20 Jan 2026\nKARPEL MOROSHKO LEVINSTEIN MEIR SOUDRY EVRON\nWe analyze a popular regularization scheme that penalizes changes in the parameter space to\nprevent forgetting previous expertise. Specifically, we focus on L2 regularization, which penalizes\nchanges isotropically, i.e., wt = argminw\n\b\nLt(w)+\u03bbd\u2225w\u2212wt\u22121\u22252\n. Despite not maintaining an\nadditional regularization weight matrix, L2 regularization has been found to work well in practice\n(Hsu et al., 2018; Lubana et al., 2021; Smith et al., 2023).\nThe regularization strength \u03bb plays a central role in methods like ours, governing the balance\nbetween stability and plasticity (Mermillod et al., 2013; Kirkpatrick et al., 2017). Theoretically\nguided scaling laws for the regularization strength can greatly reduce computational cost, alleviating\nthe need for extensive hyperparameter tuning. Only a few theoretical works have focused on the\nregularization strength. For example, Levinstein et al. (2025) shows that near-optimal convergence\nrates can be achieved using a fixed strength. However, their analysis pertains only to the training\nerror in the worst case, leaving open the question of whether the chosen strength is truly optimal.\nWhile some prior theoretical work analyzes the training loss and forgetting (e.g., Evron et al.,\n2022, 2026; Kong et al., 2023; Goldfarb et al., 2024; Levinstein et al., 2025), we adopt a com-\nplementary and widely studied statistical viewpoint, focusing on the generalization loss of our\ncontinual scheme. Following prior work in a similar spirit (e.g., Lin et al., 2023; Li et al., 2023;\nZhao et al., 2024), we study a random design in which labels are generated by an underlying (noisy)\nlinear \u2018teacher\u2019 (i.e., task ground truth). Importantly, our analysis accommodates multiple teachers,\nin contrast to several prior studies (e.g., Ding et al., 2024; Zhao et al., 2024; Levinstein et al., 2025),\nwhich assume a single global teacher for all tasks. Allowing multiple teachers provides a more\nrealistic model for practical scenarios where the underlying ground truth (e.g., user preferences or\nfraud behaviors) naturally evolve over time. Under an i.i.d. feature assumption common in statistical\nstudies (e.g., Lin et al., 2023; Goldfarb and Hand, 2023; Zhao et al., 2025), a single teacher yields\ngeneralization affected solely by label noise, while multiple teachers introduce task-level variation,\nagain, making the setting more realistic and challenging.\nA closely related paper by Lin et al. (2023), which also considered multiple teachers, studied\nan unregularized scheme where past tasks influence optimization only implicitly, through the bias\nof gradient algorithms (see also Evron et al. 2022). While their analysis enables strong theoretical\nresults, our findings highlight that incorporating a regularizer is crucial, particularly for reducing\nthe noise floor induced by label noise.\nOur results provide general insights for any setup with multiple teachers, whereas our more\nconcrete findings pertain to teachers drawn i.i.d. from some distribution\u2014allowing simple yet il-\nlustrative analysis that captures effects of teacher variance. We place particular emphasis on the\nregularization strength, proving that its optimal value scales nearly linearly with the number of\ntasks. This practical rule of thumb alleviates the need for costly hyperparameter tuning with many\ntasks, as we can now tune the regularization strength using only a few tasks, and then scale it ac-\ncordingly to longer task horizons. We empirically verify these findings on an MNIST-based dataset\nusing both a linear regression model and a ReLU-activated neural network.\nSummary of our contributions.\n1. We analyze generalization in continual linear regression in a high-dimensional regime, accom-\nmodating multiple teachers, label noise, and an arbitrary number of tasks\u2014a setup broader than\nthose in prior work (see Table 1).\n2. For both a single teacher and i.i.d. teachers, we identify the optimal regularization strength \u03bb\u22c6.\n2\nOPTIMAL L2 REGULARIZATION IN HIGH-DIMENSIONAL CONTINUAL LINEAR REGRESSION\n3. We demonstrate that L2 regularization, which does not require maintaining an additional weight-\ning matrix, can close a gap in prior theoretical literature by staying informative in noisy regimes.\n4. We validate our theoretical results in a standard regularized continual linear model, using both\nsynthetic random regression data and an MNIST-based classification problem.\n5. Finally, we demonstrate similar findings with simple neural networks, validating our theory-\nguided rule of thumb for choosing the regularization strength.\n2. Setting: Regularized Continual Linear Regression\nAs explained in the introduction, we study the illustrative continual linear regression setting using\nan L2 regularization scheme, described formally below.\nLearning tasks in a sequence.\nIn our setting, the learner is exposed to a sequence of T tasks\n((Xi, yi))T\ni=1, where Xi \u2208Rn\u00d7d, yi \u2208Rn.1 At each iteration t, the learner updates the predictor\nwt by fitting only the current task, without access to samples from previous tasks.\nTo be able to learn continually and mitigate catastrophic forgetting, it is common to add a\nregularization term towards the previous iterate wt\u22121. While the regularizer is often based on the\nFisher information of previous tasks (Kirkpatrick et al., 2017; Benzing, 2022), recent works have\nobserved empirically that L2 (isotropic) regularization achieves similar performance (Hsu et al.,\n2018; Lubana et al., 2021; Smith et al., 2023). Thus, we follow recent theoretical work (e.g., Li\net al., 2023; Levinstein et al., 2025) and focus on the following L2 regularization scheme.\nScheme 1 Continual Linear Regression with L2 Regularization\nInput: {(Xi, yi)}T\ni=1, initial iterate w0, regularization strength \u03bb\nFor each iteration t = 1, . . . , T:\nwt \u2190argminw\nn\n\u2225Xtw \u2212yt\u22252 + \u03bbd\u2225w \u2212wt\u22121\u22252o\nOutput: wT\nNote that for \u03bb \u21920, the algorithm reduces to optimizing the current task alone (as analyzed\nin Evron et al. (2022)), whereas for large values of \u03bb, the current task has negligible influence.\nWe normalize the regularization strength by the data dimensionality, following a convention com-\nmonly used in Bayesian settings, where the prior is often scaled in this way (Lee et al., 2018).\nFor ease of readability, we keep implicit any dependence of wt on other quantities (e.g., \u03bb, d).\nFurthermore, we collect all notation used throughout the paper in Table 2 (App. A).\n2.1. Statistical Setup: High-Dimensional Random Design with Linear Teachers\nTo study generalization, we adopt a statistical setup with random data, multiple (linear) teachers,\nand noisy labels. Variants of this setup are commonly used in the theoretical literature on continual\nlearning (e.g., Goldfarb and Hand, 2023; Lin et al., 2023; Zhao et al., 2024; Friedman and Meir,\n2025) and in broader machine learning research (e.g., Hsu et al., 2012; Belkin et al., 2020; Bartlett\net al., 2021). As shown in Table 1, our setup is comparatively permissive, allowing both label noise\nand multiple teachers (e.g., i.i.d. teachers).\n1. To simplify exposition, we assume all tasks have n samples, though our analysis can extend to varying sample sizes.\n3\nKARPEL MOROSHKO LEVINSTEIN MEIR SOUDRY EVRON\nData model.\nWe consider a random design, with independent random feature matrices\nX1, . . . , XT \u2208Rn\u00d7d, where the entries of each matrix Xt are i.i.d. random variables with mean 0,\nvariance vx > 0, and a finite (4 + \u03f5)th moment for some \u03f5 > 0.2 Moreover, we focus on the\nhigh-dimensional regime, where\nn, d \u2192\u221e,\nn\nd \u2192\u03b1 \u22641 .\nFor each task t \u2208[T] \u225c{1, . . . , T}, labels are generated according to a corresponding ground-truth\n\u2018teacher\u2019 vector w\u22c6\nt \u2208Rd via a noisy linear model. That is,\nyt = Xtw\u22c6\nt + zt ,\nwhere zt is a label noise vector, satisfying E[zt] = 0 and Cov(zt) = vzI for some noise variance\nvz \u22650, sampled independently of other tasks, i.e., of Xt\u2032, zt\u2032, \u2200t\u2032 \u0338= t.\nFinally, for technical reasons, we assume finite asymptotic teacher correlations. That is,\n(w\u22c6\ni \u2212w\u22c6\nj)\u22a4(w\u22c6\nk \u2212w\u22c6\nl ) \u2212\u2212\u2212\u2192\nd\u2192\u221eri,j,k,l ,\n\u2200i, j, k, l \u2208{0, 1, . . . , T} .\nImplications of the data model.\nHigh dimensionality allows us to derive explicit expressions\nusing the Marchenko\u2013Pastur theorem (Marchenko and Pastur, 1967). Importantly, accommodating\nmultiple teachers enables us to analyze i.i.d. teachers, thereby characterizing continual scenarios\nthat are more practical than those considered in most prior work (see Section 3.1).\nMetrics of interest.\nSome prior theoretical work in continual learning (e.g., Goldfarb et al., 2024;\nEvron et al., 2022, 2026) has analyzed the (training) squared error \u2225Xiw \u2212yi\u22252. However, under\nour statistical model, it is readily seen that averaging over the random test data and label noise yields\nE\nXiw \u2212yi\n2 = E\nXi(w \u2212w\u22c6\ni ) \u2212zi\n2 = vxn \u2225w \u2212w\u22c6\ni \u22252 + vzd ,\n(1)\nmaking \u2225w \u2212w\u22c6\ni \u2225the only quantity influenced by the actual solution w. Thus, we follow other\nprior work (Lin et al., 2023) and measure generalization by the distance to the teacher(s) generating\nthe labels. Our formal definitions are provided below.\nDefinition 1 (Individual Task Loss)\nFor any w \u2208Rd, the loss of task i \u2208[T] is defined as the\ndistance of w from the task\u2019s associated teacher. That is,\nLi(w) = \u2225w \u2212w\u22c6\ni \u22252 .\nDefinition 2 (Average Generalization Loss) The expected generalization loss at iteration t is de-\nfined as the average of the individual losses of all tasks encountered so far, evaluated at the iterate\nwt. That is,\nE [Gt] \u225c1\nt\nt\nX\ni=1\nE [Li(wt)] = 1\nt\nt\nX\ni=1\nE \u2225wt \u2212w\u22c6\ni \u22252 .\nNote that wt is a random variable, as it depends on Xi, zi and the possibly random w\u22c6\ni , \u2200i \u2208[t].\n2. For example, the entries of X1 can be N(0, 1) and the entries of X2 can be Unif[\u2212\n\u221a\n3,\n\u221a\n3].\n4\nOPTIMAL L2 REGULARIZATION IN HIGH-DIMENSIONAL CONTINUAL LINEAR REGRESSION\n3. Generalization Analysis and Optimal Regularization Scaling Laws\nIn this section, we derive a closed-form expression for the expected generalization loss (of Defini-\ntion 2). This general expression accommodates diverse teacher setups and serves as the foundation\nfor investigating an important special case in later sections.\nTheorem 3 (Generalization under General Teachers) Recall the high-dimensional statistical set-\nting of Section 2.1, where the number of samples and the dimension n, d \u2192\u221eand n\nd \u2192\u03b1 \u22641.\nMoreover, recall that vx, vz are the variances of features and labels, respectively. Then, the expected\ngeneralization loss (Definition 2) after learning T tasks with Scheme 1 is,\nE[GT ] = vzc 1 \u2212aT\n1 \u2212a\n|\n{z\n}\nLabel noise term\n+ 1\nT\nT\nX\ni=1\n\u2225w\u22c6\nT \u2212w\u22c6\ni \u22252\n|\n{z\n}\nTeacher variability term\n+ 2\n\u0000w\u22c6\nT \u2212\u00afw\u22c6\u0001\u22a4\nT\nX\ni=1\nb T\u2212i+1\u0000w\u22c6\ni\u22121 \u2212w\u22c6\ni\n\u0001\n|\n{z\n}\nInteraction term\n+\nT\nX\ni=1\nT\nX\ni\u2032=1\na T\u2212max(i,i\u2032)+1 b|i\u2212i\u2032|\u0000w\u22c6\ni\u22121 \u2212w\u22c6\ni\n\u0001\u22a4\u0000w\u22c6\ni\u2032\u22121 \u2212w\u22c6\ni\u2032\n\u0001\n|\n{z\n}\nAccumulated temporal (teacher) correlations\n,\nwhere we define w\u22c6\n0 \u225cw0 (by abuse of notation), the average teacher \u00afw\u22c6\u225c1\nT\nPT\ni=1 w\u22c6\ni , and\na \u225c1\n2\n\u0012\n1 \u2212\u03b1 +\n\u03bb(1 + \u03b1) + vx(1 \u2212\u03b1)2\nq\n\u03bb2 + 2\u03bbvx (1 + \u03b1) + v2x (1 \u2212\u03b1)2\n\u0013\n,\nb \u225c1\n2\n\u0012\n1 \u2212\u03b1 \u2212\u03bb\nvx + 1\nvx\np\n\u03bb2 + 2\u03bbvx(1 + \u03b1) + v2x(1 \u2212\u03b1)2\n\u0013\n,\nc \u225c\n1\n2vx\n\u0012\nvx(1 + \u03b1) + \u03bb\np\n\u03bb2 + 2\u03bbvx(1 + \u03b1) + v2x(1 \u2212\u03b1)2 \u22121\n\u0013\n.\nTo aid interpretation, we first decompose the generalization loss into its constituent terms.\nWe then provide a proof sketch outlining the key steps of the full proof presented in App. C.\nDecomposing the generalization loss.\nThe expected generalization in Theorem 3 can be decom-\nposed into four terms, interpreted as follows:\n\u2022 Label noise term: arises from label noise of the underlying model, scaling with label variance.\n\u2022 Teacher variability: captures the intrinsic mismatch between the current teacher and past teach-\ners due to task heterogeneity.\n\u2022 Interaction term: reflects how the deviation of the final teacher from the average teacher inter-\nacts with the sequence of teacher updates, discounting older changes.\n\u2022 Temporal correlations: accounts for correlations between successive teacher updates, describ-\ning how past changes compound and align (or misalign) over time.\n5\nKARPEL MOROSHKO LEVINSTEIN MEIR SOUDRY EVRON\nProof sketch.\nThe iterates of Scheme 1 follow the recursive form:\nwt =\n\u0010\nX\u22a4\nt Xt + \u03bbdI\n\u0011\u22121 \u0010\nX\u22a4\nt yt + \u03bbdwt\u22121\n\u0011\n.\nDenote Pt = \u03bbd\n\u0000X\u22a4\nt Xt + \u03bbdI\n\u0001\u22121, St:k \u225cPtPt\u22121 \u00b7 \u00b7 \u00b7 Pk and unroll the recursive form, to obtain\nwt \u2212w\u22c6\ni = St:1(w0 \u2212w\u22c6\n1) + 1\n\u03bbd\nt\nX\nk=1\nSt:kX\u22a4\nk zk +\nt\nX\nk=2\nSt:k\n\u0000w\u22c6\nk\u22121 \u2212w\u22c6\nk\n\u0001\n+ (w\u22c6\nt \u2212w\u22c6\ni ) .\nTo analyze E[GT ] = 1\nT\nP\ni E\u2225wT \u2212w\u22c6\ni \u22252, we invoke a Marchenko\u2013Pastur result to approximate\nthe resolvent\n\u0010\n1\nvxnX\u22a4X \u2212zI\n\u0011\u22121\nwith its deterministic equivalent (Couillet and Liao, 2022):\nE\n\u0014\u0010\n1\nvxnX\u22a4X \u2212zI\n\u0011\u22121\u0015\n\u2212\n\u2212\n\u00001 \u22121\n\u03b1 \u2212z\n\u0001\n+\nq\u00001 \u22121\n\u03b1 \u2212z\n\u00012 \u22124\n\u03b1z\n\u22122\u03b1/z\nI\na.s.\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nn,d\u2192\u221e, d/n\u21921/\u03b1 0 .\nWe use the resolvent and standard identities to obtain closed-form expressions for E[Pt], E[P2\nt ],\nand E[\n\u0000 1\n\u03bbd\n\u00012 Pt X\u22a4\nt Xt Pt] as scalar multiples of I, depending only on (\u03bb, vx, \u03b1). These are then\nsubstituted into the unrolled form to yield the explicit expression.\nSanity check.\nLin et al. (2023) studied our Scheme 1 in the limit \u03bb \u21920, i.e., an equivalent\nunregularized scheme where each task is solved to convergence via SGD initialized at the previous\niterate. Below, we verify that Theorem 3 recovers their Theorem 4.1 as a special case.\nExample 1 (No Regularization) Indeed, in the high-dimensional regime, Theorem 3 generalizes\nTheorem 4.1 in Lin et al. (2023). Specifically, after adjusting notation, their result becomes,\nE[GT ] = (1\u2212\u03b1)T\nT\nT\nX\ni=1\n\u2225w\u22c6\ni \u22252 + 1\nT\nT\nX\ni=1\n(1 \u2212\u03b1)T\u2212i \u03b1\nT\nX\nk=1\n\u2225w\u22c6\nk \u2212w\u22c6\ni \u22252 +\ndvz\nd\u2212n\u22121\n\u0010\n1 \u2212(1 \u2212\u03b1)T \u0011\n,\naligning with Theorem 3 in the high-dimensional, unregularized case (d, n \u2192\u221e, \u03bb \u21920, vx = 1).\nProof is given in App. C.1.\n3.1. Special Case: Multiple i.i.d. Teachers\nThus far, Theorem 3 has accommodated multiple \u201clabel-generating\u201d teachers. To obtain more mean-\ningful results, we now assume that teachers are sampled i.i.d.3 from a common distribution.\nAssumption 1 (i.i.d. Teachers)\nConsidering the statistical setting of Section 2.1, the teachers\nw\u22c6\n1, . . . , w\u22c6\nT are drawn i.i.d. from a distribution with mean w\u22c6and covariance \u03a3 where\n\u2225\u03a3\u2225\u2217\u2264C < \u221e, independently of all other random variables (features and labels).\nThis assumption is more permissive than most theoretical work in continual learning (Table 1),\nwhich typically assumes (i) a single teacher with label noise or (ii) joint realizability across training\nsets (as in highly overparameterized regimes). By contrast, in applications like autonomous systems\nand e-commerce, underlying dynamics often shift. For instance, varying agents and simulators\ninevitably introduce teacher variance due to environmental gaps (Zhao et al., 2020). Our setup more\naccurately captures these practical challenges by explicitly allowing for multiple teachers.\n3. More generally, our analysis accommodates teachers drawn independently from several distributions with a common\nmean and covariance. For ease of exposition, however, we present results only for the i.i.d. setting.\n6\nOPTIMAL L2 REGULARIZATION IN HIGH-DIMENSIONAL CONTINUAL LINEAR REGRESSION\nWe begin with Corollary 4, which specializes Theorem 3 to the i.i.d. teacher setting. We then use\nit to study how the regularization strength \u03bb impacts generalization. In this section, all expectations\nare taken over features, label noise, and teachers. For ease of presentation, we assume a zero\ninitialization w0 = 0. Full proofs are given in App. E.\nCorollary 4 (Generalization under i.i.d. Teachers) Under i.i.d. teachers (Assumption 1), and for\nall T > 1, the expected generalization loss of Scheme 1 becomes,\nE[GT ] = vzc1 \u2212aT\n1 \u2212a\n|\n{z\n}\nLabel noise\n+ aT \u2225w\u22c6\u22252\n|\n{z\n}\nTeacher scale\n+ 2Tr(\u03a3)\n| {z }\nTeacher\nvariance\n\u0012 1 \u2212b\n1 \u2212a + 2bT \u22121\nT\n+ aT (\u2212a + 2b \u22121)\n2(1 \u2212a)\n\u0013\n.\nThe following lemma contrasts regularization calibrated to the task horizon T against any fixed\nstrength. Specifically, the generalization loss with optimal \u03bb\u22c6(T) is monotonically decreasing in T\n(under a single teacher), but non-monotonic for any fixed strength (under i.i.d. teachers).\nLemma 5 (More Tasks Can Be Useful or Harmful\u2014Depending on Teacher Setup)\nConsider\nthe i.i.d. teacher setting (Assumption 1) with a label noise vz > 0.\n\u2022 Single teacher: When w\u22c6\n1 = \u00b7 \u00b7 \u00b7 = w\u22c6\nT (i.e., Tr (\u03a3) = 0), the expected generalization loss with\nan optimal horizon-dependent strength \u03bb\u22c6(T) is monotonically decreasing in the task horizon T.\n\u2022 Multiple i.i.d. teachers: When teachers vary across tasks (i.e., Tr(\u03a3) > 0), then under any\nfixed strength \u03bb > 0, the expected generalization loss is monotonically increasing in the task\nhorizon T for all T \u2265T \u2032 for a sufficiently large T \u2032.\n3.1.1. OPTIMAL SCALING OF THE REGULARIZATION STRENGTH: \u03bb\u22c6\u224d\nvx\u03b1T\nlog(SIGNAL/NOISE)\nNow, we characterize the optimal regularization strength \u03bb\u22c6that minimizes the generalization loss.\nA theory-guided rule for \u03bb is essential when hyperparameter tuning is prohibitive, as in continual\nlearning. In our setting, we prove a near linear optimal scaling with \u03b1, vx, T leading to mean-square\nconvergence to the \u201cglobal\u201d teacher, i.e., wT\nT\u2192\u221e\n\u2212\u2212\u2212\u2212\u2192w\u22c6.\nTheorem 6 (Optimal Regularization Scales Nearly Linearly with T) Under i.i.d. teachers (As-\nsumption 1) with non-zero4 mean teacher w\u22c6, the optimal fixed regularization strength that min-\nimizes the expected generalization loss after T iterations satisfies \u03bb\u22c6= \u03bb\u22c6(T) \u224d\nT\nln T . More\nprecisely, for any \u03f5 > 0, there exists T0 = O\n\u00001/\u03f52\u0001\nsuch that for all T \u2265T0,\n(1 \u2212\u03f5)\n2vx\u03b1T\nln\n\u0010\n4\u03b1T\u2225w\u22c6\u22252vx\nvz+vxTr(\u03a3)(1+\u03b1)\n\u0011 < \u03bb\u22c6< (1 + \u03f5)\n2vx\u03b1T\nln\n\u0010\n4\u03b1T\u2225w\u22c6\u22252vx\nvz+vxTr(\u03a3)(1+\u03b1)\n\u0011 .\nFurthermore, in the degenerate noiseless case of Tr(\u03a3) = vz = 0, it holds that \u03bb\u22c6\u21920.\nInterpretation.\nThe denominator can be interpreted as a log(signal/noise) factor. The signal\nconsists of \u2225w\u22c6\u22252 vx, capturing the strength of shared structure across tasks, which is reinforced\nover longer task horizons T, while n\nd \u2192\u03b1 measures how informative each task is. The internal\ndenominator scales with noise sources: label noise vz, feature noise vx, and teacher variance Tr(\u03a3).\nIn Section 6, we contrast our optimal strength with results from prior work.\n4. Since w0 = 0, additionally having E[w\u22c6\nt ] = 0 would directly imply an optimal regularization of \u03bb\u22c6\u2192\u221e.\n7\nKARPEL MOROSHKO LEVINSTEIN MEIR SOUDRY EVRON\nLemma 5 indicates that for fixed \u03bb, performance deteriorates as T exceeds a certain threshold\n(resembling overfitting). Consequently, given an unlimited \u201cbudget\u201d of tasks, the optimal strategy\nis to increase regularization as much as possible, as formalized below.\nTheorem 7 (Asymptotic Generalization under Strong Regularization)\nUnder i.i.d. teachers\n(Assumption 1), as the horizon T \u2192\u221e, expected generalization loss decreases monotonically\nwith strength \u03bb. Furthermore, the iterates converge in mean square to the \u201cglobal\u201d teacher w\u22c6:5\nlim\n\u03bb\u2192\u221e\n\u0010\nlim\nT\u2192\u221eE \u2225wT \u2212w\u22c6\u22252 \u0011\n= 0 .\n4. Experiments on Synthetic Regression Data\nNext, we empirically demonstrate how regularization shapes generalization and contrast it with the\nunregularized case, validating our theoretical analysis.\nNoisy teacher setup.\nFor each task i \u2208[T], we sample a teacher w\u22c6\ni \u223cN(w\u22c6, 1\ndI), matching\nAssumption 1. We use d = 50 dimensions, n = 30 examples per task, and initialization w0 = 0.\nLonger task sequences with stronger regularization overcome label noise.\nFigure 1(a) shows\nthat regularization reduces noise effects, but only when calibrated to the task horizon. We observe\nthat models with weaker regularization benefit from early tasks but soon plateau6 at a noise floor,\nreflecting high plasticity but low stability. Conversely, excessive regularization impairs plasticity,\neffectively preventing learning. Figure 1(b) confirms that the optimal regularization strength scales\nnearly linearly with the horizon\u2014balancing the stability\u2013plasticity tradeoff to lower the noise floor.\n100\n101\n102\nTasks (T)\n10\n1\n100\nGen. loss\nReg. strength\nd\n0\nd=10\nd=100\nd=1000\n(a) Regularization can decrease the label noise floor.\n100\n101\n102\n103\nTasks (T)\n10\n1\n100\n101\n102\nOptimal\nEmpirical\nTheoretical\n(b) Optimal strength grows nearly linearly with the\nnumber of tasks, as predicted by Theorem 6.\nWe compute \u03bb\u22c6with an exhaustive search.\nFigure 1: Regularization effects on synthetic data. Interactions between number of tasks (T), reg-\nularization strength (\u03bb), and resulting generalization loss (normalized by a trivial regressor w = 0).\nHere, the label noise and feature variances are vz = vx = 1; Each curve averages over 5 runs.\nWhile the unregularized scheme collapses under label noise, optimal regularization endures.\nThe unregularized scheme solved via gradient descent to convergence corresponds to our Scheme 1\nas \u03bb \u21920. While Evron et al. (2022, 2026) analyze this setup\u2019s training loss in jointly realizable\u2014or\nnoiseless\u2014settings, Lin et al. (2023) accommodates label noise and studies the generalization loss.\nHowever, as our Example 1 shows, that scheme fails under high label noise due to a noise floor\n5. Limits are taken sequentially (first T \u2192\u221e, then \u03bb \u2192\u221e) rather than simultaneously.\n6. Lemma 5 predicts that the expected generalization monotonically increases beyond a certain task. While this trend is\nobscured here by noise under the chosen hyperparameters, it becomes clearer in the figures of Section 5.\n8\nOPTIMAL L2 REGULARIZATION IN HIGH-DIMENSIONAL CONTINUAL LINEAR REGRESSION\ndvz\nd\u2212n\u22121 that persists as T \u2192\u221efor any number of samples n and dimension d (where \u03b1 < 1).\nIn contrast, we demonstrate next that a properly calibrated regularization in Scheme 1 overcomes\neven immense label noise vz, provided a sufficient task horizon T.\n101\n103\nLabel noise level (vz)\n10\n1\n100\n101\nGen. loss\nRegularization strength\nT = 1 :     d\n0\nT = 10 :   d\n0\nT = 100 :\nd\n0\nd\nd\nd\nFigure 2:\nOptimal regularization substantially im-\nproves generalization.\nHere, feature variance is\nvx = 1; each curve averages over 20 runs.\nFigure 2 compares the generalization\nloss of the aforementioned unregularized\nscheme (\u03bb \u21920) with that of the opti-\nmal \u03bb\u22c6. Indeed, in the presence of high\nlabel noise: (i) the unregularized scheme\nfails completely due to the high floor\nnoise, regardless of the task horizon; and\n(ii) a properly calibrated \u03bb\u22c6can substan-\ntially mitigate label noise after seeing\nenough tasks.\n5. Experiments on MNIST-Based Dataset\nWhile our analysis assumes i.i.d. features and a linear model, we now validate our results in more\nchallenging scenarios where the assumptions of Section 2 do not hold. We use finite-dimensional\nMNIST data rather than high-dimensional i.i.d. features, solve classification instead of regression,\ninject bit-flip rather than continuous label noise, and evaluate both linear models and ReLU net-\nworks. In all cases, the optimal regularization strength scales as T/ln T, consistent with Theorem 6.\n5.1. Constructing a Continual Binary Classification Dataset with Noisy (Flipped) Labels\nSamples.\nWe convert the MNIST (multiclass) dataset into a binary classification problem by\ngrouping digits 0\u20134 into class 0 and digits 5\u20139 into class 1. To facilitate our teacher models (below),\nwe narrow the data to a random subset of 500 training images and 200 test images.\nTeacher models.\nWe consider two types of teachers:\n(a) A single noiseless teacher that outputs the true labels (tasks still suffer from label noise).\n(b) Multiple i.i.d. teachers drawn from a global teacher. We first train a global teacher with the\nsame architecture as the evaluated model\u2014linear in Section 5.2 and neural in Section 5.3\u2014on\nthe entire subsample (train and test). Then, each task draws a teacher by adding Gaussian noise\nto the global teacher\u2019s weights. Teachers output binary labels (thresholded at 0.5).\nNoisy labels.\nEach task consists of the same sampled images, but differs in labels due to two noise\nsources. Specifically, each training label is first set by the evaluated teacher (noiseless or noisy), and\nthen corrupted by bit-flip label noise\u2014i.e., each label is flipped (i.i.d.) with probability p = 0.2.\n5.2. Linear Regression Experiments: Validation Beyond i.i.d. Features\nWe start from a regularized continual linear regression model (Scheme 1), trained with MSE to\npredict (noisy) binary labels generated by either a single noiseless teacher (Figure 3(a)) or multiple\nteachers drawn from a global teacher (Figure 3(b)). Predictions are thresholded at 0.5 to yield\nbinary outputs. Empirically, the optimal regularization scales roughly as T/ln T, consistent with\nTheorem 6 even though the i.i.d. feature assumption does not hold.\n9\nKARPEL MOROSHKO LEVINSTEIN MEIR SOUDRY EVRON\n10\n1\n100\n101\n102\n103\n104\n105\n106\n107\nRegularization strength ( d)\n0.2\n0.3\n0.4\n0.5\nGen. 0-1 loss\nNum. of tasks\nT = 1\nT = 10\nT = 100\nT = 1000\n(a) Linear: Single teacher.\n10\n1\n100\n101\n102\n103\n104\n105\n106\n107\nRegularization strength ( d)\n0.2\n0.3\n0.4\n0.5\nGen. 0-1 loss\n(b) Linear: Multiple i.i.d. teachers.\n10\n4\n10\n3\n10\n2\n10\n1\n100\n101\n102\nRegularization strength ( d)\n0.1\n0.2\n0.3\n0.4\n0.5\nGen. 0-1 loss\nNum. of tasks\nT = 1\nT = 10\nT = 100\nT = 1000\n(c) NN: Single teacher.\n10\n4\n10\n3\n10\n2\n10\n1\n100\n101\n102\nRegularization strength ( d)\n0.1\n0.2\n0.3\n0.4\n0.5\nGen. 0-1 loss\n(d) NN: Multiple i.i.d. teachers.\nFigure 3: Regularization in linear models and neural networks (NN). We plot the held-out clas-\nsification error after learning T noisy tasks. Curves are averaged over 5 seeds; stars mark minima.\n5.3. Neural Network Experiments: Validation Beyond Linear Models\nWe also consider a two-layer ReLU neural network (784\u2192500\u21921) with a sigmoid output, trained\ncontinually with L2 regularization. Again, noisy binary labels are generated by either a single noise-\nless teacher or multiple teachers sampled from a global (linear) teacher (Figures 3(c) and 3(d)). We\noptimize MSE for consistency with Scheme 1, thresholding predictions at 0.5. Even in this neural\nsetting, optimal regularization follows the T/ln T scaling, matching the linear theory in Theorem 6.\nTake-Home Message: Practical Recipe for Regularization Strength \u03bb\nWe observe that the optimal regularization scaling \u03bb\u22c6(T) \u224dT persists across architectures,\npotentially up to log factors.a To determine the leading constants, first tune \u03bb on a short task\nhorizon and then scale it for longer horizons.\nFor example, for T =1000, tune on T =10 and set \u03bb(1000) = 1000\n10 c\n\u03bb\u22c6(10) = 100 c\n\u03bb\u22c6(10).\na. While Theorem 6 suggests a T/ ln T, other unknown logarithmic terms\u2014e.g., \u2225w\u2217\u22252\u2014may dominate the de-\nnominator in practice. Consequently, we recommend focusing on the more significant linear scaling in T.\n6. Related Work and Discussion\nWe extend the literature on generalization in statistical continual learning settings (e.g., Lin et al.,\n2023; Li et al., 2023; Ding et al., 2024; Zhao et al., 2025) by incorporating both multiple teachers and\nL2 regularization. While optimization-focused studies typically provide worst-case upper bounds\n(e.g., Evron et al., 2022, 2026; Levinstein et al., 2025), our statistical approach enables an exact\naverage-case generalization analysis. This allows us to establish, to the best of our knowledge, the\nfirst characterization of optimal regularization strength, spanning both classical single-teacher and\ngeneral i.i.d. multi-teacher frameworks. A comparative overview is provided in Table 1.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Summary of the Paper \"Optimal L2 Regularization in High-dimensional Continual Linear Regression\"\n\n#### Main Contributions:\nThe paper explores generalization in an overparameterized continual linear regression setting, where a model is trained with L2 (isotropic) regularization across a sequence of tasks. The key contributions are:\n\n1. **Derivation of Generalization Loss**: The authors derive a closed-form expression for the expected generalization loss in high-dimensional settings that holds for arbitrary linear teachers. This provides insights into how regularization impacts performance.\n\n2. **Isotropic Regularization Effectiveness**: They demonstrate that isotropic regularization mitigates label noise under both single-teacher and multiple i.i.d. teacher settings, whereas prior work either did not employ regularization or used memory-demanding methods.\n\n3. **Optimal Regularization Strength Scaling**: The paper proves that the optimal fixed regularization strength scales nearly linearly with the number of tasks \\( T \\), specifically as \\( \\frac{T}{\\log(T)} \\). This is a significant theoretical result in continual learning, providing practical guidelines for designing continual learning systems.\n\n4. **Validation and Practical Insights**: The findings are validated through experiments on both linear regression and neural network models, illustrating how this scaling law affects generalization and offering a practical recipe for choosing the regularization strength.\n\n#### Methodology:\nThe authors adopt a statistical setup with random data, multiple (linear) teachers, and noisy labels to study generalization. They focus on L2 regularization, which does not require maintaining an additional weighting matrix but can stay informative in noisy regimes.\n\n#### Key Findings:\n- **Generalization Analysis**: The paper derives a closed-form expression for the expected generalization loss, decomposing it into four terms: label noise term, teacher variability, interaction term, and temporal (teacher) correlations.\n  \n- **Impact of Regularization Strength**: They show that the optimal regularization strength scales nearly linearly with \\( T \\), specifically as \\( \\frac{T}{\\log(T)} \\). This scaling law is crucial for understanding how to set regularization strengths in practical settings.\n\n#### Conclusions:\nThe paper provides a comprehensive theoretical and empirical analysis of L2 regularization in continual learning. It offers insights into the effects of multiple teachers, label noise, and task variability on generalization performance. The derived scaling laws offer practical guidelines for designing effective continual learning systems across various architectures (linear models and neural networks)."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "IEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n1\nFederated Balanced Learning\nJiaze Li\u2217, Haoran Xu\u2217, Wanyi Wu, Changwei Wang, Shuaiguang Li, Jianzhong Ju, Zhenbo Luo, Jian Luan,\nYouyang Qu, Longxiang Gao, Senior Member, IEEE, Xudong Yang, Lumin Xing\nAbstract\u2014Federated learning is a paradigm of joint learning in\nwhich clients collaborate by sharing model parameters instead of\ndata. However, in the non-iid setting, the global model experiences\nclient drift, which can seriously affect the final performance of\nthe model. Previous methods tend to correct the global model\nthat has already deviated based on the loss function or gradient,\noverlooking the impact of the client samples. In this paper, we\nrethink the role of the client side and propose Federated Balanced\nLearning, i.e., FBL, to prevent this issue from the beginning\nthrough sample balance on the client side. Technically, FBL\nallows unbalanced data on the client side to achieve sample\nbalance through knowledge filling and knowledge sampling using\nedge-side generation models, under the limitation of a fixed\nnumber of data samples on clients. Furthermore, we design\na Knowledge Alignment Strategy to bridge the gap between\nsynthetic and real data, and a Knowledge Drop Strategy to\nregularize our method. Meanwhile, we scale our method to\nreal and complex scenarios, allowing different clients to adopt\nvarious methods, and extend our framework to further improve\nperformance. Numerous experiments show that our method\noutperforms state-of-the-art baselines. The code is released upon\nacceptance.\nIndex Terms\u2014Out-of-distribution detection, multimodal large\nlanguage models, zero-shot learning, CLIP, anomaly detection,\nvision-language models\nI. INTRODUCTION\nFederated learning (FL) [1] allows distributed clients [2]\nto collaboratively train a shared global model using their local\nmultimedia data\u2014such as images, videos, and audio\u2014without\nuploading raw data to a centralized server. This paradigm\nnot only preserves user privacy but also significantly reduces\ncommunication overhead, which is especially critical when\nhandling large-volume multimedia content.\n\u2217Equal contribution.\nJiaze Li, Jianzhong Ju, Zhenbo Luo and Jian Luan are with MiLM Plus\nof Xiaomi Inc. Haoran Xu is with Zhejiang University. Wanyi Wu is with\nShandong University. Changwei Wang, Shuaiguang Li, Youyang Qu and\nLongxiang Gao are with the Key Laboratory of Computing Power Network\nand Information Security, Ministry of Education, Shandong Computer Sci-\nence Center (National Supercomputer Center in Jinan), Qilu University of\nTechnology (Shandong Academy of Sciences), Jinan, 250013, China; Shan-\ndong Provincial Key Laboratory of Computing Power Internet and Service\nComputing, Shandong Fundamental Research Center for Computer Science,\nJinan, 250014, China; Xudong Yang and Lumin Xing are with Shandong Key\nLaboratory of Digital Diagnosis and Treatment of Thoracic Oncology, The\nFirst Affiliated Hospital of Shandong First Medical University & Shandong\nProvincial Qianfoshan Hospital and Shandong Engineering Research Center\nof Intelligent Surgery, The First Affiliated Hospital of Shandong First Medical\nUniversity & Shandong Provincial Qianfoshan Hospital, Jinan, China.\nThis work was supported by the Shandong Provincial Key R&D\nProgram\nNo.2025KJHZ013,\nNational\nKey\nR&D\nProgram\nof\nChina\nNo.2025YFE0216800, Shandong Provincial University Youth Innovation and\nTechnology Support Program No.2022KJ291, Shandong Provincial Natural\nScience Foundation for Young Scholars Program No.ZR2025QC1627 and\nQilu University of Technology (Shandong Academy of Sciences) Youth\nOutstanding Talent Program No. 2024QZJH02, Shandong Provincial Natural\nScience Foundation for Young Scholars (Category C) No.ZR2025QC1627.\nHowever, current FL systems suffer from severe perfor-\nmance degradation due to the non-independent and identically\ndistributed (non-IID) nature of client-held multimedia data.\nFor instance, user-generated images or videos often exhibit\nstrong domain bias (e.g., specific lighting conditions, object\ncategories, or recording environments), leading to skewed local\ndistributions. Such heterogeneity can cause the global model to\noverfit to dominant client modalities or domains, resulting in\nsuboptimal generalization across diverse multimedia scenarios.\nNumerous approaches have been proposed to mitigate this\nissue, including FedProx [3], SCAFFOLD [4], and MOON [5].\nNevertheless, these methods primarily operate at the optimiza-\ntion level\u2014adjusting gradients or loss functions to correct\nmodel drift after it occurs\u2014rather than addressing the root\ncause: the imbalance and scarcity of representative multimedia\nsamples across clients. Moreover, under the traditional FL as-\nsumption of resource-constrained edge devices, most existing\nworks overlook the potential for on-device multimedia model\nenhancement.\nRecent breakthroughs in generative models, particularly\ndiffusion models [6]\u2013[8], have enabled the synthesis of high-\nfidelity multimedia content, especially photorealistic images.\nModels like Stable Diffusion [9] generate visual data that is\nsubstantially more realistic and semantically coherent than out-\nputs from earlier generative frameworks such as GANs [10],\nVAEs [11], or transformer-based synthesizers [12]. This ad-\nvancement makes synthetic multimedia data a viable surrogate\nfor real user data in privacy-sensitive learning scenarios.\nConcurrently, the computational capabilities of edge devices\nare rapidly evolving, enabling the deployment of sophisticated\ngenerative models directly on multimedia-capable hardware.\nFor example, SnapFusion [13] and MobileDiffusion [14] have\nsuccessfully quantized and optimized Stable Diffusion for\nmobile platforms, allowing second-scale generation of high-\nquality images that rival those produced by cloud-based coun-\nterparts [15]. Even in highly constrained settings, recent work\nhas demonstrated Stable Diffusion inference on a Raspberry Pi\nwith only 260 MB of RAM. These developments open a new\navenue for FL: instead of merely consuming data, edge clients\ncan now generate diverse, synthetic multimedia samples to\nbalance their local distributions.\nThis leads us to rethink the role of the client in federated\nmultimedia learning\u2014not just as a passive data holder, but\nas an active participant capable of enriching its local dataset\nthrough on-device generative modeling. By leveraging such\ncapabilities, we aim to proactively mitigate the non-IID prob-\nlem at the data level, specifically from the perspective of\nmultimedia sample balance [16].\nIn this paper, we propose Federated Balanced Learning\n(FBL), with employing edge-side generative model for knowl-\narXiv:2601.14042v1  [cs.CV]  20 Jan 2026\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n2\nedge filling to achieve sample balance, delve into the details\nand ask two questions:\nQ1: How to achieve sample balance on client side? In\na naive approach, we can generate samples using edge-side\ngenerative models to complete the number of all classes of\nsamples to the maximum number of classes to achieve sample\nbalance. The disadvantage of the above method is that in\ntheory there is no upper limit to the amount of synthetic\ndata it needs to generate. Moreover, in real scenarios, the\nusage duration of computational resources on the client side is\nlimited. To improve the generality, we propose the knowledge\nfilling strategy under constrained computation. In constrained\ncomputation, the number of client samples does not increase\nand in this scenario, the duration of available computing re-\nsources on the client side also remains unchanged. Therefore,\nour solution firstly divides the classes on every client into\ndata-excessive, data-scarce, and data-missing classes based on\nthe expected balanced number of samples. Then, knowledge\nsampling is applied to data-excessive classes, and knowledge\nfilling is used for data-scarce and data-missing classes to\nmaintain sample balance under constrained computation.\nQ2: How can the gap between synthetic data and real\ndata be bridged? Considering the domain gap [17] between\nsynthetic data generated by the model and real data, we\npropose a Knowledge Alignment Strategy. We alse design a\nKnowledge Drop Strategy to regularize the method.\nFurthermore, we scale our method FBL into real com-\nplex scenarios where the duration of computational resources\namong clients may be unbalanced and extend FBL our frame-\nwork to further improve performance. Finally, we conducted\nexperiments with real-world datasets under other domains,\nsuch as satellite datasets [18], and discussed the existing\nlimitations and future prospects of FBL.\nWe summarize the main contributions as follows:\n\u2022 We rethink the role of the client side and propose FBL to\nprevent the non-iid problem from the beginning through\nsample balance on the client side.\n\u2022 We design a Knowledge Alignment Strategy to bridge the\ngap between synthetic and real data, and a Knowledge\nDrop Strategy to regularize our method. Extensive exper-\niments validate that our approach achieves best results.\n\u2022 We extend our framework to further improve performance\nin real and complex scenarios.\nII. RELATED WORKS\nPrevious research has addressed the challenge of non-\nIID data in federated learning. FedProx [19] incorporates\nan additional term into the local model\u2019s objective func-\ntion. Works such as SCAFFOLD [4] address client drift\nby employing control variates for gradients, albeit without\ndirectly considering inconsistencies between local and global\nobjectives. MOON [5] utilizes the similarity between model\nrepresentations to correct the local training of individual\nparties. FedGen [20] enables the server to generate features\nthat conform to the global data distribution by sharing the\nlabels of clients and distilling globally. However, the sharing\nof client labels poses privacy risks [21].\nGenerative Models in FL. With the development of genera-\ntive models [22]\u2013[24], there is a renewed awareness of the im-\nportance of synthetic data and generative models. Generating\nhigh-quality images based on text prompts has become a new\nparadigm. Therefore, a few works have explored the usage of\nsynthetic data, especially images as training data. For example,\nsomeone [25] explore the ratio between synthetic data and real\ndata. On the other hand, synthetic data and generative models\nare not sufficiently explored in the FL setting. FGL [26]\ntrains the model entirely using synthetic data [27], with the\ndisadvantage of using global tag sharing, and having privacy\nrisk [21].\nTherefore, how to integrate the existing edge-side generative\nmodel in FL without privacy concerns is a key challenge.\nOur solution is for each client to achieve sample balance to\nprevent the non-IID issue from the beginning by using edge-\nside generative models.\nIII. PRELIMINARY\nIn federated learning (FL), a system with K clients is\nconsidered, where each client possesses its own dataset Dk =\n{(xi, yi)}Nk\ni=1. Here, xi represents a data sample and yi rep-\nresents the corresponding label. The goal is to collaboratively\ntrain a global model \u03b8 that minimizes the aggregate loss across\nall clients\u2019 data without requiring direct access to each client\u2019s\ndata.\nThe objective function for traditional federated learning\nmethods is formulated as:\nmin\n\u03b8\n1\nK\nK\nX\nk=1\nLDk(\u03b8),\n(1)\nwhere LDk(\u03b8) is the expected loss over the data distribution\nDk of the k-th client:\nLDk(\u03b8) = E(xi,yi)\u223cDk[\u2113(h(xi; \u03b8), yi)],\n(2)\nwith \u2113being a cross-entropy loss function and h(xi; \u03b8) the\nmodel\u2019s prediction for input xi.\nIV. METHODOLOGY\nIn this section, we present our approach in four parts: Class\nClassification, Knowledge Filling, Knowledge Sampling, and\nExtended and Unified Framework.\nSpecifically, in our algorithm, each client first divides all\nclasses into three categories: data-excessive, data-scarce, and\ndata-missing. We then apply knowledge sampling for the data-\nexcessive classes and knowledge filling for the other two\ncategories. Finally, we further expand our framework in the\nExtended and Unified Framework section. Our framework is\nshown in Figure 1, and the detailed workflow is presented in\nAlgorithm 1.\nA. Class Classification\nIn constrained computation settings, achieving sample bal-\nance while ensuring that the number of samples per client does\nnot exceed the original amount is crucial. To address this, we\ndesignate the mean of the sample counts across all classes in\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n3\nClass\nClassification\nKnowledge Filling\nKnowledge Sampling\nConcatenate\nEdge-side\nGenerative Model\nPrompt\nEmbedding\nGenerated\nImages\nclass\nnumber\nclass\nnumber\nclass\nnumber\nclass\nnumber\n...\n...\nServer\nFig. 1. Illustration of FBL. Our framework mainly includes two parts: Knowledge Filling and Knowledge Sampling. Before these two parts, our framework\nfirst needs to determine the distributional balance point and then classify all classes into three categories: data-excessive, data-scarce, and data-missing. For\ndata-excessive classes, we use knowledge sampling, while for data-scarce and data-missing classes, we utilize knowledge Filling.\neach client as the distributional balance point. Mathematically,\nthis balance point is calculated as follows:\nBk =\n$\n1\nC\nC\nX\nc=1\nnc\nk\n%\n(3)\nwhere Bk represents the distributional balance point of the\nk-th client, C denotes the total number of classes, and nc\nk\nindicates the number of samples in class c. The operator \u230a\u00b7\u230b\ndenotes the floor function, which ensures that the calculated\nmean is rounded down to the nearest integer. This method\nensures that the distribution of samples is balanced across all\nclients without increasing the overall sample size.\nTo further categorize the data based on this balance point,\nclasses with sample counts exceeding the balance point are\ntermed data-excessive classes. Conversely, classes with sample\ncounts less than the balance point but greater than zero are\ndefined as data-scarce classes. Classes with no samples are\nclassified as data-missing classes.\nB. Knowledge Filling\nTo achieve sample balance, particularly for data-scarce and\ndata-missing classes, we employ the realistic generation ca-\npabilities of edge-side Stable Diffusion, with specific prompts\ndetailed in the Appendix, to augment the sample numbers of\nthese classes to the distributional balance point. To address the\ndomain gap between generated images and real-world client\nimages, we propose two strategies: Knowledge Alignment\nStrategy and Knowledge Drop Strategy. The former aims\nto narrow the domain gap between synthetic data and real\ndata, while the latter introduces a regularization technique\nsimilar to Dropout [28] to prevent overfitting in the Knowledge\nAlignment Strategy, as shown in Figure 2.\nKnowledge Alignment Strategy. The domain gap between\nthe synthetic data and the real data may adversely affects\nmodel performance, rendering it suboptimal. To address this\nissue, we propose a Knowledge Alignment strategy. Specif-\nically, for each class i within the categories of data-scarce\nand data-missing classes, we create a learnable embedding\nPi to bridge the gap between the generated domain and\nthe real domain. Following the popular settings in previous\nfederated learning works, we use image classification tasks\nas an example, where a typical image classification network,\nsuch as ResNet-18 [29], usually comprises a feature extractor\nF and a classification head H. An image, when passed\nthrough the feature extractor F, yields a prototype, which,\nafter being processed by the classification head, results in\nthe final prediction. Inspired by FedDBE [30], we augment\nthe generated image prototypes with learnable embeddings\nand then process them through the classification head. The\nformulation is as follows:\n\u02c6yij = H(F(Iij) \u2295Pi)\n(4)\nwhere Pi is the learnable embedding for class i, and Iij is\nthe j-th generated image of class i. The image Iij is first\npassed through the feature extractor F to obtain a prototype,\nwhich is then element-wise added to the embedding Pi. The\nresultant vector is subsequently fed into the classification head\nH to produce the model\u2019s prediction \u02c6yij. This embedding-\nbased augmentation method effectively compensates for the\ndomain gap, thereby enhancing the overall model performance.\nKnowledge Drop Strategy. To further improve the robustness\nof the model and prevent overfitting during the learning of\nembeddings, we introduce a Knowledge Drop Strategy. This\nstrategy involves randomly dropping a subset of samples\nwithin each batch, thereby excluding their embeddings during\ntraining. Let \u03b6 be the drop rate, representing the proportion of\nsamples to be dropped. For a given batch size B, the number\nof samples to be dropped is \u03b6B. The formulation for the\nKnowledge Drop Strategy is as follows:\n\u02c6yij =\n(\nH(F(Iij) \u2295Pi)\nif Iij is not dropped\nH(F(Iij))\nif Iij is dropped\n(5)\nwhere Iij is the j-th image of class i in the batch. If Iij is\nselected to be dropped, its embedding Pi is not added to the\nprototype. This random dropping of embeddings, controlled by\nthe drop rate \u03b6, helps in regularizing the model and mitigating\noverfitting, as shown in Figure 2.\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n4\nFeature Extractor\nPi\nZij\nIij\nClassification Head\n\u03f4\ud456\u03e5\nTrainable\nKnowledge Drop\nFig. 2.\nIllustration of the Knowledge Alignment Strategy and Knowledge\nDrop Strategy.\nC. Knowledge Sampling\nIn scenarios characterized by data-excessive classes, it is\nimperative to employ sampling methods to downsample the\nnumber of samples to a distributional balance point. Simulta-\nneously, we must ensure that the performance of the model\non each client remains as close as possible to the performance\nobserved prior to sampling. The target is to minimize the\nperformance discrepancy between the models trained on the\npre-sampled and post-sampled data, ensuring that the diversity\nand representation of data are preserved as closely as possible.\nTo achieve this, we optimize the following objective:\nmin\nM\nfp\n\uf8eb\n\uf8edM(\nCe\nX\ni=1\nSi\nX\nj=1\nXij),\nCs\nX\nl=1\nS\n\u2032\nl\nX\nm=1\nXlm\n\uf8f6\n\uf8f8\n\u2212fp\n\uf8eb\n\uf8ed\nCe\nX\ni=1\nSi\nX\nj=1\nXij,\nCs\nX\nl=1\nS\n\u2032\nl\nX\nm=1\nXlm\n\uf8f6\n\uf8f8\n(6)\nwhere M defines the sampling operation applied to the data-\nexcessive class collection. Si represents the set of samples\nfrom the i-th data-excessive classes and Ce defines the total\nnumber of data-excessive classes with Xij denoting the j-th\nsample in the i-th data-excessive class. On the other side, Cs\ndefines the total number of special classes. S\n\u2032\nl denotes the set\nof samples from the special classes, which include data-scarce\nand data-missing classes, with Xlm denoting the m-th image\nin the l-th special class.\nHere, fp(\u00b7, \u00b7) represents the model\u2019s performance function,\nwhich takes both the aggregated Xij and Xlm as inputs,\nevaluating the combined effect on model performance. This\napproach ensures that the model maintains robust performance\nacross all clients, even after the sampling process.\nKnowledge Sampling Based on Loss. The naive sampling\nmethod for data-excessive classes is random sampling, indis-\ncriminately selecting from the available class samples under\nthe assumption that each sample contributes equally to model\ntraining. However, previous work [31]\u2013[33] demonstrates that\nthe importance of different samples to model training can\nvary significantly. This variability can be measured using the\nmodel\u2019s loss for each sample.\nInspired by this insight, we propose a loss-based knowl-\nedge sampling technique. Our method prioritizes samples that\nresult in higher losses, asserting that these are more crucial\nfor effective training. The selection process is formalized as\nfollows:\nmax\n\uf8eb\n\uf8ed\nBk\nX\nj=1\nL(Xij)\n\uf8f6\n\uf8f8, Xij \u2208Si, Si \u2286{Si}Ce\ni=1\n(7)\nHere, Ce indicates the total number of data-excessive classes,\nSi is the set of samples for the i-th data-excessive class, and\nBk represents the number of balance point for the k-th client.\nL(\u00b7) defines the loss function, while Xij is the j-th sample\nfrom i-th data-excessive class. The formula characterizes that\nall data-excessive classes in the k-th client have to pick the\ntop Bk samples by loss function L(\u00b7).\nKnowledge Replay. Previous research [31], [34] has demon-\nstrated that the importance of the same sample varies at dif-\nferent stages of the model\u2019s training. Inspired by this insight,\nand to mitigate the loss of knowledge diversity caused by\ndownsampling, we employ a strategy of periodic knowledge\nreplay every m rounds. Meanwhile, continuous learning re-\nsearch [35], [36] indicates that when a model is trained on a\nnew dataset, it tends to catastrophically forget the old data. To\naddress the forget issue, we also propose a knowledge replay\nstrategy. We define the precondition operation as follows:\ng(Rt\ni, q) := {x1, x2, . . . , xq}, xq \u2208Rt\ni\n(8)\nThe operation g(Rt\ni, q) defines a set of data after sampling\nq samples from Rt\ni, then,we formulate the knowledge replay\nstrategy:\nRt+1\ni\n= g(Rt\ni, \u03b3Bk) \u222ag((Si \\ Rt\ni), (1 \u2212\u03b3)Bk)\n(9)\nHere, Rt\ni represents the sampled dataset in t-th cycle the for\nthe i-th data-excessive class. \u03b3 is a parameter ranging from\n0 to 1 that indicates the proportion of samples retained from\nthe previous cycle\u2019s dataset and Si denotes the set of samples\nfor the i-th data-excessive class. Si \\ Rt\ni denotes the set of\nnew samples are drawn from the remaining dataset which the\nset of elements after Si removes the elements from Rt\ni. The\nknowledge replay strategy judiciously combines a portion of\npreviously selected samples with new samples.\nD. Extended and Unified Framework\nIn considering real-world or more complex scenarios where\nthe duration of available computing resources on the client\nis unevenly distributed, with some clients having no such\nlimitations on the duration of available computing resources,\nwe extend and propose a computation-aware client self-\ndecision framework. As depicted in Figure 3, under this\nframework, clients autonomously decide on knowledge fill-\ning strategies based on their computational resources, distin-\nguishing between computation-constrained and computation-\nunconstrained scenarios. In computation-constrained situa-\ntions, as shown in Eq. 3, the distributional balance point\nis the mean of samples across all classes. Conversely, in\ncomputation-unconstrained situations, the balance point is the\nmaximum value among samples of all classes, which can be\ndefined as follows:\nBk = max\nc\u2208C (nc\nk)\n(10)\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n5\nAlgorithm 1 FBL Algorithm\nInput: global rounds T, clients number K, sample number\nnc\nk in class c, local epoch E, learning rate \u03b7\nOutput: Trained global model w\n1: initialize and distribute w0 for client k, k\u2208[1, K]\n2: initialize Bk =\nj\n1\nC\nPC\nc=1 nc\nk\nk\n3: obtain data-excessive class number Ce, special class num-\nber Cs based on Bk and nc\nk\n4: for t = 1 to T do\n5:\nfor k = 1 to K in parallel do\n6:\nfor i = 1 to Ce do\n7:\nExecute knowledge sampling to make number of\nsample Si down to Bk\n8:\nend for\n9:\nfor j = 1 to Cs do\n10:\nExecute knowledge filling to make number of\nsample S\n\u2032\nj up to Bk\n11:\nend for\n12:\nfor local epoch 1 to E do\n13:\nwk = wk \u2212\u03b7\u2207wkfn(wk)\n14:\nend for\n15:\nend for\n16:\nAggregate local models: wt+1 = 1\nK\nPK\nk=1 wk\n17: end for\n18: return global model wt\nwhere nc\nk denotes the number of samples in the c-th class, and\nC denotes the total number of classes.\nWe define \u03bb as the proportion of computation-unconstrained\nclients, where \u03bb ranges between 0 and 1. To validate the\nefficacy of our framework, we conducted experiments under\nvarying \u03bb values. Details of experiments are shown later in\nSection V-C.\nE. Unified Perspective and Generalization of FBL\nWe propose that FBL follows the \u201dSampling-Generation-\nAlignment\u201d framework. Specifically, we use a loss-based\nmethod to perform knowledge sampling for data-excessive\nclasses. For data-missing and data-scarce classes, we employ\nan edge-side generation model to synthesize knowledge. Sub-\nsequently, the Knowledge Alignment Strategy and Knowledge\nDrop Strategy are applied to align the generated knowledge\nwith real-world knowledge. Our methodology is evaluated\nwithin the popular federated learning paradigm using image\nclassification tasks. However, the FBL approach based on the\n\u201dSampling-Generation-Alignment\u201d framework is not limited\nto this domain. For example, in natural language processing\n(NLP) tasks, we can leverage large language models (LLMs) to\ngenerate knowledge, while also integrating other components\nof our framework to enhance task performance.\nV. EXPERIMENTS\nA. Experiment Setting\nDataset. To assess the performance of our proposed federated\nlearning methodology, we conducted experiments on three\nServer\nClient 1\nClient 2\nClient K\nclass\nnumber\nclass\n...\nclass\nnumber\nnumber\nDistributional Balance Point\nComputation-constrained\nComputation-unconstrained\nData-excessive Classes\nData-scarce Classes\nData-missing Classes\nFig. 3. Extended and Unified Framework. Clients can self-decide to use either\ncomputation-constrained or computation-unconstrained resources based on the\nduration of available computing resources.\ndistinct datasets: CIFAR-10 [37], CIFAR-100 [37], and Tiny\nImageNet [38]. To model the heterogeneous data distribu-\ntion typical of real-world scenarios across multiple clients,\nwe utilized the Dirichlet distribution technique [39]. The\nheterogeneity level was controlled by setting the Dirichlet\nparameter \u03b1 to {0.1, 0.3, 0.5}. Lower \u03b1 values simulate\nincreased distributional heterogeneity among clients, providing\na stringent test under challenging conditions.\nModel Architecture. In our experiments, we utilize ResNet-\n18 as the backbone for all datasets. ResNet-18 consists of 18\nlayers, including convolutional layers, batch normalization lay-\ners, ReLU activation functions, and pooling layers. It features\nresidual blocks that incorporate skip connections, allowing the\nnetwork to maintain performance even with increased depth.\nBaselines. Beside of FedAvg [1], we also compare against\nvarious types of non-iid federated learning methods with the\nproposed approach in our experiments, such as FedProx [3],\nScaffold [4], MOON [5], and FedDyn [40]. In addition,\nthe knowledge-distillation-based method, FedGen [41] and\nFedNTD [42] are also as our baselines. As a method of cross-\nrounds aggregation, FedCDA [43] is our baseline too.\nImplementation Details. All experiments were conducted on\nRTX 3090 GPUs. We utilized the 4-bit quantized version of\nStable Diffusion 2-1, which can run on mobile devices. We\nperformed the experiments using 20 clients over 200 rounds.\nIn each round, 50% of the clients were randomly selected for\n10 iterations of local training. We configured the parameter m\nto 50, set \u03b3 to 0.1, used a batch size of 64, and set \u03b6B = 2.\nThe optimization utilized the SGD optimizer with a learning\nrate of 1e-3, momentum of 1e-4, and weight decay of 1e-5.\nB. Performance Evaluation\nWe demonstrate the mean accuracy of each baseline and the\nproposed method FBL under various heterogeneity levels in\nTable I.\nTable I shows the performance of our method and baselines.\nWe can find that our approach has achieved optimal results\nin almost all non-iid settings. Meanwhile, if we release the\nconstrained computation, we find that when the proportion of\nresource-unconstrained clients \u03bb is 50% or 25% , Ours(50%)\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n6\nTABLE I\nMEAN ACCURACY IN THE SCENARIO OVER THREE DATASETS ON VARIOUS HETEROGENEITY LEVELS. BOLD FONTS HIGHLIGHT THE BEST ACCURACY.\nUNDERLINE HIGHLIGHTS THE SECOND BEST ACCURACY. OURS(25%) REFERS TO OUR METHOD EXTENDED TO SCENARIOS WHERE 25% OF THE CLIENTS\nARE COMPUTATION-UNCONSTRAINED, WHILE OURS(50%) REFERS TO OUR METHOD EXTENDED TO SCENARIOS WHERE 50% OF THE CLIENTS ARE\nCOMPUTATION-UNCONSTRAINED.\nCIFAR-10\nCIFAR-100\nTiny-ImageNet\n\u03b1\n0.1\n0.3\n0.5\n0.1\n0.3\n0.5\n0.1\n0.3\n0.5\nFedAvg [1]\n56.60(1.56)\n65.96(0.85)\n69.25(0.52)\n33.89(0.86)\n37.68(0.67)\n39.86(0.75)\n28.04(0.35)\n31.37(0.13)\n32.69(0.23)\nFedProx [3]\n52.46(2.08)\n71.88(2.87)\n78.67(1.89)\n30.89(2.16)\n40.68(1.28)\n48.88(1.89)\n22.88(1.19)\n30.78(0.23)\n34.72(1.06)\nScaffold [4]\n59.81(2.28)\n77.70(2.51)\n79.57(2.02)\n35.88(0.72)\n45.79(0.28)\n49.36(0.76)\n28.37(0.19)\n34.02(0.89)\n36.88(1.02)\nMOON [5]\n51.38(5.21)\n74.87(2.96)\n79.42(0.57)\n36.18(0.42)\n45.07(0.44)\n50.57(0.54)\n26.78(0.86)\n32.78(1.22)\n36.44(1.32)\nFedGen [41]\n42.14(1.21)\n50.11(1.32)\n59.17(0.88)\n27.66(1.16)\n33.34(1.34)\n37.78(1.04)\n25.66(0.19)\n28.64(0.46)\n31.44(1.25)\nFedDyn [40]\n56.75(0.10)\n78.85(1.35)\n82.95(0.17)\n34.97(0.84)\n46.10(0.13)\n50.89(0.34)\n27.98(0.39)\n34.45(0.56)\n37.78(0.98)\nFedNTD [42]\n54.75(1.12)\n78.66(0.98)\n80.48(0.48)\n34.48(0.88)\n45.77(1.06)\n51.16(1.38)\n28.22(0.16)\n34.08(0.66)\n37.58(0.44)\nFedCDA [43]\n59.98(0.22)\n76.78(0.28)\n78.88(0.14)\n32.48(0.16)\n44.76(0.24)\n50.45(0.22)\n26.44(0.13)\n33.11(0.14)\n35.88(0.17)\nOurs\n76.06(0.24)\n81.62(0.30)\n84.58(0.08)\n36.52(0.30)\n48.65(0.20)\n52.29(0.16)\n25.61(0.20)\n35.60(0.16)\n40.32(0.22)\nOurs(25%)\n79.21(0.15)\n83.88(0.14)\n84.95(0.28)\n48.19(0.37)\n53.52(0.29)\n57.61(0.45)\n34.68(0.38)\n40.18(0.26)\n46.58(0.34)\nOurs(50%)\n79.88(0.19)\n83.28(0.21)\n85.89(0.26)\n48.47(0.26)\n53.50(0.42)\n57.36(0.56)\n35.72(0.29)\n40.39(0.20)\n46.34(0.55)\nand Ours(25%) cover best and second best results in all set-\ntings of three datasets. In particular, on the setting of CIFAR-\n10(Dir 0.1), the performance of our method(Ours(50%)) out-\nperforms the baseline method of best performance by nearly\n33%. This may be that CIFAR-10 dataset has a relatively small\nnumber of classes and the knowledge alignment embedding is\neasier to converge under 200 global rounds. At the same time,\nthe performance of FedGen method is significantly low under\nother baselines. It may be that FedGen\u2019s model is designed\nbased on the mnist dataset, and lacks scalability to the rela-\ntively large datasets such as CIFAR-10, CIFAR-100 and Tiny-\nImageNet, so the performance of FedGen is poor. Additionally,\nwe can see that our extended and unified computation-aware\nframework, with the increase of computation-unconstrained\nclients ratio \u03bb, model performance continues to improve. We\nwill discuss the upper limits of this improvement in our\nextended and unified computation-aware framework in section\nV-C. We also experiment in various pathological heterogeneity\nsettings of CIFAR-10 and CIFAR-100. Table II shows the\nresult.\nThe above experiments show that our method can improve\nthe performance of the global model in the non-iid setting\nunder constrained computation through knowledge filling and\nknowledge sampling. Meanwhile, to deal with more com-\nplex real-world situations, we also propose an extended and\nunified computation-aware framework, and the performance\nof our method can be further improved with an increasing\ncomputation-unconstrained clients ratio \u03bb.\nC. Extended and Unified Framework\nIn this section, we analyze the impact of the proportion of\ncomputation-unconstrained clients (\u03bb) on the performance of\nour extended and unified computation-aware framework. As\nshown in Table II, increasing \u03bb generally improves model\naccuracy across pathological heterogeneity, scalability, and\nrobustness settings. However, the improvement exhibits dimin-\nishing returns, with performance approaching an upper bound\nwhen \u03bb reaches 25%\u201340%.\nD. Scalability and Robustness\nTo show the scalability of our method, we conduct two\nexperiments with 50 and 100 clients on the the setting of\nCIFAR-10 DIR(0.5). Table II shows that in 50 and 100 clients\nsetting, our method still performs best, which indicates that\nour method has scalability. On the other hand, we simulate\nthe noise and absence that may occur in the real image by\ncontrolling the proportion of gaussian noise added to the real\nimage of Tiny-ImageNet. We find that most algorithms show\nmodel degradation under noise setting, which may be caused\nby the coupling of data enhancement and noise information\nduring model training. At the same time, we find that there\nis almost no model degradation in our method, indicating that\nour method is more robust.\nE. Ablation Study\nIn this section, we delve into the role of the different\ncomponents in our approach. Table\nIII shows the results of\nour approach on the CIFAR-10 and CIFAR-100 datasets with\ndifferent components. M1, M2, M3, M4, M5, M6 and M7\nrepresent our approach FBL when different components are\nadopted. M7 is adopted by our method.\n1) Analysis of Knowledge sampling.: Due to the con-\nstrained computation premise, i.e., the number of client sam-\nples remains fixed, our method must sample on the data-\nexcessive classes. Therefore, we compare the performance of\nour method on CIFAR-10 and CIFAR-100 using random sam-\npling and knowledge sampling methods respectively. Table III\nindicates that the knowledge sampling based on loss method\nis better than the random sampling method on CIFAR-10 and\nCIFAR-100. This shows that knowledge sampling based on\nloss is meaningful. The comparison between M1 and M2\nconfirms our conclusion.\n2) Analysis of knowledge Filling.: In fact, we believe that\nthere is a domain gap between the synthetic data and the\nreal data, so the knowledge alignment strategy of embedding\nis used to bridge this gap. Subsequently, we introduced the\nknowledge drop strategy to regularize the learning of embed-\ndings. As shown in Table III, M3 and M2 indicate that without\nIEEE TRANSACTIONS ON MULTIMEDIA, VOL. XX, NO. XX, MARCH 2026\n7\nTABLE II\nMEAN ACCURACY (%) UNDER PATHOLOGICAL HETEROGENEITY (SHARDS), SCALABILITY (NUMBER OF CLIENTS), AND ROBUSTNESS (NOISE\nINJECTION) SETTINGS ACROSS CIFAR-10, CIFAR-100, AND TINY-IMAGENET. BOLD DENOTES THE BEST RESULT. VALUES ARE REPORTED AS MEAN\n(STANDARD DEVIATION).\nPathological Heterogeneity\nScalability\nRobustness\nCIFAR-10\nCIFAR-100\nCIFAR-10\nTiny-ImageNet\nMethod\nShards (4)\nShards (8)\nShards (6)\nShards (12)\n50 clients\n100 clients\nNoise (0.01)\nNoise (0.05)\nNoise (0.1)\nFedAvg [1]\n63.77(1.24)\n78.46(0.91)\n14.91(0.53)\n31.74(0.78)\n68.92(1.04)\n60.63(3.16)\n31.58(0.67)\n29.12(0.79)\n27.21(0.86)\nFedProx [3]\n64.10(2.67)\n77.98(0.53)\n13.64(0.48)\n30.48(0.80)\n68.63(1.19)\n60.53(3.17)\n32.46(0.87)\n30.93(0.59)\n29.10(0.40)\nScaffold [4]\n76.75(0.64)\n79.86(0.79)\n12.68(0.66)\n26.94(0.13)\n72.39(0.91)\n62.30(1.04)\n34.18(1.51)\n32.88(1.69)\n31.14(0.98)\nMOON [5]\n78.98(0.47)\n80.42(0.73)\n18.46(0.34)\n37.46(0.96)\n69.24(1.07)\n60.53(3.17)\n33.78(2.51)\n31.39(1.86)\n30.55(1.78)\nFedGen [41]\n67.14(1.03)\n76.58(1.24)\n11.39(0.87)\n29.98(0.98)\n66.84(2.08)\n60.78(1.35)\n30.48(1.86)\n29.17(1.15)\n26.92(1.31)\nFedDyn [40]\n73.29(1.68)\n77.38(1.45)\n14.29(0.63)\n34.98(1.06)\n63.16(1.37)\n63.78(1.43)\n35.48(1.19)\n32.17(1.21)\n29.94(1.07)\nOurs\n80.74(0.66)\n82.62(0.30)\n20.47(0.46)\n38.54(0.26)\n75.27(0.56)\n66.28(0.34)\n39.95(0.33)\n39.67(0.46)\n38.89(0.26)\nTABLE III\nABLATION STUDY.\nMethod\nKnowledge Sampling\nKnowledge Filling\nKnowledge Replay\nDataset\nRandom\nKnowledge\nAlignment\nDrop\n0%\n10%\n70%\n90%\nCIFAR-10\nCIFAR-100\nM1\n\u2713\n\u2713\n75.16(0.29)\n34.13(0.17)\nM2\n\u2713\n\u2713\n76.84(0.38)\n36.26(0.30)\nM3\n\u2713\n\u2713\n\u2713\n74.51(0.34)\n34.13(0.31)\nM4\n\u2713\n\u2713\n\u2713\n\u2713\n80.84(0.26)\n47.82(0.24)\nM5\n\u2713\n\u2713\n\u2713\n\u2713\n79.84(0.52)\n46.89(0.33)\nM6\n\u2713\n\u2713\n\u2713\n\u2713\n80.14(0."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions:\nThe paper proposes Federated Balanced Learning (FBL), a novel approach to mitigate the issue of Non-Independent and Identically Distributed (non-IID) data in federated learning settings, where clients collaborate by sharing model parameters instead of raw data. The key contributions are:\n\n1. **Reimagining Client Role**: FBL rethinks the role of client-side samples, proposing that each client should balance its local dataset through edge-side generative models to prevent drift and improve overall performance.\n\n2. **Knowledge Alignment Strategy**: This strategy bridges the gap between synthetic (generated) data and real-world data by aligning their representations, thereby improving model generalization across diverse multimedia scenarios.\n\n3. **Knowledge Drop Strategy**: This regularization technique is inspired by Dropout [28], designed to prevent overfitting during knowledge alignment, ensuring robustness of the FBL method.\n\n4. **Extended and Unified Framework**: The authors extend their framework to handle more complex real-world scenarios where client computational resources are unevenly distributed, allowing computation-constrained and unconstrained clients to adopt different strategies as needed.\n\n5. **Experiments and Validation**: Extensive experiments validate that FBL outperforms state-of-the-art baselines in various non-IID settings, demonstrating its effectiveness across multiple datasets like CIFAR-10, CIFAR-100, and Tiny ImageNet.\n\n#### Methodology:\n\n**Class Classification**: Each client divides all classes into three categories: data-excessive, data-scarce, and data-missing. Knowledge sampling is applied to the data-excessive classes while knowledge filling is used for the other two categories.\n\n**Knowledge Filling**: Synthetic data from edge-side generation models are used to fill in missing samples (data-missing) or augment existing ones (data-scarce). For data-excessive classes, a Knowledge Alignment Strategy bridges the domain gap between synthetic and real data. A Knowledge Drop Strategy is introduced as a regularization technique.\n\n**Knowledge Sampling**: In scenarios with excessive data, sampling methods are used to downsample the number of samples to a distributional balance point while ensuring model performance remains consistent across pre-sampled and post-sampled datasets.\n\n#### Key Findings:\n\n- **FBL Outperforms Baselines**: FBL consistently achieves higher accuracy compared to existing federated learning approaches in non-IID settings, especially under constrained computation conditions.\n  \n- **Balanced Data Improves Performance**: By balancing the local data across clients, FBL mitigates drift and ensures that the global model generalizes well across diverse multimedia scenarios.\n\n#### Conclusions:\n\nFederated Balanced Learning (FBL) is a robust framework for federated learning in non-IID settings. It leverages edge-side generative models to balance client datasets effectively, addressing both sample imbalance and domain gap issues. The proposed strategies\u2014Knowledge Alignment and Knowledge Drop\u2014are shown to be effective in improving model performance across diverse real-world scenarios.\n\nThe paper's contributions not only advance the state-of-the-art in federated learning but also provide a scalable and robust framework for handling complex data distribution challenges."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "1\nSHARE: A Fully Unsupervised Framework for Single Hyperspectral\nImage Restoration\nJiangwei Xie\u2217, Zhang Wen\u2217, Mike Davies, Dongdong ChenB\nAbstract\u2014Hyperspectral image (HSI) restoration is a funda-\nmental challenge in computational imaging and computer vision.\nIt involves ill-posed inverse problems, such as inpainting and\nsuper-resolution. Although deep learning methods have trans-\nformed the field through data-driven learning, their effectiveness\nhinges on access to meticulously curated ground-truth datasets.\nThis fundamentally restricts their applicability in real-world\nscenarios where such data is unavailable. This paper presents\nSHARE (Single Hyperspectral Image Restoration with Equiv-\nariance), a fully unsupervised framework that unifies geometric\nequivariance principles with low-rank spectral modelling to\neliminate the need for ground truth. SHARE\u2019s core concept\nis to exploit the intrinsic invariance of hyperspectral structures\nunder differentiable geometric transformations (e.g. rotations and\nscaling) to derive self-supervision signals through equivariance\nconsistency constraints. Our novel Dynamic Adaptive Spec-\ntral Attention (DASA) module further enhances this paradigm\nshift by explicitly encoding the global low-rank property of\nHSI and adaptively refining local spectral-spatial correlations\nthrough learnable attention mechanisms. Extensive experiments\non HSI inpainting and super-resolution tasks demonstrate the\neffectiveness of SHARE. Our method outperforms many state-\nof-the-art unsupervised approaches and achieves performance\ncomparable to that of supervised methods. We hope that our\napproach will shed new light on HSI restoration and broader\nscientific imaging scenarios. The code will be released at https:\n//github.com/xuwayyy/SHARE.\nIndex Terms\u2014Hyperspectral image, Self-supervised learning,\nEquivariant imaging, Inverse problems, Image restoration\nI. INTRODUCTION\nH\nYPERSPECTRAL image (HSI) restoration aims to re-\ncover a clear HSI from a potentially noisy and degraded\nHSI that follows a known linear degradation model. Since\nreal-world hyperspectral imaging always suffers from missing\npixels or low resolution [1], it plays a crucial role in practice,\nenabling a wide range of applications in various fields, such\nas agriculture [2], mineral exploration [3], and environmental\nmonitoring [4].\nFrom a variational perspective, solving HSI restorations\ninvolves minimising the negative log-likelihood:\nmin\nx {\u2212log p(x|y)}\n(1a)\n\u221dmin\nx {\u2212log p(y|x) \u2212log p(x)}\n(1b)\n\u221dmin\nx {L(H(x), y) + R(x)}\n(1c)\nwhere LH(x) is the data fidelity term and R(x) is the regu-\nlarization term such as smoothness, symmetry and sparsity.\n\u2217: Equal Contribution, B: corresponding author (d.chen@hw.ac.uk). Jiang-\nwei Xie, Zhang Wen, Dongdong Chen are with the School of Mathematical\nand Computer Sciences, Heriot-Watt University, Edinburgh, UK. Mike Davies\nis with the School of Engineering, University of Edinburgh, UK.\nConventional HSI restoration methods primarily focus on\nexploiting abundant spectral information, such as sparsity-\nbased methods [5] and total variation (TV)-based methods [6].\nHowever, these methods often suffer from high computational\ncomplexity and difficulty in recovering fine-grained informa-\ntion, especially in challenging scenarios. In contrast to the\nconstruction of hand-crafted priors, learning-based methods\n[7] [8] [9] [10] [11] [12] directly build a mapping from\nobservation {y} to ground truth {x} through a data-driven\nstrategy. Among them, supervised methods mainly use a CNN,\na Vision Transformer (VIT) [13] or a Diffusion [14] network\nas a backbone and learn from paired {(x, y)} data sets.\nAlthough these methods have achieved promising results in\npopular restoration tasks such as denoising, super-resolution,\nand inpainting, the construction of large HSI datasets remains\nimpractical due to the high cost of imaging systems.\nMoreover, since HSI datasets are rare, these approaches\noften suffer from poor domain generalisation. Recently, self-\nsupervised methods [15] [16] [17] [18] [19] have gained\npopularity due to their ability to work without paired data\nsets. However, most of these methods still rely on auxiliary\nsupervised learning to provide pre-trained restoration models,\nindicating the use of ground truth during training. When there\nis a significant domain shift between the source and target\ndata, their performance still degrades dramatically. Although\nmethods such as [20], [15] and [16] can operate in a fully\nunsupervised manner, they often suffer from a significant\nperformance gap compared to supervised approaches. Further-\nmore, the robustness of these methods is further challenged by\nthe fact that images captured by real-world sensors are often\nnoisy and incomplete.\nTo address these issues, we propose a novel fully un-\nsupervised HSI restoration framework: Single Hyperspectral\nImAge Restoration with Equivariance (SHARE). SHARE is\nbased on the fact that image signals remain invariant under\na group of transformations. Unlike previous self-supervised\napproaches, SHARE imposes mild invariance priors, allowing\nlearning from single image at test time. Moreover, our ap-\nproach is robust to noisy observations by incorporating the\nStein Unbiased Risk Estimator (SURE) [21], which adheres\nto real-world hyperspectral imaging conditions. Furthermore,\nwe design a U-shaped architectural network in f\u03b8 to further\nexploit informative features.\nOverall, our contribution can be summarized as follows:\n\u2022 We propose a novel fully unsupervised HSI restoration\nframework SHARE, which exploits the invariance prop-\nerty of images and learning at test time on the single\nimage.\narXiv:2601.13987v1  [eess.IV]  20 Jan 2026\n2\n\u2022 We incorporate the SURE to SHARE, enhancing its\nrobustness under real-world sensor imaging conditions\nand enabling a robust self-supervised HSI restoration\nmethod.\n\u2022 Extensive experiments on HSI inpainting and super-\nresolution show that SHARE achieves state-of-the-art per-\nformance among unsupervised methods and comparable\nperformance to supervised methods.\nThe rest of this paper is organized as follows. Section II\nreviews the recent works related to HSI restoration and equiv-\nariant imaging. Section III presents our proposed method for\nHSI restoration. Section IV presents the extensive experimen-\ntal and ablation studies. We conclude our paper in section V.\nII. RELATED WORK\nA. HSI Restoration\nPrior-based methods always use hand-crafted priors and it-\neratively reconstruct a clean HSI image. They include sparsity\n[22] [23], low-rank approximation [24] are most commonly\nused. TV regularization [6] is also always used for compressed\nsensing [25] and denoising [26]. Although these methods\nalways exploit both spatial and spectral information, they\nalways suffer from difficult optimization and poor performance\nin complex scenarios.\nLearning-based methods [7] [8] [9] [10], on the other\nhand, commonly tend to adopt a CNN or Vision Transformer\nbackbone. Chang et al. [7] proposed to use CNN to restore\nHSI. Wei et al. [27] proposed a 3D quasi-recurrent network\n(QRNN3D) for HSI denoising and inpainting. Cai et al. [28]\nbuild a coarse-to-fine transformer architecture to restore HSI.\nLi et al. [11] incorporates Diffusion and VIT to further\nimprove performance. However, these methods need to be\ntrained on large datasets, which are hard to collect in the real\nworld. Although data augmentation alleviates this problem,\nthese methods always struggle with poor performance in high-\nresolution and noisy conditions.\nRecently, self-supervised learning has gained significant at-\ntention. However, in the HSI domain, self-supervised methods\nstill show a significant performance gap compared to their\nsupervised counterparts. Several studies [17] [29] [18] [19]\nhave attempted to bridge this gap by pre-training a restoration\nmodel in a supervised manner to gain generalization ability,\nfollowed by fine-tuning the model in an unsupervised setting.\nWhile these methods show promising results, they still rely on\nground truth data to support the learning process. By contrast,\nmethods such as [15] and [16], which are the closest to our\nproposed method in principle, use DIP [20] to iteratively\nreconstruct clean images from observations alone. However,\nthey often rely on heuristic architectural design and lack well-\ndefined design principles.\nB. Equivariant Imaging\nEquivariant Imaging (EI) [30] [31] [32] is the state-of-the-\nart fully unsupervised framework for solving inverse imaging\nproblems and has been successfully applied to a variety of\ncomputational imaging and low-level vision tasks such as\nFig. 1. An illustration of learning with equivariance in the HSI inpainting task.\nThe reconstructed image after applying a transformation should be equivalent\nto the restoration of the initially transformed and masked image.\nCryo-EM image reconstruction [33], image fusion [34], super-\nresolution [35], and pansharpening [36]. EI uses the group\ninvariance properties inherent in natural signals to learn a\nreconstruction function using only partial measurement data.\nIt assumes that the original signal set has symmetry properties\nand remains invariant under a group of transformations -\nsuch as rotation, translation and scaling. By embedding such\ninvariance priors, EI ensures that the entire image processing\npipeline (from acquisition to reconstruction) exhibits transfor-\nmation equivariance.\nAs demonstrated in [30] [37], it is possible to learn to\nimage from measurements alone using image signals\u2019 intrinsic\nsymmetry and low dimensional structures. To answer these key\nfactors, we carefully select Shift and Scale group actions and\nexplicitly incorporate a DASA module with low dimension,\nthereby enforcing the HSI recovery from only measurement\nHSI.\nWe note that the recent works also allows learning with\nequivariance [38] [36]. The difference between them and\nthe proposed one twofold: (1) the related work either lack\nrobustness to noise [38] or focus on a single task [36], whereas\nthis paper aims to provide a more generalized framweork\nthat can simultaneously address both inpainting and super-\nresoluton. (2) we proposed a DASA module which leverages\nthe low-dimensional (low-rank) property of HSI signals, which\nsignificantly improves EI for HSI image restoration.\nIII. METHOD\nA. Problem Statement\nWe formulate the single-image HSI restoration problem for\ninpainting and super-resolution tasks. Given only one noisy\nand incomplete observation y \u2208Rc\u00d7h\u00d7w generated by a\nknown linear degradation model H acting on the ground truth\nx \u2208RC\u00d7H\u00d7W with additive noise \u03f5, where C is the bands\nnumber, H and W represent the spatial size, the challenge is\nto recover x from only y. Since the training set contains only\none image, SHARE performs several iterations and directly\noutputs the clean HSI in a zero-shot manner, which is adhere\nto the fact that HSI dataset always contain only one image.\n\u2022 HSI Inpainting: In real-world satellite imaging, missing\nstripes may occur in certain areas due to the platform\u2019s\ninability to consistently maintain its programmed atti-\ntude, limited access to accurate digital elevation maps\n(DEMs), or misalignment between the flight path and the\n3\nFig. 2. The overall pipeline of SHARE. Given only one noisy observation y, it is first passed through the inverse mapping f\u03b8 to obtain a restored estimate\nf\u03b8(y). This estimate is then forwarded through the physics model H to compute H(f\u03b8(y)), which is compared with the observation y to calculate the SURE\nloss. Simultaneously, f\u03b8(y) undergoes a transformation Tg, followed by the physics model, noise corruption, and inverse mapping, to compute the robust\nequivariance consistency loss.\nhyperspectral sensors [38]. For the inpainting task, H is\na binary diagonal matrix and y is obtained through:\ny = H \u2299x + \u03f5\n(1)\nwhere \u2299represents the Hadamard product.\n\u2022 HSI Super-resolution: Due to limited solar irradiance,\nthere is an inherent trade-off between spatial and spectral\nresolution in HSI; prioritizing spectral resolution often\nresults in reduced spatial resolution [39]. For the super-\nresolution task, H is modeled by a Gaussian blur kernel\n\u03a6 and downsampling factor s:\ny = (x \u2217\u03a6) \u2193s +\u03f5\n(2)\nwhere \u2217is convolution operator, \u2193s means downsampling.\nClearly, this single observation constraint is more in line\nwith real HSI restoration scenarios where the acquisition is\nalways only one noisy and corrupted image, but it renders su-\npervised learning approaches inapplicable. In the next section\nwe show how our SHARE solves these problems.\nB. Single HSI Restoration with Equivariance\nIn supervised learning, the goal is to minimize the error\nbetween the predicted clean image and the corresponding\nground truth. This typically involves optimizing a loss func-\ntion, such as the mean squared error between the reconstructed\nimage f(y) and the ground truth x. However, obtaining a\nlarge amount of ground truth data is often challenging in\nreal-world hyperspectral imaging. Therefore, in unsupervised\nlearning (ground truth-free) scenarios, the dataset consists only\nof degraded images, making the availability of ground truth x\nnon-existent. Consequently, the training loss must be redefined\nto eliminate the dependence on ground truth data.\nLearning Without Robustness\n1) Measurement Consistency: Let f\u03b8 : y \u2192x be the\nreconstruction function. A straightforward approach in unsu-\npervised learning is to enforce measurement consistency (MC)\nby minimizing the discrepancy between observation y and the\ndegradation of the reconstructed image Hf\u03b8(y). This can be\nexpressed as:\nLmc = Ey\n\u0002\n\u2225H(f\u03b8(y)) \u2212y\u22252\n2\n\u0003\n(3)\nHowever, the estimation of f\u03b8 cannot be achieved by\nminimizing the error between H(f\u03b8(y)) and y for two reasons:\nFirst, we don\u2019t have the null space information of H w.r.t. the\nground truth, i.e., the clean components in the ground truth\nimage, but only have access to the information in its range\nspace, i.e., the degraded components in y, and we need to learn\nmore information beyond these range space components [30]\n[40]. Furthermore, such a naive estimation is not robust. In\nreal-world sensor imaging conditions, the captured images are\nalways noisy, which further limits the restoration. To address\nthese challenges, we follow [30] and utilize an equivariance\nmodule to learn clean components so that the ground truth {x}\ncan be recovered from its raw observation {y}, and propose\nto use SURE instead of vanilla MC.\n2) Equivariance Consistency: We start with the observation\nthat image signals often remain invariant under a group of\ntransformations. This concept can be intuitively illustrated by\nthe example of satellite imaging, where the camera continu-\nously moves and captures the same target area from different\nviewpoints. Despite these variations in viewpoint, the semantic\ncontent of the image remains unchanged. Such scenarios\nnaturally lead to transformation invariance, including rotation,\ntranslation, scaling, etc.\nAs pioneered in [30], system equivariance could enable\nthe unknown signal set to be recovered from its compressed\nmeasurements by interacting with the group action Tg and\nthe physics H. In particular, the restoration function f\u03b8 can be\nlearned from only measuremnets if the intersection of subman-\nifolds \u2229gH\u22121\ng y contains a single signal x, where Hg = H\u25e6Tg.\nIn principle, if the image set {x} is invariant with respect to\na transformation group {Tg}, then the whole degrade-restore\nsystem f\u03b8 \u25e6H should be equivariant with respect to these\ntransformations, i.e.\nf\u03b8\n\u0000H(Tgx)\n\u0001\n= Tg\n\u0000f\u03b8(H(x))\n\u0001\n.\n(4)\nSince the ground truth x is not available, inspired by\n[30], we adopt the following ground-truth-free equivariance\nconstraint:\nf\u03b8(H\n\u0000Tgf\u03b8(y)\n\u0001\n) = Tgf\u03b8(y).\n(5)\nwhere the right-hand side follows from the fact H(x) = y\nunder zero noise condition. Note that on the left-hand side,\nthe first f\u03b8 takes as input the transformed prediction passed\n4\nFig. 3. (a) The overall architecture of the inverse mapping f\u03b8, which adopts a U-shaped design. (b) The structure of the convolutional block. It doubles the\ninput\u2019s channel dimension in the encoding phase and halves it in the decoding phase. (c) The architecture of the decoder block consists of a DASA module,\na convolutional block, and a transposed convolution layer. (d) The internal structure of the proposed DASA module.\nthrough the measurement operator H, whereas the second f\u03b8\noperates directly on the original observation y. Despite this\ndifference in input, the same network f\u03b8 is used on both\nsides. According to [37], Eq. 5 does not require H or f\u03b8 to be\ninvariant under Tg, but the composition f\u03b8\u25e6H to be equivariant\nunder Tg. Fig. 1 demonstrates the idea of Eq. 4 under the HSI\ninpainting task. The reconstructed image after rotation should\nbe equal to the rotated and then reconstructed image.\nLearning With Robustness\nIn most real-world situations, observed measurements are\ncorrupted by noise. For example, measurements typically\ncontain Gaussian noise, i.e. y \u223cN(H(x), I\u03c32). To derive a\nrobust self-supervised HSI restoration method, following [31],\nwe modify Eq. 3 and Eq. 5, respectively. The modified noise\nrobust framework is illustrated in Fig. 2.\n3) Robust Equivariance Constraint: To ensure consistency\nwith the distribution in the testing phase (i.e., noisy images),\nthe degraded images used during training must also contain\nnoise, i.e. robust equivariance consistency [31]:\nf\u03b8(\u02dcy) = Tgf\u03b8(y)\n(6)\nwhere \u02dcy \u223cN(H\n\u0000Tgf\u03b8(y)\n\u0001\n, I\u03c32) is the new noisy measure-\nment derived from the transformed reconstruction Tgf\u03b8(y).\nNote in the absence of noise, Eq. 6 is equivalent to Eq. 5.\n4) SURE Loss: Recently, Chen et al. introduced a noise-\nrobust variant of the original EI by incorporating Stein\u2019s\nUnbiased Risk Estimate (SURE) function [21] to estimate the\nclean measurement consistency [31]. In this paper, we are\ninterested in investigating the potential of SURE in solving\ninverse problems in the hyperspectral domain. In general, the\nSURE loss provides an unbiased estimator of the supervised\nmean square loss between the noisy observation y and its\ncorresponding ground truth x [21]. Under the assumption of\nGaussian noise, the SURE loss is formulated as follows.\nLsure = ||y \u2212Hf\u03b8(y)||2\n2 \u2212\u03c32 + 2\u03c32\u2207f\u03b8(y)\n(7)\nwhere \u03c3 is the standard deviation of the noise. The first\nterm in Eq. 7 penalizes the discrepancy between the noisy\nobservations and the restored results, reflecting the bias of the\nreconstruction neural network, while the last term penalizes\nthe variance, promoting stability in the denoising process.\nSince direct computation of the divergence term is often\nintractable, [41] proposed a Monte Carlo method to estimate\nthe divergence:\n\u2207f\u03b8(y) = lim\n\u03c4\u21920 Eb{b\u22a4\n\u03c4 (H(f\u03b8(y + \u03c4b)) \u2212H(f\u03b8(y)))}\n(8)\n\u2248b\u22a4\n\u03c4 (H(f\u03b8(y + \u03c4b)) \u2212H(f\u03b8(y)))\n(9)\nwhere b is an i.i.d. random vector with zero mean and unit\nvariance. Thus the divergence can be efficiently approximated\nby sampling b \u223cN(0, I) and using a small scalar \u03c4.\nFinally, if the noise distribution is Gaussian, the SURE loss\nhas the following expression, where the divergence term is\nestimated via Monte Carlo approximation.\nLsure = \u2225y \u2212Hf\u03b8(y)\u22252\n2 \u2212\u03c32\n+ 2\u03c32\n\u03c4 b\u22a4\u0000H(f\u03b8(y + \u03c4b)) \u2212H(f\u03b8(y))\n\u0001\n(10)\nNote that in the ablation study we also evaluate our SHARE\nmodel under Poisson and mixed Gaussian-Poisson noise set-\ntings. A detailed derivation of the SURE loss function for all\n5\nnoise conditions can be found in [31]. In the new paragraph\nwe will introduce the architecture of f\u03b8.\nC. Inverse Mapping f\u03b8\n1) Overview of f\u03b8: The architecture of f\u03b8 is illustrated\nin Fig. 3(a). The main network adopts a U-shaped structure.\nGiven a noisy observation y \u2208Rc\u00d7H\u00d7W , it is first passed\nthrough a linear inverse module H\u2020y and a 2D convolution\nto increase the channel dimension from c to C. The resulting\nfeature map is denoted as z0 \u2208RC\u00d7H\u00d7W .\nInspired by previous success of 3D convolution in a var-\nious of HSI tasks [42] [27] [43], we also consider 3D\nconvolution in f\u03b8. After acquiring z0, it is lifted to a 4D\ntensor z1 \u2208RR\u00d7C\u00d7H\u00d7W by a 3D convolution, where R\nrepresents the spectral depth. This tensor enters the encoder\npath. In the i-th encoder stage, the feature ei is obtained\nby a convolution block that doubles the depth dimension:\nei \u2208R2iR\u00d7C/2i\u00d7H/2i\u00d7W/2i. Each convolution block consists\nof 3D convolution, batch normalisation and ReLU activation.\nAfter each block, 3D max pooling is applied to reduce spatial\nand spectral resolution.\nIn the decoder path, a transpose convolution is first ap-\nplied to the feature map to double the spatial and spectral\ndimensions and halve the depth dimension. The output is\nthen concatenated with the corresponding encoder feature ei\nalong the depth axis. This combined tensor is passed through\na decoder block that includes a spectral attention module, a\nconvolution block, and another transpose convolution. Each\ndecoder block reduces the depth dimension by a factor of 4\nwhile doubling the spatial and spectral resolution.\nFinally, the decoder output is passed through a 3D convo-\nlution to reduce the depth to 1, followed by a 2D convolution\nthat restores the channel dimension to the original c, resulting\nin the final restored HSI image \u02c6x \u2208Rc\u00d7H\u00d7W .\n2) Dynamic Adaptive Spectral Attention: Early studies [44]\nhave suggested that the spectral signatures in HSI can often\nbe modeled as a linear combination of a limited number of\nendmembers, reflecting a low-dimensional subspace structure\nin the spectral domain [45]. This insight underpins a variety\nof classical unmixing and reconstruction approaches. Building\non this foundation, recent methods have leveraged the low-\nrank nature of HSI data to enhance restoration tasks, which\ncan be adopted to alleviate the computational complexity\nof 3D convolutions while maintaining strong channel-wise\nfeature extraction capabilities. To this end, inspired by [46],\nwe propose a dynamic adaptive spectral attention (DASA)\nmodule.\nOur method uses a low-rank memory bank with a user-\ndefined rank, which plays a similar role to the reduction ratio\nin [47] and exploits the inherent low-rank structure of HSI.\nThe memory bank can be interpreted as a learnable dictionary\nof global low-rank spectral vectors and dynamically adapts to\ndifferent local features.\nThe structure of our proposed DASA module is shown\nin Fig. 3 (d). Given a 4D input tensor, we first remove the\ndepth dimension and transform it into a 3D tensor. To capture\nspectral-spatial dependencies, the features are partitioned into\nnon-overlapping cube patches of size P \u00d7 P \u00d7 C. Following\n[47], a global average pooling operation is applied within\neach patch to produce a compact spectral descriptor Xc \u2208\nR1\u00d71\u00d7RC. Then, a linear projection layer maps Xc to a low-\nrank representation Xk \u2208R1\u00d71\u00d7K.\nNext, the memory bank computes the similarity between\nXk and the stored low-rank matrix M \u2208RK\u00d7B where B\nis the number of low-rank matrix in memory bank, resulting\nin a weight vector I \u2208R1\u00d71\u00d7B. In implementation, M is\na learnable parameter and is updated via backpropagation.\nThese weights are used to aggregate vectors from the memory\nbank and obtain an enhanced low-rank representation Xl \u2208\nR1\u00d71\u00d7K, as formulated below:\nI = Softmax(XkM)\n(11)\nXl = IM \u22a4.\n(12)\nNotably, instead of projecting the entire image into a global\nrank-K subspace, the proposed memory-based mechanism\nallows each local patch to attend to global low-rank bases.\nThis allows effective global-local interaction and adaptively\nimproves the spectral representation. The resulting vector Xl\npreserves the most informative features of the original HSI and\ndynamically adapts to different inputs through content-aware\nweighting.\nFinally, Xl is projected back to the original channel di-\nmension and reshaped to match the spatial dimensions of the\ninput. A residual connection is then applied to produce the\nfinal output of the DASA module.\nD. Loss Function\nTo guarantee the robust equivariance consistency in Eq. 6,\nwe use Lrec to minimize their error:\nLrec = Ey,g[||Tgf\u03b8(y) \u2212f\u03b8( \u02dcyi)||2\n2]\n(13)\nThe final loss function for SHARE is the combination of\nthe SURE loss and the robust equivariance consistency loss.\nIts ground truth free expression is as follows\nLSHARE = Lsure + \u03b1Lrec\n(14)\nwhere \u03b1 is a trade-off parameter, with these two terms,\nSHARE is robust to noise and learns fully without ground\ntruth.\nIV. EXPERIMENTS\nA. Implementation Details\nWe implement all experiments based on Pytorch and the\nDeepInverse library [48], using a single NVIDIA GeForce\nRTX 3090 GPU. For training details. We use the Adam\noptimiser with an initial learning rate of 1e\u22123 and a cosine\nscheduler to 1e\u22124 within 2000 epochs. The \u03b1 in Eq. 14 is\nset to 1 and the \u03c4 in Eq. 10 is set to 0.01. The rank in the\nmemory bank is K = 4 and B = 256. As suggested by [30]\nand [35], we employ Shift and Scale as transformation groups\nfor inpainting and super-resolution, respectively, given their\ndemonstrated effectiveness for the associated measurement\noperator H. For both inpainting and super-resolution, the\nobservations are corrupted by Gaussian noise with zero mean\nand 25 / 255 standard deviation.\n6\ny\nGround Truth\n(MSSIM\u2191, SAM\u2193)\n(MSSIM\u2191, SAM\u2193)\nMC\nDHP\nPnP-DIP\nDeepHyIn\nR-DLRHyIn\nHEI\n(0.790, 4.35)\n(0.779, 9.10)\n(0.838, 6.22)\n(0.877, 2.92)\n(0.887, 2.81)\n(0.901, 2.58)\n(0.862, 2.64)\n(0.880, 5.62)\n(0.904, 4.54)\n(0.914, 1.95)\n(0.920, 1.89)\n(0.928, 1.78)\nSHARE\n(0.920, 2.29)\n(0.939, 1.60)\nFig. 4. Inpainting results of different methods on Chikusei dataset. MPSNR values are shown on the top left, MSSIM and SAM values are shown below\neach image. The best and second-best values are marked in red and blue , respectively. Band 90 for visualization. Please zoom in for a better view.\ny\n(MSSIM\u2191, SAM\u2193)\nGround Truth\nSHARE\n(0.814, 5.21\u00a0)\nMC\nDHP\nPnP-DIP\nDeepHyIn\nR-DLRHyIn\nHEI\n(0.773, 5.73)\n(0.785, 6.70) (0.800, 6.25) (0.794,\u00a0, 5.50)\n(0.802, 5.44)\n(0.808, 5.37)\nFig. 5. Inpainting results of different methods on Indian-Pines dataset. MPSNR values are shown on the top left, MSSIM and SAM values are shown below\neach image. The best and second-best values are marked in red and blue , respectively. Band 149 for visualization. Please zoom in for a better view.\nTABLE I\nCOMPARISON OF DIFFERENT METHODS ON CHIKUSEI AND INDIAN-PINES DATASETS. WE REPORT THE AVERAGE VALUES OF EACH METRIC ON FOUR\nMASKS AND ALL IMAGES FOR EACH DATASET. THE\nRED\nAND BLUE\nMARKERS INDICATE THE BEST AND SECOND-BEST VALUES AMONG ALL\nMETHODS. / MEANS CAN NOT COMPUTE METRIC VALUE.\nMethod\nChikusei Dataset\nIndian Pines Dataset\nMPSNR \u2191\nMSSIM \u2191\nSAM \u2193\nMPSNR \u2191\nMSSIM \u2191\nSAM \u2193\nH\u2020y\n16.96\n0.163\n/\n13.485\n0.279\n/\nMC\n29.59\n0.795\n4.68\n24.52\n0.716\n6.42\nDHP [15]\n28.93\n0.828\n6.98\n24.39\n0.756\n7.36\nPnP-DIP [16]\n29.97\n0.865\n5.57\n24.94\n0.777\n6.82\nDeepHyIn [49]\n33.52\n0.888\n3.56\n27.64\n0.771\n5.76\nR-DLRHyIn [50]\n33.97\n0.898\n3.07\n27.82\n0.775\n5.70\nHEI [38]\n34.18\n0.901\n2.93\n28.03\n0.789\n5.57\nSHARE\n35.12\n0.917\n2.56\n28.54\n0.801\n5.41\n7\nTABLE II\nCOMPARISON OF DIFFERENT METHODS ON CAVE: FAKE AND REAL BEERS MS DATASET. THE\nRED\nAND BLUE\nMARKERS INDICATE THE BEST AND\nSECOND-BEST VALUES AMONG ALL METHODS.\nMethod\nDownsample Rate \u00d72\nDownsample Rate \u00d74\nDownsample Rate \u00d78\nMPSNR \u2191\nMSSIM \u2191\nSAM \u2193\nMPSNR \u2191\nMSSIM \u2191\nSAM \u2193\nMPSNR\u2191\nMSSIM\u2191\nSAM \u2193\nBicubic\n21.38\n0.180\n18.45\n21.07\n0.265\n18.22\n20.65\n0.450\n18.05\nDHP [15]\n27.53\n0.538\n7.90\n26.13\n0.592\n9.00\n19.88\n0.182\n17.01\nPnP-DIP [16]\n29.31\n0.689\n5.55\n26.69\n0.594\n6.53\n24.15\n0.517\n7.03\nSSDL [18]\n30.45\n0.762\n4.48\n27.83\n0.730\n5.69\n25.63\n0.699\n6.94\nMC\n27.36\n0.547\n8.54\n26.37\n0.429\n10.60\n21.88\n0.227\n16.96\nSupervised [51]\n36.97\n0.961\n2.12\n33.75\n0.927\n2.92\n29.14\n0.884\n4.64\nSHARE\n35.60\n0.938\n2.64\n33.22\n0.916\n2.86\n30.46\n0.893\n3.27\nLow-Resolution\n(MSSIM\u2191, SAM\u2193) (0.180, 18.45)\n(MSSIM\u2191, SAM\u2193)\n(0.538, 7.90)\n(0.689, 5.55)\n(0.547, 8.54)\n(0.762, 4.48)\n(0.961, 2.12)\n(0.265, 18.22)\n(0.592, 9.00)\n(0.594, 6.53)\n(0.429, 10.60)\n(0.730, 5.69)\n(0.927, 2.92)\nSupervised\nSSDL\nMC\nPnP-DIP\nDHP\nBicubic\n(0.938, 2.64)\n(0.916, 2.86)\nSHARE\nHigh-Resolution\nFig. 6. Super resolution results of different methods on Cave: fake and real beers ms dataset. MPSNR values are shown on the top left, MSSIM and SAM\nvalues are shown below each image. The best and second-best values are marked in red and blue , respectively. Bands 5, 15, 25 for visualization. Please\nzoom in for a better view.\nB. Evaluation Metrics\nFor both inpainting and super-resolution tasks, we select\nthree common metrics for HSI restoration: Mean Peak Signal\nto Noise Ratio (MPSNR), Mean Structural Similarity Index\nMeasure [52] (MSSIM), and Spectral Angle Mapper (SAM).\nHigher MPSNR and MSSIM values correspond to better\nresults, while lower SAM values indicate better results.\nC. HSI Inpainting\nWe conduct experiments on two real-world remote sensing\ndatasets: Chikusei [53] and Indian-Pines [54]. The Chikusei\ndataset contains 192 spectral channels. Following [38], we\ncrop five 144 \u00d7 144 sub-images which serve as ground truth.\nThe Indian Pines dataset was acquired by the AVIRIS sensor\nand contains 220 spectral bands with a spatial resolution of\n145\u00d7145 pixels. We remove the last row and column of pixels\nto match the resolution of the Chikusei dataset and remove 20\ncorrupted spectral bands, preserving 200 bands. Four different\nmask shapes are used to corrupt the ground truth. Following\n[38], all the masks corrupt the whole bands in one column,\nwith mask ratio 12.5%, 23.6%, 16.67%, 41.67%.\nWe compare our SHARE with conventional linear pseudo-\ninverse H\u2020y, six learning-based methods: DHP [15], PnP-\nDIP [16], DeepHyIn [49], R-DLRHyIn [50], HEI [38], and\nMC. In implementation, We used the same Skip-Net [55] as\nthe backbone for the DHP, PnP-DIP and R-DLRHyIn. These\nmethods are all designed for single image inpainting and learn\nwithout ground truth.\nThe experimental results on the Chikusei and Indian Pines\nHSI inpainting datasets further verify the superiority of the\nSHARE method. As shown in Table I, SHARE achieves the\nbest performance on all three metrics on the Chikusei dataset,\nwith an MPSNR of 35.12, MSSIM of 0.917, and SAM of\n2.56, clearly outperforming the previous state-of-the-art HEI\n(MPSNR: 34.18, MSSIM: 0.901, SAM: 2.93). On the Indian\nPines dataset, SHARE also shows strong results, achieving an\nMPSNR of 28.54, MSSIM of 0.801 and SAM of 5.41, with\nparticular excellence in the structurally relevant MSSIM and\nSAM metrics, indicating improved perceptual and structural\nfidelity.\nIt is important to note that the MC method cannot inpaint\nthe masked regions. Instead, it only restores the known range\nspace, which explains its relatively high MPSNR. This sug-\ngests that MC preserves pixel-level similarity in the unmasked\nregions, but fails to recover coherent structure in the occluded\nregions. In contrast, SHARE effectively restores both masked\nand unmasked areas, demonstrating a strong ability to model\n8\nTABLE III\nCOMPARISON OF DIFFERENT METHODS ON CAVE: GLASS TILES MS DATASET. THE\nRED\nAND BLUE\nMARKERS INDICATE THE BEST AND\nSECOND-BEST VALUES AMONG ALL METHODS.\nMethod\nDownsample Rate \u00d72\nDownsample Rate \u00d74\nDownsample Rate \u00d78\nMPSNR \u2191\nMSSIM \u2191\nSAM \u2193\nMPSNR \u2191\nMSSIM \u2191\nSAM \u2193\nMPSNR\u2191\nMSSIM\u2191\nSAM \u2193\nBicubic\n20.73\n0.158\n45.38\n20.10\n0.145\n44.75\n19.54\n0.19\n44.06\nDHP [15]\n25.10\n0.642\n23.39\n22.45\n0.479\n26.48\n17.21\n0.104\n48.82\nPnP-DIP [16]\n25.79\n0.760\n20.22\n23.04\n0.648\n23.38\n20.42\n0.319\n32.29\nSSDL [18]\n28.64\n0.776\n12.78\n26.45\n0.684\n14.10\n25.87\n0.680\n14.78\nMC\n26.81\n0.703\n18.51\n25.85\n0.625\n19.02\n24.17\n0.603\n19.25\nSupervised [51]\n32.78\n0.875\n9.45\n29.15\n0.779\n11.69\n26.40\n0.667\n15.01\nSHARE\n31.22\n0.854\n8.36\n28.77\n0.775\n9.17\n26.77\n0.716\n10.49\nTABLE IV\nCOMPARISON OF DIFFERENT METHODS ON CHIKUSEI DATASET. THE\nRED\nAND BLUE\nMARKERS INDICATE THE BEST AND SECOND-BEST VALUES\nAMONG ALL METHODS. THE VALUES ARE COMPUTED AS THE AVERAGE RESULTS OF TWO SUBFIGURES OF CHIKUSEI DATASET.\nMethod\nDownsample Rate \u00d72\nDownsample Rate \u00d74\nMPSNR \u2191\nMSSIM \u2191\nSAM \u2193\nMPSNR \u2191\nMSSIM \u2191\nSAM \u2193\nBicubic\n20.03\n0.110\n23.49\n19.85\n0.153\n23.61\nDHP [15]\n28.52\n0.692\n6.36\n25.22\n0.447\n10.16\nPnP-DIP [16]\n28.85\n0.741\n5.28\n26.23\n0.554\n7.85\nSSDL [18]\n29.25\n0.757\n4.82\n26.96\n0.655\n8.65\nMC\n28.16\n0.591\n9.00\n26.87\n0.487\n10.48\nSHARE\n32.64\n0.822\n4.18\n30.52\n0.733\n5.04\nglobal structure and local details.\nThe visual results in Fig. 4 and Fig. 5 support these findings.\nSHARE produces clearer edges, more accurate textures, and\nperceptually pleasing results. In scenes with complex occlu-\nsions and structural damage, SHARE is able to seamlessly\nreconstruct missing content and maintain smooth transitions\nbetween restored and original regions, demonstrating its ad-\nvantages in contextual modelling and geometric consistency.\nPlease see Supplementary Material (SM) for more inpainting\nresults with more mask shapes.\nD. HSI Super-Resolution\nWe perform experiments on three popular datasets: CAVE\n[56], Pavia University [57], and Chikusei [53]. For the CAVE\ndataset, we select \u2019fake and real beers ms\u2019 and \u2019glass tiles\nms\u2019 as ground truth, with each HSI having a shape of\n512\u00d7512\u00d731. For the Pavia University dataset, we centrally\ncrop 320 \u00d7 320 \u00d7 103 sub-images as ground truth. For the\nChikusei dataset, we centrally crop a patch of 512\u00d7512\u00d7128.\nThe CAVE dataset is downsampled by \u00d72, \u00d74 and \u00d78, while\nthe other two datasets are downsampled by \u00d72 and \u00d74. To\nensure sufficient blurring and anti-aliasing, Gaussian blur is\napplied using kernels of sizes 15 \u00d7 15, 29 \u00d7 29, and 55 \u00d7 55\nwith standard deviations of 2, 4, and 8 for the CAVE dataset,\nand half of kernel size values for the remote sensing dataset,\nrespectively\nWe compare our SHARE method with conventional bicubic\ninterpolation and four learning-based methods: DHP [15],\nPnP-DIP [16], and SSDL [18], and MC. We also compare\nSHARE with supervised methods [51] on the CAVE dataset.\nTo the best of our knowledge, they are the most recent\nlearning-based works for unsupervised single HSI super-\nresolution with the exception of [51]. Note that [51] cannot\nbe directly applied to single HSI super-resolution, so we train\nit on other CAVE images, using rotation, flipping and patch\ncropping for data augmentation. For SSDL [18], although\nthe authors have not open-sourced their code, their core loss\nfunction components are included in the DeepInverse library\n[48], and the Bi-3DQRNN backbone [27] is in the public\ndomain. Therefore, we implement this method ourselves. In\naddition, for a fair unsupervised comparison, we remove the\nfirst stage of supervised learning in SSDL, leaving only the\nsecond stage of unsupervised learning. After this modification,\nall methods except the supervised one can be trained with\nsingle image.\nOn the CAVE dataset, we evaluate SHARE on the fake/real\nbeers ms and glass tiles ms. For the beers dataset, the\nquantitative results are reported in Table II, while the cor-\nresponding visual comparisons are shown in Fig. 6. At all\ndowmsampling scales, it ranks second overall but remains\nthe strongest among unsupervised approaches, being surpassed\nonly by the supervised method."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper \"SHARE: A Fully Unsupervised Framework for Single Hyperspectral Image Restoration\"\n\n#### Main Contributions:\n1. **SHARE (Single Hyperspectral Image Restoration with Equivariance)**: The paper introduces a novel fully unsupervised framework that leverages geometric equivariance principles to eliminate the need for ground truth data in hyperspectral image restoration tasks.\n2. **Incorporation of SURE**: The authors incorporate Stein Unbiased Risk Estimator (SURE) into SHARE, enhancing its robustness under real-world sensor imaging conditions and enabling a robust self-supervised HSI restoration method.\n3. **Dynamic Adaptive Spectral Attention (DASA)**: A new DASA module is proposed to explicitly encode the global low-rank property of hyperspectral images and adaptively refine local spectral-spatial correlations through learnable attention mechanisms.\n\n#### Methodology:\n1. **Problem Statement**: SHARE addresses single-image HSI restoration tasks such as inpainting and super-resolution, leveraging geometric equivariance principles.\n2. **Inverse Mapping f\u03b8**: The architecture consists of a U-shaped design with linear inverse modules, 2D convolutions to increase channel dimension, and encoder-decoder paths including spectral attention mechanisms.\n3. **Loss Function**: SHARE combines the SURE loss function with robust equivariance consistency constraints (Eq. 6) to ensure robustness under noisy conditions.\n\n#### Key Findings:\n1. **Inpainting Results**:\n   - On the Chikusei dataset, SHARE achieves a mean peak signal-to-noise ratio (MPSNR) of 35.12 and an MSSIM score of 0.917, outperforming previous state-of-the-art methods like HEI.\n   - On the Indian Pines dataset, SHARE demonstrates strong results with an MPSNR of 28.54 and a SAM score of 5.41, indicating improved perceptual and structural fidelity.\n\n2. **Super-Resolution Results**:\n   - On the CAVE: Fake and Real Beers MS dataset, SHARE ranks second overall but remains the strongest unsupervised method.\n   - On the CAVE: Glass Tiles MS dataset, SHARE achieves a high mean peak signal-to-noise ratio (MPSNR) of 31.22.\n\n#### Conclusions:\nSHARE represents a significant advancement in fully unsupervised hyperspectral image restoration by leveraging geometric equivariance and incorporating robust learning mechanisms like SURE. The method outperforms many state-of-the-art unsupervised approaches, achieving comparable performance to supervised methods while being free from the need for ground truth data. This work opens new avenues for HSI restoration and broader scientific imaging scenarios.\n\n#### Future Work:\n1. **Expanding Applications**: Investigating SHARE's applicability in other hyperspectral image processing tasks.\n2. **Enhancing Robustness**: Further refining the DASA module to improve spectral-spatial correlation refinement.\n3. **Cross-Domain Generalization**: Exploring how SHARE can be adapted for different types of hyperspectral imaging scenarios and datasets.\n\n### Detailed Breakdown:\n\n#### I. Introduction\n- **Problem Statement**: Hyperspectral image restoration is a fundamental challenge in computational imaging and computer vision, involving ill-posed inverse problems such as inpainting and super-resolution.\n- **Current State**: Deep learning methods have transformed the field but require meticulously curated ground-truth datasets, limiting their applicability.\n\n#### II. Related Work\n1. **HSI Restoration**:\n   - **Prior-Based Methods**: Exploit spectral information via sparsity-based or TV regularization approaches.\n   - **Learning-Based Methods**: Use CNNs and Vision Transformers (VIT) as backbones to learn from paired data sets, often requiring large datasets.\n\n2. **Equivariant Imaging**:\n   - **State-of-the-Art Framework**: EI uses group invariance properties inherent in natural signals to learn a reconstruction function using only partial measurements.\n   - **Proposed Method**: SHARE extends this framework by exploiting geometric equivariance and incorporating robust learning mechanisms like SURE.\n\n#### III. Method\n1. **Problem Statement**:\n   - **Inpainting**: Reconstructs missing stripes in HSI due to platform limitations or sensor noise.\n   - **Super-Resolution**: Enhances spatial resolution while maintaining spectral information, addressing the inherent trade-off between these dimensions.\n\n2. **Inverse Mapping f\u03b8**:\n   - **Architecture**: U-shaped design with linear inverse modules and 2D convolutions for increasing channel dimension.\n   - **Encoder-Decoder Path**: Includes spectral attention mechanisms to refine local correlations adaptively.\n\n3. **Loss Function**:\n   - **SURE Loss**: Incorporates SURE function to estimate clean measurement consistency, ensuring robustness under noisy conditions.\n   - **Robust Equivariance Constraint**: Ensures consistency with testing phase by incorporating noise into training data and using a modified loss formulation (Eq. 6).\n\n#### IV. Experiments\n1. **Implementation Details**:\n   - **Pytorch and DeepInverse Library**: Used for experiments, leveraging a single NVIDIA GeForce RTX 3090 GPU.\n   - **Training Parameters**: Adam optimizer with initial learning rate of 1e\u22123, cosine scheduler within 2000 epochs.\n\n2. **Evaluation Metrics**:\n   - **MPSNR (Mean Peak Signal-to-Noise Ratio)**: Measures the signal strength relative to noise.\n   - **MSSIM (Mean Structural Similarity Index Measure)**: Evaluates structural similarity between images.\n   - **SAM (Spectral Angle Mapper)**: Quantifies spectral angle differences, indicating perceptual and structural fidelity.\n\n3. **Results**:\n   - **Inpainting**: SHARE outperforms previous state-of-the-art methods like HEI on both Chikusei and Indian Pines datasets.\n   - **Super-Resolution**: On the CAVE dataset, SHARE ranks second overall but remains the strongest unsupervised method.\n\n#### V. Conclusions\nSHARE provides a robust and effective framework for fully unsupervised hyperspectral image restoration by leveraging geometric equivariance principles and incorporating SURE to enhance robustness under noisy conditions. The proposed DASA module further improves spectral-spatial correlation refinement, demonstrating its effectiveness in both inpainting and super-resolution tasks.\n\n### Future Work:\n1. **Cross-Domain Generalization**: Investigate SHARE's applicability across different types of hyperspectral imaging scenarios.\n2. **Enhanced Robustness**: Further refine the DASA module to improve local spectral-spatial correlation refinement.\n3. **Expanding Applications**: Explore how SHARE can be adapted for other HSI processing tasks and datasets.\n\nThis comprehensive summary encapsulates the main contributions, methodology, key findings, and conclusions of the paper \"SHARE: A Fully Unsupervised Framework for Single Hyperspectral Image Restoration\"."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "One-Shot Refiner: Boosting Feed-forward\nNovel View Synthesis via One-Step Diffusion\nYitong Dong1,2, Qi Zhang2, Minchao Jiang2,3, Zhiqiang Wu2,4, Qingnan Fan2,\nYing Feng2, Huaqi Zhang2, Hujun Bao1, Guofeng Zhang1*\n1Zhejiang University\n2Hangzhou VIVO Information Technology Co., Ltd\n3Xidian University\n4East China Normal University\ndongyitong@zju.edu.cn, nwpuqzhang@gmail.com, zhangguofeng@zju.edu.cn\nAbstract\nWe present a novel framework for high-fidelity novel view\nsynthesis (NVS) from sparse images, addressing key limita-\ntions in recent feed-forward 3D Gaussian Splatting (3DGS)\nmethods built on Vision Transformer (ViT) backbones. While\nViT-based pipelines offer strong geometric priors, they are\noften constrained by low-resolution inputs due to compu-\ntational costs. Moreover, existing generative enhancement\nmethods tend to be 3D-agnostic, resulting in inconsistent\nstructures across views, especially in unseen regions. To over-\ncome these challenges, we design a Dual-Domain Detail\nPerception Module, which enables handling high-resolution\nimages without being limited by the ViT backbone, and\nendows Gaussians with additional features to store high-\nfrequency details. We develop a feature-guided diffusion net-\nwork, which can preserve high-frequency details during the\nrestoration process. We introduce a unified training strategy\nthat enables joint optimization of the ViT-based geometric\nbackbone and the diffusion-based refinement module. Exper-\niments demonstrate that our method can maintain superior\ngeneration quality across multiple datasets.\n1\nIntroduction\nScene understanding and novel view synthesis have seen\nrapid progress in recent years (Dong et al. 2022; Zhai et al.\n2025b,a), largely driven by the transformative impact of\ndifferentiable rendering (Mildenhall et al. 2021) on dig-\nital content creation. Specifically, 3D Gaussian Splatting\n(3DGS) (Kerbl et al. 2023) has achieved an unprecedented\nbreakthrough in this domain, enabling high-fidelity render-\ning with remarkable performance. However, this approach\nfaces a critical limitation: 3DGS requires extensive, time-\nconsuming optimization for each individual scene, which\nserves as a significant bottleneck, prohibiting efficient ap-\nplications and limiting its generalizability.\nTo overcome this limitation, a new paradigm has emerged\nbuilt upon 3D foundation models (Wang et al. 2024; Leroy,\nCabon, and Revaud 2024; Wang et al. 2025), which typi-\ncally leverage a pre-trained Vision Transformer (ViT) back-\nbone. By integrating these powerful 3D geometric priors\n*Corresponding author.\nCopyright \u00a9 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nContext Views\nNovel Views\n3DGS with Feature\nFeed-forward\nInference\nDenoising\nSplatting\nFigure 1: Starting from unposed input images, our method\nreconstructs 3D Gaussians within a canonical space, and\nleverages a one-step Stable Diffusion (SD) module to syn-\nthesize high-fidelity target views.\nwith 3DGS, such methods (Ye et al. 2024; Jiang et al. 2025a)\ncan synthesize novel views from unposed sparse views via\nrapid single-pass inference. Although this advancement sig-\nnificantly boosts efficiency and generalization, a notable\nquality gap persists. These feed-forward models often strug-\ngle to generate high-fidelity novel views, particularly in ren-\ndering high-frequency details. A straightforward strategy\nmight be to feed high-resolution images directly into a larger\nViT backbone. However, this approach results in substantial\nmemory overhead and restricts the network\u2019s applicability\nin real-world scenarios. It is essential to propose an efficient\nmodule to enhance the capability of ViT-based backbones to\ninfer high-grained features more effectively.\nMoreover, 2D generative models, such as diffusion mod-\nels, excel in capturing intricate details and textures, enabling\nthem to produce high-quality 2D images. However, their di-\nrect application as a post-processing step for NVS presents a\nfundamental conflict: these models are 3D-agnostic, result-\ning in inconsistent structures across views. Compounding\nthis issue is their notoriously slow, iterative sampling pro-\ncess. Our work overcomes these hurdles with a carefully de-\nsigned Feature-Guided One-Step Diffusion architecture that\nis both fast and geometrically aware. The core of our inno-\nvation, however, lies in how we guide the generation to re-\nspect the scene structure. As illustrated in Fig. 3, we employ\na two-pronged conditioning strategy. A dedicated guidance\nbranch relays explicit geometric priors from the 3D back-\narXiv:2601.14161v1  [cs.CV]  20 Jan 2026\nbone, anchoring the generative process to the scene\u2019s true\nstructure. Simultaneously, the input view serves as a refer-\nence condition, guaranteeing that the final output preserves\nfine-grained information.\nConsequently, we achieve an end-to-end framework to en-\nhance the high-quality novel view synthesis from unposed\nsparse inputs, as shown in Fig. 1. Specifically, the main con-\ntributions of our work are as follows:\n1. We design a Dual-Domain Detail Perception Module\n(DD-DPM), which enables handling high-resolution im-\nages without being limited by the ViT backbone, and en-\ndows Gaussians with additional features to store high-\nfrequency details.\n2. We develop a Feature-Guided One-Step Diffusion archi-\ntecture, which can preserve high-frequency details during\nthe restoration process.\n3. We propose an integrated training framework that allows\nend-to-end training of the ViT reconstruction backbone\nand the diffusion-based image enhancement module.\n2\nRelated Work\nRadiance fields for novel view synthesis. Neural Radi-\nance Fields (NeRF) (Mildenhall et al. 2021) model contin-\nuous volumetric density and radiance, enabling high-quality\nnovel view synthesis but relying on positional encoding and\nimportance sampling, which limits real-time performance.\n3D Gaussian Splatting (3DGS) (Kerbl et al. 2023) instead\nrepresents a scene with millions of anisotropic Gaussians\nand uses differentiable rasterization to achieve photoreal-\nistic rendering at over 30 FPS (1080p). Its efficiency has\nspurred extensions in rendering (Lu et al. 2024; Dong et al.\n2024), surface reconstruction (Huang et al. 2024a; Chen\net al. 2024a; Yu, Sattler, and Geiger 2024), generation (Yi\net al. 2023), and scene understanding (Qin et al. 2024; Jiang\net al. 2025b). Despite these advances, both NeRF and 3DGS\nrely on dense posed images and require per-scene optimiza-\ntion, limiting their practicality in sparse-view settings.\nGeneralizable 3D Reconstruction for Sparse View. With\nthe rise of pre-trained foundation models (Wang et al.\n2024; Leroy, Cabon, and Revaud 2024; Wang et al. 2025)\nthat encode strong geometric priors, feed-forward novel\nview synthesis (NVS) from sparse inputs has received in-\ncreasing attention. Unlike other modality-based 3D recon-\nstruction methods (Zhang et al. 2024, 2025b), these ap-\nproaches (Zhang et al. 2025a; Ye et al. 2024; Jiang et al.\n2025a) generally leverage foundation models as backbones\nto extract geometric cues and improve reconstruction qual-\nity. MuRF (Xu et al. 2024) aggregates multi-view in-\nformation through cost-volume construction, while pixel-\nSplat (Charatan et al. 2024) exploits epipolar geometry to\nachieve more accurate depth estimation.\nDiffusion priors for novel view synthesis. Incorporating\ndiffusion models into reconstruction tasks has been shown\nto enhance the quality of novel view generation. Recon-\nFusion (Wu et al. 2024) leverages 2D diffusion priors to\nrecover high-fidelity NeRF from sparsely sampled input\nviews. DiffusionNeRF (Wynn and Turmukhambetov 2023)\nemploys a diffusion model to learn gradients of logarithmic\nRGBD patch priors, which serve as regularized geometry\nand color for a scene. Nerfbusters (Warburg et al. 2023)\nutilizes diffusion priors to remove artifacts. Methods such\nas ReconX (Liu et al. 2024), 3DGS-Enhancer (Liu, Zhou,\nand Huang 2024), and Difix3D+ (Wu et al. 2025a) require\nper-scene optimization (e.g., Difix3D+ takes 15\u201330 minutes\nper scene) and perform poorly under sparse inputs. MVS-\nplat360 (Chen et al. 2024b) directly denoises rendered fea-\ntures via video diffusion, but is time-consuming, whereas\nLatentSplat (Wewer et al. 2024) relies on a lightweight VAE-\nGAN decoder and does not exploit the generative capabili-\nties of a UNet for denoising. Our goal is to achieve feed-\nforward novel view synthesis under sparse viewpoints by\nusing one-step Stable Diffusion for refinement.\n3\nMethod\nGiven unposed sparse-view images {Ii}N\ni=1 \u2208RH\u00d7W \u00d73\nand their intrinsics {ki}N\ni=1 \u2208R3\u00d73, Our method learns a\nfeed-forward network to generate 3D Gaussians for novel\nview synthesis (NVS), with an additional refinement mod-\nule applied to further improve rendering quality. The scene\ncan be represented by 3D Gaussian Splatting (3DGS): gj :=\n{\u00b5j, sj, qj, oj, cj}. Here, \u00b5j \u2208R3 and oj \u2208R denote\nthe Gaussian center and opacity, sj \u2208R3 and qj \u2208R4\ndefine the 3D covariance, and cj \u2208[0, 1]3 represents RGB\ncolor via spherical harmonics coefficients. These primitives\nallow efficient modeling of 3D geometry and appearance for\nhigh-quality novel view synthesis. The overall pipeline of\nour method is illustrated in Fig. 2. Our network architecture\nconsists of an encoder, a decoder, Gaussian parameter pre-\ndiction heads, and a final enhancement module.\n3.1\nGeometry Transformer Backbone\nGiven unposed images Ii, we use a pretrained Vision Trans-\nformer (ViT) module (Leroy, Cabon, and Revaud 2024) to\nacquire the geometric information of the scene. Following\nthe design in (Leroy, Cabon, and Revaud 2024), the geome-\ntry transformer module comprises an encoder and a decoder.\nEncoder. We initially patchify each RGB image Ii into se-\nquences of image tokens tI\ni . To enhance the network\u2019s capa-\nbility of perceiving geometric information and thereby fur-\nther optimize reconstruction quality, we inject the intrinsic\nparameter information of each image into the model. Specif-\nically, we inject camera intrinsic parameters [fx.fy, cx, cy]\nof each image into a linear layer to obtain global feature to-\nkens tC\ni , which are then expanded to corresponding image\ntokens. Next, the concatenated tokens from each view are\nindividually input to a ViT encoder, with the encoder em-\nploying shared weights across all views.\nDecoder. The combined tokens from the encoder are then\ninput into the ViT decoder, where cross-view information\ninteraction is achieved through attention modules, resulting\nin features that contain global geometric information. This\nglobal feature is then used to estimate 3D scene parameters,\nsuch as point clouds and Gaussian parameters.\n\ud835\udc70\ud835\udfcf\n\ud835\udc70\ud835\udfce\nD\nIntrinsics \ud835\udc8c\ud835\udfce\nIntrinsics \ud835\udc8c\ud835\udfcf\nC\nLinear\nLinear\nViT\nEncoder\nViT\nDecoder\nDual-Domain Detail Perception Module\nGaussian\nHead\nStable Diffusion\nImage\nGround Truth\nLOSS\nLOSS\nDual-Domain Detail Perception Module\nD\nC\nViT\nEncoder\nViT\nDecoder\nD\nC\nDownsampling\nConcat\nSplatting\n3DGS\nwith Feature\nFeature Maps\nVisual Geometry\nTransformer Backbone\n\ud835\udcd5\n\ud835\udcd5\u2212\ud835\udfcf\n\ud835\udc7e\ud835\udf4d: nn.parameter\n\u2219\n\ud835\udc9b(\ud835\udf43)\nCNN\nEncoder\nU-Net\nDecoder\nDecoder\n\ud835\udcd5Fourier Transform\nDot Product\n\u2219\nDenoised Image\nFigure 2: Overview of our pipeline. Starting from a set of unposed images, we first perform spatial downsampling and feed\nthem into a Vision Transformer (ViT)-based backbone for global feature extraction. Simultaneously, we employ a Dual-Domain\nDetail Perception Module to enhance fine-grained detail perception from both spatial and frequency domains. The fused features\nare passed into Gaussian Parameter Prediction Heads to directly predict Gaussians with features in a canonical space. Finally,\na Single-step Denoising module refines these outputs to produce higher-quality novel view synthesis (NVS) results.\n3.2\nDetail-Aware Scene Reconstruction\nDue to memory constraints, existing feed-forward net-\nworks (Charatan et al. 2024; Ye et al. 2024) based on pre-\ntrained transformer foundation models (Wang et al. 2024;\nLeroy, Cabon, and Revaud 2024; Wang et al. 2025) com-\nmonly restrict input images to low resolutions such as (256\u00d7\n256). However, high-resolution images are readily available\nin real-world scenarios, and directly feeding their downsam-\npled versions into the Geometry Transformer Backbone can\nlead to the loss of critical structural and appearance infor-\nmation. Feeding high-resolution images directly into the ViT\nbackbone, or employing additional complex modules to han-\ndle them, results in substantial memory overhead. This in-\ncreases the computational burden and restricts the network\u2019s\napplicability in real-world scenarios. To address this issue,\nwe propose two methods to enable detail-aware reconstruc-\ntion. First, we introduce a lightweight Dual-Domain Detail\nPerception Module (DD-DPM), which efficiently extracts\ninformative cues from high-resolution inputs with low mem-\nory overhead and feeds them into the Gaussian Parameter\nPrediction Heads. Second, we extend the feature dimension-\nality of each Gaussian and implicitly inject information from\nneighboring pixels in the high-resolution image into the cor-\nresponding Gaussian features, thus enhancing the preserva-\ntion of fine-grained details.\nDual-Domain Detail Perception Module. To better cap-\nture both local textures and global patterns, we propose\na Dual-Domain Detail Perception Module (DD-DPM) that\nprocesses image data jointly in the spatial and frequency\ndomains. Converting images into the frequency domain\nis a widely adopted strategy, particularly in image super-\nresolution tasks, as frequency components encode global\nstructures without relying on a strict spatial correspondence.\nThis property enables more flexible and effective model-\ning of image details. Motivated by this, we incorporate a\nfrequency-domain adaptation module to enhance the expres-\nsiveness of the extracted features. The module begins by ap-\nplying a 2D Fourier transform to obtain spectral representa-\ntions of the input images.\nz(\u03be) = F(Ii),\n(1)\nwhere F denotes the Fourier transforms and \u03be is the fre-\nquency domain variables. To focus on the most informative\nspectral signals, we use an MLP to predict importance scores\nover normalized frequency coordinates. A top-k selection is\napplied to retain key components, which are then modulated\nby learned complex weights. The final output is transformed\nback through an inverse Fourier transform:\nF \u2032\ni = F\u22121(z\u2032(\u03be)),\n(2)\nwhere F\u22121 denotes the inverse Fourier transforms. On the\nother hand, a lightweight CNN is employed to capture fine-\ngrained texture information from the spatial domain, which\nis then combined with frequency-domain features and fed\ninto the Gaussian Parameter Prediction Heads.\nGaussian Parameter Prediction Heads. We design heads\nbased on the DPT decoder (Ranftl, Bochkovskiy, and Koltun\n2021) to predict Gaussian parameters. Given that the fea-\ntures derived from the ViT encoder and decoder already con-\ntain strong geometric priors, we directly utilize these global\nfeatures through a depth head to obtain depth values, which\nserve as the center of each Gaussian \u00b5j. This can largely\nensure the spatial consistency of Gaussians.\nWe utilize a second head to predict the remaining Gaus-\nsian parameters. To preserve as much valid information con-\ntained in high-resolution images as possible, we have de-\nsigned two strategies: First, in addition to the global fea-\ntures obtained from the ViT Transformer and the images\nthemselves, we fuse the features derived from the Weighted\nFourier Neural Operator, thereby further enhancing the rep-\nresentational capability of Gaussians. Second, we assign ad-\nFigure 3: Structure of the diffusion model in NVS.\nditional features to each Gaussian to contain detailed infor-\nmation, which in turn assists our enhancement module.\nRendering novel view images with feature. We de-\nsign feature-augmented 3D Gaussians, where each Gaus-\nsian is equipped with additional features to preserve high-\nfrequency details. With this design, once the ViT back-\nbone reconstructs the scene and generates the 3D Gaussians,\nnovel-view rendering produces not only RGB color values\nbut also the corresponding high-dimensional Gaussian fea-\ntures. These features can subsequently be used to support\n3D-consistent image enhancement in downstream modules.\n3.3\nBoosting NVS with Diffusion module\nWhile existing feed-forward methods benefit from the strong\nrepresentation capability of ViT models to recover accurate\ngeometric structures, the subsequent novel view rendering\nprocess still inevitably encounters 3D artifacts and blurred\ndetails. This problem largely arises from limited supervision\nprovided by the input views, ultimately reducing the quality\nand perceptual fidelity of the generated novel views. There-\nfore, a super-resolution algorithm is needed to improve the\nfidelity and realism of the rendered images. To overcome\nthis limitation, we utilize a one-step Stable Diffusion (SD)\narchitecture to enhance the synthesized novel views, yield-\ning higher-quality target images. Building on this founda-\ntion, we further optimize the SD module by incorporating\nGaussian features as auxiliary inputs within UNet structure\nand integrating reference images into UNet\u2019s self-attention\nmodule to facilitate inter-image information exchange.\nStable Diffusion. Diffusion Models (DMs) learn data dis-\ntributions through iterative denoising, and this process be-\ncomes significantly more efficient when performed in the la-\ntent space using a pre-trained autoencoder (Wu et al. 2025b).\nWe employ the pre-trained latent diffusion model to miti-\ngate 3D artifacts in novel view synthesis, typically caused by\nsparse supervision or geometric inconsistencies. Using the\nVAE encoder E\u03d5, latent diffusion network \u03f5\u03d5, and the VAE\ndecoder D\u03d5\u2014where \u03d5 denotes the model parameters\u2014the\nimages can be denoised to achieve higher quality. Build-\ning on this, we further incorporate geometric alignment with\n3D Gaussian features, enabling the SD module to generate a\nhigh-fidelity and 3D-consistent denoised target view Id via\nlatent-space diffusion decoding.\nGaussian Feature Integration in UNet. After encoding the\nlow-quality rendering image Ir through the VAE encoder\nE\u03d5 to obtain the latent feature Fr \u2208RC\u00d7 H\n8 \u00d7 W\n8 , we render\nGaussian feature Fg \u2208RC\u2032\u00d7 H\n8 \u00d7 W\n8 along the correspond-\ning dimension ( H\n8 , W\n8 ) and concatenate it with Fr along the\nchannel dimension.\nF = CAT (Fr, Fg),\n(3)\nThe fused features are then fed into the UNet module for\ncross-modal interaction. Notably, we extend the input chan-\nnels of the UNet\u2019s initial convolution to accommodate the\nadditional features, initializing the extra dimensions to zero\nto facilitate stable convergence.\nReference Image Interaction via Self-Attention. We uti-\nlize reference images as guidance to assist the diffusion\nmodel in generating high-quality images (Wu et al. 2025a;\nAgarwal et al. 2025). Specifically, we first concatenate the\nreference images with the Gaussian-rendered target image\nand feed the combined result into the VAE encoder to ob-\ntain latent features. We then simultaneously extract Gaus-\nsian features under both the reference and target views, and\nintegrate them to produce fused features z \u2208RV \u00d7C\u00d7H\u00d7W .\nWithin the latent diffusion model, we modify the self-\nattention layers to transform the image interactions within\nthe low-quality image z \u2208RV \u00d7C\u00d7(H\u00d7W ) into a mixed at-\ntention mechanism between the low-quality image and the\nreference image z \u2208RC\u00d7(V \u00d7H\u00d7W ). In this way, we can\nfurther capture fine-grained details from reference image.\nTwo-Stage Pipeline. In the first stage, we employ LoRA to\nfine-tune the latent diffusion network \u03f5\u03d5 of the Stable Dif-\nfusion (SD) module, encouraging it to restore images de-\ngraded by 3D rendering artifacts. To train the model effec-\ntively, we construct a paired dataset using DL3DV, com-\nprising low-quality rendered images and their correspond-\ning high-quality ground-truth (GT) images. Specifically, fol-\nlowing the sampling strategy of (Ye et al. 2024), we encode\nsparse input views into 3D Gaussian scene representations\nusing a geometry Transformer backbone and scene recon-\nstruction module, and render novel views based on sam-\npled camera extrinsics. These rendered images and their GT\ncounterparts form the supervision pairs for training.\nIn the second stage, directly optimizing rendered images\nwith the Stable Diffusion (SD) architecture can mitigate arti-\nfacts to some extent, yet it struggles to preserve fine-grained\ntextures in high-resolution outputs and maintain geometric\nconsistency. Gaussian features, however, contain both rich\ntextural details and geometric cues. To address this limita-\ntion, we design a pipeline that jointly performs scene re-\nconstruction and integrates a feature-guided SD module to\nachieve higher-fidelity image synthesis.\n3.4\nTraining\nThe architecture comprises a front-end encoder, a decoder,\nand a back-end diffusion-based enhancement module. The\nnetwork is fully trainable in an end-to-end manner. Given\nthe pose, the 3D Gaussian constructed by the front-end is\ncapable of rendering images Ir and their corresponding fea-\ntures. These images and features are fed into the enhance-\nment module, which then leverages the priors of diffusion to\nenhance image quality, resulting in the final images Id.\nTraining Loss. For reconstruction, we use the standard MSE\nloss together with the perceptual LPIPS loss between the\nFLARE\nNopoSplat\nAnySplat\nOurs\nGT\nInput\nFigure 4: Qualitative comparison on DL3DV and RE10K datasets, all evaluated at a uniform resolution of 512 \u00d7 512. Com-\npared with other methods, our approach is capable of recovering finer texture details.\nrendered image Ir and the ground-truth image \u02c6I:\nLr = \u03bb1 \u00b7 MSE(Ir, \u02c6I) + \u03bb2 \u00b7 LPIPS(Ir, \u02c6I),\n(4)\nFor the enhancement module, we first directly introduce\nthe MSE loss and LPIPS loss between the denoising image\nId and the target image \u02c6I:\nLd = \u03bb3 \u00b7 MSE(Id, \u02c6I) + \u03bb4 \u00b7 LPIPS(Id, \u02c6I),\n(5)\nTo enhance perceptual quality, we incorporate a DINOv2\npre-trained network as a feature extractor to guide the train-\ning of the GAN (Kumari et al. 2022). This encourages the\ngenerator to produce outputs that are not only visually re-\nalistic but also semantically consistent with the input. The\nGAN loss is formulated as:\nLg = \u03bb5E[log D(\u02c6I)] + \u03bb5E[log(1 \u2212D(G(I)))],\n(6)\nwhere G and D are the generator and discriminator using\nDINOv2 backbone for training, respectively. The total loss\ncan be expressed as:\nLtotal = \u03bbr \u00b7 Lr + \u03bbd \u00b7 Ld + \u03bbg \u00b7 Lg,\n(7)\nTraining pipeline. We first train the ViT-based 3D recon-\nstruction module independently to obtain a stable 3D repre-\nsentation. Specifically, images resized to 256\u00d7256 are input\nto the ViT backbone to extract global semantic features. Si-\nmultaneously, the Dual-Domain Detail Perception Module\nprocesses higher-resolution images at 512\u00d7512 to capture\nfine-grained details. Both feature streams are subsequently\nfused in the prediction heads to reconstruct the 3D Gaussian\nrepresentation. Following the rendering of images and fea-\ntures, the SD module is first trained independently. Subse-\nquently, we perform joint training of the SD module guided\nby the Gaussian features, as detailed in Sec. 3.3.\n4\nExperiments\n4.1\nTraining Details\nDataset. We trained our model on the 2K subset of the\nDL3DV dataset and evaluated it on the benchmark sub-\nset (Ling et al. 2024). DL3DV is a large-scale 3D scene\ndataset widely used for NVS, featuring a variety of re-\nflection, transparency, and lighting conditions. To further\nassess generalization, we evaluated our approach on the\nRealEstate10k (RE10K) dataset (Zhou et al. 2018), which\ncomprises large-scale indoor real estate videos with multi-\nview images and accurate camera poses. This setup enables\ntesting the robustness of our method in NVS under complex\nreal-world lighting, textures, and geometric variations.\nEvaluation Metrics. The quality of novel view synthesis is\nfirst measured using three standard metrics: PSNR, SSIM,\nand LPIPS. In addition, we have incorporated parameter-free\nmetrics, including DISTS, FID, NIQE, MUSIQ, M-IQA,\nand C-IQA. Detailed analyses are provided in Sec. 4.2.\nDataset\nMethods\nFull-Reference\nNo-Reference\nPerceptual Quality\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nNIQE\u2193\nMUSIQ\u2191\nM-IQA\u2191\nC-IQA\u2191\nFID\u2193\nDL3DV\nNopoSplat\n15.53\n0.47\n0.57\n4.94\n59.77\n0.51\n0.29\n118.42\nAnySplat\n18.27\n0.55\n0.27\n3.53\n64.37\n0.68\n0.36\n64.91\nFLARE\n15.90\n0.48\n0.55\n4.41\n59.73\n0.54\n0.31\n122.24\nOurs\n22.67\n0.69\n0.16\n3.87\n72.86\n0.72\n0.49\n40.46\nRE10K\nNopoSplat\n15.81\n0.60\n0.54\n5.85\n53.97\n0.49\n0.31\n79.81\nAnySplat\n19.07\n0.67\n0.23\n4.31\n60.96\n0.65\n0.29\n42.20\nFLARE\n16.87\n0.62\n0.46\n5.39\n52.83\n0.52\n0.28\n67.87\nOurs\n20.67\n0.70\n0.21\n4.71\n69.05\n0.68\n0.39\n33.52\nTable 1: Novel view synthesis performance on DL3DV and RE10K datasets, all evaluated at a uniform resolution of 512 \u00d7 512.\nMethods\nLPIPS\u2193\nMS\u2191\nSC\u2191\nBC\u2191\nMVSplat360\n0.35\n0.95\n0.90\n0.92\nLatentSplat\n0.27\n0.95\n0.90\n0.91\nOurs\n0.19\n0.95\n0.92\n0.93\nTable 2: Quantitative evaluation of the 3D consistency per-\nformance of generated videos on the DL3DV dataset.\nMethods\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nFID\u2193\nMARINER\n19.99\n0.64\n0.20\n63.04\nDifix3d+\n21.67\n0.67\n0.18\n50.30\nOurs\n22.67\n0.69\n0.16\n40.46\nTable 3: Quantitative comparison with enhancement meth-\nods (MARINER and the SD model used in Difix3D+).\n4.2\nResults\nNovel View Synthesis. We selected state-of-the-art (SOTA)\nfeed-forward methods to compare the performance of novel\nview synthesis (NVS) results with our method on the test\nset of DL3DV, where the resolution of the final output is\n512\u00d7512. As shown in Table. 1, our method attains the su-\nperior performance among all compared approaches.\n3D Consistent. To assess multi-view 3D consistency, con-\nsecutive frames are sampled as target views to gener-\nate videos. We compare our approach with NVS meth-\nods that also perform refinement on rendered images, in-\ncluding MVSplat360 and LatentSplat. The Motion Smooth-\nness (MS), Subject Consistency (SC), and Background Con-\nsistency (BC) metrics from VBench (Huang et al. 2024b;\nZheng et al. 2025; Huang et al. 2024c) are employed to\njointly evaluate 3D consistency. As shown in Table 2, our\nmethod demonstrates substantially improved geometric con-\nsistency under sparse viewpoints.\nEnhancement Module. We compare our feature-guided SD\nenhancement module with existing enhancement methods\nby substituting it with MARINER (B\u00a8osiger et al. 2024) and\nthe SD model from Difix3D+ (Wu et al. 2025a). Table 3\nshows that our approach surpasses these methods, effec-\ntively utilizing 2D reference cues while maintaining 3D con-\nsistency via Gaussian features.\nMethods\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nFID\u2193\nNopoSplat\n15.16\n0.59\n0.77\n246.80\nAnySplat\n15.36\n0.45\n0.54\n108.74\nFLARE\n15.83\n0.59\n0.73\n230.59\nOurs\n21.64\n0.69\n0.28\n41.74\nTable 4: NVS performance comparison on the DL3DV\ndataset at an extended resolution of 1024 \u00d7 1024.\nCross-Dataset Generalization. We assess the generaliza-\ntion performance of our network on the RE10K dataset with-\nout fine-tuning. As shown in Table 1, the results indicate that\nour method exhibits strong generalization.\nResolution Expansion. We expand the output image into a\nhigher-dimensional space without requiring additional train-\ning. Specifically, we first render low-resolution images and\nupsample them to high resolution (1024 \u00d7 1024), followed\nby generating high-quality outputs using Tile-VAE.\n4.3\nAblation Studies\nWe perform ablation studies on a baseline trained with the\nDL3DV dataset to assess the impact of the proposed module.\nAblation on DD-DPM. To evaluate the effectiveness of the\nDetail-Preserving Module (DPM), we construct two abla-\ntion variants: (1) CNN-DPM, which relies solely on CNN to\nprocess high-resolution images and fuses its output with the\nlow-resolution features from the ViT backbone; and (2) DD-\nDPM, which additionally introduces a frequency-domain\nprocessing branch to enable dual-domain detail perception.\nTable 5 indicates the clear advantage of DD-DPM, empha-\nsizing the importance of frequency-domain features in main-\ntaining fine details.\nAblation on Gaussian Feature. To verify the effectiveness\nof introducing features into Gaussian parameters, we di-\nrectly connected a simple two-layer CNN structure at the\nbackend, which is used to process the rendered features and\nimages. This lightweight architecture serves as a baseline to\nassess whether the inclusion of Gaussian feature can bring\ntangible improvements. By comparing its performance with\nour full model, we can explicitly quantify the gains from\nembedding features into Gaussian representations during the\nrendering and post-processing pipeline.\nBase\nCNN-DPM\nSD\nOurs\nGT\nInput\nGS Feature\nDD-DPM\nFigure 5: Qualitative ablation results validating the effectiveness of each component. Incorporating Dual-Domain Detail Per-\nception Module (DD-DPM) and the feature-guided SD refine module yields substantially higher-quality novel view synthesis.\nMethods\nFull-Reference\nNo-Reference\nPerceptual Quality\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nNIQE\u2193\nMUSIQ\u2191\nM-IQA\u2191\nC-IQA\u2191\nFID\u2193\nBase\n20.08\n0.65\n0.24\n4.11\n65.98\n0.55\n0.29\n62.19\n+CNN-DPM\n22.10\n0.69\n0.23\n4.24\n64.95\n0.59\n0.30\n62.60\n+DD-DPM\n22.14\n0.69\n0.22\n4.15\n63.47\n0.60\n0.30\n55.32\n+GS Feature(CNN)\n22.22\n0.69\n0.27\n4.93\n56.15\n0.55\n0.30\n70.51\n+SD\n22.17\n0.68\n0.17\n3.92\n70.97\n0.69\n0.44\n48.58\n+Feature guided SD\n22.67\n0.69\n0.16\n3.87\n72.86\n0.72\n0.49\n40.46\nTable 5: Ablation study validating the effectiveness of each network component on the DL3DV dataset.\nAblation on SD Model. We constructed a dataset to train\nthe SD network independently and connected it to the back-\nend of the reconstruction module to as an enhancement pro-\ncess for rendered images. This standalone training strategy\nallows the SD network to specialize in refining details and\nenhancing visual quality for novel view synthesis, while its\nintegration with the reconstruction module ensures that the\nenhanced results remain consistent with the geometric struc-\nture predicted by the front-end, thereby achieving both accu-\nrate novel view synthesis and high-fidelity image quality.\nAblation on Feature-guided SD Refinement Module. We\nintroduce a feature-guided SD network and perform joint\ntraining of both the ViT-based reconstruction module and the\nSD network. As illustrated in Table 5, incorporating the de-\ntailed features rendered by 3D Gaussians into the SD mod-\nule enables efficient image refinement. This joint optimiza-\ntion strategy ensures that the SD network learns to lever-\nage geometrically consistent detail cues from the render-\ning process, guiding its refinement efforts toward preserv-\ning structural integrity while enhancing visual fidelity, ulti-\nmately leading to more coherent and high-quality novel view\nsynthesis compared to standalone enhancement approaches.\nAnalysis. We observe that although incorporating the SD\nmodule results in a slight decrease in simple metrics such\nas PSNR and SSIM, the perceptual quality of the outputs is\nsignificantly improved, producing clear and high-quality im-\nages. In contrast, omitting the SD module yields noticeably\nblurrier results. To quantitatively validate this improvement,\nwe evaluate model-based and no-reference metrics includ-\ning LPIPS, NIQE, MUSIQ, M-IQA, C-IQA, and FID, and\nfind that the model with the SD module consistently out-\nperforms the version without it across all metrics. Indeed,\nPSNR can no longer fully capture the true perceptual qual-\nity of images. LPIPS (Zhang et al. 2018) also points out that\nPSNR and SSIM often do not perfectly align with human\nperception. Consequently, when differences in simple met-\nrics (e.g., PSNR) are marginal, model-based or no-reference\nmetrics play a more important role in evaluation.\n5\nConclution\nSummary. We propose a novel view synthesis method that\nintegrates a feed-forward pipeline with a single-step Stable\nDiffusion (SD) model. This combination leverages the geo-\nmetric efficiency of feed-forward methods and the genera-\ntive strength of SD for detail refinement, producing results\nwith accurate structure and realistic textures. To enhance\nconsistency between geometry and appearance, we intro-\nduce a unified training framework that jointly optimizes ge-\nometric representation learning and image generation using\nfeatures rendered from 3D Gaussians. This tight integration\nensures more effective use of geometric features in guiding\nhigh-quality image synthesis. Our framework also paves the\nway for future work on simplifying parts of the SD architec-\nture, aiming for tighter feature coupling and more efficient\nview synthesis. Limitation. The proposed method does not\nexplicitly model dynamic objects, which limits its applica-\nbility in real-world scenarios involving dynamic scenes.\nAcknowledgements\nThis\nwork\nwas\npartially\nsupported\nby\nthe\nNational\nKey Research and Development Program of China (No.\n2023YFF0905104) and NSF of China (No. 62441222).\nReferences\nAgarwal, N.; Ali, A.; Bala, M.; Balaji, Y.; Barker, E.; Cai, T.;\nChattopadhyay, P.; Chen, Y.; Cui, Y.; Ding, Y.; et al. 2025.\nCosmos world foundation model platform for physical ai.\narXiv preprint arXiv:2501.03575.\nB\u00a8osiger, L.; Dusmanu, M.; Pollefeys, M.; and Bauer, Z.\n2024.\nMaRINeR: Enhancing Novel Views by Matching\nRendered Images with Nearby References.\nIn European\nConference on Computer Vision, 76\u201394. Springer.\nCharatan, D.; Li, S. L.; Tagliasacchi, A.; and Sitzmann, V.\n2024. pixelsplat: 3d gaussian splats from image pairs for\nscalable generalizable 3d reconstruction. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 19457\u201319467.\nChen, D.; Li, H.; Ye, W.; Wang, Y.; Xie, W.; Zhai, S.; Wang,\nN.; Liu, H.; Bao, H.; and Zhang, G. 2024a. PGSR: Planar-\nbased Gaussian Splatting for Efficient and High-Fidelity\nSurface Reconstruction. arXiv: 2406.06521.\nChen, Y.; Zheng, C.; Xu, H.; Zhuang, B.; Vedaldi, A.; Cham,\nT.-J.; and Cai, J. 2024b.\nMvsplat360: Feed-forward 360\nscene synthesis from sparse views. Advances in Neural In-\nformation Processing Systems, 37: 107064\u2013107086.\nDong, Y.; Li, Y.; Huang, Z.; Bian, W.; Liu, J.; Bao, H.; Cui,\nZ.; Li, H.; and Zhang, G. 2024. A Global Depth-Range-Free\nMulti-View Stereo Transformer Network with Pose Embed-\nding. arXiv: 2411.01893.\nDong, Y.; Yan, D.; Li, T.; Xia, M.; and Shi, C. 2022. Pedes-\ntrian gait information aided visual inertial SLAM for in-\ndoor positioning using handheld smartphones. IEEE Sensors\nJournal, 22(20): 19845\u201319857.\nHuang, B.; Yu, Z.; Chen, A.; Geiger, A.; and Gao, S. 2024a.\n2D gaussian splatting for geometrically accurate radiance\nfields. In ACM SIGGRAPH 2024 conference papers, 1\u201311.\nHuang, Z.; He, Y.; Yu, J.; Zhang, F.; Si, C.; Jiang, Y.;\nZhang, Y.; Wu, T.; Jin, Q.; Chanpaisit, N.; Wang, Y.;\nChen, X.; Wang, L.; Lin, D.; Qiao, Y.; and Liu, Z. 2024b.\nVBench: Comprehensive Benchmark Suite for Video Gener-\native Models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition.\nHuang, Z.; Zhang, F.; Xu, X.; He, Y.; Yu, J.; Dong, Z.;\nMa, Q.; Chanpaisit, N.; Si, C.; Jiang, Y.; Wang, Y.; Chen,\nX.; Chen, Y.-C.; Wang, L.; Lin, D.; Qiao, Y.; and Liu, Z.\n2024c."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThe paper presents a novel framework for high-fidelity novel view synthesis (NVS) from sparse images. The key contributions are:\n1. **Dual-Domain Detail Perception Module (DD-DPM):** This module enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details.\n2. **Feature-Guided One-Step Diffusion Architecture:** This architecture preserves high-frequency details during restoration processes, enhancing image quality.\n3. **Unified Training Strategy:** The framework allows joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module.\n\n#### Methodology\nThe method leverages a feed-forward pipeline with a single-step Stable Diffusion (SD) model to achieve accurate structure and realistic textures in novel view synthesis. Key steps include:\n1. **Geometry Transformer Backbone:** Utilizes pretrained Vision Transformers (ViTs) to acquire geometric information.\n2. **Detail-Aware Scene Reconstruction:** Introduces a lightweight Dual-Domain Detail Perception Module to handle high-resolution inputs efficiently, enhancing fine-grained detail perception from both spatial and frequency domains.\n3. **Boosting NVS with Diffusion Module:** Incorporates Gaussian features as auxiliary inputs within the UNet structure, facilitating inter-image information exchange through self-attention mechanisms.\n\n#### Key Findings\nExperiments demonstrate that:\n1. The proposed method achieves superior generation quality across multiple datasets compared to existing feed-forward methods.\n2. The unified training strategy enables joint optimization of geometric representation learning and image generation using features from 3D Gaussians, ensuring more effective use of geometric cues in guiding high-quality image synthesis.\n\n#### Conclusions\nThe paper introduces a novel framework that integrates a feed-forward pipeline with a single-step Stable Diffusion model to produce results with accurate structure and realistic textures. The unified training strategy ensures efficient use of geometric features for detailed refinement, leading to improved consistency between geometry and appearance. The proposed method outperforms existing methods in terms of perceptual quality and 3D consistency.\n\n### Summary\nThe paper presents a novel framework for high-fidelity novel view synthesis from sparse images using a feed-forward pipeline with a single-step Stable Diffusion model. Key contributions include the Dual-Domain Detail Perception Module, feature-guided diffusion architecture, and unified training strategy. The method achieves superior generation quality across multiple datasets and demonstrates improved consistency between geometry and appearance through joint optimization of geometric representation learning and image generation."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "VERIDAH: Solving Enumeration Anomaly Aware\nVertebra Labeling across Imaging Sequences\nHendrik M\u00f6ller1,2, Hanna Schoen1,5, Robert Graf1,2, Matan Atad1,2, Nathan\nMolinier3, Anjany Sekuboyina4, Bettina K. Budai7, Fabian Bamberg8, Steffen\nRinghof8, Christopher Schlett8, Tobias Pischon9, Thoralf Niendorf10, Josua A.\nDecker11, Marc-Andr\u00e9 Weber5, Bjoern Menze4, Daniel Rueckert2,6, and Jan S.\nKirschke1\n[1] Department for Interventional and Diagnostic Neuroradiology, TUM University\nHospital\n[2] Chair for AI in Healthcare and Medicine, Technical University of Munich (TUM)\n[3] NeuroPoly Lab, Institute of Biomedical Engineering, Polytechnique Montreal, and\nMila \u2013 Quebec AI Institute\n[4] Department of Quantitative Biomedicine, University of Zurich\n[5] Department of Diagnostic and Interventional Radiology, Pediatric Radiology and\nNeuroradiology, Rostock University Medical Center\n[6] Department of Computing, Imperial College London\n[7] Department of Diagnostic and Interventional Radiology, University Hospital\nHeidelberg\n[8] Department of Diagnostic and Interventional Radiology, Medical Center \u2013\nUniversity of Freiburg\n[9] Molecular Epidemiology Research Group, Max-Delbrueck-Center for Molecular\nMedicine in the Helmholtz Association (MDC)\n[10] Berlin Ultrahigh Field Facility (B.U.F.F.), Max Delbrueck Center for Molecular\nMedicine\n[11] Department of Diagnostic and Interventional Radiology, University Hospital\nAugsburg\nAbstract. The human spine commonly consists of seven cervical, twelve\nthoracic, and five lumbar vertebrae. However, enumeration anomalies\nmay result in individuals having eleven or thirteen thoracic vertebrae\nand four or six lumbar vertebrae. Although the identification of enumer-\nation anomalies has potential clinical implications for chronic back pain\nand operation planning, the thoracolumbar junction is often poorly as-\nsessed and rarely described in clinical reports. Additionally, even though\nmultiple deep-learning-based vertebra labeling algorithms exist, there is a\nlack of methods to automatically label enumeration anomalies. Our work\ncloses that gap by introducing \"Vertebra Identification with Anomaly\nHandling\" (VERIDAH), a novel vertebra labeling algorithm based on\nmultiple classification heads combined with a weighted vertebra sequence\nprediction algorithm. We show that our approach surpasses existing mod-\nels on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all verte-\nbrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26%\nof subjects with all vertebrae correctly labeled, p < 0.001) and works in\narbitrary field-of-view images. VERIDAH correctly labeled the presence\narXiv:2601.14066v1  [cs.CV]  20 Jan 2026\n2\nM\u00f6ller et al.\nof thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT\nimages, respectively, and lumbar enumeration anomalies in 94.48% and\n97.22% for T2w and CT, respectively. Our code and models are available\nat: https://github.com/Hendrik-code/spineps.\nKeywords: Computed tomography (CT), Machine learning, Magnetic\nresonance imaging (MRI), Neural network, Pattern recognition and clas-\nsification, Spine.\n1\nIntroduction\nThe benefits of automated processing and diagnosis of medical images have often\nbeen demonstrated [25,12,6]. For the analysis of the spine, these automated as-\nsessment systems usually require accurate identification of the vertebrae, such as\nfor fracture prediction [15]. The human spine typically consists of seven cervical,\ntwelve rib-bearing thoracic, and five lumbar vertebrae [28]. However, there can\nbe enumeration anomalies, causing a deviation of the number of thoracic and/or\nlumbar vertebrae by one [27,30]. These thoracic enumeration anomalies (TEA)\nand lumbar enumeration anomalies (LEA) are often accompanied by transitional\nanomalies, such as thoracolumbar transitional vertebrae (TLTV). These TLTV\nare vertebrae with hybrid morphological features from thoracic and lumbar ver-\ntebrae [19]. Studies have shown that the presence of enumeration anomalies is\nstrongly correlated with transitional ones and has potential clinical implications\nfor chronic back pain and degenerative changes [18,1]. Ignoring enumeration\nanomalies can also lead to vertebrae being labeled incorrectly. This can result in\nwrong-site surgery, where the intended vertebra is missed and adjacent vertebrae\nare operated on by mistake [27].\nA naive approach to identifying vertebrae is to count them in a cranial-to-\ncaudal or caudal-to-cranial order. This requires a known reference point within\nthe field of view (FOV), such as the first thoracic vertebra (identified by the\npresence of a rib) or the sacrum (to identify the last lumbar vertebra). If either\nis visible, the vertebrae can be counted and labeled accordingly. However, CT\nand MRI scans often have a limited FOV along the superior-inferior direction,\nmaking manual assessments inconsistent. Furthermore, anatomical variations,\nsuch as vertebral enumeration anomalies, render the counting approach error-\nprone and make both counting directions incompatible. For example, even if\nthe last lumbar vertebra is visible, simple counting cannot distinguish between\nspines with four, five, or six lumbar vertebrae.\nThe automatic identification of vertebrae has been the subject of dedicated\nresearch [14,20,9,23,26,21,4]. There have been many development efforts, espe-\ncially in CT [14,20,9]. However, some approaches are only tested on small cohorts\n[11], and almost none of them consider TEA or LEA, as the existing systems can\nonly predict a maximum of 12 thoracic and five lumbar vertebrae [31]. We found\nthree automated approaches that theoretically take this into account. Sekuboy-\nina et al. [24] developed a method for localization and identification of vertebrae\nin CT, using a conditional random field and user input to refine errors. Meng\nTitle Suppressed Due to Excessive Length\n3\net al. [16] utilized segmentation masks of vertebrae and a graph optimization\nproblem for vertebra identification. Finally, Warszawer et al. [29] locate specific\nintervertebral discs during the task of segmentation in order to identify the se-\nquence of vertebrae visible in the image by counting from those discs in both\ndirections. However, these methods either fail to label TEA and LEA cases re-\nliably and robustly in arbitrary FOVs or have never been validated on a large\ndataset.\nSeizing this opportunity, we use large cohorts of 3D MRI and CT images\nand establish a robust labeling method that can be used even in the presence\nof enumeration anomalies and works in arbitrary FOVs. We propose to use a\nclassifier with multiple classification heads to receive vertebra-label related out-\nputs. Then, we use those outputs jointly to solve a constrained optimization\nproblem in order to find the sequence of vertebra labels where the agreement of\nthe multiple vertebrae and classification outputs is highest.\nThe contributions of this study can be summarized as follows: (1) establish\na new multi-classification head strategy of training vertebra labeling classifiers,\n(2) introduce a label sequence predictor based on the classifier outputs, (3) show\nthe superiority of our method to the current state-of-the-art on a large cohort\nof data on the CT and T2w MRI sequence, (4) make our approach and model\nweights publicly available.\n2\nMaterial and Methods\n2.1\nData\nWe utilize two data cohorts, the external German National Cohort (NAKO) [2,3]\nand an in-house CT dataset. Informed consent was waived for this retrospective\nstudy (593/21 S-NP).\nFor the T2w sagittal MRI data, we utilized a random subset of 6291 sub-\njects of the German National Cohort (NAKO) and split the data roughly into\n70/15/15% train/validation/test (see Table 1). We utilized SPINEPS [17], a seg-\nmentation network, to derive instance segmentation annotations for all vertebrae\nin our T2w sagittal MRI data. We excluded 33 subjects (0.52%) where this seg-\nmentation failed. We did not observe obvious reasons for these failures, such as\nhyperintense spots or pathologies. The segmentations yielded localizations for\neach vertebra.\nThe T2w sagittal images of the NAKO always show the whole spine split\ninto three segments and thus contain up to 26 fully visible vertebrae (see Figure\n1). Using the open-source toolbox TPTBox1, we stitched the different segments\ninto one image[7]. If we now label the vertebrae in cranial-to-caudal order, we\nwould already be correct except for TEA and LEA cases.\nFor CT, we utilized an in-house cohort consisting of images from various\nscanners. Some CT images are contrast-enhanced. The data include segmenta-\ntion annotations for the vertebrae, supervised by an expert with 24 years of\n1 https://github.com/Hendrik-code/TPTBox\n4\nM\u00f6ller et al.\nTable 1: Demographics for the whole datasets and individual train and test splits.\nAbbreviations: yrs: years, kg: kilogram, m: meters, M.: Mean, SD: standard devi-\nation, TEA: Thoracolumbar Enumeration Anomaly, LEA: Lumbar Enumeration\nAnomaly.\nNAKO T2w sagittal Summary\nTrain\nTest\nNumber of Scans\n6291\n5291\n1000\nAge Range (yrs)\n20 \u2013 72\n20 \u2013 72\n22 \u2013 72\nM. Age (yrs) \u00b1 SD\n52 \u00b1 11\n52 \u00b1 12\n52 \u00b1 11\nM. Weight (kg) \u00b1 SD\n80 \u00b1 16\n80 \u00b1 16\n80 \u00b1 16\nM. Height (m) \u00b1 SD\n1.7 \u00b1 0.1\n1.7 \u00b1 0.1\n1.7 \u00b1 0.1\nSex (% female)\n52.4\n52.4\n52.1\nTEA Rate (%)\n5.8\n5.4\n8.2\nLEA Rate (%)\n9.7\n8.8\n14.6\nM. Visible Vertebrae \u00b1 SD 23.1 \u00b1 0.3 23.1 \u00b1 0.3 23.1 \u00b1 0.4\nCT Cohort Summary\nTrain\nTest\nNumber of Scans\n1536\n1171\n365\nAge Range (yrs)\n18 \u2013 100\n18 \u2013 100\n20 \u2013 100\nM. Age (yrs) \u00b1 SD\n74 \u00b1 21\n74 \u00b1 21\n74 \u00b1 21\nM. Weight (kg) \u00b1 SD\n76 \u00b1 16\n75 \u00b1 14\n81 \u00b1 21\nM. Height (m) \u00b1 SD\nN/A\nN/A\nN/A\nSex (% female)\n49.1\n50.9\n44.3\nTEA Rate (%)\n5.5\n5.1\n6.8\nLEA Rate (%)\n7.3\n7.1\n7.9\nM. Visible Vertebrae \u00b1 SD 16.7 \u00b1 3.6 16.7 \u00b1 3.5 16.8 \u00b1 4.0\nTitle Suppressed Due to Excessive Length\n5\nA\nB\nC\nD\nFig. 1: Example data samples from our CT (A, B) and MRI (C, D) dataset. For\neach sample, it shows a sagittal and coronal projection along the spine as well as\nthe corpus centroids with vertebra labels associated with each. While the CT im-\nages contain various field-of-views, the NAKO T2w sagittal MRI images always\ncover the whole spine. A) spine with eleven thoracic vertebrae, B) normal spine\nwith twelve thoracic and five lumbar vertebrae, C) has an additional thoracic\nvertebra, and D), a subject with only eleven thoracic vertebrae but six lumbar\nones.\nexperience. In contrast to the T2w MRI of the NAKO, the FOV of the CTs\nvaried heavily, containing a range of 8 \u2013 25 fully visible vertebrae (average 16.7\n\u00b1 3.6 vertebrae). We utilized 1536 images in which the first thoracic vertebra is\npresent (see Figure 1). This is necessary to manually assess TEA and create the\nreference labels.\nIn both MRI and CT, we consciously split in a way so that the test set\ncontains more TEA and LEA to better evaluate the models.\nWhen manually rating the thoracolumbar junction, we adapted the catego-\nrization of Park et al. [19]. We defined a thoracic vertebra as one having a true\nrib, regardless of the rib\u2019s length. If there is only a unilateral or bilateral acces-\nsory ossification centre (comparable to Type 4 as defined in [19]), we determined\nthis to be a lumbar vertebra. In ambiguous cases, we judged by the morphology\n(e.g. facet orientation) whether a vertebra is thoracic.\nFor the lumbosacral transition, we adapted the analysis of Konin et al. [13].\nIn the presence of a lumbosacral transitional Castellvi anomaly, we defined the\nvertebra with this anomaly as a sacralized lumbar vertebra regardless of the\nCastellvi type. The segmentation model SPINEPS we used on the NAKO data\nwas already trained to make that distinction, which we only had to manually\n6\nM\u00f6ller et al.\nClassification\nModel\nOutputs\nVertebra\u00a0(C3, T1, ...)\nRegion\u00a0(C, T, L)\nTransition (last cervical, first\nthoracic, ...)\nWeighting\nVisibility (yes / no)\nMinimize\nCost\nPath Cost\nFunction\nPredicted\nLabel\nSequence\ne.g.:\nC6,\nC7,\nT1,\nT2,\n...\nInput Image\n+\nlocations for\nvertebrae\nto\nCrop for\neach\nvertebra\nFig. 2: Overview of the VERIDAH approach. We take an input image and the\nvertebra localizations to crop around each vertebra. We then input these 3D\ncropped images separately into our classification model. The classification model\nhas four output heads. We weigh their predictions based on our calibration and\ncombine them into a cost matrix. Finally, an algorithm finds the optimal label\nsequence path that minimizes this cost, making our final sequence prediction.\ncorrect in 16 cases (0.25%). Similarly, the annotations of the CT data already\ndistinguished between the lumbar and sacral vertebrae.\nOne expert with 3 years of experience performed the manual annotations for\nall data.\n2.2\nVERIDAH\nOur proposed approach consists of two main parts, a classification model and\na sequence predictor. The sequence predictor is a constraint optimization algo-\nrithm that guarantees a logical sequence of vertebra labels. The input variables\nfor this algorithm are derived per individual vertebra, which are obtained using\na classification model.\nData Preprocessing For training and inference, our approach requires the\nlocation of each vertebra in a given image. Since we already have instance seg-\nmentation annotations for our data, we utilize those to compute the center of\nmass locations for each vertebra. With those locations, we calculate crops for\neach vertebra, where the center of mass location becomes the center. We keep\nneighboring vertebrae visible to give the classification model context.\nWe reorient these cutouts consistently and align the resolution depending on\nwhether it is T2w MRI or CT (see Table 2).\nClassification Model We trained a 3D DenseNet169 [8] on individual vertebra\ncutouts. With a typical one-classification head approach, one would train it to\noutput the vertebra label (i.e., C3, T5, . . . ). In contrast, VERIDAH utilizes four\ndifferent classification heads. (1) Target vertebra label (such as C3, T5, . . . )\nwith 23 classes; (2) target region with the three classes cervical, thoracic, and\nlumbar; (3) and target vertebra transition with six classes (is the vertebra the last\ncervical, first thoracic, last thoracic, first lumbar, last lumbar, or none above);\n(4) target visibility (if the vertebra is fully visible or not) as one class output.\nTitle Suppressed Due to Excessive Length\n7\nWe replace the original classification head with the four prediction heads. Each\nhas an additional fully connected linear layer and a ReLU activation function,\nfollowed by the output linear layer and the softmax function.\nSince especially T13 cases are rare, training directly on class labels like T13\nwould create a huge class imbalance problem for the classifier. To circumvent this,\nwe replace the T13 and L6 labels in the reference annotation with T12 and L5,\nrespectively. Thus, the existence of a T13 and L6 is indicated by two consecutive\nT12 or L5 labels. This means that during training, the label difference between\na T12 and a T13 is only present to the model through the vertebra transition\noutput head. The T13 is labeled as the last thoracic vertebra; the T12 in a case\nwith 13 thoracic vertebrae is not.\nAll the vertebrae in our datasets are sufficiently visible to train on. Thus,\nwe introduce the visibility label during training as data augmentation (denoted\nRandCut). We randomly crop a part of the image, and if the vertebra segmen-\ntation is not fully visible anymore, we set the visibility reference label to zero,\none otherwise.\nWe used Pytorch v2.04.1 [10] with Pytorch Lightning v2.0.8 [22] and the\nMONAI libraries v1.3.0 [5].\nTo account for the rare occurrence of these TEA and LEA anomalies, we\nsample each anomalous case in the training set four times during each epoch\ninstead of only once. For the hyperparameters employed, see Table 2.\nAfter classifying a sequence of vertebrae, the output is further processed. We\nadd a Gaussian filter on each softmax output per vertebra and then normalize\nthe vector by dividing it by its length. For the vertebra transition output, we\nnormalize over the different classes (i.e., the more vertebrae the model predicts\nto be the last thoracic one, the lower the values for each of those predictions).\nLet V = {V1, V2, . . . , Vn} be the sequence of n vertebra instances in an image,\nspatially ordered from top to bottom. C = {C1, C2, . . . , C7, T1 . . . T12, L1, . . . L5}\ndefines all 24 vertebra labels (without T13 and L6) with Ci corresponding to\nthe i-th entry in C. We define the set of possible vertebra transitions label\nT = {None, Last Cervical, First Thoracic, Last Thoracic,\nFirst Lumbar, Last Lumbar}.\nWith i corresponding to the i-th vertebra instance Vi, j as the index for the\nvertebra label Cj, and k as the index for the vertebra transition set Tk, then the\nclassifier outputs normalized vertebra class softmax scores cij, normalized region\nprediction scores rij (where each region output is mapped onto each vertebra\nclass of that region, e.g. the cervical region output is applied to C1 through C7),\nnormalized vertebra transition scores tik, and a normalized visibility softmax\nscore si \u2208[0, 1].\nSequence Predictor To get the final vertebra labeling sequence for any field of\nview, we incorporate global constraints similar to Meng et al. [16]. For that, we\ncombine the outputs of our classification model into a cost function. The intu-\nition is to find the sequence with the maximum agreement between the different\nclassification outputs across the vertebrae. For instance, one classification head\n8\nM\u00f6ller et al.\nTable 2: Training Hyperparameters. Different setups for the sequences are writ-\nten with slashes (T2w sag / CT)\nParameter\nValue\nData\nNumber of 3D scans\n5291 / 1171\nOrientation\n(Posterior, Inferior, Right)\nResolution (mm)\n(0.85, 0.85, 3.3) / (1.0, 1.0, 1.0)\nCrop Size (voxel count)\n(200, 160, 32) / (128, 128, 128)\nTraining Setup\nArchitecture\nDenseNet 169\nBatch Size\n32\nMax Epochs\n100\nLR\n1e-4\nLR End Factor\n1e-2\nL2 Regularization\n1e-3\nDropout\n0.1\nData Augmentation\nRandCut\nprob=0.20\nRandAffine prob=0.40, translate=8, scale=0.15, degrees=15\u00b0\nRandScaleIntensity\nprob=0.15, offset=0.15\nRandShiftIntensity\nprob=0.15, factor=0.15\nRandGaussianSmooth\nprob=0.10, sigma=0.60\nRandGaussianNoise\nprob=0.10, std=10.0\nRandAdjustContrast\nprob=0.10, gamma=1.0\nRandLowResolution\nprob=0.10, range=(0.8, 1.0)\nTitle Suppressed Due to Excessive Length\n9\nmight indicate a T12, while the region output predicted the vertebra belonging\nto the lumbar region.\nWith wC, wR, wT being weights for the corresponding classifier output types\ncij, rij, and tik, we calculate the cost for each instance Vi and class Cj with\nlabelcost(i, j) = (cij \u00b7 wC) + (rij \u00b7 wR)\n(1)\nThis way, we can define a labelcost matrix Ln\u00d724 where Lij = labelcost(i, j).\nThe vertebra transition cost is utilized differently since it depends on the\nrest of the path. Given a candidate path p as a sequence of vertebra class index\nlabels [j0, j1, . . . , jn] with pi \u2208C being the i-th entry of p, we define the vertebra\ntransition cost for an individual vertebra Vi as:\ntranscondition(p, i, k) =\n(\ntik\nif Tk fullfilled in p\n0,\notherwise\n(2)\ntranscost(p, i) = wT \u00b7\nX\nk\u2208T\ntranscondition(p, i, k)\n(3)\nThe classifier predicts the visibility si, which we utilize to weigh each verte-\nbra. The less visible the model predicts the vertebra to be, the less it impacts\nthe sequence predictor. Additionally, by applying a Gaussian filter, we auto-\nmatically weigh the boundary vertebrae less. We do this since the classifier has\nless surrounding information when looking at a boundary vertebra or when the\nvertebra itself is not fully visible, and we observed less accurate individual pre-\ndictions there. Thus, we can compute the cost of a whole path by using the\nvisibility output and accumulating the individual costs with\npathcost(p) = \u2212\nn\nX\ni\nsi \u00b7 (transcost(p, i) + Li,pi)\n(4)\nThus, for all p \u2208P with P being the set of all possible paths, the sequence\npredictor solves the minimization problem and finds the solution path X by\nX = arg min\np\u2208P\npathcost(p)\n(5)\nIn order to do so, we utilize dynamic programming and recursively run over all\npossible path options and compute the cost. However, we can enforce constraints\nby setting them at an infinite cost. Our constraints, regardless of FOV, are as\nfollows:\n\u2013 The labels must be consecutive (e.g, T5 must be followed by T6). To account\nfor T11 cases, we allow an L1 directly after a T11.\n\u2013 The labels must be in spatial order (not T6 and then T5).\n\u2013 There can only be a maximum of seven cervical vertebrae, 11 to 13 thoracic\nvertebrae, and only four to six lumbar vertebrae.\n\u2013 We allow a T12 followed by a T12 exactly once to account for T13, and do\nthe same with two consecutive L5s for L6 cases.\n10\nM\u00f6ller et al.\nIn contrast to Meng et al. [16], we do not add artificial costs to the paths con-\ntaining a TEA or LEA.\nOnce we have computed X, all we need to do is replace the class indices pi\nwith the actual class labels Cpi, then relabel occurrences of two consecutive T12s\nto [T12, T13] and two consecutive L5s to [L5, L6] to receive the final result.\nVertebra Label Cost Matrix\n0.9\n0.2\n0.1\n0.8\n0.3\n0.7\n0.6\n0.4\nT11\nT12\nL1\nL2\nVert Trans.\nNothing\n1.0\nLast\nThoracic\n0.5\nLast\nThoracic\n0.5\nFirst\nLumbar\n1.0\nVisible\n1\n1\n0.9\n0.1\nInstances\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nFig. 3: Example of our sequence predictor on an artificially created sample con-\ntaining n = 4 vertebrae (V1 \u2013 V4) in the FOV. The classifier outputs specific\nvertebra class predictions cij and region prediction scores rij for each class (al-\nready combined into the label cost matrix L and reduced to T11 to L2 for sim-\nplicity), their visibility si, and the vertebra transitions output tik. Depending on\nthe utilized classification head, either the path [T11, T12, L1, L2] (red arrows)\nor [T11, T12, T12, L1] (blue arrows) is returned by the sequence predictor. In\nthe case of the latter one, the post-processing relabels the path [T11, T12, T12,\nL1] to [T11, T12, T13, L1], making this a TEA case. All unspecified classifier\noutputs have a value of zero.\nTo demonstrate our approach, consider the example as shown in 3. For sim-\nplicity, we have already calculated the label cost matrix L. Using only the la-\nbel cost (i.e. putting the transcost tik to zero and the visibility si always to\none), the red path of [T11, T12, L1, L2] would be the final sequence, as the\nred path cost is \u2212(0.9 + 0.8 + 0.7 + 0.4) = \u22122.8 compared to the cost of\n\u2212(0.9 + 0.8 + 0.3 + 0.6) = \u22122.6 for the blue path. If we consider the verte-\nbra transitions classifier output (Vert Trans.) as well, we need to add 1.0 to\nany path where V4 is the L1, and 0.5 to any path where V2 or V3 is the last\nthoracic one (see Equation 3). The cost of the red and blue path becomes \u22123.3\nand \u22124.1, respectively. The blue path is the final predicted label sequence. If"
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n#### Main Contributions\nThe paper introduces VERIDAH, a novel vertebra labeling algorithm designed to handle enumeration anomalies (TEA and LEA) across imaging sequences such as T2-weighted sagittal MRI and CT scans. The main contributions are:\n1. **New Multi-Classification Head Strategy**: Establishing a new multi-classification head strategy for training vertebra labeling classifiers.\n2. **Label Sequence Predictor**: Introducing a label sequence predictor based on classifier outputs to find the optimal sequence of labels where multiple vertebral classifications agree the most.\n3. **Superior Performance**: Demonstrating VERIDAH's superiority over existing models in CT and T2w MRI sequences, achieving 98.30% correct labeling for all vertebral segments with TEA cases and 99.18% for all vertebrae without any anomalies.\n4. **Robustness Across FOVs**: Ensuring the approach works effectively even in arbitrary field-of-view images.\n\n#### Methodology\nThe methodology involves:\n- **Data Collection**: Utilizing two data cohorts: an external German National Cohort (NAKO) and an in-house CT dataset. Data preprocessing includes instance segmentation annotations for vertebrae.\n- **Classification Model**: Training a 3D DenseNet169 model with four classification heads to predict the target vertebra label, region, transition, and visibility of each vertebral instance.\n- **Sequence Predictor**: Combining outputs from the classification model into a cost function that incorporates global constraints like spatial order and maximum number of cervical, thoracic, and lumbar vertebrae. The sequence predictor uses dynamic programming to find the optimal sequence with minimal total cost.\n\n#### Key Findings\nThe key findings are:\n- **Performance on Data**: VERIDAH outperforms existing models by achieving 98.30% correct labeling for all vertebral segments in NAKO T2w sagittal MRI data and 99.18% for CT scans.\n- **Handling Anomalies**: The model correctly labels the presence of TEA (87.80%) and LEA (94.48%) cases, demonstrating robustness across different imaging sequences.\n- **Generalization to Arbitrary FOVs**: VERIDAH works effectively in arbitrary field-of-view images, indicating its versatility.\n\n#### Conclusions\nVERIDAH represents a significant advancement in the automated processing of medical images for spine analysis, particularly in handling enumeration anomalies. Its multi-classification heads and label sequence predictor strategy ensure high accuracy while maintaining robustness across different imaging sequences and FOVs. The availability of the code and models at https://github.com/Hendrik-code/spineps makes it a valuable resource for further research and clinical applications.\n\n### Conclusion\nVERIDAH addresses critical challenges in spine analysis by effectively identifying vertebrae, especially those with enumeration anomalies, using advanced machine learning techniques. Its robust performance across various imaging sequences underscores its potential for improving diagnostic accuracy and operational efficiency in medical settings."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "LifeAgentBench: A Multi-dimensional Benchmark and Agent for\nPersonal Health Assistants in Digital Health\nYe Tian1\u2217, Zihao Wang1\u2217, Onat Gungor1, Xiaoran Fan2 and Tajana Rosing1\n1University of California San Diego, La Jolla, CA, USA\n2 Google Research, Mountain View, CA, USA\n{yet002,ziw140,ogungor,tajana}@ucsd.edu\nvanxf@google.com\nAbstract\nPersonalized digital health support requires long-\nhorizon, cross-dimensional reasoning over hetero-\ngeneous lifestyle signals, and recent advances in\nmobile sensing and large language models (LLMs)\nmake such support increasingly feasible.\nHow-\never, the capabilities of current LLMs in this set-\nting remain unclear due to the lack of system-\natic benchmarks.\nIn this paper, we introduce\nLifeAgentBench, a large-scale QA benchmark for\nlong-horizon, cross-dimensional, and multi-user\nlifestyle health reasoning, containing 22,573 ques-\ntions spanning from basic retrieval to complex rea-\nsoning. We release an extensible benchmark con-\nstruction pipeline and a standardized evaluation\nprotocol to enable reliable and scalable assessment\nof LLM-based health assistants. We then system-\natically evaluate 11 leading LLMs on LifeAgent-\nBench and identify key bottlenecks in long-horizon\naggregation and cross-dimensional reasoning. Mo-\ntivated by these findings, we propose LifeAgent as\na strong baseline agent for health assistant that inte-\ngrates multi-step evidence retrieval with determin-\nistic aggregation, achieving significant improve-\nments compared with two widely used baselines.\nCase studies further demonstrate its potential in re-\nalistic daily-life scenarios. The benchmark is pub-\nlicly available.1\n1\nIntroduction\nThe World Health Organization reports that noncommunica-\nble diseases (NCDs), including heart disease, cancer, and di-\nabetes, account for a substantial fraction of global mortal-\nity [World Health Organization, 2023].\nCrucially, insuffi-\ncient physical activity, inadequate sleep, unhealthy diets, and\nchronic psychological stress are well-established and mod-\nifiable risk factors for their onset and progression [World\nHealth Organization, 2023; Vaccarino et al., 2025].\nPer-\nsonalized digital health therefore aims to transform everyday\n*Ye Tian and Zihao Wang are equal contribution.\n1https://anonymous.4open.science/r/LifeAgentBench-CE7B\nDiet\nActivities\nSleep\nEmotion\n\u2026\nCross-dimension\nSingle-user\nCross-dimension\nMulti-user\nNC\nTA\nHealth\nLifeAgent\nHow was my overall condition\nthis week, and are there any\nissues I should be aware of?\nPlease also give me some\nsuggestions.\nUser Query\nAction\nThought\nObservation\nMost indicators were stable this week, but\nsleep was irregular, with two nights under 5\nhours. These days also show low activity and\ndelayed dinner times. Suggestions: keep a\nconsistent bedtime, avoid late meals, and add\naerobic exercise.\nFigure 1:\nLifeAgentBench, a QA benchmark for cross-\ndomain, long-horizon lifestyle health reasoning, together\nwith LifeAgent, a health assistant baseline.\nlifestyle signals into timely, individualized support that re-\nduces preventable disease burden and promotes healthier liv-\ning. Recent advances in wearables and digital health applica-\ntions have made continuous, fine-grained monitoring increas-\ningly accessible [Jamieson et al., 2025], capturing diverse\ndaily-life signals such as physical activity, sleep patterns, di-\netary intake, and stress-related biomarkers [Apple Inc., 2025;\nGoogle LLC, 2025; Zareeia et al., 2025; McDuff et al.,\n2025]. These longitudinal, multi-dimensional records pro-\nvide a foundation for proactive health support, but they also\nraise new challenges in long-horizon, cross-dimensional rea-\nsoning.\nA key challenge is to translate heterogeneous raw lifelog\nsignals into reliable, user-friendly natural-language support\nthat answers user-facing queries. In practice, users expect\nactionable health insights distilled from raw signals such\nas heart-rate fluctuations or sleep-stage sequences, and they\nwant to interact with the system via intuitive natural lan-\nguage. For example, \u201cHow has my sleep quality been over\nthe past week?\u201d or \u201cWhen I increase aerobic exercise, does\nmy deep sleep also increase?\u201d Answering these queries re-\nquires the system to reason over multi-dimensional lifestyle\nsignals across long time spans, including aggregating mea-\nsurements, comparing periods or conditions, and connecting\nmultiple dimensions with temporal dynamics. For instance, a\nsingle night of reduced sleep may be negligible in isolation,\nbut combined with rising stress and an irregular diet, it can in-\ndicate risks that single-dimensional analysis would miss. Ex-\nisting machine learning models excel at narrow tasks such as\nclassification and prediction, yet they offer very limited sup-\narXiv:2601.13880v1  [cs.AI]  20 Jan 2026\nport for holistic, user-facing reasoning and explanation over\nmulti-domain lifestyle factors [Abdelaal et al., 2024].\nLarge language models (LLMs) have demonstrated im-\npressive performance across diverse domains, offering new\nopportunities for digital health systems to generate natural-\nlanguage, personalized analysis and feedback [Bedi et al.,\n2025]. However, their ability to perform long-horizon, cross-\ndimensional reasoning over personal lifestyle records re-\nmains largely unexplored.\nProgress in this direction re-\nquires systematic evaluation on a robust question-answering\n(QA) benchmark with verifiable answers grounded in real-\nworld lifelog data. In recent years, several health-related QA\ndatasets have been introduced, including nutritional decision\nmaking (NGQA [Zhang et al., 2024]), activity analysis (Sen-\nsorQA [Reichman et al., 2025]), sleep health (SleepQA [Bo-\njic et al., 2022]), emotional support (MentalChat16K [Xu et\nal., 2025]), and lifelog analysis (OpenLifelogQA [Tran et\nal., 2025]). In parallel, recent studies have begun to exam-\nine LLMs for specific health and lifestyle analyses, such as\nsleep assessment [Khasentino et al., 2025], activity predic-\ntion [Yu et al., 2025], daily logs generation [Tian et al., 2025],\nand emotion analysis [Xu et al., 2024]. Nevertheless, exist-\ning benchmarks and evaluations often focus on a single do-\nmain or short time windows and are unable to capture interac-\ntions across lifestyle dimensions or support integrated long-\nhorizon analysis over heterogeneous signals. As a result, a\nunified benchmark and protocol for systematically evaluating\nLLMs under long-horizon, cross-dimensional lifestyle rea-\nsoning remains missing.\nTo bridge this gap, we propose LifeAgentBench, a\nhealth reasoning QA benchmark constructed from multi-\ndimensional lifestyle records, as shown in Figure 1. It con-\ntains 22,573 questions covering tasks from basic retrieval\nto long-horizon, cross-dimensional, and multi-user reason-\ning. We develop an extensible generation pipeline that aligns\nlifelog records into a relational database, instantiates fine-\ngrained questions from predefined task templates, and derives\nexecutable SQL queries to obtain verifiable ground-truth an-\nswers. Using this benchmark, we evaluate 11 leading LLMs\nunder two common evaluation settings and analyze their\nperformance and failure modes: Context Prompting [Lee\net al., 2024] and Database-augmented Prompting [Zhu et\nal., 2024].\nThe results reveal major bottlenecks in long-\nhorizon aggregation and cross-dimensional, multi-user rea-\nsoning, suggesting that tool-augmented grounding and deter-\nministic aggregation are critical for reliable health reasoning.\nMotivated by these findings, we design LifeAgent as a strong\nbaseline health assistant.\nLifeAgent decomposes complex\nqueries, performs multi-step evidence retrieval, and applies\ndeterministic computations to handle aggregation-intensive\nreasoning over long time horizons. Experiments show that\nLifeAgent substantially improves performance on the most\nchallenging cross-dimensional and multi-user questions, and\na realistic case study further demonstrates its potential as a\npractical health assistant in everyday settings. In summary,\nour main contributions are as follows:\n\u2022 We introduce LifeAgentBench, a large-scale benchmark\nfor long-horizon and cross-dimensional health reason-\ning over heterogeneous lifestyle records, with verifiable\nground-truth answers.\n\u2022 We release an extensible benchmark construction\npipeline and standardized evaluation protocol, enabling\nfuture research to expand to new data sources and tasks.\n\u2022 We conduct systematic evaluations of leading LLMs and\nprovide comprehensive analyses that reveal key bottle-\nnecks of existing LLMs in cross-dimensional and long-\nhorizon health reasoning.\n\u2022 We propose LifeAgent as a strong agent that achieves\nsignificant improvements over baselines, and a case\nstudy further demonstrates its practical value for digital\nhealth assistants.\n2\nRelated Work\nLifestyle Datasets for Health Analysis. Lifestyle datasets\nprovide the foundation for monitoring daily behaviors and\nstudying health-related patterns.\nPrior resources typically\nfall into short-span or single-focus recordings, such as\nMMASH [Rossi et al., 2020] for sleep and psychological\nanalysis, WESAD [Philip Schmidt et al., 2018] for stress\nand affective states in controlled settings, and CAPTURE-\n24 [Doherty et al., 2017] for large-scale activity recog-\nnition with sleep diaries.\nMore recent lifelogging efforts\nextend to longer-term and more diverse sensing, includ-\ning LifeSnaps [Yfantidou et al., 2022], GLOBEM [Xu et\nal., 2022], and ETRI Lifelog [Oh et al., 2025].\nAmong\nthem, AI4FoodDB [Romero-Tapiador et al., 2023; Lacruz-\nPleguezuelos et al., 2025] is notable for its comprehensive\ndesign, collecting one month of multimodal lifestyle and clin-\nical data from 100 participants and covering key health di-\nmensions such as nutrition, activity, sleep, and emotion. Its\nbreadth supports cross-dimensional analysis and long-term\nhealth trajectory modeling. Despite these advances in data\ncollection, existing resources remain primarily raw lifelog\nrecords and lack task definitions and evaluation protocols\nfor health reasoning. Hence, they cannot systematically as-\nsess whether models can reason over heterogeneous records\nwithin long time windows and produce actionable health in-\nsights. Therefore, building on a comprehensive source dataset\n(AI4FoodDB), we construct a QA benchmark and provide\nstandardized task formulations and evaluation criteria for\nlong-horizon, cross-dimensional health reasoning.\nHealth Reasoning QA Benchmarks.\nRecent studies\nhave proposed some QA benchmarks for personal health and\nlifestyle reasoning, as summarized in Table 1. However, most\nof them focus on a single dimension or a very limited set of\naspects. For example, SleepQA [Bojic et al., 2022] focuses\non sleep guidance, MentalChat16K [Xu et al., 2025] targets\nemotional well-being conversations, and NGQA [Zhang et\nal., 2024] emphasizes dietary decision-making and nutrition\nreasoning. SensorQA [Reichman et al., 2025] evaluates an-\nswering questions from raw sensor signals, while OpenLifel-\nogQA [Tran et al., 2025] derives questions from personal\nlifelogs. However, despite these advances, existing bench-\nmarks remain insufficient to evaluate the capability of mod-\nels for complex health reasoning grounded in personal data.\nSpecifically, these benchmarks are constrained to single-user\nMethod\nTask\nScale\nCovered Dimensions\nMulti-user\nCross-dimension\nNGQA [Zhang et al., 2024]\nNutrition reasoning\n13.8K\nNutrition Health\n\u2717\n\u2717\nSensorQA [Reichman et al., 2025]\nDaily-life QA\n5.6K\nActivity and Location\n\u2717\n\u2717\nSleepQA [Bojic et al., 2022]\nSleep guidance\n7K\nSleep data\n\u2717\n\u2717\nMentalChat16K [Xu et al., 2025]\nMental health dialogue\n16.1K\nEmotion / Mental health\n\u2717\n\u2717\nOpenLifelogQA [Tran et al., 2025]\nLifelog QA\n14.2K\nMulti-modal lifestyle\n\u2717\n\u2713\nLifeAgentBench (Ours)\nLifestyle health QA\n22.6K\nDiet, activity, sleep, and emotion data\n\u2713\n\u2713\nTable 1: Comparison of LifeAgentBench with existing health and lifestyle QA benchmarks.\nSingle-User\n13452\nMulti-User\n9121\nSleep\nActivity\nDiet\nEmotion\nSleep\nActivity\nDiet\nEmotion\n(a)\n(b)\nFQ\n972\nAS\n1387\nCQ\n3065\nNC\n2072\nTA\n2806\nSingle Number\nMulti-item Answer\nSingle Number\nMulti-item Answer\nSingle Number\nYes/No\nSingle Number\nShort Text\nYes/No\nSingle Number\nShort Text\n(c)\nFigure 2: Overview statistics of LifeAgentBench: (a) domain distribution for single-user and multi-user questions, (b) question\nword-frequency visualization, and (c) distributions of question types and answer formats.\nand single-domain settings, and thus fail to capture cross-\ndomain interactions that are central to lifestyle health anal-\nysis. In addition, support for long-horizon reasoning is lim-\nited, overlooking the need to aggregate and relate interact-\ning factors over long time horizons.\nTherefore, we con-\nstruct LifeAgentBench as the first large-scale benchmark that\nsupports long-horizon queries over heterogeneous lifestyle\nrecords, together with cross-dimensional and multi-user rea-\nsoning about health states. It provides a critical test bench-\nmark for advancing the research and development of digital\nhealth assistants.\n3\nDataset\n3.1\nDataset Overview\nWe construct LifeAgentBench based on AI4FoodDB, a com-\nprehensive longitudinal lifestyle dataset encompassing 100\nparticipants [Romero-Tapiador et al., 2023].\nTo facilitate\ncomprehensive health reasoning, we integrate data from four\nlifestyle domains, including diet, sleep, physical activity, and\nemotion, derived from self-reports, wearable sensors, and\nmedical device records.\nWe focus on these four domains\nbecause they are widely recognized modifiable lifestyle fac-\ntors for chronic disease risk and are routinely recorded in\ndaily-life logs. All records are aligned by anonymized user\nIDs and timestamps to support executable queries and verifi-\nable answers. Collectively, LifeAgentBench provides 22,573\nquestions, including 13,452 single-user and 9,121 multi-user\nqueries, serving as a comprehensive benchmark to evaluate\nand facilitate the design of personal health assistants in digi-\ntal health.\n3.2\nQA Task\nEffective digital health analysis requires moving beyond low-\nlevel fact retrieval to long-horizon, cross-dimensional reason-\ning. To reflect this practical requirement and to support the\ndesign and evaluation of agent baselines, LifeAgentBench or-\nganizes questions into five task categories, with the distri-\nbution shown in Figure 2. (1) Fact Query focuses on re-\ntrieving atomic facts at specific times and serves as the foun-\ndation for reconstructing individual behavioral trajectories.\n(2) Aggregated Statistics extends retrieval to longitudinal\nwindows and requires statistical aggregation to characterize\nlong-term behavioral patterns. (3) Numeric comparison re-\nquires computing relative differences across time periods,\nlifestyle domains, or individuals, revealing contrasts in be-\nhavior. (4) Conditional Query introduces conditions such\nas personalized thresholds or cohort-level statistics to iden-\ntify anomalies and potential risks. (5) Trend Analysis cap-\ntures dynamic changes over time, aiming to uncover emerg-\ning concerns or signs of continuous improvement. These cat-\negories jointly emphasize multi-dimensional lifestyle reason-\ning and connect behavioral records to health-related insights.\nAnswers take diverse forms, including categorical responses\n(Yes/No), scalar numerical values, short text spans, pairwise\noutputs (two items), and list outputs (three or more items).\n3.3\nDesign Principles\nLifeAgentBench is designed to reflect how health assistants\nhelp people analyze their lifestyle patterns and health-related\nstatus, where questions usually involve the interaction be-\ntween diet, sleep, physical activity and emotions. We define a\ncompositional query space over structured life records so that\neach natural language question corresponds to an executable\nprogram with a verifiable answer. To support the develop-\nment and evaluation of agent baselines for personal health as-\nsistants in digital health, our design is structured around three\naspects:\n(1) Multi-dimensional health records. We represent the\nlife records of user u as Xu = {XD\nu , XS\nu , XA\nu , XE\nu }, where\nD, S, A, and E denote the diet, sleep, physical activity, and\nemotion domains, respectively. Each X\u00b7\nu is a time-stamped\nrecord sequence in its corresponding domain. All domains\nare aligned using anonymized user IDs and timestamps, en-\nabling queries defined on specific time windows and allow-\ning evidence to be combined across domains. (2) Compo-\nsitional reasoning operators.\nWe construct questions by\ncomposing a set of operators that mirror common health in-\nquiries, including time window selection, temporal alignment\nacross-domains, aggregation (e.g., mean, min, max), compar-\nison across time periods or user groups, conditional filtering\nwith thresholds, and temporal pattern analysis such as con-\nsecutiveness and trends. By composing these operators across\ntwo or more lifestyle domains, we form cross-domain reason-\ning queries. For example, a query may analyze whether de-\ncreases in deep sleep duration over several consecutive days\nare accompanied by increases in stress levels, which requires\njointly reasoning over sleep and emotion records within a\nshared time context. This operator-based design covers the\ntask families defined in Sec. 3.2 while keeping each question\ninterpretable and executable. (3) Executable grounding for\nverifiability. We map each natural language question Q to\na deterministic executable program \u03c0(Q). Executing \u03c0(Q)\non the aligned database yields a unique ground-truth answer,\nproviding a reliable factual answer reference for evaluating\nthe reasoning performance of personal health assistant agents.\nExamples.\nSeveral example queries are as follows: (i)\nSingle-dimensional: \u201cFor each day last week, what was my\naverage sleep duration?\u201d (ii) Cross-dimensional: \u201cOver the\npast week, on how many days did I exceed both an activity-\nduration [threshold] and a sleep-duration [threshold]?\u201d (iii)\nMulti-user: \u201cOver the past week, on how many days did my\nsleep duration and aerobic activity time both exceed the co-\nhort average? On those days, what was my dominant diet\ncategory, and what trend did my emotion score exhibit?\u201d\n3.4\nDataset Generation Pipeline\nWe design an automated pipeline to generate the QA pairs in\nLifeAgentBench. First, we transform the aligned daily-life\nrecords into a relational database. Next, we instantiate query\ntemplates into diverse natural-language questions and derive\nverifiable ground-truth answers by executing the correspond-\ning SQL queries on the database. Finally, we conduct manual\ninspection and double-checking to ensure data quality. The\npipeline is scalable and can be extended to new data sources\nor lifestyle domains by adding database schemas and query\ntemplates. We release the code to support reproducibility and\nfuture benchmark expansion.\n4\nLifeAgent\nOur evaluation on LifeAgentBench shows that long-horizon,\ncross-dimensional lifestyle reasoning remains challenging for\ncurrent LLMs, largely because many queries require multi-\ndimensional evidence retrieval and aggregation over long\ntime windows. We therefore propose LifeAgent, a training-\nfree health-assistant agent that executes a query as a se-\nquence of tool-based actions, iteratively retrieving relevant\nrecords and applying deterministic operators to answer com-\nplex lifestyle questions, generate periodic summaries, and\nprovide targeted suggestions.\n4.1\nAgent Framework\nLifeAgent is built on smolagents [Roucher et al., 2025], a\nwidely used lightweight and open-source framework that fa-\ncilitates iterative development. Specifically, we implement\n\u2022 Decompose the executable sub-questions qi\n\u2460 User Query\n\u2461 In-context Decomposition\n\u2022 Determine the key parameters\n\u2022 Formulate a search plan\n\u2462 Iterative thought-action-observation loop\nThought\n\u2022 Evidence \ud835\udf00\ud835\udc61\n\u2022\nIntermediate\nContext Mt\n\u2022\nQuery Q\nAction\nObservation ot\nObservation\nTool call\nor\nSTOP\nData retrieval     Cohort-level aggregation   Deterministic computation\n\u2022 Evidence search\n\u2022 Event lookups\n\u2022 Cohort-level summaries\n\u2022 Ranking by metric\n\u2022 +   \u2212   \u00d7   \u00f7\n\u2022 Aggregation\n\u2022 Group-wise comparison\n\u2022 Average\n\u2022 Classify\nTool Design\n\u2463 Evidence-Grounded Synthesis\n\u2022 Integrate retrieved evidence\n\u2022 Identify potential concerns\n\u2022 Provide actionable suggestions\n\u2464 Response\nFigure 3: The framework of LifeAgent.\nthe iterative thought-action-observation loop shown in Fig-\nure 3. Formally, the execution follows an iterative tool-calling\nprocess \u27e8S, A, O, T , F\u27e9, where S is the state space, A the\naction space, O the tool observation space, T the state up-\ndate function, and F the termination condition. At step t,\nthe agent issues a tool call or STOP, receives an observation\not, and updates its state by augmenting the evidence cache Et\nand intermediate context Mt before final synthesis.\n\u2022 State space (S). At step t, the agent maintains a task-\noriented state st that includes the user query Q together\nwith its parsed intent \u03d5(Q) (task type, target domains,\ntime window, and required output form), a partial exe-\ncutable plan \u03c0t, an evidence cache Et storing retrieved\nrecords and intermediate results with their SQL traces,\nand intermediate statistics Mt (e.g., aggregates, com-\nparisons, detected anomalies, and trend summaries).\n\u2022 Action space (A).\nThen, the agent selects an action\nat conditioned on the current state st using the under-\nlying LLM. Each action is either a structured tool call\nat = (\u03c4t, \u03b8t), where \u03c4t denotes a tool and \u03b8t specifies its\narguments (e.g., domain index d \u2208{D, S, A, E}, time\nwindow, and aggregation granularity), or a terminal ac-\ntion STOP that triggers final synthesis.\n\u2022 Observation space (O).\nExecuting a non-terminal\naction returns a structured, verifiable observation ot,\nsuch as event-level records, aligned daily time series,\nscalar aggregates, comparison deltas, threshold checks,\nor trend parameters. Each ot is appended to the evidence\ncache and used to update Mt, providing verifiable sup-\nport for subsequent reasoning and final synthesis.\n\u2022 Transition (T ) and termination. Upon receiving the\nobservation ot, the agent updates its working state:\nst+1 = T (st, at, ot).\nThe loop terminates under F\nwhen the agent issues STOP or reaches a step budget,\nand the final response is synthesized from the accumu-\nlated evidence Et and intermediate summaries Mt.\n4.2\nMulti-step Reasoning and Decomposition\nA key design of LifeAgent is to transform a complex query\nQ into a tool-executable retrieval agenda and a final synthe-\nsis step. We implement multi-step reasoning and decomposi-\ntion with in-context demonstrations that guide the LLM to (i)\nidentify the retrievable evidence required by Q, (ii) obtain it\nvia tool calls, and (iii) synthesize an evidence-grounded re-\nsponse.\nAt inference time, LifeAgent decomposes Q into\na retrieval agenda {qi}K\ni=1 = \u2206(Q), where \u2206(\u00b7) denotes\nthe LLM-based decomposition operator prompted by our in-\ncontext demonstrations. Each sub-question qi specifies the\ntarget domain(s), time scope, and retrieval granularity. As\nnew observations become available, the agent updates the\nagenda by adding or revising sub-questions, and the execu-\ntion of the resulting tool calls incrementally updates the evi-\ndence cache Et and intermediate context Mt. This multi-step\ndecomposition is evidence-driven and adaptive, rather than\nrelying on a single-pass retrieval. For a query such as \u201cHow\nwas my sleep quality last week?\u201d, LifeAgent first retrieves\ndaily sleep-related indicators (e.g., sleep duration) over the\nspecified window, summarizes week-level patterns, and flags\nabnormal days. When potential anomalies are found, it fur-\nther drills down to event-level records on those days, such\nas irregular diet or unusual activity, to contextualize the de-\nviation. This adaptive retrieval supports long-horizon, cross-\ndimensional analysis that is difficult to capture with a single\nretrieval step. Finally, LifeAgent synthesizes the final answer\nconditioned on Q and the accumulated evidence Et (and in-\ntermediate context Mt), producing an integrated response.\n4.3\nTool Design\nTo support the multi-step decomposition and reliable\nevidence-grounded analysis in LifeAgent, we design three\ncategories of tools: (i) data retrieval over life records, (ii)\ncohort-level aggregation operations for multi-user reasoning,\nand (iii) deterministic basic computation. This design allows\nthe agent to iteratively retrieve evidence, derive intermediate\nstatistics, and provide reliable computational support for the\nfinal complex reasoning and decision-making.\n\u2022 Structured data retrieval. These tools provide struc-\ntured access to the life-record database under explicit\nconstraints (domain, time window, and granularity), sup-\nporting event-level lookups and daily aggregated time-\nseries extraction. They return structured outputs with\nSQL traces, allowing the agent to cache evidence and\ndecide whether further retrieval is needed.\n\u2022 Cohort-level aggregation. Multi-user queries require\naggregating statistics over many users.\nWe provide\ngeneric cohort operators that compute cohort-level sum-\nmaries (e.g., mean/median/percentiles over a window),\nrank users by a selected metric, and perform group-\nwise comparisons by splitting users into cohorts based\non specified criteria. These tools enable efficient aggre-\ngation and comparison for multi-user reasoning.\n\u2022 Deterministic computation. These tools perform basic\narithmetic operations deterministically, reducing errors\ncaused by model-side calculation hallucinations while\nleaving higher-level analysis to the agent.\n5\nExperiments\n5.1\nExperimental Setup\nModels. We evaluate a diverse set of mainstream LLMs, cov-\nering both open-source and closed-source models with a wide\nrange of scales. (1) Open-source models. We test representa-\ntive instruction-tuned families, including Llama [Grattafiori\net al., 2024] (Llama-3.2-3B-Instruct; Llama-3.1-8B/70B-\nInstruct), Qwen [Hui et al., 2024] (Qwen-2.5-7B/14B/32B-\nInstruct), Phi [Abdin et al., 2024] (Phi-3.5-mini-instruct),\nMistral [Jiang et al., 2023] (Mistral-7B-Instruct-v0.3), and\nDeepSeek [Guo et al., 2025] (DeepSeek-R1-Distill-Qwen-\n7B). (2) Closed-source models.\nWe include three widely\nused proprietary models: GPT-4o [OpenAI, 2024], Claude-3-\nhaiku [Anthropic AI, 2024], and Gemini 2.5 Flash-Lite [Co-\nmanici et al., 2025].\nBaselines. We evaluate mainstream LLMs on LifeAgent-\nBench under two baseline settings: Context Prompting [Lee\net al., 2024] and Database-augmented Prompting [Zhu et al.,\n2024]. (1) Context Prompting (CP). Given a health reason-\ning question, the system first pre-filters the life record data\nto keep the target user\u2019s data, and then embeds the filtered\ndata along with the question into the prompt for the LLM to\nreason. (2) Database-augmented Prompting (DP). DP adopts\na two-stage, LLM-driven retrieval-then-reasoning workflow.\nGiven the question and the database schema, the LLM gen-\nerates an SQL query to retrieve evidence from the database.\nThe system validates the query and executes only complete\nSELECT statements; the returned results are then fed back to\nthe LLM to produce the final answer.\nMetrics. We use Accuracy as the primary metric for all\nsettings. For DP, we additionally report SQL Validity (VA),\nExecution Accuracy (EX), and Acc|EX to quantify LLMs per-\nformance at different stages, including SQL generation, evi-\ndence retrieval, and final reasoning.\n\u2022 Accuracy (Acc). We evaluate each instance by answer\ntype: (i) Yes/No: the prediction must exactly match the\nground truth.\n(ii) Numeric: we allow a small toler-\nance. For small integer answers (gt \u226414), a predic-\ntion is counted as correct if it is within \u00b11 of the ground\ntruth. For larger integers (gt > 14) or real-valued an-\nswers, we require the absolute error to be no more than\nmax(0.5% \u00b7 |gt|, 0.01). (iii) Multi-item: the prediction\nmust contain the same number of items as the ground\ntruth, and each item must be correct.\n\u2022 SQL Validity (VA). The percentage of LLM-generated\nSQL queries that are complete SELECT statements and\nexecute on the database without errors.\n\u2022 Execution Accuracy (EX). Among executable queries,\nthe fraction whose returned results contain correct in-\nformation to derive the final answer.\n\u2022 Acc|EX. The fraction of correct final answers among in-\nstances where the SQL executes effectively.\n5.2\nBenchmarking Results\nOverall Results. Table 2 and Figure 4 summarize the per-\nformance of all LLMs on the full LifeAgentBench dataset.\nAs shown in Figure 4, closed-source models overall out-\nperform open-source models. Under CP, GPT-4o achieves\nthe highest accuracy of 57.02%, while the best open-source\nmodel Qwen-2.5-7B reaches 40.45%; Under DP, Gemini 2.5\nLite performs best with 39.04%, followed by GPT-4o with\n34.71%. These results reveal a clear capability gap of cur-\nrent LLMs in performing long-horizon health reasoning over\nlarge-scale, cross-dimensional life records. However, we find\nthat under the DP setting, the final-answer accuracy improves\nDataset\nCP\nDP\nMetrics\nAcc (%)\nAcc (%)\nVA(%)\nEX(%)\nAcc|EX(%)\nOpen-source LLMs\ndeepseek-coder-1.3B\n1.09\n1.26\n43.83\n14.62\n4.00\nLlama-3.2-3B\n20.18\n13.47\n47.29\n17.99\n67.68\nPhi-3.5-mini-3.8B\n20.57\n16.16\n56.67\n20.23\n77.34\nMistral-v0.3-7B\n30.97\n9.03\n28.46\n11.48\n75.35\nQwen-2.5-7B\n40.45\n21.45\n55.83\n24.71\n84.78\nLlama-3.1-8B\n20.65\n21.53\n63.33\n27.25\n77.73\ngemma-2-IT-9B\n24.44\n14.54\n56.24\n27.85\n51.15\nLlama-3.1-70B\n40.51\n13.91\n45.77\n22.73\n58.41\nClosed-source LLMs\nGemini 2.5 Lite\n44.81\n39.04\n84.84\n45.97\n82.92\nClaude-3-haiku\n35.30\n29.30\n74.06\n36.49\n75.71\nGPT-4o\n57.02\n34.71\n63.85\n35.97\n95.65\nTable 2: Overall results of all LLMs on LifeAgentBench.\nThe most difficult category\nCP\nDP\nLifeAgent\nQuestion Type\nAS\n5.21\n17.35\n24.71\nNC\n24.42\n12.29\n40.15\nAnswer Type\nPairwise Answer\n1.78\n6.06\n38.31\nMulti-item Answer\n0.36\n3.04\n32.31\nAll dimensions\nSingle-user\n14.69\n2.81\n53.06\nMulti-user\n0.00\n15.00\n52.44\nAverage\n\u2013\n7.74\n9.43\n40.16\nTable 3:\nAccuracy (%) on the most challenging subsets\nof LifeAgentBench, comparing LifeAgent with CP and DP\nbaselines under the same backbone (Qwen2.5-7B).\nsubstantially when correct intermediate evidence is avail-\nable: seven models achieve Acc|EX above 70%, and GPT-\n4o reaches 95.65%. This suggests that tool interaction and\nstructured evidence retrieval can substantially benefit com-\nplex cross-domain, long-horizon health reasoning. Neverthe-\nless, all current models exhibit low SQL execution accuracy\n(EX), averaging only 25.94%, indicating that accurately inter-\npreting database schemas, understanding inter-table relation-\nships, and effectively invoking tools to retrieve and validate\nevidence remain major challenges for current LLMs.\nPerformance with Varying Task and Answer Complex-\nity. To identify the bottlenecks of current LLMs for health\nreasoning, we summarize performance by task type and an-\nswer format.\nBy task type, models perform best on fact-\noriented queries such as FQ, but fail on more aggregation-\nintensive reasoning: for AS, the average accuracy across\nmodels is only 5.72% under CP and 15.31% under DP, with\nNC showing similarly low performance.\nBy answer type,\nsimple outputs (e.g., Yes/No and single-number answers)\nare handled relatively well, whereas structured outputs are\na major source of errors: for multi-item answers, the aver-\nage accuracy is only 8.62% under CP and 3.54% under DP.\nThese difficulties persist even for the strongest model. As\nshown in Figure 5, GPT-4o achieves high accuracy on eas-\nier categories (e.g., 69.8% on FQ under CP) and once suf-\nficient evidence is retrieved under DP, its final reasoning is\nhighly reliable (Acc|EX reaches 93.7%). However, it still\nperforms very poorly on AS questions and on complex out-\nputs such as multi-item answers, indicating that aggregation\nand structured answer synthesis remain key bottlenecks be-\nyond general language understanding. Overall, these results\nmotivate LifeAgent, which decomposes complex queries into\nmulti-step tool-executable retrieval and introduces determin-\nistic computation to reliably support aggregation-intensive\n0\n10\n20\n30\n40\n50\nContext Prompting Acc (%)\n0\n5\n10\n15\n20\n25\n30\n35\n40\nDatabase-augmented Prompting Acc (%)\ndeepseek-coder-1.3B\nLlama-3.2-3B\nPhi-3.5-mini-3.8B\nMistral-v0.3-7B\nQwen-2.5-7B\nLlama-3.1-8B\ngemma-2-IT-9B\nLlama-3.1-70B\nGemini 2.5 Lite\nClaude-3-haiku\nGPT-4o\nFigure 4: Accuracy (%) comparison of all LLMs under two\nevaluation settings: CP vs. DP.\nFQ\nAS\nCQ\nNC\nTA\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n69.8\n15.6\n71.5\n49.6\n79.1\n42.5\n26.4\n58.5\n31.3\n14.9\n93.7\n98.9\n99.3\n87.5\n96.8\nYes/No\nSingle Num\nShort Text\nPairwise\nMulti-Item\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n75.1\n68.3\n61.6\n33.9\n28.1\n19.3\n41.5\n50.4\n15.6\n17.0\n77.6\n99.2\n99.2\n86.0\n87.8\nCP Acc(%)\nDP Acc(%)\nDP Acc|EX(%)\nFigure 5: GPT-4o performance across question types (left)\nand answer formats (right) in CP and DP.\nreasoning and structured outputs.\nPerformance with Varying Dimensions and User Set-\ntings. We further examine how model performance changes\nas the evidence scope expands, including cross-dimensional\nintegration and multi-user reasoning. Across dimensions, ac-\ncuracy of all models degrades as more lifestyle domains are\ninvolved: under CP, the average accuracy drops from 41.54%\non single-dimension tasks to 23.42% on all-dimension tasks;\nunder DP, the decline is even steeper, from 30.84% to 9.63%.\nThis trend highlights the difficulty of cross-dimensional rea-\nsoning, where models must jointly capture fine-grained evi-\ndence within each domain and their interactions across do-\nmains.\nThe challenge becomes more severe under multi-\nuser settings. Multi-user queries involve substantially larger\nevidence scopes, so that CP frequently exceeds the con-\ntext window and breaks down. Under DP, performance also\ndropped lower, indicating that current LLMs struggle to reli-\nably retrieve and aggregate evidence at scale, especially when\nqueries involve both cross-domain interactions and multi-user\nanalysis. Motivated by these limitations, LifeAgent is de-\nsigned with a general-purpose tool suite for cross-domain and\nmulti-user reasoning, and iteratively expands the evidence\nscope through multi-step retrieval to enable robust reasoning\nfor cross-domain and cross-user queries.\n5.3\nEffectiveness of LifeAgent\nTo test whether LifeAgent better addresses the bottlenecks\nidentified in LifeAgentBench, we compare it with CP and\nDP on the most challenging reasoning subsets under the\nsame backbone (Qwen2.5-7B), as shown in Table 3. Over-\nall, LifeAgent substantially improves accuracy on these tasks:\nthe average accuracy increases from 7.74% (CP) and 9.43%\n(DP) to 40.16%, corresponding to +32.42 and +30.73 points.\nSpecifically, LifeAgent markedly improves performance on\nTask\nHealth-indicator queries (Acc %)\nComprehensive health reasoning (Score 1\u20135)\nTargeted lifestyle recommendations (Score 1\u20135)\nDimension\nCP\nDP\nLifeAgent\nCP\nDP\nLifeAgent\nCP\nDP\nLifeAgent\nActivity\n40.22\n59.40\n78.56\n1.23\n1.54\n2.76\n3.73\n3.91\n3.99\nSleep\n38.86\n39.62\n71.51\n1.45\n1.38\n2.47\n4.16\n4.30\n4.39\nEmotion\n49.70\n60.47\n79.68\n1.21\n1.66\n2.63\n3."
      },
      {
        "from": "human",
        "value": "<image>\nSummarize the paper: "
      },
      {
        "from": "gpt",
        "value": "### Comprehensive Summary of the Paper\n\n**Title:** LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health\n\n**Main Contributions:**\n- **Introduction of LifeAgentBench:** A large-scale QA benchmark designed to evaluate long-horizon, cross-dimensional reasoning over heterogeneous lifestyle records. The benchmark contains 22,573 questions spanning from basic retrieval tasks to complex reasoning.\n  \n- **Benchmark Construction and Evaluation Protocol:** Release of an extensible benchmark construction pipeline and standardized evaluation protocol that enables reliable and scalable assessment of LLM-based health assistants.\n\n- **Systematic Evaluations of Leading LLMs:** Systematically evaluated 11 leading large language models (LLMs) on LifeAgentBench, identifying key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. \n\n- **Proposed LifeAgent as a Baseline Agent:** Introduced LifeAgent as a strong baseline agent for health assistants that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared to two widely used baselines.\n\n- **Realistic Scenarios Case Studies:** Demonstrated the potential of LifeAgent in realistic daily-life scenarios through case studies.\n\n- **Public Availability:** The benchmark is publicly available for future research and expansion.\n\n**Methodology:**\n- **Dataset Construction:** Based on AI4FoodDB, a comprehensive longitudinal lifestyle dataset encompassing 100 participants. Data from four domains (diet, sleep, physical activity, emotion) were integrated.\n  \n- **QA Task Organization:** Organized questions into five task categories to reflect practical health analysis requirements and support agent baseline development.\n\n- **Design Principles for LifeAgent:** Designed LifeAgent around three key aspects: multi-dimensional health records, compositional reasoning operators, and executable grounding for verifiable answers. \n\n- **Multi-step Reasoning and Decomposition:** Implemented a framework that decomposes complex queries into tool-executable retrieval agendas and synthesis steps, enabling iterative evidence retrieval and aggregation.\n\n**Key Findings:**\n- Current LLMs struggle with long-horizon cross-dimensional reasoning over large-scale heterogeneous lifestyle records.\n  \n- Multi-user reasoning poses significant challenges for current models due to the need to aggregate and relate interacting factors across time horizons.\n  \n- The proposed LifeAgent agent achieves significant improvements in accuracy, particularly on challenging multi-user and cross-dimension tasks.\n\n**Conclusions:**\nThe paper introduces a comprehensive benchmark (LifeAgentBench) and proposes an advanced baseline model (LifeAgent) for evaluating and improving the capabilities of LLMs in handling complex health assistant tasks. The findings highlight key challenges and potential improvements in long-horizon, multi-dimensional reasoning within digital health applications."
      }
    ]
  }
]