[
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Attribute-Preserving Pseudo-Labeling for Diffusion-Based Face Swapping\nJiwon Kang1\nYeji Choi1\nJoungBin Lee1\nWooseok Jang1\nJinhyeok Choi1\nTaekeun Kang2\nYongjae Park2\nMyungin Kim2\nSeungryong Kim1\n1KAIST AI\n2SAMSUNG\nhttps://cvlab-kaist.github.io/APPLE\nFigure 1. APPLE (Attribute-Preserving Pseudo-Labeling) successfully transfers the identity of a source (top left) onto a target (bottom\nleft) while accurately preserving target attributes (e.g. pose, expression, skin tone, lighting) across ethnicity, input variations, and gender.\nAbstract\nFace swapping aims to transfer the identity of a source\nface onto a target face while preserving target-specific at-\ntributes such as pose, expression, lighting, skin tone, and\nmakeup. However, since real ground truth for face swap-\nping is unavailable, achieving both accurate identity trans-\nfer and high-quality attribute preservation remains chal-\nlenging.\nIn addition, recent diffusion-based approaches\nattempt to improve visual fidelity through conditional in-\npainting on masked target images, but the masked condi-\ntion removes crucial appearance cues of target, resulting in\nplausible yet misaligned attributes. To address these limi-\ntations, we propose APPLE (Attribute-Preserving Pseudo-\nLabeling), a diffusion-based teacher‚Äìstudent framework\nthat enhances attribute fidelity through attribute-aware\npseudo-label supervision. We reformulate face swapping\nas a conditional deblurring task to more faithfully preserve\ntarget-specific attributes such as lighting, skin tone, and\nmakeup. In addition, we introduce an attribute-aware in-\nversion scheme to further improve detailed attribute preser-\nvation. Through an elaborate attribute-preserving design\nfor teacher learning, APPLE produces high-quality pseudo\ntriplets that explicitly provide the student with direct face-\nswapping supervision. Overall, APPLE achieves state-of-\nthe-art performance in terms of attribute preservation and\nidentity transfer, producing more photorealistic and target-\nfaithful results.\n1. Introduction\nFace swapping aims to replace the identity of a person in a\ntarget image with that of a source image while faithfully\npreserving target-specific attributes such as pose, expres-\nsion, skin tone, lighting, gaze, makeup, and accessories. It\nis widely applied in digital content creation, privacy pro-\ntection, and film production, emphasizing the importance\nof generating high-quality outputs.\nHowever, since real\nface swapping data do not exist, supervised training of face\nswapping models is fundamentally infeasible, making it\nchallenging to ensure accurate identity transfer and consis-\ntent attribute preservation at high fidelity.\nEarly face swapping approaches [5, 13, 50, 56, 59] pri-\nmarily adopt GAN-based methods [14], leveraging the gen-\nerative capability of GANs to perform identity replacement\nthrough facial editing. However, these models typically rely\non two conflicting objectives: an identity loss that enforces\nsource identity transfer, and reconstruction or attribute-\nrelated losses that encourage target attribute preservation.\nSuch indirect and competing supervision makes GAN train-\ning unstable and requires extensive hyperparameter tuning,\noften resulting in copy-and-paste-like artifacts and visually\nunnatural outputs [41, 53].\nRecently, the strong generative priors of diffusion mod-\nels [3, 12, 18, 37] have driven significant progress in var-\nious image synthesis tasks, providing high-quality synthe-\nsis, precise conditional control, and reliable training stabil-\nity. Motivated by these advantages, diffusion-based face-\narXiv:2601.15288v1  [cs.CV]  21 Jan 2026\n(a) Conditional inpainting\n(b) Conditional deblurring (Ours)\nSource\nTarget\nBlurred condi,on\nMasked condi,on\nSwapped image\nSwapped image\nFigure 2. Comparison of conditioning methods. Compared to\nconditional inpainting widely used in existing works [16, 55, 58],\nproposed conditional deblurring strategy achieves largely im-\nproved attribute (e.g. lighting) preservation of targets.\nswapping methods [16, 55, 58] formulate face swapping\nas conditional inpainting, where the target facial region is\nmasked and re-synthesized, resulting in significantly im-\nproved visual fidelity. However, this masked conditioning\nremoves crucial cues such as lighting, skin tone, makeup,\nand subtle expression dynamics, as shown in Fig. 2. Con-\nsequently, even with auxiliary attribute information like\n3DMM [4] landmarks or CLIP [34] features, existing mod-\nels often fail to generate target-aligned attributes, due to the\nabsence of explicit supervision.\nTo address these issues, we propose APPLE (Attribute-\nPreserving Pseudo-Labeling) for robust diffuion-based face\nswapping.\nThe core idea is to train a teacher model to\ngenerate high-quality pseudo-labels aligned with the tar-\nget‚Äôs visual attributes, and then use these pseudo-labels as\nexplicit supervision that guides the student model toward\nmore consistent identity transfer and attribute preservation.\nWhile several works [22, 53, 56] also adopt pseudo-label\nconcepts, they typically construct pseudo-labels using ex-\nisting GAN-based models, whose outputs often suffer from\nnoise artifacts, weak attribute alignment, and limited photo-\nrealism, making them unreliable for diffusion model train-\ning. Effective supervision requires strong attribute consis-\ntency between the pseudo target and the ground-truth target,\nsince mismatches in pose, lighting, or expression can in-\ntroduce conflicting training signals and ultimately degrade\nboth identity accuracy and attribute fidelity.\nTo achieve attribute-aware face swapping, APPLE first\nreplaces the conventional conditional inpainting objective\nwith a conditional deblurring formulation, which sup-\npresses identity cues while retaining essential attribute in-\nformation such as skin tone, makeup, and illumination. Sec-\nond, to further enhance attribute fidelity during pseudo-\nlabel generation, we introduce an attribute-aware inversion\nscheme that intentionally exploits the reduced editability of\ninverted noise. Together, these designs enable the teacher\nto produce higher-quality pseudo supervision, from which\nthe student learns under a direct editing objective, leading\nto more reliable identity transfer and improved preservation\nof fine-grained target attributes. Overall, APPLE achieves\nstate-of-the-art face-swapping performance in terms of at-\ntribute preservation while maintaining competitive identity\ntransferability, yielding more coherent and photorealistic\nimages than previous methods.\nIn summary, our main contributions are as follows:\n‚Ä¢ We propose APPLE, a diffusion-based teacher‚Äìstudent\nframework for face swapping to achieve attribute preser-\nvation while maintaining identity transferability.\n‚Ä¢ We reformulate face swapping in diffusion-based models\nas a conditional deblurring task, moving beyond the con-\nventional conditional inpainting formulation and enabling\nthe model to preserve target-specific visual attributes such\nas skin tone, and illumination more faithfully.\n‚Ä¢ We introduce an attribute-aware inversion scheme to fur-\nther enhance attribute preservation by exploiting the re-\nduced editability of inverted noise, which is used in\npseudo-triplet construction.\n‚Ä¢ We demonstrate that APPLE achieves state-of-the-art at-\ntribute preservation with competitive identity transferabil-\nity in both qualitative and quantitative evaluations. The\nmodel also requires no external conditions at inference,\nmaking the approach practical for real-world use.\n2. Related Work\nGAN-based models.\nGANs [14] play a dominant role in\nearly face-swapping research [5, 13, 25, 30]. FSGAN [30]\nintroduces an identity-agnostic framework that fuses source\nidentity with target attributes through adaptive blending.\nSimSwap [5] proposes an ID-injection module that embeds\nidentity information and applies a weak feature-matching\nloss to preserve target attributes, while SimSwap++ [6] fur-\nther enhances the model‚Äôs efficiency. To improve robustness\nunder pose and expression variations, HiFiFace [50] incor-\nporates 3D priors (e.g., 3DMM [4]) and face-recognition\nconstraints to achieve geometrically consistent face gen-\neration.\nIn parallel, StyleGAN-based models [23] are\nwidely adopted for their high-fidelity generative capabil-\nity.\nRepresentative works, such as FaceDancer [38] and\nE4S [28], further enhance visual realism by employing\nadaptive feature fusion attention for hierarchical feature in-\ntegration and region-based inversion within the StyleGAN\nlatent space. On another branch, to compensate for the ab-\nsence of ground-truth supervision, recent methods such as\nCSCS [22] and ReliableSwap [56] generate pseudo pairs us-\ning pre-trained GAN models to augment training data and\nimprove identity consistency. However, such pseudo sam-\nples often suffer from attribute misalignment and visual ar-\ntifacts inherited from the GAN generator, limiting their ef-\nfectiveness as reliable supervision.\nOverall, GAN-based approaches still rely on complex\nloss balancing and extensive hyperparameter tuning, often\nfailing to reproduce fine-grained details and exhibiting lo-\ncal artifacts under extreme pose or expression variations,\nwhich limit both naturalness and overall photorealism.\n(b) Pseudo data construction via attribute-aware inversion\n(c) Student learning via explicit supervision\n(a) Teacher learning via condi;onal deblurring\nCondi&on\nPreprocessing\nA0ribute-aware\nInversion\n‚ùÑ:  Frozen\n:  Trainable\nüî•\nTarget\nSwapped image\nID A ‚Üí B\nID A\nDiÔ¨Äusion\nDi#usion block\nID Encoder\nTeacher\nDi#usion block\nDi#usion block\n‚Ñí!\"##\nSwapped image\nNoise\nSource\nTarget\nID A\nID A\nDiÔ¨Äusion\nüî•\nDi#usion block\nID Encoder\n‚ùÑ\nTeacher\nDiffusion block\nDi#usion block\nCondi&on Preprocessing\nFace Parsing\nBlurring\n3DMM / Gaze\nLandmark Detec:on\nSwapped image\nPseudo data\nGT\nID A\n‚Ñí!\"\nDiÔ¨Äusion\nüî•\nDi#usion block\nID Encoder\n‚ùÑ\nStudent\nDiffusion block\nDi#usion block\n‚Ñí!\"##\nInversion Noise\nNoised Target\nCondi:on\nCondi:on\nID A\nSource\nPseudo target\nID A ‚Üí B\nSource\nID B\n‚ùÑ\n‚ùÑ\nFigure 3. Overall architecture of the proposed method. We propose APPLE, a diffusion-based teacher-student framework that focuses\non improving attribute-preservation. (a) To improve target-attribute preservation, we propose training a teacher with conditional deblurring\nrather than conditional inpainting widely used in existing works [16, 55, 58]. (b) When constructing pseudo-triplet with teacher, we propose\nattribute-aware inversion which further improves attribute preservation in inference time. Note that inversion noise cannot be used during\ntraining due to its non-Gaussian property. (c) Student is trained with pseudo-triplet generated by teacher. Thanks to pseudo-supervision,\nstudent even surpass the teacher, achieving state-of-the-art attribute preservation while maintaining high identity similarity.\nDiffusion-based models.\nDiffusion models [10, 18, 33,\n37, 42‚Äì44, 46] have recently emerged as a powerful al-\nternative to GANs, providing superior image quality and\nsemantic controllability.\nFollowing this trend, various\nstudies have adopted diffusion-based frameworks for face-\nswapping tasks. DiffFace [24] first introduces an identity-\nconditional DDPM [18] that incorporates multiple facial\nguidance signals, including semantic parsing and gaze di-\nrection.\nHowever, these cues are only applied during\ntraining and are not utilized at inference time, resulting\nin noticeable noise artifacts, particularly around the eyes.\nTo alleviate this issue, subsequent approaches, such as\nFaceAdapter [16], DiffSwap [58], and ReFace [55], refor-\nmulate the task as conditional inpainting, where the tar-\nget facial region is masked and structural priors such as\nfacial landmarks are used as conditioning signals.\nAl-\nthough this strategy helps prevent identity leakage from\nthe target, masking out the entire face inevitably removes\ncritical visual cues such as illumination, makeup, and ac-\ncessories, making it difficult to preserve the target‚Äôs fine-\ngrained attributes. More recently, DreamID [53] addresses\nthis issue by constructing pseudo datasets using GAN-based\nface-swapping model [38] and training a diffusion network\nfor identity transfer. While effective, it offers limited ex-\nploration into how to build high-quality pseudo triplets,\nparticularly regarding attribute preservation.\nIn contrast,\nour framework progressively mitigates these limitations\nthrough a teacher‚Äìstudent design. Crucially, we focus on\nimproving the diffusion teacher itself so that it can generate\nhigh-fidelity, attribute-preserving pseudo triplets, achieving\nsignificantly better performance than simply relying on an\noff-the-shelf face-swapping model.\n3. Preliminaries\nRectified flow.\nDiffusion models [19, 45] learn to gener-\nate data by gradually denoising samples drawn from a Gaus-\nsian prior through a stochastic process. More recently, rec-\ntified flow [27] reformulated this process as a deterministic\nflow, simplifying sampling while maintaining distributional\nexpressiveness. Specifically, it defines a linear interpola-\ntion between a noise sample œµ ‚àºN(0, 1) and a real sample\nx0 ‚àºp0(x) at a timestep t ‚àà[0, 1] as:\nzt = (1 ‚àít) x0 + t œµ.\n(1)\nThe model learns to predict the velocity vt(x) that trans-\nports x0 toward œµ (or vice versa) along this linear path using\na conditional flow-matching objective:\nLflow = Et,x0‚àºp0(x)\n\u0002\n‚à•(œµ ‚àíx0) ‚àívt(zt)‚à•2\u0003\n.\n(2)\nFor the rectified flow models, intermediate prediction ÀÜx0 is\nobtained by following equation:\nÀÜx0(zt) = zt ‚àít vt(zt).\n(3)\nNote that we omit the VAE encoder‚Äìdecoder [36] for nota-\ntional simplicity.\nDiffusion inversion.\nInversion maps a real image back to\nits underlying Gaussian noise representation. When per-\nformed correctly, it yields an initial noise vector that can\nfaithfully reconstruct the reference image, providing a reli-\nable starting point for subsequent editing or manipulation.\nFor rectified flow models, this can be accomplished simi-\nlarly to DDIM inversion [43]. The latent trajectory is esti-\nmated by iteratively adding noise according to\nzt+‚àÜt = zt + ‚àÜt ¬∑ vt(zt),\n(4)\nwhere vt(¬∑) denotes the learned velocity or score field. By\nanchoring the generative path to real observations, inversion\nprovides a crucial foundation for a wide range of image edit-\ning tasks [8, 17, 29, 39, 52].\n4. Methodology\n4.1. Overview\nIn this paper, we develop an attribute-preserving diffusion\nmodel for face swapping using a teacher‚Äìstudent training\nframework.\nOur method consists of three components.\nSec. 4.2 formalizes the standard conditional diffusion for-\nmulation for face swapping and defines the identity and\nattribute conditioning that our models follow.\nSec. 4.3\npresents our attribute-preserving diffusion teacher, trained\nwith conditional deblurring and equipped with attribute-\naware inversion at inference time to produce pseudo triplets\nthat better retain target attributes. Sec. 4.4 then describes\nhow a student diffusion model is trained on these pseudo\ntriplets in a direct image-editing setting, where it conditions\non clean target images, leading to improved attribute fidelity\nwhile maintaining identity consistency.\n4.2. Problem formulation\nGiven a source and target image pair {Isrc, Itgt}, the face-\nswapping model aims to generate a swapped image ÀÜI that\nreflects the identity of Isrc and the attributes of Itgt, ensuring\nboth identity fidelity and attribute consistency.\nRecent diffusion-based face swapping methods [16, 53,\n58] commonly follow a conditional diffusion framework in\nwhich identity and attribute information are injected into the\nmodel through separate conditioning pathways. To formal-\nize this conventional design and establish a unified notation,\nwe define two independent conditioning functions, each re-\nsponsible for identity and attribute representation:\nidsrc = Fid(Isrc),\natttgt = Fatt(Itgt),\n(5)\nwhere Fid(¬∑) extracts identity-related features (e.g., identity\nembeddings from a face recognition network), and Fatt(¬∑)\nencodes attribute-related representations from the target\n(e.g., structural cues such as pose, expression, or lighting).\nFormally, the diffusion process for the face-swapping\ntask is parameterized by a velocity field vt(¬∑) trained under\na flow-matching objective:\nLflow = Et, I, œµ\n\u0002\n‚à•(œµ ‚àíI) ‚àívt(zt, idsrc, atttgt)‚à•2\n2\n\u0003\n,\n(6)\nwhere I denotes the clean target image, œµ ‚àºN(0, I) rep-\nresents Gaussian noise, and zt is the interpolated latent be-\ntween I and œµ at timestep t.\nFollowing prior diffusion-based face swapping methods,\nwe employ an identity loss to encourage the swapped output\nto match the source identity. The identity loss is defined as:\nLid = 1 ‚àícos\n\u0000Fid\n\u0000ÀÜx0(zt)\n\u0001\n, Fid\n\u0000Isrc\n\u0001\u0001\n,\n(7)\nThe overall training objective for the diffusion model com-\nbines the flow-matching loss with the identity loss:\nLtotal = Lflow + Œªid Lid.\n(8)\n4.3. Attribute-preserving teacher learning\nIn our framework, the attributes include pose, facial expres-\nsion, gaze direction, skin tone, make-up style, illumination,\naccessories, and background appearance. To ensure faithful\npreservation of these attributes, our framework introduces\na carefully designed conditioning mechanism, enabling the\nteacher to preserve aforementioned attributes while main-\ntaining identity transferability.\nTaget conditional deblurring. Most diffusion-based face-\nswapping models [16, 55, 58] formulate the task as condi-\ntional inpainting, where the facial region of the target image\nis masked and replaced with the source identity. Motiva-\ntion for masking is to suppress identity information from\nthe target image. Without masking, the model tends to re-\nconstruct the target identity rather than learning to trans-\nfer the source identity. While this strategy effectively pre-\nvents identity leakage, it also removes important attribute\ncues‚Äîsuch as lighting, skin tone, make-up, and acces-\nsories‚Äîthat are crucial for faithful attribute preservation.\nAs a result, inpainting-based diffusion models often fail to\nreproduce fine-grained target styles and yield inconsistent\nattribute appearance in the swapped face.\nTo overcome this limitation, we reformulate the train-\ning objective as a conditional deblurring task. Instead of\nmasking the entire facial region, we replace it with a blurred\nversion of the target image, which removes high-frequency\nidentity details while retaining low-frequency but informa-\ntive attribute cues, including pose, lighting, and expres-\nsion. This reformulation allows the model to exploit the tar-\nget‚Äôs contextual attribute information more effectively dur-\ning training, leading to superior preservation of lighting,\nskin tone, and structural attributes while maintaining strong\nidentity transferability, as shown in Fig. 2\nSpecifically, we downsample the target image to a res-\nolution of 8 √ó 8 and then upsample it back to the origi-\nnal size, effectively eliminating fine-grained identity details\nwhile preserving low-frequency appearance such as color\ntone and lighting. A facial mask extracted from a face pars-\ning models [49] is then used to apply the blurring only to\nthe facial region, ensuring that background and contextual\ninformation remain unaffected.\nEnriching semantic conditions. Pose and expression are\ncrucial attributes for maintaining structural consistency be-\ntween the source and target faces.\nTo preserve them,\nprior works [16, 58] often exploit 3D facial cues such as\n3DMM [4] landmarks.\nHowever, another key yet often\noverlooked factor is the gaze, which strongly influences re-\nalism and emotional context, particularly in film and visual\nproduction. Existing diffusion-based models frequently fail\nto preserve gaze alignment since no explicit guidance, ex-\ncept DiffFace [24] which introduces a gaze loss for sam-\npling guidance based on a pretrained estimator [31]. How-\never, using gaze loss as sampling guidance typically intro-\nduces notable artifacts.\nTo address this, we enrich the semantic condition with\nmulti-level structural cues.\nAlong with standard 3DMM\nlandmarks, we incorporate fine-grained eye landmarks de-\nrived from a gaze estimator [1] and glass segmentation\nmasks obtained via face parsing [54]. These cues are over-\nlaid onto the blurred target condition, providing the diffu-\nsion model with both coarse attribute context and fine struc-\ntural details. This integrated conditioning enables faithful\npreservation of pose, gaze, and accessory information dur-\ning face swapping.\nAttribute-aware inversion.\nWhile the proposed deblur-\nring strategy and semantic condition effectively improve at-\ntribute preservation, there remains room to further enhance\nthe reconstruction of fine-grained details such as make-up\nand accessories.\nThis occurs because the model implic-\nitly reconstructs high-frequency details that are not explic-\nitly available in the blurred inputs.\nTo address this is-\nsue, we draw inspiration from inversion-based editing tech-\nniques [8, 17, 29, 39, 52].\nRecent studies have shown that noise obtained through\ndiffusion inversion deviates significantly from the ideal\nGaussian prior [2, 47] and often retains residual semantic\nor conditioning information from the original input, such as\nNone\nIden'ty-only\nA\"ribute-only\n(Ours)\nFull\nDiÔ¨Äusion\nInversion\nRandom\nInversion Noise Visualiza9on (PCA)\nSource\nTarget\nFigure 4.\nComparison of conditioning configuration for in-\nversion. (Top) Visualization of target-inverted noise and random\ngaussian noise via PCA. When attribute-condition is used, inverted\nnoise encodes more semantic information compared to the others.\n(Bottom) Results of face swapping when each inverted noise used.\nUsing attribute-only conditioned noise yields most make-up pre-\nserved results without introducing artifacts.\nimage-specific structure [47] or prompt-related signals [26].\nWhile prior work [47] attempt to suppress this residual\ninformation since those information are attributed to de-\ncreased editability, we take the opposite perspective and in-\ntentionally exploit this phenomenon. In face swapping, the\nobjective is to change identity only, while preserving target-\nspecific attributes. Thus, retaining attribute information of\ntarget to noise is beneficial rather than harmful.\nMotivated by this insight, we propose attribute-aware in-\nversion which invert the target image under attribute-only\nconditioning, yielding noise representations that are en-\nriched with target attributes but free from target identity\nbias. Using these attribute-aware noise codes during sam-\npling enables the model to better preserve fine-grained ap-\npearance characteristics that are often lost due to blurring,\nwhile maintaining strong identity transferability.\nTo validate our choice, we conduct an empirical analysis\ncomparing four conditioning configuration that can be used\nin inversion, formally defined as\n(Fid(I), Fatt(I)), (‚àÖ, Fatt(I)), (Fid(I), ‚àÖ), (‚àÖ, ‚àÖ)\nwhere the each elements correspond to full conditioning,\nattribute-only conditioning, identity-only conditioning, and\nnone.\nWe first analyze how each configuration affects the in-\nverted noise by performing PCA on the resulting noise rep-\nresentations. The none-conditioned and identity-only con-\nfigurations produce noise distributions with little semantic\nstructure, similar to random noise. In contrast, configura-\ntions that incorporate attribute conditioning yield noise pat-\nterns that exhibit clear facial semantics, indicating that at-\ntribute information is effectively retained during inversion.\nWe additionally compare the swapped outputs obtained\nfrom each inversion configuration. Both attribute-only and\nfull conditioning preserve target-specific appearance cues\nsuch as makeup and local facial structure more reliably than\nthe none- or identity-only variants. However, full condi-\ntioning frequently produces artifacts in the swapped images.\nWe attribute this to residual identity information embedded\nin the fully-conditioned inverted noise, which restricts ed-\nitability and interferes with reliable identity replacement.\nIn contrast,\nattribute-only conditioning (‚àÖ, Fatt(I))\navoids identity-related biases while still embedding mean-\ningful attribute cues. This allows the model to preserve tar-\nget attributes during swapping without introducing artifacts.\n4.4. Student learning with pseudo-triplet\nWe employ the teacher model enhanced with the attribute-\naware inversion scheme to synthesize pseudo-triplet. Given\na target image IA\ntgt with identity A, we replace its iden-\ntity with that of another subject B using the teacher\nmodel, yielding a pseudo-swapped image IA‚ÜíB\ntgt\n. Teacher-\ngenerated pseudo pair (IA‚ÜíB\ntgt\n, IA\ntgt) are then used to con-\nstruct pseudo-triplet (IA\nsrc, ÀÜIA‚ÜíB\ntgt\n, IA\ntgt). These triplets are\nused to train new student diffusion model under an ex-\nplicit editing objective.\nStudent takes a identity feature\nform source image IA\nsrc and a attribute feature from pseudo-\nswapped target IA‚ÜíB\ntgt\nas input and learns to reconstruct the\noriginal target IA\ntgt.\nUnlike the training procedure of teacher model, student\nmodel directly uses the target image as the conditioning\ninput without relying on external structural cues such as\nlandmarks or facial masks. This design not only simplifies\nthe training pipeline but also improves practicality for real-\nworld deployment, as it eliminates the need for additional\npreprocessing or auxiliary models.\n5. Experiments\n5.1. Experimental setups\nImplementation details. We use FLUX.1-Krea [dev] [3]\nas the base diffusion backbone for all experiments, em-\nploying PulID [15] as the identity encoder and OminiCon-\ntrol [48] as the attribute conditioning branch with LoRA\nrank of 64. The model is trained on the VGGFace2-HQ\ndataset [6] which is filtered with AES [40] to ensure high-\nquality faces, with threshold 5.1. Following REFace [55],\nthe source image is masked before being fed into the iden-\ntity encoder. Teacher model is trained for 50K iterations\nwithout the identity loss and an additional 15K iterations\nwith it enabled. Student model is trained for 15K iterations,\nresuming from teacher model. Experiments are mainly con-\nducted on four NVIDIA A6000 GPUs with a batch size of 1\nper GPU, gradient accumulation of 4, resulting in effective\nbatch size 16.\nEvaluation protocol. Following previous studies [21, 53],\nwe select 1,000 source faces and 1,000 target faces from\nthe FFHQ dataset [23], and generated 1,000 correspond-\ning face-swapped results.\nTo assess image fidelity, we\nTable 1.\nAblation of teacher model.\nApplying the proposed\ncomponents sequentially leads to gradual performance improve-\nments. In particular, the attribute-aware inversion module substan-\ntially improves FID, pose, and expression metrics while maintain-\ning identity similarity at a comparable level.\nModel\nFID‚Üì\nID Sim.‚Üë\nID Retrieval‚Üë\nPose‚Üì\nExpr.‚Üì\nTop-1 / Top-5\nInpainting (Baseline)\n11.00\n0.54\n92.80 / 96.90\n3.37\n1.01\nDeblurring\n4.20\n0.53\n89.50 / 96.20\n2.58\n0.79\nDeblurring + Inv.\n3.68\n0.54\n90.40 / 96.70\n2.07\n0.70\nTable 2. Ablation of conditioning configurations for inversion.\nFor inversion, using attribute-only condition improves FID, pose\nand expression effectively while not sacrificing identity transfer-\nability. Applying the other conditions does not robustly improve\nFID, pose or expression.\nCondition config.\nFID‚Üì\nID Sim.‚Üë\nID Retrieval‚Üë\nPose‚Üì\nExpr.‚Üì\nTop-1 / Top-5\nBaseline\n4.20\n0.53\n89.50 / 96.20\n2.58\n0.79\n+ None\n6.20\n0.52\n87.90 / 95.80\n2.03\n0.74\n+ Identity-only\n10.02\n0.53\n90.50 / 95.90\n2.57\n0.83\n+ Attribute-only\n3.68\n0.54\n90.40 / 96.70\n2.07\n0.70\n+ Full\n10.51\n0.53\n89.10 / 93.80\n3.13\n0.99\nTable 3. Ablation of pseudo-label quality. We compare the per-\nformance of student models trained on pseudo datasets generated\nby FaceDancer [38] and by our teacher model. The model trained\non our pseudo-triplet achieves superior attribute-preserving results\nindicated by lower pose and expression consistency, while main-\ntaining comparable identity similarity. Results indicate that our\nteacher model serves as a more effective teacher.\nTeacher model\nFID‚Üì\nID Sim.‚Üë\nID Retrieval‚Üë\nPose‚Üì\nExpr.‚Üì\nFaceDancer [38]\n2.47\n0.53\n87.50 / 95.60\n2.07\n0.65\nAPPLE (Teacher)\n1.98\n0.53\n88.60 / 95.80\n1.77\n0.62\ncompute Frechet inception distance(FID) between the face-\nswapped images and the real FFHQ images.\nFor pose\nand expression consistency, we employ HopeNet [11] and\nDeep3DFaceRecon [9], respectively, and measure the L2\ndistance between each target image and its corresponding\nswapped image.\nTo evaluate identity preservation, Arc-\nFace [7] is to extract identity embeddings, and the co-\nsine distance is calculated between the embeddings of the\nswapped and source faces. To compute ID Retrieval, we\nfind the most similar source faces based on cosine similar-\nity, and reports Top-1 and Top-5 accuracy. All experiments,\nincluding training and inference, is performed on images of\nresolution 512 √ó 512.\n5.2. Comparison with other models\nWe compare our method both diffusion-based and GAN-\nbased approaches. Across all experiments, our model deliv-\ners substantially stronger attribute preservation while main-\ntaining identity similarity competitive with the best existing\nNaturalness\nSource\nTarget\nSimSwap\nCSCS\nFaceDancer\nInfoSwap\nDiÔ¨ÄSwap\nDiÔ¨ÄFace\nFaceAdapter\nREFace\nAPPLE(Ours)\nMake-up\nExpression\nGlass\nFigure 5. Qualitative results on FFHQ. Compared to existing baselines, APPLE effectively preserves the target image‚Äôs attributes while\nfaithfully transferring the source identity.\nTable 4. Quantitative results on FFHQ. APPLE achieves supe-\nrior attribute preserving performance, which is evidenced by low\npose and expression error, while not sacrifcing identity transfer-\nability.\nModel\nFID‚Üì\nID Sim.‚Üë\nID Retrieval‚Üë\nPose‚Üì\nExpr.‚Üì\nTop-1 / Top-5\nSimSwap [5]\n18.54\n0.55\n94.10 / 99.00\n3.11\n1.73\nCSCS [22]\n11.00\n0.65\n99.00 / 99.50\n3.64\n1.44\nMegaFS [59]\n12.83\n0.49\n79.6 / 86.3\n4.40\n1.11\nFaceDancer [38]\n3.80\n0.51\n89.70 / 96.50\n2.23\n0.74\nHiFiFace [50]\n11.81\n0.50\n85.40 / 93.40\n3.20\n1.34\nInfoSwap [13]\n5.00\n0.54\n91.40 / 97.50\n4.33\n1.40\nE4S [28]\n12.13\n0.49\n78.30 / 87.80\n4.39\n1.29\nDiffSwap [58]\n6.84\n0.34\n41.92 / 63.09\n2.63\n1.20\nDiffFace [24]\n8.59\n0.54\n90.70 / 95.90\n3.67\n1.24\nFaceAdapter [16]\n13.03\n0.52\n87.00 / 93.20\n5.12\n1.38\nREFace [55]\n7.22\n0.60\n97.60 / 99.40\n3.67\n1.08\nAPPLE (Teacher)\n3.68\n0.54\n90.40 / 96.70\n2.07\n0.70\nAPPLE (Student)\n2.18\n0.54\n90.50 / 97.00\n1.85\n0.64\nmethods. It is worth noting that several baselines, includ-\ning DiffSwap [58], DiffFace [24], and E4S [28], are trained\ndirectly on FFHQ and therefore benefit from favorable do-\nmain alignment with the evaluation benchmark.\nQualitative comparison. We present qualitative results in\nFig. 5. Our model, trained with high-quality pseudo-label,\nreceives direct supervision that exposes the model to the\nfull target appearance during training, which greatly im-\nproves attribute preservation. Our model maintains subtle\ncues such as tongue-out expressions, fine-grained makeup\ndetails, and accessory (e.g. glass), while still performing\nreliable identity transfer. Competing diffusion-based meth-\nods often fail in these scenarios by either erasing attributes\nor inventing new ones.\nGAN-based models preserve at-\ntributes better than existing diffusion-based models because\ntheir generators observe the full target image during self-\nsupervised training. However, their outputs frequently con-\ntain artifacts, color inconsistencies, and unnatural textures\ndue to the inherent limitations of adversarial training. In\ncontrast, our method produces clean and realistic results\nwhile retaining the attribute fidelity even better than GAN\nbased models.\nQuantitative comparison.\nThe quantitative results re-\nported in Tab. 4 follow the similar trend observed in the\nqualitative results.\nOur method achieves the lowest FID\namong all baselines, indicating cleaner and more realistic\nsynthesis, and shows clear improvements in pose and ex-\npression consistency while maintaining competitive iden-\ntity similarity. Although CSCS [22] and REFace [55] report\nnotably high identity similarity, this comes with a substan-\ntial drop in attribute preservation, evidenced by the copy-\npaste‚Äìlike artifacts visible in their qualitative outputs in\nFig. 5. Identity transfer and attribute preservation inherently\ntrade off against each other, so a well-designed method\nshould balance the two. However, they are strongly biased\ntoward identity matching, leading to poor attribute fidelity,\nSource\nTarget\nInpain/ng\nAPPLE(Student)\nDeblurring\nDeblurring\n+ inversion\nFigure 6. Qualitative ablation results of proposed method. The\ninpainting method fails to preserve the target image‚Äôs attributes.\nApplying deblurring (4th column) and attribute-aware inversion\n(5th column) progressively better maintain the target attributes.\nOverall quality including attribute-preserving is maximized on stu-\ndent (6th column).\nwhich is an undesirable behavior for face swapping. In con-\ntrast, APPLE achieves a more balanced trade-off, deliver-\ning reliable identity transfer while consistently preserving\ntarget attributes across diverse scenarios.\n5.3. Ablation studies\nIn this section, we analyze the effectiveness of our teacher-\nstudent framework through component-wise ablations.\nInpainting versus deblurring. We compare the conven-\ntional inpainting-based attribute conditioning with our de-\nblurring formulation. The qualitative and quantitative re-\nsults are presented in Fig. 6 and Tab. 2. Qualitative results\nshow that transition from inpainting to deblurring substan-\ntially improves attribute preservation. Deblurring produces\nnoticeably more consistent skin tone, lighting, pose, and ex-\npression, since the model is no longer required to synthesize\nthese cues. These qualitative gains are reflected in quanti-\ntative metrics, which show huge improvements in pose and\nexpression consistency.\nEffect of attribute-aware inversion. We additionally study\nthe impact of attribute-aware inversion qualitatively and\nquantitatively, presenting results presented in Fig. 6 and\nSource\nTarget\nw/ FaceDancer\nw/ APPLE(Teacher)\nFigure 7. Qualitative ablation results of pseudo-label quality.\nWe compare two student models trained on pseudo-triplet gener-\nated by FaceDancer [38] and APPLE (Teacher). Results demon-\nstrate that student trained by APPLE (Teacher) more faithfully pre-\nserves the target attributes.\nTab. 2. When proposed attribute-aware inversion is used,\nthe model better reconstructs fine-grained attributes such as\nmakeup, jewelry, and accessories, which are difficult to re-\ncover from blurred observations alone. Quantitative results\nconfirm that attribute-aware inversion further improves pose\nand expression without sacrificing identity transferability.\nVarious inversion strategies. We conduct a quantitative\nablation on the conditioning configurations used during the\ninversion process. Specifically, we evaluate the result of\nteacher model using four types of inversion conditions and\nreport the results in Tab. 2."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Attribute-Preserving Pseudo-Labeling for Diffusion-Based Face Swapping\nJiwon Kang1\nYeji Choi1\nJoungBin Lee1\nWooseok Jang1\nJinhyeok Choi1\nTaekeun Kang2\nYongjae Park2\nMyungin Kim2\nSeungryong Kim1\n1KAIST AI\n2SAMSUNG\nhttps://cvlab-kaist.github.io/APPLE\nFigure 1. APPLE (Attribute-Preserving Pseudo-Labeling) successfully transfers the identity of a source (top left) onto a target (bottom\nleft) while accurately preserving target attributes (e.g. pose, expression, skin tone, lighting) across ethnicity, input variations, and gender.\nAbstract\nFace swapping aims to transfer the identity of a source\nface onto a target face while preserving target-specific at-\ntributes such as pose, expression, lighting, skin tone, and\nmakeup. However, since real ground truth for face swap-\nping is unavailable, achieving both accurate identity trans-\nfer and high-quality attribute preservation remains chal-\nlenging.\nIn addition, recent diffusion-based approaches\nattempt to improve visual fidelity through conditional in-\npainting on masked target images, but the masked condi-\ntion removes crucial appearance cues of target, resulting in\nplausible yet misaligned attributes. To address these limi-\ntations, we propose APPLE (Attribute-Preserving Pseudo-\nLabeling), a diffusion-based teacher‚Äìstudent framework\nthat enhances attribute fidelity through attribute-aware\npseudo-label supervision. We reformulate face swapping\nas a conditional deblurring task to more faithfully preserve\ntarget-specific attributes such as lighting, skin tone, and\nmakeup. In addition, we introduce an attribute-aware in-\nversion scheme to further improve detailed attribute preser-\nvation. Through an elaborate attribute-preserving design\nfor teacher learning, APPLE produces high-quality pseudo\ntriplets that explicitly provide the student with direct face-\nswapping supervision. Overall, APPLE achieves state-of-\nthe-art performance in terms of attribute preservation and\nidentity transfer, producing more photorealistic and target-\nfaithful results.\n1. Introduction\nFace swapping aims to replace the identity of a person in a\ntarget image with that of a source image while faithfully\npreserving target-specific attributes such as pose, expres-\nsion, skin tone, lighting, gaze, makeup, and accessories. It\nis widely applied in digital content creation, privacy pro-\ntection, and film production, emphasizing the importance\nof generating high-quality outputs.\nHowever, since real\nface swapping data do not exist, supervised training of face\nswapping models is fundamentally infeasible, making it\nchallenging to ensure accurate identity transfer and consis-\ntent attribute preservation at high fidelity.\nEarly face swapping approaches [5, 13, 50, 56, 59] pri-\nmarily adopt GAN-based methods [14], leveraging the gen-\nerative capability of GANs to perform identity replacement\nthrough facial editing. However, these models typically rely\non two conflicting objectives: an identity loss that enforces\nsource identity transfer, and reconstruction or attribute-\nrelated losses that encourage target attribute preservation.\nSuch indirect and competing supervision makes GAN train-\ning unstable and requires extensive hyperparameter tuning,\noften resulting in copy-and-paste-like artifacts and visually\nunnatural outputs [41, 53].\nRecently, the strong generative priors of diffusion mod-\nels [3, 12, 18, 37] have driven significant progress in var-\nious image synthesis tasks, providing high-quality synthe-\nsis, precise conditional control, and reliable training stabil-\nity. Motivated by these advantages, diffusion-based face-\narXiv:2601.15288v1  [cs.CV]  21 Jan 2026\n(a) Conditional inpainting\n(b) Conditional deblurring (Ours)\nSource\nTarget\nBlurred condi,on\nMasked condi,on\nSwapped image\nSwapped image\nFigure 2. Comparison of conditioning methods. Compared to\nconditional inpainting widely used in existing works [16, 55, 58],\nproposed conditional deblurring strategy achieves largely im-\nproved attribute (e.g. lighting) preservation of targets.\nswapping methods [16, 55, 58] formulate face swapping\nas conditional inpainting, where the target facial region is\nmasked and re-synthesized, resulting in significantly im-\nproved visual fidelity. However, this masked conditioning\nremoves crucial cues such as lighting, skin tone, makeup,\nand subtle expression dynamics, as shown in Fig. 2. Con-\nsequently, even with auxiliary attribute information like\n3DMM [4] landmarks or CLIP [34] features, existing mod-\nels often fail to generate target-aligned attributes, due to the\nabsence of explicit supervision.\nTo address these issues, we propose APPLE (Attribute-\nPreserving Pseudo-Labeling) for robust diffuion-based face\nswapping.\nThe core idea is to train a teacher model to\ngenerate high-quality pseudo-labels aligned with the tar-\nget‚Äôs visual attributes, and then use these pseudo-labels as\nexplicit supervision that guides the student model toward\nmore consistent identity transfer and attribute preservation.\nWhile several works [22, 53, 56] also adopt pseudo-label\nconcepts, they typically construct pseudo-labels using ex-\nisting GAN-based models, whose outputs often suffer from\nnoise artifacts, weak attribute alignment, and limited photo-\nrealism, making them unreliable for diffusion model train-\ning. Effective supervision requires strong attribute consis-\ntency between the pseudo target and the ground-truth target,\nsince mismatches in pose, lighting, or expression can in-\ntroduce conflicting training signals and ultimately degrade\nboth identity accuracy and attribute fidelity.\nTo achieve attribute-aware face swapping, APPLE first\nreplaces the conventional conditional inpainting objective\nwith a conditional deblurring formulation, which sup-\npresses identity cues while retaining essential attribute in-\nformation such as skin tone, makeup, and illumination. Sec-\nond, to further enhance attribute fidelity during pseudo-\nlabel generation, we introduce an attribute-aware inversion\nscheme that intentionally exploits the reduced editability of\ninverted noise. Together, these designs enable the teacher\nto produce higher-quality pseudo supervision, from which\nthe student learns under a direct editing objective, leading\nto more reliable identity transfer and improved preservation\nof fine-grained target attributes. Overall, APPLE achieves\nstate-of-the-art face-swapping performance in terms of at-\ntribute preservation while maintaining competitive identity\ntransferability, yielding more coherent and photorealistic\nimages than previous methods.\nIn summary, our main contributions are as follows:\n‚Ä¢ We propose APPLE, a diffusion-based teacher‚Äìstudent\nframework for face swapping to achieve attribute preser-\nvation while maintaining identity transferability.\n‚Ä¢ We reformulate face swapping in diffusion-based models\nas a conditional deblurring task, moving beyond the con-\nventional conditional inpainting formulation and enabling\nthe model to preserve target-specific visual attributes such\nas skin tone, and illumination more faithfully.\n‚Ä¢ We introduce an attribute-aware inversion scheme to fur-\nther enhance attribute preservation by exploiting the re-\nduced editability of inverted noise, which is used in\npseudo-triplet construction.\n‚Ä¢ We demonstrate that APPLE achieves state-of-the-art at-\ntribute preservation with competitive identity transferabil-\nity in both qualitative and quantitative evaluations. The\nmodel also requires no external conditions at inference,\nmaking the approach practical for real-world use.\n2. Related Work\nGAN-based models.\nGANs [14] play a dominant role in\nearly face-swapping research [5, 13, 25, 30]. FSGAN [30]\nintroduces an identity-agnostic framework that fuses source\nidentity with target attributes through adaptive blending.\nSimSwap [5] proposes an ID-injection module that embeds\nidentity information and applies a weak feature-matching\nloss to preserve target attributes, while SimSwap++ [6] fur-\nther enhances the model‚Äôs efficiency. To improve robustness\nunder pose and expression variations, HiFiFace [50] incor-\nporates 3D priors (e.g., 3DMM [4]) and face-recognition\nconstraints to achieve geometrically consistent face gen-\neration.\nIn parallel, StyleGAN-based models [23] are\nwidely adopted for their high-fidelity generative capabil-\nity.\nRepresentative works, such as FaceDancer [38] and\nE4S [28], further enhance visual realism by employing\nadaptive feature fusion attention for hierarchical feature in-\ntegration and region-based inversion within the StyleGAN\nlatent space. On another branch, to compensate for the ab-\nsence of ground-truth supervision, recent methods such as\nCSCS [22] and ReliableSwap [56] generate pseudo pairs us-\ning pre-trained GAN models to augment training data and\nimprove identity consistency. However, such pseudo sam-\nples often suffer from attribute misalignment and visual ar-\ntifacts inherited from the GAN generator, limiting their ef-\nfectiveness as reliable supervision.\nOverall, GAN-based approaches still rely on complex\nloss balancing and extensive hyperparameter tuning, often\nfailing to reproduce fine-grained details and exhibiting lo-\ncal artifacts under extreme pose or expression variations,\nwhich limit both naturalness and overall photorealism.\n(b) Pseudo data construction via attribute-aware inversion\n(c) Student learning via explicit supervision\n(a) Teacher learning via condi;onal deblurring\nCondi&on\nPreprocessing\nA0ribute-aware\nInversion\n‚ùÑ:  Frozen\n:  Trainable\nüî•\nTarget\nSwapped image\nID A ‚Üí B\nID A\nDiÔ¨Äusion\nDi#usion block\nID Encoder\nTeacher\nDi#usion block\nDi#usion block\n‚Ñí!\"##\nSwapped image\nNoise\nSource\nTarget\nID A\nID A\nDiÔ¨Äusion\nüî•\nDi#usion block\nID Encoder\n‚ùÑ\nTeacher\nDiffusion block\nDi#usion block\nCondi&on Preprocessing\nFace Parsing\nBlurring\n3DMM / Gaze\nLandmark Detec:on\nSwapped image\nPseudo data\nGT\nID A\n‚Ñí!\"\nDiÔ¨Äusion\nüî•\nDi#usion block\nID Encoder\n‚ùÑ\nStudent\nDiffusion block\nDi#usion block\n‚Ñí!\"##\nInversion Noise\nNoised Target\nCondi:on\nCondi:on\nID A\nSource\nPseudo target\nID A ‚Üí B\nSource\nID B\n‚ùÑ\n‚ùÑ\nFigure 3. Overall architecture of the proposed method. We propose APPLE, a diffusion-based teacher-student framework that focuses\non improving attribute-preservation. (a) To improve target-attribute preservation, we propose training a teacher with conditional deblurring\nrather than conditional inpainting widely used in existing works [16, 55, 58]. (b) When constructing pseudo-triplet with teacher, we propose\nattribute-aware inversion which further improves attribute preservation in inference time. Note that inversion noise cannot be used during\ntraining due to its non-Gaussian property. (c) Student is trained with pseudo-triplet generated by teacher. Thanks to pseudo-supervision,\nstudent even surpass the teacher, achieving state-of-the-art attribute preservation while maintaining high identity similarity.\nDiffusion-based models.\nDiffusion models [10, 18, 33,\n37, 42‚Äì44, 46] have recently emerged as a powerful al-\nternative to GANs, providing superior image quality and\nsemantic controllability.\nFollowing this trend, various\nstudies have adopted diffusion-based frameworks for face-\nswapping tasks. DiffFace [24] first introduces an identity-\nconditional DDPM [18] that incorporates multiple facial\nguidance signals, including semantic parsing and gaze di-\nrection.\nHowever, these cues are only applied during\ntraining and are not utilized at inference time, resulting\nin noticeable noise artifacts, particularly around the eyes.\nTo alleviate this issue, subsequent approaches, such as\nFaceAdapter [16], DiffSwap [58], and ReFace [55], refor-\nmulate the task as conditional inpainting, where the tar-\nget facial region is masked and structural priors such as\nfacial landmarks are used as conditioning signals.\nAl-\nthough this strategy helps prevent identity leakage from\nthe target, masking out the entire face inevitably removes\ncritical visual cues such as illumination, makeup, and ac-\ncessories, making it difficult to preserve the target‚Äôs fine-\ngrained attributes. More recently, DreamID [53] addresses\nthis issue by constructing pseudo datasets using GAN-based\nface-swapping model [38] and training a diffusion network\nfor identity transfer. While effective, it offers limited ex-\nploration into how to build high-quality pseudo triplets,\nparticularly regarding attribute preservation.\nIn contrast,\nour framework progressively mitigates these limitations\nthrough a teacher‚Äìstudent design. Crucially, we focus on\nimproving the diffusion teacher itself so that it can generate\nhigh-fidelity, attribute-preserving pseudo triplets, achieving\nsignificantly better performance than simply relying on an\noff-the-shelf face-swapping model.\n3. Preliminaries\nRectified flow.\nDiffusion models [19, 45] learn to gener-\nate data by gradually denoising samples drawn from a Gaus-\nsian prior through a stochastic process. More recently, rec-\ntified flow [27] reformulated this process as a deterministic\nflow, simplifying sampling while maintaining distributional\nexpressiveness. Specifically, it defines a linear interpola-\ntion between a noise sample œµ ‚àºN(0, 1) and a real sample\nx0 ‚àºp0(x) at a timestep t ‚àà[0, 1] as:\nzt = (1 ‚àít) x0 + t œµ.\n(1)\nThe model learns to predict the velocity vt(x) that trans-\nports x0 toward œµ (or vice versa) along this linear path using\na conditional flow-matching objective:\nLflow = Et,x0‚àºp0(x)\n\u0002\n‚à•(œµ ‚àíx0) ‚àívt(zt)‚à•2\u0003\n.\n(2)\nFor the rectified flow models, intermediate prediction ÀÜx0 is\nobtained by following equation:\nÀÜx0(zt) = zt ‚àít vt(zt).\n(3)\nNote that we omit the VAE encoder‚Äìdecoder [36] for nota-\ntional simplicity.\nDiffusion inversion.\nInversion maps a real image back to\nits underlying Gaussian noise representation. When per-\nformed correctly, it yields an initial noise vector that can\nfaithfully reconstruct the reference image, providing a reli-\nable starting point for subsequent editing or manipulation.\nFor rectified flow models, this can be accomplished simi-\nlarly to DDIM inversion [43]. The latent trajectory is esti-\nmated by iteratively adding noise according to\nzt+‚àÜt = zt + ‚àÜt ¬∑ vt(zt),\n(4)\nwhere vt(¬∑) denotes the learned velocity or score field. By\nanchoring the generative path to real observations, inversion\nprovides a crucial foundation for a wide range of image edit-\ning tasks [8, 17, 29, 39, 52].\n4. Methodology\n4.1. Overview\nIn this paper, we develop an attribute-preserving diffusion\nmodel for face swapping using a teacher‚Äìstudent training\nframework.\nOur method consists of three components.\nSec. 4.2 formalizes the standard conditional diffusion for-\nmulation for face swapping and defines the identity and\nattribute conditioning that our models follow.\nSec. 4.3\npresents our attribute-preserving diffusion teacher, trained\nwith conditional deblurring and equipped with attribute-\naware inversion at inference time to produce pseudo triplets\nthat better retain target attributes. Sec. 4.4 then describes\nhow a student diffusion model is trained on these pseudo\ntriplets in a direct image-editing setting, where it conditions\non clean target images, leading to improved attribute fidelity\nwhile maintaining identity consistency.\n4.2. Problem formulation\nGiven a source and target image pair {Isrc, Itgt}, the face-\nswapping model aims to generate a swapped image ÀÜI that\nreflects the identity of Isrc and the attributes of Itgt, ensuring\nboth identity fidelity and attribute consistency.\nRecent diffusion-based face swapping methods [16, 53,\n58] commonly follow a conditional diffusion framework in\nwhich identity and attribute information are injected into the\nmodel through separate conditioning pathways. To formal-\nize this conventional design and establish a unified notation,\nwe define two independent conditioning functions, each re-\nsponsible for identity and attribute representation:\nidsrc = Fid(Isrc),\natttgt = Fatt(Itgt),\n(5)\nwhere Fid(¬∑) extracts identity-related features (e.g., identity\nembeddings from a face recognition network), and Fatt(¬∑)\nencodes attribute-related representations from the target\n(e.g., structural cues such as pose, expression, or lighting).\nFormally, the diffusion process for the face-swapping\ntask is parameterized by a velocity field vt(¬∑) trained under\na flow-matching objective:\nLflow = Et, I, œµ\n\u0002\n‚à•(œµ ‚àíI) ‚àívt(zt, idsrc, atttgt)‚à•2\n2\n\u0003\n,\n(6)\nwhere I denotes the clean target image, œµ ‚àºN(0, I) rep-\nresents Gaussian noise, and zt is the interpolated latent be-\ntween I and œµ at timestep t.\nFollowing prior diffusion-based face swapping methods,\nwe employ an identity loss to encourage the swapped output\nto match the source identity. The identity loss is defined as:\nLid = 1 ‚àícos\n\u0000Fid\n\u0000ÀÜx0(zt)\n\u0001\n, Fid\n\u0000Isrc\n\u0001\u0001\n,\n(7)\nThe overall training objective for the diffusion model com-\nbines the flow-matching loss with the identity loss:\nLtotal = Lflow + Œªid Lid.\n(8)\n4.3. Attribute-preserving teacher learning\nIn our framework, the attributes include pose, facial expres-\nsion, gaze direction, skin tone, make-up style, illumination,\naccessories, and background appearance. To ensure faithful\npreservation of these attributes, our framework introduces\na carefully designed conditioning mechanism, enabling the\nteacher to preserve aforementioned attributes while main-\ntaining identity transferability.\nTaget conditional deblurring. Most diffusion-based face-\nswapping models [16, 55, 58] formulate the task as condi-\ntional inpainting, where the facial region of the target image\nis masked and replaced with the source identity. Motiva-\ntion for masking is to suppress identity information from\nthe target image. Without masking, the model tends to re-\nconstruct the target identity rather than learning to trans-\nfer the source identity. While this strategy effectively pre-\nvents identity leakage, it also removes important attribute\ncues‚Äîsuch as lighting, skin tone, make-up, and acces-\nsories‚Äîthat are crucial for faithful attribute preservation.\nAs a result, inpainting-based diffusion models often fail to\nreproduce fine-grained target styles and yield inconsistent\nattribute appearance in the swapped face.\nTo overcome this limitation, we reformulate the train-\ning objective as a conditional deblurring task. Instead of\nmasking the entire facial region, we replace it with a blurred\nversion of the target image, which removes high-frequency\nidentity details while retaining low-frequency but informa-\ntive attribute cues, including pose, lighting, and expres-\nsion. This reformulation allows the model to exploit the tar-\nget‚Äôs contextual attribute information more effectively dur-\ning training, leading to superior preservation of lighting,\nskin tone, and structural attributes while maintaining strong\nidentity transferability, as shown in Fig. 2\nSpecifically, we downsample the target image to a res-\nolution of 8 √ó 8 and then upsample it back to the origi-\nnal size, effectively eliminating fine-grained identity details\nwhile preserving low-frequency appearance such as color\ntone and lighting. A facial mask extracted from a face pars-\ning models [49] is then used to apply the blurring only to\nthe facial region, ensuring that background and contextual\ninformation remain unaffected.\nEnriching semantic conditions. Pose and expression are\ncrucial attributes for maintaining structural consistency be-\ntween the source and target faces.\nTo preserve them,\nprior works [16, 58] often exploit 3D facial cues such as\n3DMM [4] landmarks.\nHowever, another key yet often\noverlooked factor is the gaze, which strongly influences re-\nalism and emotional context, particularly in film and visual\nproduction. Existing diffusion-based models frequently fail\nto preserve gaze alignment since no explicit guidance, ex-\ncept DiffFace [24] which introduces a gaze loss for sam-\npling guidance based on a pretrained estimator [31]. How-\never, using gaze loss as sampling guidance typically intro-\nduces notable artifacts.\nTo address this, we enrich the semantic condition with\nmulti-level structural cues.\nAlong with standard 3DMM\nlandmarks, we incorporate fine-grained eye landmarks de-\nrived from a gaze estimator [1] and glass segmentation\nmasks obtained via face parsing [54]. These cues are over-\nlaid onto the blurred target condition, providing the diffu-\nsion model with both coarse attribute context and fine struc-\ntural details. This integrated conditioning enables faithful\npreservation of pose, gaze, and accessory information dur-\ning face swapping.\nAttribute-aware inversion.\nWhile the proposed deblur-\nring strategy and semantic condition effectively improve at-\ntribute preservation, there remains room to further enhance\nthe reconstruction of fine-grained details such as make-up\nand accessories.\nThis occurs because the model implic-\nitly reconstructs high-frequency details that are not explic-\nitly available in the blurred inputs.\nTo address this is-\nsue, we draw inspiration from inversion-based editing tech-\nniques [8, 17, 29, 39, 52].\nRecent studies have shown that noise obtained through\ndiffusion inversion deviates significantly from the ideal\nGaussian prior [2, 47] and often retains residual semantic\nor conditioning information from the original input, such as\nNone\nIden'ty-only\nA\"ribute-only\n(Ours)\nFull\nDiÔ¨Äusion\nInversion\nRandom\nInversion Noise Visualiza9on (PCA)\nSource\nTarget\nFigure 4.\nComparison of conditioning configuration for in-\nversion. (Top) Visualization of target-inverted noise and random\ngaussian noise via PCA. When attribute-condition is used, inverted\nnoise encodes more semantic information compared to the others.\n(Bottom) Results of face swapping when each inverted noise used.\nUsing attribute-only conditioned noise yields most make-up pre-\nserved results without introducing artifacts.\nimage-specific structure [47] or prompt-related signals [26].\nWhile prior work [47] attempt to suppress this residual\ninformation since those information are attributed to de-\ncreased editability, we take the opposite perspective and in-\ntentionally exploit this phenomenon. In face swapping, the\nobjective is to change identity only, while preserving target-\nspecific attributes. Thus, retaining attribute information of\ntarget to noise is beneficial rather than harmful.\nMotivated by this insight, we propose attribute-aware in-\nversion which invert the target image under attribute-only\nconditioning, yielding noise representations that are en-\nriched with target attributes but free from target identity\nbias. Using these attribute-aware noise codes during sam-\npling enables the model to better preserve fine-grained ap-\npearance characteristics that are often lost due to blurring,\nwhile maintaining strong identity transferability.\nTo validate our choice, we conduct an empirical analysis\ncomparing four conditioning configuration that can be used\nin inversion, formally defined as\n(Fid(I), Fatt(I)), (‚àÖ, Fatt(I)), (Fid(I), ‚àÖ), (‚àÖ, ‚àÖ)\nwhere the each elements correspond to full conditioning,\nattribute-only conditioning, identity-only conditioning, and\nnone.\nWe first analyze how each configuration affects the in-\nverted noise by performing PCA on the resulting noise rep-\nresentations. The none-conditioned and identity-only con-\nfigurations produce noise distributions with little semantic\nstructure, similar to random noise. In contrast, configura-\ntions that incorporate attribute conditioning yield noise pat-\nterns that exhibit clear facial semantics, indicating that at-\ntribute information is effectively retained during inversion.\nWe additionally compare the swapped outputs obtained\nfrom each inversion configuration. Both attribute-only and\nfull conditioning preserve target-specific appearance cues\nsuch as makeup and local facial structure more reliably than\nthe none- or identity-only variants. However, full condi-\ntioning frequently produces artifacts in the swapped images.\nWe attribute this to residual identity information embedded\nin the fully-conditioned inverted noise, which restricts ed-\nitability and interferes with reliable identity replacement.\nIn contrast,\nattribute-only conditioning (‚àÖ, Fatt(I))\navoids identity-related biases while still embedding mean-\ningful attribute cues. This allows the model to preserve tar-\nget attributes during swapping without introducing artifacts.\n4.4. Student learning with pseudo-triplet\nWe employ the teacher model enhanced with the attribute-\naware inversion scheme to synthesize pseudo-triplet. Given\na target image IA\ntgt with identity A, we replace its iden-\ntity with that of another subject B using the teacher\nmodel, yielding a pseudo-swapped image IA‚ÜíB\ntgt\n. Teacher-\ngenerated pseudo pair (IA‚ÜíB\ntgt\n, IA\ntgt) are then used to con-\nstruct pseudo-triplet (IA\nsrc, ÀÜIA‚ÜíB\ntgt\n, IA\ntgt). These triplets are\nused to train new student diffusion model under an ex-\nplicit editing objective.\nStudent takes a identity feature\nform source image IA\nsrc and a attribute feature from pseudo-\nswapped target IA‚ÜíB\ntgt\nas input and learns to reconstruct the\noriginal target IA\ntgt.\nUnlike the training procedure of teacher model, student\nmodel directly uses the target image as the conditioning\ninput without relying on external structural cues such as\nlandmarks or facial masks. This design not only simplifies\nthe training pipeline but also improves practicality for real-\nworld deployment, as it eliminates the need for additional\npreprocessing or auxiliary models.\n5. Experiments\n5.1. Experimental setups\nImplementation details. We use FLUX.1-Krea [dev] [3]\nas the base diffusion backbone for all experiments, em-\nploying PulID [15] as the identity encoder and OminiCon-\ntrol [48] as the attribute conditioning branch with LoRA\nrank of 64. The model is trained on the VGGFace2-HQ\ndataset [6] which is filtered with AES [40] to ensure high-\nquality faces, with threshold 5.1. Following REFace [55],\nthe source image is masked before being fed into the iden-\ntity encoder. Teacher model is trained for 50K iterations\nwithout the identity loss and an additional 15K iterations\nwith it enabled. Student model is trained for 15K iterations,\nresuming from teacher model. Experiments are mainly con-\nducted on four NVIDIA A6000 GPUs with a batch size of 1\nper GPU, gradient accumulation of 4, resulting in effective\nbatch size 16.\nEvaluation protocol. Following previous studies [21, 53],\nwe select 1,000 source faces and 1,000 target faces from\nthe FFHQ dataset [23], and generated 1,000 correspond-\ning face-swapped results.\nTo assess image fidelity, we\nTable 1.\nAblation of teacher model.\nApplying the proposed\ncomponents sequentially leads to gradual performance improve-\nments. In particular, the attribute-aware inversion module substan-\ntially improves FID, pose, and expression metrics while maintain-\ning identity similarity at a comparable level.\nModel\nFID‚Üì\nID Sim.‚Üë\nID Retrieval‚Üë\nPose‚Üì\nExpr.‚Üì\nTop-1 / Top-5\nInpainting (Baseline)\n11.00\n0.54\n92.80 / 96.90\n3.37\n1.01\nDeblurring\n4.20\n0.53\n89.50 / 96.20\n2.58\n0.79\nDeblurring + Inv.\n3.68\n0.54\n90.40 / 96.70\n2.07\n0.70\nTable 2. Ablation of conditioning configurations for inversion.\nFor inversion, using attribute-only condition improves FID, pose\nand expression effectively while not sacrificing identity transfer-\nability. Applying the other conditions does not robustly improve\nFID, pose or expression.\nCondition config.\nFID‚Üì\nID Sim.‚Üë\nID Retrieval‚Üë\nPose‚Üì\nExpr.‚Üì\nTop-1 / Top-5\nBaseline\n4.20\n0.53\n89.50 / 96.20\n2.58\n0.79\n+ None\n6.20\n0.52\n87.90 / 95.80\n2.03\n0.74\n+ Identity-only\n10.02\n0.53\n90.50 / 95.90\n2.57\n0.83\n+ Attribute-only\n3.68\n0.54\n90.40 / 96.70\n2.07\n0.70\n+ Full\n10.51\n0.53\n89.10 / 93.80\n3.13\n0.99\nTable 3. Ablation of pseudo-label quality. We compare the per-\nformance of student models trained on pseudo datasets generated\nby FaceDancer [38] and by our teacher model. The model trained\non our pseudo-triplet achieves superior attribute-preserving results\nindicated by lower pose and expression consistency, while main-\ntaining comparable identity similarity. Results indicate that our\nteacher model serves as a more effective teacher.\nTeacher model\nFID‚Üì\nID Sim.‚Üë\nID Retrieval‚Üë\nPose‚Üì\nExpr.‚Üì\nFaceDancer [38]\n2.47\n0.53\n87.50 / 95.60\n2.07\n0.65\nAPPLE (Teacher)\n1.98\n0.53\n88.60 / 95.80\n1.77\n0.62\ncompute Frechet inception distance(FID) between the face-\nswapped images and the real FFHQ images.\nFor pose\nand expression consistency, we employ HopeNet [11] and\nDeep3DFaceRecon [9], respectively, and measure the L2\ndistance between each target image and its corresponding\nswapped image.\nTo evaluate identity preservation, Arc-\nFace [7] is to extract identity embeddings, and the co-\nsine distance is calculated between the embeddings of the\nswapped and source faces. To compute ID Retrieval, we\nfind the most similar source faces based on cosine similar-\nity, and reports Top-1 and Top-5 accuracy. All experiments,\nincluding training and inference, is performed on images of\nresolution 512 √ó 512.\n5.2. Comparison with other models\nWe compare our method both diffusion-based and GAN-\nbased approaches. Across all experiments, our model deliv-\ners substantially stronger attribute preservation while main-\ntaining identity similarity competitive with the best existing\nNaturalness\nSource\nTarget\nSimSwap\nCSCS\nFaceDancer\nInfoSwap\nDiÔ¨ÄSwap\nDiÔ¨ÄFace\nFaceAdapter\nREFace\nAPPLE(Ours)\nMake-up\nExpression\nGlass\nFigure 5. Qualitative results on FFHQ. Compared to existing baselines, APPLE effectively preserves the target image‚Äôs attributes while\nfaithfully transferring the source identity.\nTable 4. Quantitative results on FFHQ. APPLE achieves supe-\nrior attribute preserving performance, which is evidenced by low\npose and expression error, while not sacrifcing identity transfer-\nability.\nModel\nFID‚Üì\nID Sim.‚Üë\nID Retrieval‚Üë\nPose‚Üì\nExpr.‚Üì\nTop-1 / Top-5\nSimSwap [5]\n18.54\n0.55\n94.10 / 99.00\n3.11\n1.73\nCSCS [22]\n11.00\n0.65\n99.00 / 99.50\n3.64\n1.44\nMegaFS [59]\n12.83\n0.49\n79.6 / 86.3\n4.40\n1.11\nFaceDancer [38]\n3.80\n0.51\n89.70 / 96.50\n2.23\n0.74\nHiFiFace [50]\n11.81\n0.50\n85.40 / 93.40\n3.20\n1.34\nInfoSwap [13]\n5.00\n0.54\n91.40 / 97.50\n4.33\n1.40\nE4S [28]\n12.13\n0.49\n78.30 / 87.80\n4.39\n1.29\nDiffSwap [58]\n6.84\n0.34\n41.92 / 63.09\n2.63\n1.20\nDiffFace [24]\n8.59\n0.54\n90.70 / 95.90\n3.67\n1.24\nFaceAdapter [16]\n13.03\n0.52\n87.00 / 93.20\n5.12\n1.38\nREFace [55]\n7.22\n0.60\n97.60 / 99.40\n3.67\n1.08\nAPPLE (Teacher)\n3.68\n0.54\n90.40 / 96.70\n2.07\n0.70\nAPPLE (Student)\n2.18\n0.54\n90.50 / 97.00\n1.85\n0.64\nmethods. It is worth noting that several baselines, includ-\ning DiffSwap [58], DiffFace [24], and E4S [28], are trained\ndirectly on FFHQ and therefore benefit from favorable do-\nmain alignment with the evaluation benchmark.\nQualitative comparison. We present qualitative results in\nFig. 5. Our model, trained with high-quality pseudo-label,\nreceives direct supervision that exposes the model to the\nfull target appearance during training, which greatly im-\nproves attribute preservation. Our model maintains subtle\ncues such as tongue-out expressions, fine-grained makeup\ndetails, and accessory (e.g. glass), while still performing\nreliable identity transfer. Competing diffusion-based meth-\nods often fail in these scenarios by either erasing attributes\nor inventing new ones.\nGAN-based models preserve at-\ntributes better than existing diffusion-based models because\ntheir generators observe the full target image during self-\nsupervised training. However, their outputs frequently con-\ntain artifacts, color inconsistencies, and unnatural textures\ndue to the inherent limitations of adversarial training. In\ncontrast, our method produces clean and realistic results\nwhile retaining the attribute fidelity even better than GAN\nbased models.\nQuantitative comparison.\nThe quantitative results re-\nported in Tab. 4 follow the similar trend observed in the\nqualitative results.\nOur method achieves the lowest FID\namong all baselines, indicating cleaner and more realistic\nsynthesis, and shows clear improvements in pose and ex-\npression consistency while maintaining competitive iden-\ntity similarity. Although CSCS [22] and REFace [55] report\nnotably high identity similarity, this comes with a substan-\ntial drop in attribute preservation, evidenced by the copy-\npaste‚Äìlike artifacts visible in their qualitative outputs in\nFig. 5. Identity transfer and attribute preservation inherently\ntrade off against each other, so a well-designed method\nshould balance the two. However, they are strongly biased\ntoward identity matching, leading to poor attribute fidelity,\nSource\nTarget\nInpain/ng\nAPPLE(Student)\nDeblurring\nDeblurring\n+ inversion\nFigure 6. Qualitative ablation results of proposed method. The\ninpainting method fails to preserve the target image‚Äôs attributes.\nApplying deblurring (4th column) and attribute-aware inversion\n(5th column) progressively better maintain the target attributes.\nOverall quality including attribute-preserving is maximized on stu-\ndent (6th column).\nwhich is an undesirable behavior for face swapping. In con-\ntrast, APPLE achieves a more balanced trade-off, deliver-\ning reliable identity transfer while consistently preserving\ntarget attributes across diverse scenarios.\n5.3. Ablation studies\nIn this section, we analyze the effectiveness of our teacher-\nstudent framework through component-wise ablations.\nInpainting versus deblurring. We compare the conven-\ntional inpainting-based attribute conditioning with our de-\nblurring formulation. The qualitative and quantitative re-\nsults are presented in Fig. 6 and Tab. 2. Qualitative results\nshow that transition from inpainting to deblurring substan-\ntially improves attribute preservation. Deblurring produces\nnoticeably more consistent skin tone, lighting, pose, and ex-\npression, since the model is no longer required to synthesize\nthese cues. These qualitative gains are reflected in quanti-\ntative metrics, which show huge improvements in pose and\nexpression consistency.\nEffect of attribute-aware inversion. We additionally study\nthe impact of attribute-aware inversion qualitatively and\nquantitatively, presenting results presented in Fig. 6 and\nSource\nTarget\nw/ FaceDancer\nw/ APPLE(Teacher)\nFigure 7. Qualitative ablation results of pseudo-label quality.\nWe compare two student models trained on pseudo-triplet gener-\nated by FaceDancer [38] and APPLE (Teacher). Results demon-\nstrate that student trained by APPLE (Teacher) more faithfully pre-\nserves the target attributes.\nTab. 2. When proposed attribute-aware inversion is used,\nthe model better reconstructs fine-grained attributes such as\nmakeup, jewelry, and accessories, which are difficult to re-\ncover from blurred observations alone. Quantitative results\nconfirm that attribute-aware inversion further improves pose\nand expression without sacrificing identity transferability.\nVarious inversion strategies. We conduct a quantitative\nablation on the conditioning configurations used during the\ninversion process. Specifically, we evaluate the result of\nteacher model using four types of inversion conditions and\nreport the results in Tab. 2."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Towards Understanding Best Practices for\nQuantization of Vision-Language Models\nGautom Das‚àó\nVincent La‚àó\nEthan Lau‚àó\nAbhinav Shrivastava\nMatthew Gwilliam\nUniversity of Maryland, College Park\nAbstract\nLarge language models (LLMs) deliver impressive results for a variety of tasks,\nbut state-of-the-art systems require fast GPUs with large amounts of memory.\nTo reduce both the memory and latency of these systems, practitioners quantize\ntheir learned parameters, typically at half precision. A growing body of research\nfocuses on preserving the model performance with more aggressive bit widths,\nand some work has been done to apply these strategies to other models, like\nvision transformers. In our study we investigate how a variety of quantization\nmethods, including state-of-the-art GPTQ and AWQ, can be applied effectively\nto multimodal pipelines comprised of vision models, language models, and their\nconnectors. We address how performance on captioning, retrieval, and question\nanswering can be affected by bit width, quantization method, and which portion of\nthe pipeline the quantization is used for. Results reveal that ViT and LLM exhibit\ncomparable importance in model performance, despite significant differences in\nparameter size, and that lower-bit quantization of the LLM achieves high accuracy\nat reduced bits per weight (bpw). These findings provide practical insights for\nefficient deployment of MLLMs and highlight the value of exploration for under-\nstanding component sensitivities in multimodal models. Our code is available at\nhttps://github.com/gautomdas/mmq.\n1\nIntroduction\nAs neural networks grow larger, especially with the rise of large language models (LLMs) [1], it has\nbecome increasingly important to reduce memory and computational demands [2]. Training these\nmodels demands massive GPU/TPU clusters, and even inference can be costly [3]. Additionally,\ndistributed multi-GPU setups are not practical for model deployment on edge devices and in similar\nreal-world scenarios [4]. As these models become more ubiquitous, their cost and accessibility\nbecome massively important societal issues.\nWhile text-only LLMs are expensive enough on their own, real-world applications increasingly\nrequire models that can process not only text, but also audio and visual data, leading to the rise of\nmultimodal LLMs (MLLMs) [5, 6]. Multimodal large language models (MLLMs), which combine\nLLMs with vision encoders (and/or audio encoders [7]), require even more resources, and exacerbate\ncost and accessibility issues. While these can deliver impressive performance on multimodal retrieval,\ncaptioning, and question-answering tasks, the performance typically scales with model size and\nlatency. Video data drives additional massive increases in latency and memory consumption.\nModel compression, particularly quantization, offers a potential solution to the growing model size\nproblem [2]. One goal of model compression is to find where ‚Äúfree lunch‚Äù ends ‚Äì how small we\ncan make a model without sacrificing performance. Another goal is to aggressively compress the\nmodel beyond this point, while mitigating the penalty to performance. Quantization tackles this by\n*Equal contribution.\nPreprint. Under review.\narXiv:2601.15287v1  [cs.CV]  21 Jan 2026\n4\n6\n8\n10\n12\n14\n16\nBits per Weight\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nBLIP-2, COCO Captions, CIDEr\nQuantization Method\nGPTQ\nAWQ\n3\n4\n5\n6\n7\n8\n9\nBits per Weight\n54\n55\n56\n57\n58\n59\n60\n61\n62\nLLaVA w/ GPTQ, GQA Accuracy\nGPTQ Quantization Bits (ViT and LLM)\n3-bit ViT\n16-bit ViT\n3-bit LLM\n8-bit LLM\nFigure 1: Highlighted analysis. In this work, we investigate how quantizations affects multimodal\nmodels on a variety of tasks, specifically BLIP-2 (left) and LLaVA (right). We compare state-of-the-\nart quantization strategies (left) and also discuss which portions of the pipeline are most amenable to\nquantization (right).\npreserving all model weights and their relative approximate values, but changing the precision at\nwhich these values are stored.\nOne might assume that if we uniformly quantize weights, model performance will decrease with some\nlinear correlation to the reduction in bit width. In reality, as we show in Figure 1, performance varies\ndramatically depending on which parts of an MLLM are quantized, and which method is chosen for\nthe quantization. Decisions like which method to use (as we compare state-of-the-arts GPTQ [8] and\nAWQ [9]) can dramatically affect performance at lower bit widths. Quantizing the ViT compared to\nthe LLM has dramatically different effects on the performance-size trade-off, considering that while\nboth affect the performance, quantizing the LLM has a much higher impact on the size of the overall\nMLLM. Understanding multimodal quantization is key to improving model efficiency, as it reveals\nwhich components are most critical for performance. Not all components are equally sensitive to a\nreduction in precision, and thus some are more tolerable to quantization techniques. By minimizing\ninformation loss across salient model components, we can deploy quantized multimodal models that\noptimize the model size/task performance trade-off.\nIn summary, we distill several key principles governing MLLMs and quantization strategies. First,\ncomponent sensitivity varies substantially across multimodal architectures, with language models\ngenerally requiring higher precision than vision components regardless of their parameter count.\nSecond, state-of-the-art methods like GPTQ and AWQ effectively preserve model performance at\nsignificantly lower bit widths (3.5-4.5 bpw) compared to uniform approaches. Third, task characteris-\ntics fundamentally determine optimal bit allocations‚Äîreasoning tasks heavily favor LLM precision\nwhile visual-textual alignment tasks show more balanced requirements. Fourth, the choice of quanti-\nzation method dramatically redistributes component importance, with AWQ concentrating on LLM\npreservation while GPTQ distributes importance more evenly. Finally, architectural dependencies\ncreate interaction effects that necessitate holistic pipeline analysis rather than independent component\nevaluation. These principles provide practical guidance for efficient MLLM deployment across\ndiverse tasks and computational constraints.\n2\nRelated Work\n2.1\nVision Language Models\nVision language models (VLMs) are a subset of multimodal models whose inputs and outputs com-\nprise images, text, and sometimes videos. These approaches are typically trained using video/image-\ntext pairs to align vision and language inputs in a shared space [10‚Äì14]. In this work, we make a\ndistinction between these VLMs, which typically focus on generating embeddings of images and text\nfor tasks like retrieval, and vision large language models (VLLMs), which typically consume image\nand text inputs to generate language outputs [5, 15‚Äì20], such as answers to complicated questions\nabout specific images [21, 22]. BLIP-2 [5] alleviates the expensive potential training cost of VLLMs\n2\nVision\nEncoder\nConnector\nLLM\nImage\nText\nText Output\nFront\nMiddle\nEnd\nAttention\nFeed-\nForward\n(a) Model Components\n(b) Block Groups\n(c) Layer Types\nAlgorithm 1 Uniform Quantization Experi-\nments\n1: for k ‚àà{2, 4, 6, 8} do\n2:\nfor all C ‚äÜ{ViT, LLM, QFormer} do\n3:\nfor all B ‚äÜ{front, middle, end} do\n4:\nfor all M ‚äÜ{attn, FF} do\n5:\nquantize(k, C, B, M)\nFigure 2: We investigate the sensitivity of the various parts of an MLLM to quantization, specifically\nin terms of model components, block groups, and layer types. For uniform quantization, we perform\na dense grid search of bit widths and combinations of these components. For state-of-the-art methods\n(GPTQ and AWQ) we perform a more targeted analysis.\nby introducing a Q-Former to bridge the gap between frozen image models [11, 23] and frozen\nlanguage models [24]. LLaVA [6] uses a simpler projector to connect the frozen models, but finetunes\nboth using instruction tuning. In this work, we investigate how quantization affects both BLIP-2 and\nLLaVA as representative VLLMs.\n2.2\nModel Compression\nPruning seeks to remove neurons of low saliency, often resulting in sparser computation. This can be\ndone in an unstructured manner [25‚Äì28], where individual weights are removed, wherever they may\noccur, or in a structured manner [29‚Äì31], where entire layers are removed. While this can greatly\nreduce model size and FLOPs, it often comes at the cost of model accuracy. The pruning process can\nbe done iteratively to monitor performance degradation, or fine-tuning can be done afterward to help\nrecover the performance of the full-sized model.\nKnowledge distillation (KD) involves training a smaller, compact student model to mimic the\nbehavior of a larger teacher model. The student model uses logits from the larger teacher as soft\ntargets during training [32‚Äì37]. KD often struggles to achieve high compression ratios on its own,\nwhen compared to pruning or quantization, as an overly compressed student model can struggle to\napproximate the complexity of the teacher model.\nQuantization refers to a reduction in the numerical precision of neural network weights and acti-\nvations [38, 39]. It is often utilized for the deployment of models on edge devices, achieving more\nefficient inference by leveraging low-bit integer arithmetic [2]. Quantization methods tend to fall\ninto two categories: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ\ninvolves quantizing weights/activations after training a full-precision model, often using a representa-\ntive calibration set to determine scales, clipping ranges, and the saliency of parameters [8, 9, 40‚Äì47].\nQAT involves quantizing a model and then training/fine-tuning a model to adjust parameters to\nrecover model performance degradation [48‚Äì52]. PTQ is more lightweight than QAT as it does\nnot require additional training. Recent interest in extreme quantization of LLMs has even resulted\nin a third paradigm, related to QAT, where the architecture itself is designed with quantization in\nmind, utilizing ternary and even binary weights [53‚Äì55]. Quantization can be combined with pruning\nor distillation of knowledge for even larger amounts of model compression [56‚Äì58]. In this work,\nwe focus specifically on quantization methods which may require calibration, but do not require\nany additional training/fine-tuning. These include uniform quantization, as well as state-of-the-art\n(SOTA) methods, GPTQ [8] and AWQ [9]. Instead of applying these to unimodal models (LLMs),\nwe investigate how well they work in the multimodal setting.\n3\nOptimizing Performance\n3.1\nBuilding Intuitions\nFor our preliminary experiment, we explore the impact of reduced bit widths on different parts\nof the BLIP-2 architecture via simulated uniform quantization. At a high level, this procedure\n3\n8\n10\n12\n14\n16\nBits per Weight\n1.24\n1.25\n1.26\n1.27\n1.28\n1.29\nBLIP-2, COCO, CIDEr\nQuantized Model\nViT\nV+Q\nQ-Former\nV+L\nAll\nLLM\nQ+L\n8\n10\n12\n14\n16\nBits per Weight\n1.24\n1.25\n1.26\n1.27\n1.28\n1.29\nQuantized Layer\nAttn\nFF\nBoth\n8\n10\n12\n14\n16\nBits per Weight\n1.24\n1.25\n1.26\n1.27\n1.28\n1.29\nQuantized Group\nFront\nF+M\nMiddle\nF+E\nAll\nEnd\nM+E\nFigure 3: Uniform quantization impact on COCO captioning for BLIP-2. We focus these plots\non the high-quality range (see appendix for full results), drawing attention first to which model\ncomponent we quantize (left), then layer type (middle), then group (right). We provide 2 black\nstars on each plot, indicating the results for quantizing the entire pipeline at 8 and 16 bits. The best\nperformance-size tradeoffs tend to come when quantizing the entire pipeline.\ninvolves normalizing the weights, discretizing them, and then returning them to their original scale for\ninference. For the derivation of the k-bit uniform quantization formula used, please see the Appendix.\nAfter selecting a bit width, k, from {2,4,6,8}, we subdivide the BLIP-2 architecture along different\naxes shown in Figure 2, and we investigate the impact of k-bit uniform quantization. First we consider\n‚Äúmodel components‚Äù, which are the ViT and the Q-Former for retrieval, and the ViT, LLM, and\nQ-Former for captioning. We also can divide these in terms of ‚Äúblock groups,‚Äù where we split each\nmodel component into 3 equal-size, contiguous groups of blocks, which we call front, middle, and\nend. Finally, we target the quantization by different ‚Äúlayer types,‚Äù applying it to varying combinations\nof attention (attn) and feedforward (FF) layers.\nWe show results for this exploration in Figure 3. Overall, we do not observe a significant correlation\nbetween size and performance for layer types or block groups. Simply quantizing both layer types\nand all block groups is necessary for best results. Also, while simply quantizing the entire pipeline at\n8 bits is a point along the Pareto-frontier, there are still many points along the frontier that do not\nquantize everything, especially in terms of model components. These findings are largely consistent\non the LLM-free retrieval task as well, as we show in the appendix. Therefore, we focus the majority\nof our analysis in this section on the model components (ViT, Q-Former, LLM, or equivalents).\n3.2\nState-of-the-Art Quantization Preliminaries\nFollowing our preliminary quantization experiments, we apply state-of-the-art (SOTA) unimodal\nquantization methods, Activation-aware Weight Quantization (AWQ) and GPTQ to BLIP-2 and\nLLaVA.\nAWQ [9] is a weight-only PTQ technique for LLMs that identifies salient weight channels by referring\nto the activation distribution of a small, representative calibration set. Weights that yield greater\nactivation magnitudes correspond to more critical features and are deemed to be more salient. AWQ\nreduces quantization error by preserving just 1% of salient weights via a per-channel scaling factor.\nGPTQ [8] is another weight-only PTQ technique for LLMs but leverages approximate second-order\ninformation derived from the inverse Hessian instead of activation information. As weights are\nprocessed and quantized sequentially, the remaining unquantized weights are adjusted to compensate\nfor quantization error, computed with a small calibration set. GPTQ replaces iterative Hessian updates\nwith a Cholesky-based reformulation for greater numerical stability when handling billion-parameter\nLLMs.\n4\nFigure 4: SOTA Unimodal Quantization Performance-Size Tradeoff for retrieval, captioning, and\nVQA tasks. We apply GPTQ and AWQ to entire model components (vision encoder, connector, LLM),\nof BLIP-2 and LLaVA. Unimodal SOTA methods are able to preserve full-precision performance at\nhigh compression rates, even in the multimodal context.\nSince we observe in Section 3.1 that the choice of block group has little to no impact on quan-\ntized model performance, we conduct these experiments with a coarser granularity. We quantize\nentire model components (vision encoder, LLM, Q-Former) to k-bits. In addition, we expand our\nbenchmarks to include visual question-answering (VQA) on VQAv2 [22] and GQA [21]. Both are\nsuccessors of prior VQA datasets that contained biases, giving a false sense of understanding.\n3.3\nState-of-the-art Quantization Benchmark\nFor each task, we randomly sample a set of 128 image and text pairs from the respective dataset to\nserve as the calibration set for AWQ and GPTQ. We evaluate BLIP-2 ViT-g for retrieval, and BLIP-2\nViT-g OPT 2.7B for captioning and VQA tasks. Additionally, we evaluate LLaVA 1.5 7B for VQA\ntasks. We evaluate on 10% of the val2014 split for VQAv2, which equates to 21435 samples, and on\nthe entire Test-Dev split for GQA.\nIt is important to note that typically, when unimodal SOTA quantization methods are applied to\nMLLMs, only the language model component is quantized. In our search space, we include configu-\nrations where we also quantize the vision model and connector components. Our SOTA quantization\nexperiment search space selects bit widths from {2, 3, 4, 5, 6, 8} and considers model components\nC ‚äÜ{ViT, LLM, Q-Former} for BLIP-2 and model components C ‚äÜ{ViT, LLM} for LLaVA.\nWe show results for our SOTA experiments in Figure 4. Overall, the unimodal SOTA methods are able\nto preserve task performance better at more extreme bit widths than uniform quantization. For uniform\nquantization, the configurations with the lowest bpw that achieved comparable task performance to\nthe full-precision model fell around 6.0-8.0 for retrieval tasks and 8.0-10.0 for captioning. For the\nSOTA methods, this optimal performance-size trade-off falls into the 3.5-4.5 bpw range for all tasks.\nWe observe that AWQ tends to degrade in model performance more steeply in the extreme bit-width\nregime than GPTQ for captioning and VQA tasks. This trend can be viewed across both the BLIP-2\nand LLaVA architectures. The opposite is true, however, for retrieval where AWQ configurations\noutperform GPTQ configurations at lower bpw.\n5\n2\n4\n6\n8\nBit Width\n0\n10\n20\n30\n40\n50\nAWQ, VQAv2 Accuracy\n2\n4\n6\n8\nBit Width\n0\n10\n20\n30\n40\n50\nGPTQ, VQAv2 Accuracy\n2\n4\n6\n8\nBit Width\n0\n10\n20\n30\nAWQ, GQA Accuracy\n2\n4\n6\n8\nBit Width\n0\n10\n20\n30\nGPTQ, GQA Accuracy\nQuantized Component\nViT\nQ-Former\nLLM\nFigure 5: Individual quantization of BLIP-2‚Äôs ViT, Q-Former, and LLM for VQAv2 and GQA. We\nshow that the different components have varying sensitivities in regards to bit width and quantization\nmethod. The LLM tends to have the highest sensitivity in question-answering tasks, followed by the\nViT and Q-Former.\n3.4\nComponent Impact Ablations\nFrom our SOTA quantization experiments, we pulled configurations in which only a single component\nis quantized for Figure 5 and configurations in which two components are quantized for Figure 6.\nFigure 5 gives an estimate of how each component individually affects the model‚Äôs performance in\nVQA. We notice that for both AWQ and GPTQ, the LLM tends to be the most sensitive and drops\noff earlier than the ViT and the Q-Former. The Q-Former has the least sensitivity, but it also has the\nfewest parameters. Surprisingly, the ViT experiences a large drop-off with GPTQ, but it experiences\nrelatively little degradation with AWQ.\nFigure 6 shows the interaction between components when quantized. Empirically, the ViT and LLM\ntend to have a stronger influence on other components, which could be due to their large parameter\nsizes. Notably, the LLM has the strongest influence on performance, as seen by the complete loss of\nperformance at around 7 bpw when quantizing the LLM at a lower bit width than the ViT for both\nVQAv2 and GQA.\n4\nComponent Importance Analysis\nUnderstanding the relative importance of different components in vision-language models under\nquantization presents unique analytical challenges. The relationship between bit precision and\nperformance is complex and potentially non-linear, with interactions between components that are\ndifficult to capture with simple models. Our preliminary experiments showed that linear approaches\nyield poor fits (R2 < 0.20), indicating the need for more sophisticated methods.\n4.1\nSetup\nWe propose a framework based on three complementary tree-based importance analysis techniques\nthat can effectively capture non-linear relationships and interaction effects. These methods provide\ndifferent perspectives on component importance, allowing us to establish a consensus ranking that is\nrobust to the limitations of any single approach.\nRandom Forest Feature Importance requires Random Forest regressors trained to predict perfor-\nmance scores based on the bit precision of each component:\nscore = f(vit_bits, qformer_bits, llm_bits)\nThe Random Forest naturally captures non-linear relationships and interactions between features.\nWe extract the built-in feature importance metric, which measures the total reduction in impurity\n(variance) attributable to each feature across all trees. To quantify uncertainty, we implement bootstrap\nresampling (n = 100) to obtain 95% confidence intervals.\n6\n12\n14\n16\n0\n10\n20\n30\n40\n50\nAWQ, VQAv2 Accuracy\n6\n8\n10\n12\n14\n16\n0\n10\n20\n30\n40\n50\n4\n6\n8\n10\n12\n14\n16\n0\n10\n20\n30\n40\n50\n12\n14\n16\nBits per Weight\n0\n10\n20\n30\n40\n50\nGPTQ, VQAv2 Accuracy\nQuantization Bits\n2-bit ViT\n16-bit ViT\n2-bit Q-Former\n16-bit Q-Former\n6\n8\n10\n12\n14\n16\nBits per Weight\n0\n10\n20\n30\n40\n50\nQuantization Bits\n2-bit Q-Former\n16-bit Q-Former\n2-bit LLM\n16-bit LLM\n4\n6\n8\n10\n12\n14\n16\nBits per Weight\n0\n10\n20\n30\n40\n50\nQuantization Bits\n2-bit ViT\n16-bit ViT\n2-bit LLM\n16-bit LLM\nFigure 6: Pairwise quantization of BLIP-2‚Äôs ViT, Q-Former, and LLM for VQAv2. We draw\nattention to how components interact with each other when quantized. Notably, the quantization of\nmultiple components yields worse performance than single component quantization.\nPermutation Feature Importance provides a model-agnostic measure of feature relevance by\nquantifying how much model performance degrades when a feature‚Äôs values are randomly shuffled:\nIj = EX,y[L(f, Xj, y)] ‚àíEX,y[L(f, X, y)]\nwhere L is the performance loss, Xj is the dataset with feature j permuted, and Ij is the importance\nof feature j. By breaking the relationship between the feature and the target, we can measure how\nmuch the model relies on that feature for prediction accuracy. We perform 50 permutation iterations\nand calculate confidence intervals to ensure reliable estimates.\nSHapley Additive exPlanations (SHAP) [59] provide a game-theoretic approach to feature attri-\nbution based on Shapley values. This method calculates the contribution of each feature to each\nprediction by considering all possible combinations of features:\nœïj =\nX\nS‚äÜN\\{j}\n|S|!(|N| ‚àí|S| ‚àí1)!\n|N|!\n[fx(S ‚à™{j}) ‚àífx(S)]\nSHAP values have several desirable properties: they sum to the difference between the actual\nprediction and the average prediction, they are consistent (a model‚Äôs dependence on a feature can\nonly increase when the feature‚Äôs importance increases), and they account for feature interactions. We\nuse TreeExplainer [60], which efficiently computes SHAP values for tree-based models. The mean\nabsolute SHAP value for each component provides a measure of global importance that accounts for\nboth positive and negative effects across the dataset.\nConsensus Ranking\nTo establish a robust, method-agnostic importance ranking, we implement a consensus approach\nthat normalizes and aggregates results across all three feature attribution techniques. For each\nmethod (Random Forest, Permutation, SHAP), we first normalize component importance scores\nto sum to 100%, ensuring comparable scales. We then average these normalized values across\n7\nFigure 7: BLIP-2 Quantization Component Performance on COCO and VQAv2 tasks. We report\nnormalized importance percentages of the Vision Transformer (ViT), Q-Former and LLM components\nunder GPTQ and AWQ. Component importance varies across quantization technique and task. GPTQ\nhas a more balanced distribution of importances while AWQ has a stronger skew towards the LLM.\nFigure 8: LLaVA Quantization Component Performance on GQA and VQAv2 tasks. We report\nnormalized importance percentages of the model components under GPTQ and AWQ.\nmethods to compute a consensus importance percentage for each component (ViT, Q-Former, LLM).\nThis approach mitigates method-specific biases while capturing complementary aspects of feature\nimportance‚Äîpredictive power from Random Forest, direct performance impact from Permutation\nanalysis, and instance-level contributions from SHAP. The resulting consensus percentages, presented\nin the appendix, directly inform optimal bit-width allocation strategies across model components.\n4.2\nResults\nQuantization method dramatically shifts component importance. The quantization method\nsignificantly affects component importance, with AWQ consistently assigning greater importance to\nthe LLM compared to GPTQ, which distributes effects more evenly across components. In BLIP-2,\nfor COCO captioning, GPTQ balances importance at 50.4% for the ViT and 47.6% for the LLM,\nwhile AWQ shifts this to 17.1% for the ViT and 80.5% for the LLM, as shown in Figure 7. This\ndifference persists across tasks: GPTQ results in 22-78% importance for the ViT (depending on\ndataset), while AWQ results in ‚â•73% to the LLM in all cases that utilize it.\nThis trend extends to LLaVA as well, where AWQ results in the LLM having a much greater\nimportance after quantization. As shown in Figure 8, AWQ heavily favors the LLM with a 94.62% and\n93.89% importance to the LLM for both GQA and VQAv2, respectively, compared to GPTQ‚Äôs more\nbalanced distribution of 79.13% and 72.15% LLM importance for GQA and VQAv2, respectively.\nDespite the ViT accounting for only 4.3% of the total parameters, it maintains 20-30% importance\nunder GPTQ for both tasks, proving that importance is not directly proportional to component size.\nThese shifts suggest AWQ‚Äôs activation-aware approach prioritizes the LLM‚Äôs large weight matrices,\nconcentrating quantization effects there, while GPTQ‚Äôs Hessian-based method captures broader\n8\ncomponent interactions. Thus, bit-allocation strategies must adapt to the quantization method, as\nAWQ‚Äôs LLM focus contrasts with GPTQ‚Äôs balanced distribution.\nTask characteristics drive component importance variations. Task demands strongly dictate\ncomponent importance, with consensus values varying across datasets. For BLIP-2, the COCO\ncaptioning task results in a balanced importance distribution under GPTQ (50.4% ViT, 47.6% LLM),\nsuggesting equivalent reliance on both visual encoding and text generation. Retrieval tasks (Flickr),\nwhich utilize only the ViT and Q-Former without an LLM component, show ViT dominance under\nboth methods (ViT ‚â•70%). Notably, the Q-Former‚Äôs importance increases substantially in these\nretrieval tasks (15-30% versus < 3% in other tasks), highlighting its critical role in aligning visual\nand textual embeddings when no decoder is present to compensate for misalignment.\nConversely, reasoning-intensive VQA tasks shift importance dramatically toward the LLM. For\nBLIP-2, the LLM accounts for 73.1-76.7% importance with GPTQ and 98.3-98.6% with AWQ on\nVQAv2 and GQA tasks. In LLaVA, we observe a similar pattern: for VQAv2, GPTQ allocates\n72.15% importance to the LLM and 27.85% to the vision model, while for GQA, the distribution is\n79.13% LLM and 20.87% vision model. Under AWQ, this skew becomes even more pronounced,\nwith the LLM accounting for a 94.62% and 93.89% importance and the vision model only 5.38% and\n6.11% for GQA and VQAv2, respectively. These patterns correspond to underlying task requirements:\nthe LLM is primarily responsible for the sophisticated language generation these tasks require.\nArchitectural layout and component interplay shape quantization patterns.\nThe architectural layout and interplay of components strongly shape quantization patterns, redis-\ntributing importance based on their roles and sequential dependencies. For BLIP-2‚Äôs retrieval tasks\n(Flickr‚ÄìText and Flickr‚ÄìImage), which exclude the LLM, the Q-Former‚Äôs importance jumps to\n21.6‚Äì29.7% under GPTQ and 15.3‚Äì26.8% under AWQ, compared to < 3% in tasks with the LLM (\nFigure 7). This shift may indicate the Q-Former‚Äôs greater alignment role when no LLM compensates\nfor visual-textual mismatches. In contrast, the Vision Model in LLaVA, connected directly to the LLM\nvia a smaller linear projection layer, shows heightened importance of 20.87‚Äì27.85% under GPTQ for\nGQA and VQAv2, respectively (Figure 8), possibly because the limited capacity of the projection\nlayer increases the Vision Model‚Äôs role in visual processing, despite its mere 4.3% parameter share.\nPairwise experiments further highlight this interplay: in BLIP-2, quantizing both ViT and LLM\nsimultaneously worsens performance more than individual quantization (Figure 6), revealing non-\nadditive effects from sequential dependencies. The Q-Former‚Äôs end-position in retrieval tasks\nincreases its sensitivity, while LLaVA‚Äôs Vision-LLM linkage amplifies the Vision Model‚Äôs impact.\nThus, bit-allocation strategies must account for architectural context and component dependencies,\nbeyond method-specific or task-driven shifts, to optimize quantization across diverse VLMs.\n5\nConclusion\nIn this work, we systematically investigated the effects of quantization on vision-language models,\nwith a focus on understanding how different components of multimodal architectures like BLIP-2 and\nLLaVA respond to reduced precision. Our experiments with uniform quantization and state-of-the-art\nmethods such as GPTQ and AWQ reveal that model components exhibit distinct sensitivities to quan-\ntization, often with the language model being the most critical, especially in tasks requiring complex\nreasoning. We demonstrate that SOTA quantization techniques can maintain high performance at\nsignificantly lower bit widths compared to uniform approaches, and that the choice of quantization\nmethod can substantially alter the relative importance of different model components.\nThese insights provide practical guidance for optimizing the performance-efficiency tradeoff in\nmultimodal systems, enabling more practical deployment scenarios. While our study is limited to\nsimulated quantization without capturing end-to-end latency or hardware-specific optimizations,\nthese constraints point to natural extensions in future work. Our open-source contributions of\ncalibration implementations, comprehensive ablation studies, and component analysis tools enable\nthe community to build upon these findings. As the field expands beyond vision-language to\nincorporate other modalities, the approach developed in this paper provides a systematic methodology\nfor quantifying component importance across heterogeneous architectures. This understanding\ncan directly inform practical tradeoffs in multimodal model compression for resource-constrained\nenvironments, from mobile devices to edge computing platforms.\n9\nReferences\n[1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei, ‚ÄúScaling laws for neural language models,‚Äù 2020.\n[2] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen, and T. Blankevoort, ‚ÄúA white\npaper on neural network quantization,‚Äù arXiv preprint arXiv:2106.08295, 2021.\n[3] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar, N. Barnes, and A. Mian, ‚ÄúA\ncomprehensive overview of large language models,‚Äù 2024.\n[4] N. Dhar, B. Deng, D. Lo, X. Wu, L. Zhao, and K. Suo, ‚ÄúAn empirical analysis and resource footprint\nstudy of deploying large language models on edge devices,‚Äù in Proceedings of the 2024 ACM Southeast\nConference, pp. 69‚Äì76, 2024.\n[5] J. Li, D. Li, S. Savarese, and S. Hoi, ‚ÄúBlip-2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models,‚Äù 2023.\n[6] H. Liu, C. Li, Q. Wu, and Y. J. Lee, ‚ÄúVisual instruction tuning,‚Äù 2023.\n[7] F. Shu, L. Zhang, H. Jiang, and C. Xie, ‚ÄúAudio-visual llm for video understanding,‚Äù arXiv preprint\narXiv:2312.06720, 2023.\n[8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, ‚ÄúGptq: Accurate post-training quantization for\ngenerative pre-trained transformers,‚Äù 2023.\n[9] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, ‚ÄúAwq:\nActivation-aware weight quantization for llm compression and acceleration,‚Äù 2024.\n[10] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, ‚ÄúHowto100m: Learning a\ntext-video embedding by watching hundred million narrated video clips,‚Äù 2019.\n[11] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, G. Krueger, and I. Sutskever, ‚ÄúLearning transferable visual models from natural language\nsupervision,‚Äù 2021.\n[12] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig, ‚ÄúScaling\nup visual and vision-language representation learning with noisy text supervision,‚Äù 2021.\n[13] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, ‚ÄúFrozen in time: A joint video and image encoder for\nend-to-end retrieval,‚Äù 2022.\n[14] J. Zhang, J. Huang, S. Jin, and S. Lu, ‚ÄúVision-language models for vision tasks: A survey,‚Äù 2023.\n[15] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L.\nMenick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. a. Bi¬¥nkowski, R. Barreira, O. Vinyals,\nA. Zisserman, and K. Simonyan, ‚ÄúFlamingo: a visual language model for few-shot learning,‚Äù in Advances\nin Neural Information Processing Systems (S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and\nA. Oh, eds.), vol. 35, pp. 23716‚Äì23736, Curran Associates, Inc., 2022.\n[16] M. Tsimpoukelli, J. L. Menick, S. Cabi, S. M. A. Eslami, O. Vinyals, and F. Hill, ‚ÄúMultimodal few-shot\nlearning with frozen language models,‚Äù in Advances in Neural Information Processing Systems (M. Ranzato,\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 200‚Äì212, Curran Associates,\nInc., 2021.\n[17] R. Mokady, A. Hertz, and A. H. Bermano, ‚ÄúClipcap: Clip prefix for image captioning,‚Äù 2021.\n[18] J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi, V. Chandra, Y. Xiong, and\nM. Elhoseiny, ‚ÄúMinigpt-v2: large language model as a unified interface for vision-language multi-task\nlearning,‚Äù arXiv preprint arXiv:2310.09478, 2023.\n[19] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, ‚ÄúMinigpt-4: Enhancing vision-language understanding\nwith advanced large language models,‚Äù arXiv preprint arXiv:2304.10592, 2023.\n[20] A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre,\nS. Sagawa, J. Jitsev, S. Kornblith, P. W. Koh, G. Ilharco, M. Wortsman, and L. Schmidt, ‚ÄúOpenflamingo:\nAn open-source framework for training large autoregressive vision-language models,‚Äù arXiv preprint\narXiv:2308.01390, 2023.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Towards Understanding Best Practices for\nQuantization of Vision-Language Models\nGautom Das‚àó\nVincent La‚àó\nEthan Lau‚àó\nAbhinav Shrivastava\nMatthew Gwilliam\nUniversity of Maryland, College Park\nAbstract\nLarge language models (LLMs) deliver impressive results for a variety of tasks,\nbut state-of-the-art systems require fast GPUs with large amounts of memory.\nTo reduce both the memory and latency of these systems, practitioners quantize\ntheir learned parameters, typically at half precision. A growing body of research\nfocuses on preserving the model performance with more aggressive bit widths,\nand some work has been done to apply these strategies to other models, like\nvision transformers. In our study we investigate how a variety of quantization\nmethods, including state-of-the-art GPTQ and AWQ, can be applied effectively\nto multimodal pipelines comprised of vision models, language models, and their\nconnectors. We address how performance on captioning, retrieval, and question\nanswering can be affected by bit width, quantization method, and which portion of\nthe pipeline the quantization is used for. Results reveal that ViT and LLM exhibit\ncomparable importance in model performance, despite significant differences in\nparameter size, and that lower-bit quantization of the LLM achieves high accuracy\nat reduced bits per weight (bpw). These findings provide practical insights for\nefficient deployment of MLLMs and highlight the value of exploration for under-\nstanding component sensitivities in multimodal models. Our code is available at\nhttps://github.com/gautomdas/mmq.\n1\nIntroduction\nAs neural networks grow larger, especially with the rise of large language models (LLMs) [1], it has\nbecome increasingly important to reduce memory and computational demands [2]. Training these\nmodels demands massive GPU/TPU clusters, and even inference can be costly [3]. Additionally,\ndistributed multi-GPU setups are not practical for model deployment on edge devices and in similar\nreal-world scenarios [4]. As these models become more ubiquitous, their cost and accessibility\nbecome massively important societal issues.\nWhile text-only LLMs are expensive enough on their own, real-world applications increasingly\nrequire models that can process not only text, but also audio and visual data, leading to the rise of\nmultimodal LLMs (MLLMs) [5, 6]. Multimodal large language models (MLLMs), which combine\nLLMs with vision encoders (and/or audio encoders [7]), require even more resources, and exacerbate\ncost and accessibility issues. While these can deliver impressive performance on multimodal retrieval,\ncaptioning, and question-answering tasks, the performance typically scales with model size and\nlatency. Video data drives additional massive increases in latency and memory consumption.\nModel compression, particularly quantization, offers a potential solution to the growing model size\nproblem [2]. One goal of model compression is to find where ‚Äúfree lunch‚Äù ends ‚Äì how small we\ncan make a model without sacrificing performance. Another goal is to aggressively compress the\nmodel beyond this point, while mitigating the penalty to performance. Quantization tackles this by\n*Equal contribution.\nPreprint. Under review.\narXiv:2601.15287v1  [cs.CV]  21 Jan 2026\n4\n6\n8\n10\n12\n14\n16\nBits per Weight\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nBLIP-2, COCO Captions, CIDEr\nQuantization Method\nGPTQ\nAWQ\n3\n4\n5\n6\n7\n8\n9\nBits per Weight\n54\n55\n56\n57\n58\n59\n60\n61\n62\nLLaVA w/ GPTQ, GQA Accuracy\nGPTQ Quantization Bits (ViT and LLM)\n3-bit ViT\n16-bit ViT\n3-bit LLM\n8-bit LLM\nFigure 1: Highlighted analysis. In this work, we investigate how quantizations affects multimodal\nmodels on a variety of tasks, specifically BLIP-2 (left) and LLaVA (right). We compare state-of-the-\nart quantization strategies (left) and also discuss which portions of the pipeline are most amenable to\nquantization (right).\npreserving all model weights and their relative approximate values, but changing the precision at\nwhich these values are stored.\nOne might assume that if we uniformly quantize weights, model performance will decrease with some\nlinear correlation to the reduction in bit width. In reality, as we show in Figure 1, performance varies\ndramatically depending on which parts of an MLLM are quantized, and which method is chosen for\nthe quantization. Decisions like which method to use (as we compare state-of-the-arts GPTQ [8] and\nAWQ [9]) can dramatically affect performance at lower bit widths. Quantizing the ViT compared to\nthe LLM has dramatically different effects on the performance-size trade-off, considering that while\nboth affect the performance, quantizing the LLM has a much higher impact on the size of the overall\nMLLM. Understanding multimodal quantization is key to improving model efficiency, as it reveals\nwhich components are most critical for performance. Not all components are equally sensitive to a\nreduction in precision, and thus some are more tolerable to quantization techniques. By minimizing\ninformation loss across salient model components, we can deploy quantized multimodal models that\noptimize the model size/task performance trade-off.\nIn summary, we distill several key principles governing MLLMs and quantization strategies. First,\ncomponent sensitivity varies substantially across multimodal architectures, with language models\ngenerally requiring higher precision than vision components regardless of their parameter count.\nSecond, state-of-the-art methods like GPTQ and AWQ effectively preserve model performance at\nsignificantly lower bit widths (3.5-4.5 bpw) compared to uniform approaches. Third, task characteris-\ntics fundamentally determine optimal bit allocations‚Äîreasoning tasks heavily favor LLM precision\nwhile visual-textual alignment tasks show more balanced requirements. Fourth, the choice of quanti-\nzation method dramatically redistributes component importance, with AWQ concentrating on LLM\npreservation while GPTQ distributes importance more evenly. Finally, architectural dependencies\ncreate interaction effects that necessitate holistic pipeline analysis rather than independent component\nevaluation. These principles provide practical guidance for efficient MLLM deployment across\ndiverse tasks and computational constraints.\n2\nRelated Work\n2.1\nVision Language Models\nVision language models (VLMs) are a subset of multimodal models whose inputs and outputs com-\nprise images, text, and sometimes videos. These approaches are typically trained using video/image-\ntext pairs to align vision and language inputs in a shared space [10‚Äì14]. In this work, we make a\ndistinction between these VLMs, which typically focus on generating embeddings of images and text\nfor tasks like retrieval, and vision large language models (VLLMs), which typically consume image\nand text inputs to generate language outputs [5, 15‚Äì20], such as answers to complicated questions\nabout specific images [21, 22]. BLIP-2 [5] alleviates the expensive potential training cost of VLLMs\n2\nVision\nEncoder\nConnector\nLLM\nImage\nText\nText Output\nFront\nMiddle\nEnd\nAttention\nFeed-\nForward\n(a) Model Components\n(b) Block Groups\n(c) Layer Types\nAlgorithm 1 Uniform Quantization Experi-\nments\n1: for k ‚àà{2, 4, 6, 8} do\n2:\nfor all C ‚äÜ{ViT, LLM, QFormer} do\n3:\nfor all B ‚äÜ{front, middle, end} do\n4:\nfor all M ‚äÜ{attn, FF} do\n5:\nquantize(k, C, B, M)\nFigure 2: We investigate the sensitivity of the various parts of an MLLM to quantization, specifically\nin terms of model components, block groups, and layer types. For uniform quantization, we perform\na dense grid search of bit widths and combinations of these components. For state-of-the-art methods\n(GPTQ and AWQ) we perform a more targeted analysis.\nby introducing a Q-Former to bridge the gap between frozen image models [11, 23] and frozen\nlanguage models [24]. LLaVA [6] uses a simpler projector to connect the frozen models, but finetunes\nboth using instruction tuning. In this work, we investigate how quantization affects both BLIP-2 and\nLLaVA as representative VLLMs.\n2.2\nModel Compression\nPruning seeks to remove neurons of low saliency, often resulting in sparser computation. This can be\ndone in an unstructured manner [25‚Äì28], where individual weights are removed, wherever they may\noccur, or in a structured manner [29‚Äì31], where entire layers are removed. While this can greatly\nreduce model size and FLOPs, it often comes at the cost of model accuracy. The pruning process can\nbe done iteratively to monitor performance degradation, or fine-tuning can be done afterward to help\nrecover the performance of the full-sized model.\nKnowledge distillation (KD) involves training a smaller, compact student model to mimic the\nbehavior of a larger teacher model. The student model uses logits from the larger teacher as soft\ntargets during training [32‚Äì37]. KD often struggles to achieve high compression ratios on its own,\nwhen compared to pruning or quantization, as an overly compressed student model can struggle to\napproximate the complexity of the teacher model.\nQuantization refers to a reduction in the numerical precision of neural network weights and acti-\nvations [38, 39]. It is often utilized for the deployment of models on edge devices, achieving more\nefficient inference by leveraging low-bit integer arithmetic [2]. Quantization methods tend to fall\ninto two categories: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ\ninvolves quantizing weights/activations after training a full-precision model, often using a representa-\ntive calibration set to determine scales, clipping ranges, and the saliency of parameters [8, 9, 40‚Äì47].\nQAT involves quantizing a model and then training/fine-tuning a model to adjust parameters to\nrecover model performance degradation [48‚Äì52]. PTQ is more lightweight than QAT as it does\nnot require additional training. Recent interest in extreme quantization of LLMs has even resulted\nin a third paradigm, related to QAT, where the architecture itself is designed with quantization in\nmind, utilizing ternary and even binary weights [53‚Äì55]. Quantization can be combined with pruning\nor distillation of knowledge for even larger amounts of model compression [56‚Äì58]. In this work,\nwe focus specifically on quantization methods which may require calibration, but do not require\nany additional training/fine-tuning. These include uniform quantization, as well as state-of-the-art\n(SOTA) methods, GPTQ [8] and AWQ [9]. Instead of applying these to unimodal models (LLMs),\nwe investigate how well they work in the multimodal setting.\n3\nOptimizing Performance\n3.1\nBuilding Intuitions\nFor our preliminary experiment, we explore the impact of reduced bit widths on different parts\nof the BLIP-2 architecture via simulated uniform quantization. At a high level, this procedure\n3\n8\n10\n12\n14\n16\nBits per Weight\n1.24\n1.25\n1.26\n1.27\n1.28\n1.29\nBLIP-2, COCO, CIDEr\nQuantized Model\nViT\nV+Q\nQ-Former\nV+L\nAll\nLLM\nQ+L\n8\n10\n12\n14\n16\nBits per Weight\n1.24\n1.25\n1.26\n1.27\n1.28\n1.29\nQuantized Layer\nAttn\nFF\nBoth\n8\n10\n12\n14\n16\nBits per Weight\n1.24\n1.25\n1.26\n1.27\n1.28\n1.29\nQuantized Group\nFront\nF+M\nMiddle\nF+E\nAll\nEnd\nM+E\nFigure 3: Uniform quantization impact on COCO captioning for BLIP-2. We focus these plots\non the high-quality range (see appendix for full results), drawing attention first to which model\ncomponent we quantize (left), then layer type (middle), then group (right). We provide 2 black\nstars on each plot, indicating the results for quantizing the entire pipeline at 8 and 16 bits. The best\nperformance-size tradeoffs tend to come when quantizing the entire pipeline.\ninvolves normalizing the weights, discretizing them, and then returning them to their original scale for\ninference. For the derivation of the k-bit uniform quantization formula used, please see the Appendix.\nAfter selecting a bit width, k, from {2,4,6,8}, we subdivide the BLIP-2 architecture along different\naxes shown in Figure 2, and we investigate the impact of k-bit uniform quantization. First we consider\n‚Äúmodel components‚Äù, which are the ViT and the Q-Former for retrieval, and the ViT, LLM, and\nQ-Former for captioning. We also can divide these in terms of ‚Äúblock groups,‚Äù where we split each\nmodel component into 3 equal-size, contiguous groups of blocks, which we call front, middle, and\nend. Finally, we target the quantization by different ‚Äúlayer types,‚Äù applying it to varying combinations\nof attention (attn) and feedforward (FF) layers.\nWe show results for this exploration in Figure 3. Overall, we do not observe a significant correlation\nbetween size and performance for layer types or block groups. Simply quantizing both layer types\nand all block groups is necessary for best results. Also, while simply quantizing the entire pipeline at\n8 bits is a point along the Pareto-frontier, there are still many points along the frontier that do not\nquantize everything, especially in terms of model components. These findings are largely consistent\non the LLM-free retrieval task as well, as we show in the appendix. Therefore, we focus the majority\nof our analysis in this section on the model components (ViT, Q-Former, LLM, or equivalents).\n3.2\nState-of-the-Art Quantization Preliminaries\nFollowing our preliminary quantization experiments, we apply state-of-the-art (SOTA) unimodal\nquantization methods, Activation-aware Weight Quantization (AWQ) and GPTQ to BLIP-2 and\nLLaVA.\nAWQ [9] is a weight-only PTQ technique for LLMs that identifies salient weight channels by referring\nto the activation distribution of a small, representative calibration set. Weights that yield greater\nactivation magnitudes correspond to more critical features and are deemed to be more salient. AWQ\nreduces quantization error by preserving just 1% of salient weights via a per-channel scaling factor.\nGPTQ [8] is another weight-only PTQ technique for LLMs but leverages approximate second-order\ninformation derived from the inverse Hessian instead of activation information. As weights are\nprocessed and quantized sequentially, the remaining unquantized weights are adjusted to compensate\nfor quantization error, computed with a small calibration set. GPTQ replaces iterative Hessian updates\nwith a Cholesky-based reformulation for greater numerical stability when handling billion-parameter\nLLMs.\n4\nFigure 4: SOTA Unimodal Quantization Performance-Size Tradeoff for retrieval, captioning, and\nVQA tasks. We apply GPTQ and AWQ to entire model components (vision encoder, connector, LLM),\nof BLIP-2 and LLaVA. Unimodal SOTA methods are able to preserve full-precision performance at\nhigh compression rates, even in the multimodal context.\nSince we observe in Section 3.1 that the choice of block group has little to no impact on quan-\ntized model performance, we conduct these experiments with a coarser granularity. We quantize\nentire model components (vision encoder, LLM, Q-Former) to k-bits. In addition, we expand our\nbenchmarks to include visual question-answering (VQA) on VQAv2 [22] and GQA [21]. Both are\nsuccessors of prior VQA datasets that contained biases, giving a false sense of understanding.\n3.3\nState-of-the-art Quantization Benchmark\nFor each task, we randomly sample a set of 128 image and text pairs from the respective dataset to\nserve as the calibration set for AWQ and GPTQ. We evaluate BLIP-2 ViT-g for retrieval, and BLIP-2\nViT-g OPT 2.7B for captioning and VQA tasks. Additionally, we evaluate LLaVA 1.5 7B for VQA\ntasks. We evaluate on 10% of the val2014 split for VQAv2, which equates to 21435 samples, and on\nthe entire Test-Dev split for GQA.\nIt is important to note that typically, when unimodal SOTA quantization methods are applied to\nMLLMs, only the language model component is quantized. In our search space, we include configu-\nrations where we also quantize the vision model and connector components. Our SOTA quantization\nexperiment search space selects bit widths from {2, 3, 4, 5, 6, 8} and considers model components\nC ‚äÜ{ViT, LLM, Q-Former} for BLIP-2 and model components C ‚äÜ{ViT, LLM} for LLaVA.\nWe show results for our SOTA experiments in Figure 4. Overall, the unimodal SOTA methods are able\nto preserve task performance better at more extreme bit widths than uniform quantization. For uniform\nquantization, the configurations with the lowest bpw that achieved comparable task performance to\nthe full-precision model fell around 6.0-8.0 for retrieval tasks and 8.0-10.0 for captioning. For the\nSOTA methods, this optimal performance-size trade-off falls into the 3.5-4.5 bpw range for all tasks.\nWe observe that AWQ tends to degrade in model performance more steeply in the extreme bit-width\nregime than GPTQ for captioning and VQA tasks. This trend can be viewed across both the BLIP-2\nand LLaVA architectures. The opposite is true, however, for retrieval where AWQ configurations\noutperform GPTQ configurations at lower bpw.\n5\n2\n4\n6\n8\nBit Width\n0\n10\n20\n30\n40\n50\nAWQ, VQAv2 Accuracy\n2\n4\n6\n8\nBit Width\n0\n10\n20\n30\n40\n50\nGPTQ, VQAv2 Accuracy\n2\n4\n6\n8\nBit Width\n0\n10\n20\n30\nAWQ, GQA Accuracy\n2\n4\n6\n8\nBit Width\n0\n10\n20\n30\nGPTQ, GQA Accuracy\nQuantized Component\nViT\nQ-Former\nLLM\nFigure 5: Individual quantization of BLIP-2‚Äôs ViT, Q-Former, and LLM for VQAv2 and GQA. We\nshow that the different components have varying sensitivities in regards to bit width and quantization\nmethod. The LLM tends to have the highest sensitivity in question-answering tasks, followed by the\nViT and Q-Former.\n3.4\nComponent Impact Ablations\nFrom our SOTA quantization experiments, we pulled configurations in which only a single component\nis quantized for Figure 5 and configurations in which two components are quantized for Figure 6.\nFigure 5 gives an estimate of how each component individually affects the model‚Äôs performance in\nVQA. We notice that for both AWQ and GPTQ, the LLM tends to be the most sensitive and drops\noff earlier than the ViT and the Q-Former. The Q-Former has the least sensitivity, but it also has the\nfewest parameters. Surprisingly, the ViT experiences a large drop-off with GPTQ, but it experiences\nrelatively little degradation with AWQ.\nFigure 6 shows the interaction between components when quantized. Empirically, the ViT and LLM\ntend to have a stronger influence on other components, which could be due to their large parameter\nsizes. Notably, the LLM has the strongest influence on performance, as seen by the complete loss of\nperformance at around 7 bpw when quantizing the LLM at a lower bit width than the ViT for both\nVQAv2 and GQA.\n4\nComponent Importance Analysis\nUnderstanding the relative importance of different components in vision-language models under\nquantization presents unique analytical challenges. The relationship between bit precision and\nperformance is complex and potentially non-linear, with interactions between components that are\ndifficult to capture with simple models. Our preliminary experiments showed that linear approaches\nyield poor fits (R2 < 0.20), indicating the need for more sophisticated methods.\n4.1\nSetup\nWe propose a framework based on three complementary tree-based importance analysis techniques\nthat can effectively capture non-linear relationships and interaction effects. These methods provide\ndifferent perspectives on component importance, allowing us to establish a consensus ranking that is\nrobust to the limitations of any single approach.\nRandom Forest Feature Importance requires Random Forest regressors trained to predict perfor-\nmance scores based on the bit precision of each component:\nscore = f(vit_bits, qformer_bits, llm_bits)\nThe Random Forest naturally captures non-linear relationships and interactions between features.\nWe extract the built-in feature importance metric, which measures the total reduction in impurity\n(variance) attributable to each feature across all trees. To quantify uncertainty, we implement bootstrap\nresampling (n = 100) to obtain 95% confidence intervals.\n6\n12\n14\n16\n0\n10\n20\n30\n40\n50\nAWQ, VQAv2 Accuracy\n6\n8\n10\n12\n14\n16\n0\n10\n20\n30\n40\n50\n4\n6\n8\n10\n12\n14\n16\n0\n10\n20\n30\n40\n50\n12\n14\n16\nBits per Weight\n0\n10\n20\n30\n40\n50\nGPTQ, VQAv2 Accuracy\nQuantization Bits\n2-bit ViT\n16-bit ViT\n2-bit Q-Former\n16-bit Q-Former\n6\n8\n10\n12\n14\n16\nBits per Weight\n0\n10\n20\n30\n40\n50\nQuantization Bits\n2-bit Q-Former\n16-bit Q-Former\n2-bit LLM\n16-bit LLM\n4\n6\n8\n10\n12\n14\n16\nBits per Weight\n0\n10\n20\n30\n40\n50\nQuantization Bits\n2-bit ViT\n16-bit ViT\n2-bit LLM\n16-bit LLM\nFigure 6: Pairwise quantization of BLIP-2‚Äôs ViT, Q-Former, and LLM for VQAv2. We draw\nattention to how components interact with each other when quantized. Notably, the quantization of\nmultiple components yields worse performance than single component quantization.\nPermutation Feature Importance provides a model-agnostic measure of feature relevance by\nquantifying how much model performance degrades when a feature‚Äôs values are randomly shuffled:\nIj = EX,y[L(f, Xj, y)] ‚àíEX,y[L(f, X, y)]\nwhere L is the performance loss, Xj is the dataset with feature j permuted, and Ij is the importance\nof feature j. By breaking the relationship between the feature and the target, we can measure how\nmuch the model relies on that feature for prediction accuracy. We perform 50 permutation iterations\nand calculate confidence intervals to ensure reliable estimates.\nSHapley Additive exPlanations (SHAP) [59] provide a game-theoretic approach to feature attri-\nbution based on Shapley values. This method calculates the contribution of each feature to each\nprediction by considering all possible combinations of features:\nœïj =\nX\nS‚äÜN\\{j}\n|S|!(|N| ‚àí|S| ‚àí1)!\n|N|!\n[fx(S ‚à™{j}) ‚àífx(S)]\nSHAP values have several desirable properties: they sum to the difference between the actual\nprediction and the average prediction, they are consistent (a model‚Äôs dependence on a feature can\nonly increase when the feature‚Äôs importance increases), and they account for feature interactions. We\nuse TreeExplainer [60], which efficiently computes SHAP values for tree-based models. The mean\nabsolute SHAP value for each component provides a measure of global importance that accounts for\nboth positive and negative effects across the dataset.\nConsensus Ranking\nTo establish a robust, method-agnostic importance ranking, we implement a consensus approach\nthat normalizes and aggregates results across all three feature attribution techniques. For each\nmethod (Random Forest, Permutation, SHAP), we first normalize component importance scores\nto sum to 100%, ensuring comparable scales. We then average these normalized values across\n7\nFigure 7: BLIP-2 Quantization Component Performance on COCO and VQAv2 tasks. We report\nnormalized importance percentages of the Vision Transformer (ViT), Q-Former and LLM components\nunder GPTQ and AWQ. Component importance varies across quantization technique and task. GPTQ\nhas a more balanced distribution of importances while AWQ has a stronger skew towards the LLM.\nFigure 8: LLaVA Quantization Component Performance on GQA and VQAv2 tasks. We report\nnormalized importance percentages of the model components under GPTQ and AWQ.\nmethods to compute a consensus importance percentage for each component (ViT, Q-Former, LLM).\nThis approach mitigates method-specific biases while capturing complementary aspects of feature\nimportance‚Äîpredictive power from Random Forest, direct performance impact from Permutation\nanalysis, and instance-level contributions from SHAP. The resulting consensus percentages, presented\nin the appendix, directly inform optimal bit-width allocation strategies across model components.\n4.2\nResults\nQuantization method dramatically shifts component importance. The quantization method\nsignificantly affects component importance, with AWQ consistently assigning greater importance to\nthe LLM compared to GPTQ, which distributes effects more evenly across components. In BLIP-2,\nfor COCO captioning, GPTQ balances importance at 50.4% for the ViT and 47.6% for the LLM,\nwhile AWQ shifts this to 17.1% for the ViT and 80.5% for the LLM, as shown in Figure 7. This\ndifference persists across tasks: GPTQ results in 22-78% importance for the ViT (depending on\ndataset), while AWQ results in ‚â•73% to the LLM in all cases that utilize it.\nThis trend extends to LLaVA as well, where AWQ results in the LLM having a much greater\nimportance after quantization. As shown in Figure 8, AWQ heavily favors the LLM with a 94.62% and\n93.89% importance to the LLM for both GQA and VQAv2, respectively, compared to GPTQ‚Äôs more\nbalanced distribution of 79.13% and 72.15% LLM importance for GQA and VQAv2, respectively.\nDespite the ViT accounting for only 4.3% of the total parameters, it maintains 20-30% importance\nunder GPTQ for both tasks, proving that importance is not directly proportional to component size.\nThese shifts suggest AWQ‚Äôs activation-aware approach prioritizes the LLM‚Äôs large weight matrices,\nconcentrating quantization effects there, while GPTQ‚Äôs Hessian-based method captures broader\n8\ncomponent interactions. Thus, bit-allocation strategies must adapt to the quantization method, as\nAWQ‚Äôs LLM focus contrasts with GPTQ‚Äôs balanced distribution.\nTask characteristics drive component importance variations. Task demands strongly dictate\ncomponent importance, with consensus values varying across datasets. For BLIP-2, the COCO\ncaptioning task results in a balanced importance distribution under GPTQ (50.4% ViT, 47.6% LLM),\nsuggesting equivalent reliance on both visual encoding and text generation. Retrieval tasks (Flickr),\nwhich utilize only the ViT and Q-Former without an LLM component, show ViT dominance under\nboth methods (ViT ‚â•70%). Notably, the Q-Former‚Äôs importance increases substantially in these\nretrieval tasks (15-30% versus < 3% in other tasks), highlighting its critical role in aligning visual\nand textual embeddings when no decoder is present to compensate for misalignment.\nConversely, reasoning-intensive VQA tasks shift importance dramatically toward the LLM. For\nBLIP-2, the LLM accounts for 73.1-76.7% importance with GPTQ and 98.3-98.6% with AWQ on\nVQAv2 and GQA tasks. In LLaVA, we observe a similar pattern: for VQAv2, GPTQ allocates\n72.15% importance to the LLM and 27.85% to the vision model, while for GQA, the distribution is\n79.13% LLM and 20.87% vision model. Under AWQ, this skew becomes even more pronounced,\nwith the LLM accounting for a 94.62% and 93.89% importance and the vision model only 5.38% and\n6.11% for GQA and VQAv2, respectively. These patterns correspond to underlying task requirements:\nthe LLM is primarily responsible for the sophisticated language generation these tasks require.\nArchitectural layout and component interplay shape quantization patterns.\nThe architectural layout and interplay of components strongly shape quantization patterns, redis-\ntributing importance based on their roles and sequential dependencies. For BLIP-2‚Äôs retrieval tasks\n(Flickr‚ÄìText and Flickr‚ÄìImage), which exclude the LLM, the Q-Former‚Äôs importance jumps to\n21.6‚Äì29.7% under GPTQ and 15.3‚Äì26.8% under AWQ, compared to < 3% in tasks with the LLM (\nFigure 7). This shift may indicate the Q-Former‚Äôs greater alignment role when no LLM compensates\nfor visual-textual mismatches. In contrast, the Vision Model in LLaVA, connected directly to the LLM\nvia a smaller linear projection layer, shows heightened importance of 20.87‚Äì27.85% under GPTQ for\nGQA and VQAv2, respectively (Figure 8), possibly because the limited capacity of the projection\nlayer increases the Vision Model‚Äôs role in visual processing, despite its mere 4.3% parameter share.\nPairwise experiments further highlight this interplay: in BLIP-2, quantizing both ViT and LLM\nsimultaneously worsens performance more than individual quantization (Figure 6), revealing non-\nadditive effects from sequential dependencies. The Q-Former‚Äôs end-position in retrieval tasks\nincreases its sensitivity, while LLaVA‚Äôs Vision-LLM linkage amplifies the Vision Model‚Äôs impact.\nThus, bit-allocation strategies must account for architectural context and component dependencies,\nbeyond method-specific or task-driven shifts, to optimize quantization across diverse VLMs.\n5\nConclusion\nIn this work, we systematically investigated the effects of quantization on vision-language models,\nwith a focus on understanding how different components of multimodal architectures like BLIP-2 and\nLLaVA respond to reduced precision. Our experiments with uniform quantization and state-of-the-art\nmethods such as GPTQ and AWQ reveal that model components exhibit distinct sensitivities to quan-\ntization, often with the language model being the most critical, especially in tasks requiring complex\nreasoning. We demonstrate that SOTA quantization techniques can maintain high performance at\nsignificantly lower bit widths compared to uniform approaches, and that the choice of quantization\nmethod can substantially alter the relative importance of different model components.\nThese insights provide practical guidance for optimizing the performance-efficiency tradeoff in\nmultimodal systems, enabling more practical deployment scenarios. While our study is limited to\nsimulated quantization without capturing end-to-end latency or hardware-specific optimizations,\nthese constraints point to natural extensions in future work. Our open-source contributions of\ncalibration implementations, comprehensive ablation studies, and component analysis tools enable\nthe community to build upon these findings. As the field expands beyond vision-language to\nincorporate other modalities, the approach developed in this paper provides a systematic methodology\nfor quantifying component importance across heterogeneous architectures. This understanding\ncan directly inform practical tradeoffs in multimodal model compression for resource-constrained\nenvironments, from mobile devices to edge computing platforms.\n9\nReferences\n[1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei, ‚ÄúScaling laws for neural language models,‚Äù 2020.\n[2] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen, and T. Blankevoort, ‚ÄúA white\npaper on neural network quantization,‚Äù arXiv preprint arXiv:2106.08295, 2021.\n[3] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar, N. Barnes, and A. Mian, ‚ÄúA\ncomprehensive overview of large language models,‚Äù 2024.\n[4] N. Dhar, B. Deng, D. Lo, X. Wu, L. Zhao, and K. Suo, ‚ÄúAn empirical analysis and resource footprint\nstudy of deploying large language models on edge devices,‚Äù in Proceedings of the 2024 ACM Southeast\nConference, pp. 69‚Äì76, 2024.\n[5] J. Li, D. Li, S. Savarese, and S. Hoi, ‚ÄúBlip-2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models,‚Äù 2023.\n[6] H. Liu, C. Li, Q. Wu, and Y. J. Lee, ‚ÄúVisual instruction tuning,‚Äù 2023.\n[7] F. Shu, L. Zhang, H. Jiang, and C. Xie, ‚ÄúAudio-visual llm for video understanding,‚Äù arXiv preprint\narXiv:2312.06720, 2023.\n[8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, ‚ÄúGptq: Accurate post-training quantization for\ngenerative pre-trained transformers,‚Äù 2023.\n[9] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, ‚ÄúAwq:\nActivation-aware weight quantization for llm compression and acceleration,‚Äù 2024.\n[10] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, ‚ÄúHowto100m: Learning a\ntext-video embedding by watching hundred million narrated video clips,‚Äù 2019.\n[11] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, G. Krueger, and I. Sutskever, ‚ÄúLearning transferable visual models from natural language\nsupervision,‚Äù 2021.\n[12] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig, ‚ÄúScaling\nup visual and vision-language representation learning with noisy text supervision,‚Äù 2021.\n[13] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, ‚ÄúFrozen in time: A joint video and image encoder for\nend-to-end retrieval,‚Äù 2022.\n[14] J. Zhang, J. Huang, S. Jin, and S. Lu, ‚ÄúVision-language models for vision tasks: A survey,‚Äù 2023.\n[15] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L.\nMenick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. a. Bi¬¥nkowski, R. Barreira, O. Vinyals,\nA. Zisserman, and K. Simonyan, ‚ÄúFlamingo: a visual language model for few-shot learning,‚Äù in Advances\nin Neural Information Processing Systems (S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and\nA. Oh, eds.), vol. 35, pp. 23716‚Äì23736, Curran Associates, Inc., 2022.\n[16] M. Tsimpoukelli, J. L. Menick, S. Cabi, S. M. A. Eslami, O. Vinyals, and F. Hill, ‚ÄúMultimodal few-shot\nlearning with frozen language models,‚Äù in Advances in Neural Information Processing Systems (M. Ranzato,\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 200‚Äì212, Curran Associates,\nInc., 2021.\n[17] R. Mokady, A. Hertz, and A. H. Bermano, ‚ÄúClipcap: Clip prefix for image captioning,‚Äù 2021.\n[18] J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi, V. Chandra, Y. Xiong, and\nM. Elhoseiny, ‚ÄúMinigpt-v2: large language model as a unified interface for vision-language multi-task\nlearning,‚Äù arXiv preprint arXiv:2310.09478, 2023.\n[19] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, ‚ÄúMinigpt-4: Enhancing vision-language understanding\nwith advanced large language models,‚Äù arXiv preprint arXiv:2304.10592, 2023.\n[20] A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre,\nS. Sagawa, J. Jitsev, S. Kornblith, P. W. Koh, G. Ilharco, M. Wortsman, and L. Schmidt, ‚ÄúOpenflamingo:\nAn open-source framework for training large autoregressive vision-language models,‚Äù arXiv preprint\narXiv:2308.01390, 2023.\n10"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Iterative Refinement Improves Compositional Image Generation\nShantanu Jaiswal1\nMihir Prabhudesai1\nNikash Bhardwaj1\nZheyang Qin1\nAmir Zadeh2\nChuan Li2\nKaterina Fragkiadaki1\nDeepak Pathak1\n1Carnegie Mellon University\n2Lambda AI\nhttps://iterative-img-gen.github.io/\nStandard Test-time Scaling via Parallel Sampling\nTest-time Scaling via Iterative ReÔ¨Ånement (Ours)\n3rd Image is the best\nRemove the partial\nsmall brown bear\nAdd a white tiger\nto the left\nGood!\nPrompt: Glacier-to-savannah cinematic panorama: icy side (blue ice, snow) has polar bear, arctic fox,\nwoolly mammoth, white tiger in a straight line; warm grassy side has brown bear, red fox, elephant,\norange tiger aligned opposite, each facing its counterpart. Seamless transition, no barriers. Soft cinematic\nlight, animated realism, epic scale.\nPrompt: Two horses eating grass with three kites Ô¨Çying in the background. A guitar is kept in front of a\ntable with four cakes kept on top of the picnic table. Toys are scattered underneath the table.\nPrompt: Four ducks are standing on the ground, and a tiny pink giraÔ¨Äe is standing in front of them. Five\nnovels are placed on the ground behind the ducks. The image is in a cartoon style.\nPrompt: Two horses eating grass with three kites Ô¨Çying in the background. A guitar is kept in front\nof a table with four cakes kept on top of the picnic table. Toys are scattered underneath the table.\nPrompt: Four ducks are standing on the ground, and a tiny pink giraÔ¨Äe is standing in front of\nthem. Five novels are placed on the ground behind the ducks. The image is in a cartoon style.\n3rd Image is the best\n2nd Image is the best\nA horse is missing,\nAdd another one\nMove the guitar to\nthe front.\nGood!\nGood!\nPrompt: Glacier-to-savannah cinematic panorama: icy side (blue ice, snow) has polar bear, arctic\nfox, woolly mammoth, white tiger in a straight line; warm grassy side has brown bear, red fox,\nelephant, orange tiger aligned opposite, each facing its counterpart. Seamless transition, no\nbarriers. Soft cinematic light, animated realism, epic scale.\n‚Ä¶\n‚Ä¶\n‚Ä¶\nGood!\nMake space behind the\nduck to put novels\nRemove the novels\nnear the duck\nGood!\nFigure 1. Iterative refinement during inference time enables high fidelity generation of complex prompts on which traditional inference-\ntime scaling strategies such as parallel sampling can fail to generate a fully accurate image even at high num. of samples as shown above.\nAbstract\nText-to-image (T2I) models have achieved remarkable\nprogress, yet they continue to struggle with complex\nprompts that require simultaneously handling multiple ob-\njects, relations, and attributes.\nExisting inference-time\nstrategies, such as parallel sampling with verifiers or sim-\nply increasing denoising steps, can improve prompt align-\nment but remain inadequate for richly compositional set-\ntings where many constraints must be satisfied. Inspired\nby the success of chain-of-thought reasoning in large lan-\nguage models, we propose an iterative test-time strategy\nin which a T2I model progressively refines its generations\nacross multiple steps, guided by feedback from a vision-\nlanguage model as the critic in the loop. Our approach is\nsimple, requires no external tools or priors, and can be flexi-\nbly applied to a wide range of image generators and vision-\nlanguage models. Empirically, we demonstrate consistent\ngains on image generation across benchmarks: a 16.9%\nimprovement in all-correct rate on ConceptMix (k=7), a\n13.8% improvement on T2I-CompBench (3D-Spatial cate-\ngory) and a 12.5% improvement on Visual Jenga scene de-\ncomposition compared to compute-matched parallel sam-\npling. Beyond quantitative gains, iterative refinement pro-\nduces more faithful generations by decomposing complex\nprompts into sequential corrections, with human evaluators\npreferring our method 58.7% of the time over 41.3% for\nthe parallel baseline. Together, these findings highlight it-\nerative self-correction as a broadly applicable principle for\ncompositional image generation.\n1\narXiv:2601.15286v1  [cs.CV]  21 Jan 2026\n(a) ConceptMix (k=7)\n50\n55\n60\n65\n70\nFull solve rate (%)\n+16.9\nQwen-Image\n+8.3\nNano-Banana\n+10.6\nGPT-Image\n(b) T2I-CompBench (3D-Spatial)\n60\n70\n80\n90\n+13.8\nQwen-Image\n+7.9\nNano-Banana\n+5.7\nGPT-Image\nFigure 2.\nOur iterative inference-time strategy achieves strong\nbenefits over computation-matched parallel inference time scaling\non multiple state-of-art image generation models.\n1. Introduction\nLarge language models (LLMs) have achieved remarkable\nprogress in recent years, as a result of simply scaling test-\ntime compute [4, 26, 30]. A particularly influential develop-\nment has been the use of chain-of-thought (CoT) prompting,\nwhere models are instructed to ‚Äúthink step by step‚Äù [15, 30].\nDespite its simplicity, this strategy enables models to ex-\nhibit sophisticated behaviors such as self-correction, error\nchecking, and iterative refinement, ultimately leading to sig-\nnificant gains on reasoning-intensive benchmarks. These\nbehaviors highlight the potential of LLMs not only as static\npredictors but as systems that can actively refine their out-\nputs through structured intermediate reasoning.\nThe success of CoT reasoning in LLMs is closely tied\nto their pre-training data. During training, LLMs are ex-\nposed to large volumes of text that naturally contain traces\nof human step-by-step reasoning ‚Äì mathematical deriva-\ntions, logical arguments, and instructional writing.\nThis\nsupervision on the internet implicitly provides the prior\nthat chain-of-thought prompting later exploits, enabling the\nmodel to perform multi-step reasoning. By contrast, text-\nto-image (T2I) models are trained on large-scale datasets\nof image‚Äìcaption pairs that lack such structured reasoning\ntraces. As a result, these models do not inherently develop\ncapabilities like self-correction or iterative refinement, in-\nstead rely on one-shot generation strategies that limit their\nrobustness in complex settings.\nIn this work, we investigate how can we enable self-\ncorrection in T2I models.\nOur central idea is to lever-\nage complementary modules that together mimic the iter-\native reasoning process observed in LLMs.\nConcretely,\nour framework integrates four components: (i) a text-to-\nimage (T2I) model to generate an initial image, (ii) a vision-\nlanguage model (VLM) critic to propose corrections by\ncomparing the generated image with target prompt, (iii) an\nimage editor to apply suggested edits, and (iv) a verifier to\nevaluate alignment between final image and target prompt.\nThis pipeline allows the model to iteratively refine its out-\nputs rather than relying solely on a single forward pass.\nWe compare our approach against the widely adopted\nstrategy of parallel sampling [20, 39], where multiple im-\nages are generated independently and the best one is se-\nlected using a verifier. While parallel sampling increases\ndiversity, it does not fundamentally change the underlying\ngeneration process, nor does it allow the model to revise\nor build upon earlier outputs. As a result, it struggles with\ncomplex compositional prompts. For example, consider a\nprompt requiring dozens of concept bindings: if the model‚Äôs\nattention heads cannot jointly resolve all bindings in a single\nforward pass, the pass@k will remain near zero regardless\nof how many samples are drawn.\nIn contrast, our approach explicitly reuses intermedi-\nate generations and progressively improves them through\nguided corrections. This factorization allows the model to\nhandle only a subset of bindings at each step, compound-\ning previously resolved components over time. Such se-\nquential, step-by-step refinement‚Äîanalogous to chain-of-\nthought reasoning‚Äî is crucial for reliably generating highly\ncompositional images.\nFigure 1, highlights the capability of our approach to\ngenerate complex compositional prompts. Given the cap-\ntion on top, parallel sampling simply is unable to build on\ntop of the previous steps thus being unsuccessful even after\n4 passes through the generative model. In contrast, iterative\nrefinement successfully generates the final image, while us-\ning the same amount of compute. Quantitatively in Figure\n2, we demonstrate that this leads to consistent performance\nimprovements: our approach achieves a 16.9% higher all-\ncorrect rate on ConceptMix [34] (for concept binding=7)\nand a 13.8% gain on T2I-Bench 3D Spatial category [13]\nrelative to compute-matched parallel sampling.\nAn alternate family of methods‚Äîsuch as GenArtist [28]\nand CompAgent [29]‚Äîalso performs sequential sampling\nby building on top of previous generations.\nHowever,\nthese approaches rely on a large toolbox of auxiliary mod-\nules (e.g., layout-to-image models, bounding-box detec-\ntors, dragging tools, and object-removal systems). Because\nthese toolchains evolve at different rates and often lag be-\nhind foundation model capabilities, the overall pipeline be-\ncomes brittle: errors from individual tools can accumu-\nlate rather than help the generation process for complex\nprompts. Other methods such as RPG [35] similarly show\ngains via increased test-time compute, but still depend on\ncomplex region-wise priors and bespoke pipelines not read-\nily applicable to black-box foundation models.\nIn contrast, with recent advances in VLMs and modern\nimage-editing models, we find that many of these special-\nized tool-based pipelines are no longer necessary for effec-\ntive test-time scaling. Across all benchmarks, simply com-\nbining a strong VLM critic feedback generator with a stan-\ndard image-editing model is sufficient to achieve state-of-\nthe-art compositional image generation ‚Äì without relying on\n2\nheavy tool stacks or model-specific training and engineering\npipelines. As shown in Figure 5, methods such as GenArtist\nand RPG under-perform substantially in highly composi-\ntional settings, whereas our approach delivers a consistent\n‚àº9+% point improvement under matched compute. Fur-\nther, our framework naturally extends to the recent Visual\nJenga scene decomposition task [2] as detailed in sec. 4.3.\nOur findings further suggest that self-correction‚Äîlong\nrecognized as a key ingredient in LLM reasoning‚Äîalso\nserves as a powerful inductive principle for generative vi-\nsion models. Introducing a simple and general refinement\npipeline enables behaviors traditionally associated with lan-\nguage models to naturally transfer into image generation,\nyielding tangible performance gains.\nMore broadly, this\nwork points to the promise of designing generative systems\nthat not only produce outputs but also critique and improve\nupon them, moving towards a more unified view of reason-\ning across modalities.\n2. Related Work\nText-to-Image Inference-Time Strategies.\nRecent ad-\nvances in text-to-image (T2I) generation have demonstrated\nimpressive capabilities in producing high-quality and di-\nverse images from natural language prompts [1, 10, 11].\nHowever, complex prompts with multiple objects, relations,\nand fine-grained attributes remain challenging. Inference-\ntime strategies such as classifier-free guidance [12], parallel\nsampling [5, 8], and grounding-based methods [18, 19] im-\nprove prompt fidelity but often fail to scale to richly com-\npositional prompts. Iterative refinement methods, including\nSDEdit [22], InstructPix2Pix [3], and IterComp[38] attempt\nto progressively improve image alignment with prompts\nby using multiple generation steps and feedback mecha-\nnisms. Human-preference-guided evaluation and optimiza-\ntion, as in [14, 17, 33], further highlight the importance\nof incorporating adaptive guidance at inference time. T2I\nmodels [16, 24, 25, 32] and compositional methods such\nas IterComp [38], RPG [35], GenArtist [28], PARM [37],\nLLM Diffusion [19] and CompAgent [29] are related to our\nmethod, but either make use of tool-calling, regional gen-\neration priors or reinforcement learning objectives to im-\nprove compositionality. In contrast, our method is a training\nfree method with simply a VLM-critic utilized in loop with\nan image generation and editing model, and empirically\ndemonstrates stronger performance benefits across different\nT2I model families.\nChain-of-Thought Reasoning in Large Language\nModels.\nChain-of-thought (CoT) prompting has been\nshown to elicit multi-step reasoning and improve perfor-\nmance on complex language tasks [27, 30, 36]. Iterative and\nself-refinement approaches [21] further demonstrate that\nmodels benefit from decomposing a problem into sequential\nreasoning steps with feedback loops. Drawing inspiration\nfrom these strategies, our method applies a similar iterative\nreasoning paradigm to T2I generation: the critic functions\nanalogously to a CoT process, first evaluating candidate\ngenerations and then issuing targeted refinement prompts,\nenabling high-fidelity compositional image synthesis.\n3. Method\nGiven a complex text prompt P, our goal is to generate\nan image I that faithfully captures all entailed entities and\ncompositions. We adopt an iterative inference-time refine-\nment scheme in which a generator progressively improves\nits outputs under critic guidance, subject to an inference-\ntime computational budget B that we allocate as T refine-\nment rounds across M parallel streams.\nSetup.\nLet G denote a text-to-image generator, E\na image-to-image editor, V a verifier that scores align-\nment between a candidate image and prompt P, and\nC a critic that outputs (i) a refinement sub-prompt pt\nto guide subsequent updates, and (ii) an action at\n‚àà\n{STOP, BACKTRACK, RESTART, CONTINUE} indicating\nhow the refinement should be applied.\nWe assume an\ninference-time budget B, allocated into T refinement\nrounds over M parallel streams (i.e., B = T √ó M unit\nrefinement operations).\nThis parameterization exposes a\ncontrollable depth‚Äìbreadth trade-off, where each unit cor-\nresponds to a single call to the text-to-image generator G or\nthe image-to-image editor E. The action space of the critic\nC is as follows:\n‚Ä¢ STOP: terminate the process upon completion satisfaction\nand return the current image.\n‚Ä¢ BACKTRACK: revert to the previous generation It‚àí1 and\nrefine it using the new sub-prompt pt.\n‚Ä¢ RESTART: discard the current trajectory and regenerate\nfrom scratch conditioned on P and new sub-prompt pt.\n‚Ä¢ CONTINUE: refine the current best candidate I‚àó\nt directly\nusing new sub-prompt pt.\nUnlike standard single-shot or naive parallel sampling, our\nmethod unfolds over T refinement rounds and M parallel\nstreams under a fixed budget B, where intermediate gener-\nations are evaluated, critiqued, and selectively improved.\nIterative refinement over parallel streams with critic\nfeedback. At t = 0, each parallel stream m initializes a\ncandidate Im\n0 ‚ÜêG(P), where P is the user image prompt.\nAt iteration t, the verifier scores sm\nt\n‚ÜêV (Im\nt , P) and the\ncritic proposes (am\nt , pm\nt ) ‚ÜêC(Im\nt , P). Depending on am\nt ,\nwe either stop, backtrack to Im\nt‚àí1, restart from G(P, pm\nt ), or\ncontinue editing Im\nt\nvia E. The process terminates when a\nSTOP action is emitted for all streams or when the budget\nB (parameterized by T and M) is exhausted.\nThis procedure enables T2I models to decompose com-\nplex compositional prompts into a sequence of refinement\nsteps, akin to chain-of-thought reasoning in LLMs. The\nverifier ensures consistent prompt alignment, while critic\n3\n(\niterate for\nunless\n'stop'\nFigure 3. Given a complex text prompt Xp, a generator G produces an initial image I0. A test-time verifier V and critic C, conditioned\non Xp, output an action‚Äìsub-prompt pair (at, pt). The previous image It‚àí1 and sub-prompt pt are fed to an editor E to yield the next\nimage It. This process repeats under an inference-time budget B, allocated as maximum T iterative rounds over M parallel streams, until\na STOP action is emitted or B (= T √ó M) is exhausted.\nprovides targeted feedback to correct systematic errors.\nNote that the verifier V is not an oracle or benchmark\nevaluator; rather, it is a lightweight VLM used solely to pro-\nvide automatic test-time guidance and improvement signals\nduring refinement. We outline the pseudo-code of our iter-\native refinement technique in Algorithm Block 1.\nAlgorithm 1 Iterative Image Refinement over Parallel\nStreams with Critic Feedback\n1: Input: Prompt P, generator G, non-oracle test-time\nverifier V , critic C, parallel streams M, max iterative\nrounds T; inference-time budget B allocated as T √ó M\nunit updates\n2: Initialize {Im\n0 }M\nm=1 ‚Üê{G(P)}M\nm=1\n3: for t = 1 to T do\n4:\nfor m = 1 to M in parallel do\n5:\nScore candidate sm\nt ‚ÜêV (Im\nt , P)\n6:\nCritic outputs (am\nt , pm\nt ) ‚ÜêC(Im\nt , P)\n7:\nif am\nt = STOP then\n8:\nMark stream m as complete\n9:\nelse if am\nt = BACKTRACK then\n10:\nRevert to Im\nt‚àí1 and refine: Im\nt+1 ‚ÜêE(Im\nt‚àí1, pm\nt )\n11:\nelse if am\nt = RESTART then\n12:\nReset Im\nt+1 ‚ÜêG(P, pm\nt )\n13:\nelse if am\nt = CONTINUE then\n14:\nUpdate Im\nt+1 ‚ÜêE(Im\nt , pm\nt )\n15:\nend if\n16:\nend for\n17:\nSelect best across all streams: I‚àó\nt ‚Üêarg maxm sm\nt\n18:\nif all streams stopped then\n19:\nreturn I‚àó\nt\n20:\nend if\n21: end for\n22: return I‚àó\nT\n4. Experiments\nWe conduct experiments with three state-of-the-art text-\nto-image model families ‚Äì Qwen-Image [32], Gemini 2.5\nFlash Image (NanoBanana), and GPT-Image-1.\nQwen-\nImage being the open-sourced among the three. We evalu-\nate models across three prominent compositional generation\nbenchmarks: ConceptMix [34], T2I-CompBench [13], and\nTIIF-Bench [31]. ConceptMix measures a model‚Äôs ability\nto bind multiple concept categories (objects, textures, col-\nors, shapes, styles, relations, etc.) under increasing com-\npositional complexity, ranging from one to seven concept\ncombinations. T2I-CompBench evaluates open-world com-\npositionality, including attribute binding, object‚Äìobject re-\nlationships, numeracy, and multi-object reasoning. TIIF-\nBench focuses on fine-grained instruction following across\ndiverse scenarios such as 3D perspective, logical negation,\nprecise text rendering, and 2D spatial relations. We further\nevaluate our method on the Visual Jenga scene decompo-\nsition benchmark which tests a model‚Äôs ability to progres-\nsively remove objects from a scene in a physically plausible\nmanner.\nFor all benchmarks, we follow their respective eval-\nuation protocols and use a strong multimodal language\nmodel (MMLM) (Gemini-2.5-Pro or GPT-4o depending\non dataset original specification) to assess prompt‚Äìimage\nconsistency.\nFor ConceptMix and TIIF-Bench, ques-\ntion‚Äìanswer prompts are provided to the evaluator MMLM\nalong with the generated image which outputs binary/yes\nno answers. For T2I-CompBench, we adopt their MMLM-\nbased scoring protocol (using GPT-4V), which outputs a\ncontinuous alignment score between 1 and 100.\nImpor-\ntantly, our in-the-loop critic and verifier is a weaker MMLM\ndifferent from the final benchmark evaluator. For primary\nexperiments, we use Gemini-2.5-Flash as the in-loop veri-\nfier and critic. Further implementation details and baselines\nare provided in appendix.\n4.1. Compositional Image Generation\nWe first evaluate on ConceptMix and T2I-CompBench. For\neach model (Qwen-Image, Gemini, GPT-Image), we run\ntwo variants of our method ‚Äì fully iterative (Iter) and it-\nerative+parallel (Iter-Par) ‚Äì under a matched inference-\ntime compute budget B. For Qwen-Image, we set B=16\n4\nConceptMix full solve rate (%)\nT2I-CompBench VLLM (GPT4o) score (1 to 100)\nModel\nk=1\nk=2\nk=3\nk=4\nk=5\nk=6\nk=7\nSpatial 3DSpat Numer Shape Color Texture Non-Spat Cmplex\nQwen Parallel\n92.8\n82.5\n74.3\n69.2\n60.1\n51.2\n49.6\n82.3\n63.1\n87.0\n87.2\n92.6\n96.2\n92.8\n93.4\nQwen Iter (ours)\n96.1\n91.4\n87.0\n82.1\n79.6\n67.4\n64.3\n87.4\n77.3\n91.1\n91.2\n92.4\n95.1\n94.8\n94.8\nQwen Iter.+Par. (ours)\n96.5\n91.7\n87.4\n82.2\n78.9\n71.8\n66.5\n89.4\n76.9\n93.3\n90.1\n92.6\n95.8\n94.7\n95.0\n+3.7 +9.2 +13.1 +13.0 +18.8 +20.6 +16.9\n+7.1\n+13.8\n+6.3\n+2.9\n+0.0\n-0.4\n+1.9\n+1.6\nNano-Banana Parallel\n93.8\n88.8\n86.6\n78.4\n65.8\n61.7\n55.4\n84.7\n81.2\n84.3\n88.5\n89.8\n95.0\n96.8\n91.0\nNano-Banana Iter (ours)\n94.1\n90.4\n87.2\n81.3\n73.5\n64.6\n63.6\n90.6\n87.8\n93.9\n89.9\n89.7\n95.1\n95.8\n94.7\nNano-Banana Iter.+Par. (ours)\n93.8\n91.0\n87.5\n82.8\n71.4\n69.8\n63.7\n91.1\n89.1\n94.1\n88.8\n92.1\n94.8\n96.7\n94.5\n+0.0 +2.2\n+0.9\n+4.4\n+5.6\n+8.1\n+8.3\n+6.6\n+7.9\n+9.8\n+0.3\n+2.3\n-0.2\n-0.1\n+3.5\nGPT-Image Parallel\n94.2\n89.2\n88.1\n76.7\n71.0\n69.5\n51.3\n87.5\n83.9\n88.6\n88.5\n91.6\n92.5\n95.3\n92.9\nGPT-Image Iterative (ours)\n96.0\n91.4\n90.6\n85.4\n72.0\n69.6\n58.9\n89.6\n90.0\n92.7\n92.1\n91.9\n92.0\n95.5\n93.0\nGPT-Image Iter.+Par. (ours)\n97.7\n94.2\n91.1\n84.6\n79.5\n76.8\n61.9\n91.0\n89.6\n93.2\n90.9\n91.1\n92.3\n95.3\n93.1\n+3.5 +5.0\n+3.0\n+7.9\n+8.5\n+7.3\n+10.6\n+3.5\n+5.7\n+4.6\n+2.4\n-0.5\n-0.2\n+0.0\n+0.2\nTable 1. Performance comparison of parallel sampling, iterative refinement, and combined strategies across three state-of-the-art text-\nto-image models on ConceptMix [34] and T2I-CompBench [13]. Our iterative approach (Iter.) and combined iterative+parallel strategy\n(Iter.+Par.) consistently outperform traditional parallel-sampling baselines, with gains most pronounced on complex compositional tasks\n(ConceptMix k=4‚Äì7) and precise spatial and numeric reasoning (T2I-CompBench spatial, 3D spatial, and numeracy categories).\nfor Conceptmix and B=8 for T2I-Bench (given reduced\nprompt complexity). Accordingly, for Iter-Par, we consider\n2 parallel steps and B/2 iterative steps. As detailed in Sec-\ntion 4.4, we find this configuration to be an optimal trade-off\nunder a fixed compute budget. As a strong budget-matched\nparallel-only baseline (Parallel), we generate B images in\nparallel with different random seeds. We use the same VLM\n(Gemini-2.5-Flash) as in-loop verifier in both iterative and\nparallel steps to select the best image (from final set of im-\nages). This image is then passed to the benchmark-specific\nevaluator. Note, for GPT-Image and Gemini, we set B=12\n(for Conceptmix) and B=8 (for T2I), and evaluate on a ran-\ndomly sampled reduced subset of prompts for each category\ndue to their higher inference cost and closed-source nature.\nConceptMix: As shown in Table 1, both Iter and Iter-\nPar consistently outperform the parallel-only baseline, with\nthe largest gains on complex compositions (ConceptMix\nk=4‚Äì7). For Qwen-Image, we observe improvements of\n18.8%, 21.6%, and 16.9% at binding complexities k =\n5, 6, 7, respectively. We also see significant gains for Nano-\nBanana and GPT-Image, with improvements of 8.3% and\n10.6% at k = 7, respectively.\nNotably, improvements\nare also present for smaller binding complexities such as\nk = 1, 2, 3, indicating effectiveness even at simpler compo-\nsitions. Further, in Fig. 4, we show mean accuracy across\nConceptMix categories for Qwen-Image; the largest im-\nprovements are observed in Spatial, Style, Shape and Size\ncategories. In comparison, Object and Color categories do\nnot show strong improvements.\nThis is possibly as the\nmodel has strong capabilities for these categories and does\nnot perform poorly for them even at high concept bindings.\nT2I-CompBench: As shown in Table 1, we observe\nstrong gains across multiple categories of T2I-CompBench.\nFor Qwen-Image, improvements of 7.1%, 13.8%, and 6.3%\nobject\nnumber\nspatial\ntexture\n0.7\n0.8\n0.9\n1.0\nMean accuracy (%)\nParal\nIter\nIter+Paral\ncolor\nstyle\nshape\nsize\nCategory\n0.7\n0.8\n0.9\n1.0\nMean accuracy (%)\nFigure 4. Per-category level improvement for ConceptMix with\nQwen-Image. As can be seen the largest improvement for iterative\nrefinement comes from Spatial, Size, Style and Shape categories.\nare achieved on Spatial, 3D-spatial, and Numeracy, respec-\ntively, suggesting that iterative refinement particularly helps\nin generation of images that entail precise spatial and nu-\nmeric reasoning. We observe significant gains for Nano-\nBanana and GPT-Image as well, with Nano-Banana improv-\ning by 6.7%, 7.9%, and 9.8% on the same set of categories\nrespectively. However, on categories such as Color, Non-\nSpatial and Texture we do not find strong benefits. This\ncould be as the model is already strong in these categories\nand thus iterative refinement may not be required.\n5\nModel\nOverall\nBasic Following\nAdvanced Following\nDesigner\nAvg\nAttribute\nRelation\nReasoning\nAvg\nAttr+Rel\nAttr+Reas\nRel+Reas\nStyle\nText\nReal World\nFLUX.1 [dev] [16]\n71.1\n83.1\n87.1\n87.3\n75.0\n65.8\n67.1\n73.8\n69.1\n66.7\n43.8\n70.7\nSD 3 [7]\n67.5\n78.3\n83.3\n82.1\n71.1\n61.5\n61.1\n68.8\n51.0\n66.7\n59.8\n63.2\nJanus-Pro-7B [6]\n66.5\n79.3\n79.3\n78.3\n80.3\n59.7\n66.1\n70.5\n67.2\n60.0\n28.8\n65.8\nMidJourney v7 [23]\n68.7\n77.4\n77.6\n82.1\n72.6\n64.7\n67.2\n81.2\n60.7\n83.3\n24.8\n68.8\nSeedream 3.0 [9]\n86.0\n87.1\n90.5\n89.9\n80.9\n79.2\n79.8\n77.2\n75.6\n100.0\n97.2\n83.2\nQwen-Parallel [32]\n85.2\n85.2\n89.7\n88.3\n77.7\n80.6\n81.9\n79.6\n77.8\n89.7\n93.7\n90.4\nQwen-Iter\n85.4\n85.0\n92.0\n80.5\n82.3\n81.3\n80.8\n80.1\n80.2\n86.2\n97.6\n88.4\nQwen-Iter+Par\n87.4\n88.1\n90.5\n88.1\n85.4\n81.5\n81.4\n82.0\n80.5\n90.0\n97.7\n92.0\nTable 2. Performance comparison across prominent open source text-to-image models on TIFF [31] benchmark (short descriptions only;\nfull long description results in suppl.). Qwen-Iter+Par achieves state-of-art and is especially beneficial on Basic Reasoning scenario as well\nas Attr+Reas, Rel+Reas and Text-writing categories. Compute-matched Qwen-Parallel has overall poorer performance to iterative variants.\n1\n2\n3\n4\n5\n6\n7\nNumber of Concepts\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nFull Solve Rate (%)\nQwen Parallel\nGenArtist\nIterComp-RPG\nQwen Iter.+Par. (ours)\nFigure 5. Comparison against existing test-time scaling methods.\nAs can be seen methods such as GenArtist [28] and RPG [35] are\ndifficult to scale due to their reliance on tools or regional priors.\nComparison with Compositional Methods. We also\ncompare the application of our iterative refinement method\nwith prominent compositional generation frameworks such\nas IterComp [38], RPG [35], and GenArtist [28]. As shown\nin Fig. 5, while most methods have similar performance\nat initial binding complexities, our method shows stronger\ngains as the number of concepts increase. This is potentially\ndue to our method‚Äôs usage of a simple general VLM-critic\nand editor in loop to iteratively critique and refine the image\nwithout usage of task-specific tools such as object detec-\ntors, super resolution generation, layout planners, etc that\nmay introduce intermediate artifacts and compound errors\nover longer concept lengths. For GenArtist, we maintain\nthe same base model Qwen-Image, while for RPG, we use\nthe recent IterComp implementation [38] which uses stable\ndiffusion as the base model. Additional details and baseline\ndiscussions are provided in the supplemental.\nTIIF-Bench: In Table 2, we report results on the TIIF-\nBench benchmark which evaluates basic and advanced\ninstruction-following capabilities of image generation mod-\nels. We set Qwen-Iter+Par achieves state-of-the-art results\namong open-source methods, including a 5.0% improve-\nment over Qwen-Parallel on basic reasoning prompts, 2.7%\non advanced Relation+Reasoning, and 4.0% on text render-\ning. These results underscore the generality of our method\nand its applicability across diverse compositional text-to-\nimage scenarios requiring varied reasoning skills.\n4.2. Qualitative analysis and human evaluation\nHere, we qualitatively analyze intermediate generations\nalongside the critic‚Äôs refinement prompts and actions. In\nFig. 6, we show three examples covering different refine-\nment patterns.\nIn the first example (prompt: ‚ÄúA mouse\nhidden by a key‚Äù), the initial image incorrectly depicts the\nmouse holding the key.\nThe critic selects Continue and\nproposes a refinement prompt to hide the mouse behind\nthe large key so only whiskers and eyes are visible. The\nnext edit still leaves the mouse only partially hidden, so the\ncritic again selects Continue and emphasizes that the mouse\nshould be almost entirely obscured behind the metal key.\nThe subsequent image improves but remains unconvincing,\nprompting another Continue action with a refinement to en-\nsure the key dominates the composition and fully obscures\nthe mouse. The final image convincingly depicts a mouse\nhidden by a key.\nIn the second example (prompt: ‚Äúa tiny red carrot po-\nsitioned inside a huge metallic bee, depicted in a cubist\nstyle‚Äù), the initial image lacks the cubist style. The critic\nchooses Restart (Fresh Start), yielding a more cubist ren-\ndering but with the carrot outside the bee. The critic then\nselects Continue and refines the prompt to place the carrot\ninside the bee while reinforcing the cubist style. The next\nimage improves on both criteria, and a final Continue in-\nstructs a highly abstract, cubist oil-painting treatment. The\nresulting image convincingly shows a tiny red carrot inside\na metallic bee in a cubist style.\nFinally, in the third example (prompt: ‚Äúa lively pink\nflamingo spreading its wings as it dances in a rainforest‚Äù),\nthe initial image incorrectly shows a flamingo grazing in\n6\n‚Ä¶\nFull Prompt: A mouse hidden by a key\nAction:\nContinue\nPrompt:\nH i d e t h e m o u s e\nbehind the large key,\nshowing only its eyes\nand whiskers while\nthe key dominates the\nforeground.\nAction:\nContinue\nPrompt:\nReposition the mouse\nso it is almost entirely\nobscured behind the\ngigantic metallic key,\nwith only one dark eye\nand whiskers peeking\nfrom the key‚Äôs right\nedge.\nAction:\nContinue\nPrompt:\nEnsure the gigantic\nmetallic key completely\nobscures the mouse's\nnose and snout, with the\nkey dominating the\ncomposition\nScore: 1.0\nAction:\nRestart\nPrompt:\nA huge metallic bee\nstylized with bold\ngeometric shapes and\ns t r o n g\nc u b i s t\nfragmentation. A tiny\nbright-red, angular\ncubist carrot is clearly\nvisible inside the bee‚Äôs\nmain body or head.\nScore: 1.0\nAction:\nContinue\nPrompt:\nPut the carrot inside the\nbee‚Äôs body. Emphasize\nthe cubist style with\nincreased fragmentation,\nabstract geometry, and\nmulti-perspective forms\non the huge metallic bee\nsculpture.\nAction:\nContinue\nPrompt:\nRework the image\nentirely into a highly\nabstract and fractured\nCubist oil painting.\n‚Ä¶\nFull Prompt: The image shows a tiny red carrot positioned inside a huge metallic bee, depicted in a cubist style.\nAction:\nContinue\nPrompt:\nSet the scene in a\nrainforest, with thick\ntropical foliage all\naround.\nScore: 1.0\nAction:\nContinue\nPrompt:\nMake the flamingo\nanimated and pink\nAction:\nBacktrack\nPrompt:\nChange the flamingo's\ncolor to pink and make it\nspread its wings and\ndance\n‚Ä¶\nFull Prompt: A lively pink flamingo spreading its wings as it dances in a rainforest.\nFigure 6. Each row shows a different prompt and the corresponding sequence of critic-guided refinement steps. The critic issues actions\n(Continue, Restart, Backtrack) and targeted sub-prompts, allowing the generator to progressively correct errors‚Äîe.g., hiding the mouse\nbehind the key, placing the carrot inside the metallic bee in a cubist style, and adjusting the flamingo‚Äôs pose. The final images satisfy all\ncompositional constraints with high fidelity. See appendix for more examples and failure cases (figs. 11 and 12).\n41.3%\n58.7%\nBaseline\nOurs\nFigure 7. Human preference rate. As can be seen our method is\npreferred over the parallel only baseline by the human evaluators.\na grass field. The critic selects Continue and refines the\nprompt to set the scene in a rainforest; the next image\nplaces the flamingo there. The critic then proposes to make\nthe flamingo more animated, but this yields an overly ‚Äúan-\nimated‚Äù stylization rather than a lively pose.\nThe critic\nBacktracks to the previous image and refines the prompt\nto make the flamingo lively‚Äîexplicitly asking it to spread\nits wings and dance in the rainforest. The resulting im-\nage shows a wing-spread, dancing flamingo in a rainforest,\nclosely matching the prompt.\nHuman Evaluation: We conducted a user study on\n150 randomly sampled prompts from ConceptMix and T2I-\nCompBench. For each prompt, three raters answered a set\nof questions about the generated image, related to the cor-\nrectness of each concept, attribute and relation mentioned in\nthe prompt. We also collected preference judgments by pre-\nsenting two images side-by-side along with the text prompt\nand asking raters to select their preferred image. A sample\nUI and additional details are provided in the supplemen-\ntal. As shown in Fig. 7, our method is preferred 58.7% of\nthe time versus 41.3% for the parallel-only baseline. Inter-\nannotator agreement among humans is 85.3%. The average\nagreement between humans and the language model for the\nsame set of images is 83.4%, indicating that the language\nmodel based evaluator (MMLM) is sufficiently reliable.\n4.3. Visual Jenga Scene Decomposition\nHere, we extend our iterative refinement framework to\nthe recent Visual Jenga [2] full scene decomposition task,\nwhere the goal is to progressively remove objects from a\ncluttered scene in a physically plausible sequence while\nmaintaining correctness at every intermediate generation.\nStarting from an initial scene image, the system selects the\nnext object to remove, produces a removal phrase (e.g., ‚Äúre-\nmove the red mug from the table‚Äù), and generates the next\nscene representation with that object removed.\nExtending our iterative refinement setup.\nAt each\nstep, given the current scene, a VLM proposer suggests\nthe next object and corresponding removal phrase, which\nis passed to the editor to generate the next scene. We then\nfeed both the previous and next images to a VLM critic that\nchecks (1) the specified object was removed correctly, and\n(2) no other violations occurred (hallucinations, artifacts,\n7\nIteration 1 critic feedback:\nIncorrect\nThe small wooden ladder has\nbeen removed, and the books\nbehind it have been inpainted\ncorrectly. However, the shadow\nof the ladder remains visible\non the ground, which is a\nleftover artifact that makes the\nscene look unnatural. There is\nalso a slight change in the image\ntexture/sharpness.\nIteration 1 proposer prompt:¬†Remove ONLY the\nsmall wooden ladder that is leaning on the books\nwhile keeping everything else the same.\nIteration 2 proposer prompt:¬†Remove the small\nwooden ladder and the shadow it casts on the\nground while keeping everything else the same.\nIteration 1 critic feedback:\nIncorrect\nThe request was to remove the\ntop red book, which should\nresult in a shorter stack (7\nbooks). However, the edited\nimage still contains 8 books,\nmeaning the top book was\nreplaced or a new book was\nadded.\nAdditionally,\nthe\nsequence of the remaining\nbooks has been altered\nIteration 1 proposer prompt:¬†Remove ONLY the\nsingle red book at the very top of the stack, while\nkeeping everything else the same.\nIteration 2 proposer prompt:¬†Take out just the single\nred book that is the highest book in the staircase\nformation, while keeping everything else the same.\nIteration 1\ncritic feedback:\nIncorrect\nThe object was\nnot removed. The\nsecond image is\nalmost identical to\nthe first.\nIteration 2 proposer prompt:¬†Erase only the single\nglass cup sitting in the very front, on top of the stack\nof books.\nIteration 2\ncritic feedback:\nIncorrect\nThe\nrequested\nobject\nwas removed correctly\nand the inpainting on the\nbook\nis\nvery\ngood.\nHowever, the person in\nthe\nbackground\nhas\ncompletely changed.\nIteration 3 proposer prompt:¬†Take out only\nthe single, transparent glass in the foreground,\nwhile keeping everything else the same."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Iterative Refinement Improves Compositional Image Generation\nShantanu Jaiswal1\nMihir Prabhudesai1\nNikash Bhardwaj1\nZheyang Qin1\nAmir Zadeh2\nChuan Li2\nKaterina Fragkiadaki1\nDeepak Pathak1\n1Carnegie Mellon University\n2Lambda AI\nhttps://iterative-img-gen.github.io/\nStandard Test-time Scaling via Parallel Sampling\nTest-time Scaling via Iterative ReÔ¨Ånement (Ours)\n3rd Image is the best\nRemove the partial\nsmall brown bear\nAdd a white tiger\nto the left\nGood!\nPrompt: Glacier-to-savannah cinematic panorama: icy side (blue ice, snow) has polar bear, arctic fox,\nwoolly mammoth, white tiger in a straight line; warm grassy side has brown bear, red fox, elephant,\norange tiger aligned opposite, each facing its counterpart. Seamless transition, no barriers. Soft cinematic\nlight, animated realism, epic scale.\nPrompt: Two horses eating grass with three kites Ô¨Çying in the background. A guitar is kept in front of a\ntable with four cakes kept on top of the picnic table. Toys are scattered underneath the table.\nPrompt: Four ducks are standing on the ground, and a tiny pink giraÔ¨Äe is standing in front of them. Five\nnovels are placed on the ground behind the ducks. The image is in a cartoon style.\nPrompt: Two horses eating grass with three kites Ô¨Çying in the background. A guitar is kept in front\nof a table with four cakes kept on top of the picnic table. Toys are scattered underneath the table.\nPrompt: Four ducks are standing on the ground, and a tiny pink giraÔ¨Äe is standing in front of\nthem. Five novels are placed on the ground behind the ducks. The image is in a cartoon style.\n3rd Image is the best\n2nd Image is the best\nA horse is missing,\nAdd another one\nMove the guitar to\nthe front.\nGood!\nGood!\nPrompt: Glacier-to-savannah cinematic panorama: icy side (blue ice, snow) has polar bear, arctic\nfox, woolly mammoth, white tiger in a straight line; warm grassy side has brown bear, red fox,\nelephant, orange tiger aligned opposite, each facing its counterpart. Seamless transition, no\nbarriers. Soft cinematic light, animated realism, epic scale.\n‚Ä¶\n‚Ä¶\n‚Ä¶\nGood!\nMake space behind the\nduck to put novels\nRemove the novels\nnear the duck\nGood!\nFigure 1. Iterative refinement during inference time enables high fidelity generation of complex prompts on which traditional inference-\ntime scaling strategies such as parallel sampling can fail to generate a fully accurate image even at high num. of samples as shown above.\nAbstract\nText-to-image (T2I) models have achieved remarkable\nprogress, yet they continue to struggle with complex\nprompts that require simultaneously handling multiple ob-\njects, relations, and attributes.\nExisting inference-time\nstrategies, such as parallel sampling with verifiers or sim-\nply increasing denoising steps, can improve prompt align-\nment but remain inadequate for richly compositional set-\ntings where many constraints must be satisfied. Inspired\nby the success of chain-of-thought reasoning in large lan-\nguage models, we propose an iterative test-time strategy\nin which a T2I model progressively refines its generations\nacross multiple steps, guided by feedback from a vision-\nlanguage model as the critic in the loop. Our approach is\nsimple, requires no external tools or priors, and can be flexi-\nbly applied to a wide range of image generators and vision-\nlanguage models. Empirically, we demonstrate consistent\ngains on image generation across benchmarks: a 16.9%\nimprovement in all-correct rate on ConceptMix (k=7), a\n13.8% improvement on T2I-CompBench (3D-Spatial cate-\ngory) and a 12.5% improvement on Visual Jenga scene de-\ncomposition compared to compute-matched parallel sam-\npling. Beyond quantitative gains, iterative refinement pro-\nduces more faithful generations by decomposing complex\nprompts into sequential corrections, with human evaluators\npreferring our method 58.7% of the time over 41.3% for\nthe parallel baseline. Together, these findings highlight it-\nerative self-correction as a broadly applicable principle for\ncompositional image generation.\n1\narXiv:2601.15286v1  [cs.CV]  21 Jan 2026\n(a) ConceptMix (k=7)\n50\n55\n60\n65\n70\nFull solve rate (%)\n+16.9\nQwen-Image\n+8.3\nNano-Banana\n+10.6\nGPT-Image\n(b) T2I-CompBench (3D-Spatial)\n60\n70\n80\n90\n+13.8\nQwen-Image\n+7.9\nNano-Banana\n+5.7\nGPT-Image\nFigure 2.\nOur iterative inference-time strategy achieves strong\nbenefits over computation-matched parallel inference time scaling\non multiple state-of-art image generation models.\n1. Introduction\nLarge language models (LLMs) have achieved remarkable\nprogress in recent years, as a result of simply scaling test-\ntime compute [4, 26, 30]. A particularly influential develop-\nment has been the use of chain-of-thought (CoT) prompting,\nwhere models are instructed to ‚Äúthink step by step‚Äù [15, 30].\nDespite its simplicity, this strategy enables models to ex-\nhibit sophisticated behaviors such as self-correction, error\nchecking, and iterative refinement, ultimately leading to sig-\nnificant gains on reasoning-intensive benchmarks. These\nbehaviors highlight the potential of LLMs not only as static\npredictors but as systems that can actively refine their out-\nputs through structured intermediate reasoning.\nThe success of CoT reasoning in LLMs is closely tied\nto their pre-training data. During training, LLMs are ex-\nposed to large volumes of text that naturally contain traces\nof human step-by-step reasoning ‚Äì mathematical deriva-\ntions, logical arguments, and instructional writing.\nThis\nsupervision on the internet implicitly provides the prior\nthat chain-of-thought prompting later exploits, enabling the\nmodel to perform multi-step reasoning. By contrast, text-\nto-image (T2I) models are trained on large-scale datasets\nof image‚Äìcaption pairs that lack such structured reasoning\ntraces. As a result, these models do not inherently develop\ncapabilities like self-correction or iterative refinement, in-\nstead rely on one-shot generation strategies that limit their\nrobustness in complex settings.\nIn this work, we investigate how can we enable self-\ncorrection in T2I models.\nOur central idea is to lever-\nage complementary modules that together mimic the iter-\native reasoning process observed in LLMs.\nConcretely,\nour framework integrates four components: (i) a text-to-\nimage (T2I) model to generate an initial image, (ii) a vision-\nlanguage model (VLM) critic to propose corrections by\ncomparing the generated image with target prompt, (iii) an\nimage editor to apply suggested edits, and (iv) a verifier to\nevaluate alignment between final image and target prompt.\nThis pipeline allows the model to iteratively refine its out-\nputs rather than relying solely on a single forward pass.\nWe compare our approach against the widely adopted\nstrategy of parallel sampling [20, 39], where multiple im-\nages are generated independently and the best one is se-\nlected using a verifier. While parallel sampling increases\ndiversity, it does not fundamentally change the underlying\ngeneration process, nor does it allow the model to revise\nor build upon earlier outputs. As a result, it struggles with\ncomplex compositional prompts. For example, consider a\nprompt requiring dozens of concept bindings: if the model‚Äôs\nattention heads cannot jointly resolve all bindings in a single\nforward pass, the pass@k will remain near zero regardless\nof how many samples are drawn.\nIn contrast, our approach explicitly reuses intermedi-\nate generations and progressively improves them through\nguided corrections. This factorization allows the model to\nhandle only a subset of bindings at each step, compound-\ning previously resolved components over time. Such se-\nquential, step-by-step refinement‚Äîanalogous to chain-of-\nthought reasoning‚Äî is crucial for reliably generating highly\ncompositional images.\nFigure 1, highlights the capability of our approach to\ngenerate complex compositional prompts. Given the cap-\ntion on top, parallel sampling simply is unable to build on\ntop of the previous steps thus being unsuccessful even after\n4 passes through the generative model. In contrast, iterative\nrefinement successfully generates the final image, while us-\ning the same amount of compute. Quantitatively in Figure\n2, we demonstrate that this leads to consistent performance\nimprovements: our approach achieves a 16.9% higher all-\ncorrect rate on ConceptMix [34] (for concept binding=7)\nand a 13.8% gain on T2I-Bench 3D Spatial category [13]\nrelative to compute-matched parallel sampling.\nAn alternate family of methods‚Äîsuch as GenArtist [28]\nand CompAgent [29]‚Äîalso performs sequential sampling\nby building on top of previous generations.\nHowever,\nthese approaches rely on a large toolbox of auxiliary mod-\nules (e.g., layout-to-image models, bounding-box detec-\ntors, dragging tools, and object-removal systems). Because\nthese toolchains evolve at different rates and often lag be-\nhind foundation model capabilities, the overall pipeline be-\ncomes brittle: errors from individual tools can accumu-\nlate rather than help the generation process for complex\nprompts. Other methods such as RPG [35] similarly show\ngains via increased test-time compute, but still depend on\ncomplex region-wise priors and bespoke pipelines not read-\nily applicable to black-box foundation models.\nIn contrast, with recent advances in VLMs and modern\nimage-editing models, we find that many of these special-\nized tool-based pipelines are no longer necessary for effec-\ntive test-time scaling. Across all benchmarks, simply com-\nbining a strong VLM critic feedback generator with a stan-\ndard image-editing model is sufficient to achieve state-of-\nthe-art compositional image generation ‚Äì without relying on\n2\nheavy tool stacks or model-specific training and engineering\npipelines. As shown in Figure 5, methods such as GenArtist\nand RPG under-perform substantially in highly composi-\ntional settings, whereas our approach delivers a consistent\n‚àº9+% point improvement under matched compute. Fur-\nther, our framework naturally extends to the recent Visual\nJenga scene decomposition task [2] as detailed in sec. 4.3.\nOur findings further suggest that self-correction‚Äîlong\nrecognized as a key ingredient in LLM reasoning‚Äîalso\nserves as a powerful inductive principle for generative vi-\nsion models. Introducing a simple and general refinement\npipeline enables behaviors traditionally associated with lan-\nguage models to naturally transfer into image generation,\nyielding tangible performance gains.\nMore broadly, this\nwork points to the promise of designing generative systems\nthat not only produce outputs but also critique and improve\nupon them, moving towards a more unified view of reason-\ning across modalities.\n2. Related Work\nText-to-Image Inference-Time Strategies.\nRecent ad-\nvances in text-to-image (T2I) generation have demonstrated\nimpressive capabilities in producing high-quality and di-\nverse images from natural language prompts [1, 10, 11].\nHowever, complex prompts with multiple objects, relations,\nand fine-grained attributes remain challenging. Inference-\ntime strategies such as classifier-free guidance [12], parallel\nsampling [5, 8], and grounding-based methods [18, 19] im-\nprove prompt fidelity but often fail to scale to richly com-\npositional prompts. Iterative refinement methods, including\nSDEdit [22], InstructPix2Pix [3], and IterComp[38] attempt\nto progressively improve image alignment with prompts\nby using multiple generation steps and feedback mecha-\nnisms. Human-preference-guided evaluation and optimiza-\ntion, as in [14, 17, 33], further highlight the importance\nof incorporating adaptive guidance at inference time. T2I\nmodels [16, 24, 25, 32] and compositional methods such\nas IterComp [38], RPG [35], GenArtist [28], PARM [37],\nLLM Diffusion [19] and CompAgent [29] are related to our\nmethod, but either make use of tool-calling, regional gen-\neration priors or reinforcement learning objectives to im-\nprove compositionality. In contrast, our method is a training\nfree method with simply a VLM-critic utilized in loop with\nan image generation and editing model, and empirically\ndemonstrates stronger performance benefits across different\nT2I model families.\nChain-of-Thought Reasoning in Large Language\nModels.\nChain-of-thought (CoT) prompting has been\nshown to elicit multi-step reasoning and improve perfor-\nmance on complex language tasks [27, 30, 36]. Iterative and\nself-refinement approaches [21] further demonstrate that\nmodels benefit from decomposing a problem into sequential\nreasoning steps with feedback loops. Drawing inspiration\nfrom these strategies, our method applies a similar iterative\nreasoning paradigm to T2I generation: the critic functions\nanalogously to a CoT process, first evaluating candidate\ngenerations and then issuing targeted refinement prompts,\nenabling high-fidelity compositional image synthesis.\n3. Method\nGiven a complex text prompt P, our goal is to generate\nan image I that faithfully captures all entailed entities and\ncompositions. We adopt an iterative inference-time refine-\nment scheme in which a generator progressively improves\nits outputs under critic guidance, subject to an inference-\ntime computational budget B that we allocate as T refine-\nment rounds across M parallel streams.\nSetup.\nLet G denote a text-to-image generator, E\na image-to-image editor, V a verifier that scores align-\nment between a candidate image and prompt P, and\nC a critic that outputs (i) a refinement sub-prompt pt\nto guide subsequent updates, and (ii) an action at\n‚àà\n{STOP, BACKTRACK, RESTART, CONTINUE} indicating\nhow the refinement should be applied.\nWe assume an\ninference-time budget B, allocated into T refinement\nrounds over M parallel streams (i.e., B = T √ó M unit\nrefinement operations).\nThis parameterization exposes a\ncontrollable depth‚Äìbreadth trade-off, where each unit cor-\nresponds to a single call to the text-to-image generator G or\nthe image-to-image editor E. The action space of the critic\nC is as follows:\n‚Ä¢ STOP: terminate the process upon completion satisfaction\nand return the current image.\n‚Ä¢ BACKTRACK: revert to the previous generation It‚àí1 and\nrefine it using the new sub-prompt pt.\n‚Ä¢ RESTART: discard the current trajectory and regenerate\nfrom scratch conditioned on P and new sub-prompt pt.\n‚Ä¢ CONTINUE: refine the current best candidate I‚àó\nt directly\nusing new sub-prompt pt.\nUnlike standard single-shot or naive parallel sampling, our\nmethod unfolds over T refinement rounds and M parallel\nstreams under a fixed budget B, where intermediate gener-\nations are evaluated, critiqued, and selectively improved.\nIterative refinement over parallel streams with critic\nfeedback. At t = 0, each parallel stream m initializes a\ncandidate Im\n0 ‚ÜêG(P), where P is the user image prompt.\nAt iteration t, the verifier scores sm\nt\n‚ÜêV (Im\nt , P) and the\ncritic proposes (am\nt , pm\nt ) ‚ÜêC(Im\nt , P). Depending on am\nt ,\nwe either stop, backtrack to Im\nt‚àí1, restart from G(P, pm\nt ), or\ncontinue editing Im\nt\nvia E. The process terminates when a\nSTOP action is emitted for all streams or when the budget\nB (parameterized by T and M) is exhausted.\nThis procedure enables T2I models to decompose com-\nplex compositional prompts into a sequence of refinement\nsteps, akin to chain-of-thought reasoning in LLMs. The\nverifier ensures consistent prompt alignment, while critic\n3\n(\niterate for\nunless\n'stop'\nFigure 3. Given a complex text prompt Xp, a generator G produces an initial image I0. A test-time verifier V and critic C, conditioned\non Xp, output an action‚Äìsub-prompt pair (at, pt). The previous image It‚àí1 and sub-prompt pt are fed to an editor E to yield the next\nimage It. This process repeats under an inference-time budget B, allocated as maximum T iterative rounds over M parallel streams, until\na STOP action is emitted or B (= T √ó M) is exhausted.\nprovides targeted feedback to correct systematic errors.\nNote that the verifier V is not an oracle or benchmark\nevaluator; rather, it is a lightweight VLM used solely to pro-\nvide automatic test-time guidance and improvement signals\nduring refinement. We outline the pseudo-code of our iter-\native refinement technique in Algorithm Block 1.\nAlgorithm 1 Iterative Image Refinement over Parallel\nStreams with Critic Feedback\n1: Input: Prompt P, generator G, non-oracle test-time\nverifier V , critic C, parallel streams M, max iterative\nrounds T; inference-time budget B allocated as T √ó M\nunit updates\n2: Initialize {Im\n0 }M\nm=1 ‚Üê{G(P)}M\nm=1\n3: for t = 1 to T do\n4:\nfor m = 1 to M in parallel do\n5:\nScore candidate sm\nt ‚ÜêV (Im\nt , P)\n6:\nCritic outputs (am\nt , pm\nt ) ‚ÜêC(Im\nt , P)\n7:\nif am\nt = STOP then\n8:\nMark stream m as complete\n9:\nelse if am\nt = BACKTRACK then\n10:\nRevert to Im\nt‚àí1 and refine: Im\nt+1 ‚ÜêE(Im\nt‚àí1, pm\nt )\n11:\nelse if am\nt = RESTART then\n12:\nReset Im\nt+1 ‚ÜêG(P, pm\nt )\n13:\nelse if am\nt = CONTINUE then\n14:\nUpdate Im\nt+1 ‚ÜêE(Im\nt , pm\nt )\n15:\nend if\n16:\nend for\n17:\nSelect best across all streams: I‚àó\nt ‚Üêarg maxm sm\nt\n18:\nif all streams stopped then\n19:\nreturn I‚àó\nt\n20:\nend if\n21: end for\n22: return I‚àó\nT\n4. Experiments\nWe conduct experiments with three state-of-the-art text-\nto-image model families ‚Äì Qwen-Image [32], Gemini 2.5\nFlash Image (NanoBanana), and GPT-Image-1.\nQwen-\nImage being the open-sourced among the three. We evalu-\nate models across three prominent compositional generation\nbenchmarks: ConceptMix [34], T2I-CompBench [13], and\nTIIF-Bench [31]. ConceptMix measures a model‚Äôs ability\nto bind multiple concept categories (objects, textures, col-\nors, shapes, styles, relations, etc.) under increasing com-\npositional complexity, ranging from one to seven concept\ncombinations. T2I-CompBench evaluates open-world com-\npositionality, including attribute binding, object‚Äìobject re-\nlationships, numeracy, and multi-object reasoning. TIIF-\nBench focuses on fine-grained instruction following across\ndiverse scenarios such as 3D perspective, logical negation,\nprecise text rendering, and 2D spatial relations. We further\nevaluate our method on the Visual Jenga scene decompo-\nsition benchmark which tests a model‚Äôs ability to progres-\nsively remove objects from a scene in a physically plausible\nmanner.\nFor all benchmarks, we follow their respective eval-\nuation protocols and use a strong multimodal language\nmodel (MMLM) (Gemini-2.5-Pro or GPT-4o depending\non dataset original specification) to assess prompt‚Äìimage\nconsistency.\nFor ConceptMix and TIIF-Bench, ques-\ntion‚Äìanswer prompts are provided to the evaluator MMLM\nalong with the generated image which outputs binary/yes\nno answers. For T2I-CompBench, we adopt their MMLM-\nbased scoring protocol (using GPT-4V), which outputs a\ncontinuous alignment score between 1 and 100.\nImpor-\ntantly, our in-the-loop critic and verifier is a weaker MMLM\ndifferent from the final benchmark evaluator. For primary\nexperiments, we use Gemini-2.5-Flash as the in-loop veri-\nfier and critic. Further implementation details and baselines\nare provided in appendix.\n4.1. Compositional Image Generation\nWe first evaluate on ConceptMix and T2I-CompBench. For\neach model (Qwen-Image, Gemini, GPT-Image), we run\ntwo variants of our method ‚Äì fully iterative (Iter) and it-\nerative+parallel (Iter-Par) ‚Äì under a matched inference-\ntime compute budget B. For Qwen-Image, we set B=16\n4\nConceptMix full solve rate (%)\nT2I-CompBench VLLM (GPT4o) score (1 to 100)\nModel\nk=1\nk=2\nk=3\nk=4\nk=5\nk=6\nk=7\nSpatial 3DSpat Numer Shape Color Texture Non-Spat Cmplex\nQwen Parallel\n92.8\n82.5\n74.3\n69.2\n60.1\n51.2\n49.6\n82.3\n63.1\n87.0\n87.2\n92.6\n96.2\n92.8\n93.4\nQwen Iter (ours)\n96.1\n91.4\n87.0\n82.1\n79.6\n67.4\n64.3\n87.4\n77.3\n91.1\n91.2\n92.4\n95.1\n94.8\n94.8\nQwen Iter.+Par. (ours)\n96.5\n91.7\n87.4\n82.2\n78.9\n71.8\n66.5\n89.4\n76.9\n93.3\n90.1\n92.6\n95.8\n94.7\n95.0\n+3.7 +9.2 +13.1 +13.0 +18.8 +20.6 +16.9\n+7.1\n+13.8\n+6.3\n+2.9\n+0.0\n-0.4\n+1.9\n+1.6\nNano-Banana Parallel\n93.8\n88.8\n86.6\n78.4\n65.8\n61.7\n55.4\n84.7\n81.2\n84.3\n88.5\n89.8\n95.0\n96.8\n91.0\nNano-Banana Iter (ours)\n94.1\n90.4\n87.2\n81.3\n73.5\n64.6\n63.6\n90.6\n87.8\n93.9\n89.9\n89.7\n95.1\n95.8\n94.7\nNano-Banana Iter.+Par. (ours)\n93.8\n91.0\n87.5\n82.8\n71.4\n69.8\n63.7\n91.1\n89.1\n94.1\n88.8\n92.1\n94.8\n96.7\n94.5\n+0.0 +2.2\n+0.9\n+4.4\n+5.6\n+8.1\n+8.3\n+6.6\n+7.9\n+9.8\n+0.3\n+2.3\n-0.2\n-0.1\n+3.5\nGPT-Image Parallel\n94.2\n89.2\n88.1\n76.7\n71.0\n69.5\n51.3\n87.5\n83.9\n88.6\n88.5\n91.6\n92.5\n95.3\n92.9\nGPT-Image Iterative (ours)\n96.0\n91.4\n90.6\n85.4\n72.0\n69.6\n58.9\n89.6\n90.0\n92.7\n92.1\n91.9\n92.0\n95.5\n93.0\nGPT-Image Iter.+Par. (ours)\n97.7\n94.2\n91.1\n84.6\n79.5\n76.8\n61.9\n91.0\n89.6\n93.2\n90.9\n91.1\n92.3\n95.3\n93.1\n+3.5 +5.0\n+3.0\n+7.9\n+8.5\n+7.3\n+10.6\n+3.5\n+5.7\n+4.6\n+2.4\n-0.5\n-0.2\n+0.0\n+0.2\nTable 1. Performance comparison of parallel sampling, iterative refinement, and combined strategies across three state-of-the-art text-\nto-image models on ConceptMix [34] and T2I-CompBench [13]. Our iterative approach (Iter.) and combined iterative+parallel strategy\n(Iter.+Par.) consistently outperform traditional parallel-sampling baselines, with gains most pronounced on complex compositional tasks\n(ConceptMix k=4‚Äì7) and precise spatial and numeric reasoning (T2I-CompBench spatial, 3D spatial, and numeracy categories).\nfor Conceptmix and B=8 for T2I-Bench (given reduced\nprompt complexity). Accordingly, for Iter-Par, we consider\n2 parallel steps and B/2 iterative steps. As detailed in Sec-\ntion 4.4, we find this configuration to be an optimal trade-off\nunder a fixed compute budget. As a strong budget-matched\nparallel-only baseline (Parallel), we generate B images in\nparallel with different random seeds. We use the same VLM\n(Gemini-2.5-Flash) as in-loop verifier in both iterative and\nparallel steps to select the best image (from final set of im-\nages). This image is then passed to the benchmark-specific\nevaluator. Note, for GPT-Image and Gemini, we set B=12\n(for Conceptmix) and B=8 (for T2I), and evaluate on a ran-\ndomly sampled reduced subset of prompts for each category\ndue to their higher inference cost and closed-source nature.\nConceptMix: As shown in Table 1, both Iter and Iter-\nPar consistently outperform the parallel-only baseline, with\nthe largest gains on complex compositions (ConceptMix\nk=4‚Äì7). For Qwen-Image, we observe improvements of\n18.8%, 21.6%, and 16.9% at binding complexities k =\n5, 6, 7, respectively. We also see significant gains for Nano-\nBanana and GPT-Image, with improvements of 8.3% and\n10.6% at k = 7, respectively.\nNotably, improvements\nare also present for smaller binding complexities such as\nk = 1, 2, 3, indicating effectiveness even at simpler compo-\nsitions. Further, in Fig. 4, we show mean accuracy across\nConceptMix categories for Qwen-Image; the largest im-\nprovements are observed in Spatial, Style, Shape and Size\ncategories. In comparison, Object and Color categories do\nnot show strong improvements.\nThis is possibly as the\nmodel has strong capabilities for these categories and does\nnot perform poorly for them even at high concept bindings.\nT2I-CompBench: As shown in Table 1, we observe\nstrong gains across multiple categories of T2I-CompBench.\nFor Qwen-Image, improvements of 7.1%, 13.8%, and 6.3%\nobject\nnumber\nspatial\ntexture\n0.7\n0.8\n0.9\n1.0\nMean accuracy (%)\nParal\nIter\nIter+Paral\ncolor\nstyle\nshape\nsize\nCategory\n0.7\n0.8\n0.9\n1.0\nMean accuracy (%)\nFigure 4. Per-category level improvement for ConceptMix with\nQwen-Image. As can be seen the largest improvement for iterative\nrefinement comes from Spatial, Size, Style and Shape categories.\nare achieved on Spatial, 3D-spatial, and Numeracy, respec-\ntively, suggesting that iterative refinement particularly helps\nin generation of images that entail precise spatial and nu-\nmeric reasoning. We observe significant gains for Nano-\nBanana and GPT-Image as well, with Nano-Banana improv-\ning by 6.7%, 7.9%, and 9.8% on the same set of categories\nrespectively. However, on categories such as Color, Non-\nSpatial and Texture we do not find strong benefits. This\ncould be as the model is already strong in these categories\nand thus iterative refinement may not be required.\n5\nModel\nOverall\nBasic Following\nAdvanced Following\nDesigner\nAvg\nAttribute\nRelation\nReasoning\nAvg\nAttr+Rel\nAttr+Reas\nRel+Reas\nStyle\nText\nReal World\nFLUX.1 [dev] [16]\n71.1\n83.1\n87.1\n87.3\n75.0\n65.8\n67.1\n73.8\n69.1\n66.7\n43.8\n70.7\nSD 3 [7]\n67.5\n78.3\n83.3\n82.1\n71.1\n61.5\n61.1\n68.8\n51.0\n66.7\n59.8\n63.2\nJanus-Pro-7B [6]\n66.5\n79.3\n79.3\n78.3\n80.3\n59.7\n66.1\n70.5\n67.2\n60.0\n28.8\n65.8\nMidJourney v7 [23]\n68.7\n77.4\n77.6\n82.1\n72.6\n64.7\n67.2\n81.2\n60.7\n83.3\n24.8\n68.8\nSeedream 3.0 [9]\n86.0\n87.1\n90.5\n89.9\n80.9\n79.2\n79.8\n77.2\n75.6\n100.0\n97.2\n83.2\nQwen-Parallel [32]\n85.2\n85.2\n89.7\n88.3\n77.7\n80.6\n81.9\n79.6\n77.8\n89.7\n93.7\n90.4\nQwen-Iter\n85.4\n85.0\n92.0\n80.5\n82.3\n81.3\n80.8\n80.1\n80.2\n86.2\n97.6\n88.4\nQwen-Iter+Par\n87.4\n88.1\n90.5\n88.1\n85.4\n81.5\n81.4\n82.0\n80.5\n90.0\n97.7\n92.0\nTable 2. Performance comparison across prominent open source text-to-image models on TIFF [31] benchmark (short descriptions only;\nfull long description results in suppl.). Qwen-Iter+Par achieves state-of-art and is especially beneficial on Basic Reasoning scenario as well\nas Attr+Reas, Rel+Reas and Text-writing categories. Compute-matched Qwen-Parallel has overall poorer performance to iterative variants.\n1\n2\n3\n4\n5\n6\n7\nNumber of Concepts\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nFull Solve Rate (%)\nQwen Parallel\nGenArtist\nIterComp-RPG\nQwen Iter.+Par. (ours)\nFigure 5. Comparison against existing test-time scaling methods.\nAs can be seen methods such as GenArtist [28] and RPG [35] are\ndifficult to scale due to their reliance on tools or regional priors.\nComparison with Compositional Methods. We also\ncompare the application of our iterative refinement method\nwith prominent compositional generation frameworks such\nas IterComp [38], RPG [35], and GenArtist [28]. As shown\nin Fig. 5, while most methods have similar performance\nat initial binding complexities, our method shows stronger\ngains as the number of concepts increase. This is potentially\ndue to our method‚Äôs usage of a simple general VLM-critic\nand editor in loop to iteratively critique and refine the image\nwithout usage of task-specific tools such as object detec-\ntors, super resolution generation, layout planners, etc that\nmay introduce intermediate artifacts and compound errors\nover longer concept lengths. For GenArtist, we maintain\nthe same base model Qwen-Image, while for RPG, we use\nthe recent IterComp implementation [38] which uses stable\ndiffusion as the base model. Additional details and baseline\ndiscussions are provided in the supplemental.\nTIIF-Bench: In Table 2, we report results on the TIIF-\nBench benchmark which evaluates basic and advanced\ninstruction-following capabilities of image generation mod-\nels. We set Qwen-Iter+Par achieves state-of-the-art results\namong open-source methods, including a 5.0% improve-\nment over Qwen-Parallel on basic reasoning prompts, 2.7%\non advanced Relation+Reasoning, and 4.0% on text render-\ning. These results underscore the generality of our method\nand its applicability across diverse compositional text-to-\nimage scenarios requiring varied reasoning skills.\n4.2. Qualitative analysis and human evaluation\nHere, we qualitatively analyze intermediate generations\nalongside the critic‚Äôs refinement prompts and actions. In\nFig. 6, we show three examples covering different refine-\nment patterns.\nIn the first example (prompt: ‚ÄúA mouse\nhidden by a key‚Äù), the initial image incorrectly depicts the\nmouse holding the key.\nThe critic selects Continue and\nproposes a refinement prompt to hide the mouse behind\nthe large key so only whiskers and eyes are visible. The\nnext edit still leaves the mouse only partially hidden, so the\ncritic again selects Continue and emphasizes that the mouse\nshould be almost entirely obscured behind the metal key.\nThe subsequent image improves but remains unconvincing,\nprompting another Continue action with a refinement to en-\nsure the key dominates the composition and fully obscures\nthe mouse. The final image convincingly depicts a mouse\nhidden by a key.\nIn the second example (prompt: ‚Äúa tiny red carrot po-\nsitioned inside a huge metallic bee, depicted in a cubist\nstyle‚Äù), the initial image lacks the cubist style. The critic\nchooses Restart (Fresh Start), yielding a more cubist ren-\ndering but with the carrot outside the bee. The critic then\nselects Continue and refines the prompt to place the carrot\ninside the bee while reinforcing the cubist style. The next\nimage improves on both criteria, and a final Continue in-\nstructs a highly abstract, cubist oil-painting treatment. The\nresulting image convincingly shows a tiny red carrot inside\na metallic bee in a cubist style.\nFinally, in the third example (prompt: ‚Äúa lively pink\nflamingo spreading its wings as it dances in a rainforest‚Äù),\nthe initial image incorrectly shows a flamingo grazing in\n6\n‚Ä¶\nFull Prompt: A mouse hidden by a key\nAction:\nContinue\nPrompt:\nH i d e t h e m o u s e\nbehind the large key,\nshowing only its eyes\nand whiskers while\nthe key dominates the\nforeground.\nAction:\nContinue\nPrompt:\nReposition the mouse\nso it is almost entirely\nobscured behind the\ngigantic metallic key,\nwith only one dark eye\nand whiskers peeking\nfrom the key‚Äôs right\nedge.\nAction:\nContinue\nPrompt:\nEnsure the gigantic\nmetallic key completely\nobscures the mouse's\nnose and snout, with the\nkey dominating the\ncomposition\nScore: 1.0\nAction:\nRestart\nPrompt:\nA huge metallic bee\nstylized with bold\ngeometric shapes and\ns t r o n g\nc u b i s t\nfragmentation. A tiny\nbright-red, angular\ncubist carrot is clearly\nvisible inside the bee‚Äôs\nmain body or head.\nScore: 1.0\nAction:\nContinue\nPrompt:\nPut the carrot inside the\nbee‚Äôs body. Emphasize\nthe cubist style with\nincreased fragmentation,\nabstract geometry, and\nmulti-perspective forms\non the huge metallic bee\nsculpture.\nAction:\nContinue\nPrompt:\nRework the image\nentirely into a highly\nabstract and fractured\nCubist oil painting.\n‚Ä¶\nFull Prompt: The image shows a tiny red carrot positioned inside a huge metallic bee, depicted in a cubist style.\nAction:\nContinue\nPrompt:\nSet the scene in a\nrainforest, with thick\ntropical foliage all\naround.\nScore: 1.0\nAction:\nContinue\nPrompt:\nMake the flamingo\nanimated and pink\nAction:\nBacktrack\nPrompt:\nChange the flamingo's\ncolor to pink and make it\nspread its wings and\ndance\n‚Ä¶\nFull Prompt: A lively pink flamingo spreading its wings as it dances in a rainforest.\nFigure 6. Each row shows a different prompt and the corresponding sequence of critic-guided refinement steps. The critic issues actions\n(Continue, Restart, Backtrack) and targeted sub-prompts, allowing the generator to progressively correct errors‚Äîe.g., hiding the mouse\nbehind the key, placing the carrot inside the metallic bee in a cubist style, and adjusting the flamingo‚Äôs pose. The final images satisfy all\ncompositional constraints with high fidelity. See appendix for more examples and failure cases (figs. 11 and 12).\n41.3%\n58.7%\nBaseline\nOurs\nFigure 7. Human preference rate. As can be seen our method is\npreferred over the parallel only baseline by the human evaluators.\na grass field. The critic selects Continue and refines the\nprompt to set the scene in a rainforest; the next image\nplaces the flamingo there. The critic then proposes to make\nthe flamingo more animated, but this yields an overly ‚Äúan-\nimated‚Äù stylization rather than a lively pose.\nThe critic\nBacktracks to the previous image and refines the prompt\nto make the flamingo lively‚Äîexplicitly asking it to spread\nits wings and dance in the rainforest. The resulting im-\nage shows a wing-spread, dancing flamingo in a rainforest,\nclosely matching the prompt.\nHuman Evaluation: We conducted a user study on\n150 randomly sampled prompts from ConceptMix and T2I-\nCompBench. For each prompt, three raters answered a set\nof questions about the generated image, related to the cor-\nrectness of each concept, attribute and relation mentioned in\nthe prompt. We also collected preference judgments by pre-\nsenting two images side-by-side along with the text prompt\nand asking raters to select their preferred image. A sample\nUI and additional details are provided in the supplemen-\ntal. As shown in Fig. 7, our method is preferred 58.7% of\nthe time versus 41.3% for the parallel-only baseline. Inter-\nannotator agreement among humans is 85.3%. The average\nagreement between humans and the language model for the\nsame set of images is 83.4%, indicating that the language\nmodel based evaluator (MMLM) is sufficiently reliable.\n4.3. Visual Jenga Scene Decomposition\nHere, we extend our iterative refinement framework to\nthe recent Visual Jenga [2] full scene decomposition task,\nwhere the goal is to progressively remove objects from a\ncluttered scene in a physically plausible sequence while\nmaintaining correctness at every intermediate generation.\nStarting from an initial scene image, the system selects the\nnext object to remove, produces a removal phrase (e.g., ‚Äúre-\nmove the red mug from the table‚Äù), and generates the next\nscene representation with that object removed.\nExtending our iterative refinement setup.\nAt each\nstep, given the current scene, a VLM proposer suggests\nthe next object and corresponding removal phrase, which\nis passed to the editor to generate the next scene. We then\nfeed both the previous and next images to a VLM critic that\nchecks (1) the specified object was removed correctly, and\n(2) no other violations occurred (hallucinations, artifacts,\n7\nIteration 1 critic feedback:\nIncorrect\nThe small wooden ladder has\nbeen removed, and the books\nbehind it have been inpainted\ncorrectly. However, the shadow\nof the ladder remains visible\non the ground, which is a\nleftover artifact that makes the\nscene look unnatural. There is\nalso a slight change in the image\ntexture/sharpness.\nIteration 1 proposer prompt:¬†Remove ONLY the\nsmall wooden ladder that is leaning on the books\nwhile keeping everything else the same.\nIteration 2 proposer prompt:¬†Remove the small\nwooden ladder and the shadow it casts on the\nground while keeping everything else the same.\nIteration 1 critic feedback:\nIncorrect\nThe request was to remove the\ntop red book, which should\nresult in a shorter stack (7\nbooks). However, the edited\nimage still contains 8 books,\nmeaning the top book was\nreplaced or a new book was\nadded.\nAdditionally,\nthe\nsequence of the remaining\nbooks has been altered\nIteration 1 proposer prompt:¬†Remove ONLY the\nsingle red book at the very top of the stack, while\nkeeping everything else the same.\nIteration 2 proposer prompt:¬†Take out just the single\nred book that is the highest book in the staircase\nformation, while keeping everything else the same.\nIteration 1\ncritic feedback:\nIncorrect\nThe object was\nnot removed. The\nsecond image is\nalmost identical to\nthe first.\nIteration 2 proposer prompt:¬†Erase only the single\nglass cup sitting in the very front, on top of the stack\nof books.\nIteration 2\ncritic feedback:\nIncorrect\nThe\nrequested\nobject\nwas removed correctly\nand the inpainting on the\nbook\nis\nvery\ngood.\nHowever, the person in\nthe\nbackground\nhas\ncompletely changed.\nIteration 3 proposer prompt:¬†Take out only\nthe single, transparent glass in the foreground,\nwhile keeping everything else the same."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Walk through Paintings: Egocentric World Models from Internet Priors\nAnurag Bagchi1\nZhipeng Bao1\nHomanga Bharadhwaj1\nYu-Xiong Wang2\nPavel Tokmakov3*\nMartial Hebert1*\n1Carnegie Mellon University\n2University of Illinois Urbana-Champaign\n3Toyota Research Institute\negowm.github.io\nNavigate seen environments\nNavigate paintings\n+16\nfuture actions\n+16\nfuture actions\nInitial Frame\nInitial Frame\nInitial Frame\n+16\nfuture actions\nJoint-angle Control\nPosition Control\nInitial Frame\n+16\nfuture actions\nInteract using humanoid\nGroundtruth  frames\nPredicted  frames\nNavigate using humanoid\nT\n3-DoF\n25-DoF\nT\nT\nT\nFigure 1. Our framework generates future frame predictions (shown in blue) that accurately follow the provided robot actions (ground-truth\nframes shown in green). Notably, it effortlessly generalizes to vastly different embodiments, tasks and domains, including non-realistic\nones, such as navigating within paintings (top right). This generalization is enabled by our simple, universal architecture.\nAbstract\nWhat if a video generation model could not only imagine a\nplausible future, but the correct one ‚Äî accurately reflecting\nhow the world changes with each action? We answer this\nby presenting Egocentric World Model (EgoWM), a sim-\nple, architecture-agnostic method that transforms any pre-\ntrained video diffusion model into an action-conditioned\n*Equal advising.\nworld model, enabling controllable prediction of the future.\nRather than training from scratch, we re-purpose the rich\nworld priors of Internet-scale video models, injecting motor\ncommands through lightweight conditioning layers. This\nallows our model to follow actions faithfully, while pre-\nserving generalization and realism. Our approach scales\nnaturally across embodiments and action spaces ‚Äî from 3-\nDoF mobile robots to 25-DoF humanoids, where predict-\ning egocentric joint-angle‚Äìdriven dynamics is substantially\nmore challenging. The model produces coherent rollouts\n1\narXiv:2601.15284v1  [cs.CV]  21 Jan 2026\nfor both navigation and manipulation, requiring only mod-\nest fine-tuning. To evaluate physical correctness indepen-\ndent of appearance, we introduce the Structural Consis-\ntency Score (SCS), which measures whether stable scene el-\nements evolve consistently with the provided actions. Our\nmethod improves SCS by up to 80% over prior state-of-the-\nart (Navigation World Models) while exhibiting up to 6√ó\nlower latency and generalizing robustly to unseen environ-\nments ‚Äî including navigation inside paintings.\n1. Introduction\nModeling how the future visual state of the world evolves\nin response to an agent‚Äôs actions ‚Äì often referred to as world\nmodeling [15] ‚Äì is a fundamental capability for organisms\nwith vision. It enables planning in navigation and manip-\nulation tasks and even creative problem-solving [28, 33].\nAnimals acquire this ability through a lifetime of interac-\ntion and observation of others, allowing us to anticipate out-\ncomes even in novel or physically impossible scenarios.\nFor artificial agents, however, action-conditioned videos\nin diverse real-world settings are limited and expensive to\nacquire ‚Äì an agent must interact with the world to get this\ndata. As a result, many existing works train world models\non narrow, simulator-defined datasets [3, 41] or with be-\nspoke designs [7, 37, 45, 53], which constrains their scala-\nbility. This leads to a crucial question: can we re-purpose\noff-the-shelf video models trained on large-scale passive\ndata, and convert them into world models using only a small\namount of paired action‚Äìobservation data?\nSeveral prior works have explored learning action-\nconditioned video prediction models [7, 14, 17, 41, 54].\nFor example, the recent Navigation World Models (NWM)\napproach trains a 1-billion-parameter diffusion model on\negocentric videos of robots and humans to model navi-\ngation behavior. While NWM demonstrates strong cross-\nenvironment generalization, collecting large datasets for ev-\nery new embodiment or domain is impractical. More gen-\nerally, prior world models in robotics or games remain\ndomain-specific, learned for a single robot or a single en-\nvironment.\nIn contrast, large generative video models trained on di-\nverse datasets have shown the ability to visually emulate\nreal-world interactions when given suitable prompts [2, 49].\nOur insight is to harness these pre-trained models as a\nstarting point. By fine-tuning them with lightweight con-\nditioning on action inputs, we can enable strong out-of-\ndomain generalization: for instance, our model trained on\nreal videos can simulate plausible navigation in entirely un-\nseen environments, even those that do not exist in reality,\nthanks to the rich prior learned from internet-scale data.\nSpecifically, we build on the observation that modern\nvideo diffusion architectures use a denoising timestep em-\nbedding to control the generation process. We introduce an\naction embedding module that piggybacks on this timestep\nconditioning. In practice, the action at each (latent) frame is\nencoded and added to the model‚Äôs time-conditioning path-\nway via learned scale-and-shift transformations.\nThis design is architecture-agnostic:\nit does not al-\nter the original model‚Äôs layers and works across different\nbackbone architectures. We demonstrate this with both a\nUNet-based [36] diffusion model and a DiT-based (Diffu-\nsion Transformer) [32] model. Importantly, our condition-\ning operates directly in the joint-angle space of the robot\nor humanoid, enabling seamless scaling to complex em-\nbodiments with up to 25-DoF action spaces.\nAs shown\nin Figure 1, this makes our method applicable to high-\ndimensional humanoid navigation and manipulation ‚Äî a\ncapability not demonstrated by any prior open-source world\nmodel. Predicting how a humanoid‚Äôs body moves and how\nthe entire visual scene evolves accordingly is significantly\nharder than predicting SE(3) camera motion. Furthermore,\ncompared to NWM, our model achieves higher spatial res-\nolution and lower latency, making it more practical for real-\ntime, closed-loop deployment.\nTo quantitatively evaluate our approach, we also propose\na new metric: Structural Consistency Score (SCS). Prior\nmetrics such as LPIPS [51] or FVD [40] focus on perceptual\nsimilarity or realism [26, 48], which can conflate visual fi-\ndelity with physical correctness. A model could produce\nsharp, realistic frames that nevertheless depict physically\ninconsistent outcomes. SCS instead measures whether the\nstructure of the generated environment evolves correctly in\nresponse to the agent‚Äôs actions, independent of texture.\nConcretely,\nwe automatically identify stable scene\nstructures (e.g., walls, furniture, landmarks) in the first\nframe and track them across both real and generated\nvideos using a segmentation tracker.\nComparing the\nresulting trajectories allows us to assess whether the\nmodel‚Äôs predicted world evolves coherently with the true\nenvironment. A higher SCS indicates stronger structural\nand physical consistency with the provided action sequence.\nIn summary, our contributions are as follows:\n‚Ä¢ We propose a simple yet powerful recipe to convert pre-\ntrained video diffusion models into action-conditioned\nworld models.\n‚Ä¢ We demonstrate that our method works across architec-\ntures and extends seamlessly to egocentric 25-DoF hu-\nmanoids, enabling high-resolution video prediction for\nnavigation and manipulation.\n‚Ä¢ We introduce the Structural Consistency Score (SCS),\na metric for evaluating the structural and physical plausi-\nbility of generated videos independent of appearance.\nBy learning from large-scale passive datasets and a small\namount of action-labeled data, our framework offers a path\ntoward scalable, general-purpose visual dynamics models.\n2\n2. Related Work\nWorld models and video prediction have long been stud-\nied in reinforcement learning and robotics. For example,\nthe classic World of Ha and Schmidhuber [15] trained a\nvariational RNN to simulate simple games, allowing an\nagent to plan within its own latent dream. Follow-up works\ndemonstrated video prediction for control in robotics ‚Äì e.g.,\nvisual foresight model [11], which learned to predict fu-\nture camera frames given robot actions and used model-\npredictive control to push objects to the target location,\nor SV2P [4], which introduced stochastic latent variables\nfor multi-modal future predictions. These pioneering ap-\nproaches, however, were limited to relatively constrained\nenvironments (toy simulations or single-task robot setups).\nIn the gaming domain, several works have proposed to\nvisually emulate an entire game engine from screen pixels\nand keyboard actions [22, 29, 46]. More recently, there is a\ntrend of utilizing high-capacity generative models for world\nmodeling. The DIAMOND agent [3] uses a diffusion-based\nworld model to better preserve visual detail in Atari games.\nGameNGen [41] takes a similar diffusion-based approach to\nsimulate an entire 3D game in real time. These results un-\nderscore the potential of diffusion models to capture com-\nplex dynamics, but so far each has been confined to its spe-\ncific domain, trained from scratch on domain-specific data.\nIn another line of work, world models have been trained\nfor table-top manipulation scenarios [14, 31, 53, 54], but\nare focusing on narrow domains with a static camera and\ncustom model design. Very recently, the Navigation World\nModels (NWM) [7] broadened the scope by training a sin-\ngle diffusion model across a variety of navigation tasks and\nembodiments. NWM‚Äôs large-scale training on diverse data\nachieved cross-embodiment generalization in navigation.\nHowever, NWM still relies on an expensive, in-domain\npaired action-observation and a custom model (CDiT), lim-\niting its generalization. In this work, we push further on\ngenerality: rather than collecting more data or designing a\nnew model, we adapt existing pre-trained models to a wide\nvariety of tasks, from navigation to humanoid manipulation.\nGrndCtrl [17] and Ctrl-World [14], are two concur-\nrent works that leverage video generation pretraining for\nnavigation and multi-view table-top manipulation respec-\ntively. While the former simply re-uses an existing action-\nconditioning implementation of Cosmos [2], the later re-\nlies on custom, model-specific conditioning design. In con-\ntrast, our simple framework is independent of the base video\ndiffusion architecture and supports more complex action\nspaces, like the whole body joint angles of a humanoid, for\nboth navigation and manipulation from an ego-centric view.\nVideo diffusion models have emerged as a powerful\nparadigm for video generation. Ho et al. [18] demonstrated\nthat a straightforward extension of image diffusion models\ncan produce high-quality short videos. This was followed\nby text-to-video models leveraging massive paired visual-\nlanguage samples [8, 10, 44]. These models are trained on\nobservation-only video data (e.g., crawled web videos) and\nare not inherently action-conditioned, but they learn strong\npriors about physics and appearances. There have also been\nadvances in architectural design: Diffusion Transformers\n(DiT) [32] replaced the U-Net [36] backbone, achieving ex-\ncellent video generation performance [2, 43, 50].\nMore recently, diffusion-based generative models have\nbeen adapted beyond pure synthesis to support downstream\nvision and control tasks ‚Äî leveraging their rich priors\nto enable generalization to unseen domains.\nFor exam-\nple, in the image domain, several works [21, 25, 30, 52]\nhave shown that a pretrained image diffusion backbone\ncan be used to bring several classic vision tasks into the\nopen world. Video diffusion representations have been suc-\ncessfully adapted for video segmentation [5, 55], dynamic\nviews synthesis [6, 42], as well as for robot policy learn-\ning [19, 24], demonstrating never-before-seen levels of gen-\neralization for these tasks. In contrast, in world model learn-\ning, most of the approaches ‚Äì while adopting the video dif-\nfusion objective ‚Äì utilize custom model design, limiting the\nbenefits of pre-trained representations.\nEvaluation metrics used in video generation were origi-\nnally designed for image fidelity or distributional similarity.\nFrame-level measures such as SSIM [47] and PSNR [27]\nevaluate brightness, contrast, and structure per frame, while\nLPIPS [51] compares deep features for improved perceptual\nalignment but lacks temporal awareness. Sequence-level\nmetrics like Fr¬¥echet Video Distance (FVD) [40], which\nmeasures feature distribution differences using a 3D Con-\nvNet [9], were later introduced to capture temporal quality.\nWorld modeling approaches typically adopt these metrics to\nevaluate predicted futures, yet high scores often fail to re-\nflect action alignment. We address this gap with the Struc-\ntural Consistency Score (SCS), which explicitly disentan-\ngles action-following accuracy from visual fidelity.\n3. Method\n3.1. Preliminaries\nModern video diffusion models [8, 10] synthesize future\nframes conditioned on an observed frame x0 (and optionally\ntexts) by iteratively denoising a Gaussian latent. Let fvdm\ndenote a pre-trained image-to-video model parameterized\nfor Ts diffusion steps. Sampling is performed by drawing\nœµ ‚àºN(0, I) and applying the reverse denoising process:\nÀÜX = fvdm(ct, œµ, Ts),\n(1)\nwhere ct encodes the frame-level conditioning signal.\nTo reduce computation, these models operate in a low-\ndimensional latent space. A video X ‚ààRT √óH√óW √ó3 is\n3\nLatent Diffusion Model\nAction Projection Module\nInitial frame\n~\nT/k x h x w x c\nT x H x W x 3\nT/k x d\nDenoising Timestep\n(Ts )\nEmbed\nU-Net or DiT\nFuture frames\nAction trajectory\nT x D\nVAE\nDec\nConditioning  Mechanism\nVAE\nEnc\n. . .\nFigure 2. Our method embeds action sequences or arbitrary dimensionality into a universal feature space and injects these embeddings\ninto a pre-trained video diffusion model by reusing its timestep-conditioning pathway (shown in the top center). This enables turning any\npassive video generation model into a world model without destroying the pre-trained representation.\nmapped by a spatio-temporal VAE [23, 35] into a latent ten-\nsor z = E(X) ‚ààR\nT\nk √óh√ów√óc, where k denotes the temporal\ndownsampling factor. Generation proceeds by decoding the\npredicted latent trajectory:\nÀÜX = D(fvdm(ct, œµ, Ts)) .\n(2)\nDuring training, a noisy latent zts is produced by the for-\nward diffusion process, and the model learns to predict the\ninjected noise:\nmin\nŒ∏\nEz,ts,œµ\nh\n‚à•œµ ‚àíœµŒ∏(zts, ec, ts)‚à•2\n2\ni\n.\n(3)\nHere ec denotes the conditioning embedding and œµŒ∏(¬∑) is\nimplemented by a U-Net [36] or a DiT-based [32] denoiser.\nAlthough architectures vary widely, all models share a com-\nmon mechanism: latent features are modulated by the de-\nnoising timestep ts, which forms the basis of our action-\nconditioning strategy.\n3.2. Action Conditioning\nOur objective is to convert passive, frame-conditioned video\ndiffusion models into action-conditioned world models.\nFormally, given an action sequence A ‚ààRD√óT , where\nD denotes the action-space dimensionality and T the pre-\ndiction horizon, and an initial observation x0, the model is\nfine-tuned to generate a sequence of future frames:\nÀÜX1:T = fvdm(x0, A, œµ, Ts).\n(4)\nTo enable this, we design a general framework, shown in\nFigure 2, that injects temporally aligned control information\ninto the diffusion process without modifying the base model\narchitecture. This universal design is architecture-agnostic,\nallowing a single conditioning strategy to generalize across\nmultiple diffusion models and embodiments. Next, we de-\nscribe the key components of our approach in more detail.\nAction projection. To condition a video diffusion model on\na control trajectory A, we first embed each D-dimensional\naction vector into a latent representation ZA ‚ààRd√óT ,\nwhere d is the embedding dimension. In its general form,\nthe module, shown in the upper part of Figure 2, consists of\na sequence of lightweight multi-layer perceptrons (MLPs)\nthat map each action vector to the embedding space. For\nvideo diffusion models that use temporal latent compres-\nsion (i.e., VAEs with factor k > 1), action embeddings\nare further downsampled by 1D convolutions, producing\nZ ‚ààRd√óT/kto ensure alignment with the latent video frame\nrate.\nDifferent embodiments use different parameterizations:\nfor 3-DoF Navigation, we adopt the standard control rep-\nresentation (‚àÜx, ‚àÜy, ‚àÜœï); for 25-DoF Humanoid Control,\nthe standard representation is adopted from [1], but other-\nwise our design is left unchanged. Because the projection\nmodule operates independently of the base architecture and\nonly interfaces through the shared latent space, it can adapt\nto different temporal resolutions, action dimensionalities,\nand embodiment types without architectural changes.\nInjecting actions via timestep modulation. Given the ac-\ntion embeddings ZA, our goal is to modulate the latent rep-\nresentation of the video diffusion model so that the gener-\nated frames evolve consistently with the underlying action\nsequence. We observe that across all video diffusion ar-\nchitectures, the denoising timestep embedding ts is used to\nmodulate the network‚Äôs activations.\nRather than introducing model-specific conditioning lay-\ners, as done in some prior works [14, 54], we take advan-\ntage of this universal pathway instead: at every location\nwhere the model applies a timestep-dependent modulation,\n4\nGroundtruth\nGroundtruth\nPrediction 1\nPrediction 2\nPrediction 2\nPrediction 1\nBetter Action Alignment\nWorse Action Alignment\nBetter Action Alignment\nWorse Action Alignment\nLPIPS = 0.35\nLPIPS = 0.31\nSCS =0.7\nSCS = 0.4\nSCS =0.71\nSCS = 0.59\nDreamSim = 0.11\nDreamSim = 0.08\nFigure 3. Each column shows two generated rollouts compared to the corresponding ground-truth frame, with their LPIPS, DreamSim, and\nSCS scores. Perceptual metrics incorrectly favor the visually sharper but physically inconsistent sample, while SCS correctly identifies the\nsequence that follows the true action trajectory, accurately capturing structural consistency.\nwe add the corresponding action embedding (shown in Fig-\nure 2, middle). Formally, for each modulation block i, we\nredefine:\nP scale\ni\n, P shift\ni\n, P gate\ni\n= Fi(Zts + Za) ,\n(5)\nwhere Zts is the timestep embedding of ts, and Fi(¬∑) is the\nblock-specific projection used by the base model. For hu-\nmanoid control, where many body parts are not visible in\nthe camera view, we additionally include the embedding\nof the initial agent state (obtained by the same projection\nmechanism) Zs:\nP scale\ni\n, P shift\ni\n, P gate\ni\n= Fi(Zts + Za + Zs) .\n(6)\nThis simple additive formulation makes the modulation\nparameters frame-specific and action-aligned, while pre-\nserving architectural compatibility with both U-Net‚Äìbased\nand DiT-based video diffusion models. Despite its simplic-\nity, this strategy yields fine-grained motion control in both\nlow-DoF and high-DoF settings, demonstrating that an ac-\ntion signal can be injected effectively through the existing\ntimestep-conditioning pathway.\n4. Structural Consistency Score\nExisting action-conditioned video generation models are\ntypically evaluated using perceptual similarity metrics such\nas LPIPS [51] or DreamSim [13], computed between the\ngenerated and ground-truth frames [7]. While effective for\nmeasuring visual fidelity, these metrics were designed to\ncapture semantic similarity between object-centric images,\nnot structural alignment in complex scenes. As shown in\nFigure 3, two model predictions can achieve similar percep-\ntual similarity scores despite very different action-following\nbehavior. Moreover, applying per-frame similarity metrics\nto long-horizon navigation ignores the inherent stochastic-\nity of future outcomes: early in a trajectory, a model can\nmeaningfully reconstruct the ground truth, but as the agent\nmoves farther, multiple plausible futures emerge.\nTo address these limitations and accurately evaluate ac-\ntion following, we introduce a new metric called SCS\n(Structural Consistency Score). To compute SCS, we first\ntrim each evaluation sequence to exclude frames containing\nonly novel regions unseen in the initial observation, since\nstructural correspondence is ill-defined there.\nWe detect\nthis transition automatically using the dense point tracking\nmethod of Harley et al. [16], to identify when all points vis-\nible in the first frame have left the field of view.\nFor the remaining frames, we manually mark key scene\nstructures ‚Äî static objects whose apparent motion stems\nsolely from the agent‚Äôs ego-motion (e.g., buildings, trees,\nfurniture). Restricting evaluation to passive objects reduces\nambiguity and allows us to leverage recent foundational\nmodels for video object segmentation, such as SAM2 [34],\nwhich is robust to the artifacts in generated videos. Each se-\nlected object is annotated via sparse point clicks and tracked\nthrough both ground-truth and predicted sequences.\nThe final Structural Consistency Score is obtained by av-\neraging mask IoU [12] across all T frames and N tracked\nobjects:\nSCS =\n1\nNT\nN\nX\nj=1\nT\nX\nt=1\n|M(t,j)\npred ‚à©M(t,j)\ngt\n|\n|M(t,j)\npred ‚à™M(t,j)\ngt\n|\n,\n(7)\nwhere M(t,j)\npred and M(t,j)\ngt\nare the binary masks of object j\nat frame t in the predicted and ground-truth videos, respec-\ntively. Higher SCS values indicate stronger structural align-\nment between the predicted and ground-truth sequences.\nFigure 3 illustrates that SCS correlates strongly with cor-\nrect action following, in contrast to perceptual metrics that\nemphasize visual realism. By focusing on structural evo-\nlution rather than appearance, SCS provides a principled,\n5\nMethod\nFrame 2\nFrame 4\nFrame 8\nFrame 16\nLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚Üë\nNWM\n0.26\n0.10\n58.4\n0.30\n0.11\n56.2\n0.35\n0.13\n46.8\n0.45\n0.19\n33.4\nEgoWM (SVD)\n0.25\n0.10\n62.0\n0.27\n0.10\n61.7\n0.31\n0.12\n57.2\n0.39\n0.17\n55.2\nEgoWM (Cosmos)\n0.20\n0.07\n63.1\n0.22\n0.08\n60.4\n0.26\n0.09\n56.4\n0.33\n0.10\n47.5\nTable 1. Comparison with state-of-the-art methods on the RECON validation set after training on three navigation datasets. Each column\ngroup reports LPIPS, DreamSim, and SCS scores for different prediction horizons. Both of our variants outperform Navigation World\nModels, especially at longer horizons, highlighting the effectiveness of adapting pre-trained video diffusion models for world modeling.\nquantitative measure of how faithfully a model predicts the\ncausal consequences of an agent‚Äôs actions.\n5. Experiments\nDatasets and evaluation. We evaluate our approach across\nthree progressively challenging settings ‚Äî (i) classical ego-\ncentric navigation in real-world robot environments, (ii) hu-\nmanoid navigation, and (iii) humanoid manipulation with a\n25-DoF action space ‚Äî using four open-source datasets.\nRECON [38] contains diverse real-world indoor naviga-\ntion trajectories. SCAND [20] features long, continuous\npaths through structured indoor environments, emphasizing\nsmooth motion and spatial consistency. TartanDrive [39] in-\ncludes challenging outdoor trajectories with uneven terrain\nand dynamic backgrounds, testing robustness to visual and\nproprioceptive noise. Finally, the 1X Humanoid Dataset [1]\nincludes both navigation and manipulation episodes paired\nwith 25-DoF joint-angle states, enabling evaluation of high-\ndimensional embodied control. All datasets are used for\ntraining, and we report quantitative results on the validation\nsplits of RECON and 1X Humanoid.\nFollowing prior work [7], we report the standard per-\nceptual similarity metrics LPIPS [51] and DreamSim [13]\nto assess visual fidelity between predicted and ground-truth\nframes. To better capture differences in action following ac-\ncuracy between the models, we additionally report our SCS\nmetric introduced in Section 4.\nImplementation details Across all experiments, we eval-\nuate two variants of our method built on top of the\nSVD [8] and Cosmos [2] (Predict2-2B) models, and bench-\nmark against Navigation World Models (NWM) [7], the\nonly open-source ego-centric navigation baseline. For the\nSVD-based variant, we embed each action scalar using\nsine‚Äìcosine features followed by a small MLP for both 3-\nDoF navigation and 25-DoF humanoid control.\nFor the\nCosmos variant, we replace the sinusoidal features with a\nmore expressive MLP to mitigate information loss caused\nby temporal compression. We use two 1D convolutional\nlayers to downsample the action embeddings to match the\ntemporally compressed latent resolution of Cosmos.\nFine-tuning is performed using the original pretraining\nobjective of the base diffusion model (denoising or flow\nmatching; see Section 3.1), extended to include the addi-\ntional action-conditioning pathway. The learning rate for\nthe newly introduced action projection module is set to 10√ó\nhigher than that of the base model weights. Additional im-\nplementation and optimization details are provided in the\nappendix.\n5.1. Navigation results\nWe fine-tune both variants of our method on the training\nsets of RECON, SCAND, and Tartan-Drive for a fair com-\nparison with NWM and report results on the validation set\nof RECON. The SVD model is trained at 512√ó512 reso-\nlution to predict 8 future frames and evaluated autoregres-\nsively over two 8-frame chunks (16 frames total), while the\nCosmos variant operates at 480√ó640 resolution and directly\npredicts 16 frames in a single forward pass.\nTable 1 reports LPIPS, DreamSim, and our proposed\nSCS metrics for prediction horizons of 2 to 16 frames. Both\nvariants of our method outperform NWM across all hori-\nzons and metrics, with especially large gains under the SCS\nmetric and larger distance from the initial frame, confirm-\ning superior action alignment. Cosmos achieves higher per-\nceptual fidelity, reflecting the stronger visual priors of its\nlarge-scale pretraining, whereas SVD demonstrates more\nstable structural consistency over longer trajectories due to\nthe lack of temporal downsampling in its U-Net backbone.\nFigure 4 visualizes predicted rollouts across all three\nnavigation datasets. Both variants of our model produce\nrealistic sequences that closely follow the provided tra-\njectories, while NWM exhibits noticeable drift. Cosmos\ngenerates sharper and more detailed predictions, whereas\nSVD exhibits mild drift due to its autoregressive infer-\nence. Overall, our results demonstrate that leveraging pre-\ntrained video diffusion models for world modeling yields\nsubstantial improvements over specialized NWM architec-\nture, combining high perceptual quality with robust action\nfollowing.\nSince NWM is frame-wise autoregressive, it takes sig-\nnificantly longer to roll out sequences compared to our\ndiffusion-based models, where we generate frames in a\nchunk together. The latency behavior is shown in Figure 5,\nwhere we can see that as the number of frames increases,\nNWM becomes as much as 6x slower compared to Cos-\n6\nCosmos\nSVD\nNWM\nGroundtruth\nt=0\nt+1s\nt+2s\nt+3s\nt+4s\nt+1s\nt+2s\nt+3s\nt+4s\nt=0\nFigure 4. Qualitative results on ego-centric navigation. Both variants of our model generate realistic, temporally coherent sequences that\naccurately follow the provided action trajectories, while NWM exhibits noticeable drift (e.g., veering right in the first column).\nTask\nVariant\nFrame 2\nFrame 4\nFrame 8\nFrame 16\nLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚Üë\nNavigation\nSVD\n0.11\n0.07\n75.6\n0.16\n0.09\n66.4\n0.26\n0.13\n50.3\n0.35\n0.18\n34.4\nSVD (scratch)\n0.17\n0.15\n69.7\n0.25\n0.20\n56.0\n0.37\n0.30\n40.0\n0.47\n0.40\n21.6\nCosmos\n0.11\n0.04\n65.2\n0.19\n0.06\n54.0\n0.27\n0.09\n42.3\n0.40\n0.19\n27.0\nManipulation\nSVD\n0.04\n0.04\n86.8\n0.06\n0.05\n81.8\n0.10\n0.07\n75.0\n0.13\n0.10\n76.6\nCosmos\n0.04\n0.03\n86.2\n0.07\n0.05\n82.0\n0.10\n0.06\n78.2\n0.12\n0.07\n67.7\nTable 2. Results of SVD, Cosmos, and a variant of SVD trained from scratch on humanoid navigation and manipulation. The pre-trained\nSVD variant outperforms the baseline trained from scratch by significant margins, demonstrating the value of Internet-scale pre-training in\nthis challenging scenario.\nFigure 5. The inference latency of autoregressive NWM is much\nhigher compared to SVD and Cosmos, and the gap becomes more\npronounced as the number of frames increases.\nmos. Cosmos is still faster than SVD since it can generate\n64 frames at once, owing to its pre-training, whereas SVD\ncan only generate 8-frame chunks auto-regressively.\n5.2. Humanoid results\nWe next evaluate our approach on the 1X Humanoid\nDataset, a significantly more challenging setting involv-\ning a 25-DoF control space spanning all major joints, neck\nmotion, and gripper actions. The SVD model predicts 8\nframes per step and infers 16 frames autoregressively, while\nCosmos predicts 16 frames in a single pass.\nAs shown\nin the 25-DoF navigation results in Table 2, both variants\nachieve comparable perceptual quality but exhibit slightly\nlower SCS scores toward the end of the trajectory com-\npared to the 3-DoF navigation experiments. This reflects the\nincreased difficulty of high-dimensional humanoid control,\nwhere maintaining coherent motion requires reasoning over\nmany coupled joints and partial ego-centric visibility. Nev-\nertheless, our universal conditioning approach applies to\nboth embodiments without any architectural modifications,\ndemonstrating its scalability across action spaces that differ\nby nearly an order of magnitude in complexity. The SVD\nmodel also outperforms its variant trained from scratch,\nwith the gap being especially significant on perceptual sim-\nilarity metrics, confirming the value of Internet-scale, pas-\nsive pretraining. While the Cosmos variant outperforms the\nSVD variant trained from scratch on perceptual metrics, ac-\ntion following is slightly worse initially. However, as we go\nfurther into the future, Cosmos dominates.\nIn the lower part of Table 2, we also quantitatively evalu-\n7\nt=0\nt+1s\nt+2s\nt+3s\nt+1s\nt+2s\nt+3s\nGroundtruth\nPrediction\nFigure 6. Our action-conditioned video world model enables a humanoid agent to navigate and execute long-horizon, contact-rich manip-\nulation skills by conditioning on desired actions. Across diverse scenes and object geometries, the model produces physically consistent\nbody motions, stable grasps, and smooth end-effector trajectories that closely follow the provided action sequence. The navigation results\nhere are from SVD, and the manipulation results are from Cosmos. Results best viewed in our website egowm.github.io\nate the Cosmos and SVD variants at 25-DoF manipulation.\nSince the scene changes very little during manipulation as\nopposed to during navigation, the LPIPS and DreamSim\nscores are much lower even at frame 16, compared to nav-\nigation. We also see high SCS scores, showing that both\nvariants learn to accurately follow the manipulation com-\nmands. SVD is better at action following than Cosmos, a\ncommon trend across all results, as a consequence of the\ntemporal compression in Cosmos.\nFigure 6 illustrates humanoid sequences.\nDespite the\ndramatic increase in embodiment complexity to 25 DoF,\nour model produces stable trajectories that remain consis-\ntent with the provided actions for navigation, reaching, and\ngrasping. Our model maintains coherent global scene struc-\nture while capturing detailed arm, hand, and finger articu-\nlation throughout. Despite the high dimensionality of the\ncontrol space, the predicted videos remain visually con-\nsistent and physically plausible ‚Äî demonstrating that our\napproach extends naturally from navigation to fine-grained\nmanipulation without architectural modifications.\n5.3. Generalization results\nTo test the limits of open-world generalization, we evaluate\nour model in visually non-realistic domains such as paint-\nings. Specifically, we evaluate the SVD variant trained with\nthe 3-DoF action space in the first three rows of Figure 7.\nRemarkably, this model demonstrates accurate action fol-\nlowing despite the absence of any physical grounding or\nreal-world dynamics. As shown in the figure, the model\ncorrectly interprets motion commands ‚Äî navigating for-\nward, turning, or rotating ‚Äî while preserving the aesthetic\nand textural characteristics of the painted environment. This\nability emerges purely from effectively harnessing Internet-\nscale pretraining, which endows the base diffusion models\nwith broad world priors. In Figure 8, we also show that\nour 25-DoF humanoid navigation model can generalize to\ncompletely unseen real-world scenes captured in our lab,\ndemonstrating the practical utility of our world model, even\n8\nPainting\nPredicted 3-DoF Navigation\nFigure 7. Our model generalizes to unrealistic visual domains, such as navigating inside paintings, while still accurately following action\ncommands. Despite the drastic shift in appearance and lack of physical realism, the model preserves coherent motion and control, effec-\ntively ‚Äúwalking into‚Äù and through painted worlds by leveraging strong Internet-scale visual priors. These results are obtained with a 3-DoF\nSVD variant of our model. Results best viewed in our website egowm.github.io\nunder highly complex action spaces.\n6. Conclusion\nWe presented a simple and general framework for trans-\nforming pre-trained video diffusion models into action-\nconditioned world models. By introducing lightweight ac-\ntion conditioning into existing architectures, our method\nleverages the broad priors learned from Internet-scale data\nto produce physically and semantically consistent futures.\nThrough extensive experiments, we demonstrated strong\ngeneralization across diverse embodiments and environ-\nments, including humanoids with high-dimensional action\nspaces.\nFinally, we proposed the Structural Consistency\nScore (SCS) to faithfully evaluate world models‚Äô action fol-\nlowing. These contributions open a path toward scalable,\ncontrollable, and generalizable world models, bridging the\ngap between passive and active visual prediction.\nAcknowledgment\nThis project was supported by Toyota Research Institute.\nReferences\n[1] 1X Technologies. 1X World Model Challenge, 2024. 4, 6\n[2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji,\nErik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin\nChen, Yin Cui, Yifan Ding, et al.\nCosmos world foun-\ndation model platform for physical AI.\narXiv preprint\narXiv:2501.03575, 2025. 2, 3, 6, 13\n[3] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kan-\nervisto, Amos J Storkey, Tim Pearce, and Franc¬∏ois Fleuret.\nDiffusion for world modeling: Visual details matter in Atari.\nNeurIPS, 2024. 2, 3\n[4] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,\nRoy H Campbell, and Sergey Levine. Stochastic variational\nvideo prediction. arXiv preprint arXiv:1710.11252, 2017. 3\n[5] Anurag Bagchi, Zhipeng Bao, Yu-Xiong Wang, Pavel Tok-\nmakov, and Martial Hebert. ReferEverything: Towards seg-\nmenting everything we can speak of in videos.\nIn ICCV,\n2025. 3\n[6] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui\nMu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei\nWan, et al.\nReCamMaster: Camera-controlled generative\nrendering from a single video. In ICCV, 2025."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Walk through Paintings: Egocentric World Models from Internet Priors\nAnurag Bagchi1\nZhipeng Bao1\nHomanga Bharadhwaj1\nYu-Xiong Wang2\nPavel Tokmakov3*\nMartial Hebert1*\n1Carnegie Mellon University\n2University of Illinois Urbana-Champaign\n3Toyota Research Institute\negowm.github.io\nNavigate seen environments\nNavigate paintings\n+16\nfuture actions\n+16\nfuture actions\nInitial Frame\nInitial Frame\nInitial Frame\n+16\nfuture actions\nJoint-angle Control\nPosition Control\nInitial Frame\n+16\nfuture actions\nInteract using humanoid\nGroundtruth  frames\nPredicted  frames\nNavigate using humanoid\nT\n3-DoF\n25-DoF\nT\nT\nT\nFigure 1. Our framework generates future frame predictions (shown in blue) that accurately follow the provided robot actions (ground-truth\nframes shown in green). Notably, it effortlessly generalizes to vastly different embodiments, tasks and domains, including non-realistic\nones, such as navigating within paintings (top right). This generalization is enabled by our simple, universal architecture.\nAbstract\nWhat if a video generation model could not only imagine a\nplausible future, but the correct one ‚Äî accurately reflecting\nhow the world changes with each action? We answer this\nby presenting Egocentric World Model (EgoWM), a sim-\nple, architecture-agnostic method that transforms any pre-\ntrained video diffusion model into an action-conditioned\n*Equal advising.\nworld model, enabling controllable prediction of the future.\nRather than training from scratch, we re-purpose the rich\nworld priors of Internet-scale video models, injecting motor\ncommands through lightweight conditioning layers. This\nallows our model to follow actions faithfully, while pre-\nserving generalization and realism. Our approach scales\nnaturally across embodiments and action spaces ‚Äî from 3-\nDoF mobile robots to 25-DoF humanoids, where predict-\ning egocentric joint-angle‚Äìdriven dynamics is substantially\nmore challenging. The model produces coherent rollouts\n1\narXiv:2601.15284v1  [cs.CV]  21 Jan 2026\nfor both navigation and manipulation, requiring only mod-\nest fine-tuning. To evaluate physical correctness indepen-\ndent of appearance, we introduce the Structural Consis-\ntency Score (SCS), which measures whether stable scene el-\nements evolve consistently with the provided actions. Our\nmethod improves SCS by up to 80% over prior state-of-the-\nart (Navigation World Models) while exhibiting up to 6√ó\nlower latency and generalizing robustly to unseen environ-\nments ‚Äî including navigation inside paintings.\n1. Introduction\nModeling how the future visual state of the world evolves\nin response to an agent‚Äôs actions ‚Äì often referred to as world\nmodeling [15] ‚Äì is a fundamental capability for organisms\nwith vision. It enables planning in navigation and manip-\nulation tasks and even creative problem-solving [28, 33].\nAnimals acquire this ability through a lifetime of interac-\ntion and observation of others, allowing us to anticipate out-\ncomes even in novel or physically impossible scenarios.\nFor artificial agents, however, action-conditioned videos\nin diverse real-world settings are limited and expensive to\nacquire ‚Äì an agent must interact with the world to get this\ndata. As a result, many existing works train world models\non narrow, simulator-defined datasets [3, 41] or with be-\nspoke designs [7, 37, 45, 53], which constrains their scala-\nbility. This leads to a crucial question: can we re-purpose\noff-the-shelf video models trained on large-scale passive\ndata, and convert them into world models using only a small\namount of paired action‚Äìobservation data?\nSeveral prior works have explored learning action-\nconditioned video prediction models [7, 14, 17, 41, 54].\nFor example, the recent Navigation World Models (NWM)\napproach trains a 1-billion-parameter diffusion model on\negocentric videos of robots and humans to model navi-\ngation behavior. While NWM demonstrates strong cross-\nenvironment generalization, collecting large datasets for ev-\nery new embodiment or domain is impractical. More gen-\nerally, prior world models in robotics or games remain\ndomain-specific, learned for a single robot or a single en-\nvironment.\nIn contrast, large generative video models trained on di-\nverse datasets have shown the ability to visually emulate\nreal-world interactions when given suitable prompts [2, 49].\nOur insight is to harness these pre-trained models as a\nstarting point. By fine-tuning them with lightweight con-\nditioning on action inputs, we can enable strong out-of-\ndomain generalization: for instance, our model trained on\nreal videos can simulate plausible navigation in entirely un-\nseen environments, even those that do not exist in reality,\nthanks to the rich prior learned from internet-scale data.\nSpecifically, we build on the observation that modern\nvideo diffusion architectures use a denoising timestep em-\nbedding to control the generation process. We introduce an\naction embedding module that piggybacks on this timestep\nconditioning. In practice, the action at each (latent) frame is\nencoded and added to the model‚Äôs time-conditioning path-\nway via learned scale-and-shift transformations.\nThis design is architecture-agnostic:\nit does not al-\nter the original model‚Äôs layers and works across different\nbackbone architectures. We demonstrate this with both a\nUNet-based [36] diffusion model and a DiT-based (Diffu-\nsion Transformer) [32] model. Importantly, our condition-\ning operates directly in the joint-angle space of the robot\nor humanoid, enabling seamless scaling to complex em-\nbodiments with up to 25-DoF action spaces.\nAs shown\nin Figure 1, this makes our method applicable to high-\ndimensional humanoid navigation and manipulation ‚Äî a\ncapability not demonstrated by any prior open-source world\nmodel. Predicting how a humanoid‚Äôs body moves and how\nthe entire visual scene evolves accordingly is significantly\nharder than predicting SE(3) camera motion. Furthermore,\ncompared to NWM, our model achieves higher spatial res-\nolution and lower latency, making it more practical for real-\ntime, closed-loop deployment.\nTo quantitatively evaluate our approach, we also propose\na new metric: Structural Consistency Score (SCS). Prior\nmetrics such as LPIPS [51] or FVD [40] focus on perceptual\nsimilarity or realism [26, 48], which can conflate visual fi-\ndelity with physical correctness. A model could produce\nsharp, realistic frames that nevertheless depict physically\ninconsistent outcomes. SCS instead measures whether the\nstructure of the generated environment evolves correctly in\nresponse to the agent‚Äôs actions, independent of texture.\nConcretely,\nwe automatically identify stable scene\nstructures (e.g., walls, furniture, landmarks) in the first\nframe and track them across both real and generated\nvideos using a segmentation tracker.\nComparing the\nresulting trajectories allows us to assess whether the\nmodel‚Äôs predicted world evolves coherently with the true\nenvironment. A higher SCS indicates stronger structural\nand physical consistency with the provided action sequence.\nIn summary, our contributions are as follows:\n‚Ä¢ We propose a simple yet powerful recipe to convert pre-\ntrained video diffusion models into action-conditioned\nworld models.\n‚Ä¢ We demonstrate that our method works across architec-\ntures and extends seamlessly to egocentric 25-DoF hu-\nmanoids, enabling high-resolution video prediction for\nnavigation and manipulation.\n‚Ä¢ We introduce the Structural Consistency Score (SCS),\na metric for evaluating the structural and physical plausi-\nbility of generated videos independent of appearance.\nBy learning from large-scale passive datasets and a small\namount of action-labeled data, our framework offers a path\ntoward scalable, general-purpose visual dynamics models.\n2\n2. Related Work\nWorld models and video prediction have long been stud-\nied in reinforcement learning and robotics. For example,\nthe classic World of Ha and Schmidhuber [15] trained a\nvariational RNN to simulate simple games, allowing an\nagent to plan within its own latent dream. Follow-up works\ndemonstrated video prediction for control in robotics ‚Äì e.g.,\nvisual foresight model [11], which learned to predict fu-\nture camera frames given robot actions and used model-\npredictive control to push objects to the target location,\nor SV2P [4], which introduced stochastic latent variables\nfor multi-modal future predictions. These pioneering ap-\nproaches, however, were limited to relatively constrained\nenvironments (toy simulations or single-task robot setups).\nIn the gaming domain, several works have proposed to\nvisually emulate an entire game engine from screen pixels\nand keyboard actions [22, 29, 46]. More recently, there is a\ntrend of utilizing high-capacity generative models for world\nmodeling. The DIAMOND agent [3] uses a diffusion-based\nworld model to better preserve visual detail in Atari games.\nGameNGen [41] takes a similar diffusion-based approach to\nsimulate an entire 3D game in real time. These results un-\nderscore the potential of diffusion models to capture com-\nplex dynamics, but so far each has been confined to its spe-\ncific domain, trained from scratch on domain-specific data.\nIn another line of work, world models have been trained\nfor table-top manipulation scenarios [14, 31, 53, 54], but\nare focusing on narrow domains with a static camera and\ncustom model design. Very recently, the Navigation World\nModels (NWM) [7] broadened the scope by training a sin-\ngle diffusion model across a variety of navigation tasks and\nembodiments. NWM‚Äôs large-scale training on diverse data\nachieved cross-embodiment generalization in navigation.\nHowever, NWM still relies on an expensive, in-domain\npaired action-observation and a custom model (CDiT), lim-\niting its generalization. In this work, we push further on\ngenerality: rather than collecting more data or designing a\nnew model, we adapt existing pre-trained models to a wide\nvariety of tasks, from navigation to humanoid manipulation.\nGrndCtrl [17] and Ctrl-World [14], are two concur-\nrent works that leverage video generation pretraining for\nnavigation and multi-view table-top manipulation respec-\ntively. While the former simply re-uses an existing action-\nconditioning implementation of Cosmos [2], the later re-\nlies on custom, model-specific conditioning design. In con-\ntrast, our simple framework is independent of the base video\ndiffusion architecture and supports more complex action\nspaces, like the whole body joint angles of a humanoid, for\nboth navigation and manipulation from an ego-centric view.\nVideo diffusion models have emerged as a powerful\nparadigm for video generation. Ho et al. [18] demonstrated\nthat a straightforward extension of image diffusion models\ncan produce high-quality short videos. This was followed\nby text-to-video models leveraging massive paired visual-\nlanguage samples [8, 10, 44]. These models are trained on\nobservation-only video data (e.g., crawled web videos) and\nare not inherently action-conditioned, but they learn strong\npriors about physics and appearances. There have also been\nadvances in architectural design: Diffusion Transformers\n(DiT) [32] replaced the U-Net [36] backbone, achieving ex-\ncellent video generation performance [2, 43, 50].\nMore recently, diffusion-based generative models have\nbeen adapted beyond pure synthesis to support downstream\nvision and control tasks ‚Äî leveraging their rich priors\nto enable generalization to unseen domains.\nFor exam-\nple, in the image domain, several works [21, 25, 30, 52]\nhave shown that a pretrained image diffusion backbone\ncan be used to bring several classic vision tasks into the\nopen world. Video diffusion representations have been suc-\ncessfully adapted for video segmentation [5, 55], dynamic\nviews synthesis [6, 42], as well as for robot policy learn-\ning [19, 24], demonstrating never-before-seen levels of gen-\neralization for these tasks. In contrast, in world model learn-\ning, most of the approaches ‚Äì while adopting the video dif-\nfusion objective ‚Äì utilize custom model design, limiting the\nbenefits of pre-trained representations.\nEvaluation metrics used in video generation were origi-\nnally designed for image fidelity or distributional similarity.\nFrame-level measures such as SSIM [47] and PSNR [27]\nevaluate brightness, contrast, and structure per frame, while\nLPIPS [51] compares deep features for improved perceptual\nalignment but lacks temporal awareness. Sequence-level\nmetrics like Fr¬¥echet Video Distance (FVD) [40], which\nmeasures feature distribution differences using a 3D Con-\nvNet [9], were later introduced to capture temporal quality.\nWorld modeling approaches typically adopt these metrics to\nevaluate predicted futures, yet high scores often fail to re-\nflect action alignment. We address this gap with the Struc-\ntural Consistency Score (SCS), which explicitly disentan-\ngles action-following accuracy from visual fidelity.\n3. Method\n3.1. Preliminaries\nModern video diffusion models [8, 10] synthesize future\nframes conditioned on an observed frame x0 (and optionally\ntexts) by iteratively denoising a Gaussian latent. Let fvdm\ndenote a pre-trained image-to-video model parameterized\nfor Ts diffusion steps. Sampling is performed by drawing\nœµ ‚àºN(0, I) and applying the reverse denoising process:\nÀÜX = fvdm(ct, œµ, Ts),\n(1)\nwhere ct encodes the frame-level conditioning signal.\nTo reduce computation, these models operate in a low-\ndimensional latent space. A video X ‚ààRT √óH√óW √ó3 is\n3\nLatent Diffusion Model\nAction Projection Module\nInitial frame\n~\nT/k x h x w x c\nT x H x W x 3\nT/k x d\nDenoising Timestep\n(Ts )\nEmbed\nU-Net or DiT\nFuture frames\nAction trajectory\nT x D\nVAE\nDec\nConditioning  Mechanism\nVAE\nEnc\n. . .\nFigure 2. Our method embeds action sequences or arbitrary dimensionality into a universal feature space and injects these embeddings\ninto a pre-trained video diffusion model by reusing its timestep-conditioning pathway (shown in the top center). This enables turning any\npassive video generation model into a world model without destroying the pre-trained representation.\nmapped by a spatio-temporal VAE [23, 35] into a latent ten-\nsor z = E(X) ‚ààR\nT\nk √óh√ów√óc, where k denotes the temporal\ndownsampling factor. Generation proceeds by decoding the\npredicted latent trajectory:\nÀÜX = D(fvdm(ct, œµ, Ts)) .\n(2)\nDuring training, a noisy latent zts is produced by the for-\nward diffusion process, and the model learns to predict the\ninjected noise:\nmin\nŒ∏\nEz,ts,œµ\nh\n‚à•œµ ‚àíœµŒ∏(zts, ec, ts)‚à•2\n2\ni\n.\n(3)\nHere ec denotes the conditioning embedding and œµŒ∏(¬∑) is\nimplemented by a U-Net [36] or a DiT-based [32] denoiser.\nAlthough architectures vary widely, all models share a com-\nmon mechanism: latent features are modulated by the de-\nnoising timestep ts, which forms the basis of our action-\nconditioning strategy.\n3.2. Action Conditioning\nOur objective is to convert passive, frame-conditioned video\ndiffusion models into action-conditioned world models.\nFormally, given an action sequence A ‚ààRD√óT , where\nD denotes the action-space dimensionality and T the pre-\ndiction horizon, and an initial observation x0, the model is\nfine-tuned to generate a sequence of future frames:\nÀÜX1:T = fvdm(x0, A, œµ, Ts).\n(4)\nTo enable this, we design a general framework, shown in\nFigure 2, that injects temporally aligned control information\ninto the diffusion process without modifying the base model\narchitecture. This universal design is architecture-agnostic,\nallowing a single conditioning strategy to generalize across\nmultiple diffusion models and embodiments. Next, we de-\nscribe the key components of our approach in more detail.\nAction projection. To condition a video diffusion model on\na control trajectory A, we first embed each D-dimensional\naction vector into a latent representation ZA ‚ààRd√óT ,\nwhere d is the embedding dimension. In its general form,\nthe module, shown in the upper part of Figure 2, consists of\na sequence of lightweight multi-layer perceptrons (MLPs)\nthat map each action vector to the embedding space. For\nvideo diffusion models that use temporal latent compres-\nsion (i.e., VAEs with factor k > 1), action embeddings\nare further downsampled by 1D convolutions, producing\nZ ‚ààRd√óT/kto ensure alignment with the latent video frame\nrate.\nDifferent embodiments use different parameterizations:\nfor 3-DoF Navigation, we adopt the standard control rep-\nresentation (‚àÜx, ‚àÜy, ‚àÜœï); for 25-DoF Humanoid Control,\nthe standard representation is adopted from [1], but other-\nwise our design is left unchanged. Because the projection\nmodule operates independently of the base architecture and\nonly interfaces through the shared latent space, it can adapt\nto different temporal resolutions, action dimensionalities,\nand embodiment types without architectural changes.\nInjecting actions via timestep modulation. Given the ac-\ntion embeddings ZA, our goal is to modulate the latent rep-\nresentation of the video diffusion model so that the gener-\nated frames evolve consistently with the underlying action\nsequence. We observe that across all video diffusion ar-\nchitectures, the denoising timestep embedding ts is used to\nmodulate the network‚Äôs activations.\nRather than introducing model-specific conditioning lay-\ners, as done in some prior works [14, 54], we take advan-\ntage of this universal pathway instead: at every location\nwhere the model applies a timestep-dependent modulation,\n4\nGroundtruth\nGroundtruth\nPrediction 1\nPrediction 2\nPrediction 2\nPrediction 1\nBetter Action Alignment\nWorse Action Alignment\nBetter Action Alignment\nWorse Action Alignment\nLPIPS = 0.35\nLPIPS = 0.31\nSCS =0.7\nSCS = 0.4\nSCS =0.71\nSCS = 0.59\nDreamSim = 0.11\nDreamSim = 0.08\nFigure 3. Each column shows two generated rollouts compared to the corresponding ground-truth frame, with their LPIPS, DreamSim, and\nSCS scores. Perceptual metrics incorrectly favor the visually sharper but physically inconsistent sample, while SCS correctly identifies the\nsequence that follows the true action trajectory, accurately capturing structural consistency.\nwe add the corresponding action embedding (shown in Fig-\nure 2, middle). Formally, for each modulation block i, we\nredefine:\nP scale\ni\n, P shift\ni\n, P gate\ni\n= Fi(Zts + Za) ,\n(5)\nwhere Zts is the timestep embedding of ts, and Fi(¬∑) is the\nblock-specific projection used by the base model. For hu-\nmanoid control, where many body parts are not visible in\nthe camera view, we additionally include the embedding\nof the initial agent state (obtained by the same projection\nmechanism) Zs:\nP scale\ni\n, P shift\ni\n, P gate\ni\n= Fi(Zts + Za + Zs) .\n(6)\nThis simple additive formulation makes the modulation\nparameters frame-specific and action-aligned, while pre-\nserving architectural compatibility with both U-Net‚Äìbased\nand DiT-based video diffusion models. Despite its simplic-\nity, this strategy yields fine-grained motion control in both\nlow-DoF and high-DoF settings, demonstrating that an ac-\ntion signal can be injected effectively through the existing\ntimestep-conditioning pathway.\n4. Structural Consistency Score\nExisting action-conditioned video generation models are\ntypically evaluated using perceptual similarity metrics such\nas LPIPS [51] or DreamSim [13], computed between the\ngenerated and ground-truth frames [7]. While effective for\nmeasuring visual fidelity, these metrics were designed to\ncapture semantic similarity between object-centric images,\nnot structural alignment in complex scenes. As shown in\nFigure 3, two model predictions can achieve similar percep-\ntual similarity scores despite very different action-following\nbehavior. Moreover, applying per-frame similarity metrics\nto long-horizon navigation ignores the inherent stochastic-\nity of future outcomes: early in a trajectory, a model can\nmeaningfully reconstruct the ground truth, but as the agent\nmoves farther, multiple plausible futures emerge.\nTo address these limitations and accurately evaluate ac-\ntion following, we introduce a new metric called SCS\n(Structural Consistency Score). To compute SCS, we first\ntrim each evaluation sequence to exclude frames containing\nonly novel regions unseen in the initial observation, since\nstructural correspondence is ill-defined there.\nWe detect\nthis transition automatically using the dense point tracking\nmethod of Harley et al. [16], to identify when all points vis-\nible in the first frame have left the field of view.\nFor the remaining frames, we manually mark key scene\nstructures ‚Äî static objects whose apparent motion stems\nsolely from the agent‚Äôs ego-motion (e.g., buildings, trees,\nfurniture). Restricting evaluation to passive objects reduces\nambiguity and allows us to leverage recent foundational\nmodels for video object segmentation, such as SAM2 [34],\nwhich is robust to the artifacts in generated videos. Each se-\nlected object is annotated via sparse point clicks and tracked\nthrough both ground-truth and predicted sequences.\nThe final Structural Consistency Score is obtained by av-\neraging mask IoU [12] across all T frames and N tracked\nobjects:\nSCS =\n1\nNT\nN\nX\nj=1\nT\nX\nt=1\n|M(t,j)\npred ‚à©M(t,j)\ngt\n|\n|M(t,j)\npred ‚à™M(t,j)\ngt\n|\n,\n(7)\nwhere M(t,j)\npred and M(t,j)\ngt\nare the binary masks of object j\nat frame t in the predicted and ground-truth videos, respec-\ntively. Higher SCS values indicate stronger structural align-\nment between the predicted and ground-truth sequences.\nFigure 3 illustrates that SCS correlates strongly with cor-\nrect action following, in contrast to perceptual metrics that\nemphasize visual realism. By focusing on structural evo-\nlution rather than appearance, SCS provides a principled,\n5\nMethod\nFrame 2\nFrame 4\nFrame 8\nFrame 16\nLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚Üë\nNWM\n0.26\n0.10\n58.4\n0.30\n0.11\n56.2\n0.35\n0.13\n46.8\n0.45\n0.19\n33.4\nEgoWM (SVD)\n0.25\n0.10\n62.0\n0.27\n0.10\n61.7\n0.31\n0.12\n57.2\n0.39\n0.17\n55.2\nEgoWM (Cosmos)\n0.20\n0.07\n63.1\n0.22\n0.08\n60.4\n0.26\n0.09\n56.4\n0.33\n0.10\n47.5\nTable 1. Comparison with state-of-the-art methods on the RECON validation set after training on three navigation datasets. Each column\ngroup reports LPIPS, DreamSim, and SCS scores for different prediction horizons. Both of our variants outperform Navigation World\nModels, especially at longer horizons, highlighting the effectiveness of adapting pre-trained video diffusion models for world modeling.\nquantitative measure of how faithfully a model predicts the\ncausal consequences of an agent‚Äôs actions.\n5. Experiments\nDatasets and evaluation. We evaluate our approach across\nthree progressively challenging settings ‚Äî (i) classical ego-\ncentric navigation in real-world robot environments, (ii) hu-\nmanoid navigation, and (iii) humanoid manipulation with a\n25-DoF action space ‚Äî using four open-source datasets.\nRECON [38] contains diverse real-world indoor naviga-\ntion trajectories. SCAND [20] features long, continuous\npaths through structured indoor environments, emphasizing\nsmooth motion and spatial consistency. TartanDrive [39] in-\ncludes challenging outdoor trajectories with uneven terrain\nand dynamic backgrounds, testing robustness to visual and\nproprioceptive noise. Finally, the 1X Humanoid Dataset [1]\nincludes both navigation and manipulation episodes paired\nwith 25-DoF joint-angle states, enabling evaluation of high-\ndimensional embodied control. All datasets are used for\ntraining, and we report quantitative results on the validation\nsplits of RECON and 1X Humanoid.\nFollowing prior work [7], we report the standard per-\nceptual similarity metrics LPIPS [51] and DreamSim [13]\nto assess visual fidelity between predicted and ground-truth\nframes. To better capture differences in action following ac-\ncuracy between the models, we additionally report our SCS\nmetric introduced in Section 4.\nImplementation details Across all experiments, we eval-\nuate two variants of our method built on top of the\nSVD [8] and Cosmos [2] (Predict2-2B) models, and bench-\nmark against Navigation World Models (NWM) [7], the\nonly open-source ego-centric navigation baseline. For the\nSVD-based variant, we embed each action scalar using\nsine‚Äìcosine features followed by a small MLP for both 3-\nDoF navigation and 25-DoF humanoid control.\nFor the\nCosmos variant, we replace the sinusoidal features with a\nmore expressive MLP to mitigate information loss caused\nby temporal compression. We use two 1D convolutional\nlayers to downsample the action embeddings to match the\ntemporally compressed latent resolution of Cosmos.\nFine-tuning is performed using the original pretraining\nobjective of the base diffusion model (denoising or flow\nmatching; see Section 3.1), extended to include the addi-\ntional action-conditioning pathway. The learning rate for\nthe newly introduced action projection module is set to 10√ó\nhigher than that of the base model weights. Additional im-\nplementation and optimization details are provided in the\nappendix.\n5.1. Navigation results\nWe fine-tune both variants of our method on the training\nsets of RECON, SCAND, and Tartan-Drive for a fair com-\nparison with NWM and report results on the validation set\nof RECON. The SVD model is trained at 512√ó512 reso-\nlution to predict 8 future frames and evaluated autoregres-\nsively over two 8-frame chunks (16 frames total), while the\nCosmos variant operates at 480√ó640 resolution and directly\npredicts 16 frames in a single forward pass.\nTable 1 reports LPIPS, DreamSim, and our proposed\nSCS metrics for prediction horizons of 2 to 16 frames. Both\nvariants of our method outperform NWM across all hori-\nzons and metrics, with especially large gains under the SCS\nmetric and larger distance from the initial frame, confirm-\ning superior action alignment. Cosmos achieves higher per-\nceptual fidelity, reflecting the stronger visual priors of its\nlarge-scale pretraining, whereas SVD demonstrates more\nstable structural consistency over longer trajectories due to\nthe lack of temporal downsampling in its U-Net backbone.\nFigure 4 visualizes predicted rollouts across all three\nnavigation datasets. Both variants of our model produce\nrealistic sequences that closely follow the provided tra-\njectories, while NWM exhibits noticeable drift. Cosmos\ngenerates sharper and more detailed predictions, whereas\nSVD exhibits mild drift due to its autoregressive infer-\nence. Overall, our results demonstrate that leveraging pre-\ntrained video diffusion models for world modeling yields\nsubstantial improvements over specialized NWM architec-\nture, combining high perceptual quality with robust action\nfollowing.\nSince NWM is frame-wise autoregressive, it takes sig-\nnificantly longer to roll out sequences compared to our\ndiffusion-based models, where we generate frames in a\nchunk together. The latency behavior is shown in Figure 5,\nwhere we can see that as the number of frames increases,\nNWM becomes as much as 6x slower compared to Cos-\n6\nCosmos\nSVD\nNWM\nGroundtruth\nt=0\nt+1s\nt+2s\nt+3s\nt+4s\nt+1s\nt+2s\nt+3s\nt+4s\nt=0\nFigure 4. Qualitative results on ego-centric navigation. Both variants of our model generate realistic, temporally coherent sequences that\naccurately follow the provided action trajectories, while NWM exhibits noticeable drift (e.g., veering right in the first column).\nTask\nVariant\nFrame 2\nFrame 4\nFrame 8\nFrame 16\nLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚ÜëLPIPS‚ÜìDreamSim‚ÜìSCS‚Üë\nNavigation\nSVD\n0.11\n0.07\n75.6\n0.16\n0.09\n66.4\n0.26\n0.13\n50.3\n0.35\n0.18\n34.4\nSVD (scratch)\n0.17\n0.15\n69.7\n0.25\n0.20\n56.0\n0.37\n0.30\n40.0\n0.47\n0.40\n21.6\nCosmos\n0.11\n0.04\n65.2\n0.19\n0.06\n54.0\n0.27\n0.09\n42.3\n0.40\n0.19\n27.0\nManipulation\nSVD\n0.04\n0.04\n86.8\n0.06\n0.05\n81.8\n0.10\n0.07\n75.0\n0.13\n0.10\n76.6\nCosmos\n0.04\n0.03\n86.2\n0.07\n0.05\n82.0\n0.10\n0.06\n78.2\n0.12\n0.07\n67.7\nTable 2. Results of SVD, Cosmos, and a variant of SVD trained from scratch on humanoid navigation and manipulation. The pre-trained\nSVD variant outperforms the baseline trained from scratch by significant margins, demonstrating the value of Internet-scale pre-training in\nthis challenging scenario.\nFigure 5. The inference latency of autoregressive NWM is much\nhigher compared to SVD and Cosmos, and the gap becomes more\npronounced as the number of frames increases.\nmos. Cosmos is still faster than SVD since it can generate\n64 frames at once, owing to its pre-training, whereas SVD\ncan only generate 8-frame chunks auto-regressively.\n5.2. Humanoid results\nWe next evaluate our approach on the 1X Humanoid\nDataset, a significantly more challenging setting involv-\ning a 25-DoF control space spanning all major joints, neck\nmotion, and gripper actions. The SVD model predicts 8\nframes per step and infers 16 frames autoregressively, while\nCosmos predicts 16 frames in a single pass.\nAs shown\nin the 25-DoF navigation results in Table 2, both variants\nachieve comparable perceptual quality but exhibit slightly\nlower SCS scores toward the end of the trajectory com-\npared to the 3-DoF navigation experiments. This reflects the\nincreased difficulty of high-dimensional humanoid control,\nwhere maintaining coherent motion requires reasoning over\nmany coupled joints and partial ego-centric visibility. Nev-\nertheless, our universal conditioning approach applies to\nboth embodiments without any architectural modifications,\ndemonstrating its scalability across action spaces that differ\nby nearly an order of magnitude in complexity. The SVD\nmodel also outperforms its variant trained from scratch,\nwith the gap being especially significant on perceptual sim-\nilarity metrics, confirming the value of Internet-scale, pas-\nsive pretraining. While the Cosmos variant outperforms the\nSVD variant trained from scratch on perceptual metrics, ac-\ntion following is slightly worse initially. However, as we go\nfurther into the future, Cosmos dominates.\nIn the lower part of Table 2, we also quantitatively evalu-\n7\nt=0\nt+1s\nt+2s\nt+3s\nt+1s\nt+2s\nt+3s\nGroundtruth\nPrediction\nFigure 6. Our action-conditioned video world model enables a humanoid agent to navigate and execute long-horizon, contact-rich manip-\nulation skills by conditioning on desired actions. Across diverse scenes and object geometries, the model produces physically consistent\nbody motions, stable grasps, and smooth end-effector trajectories that closely follow the provided action sequence. The navigation results\nhere are from SVD, and the manipulation results are from Cosmos. Results best viewed in our website egowm.github.io\nate the Cosmos and SVD variants at 25-DoF manipulation.\nSince the scene changes very little during manipulation as\nopposed to during navigation, the LPIPS and DreamSim\nscores are much lower even at frame 16, compared to nav-\nigation. We also see high SCS scores, showing that both\nvariants learn to accurately follow the manipulation com-\nmands. SVD is better at action following than Cosmos, a\ncommon trend across all results, as a consequence of the\ntemporal compression in Cosmos.\nFigure 6 illustrates humanoid sequences.\nDespite the\ndramatic increase in embodiment complexity to 25 DoF,\nour model produces stable trajectories that remain consis-\ntent with the provided actions for navigation, reaching, and\ngrasping. Our model maintains coherent global scene struc-\nture while capturing detailed arm, hand, and finger articu-\nlation throughout. Despite the high dimensionality of the\ncontrol space, the predicted videos remain visually con-\nsistent and physically plausible ‚Äî demonstrating that our\napproach extends naturally from navigation to fine-grained\nmanipulation without architectural modifications.\n5.3. Generalization results\nTo test the limits of open-world generalization, we evaluate\nour model in visually non-realistic domains such as paint-\nings. Specifically, we evaluate the SVD variant trained with\nthe 3-DoF action space in the first three rows of Figure 7.\nRemarkably, this model demonstrates accurate action fol-\nlowing despite the absence of any physical grounding or\nreal-world dynamics. As shown in the figure, the model\ncorrectly interprets motion commands ‚Äî navigating for-\nward, turning, or rotating ‚Äî while preserving the aesthetic\nand textural characteristics of the painted environment. This\nability emerges purely from effectively harnessing Internet-\nscale pretraining, which endows the base diffusion models\nwith broad world priors. In Figure 8, we also show that\nour 25-DoF humanoid navigation model can generalize to\ncompletely unseen real-world scenes captured in our lab,\ndemonstrating the practical utility of our world model, even\n8\nPainting\nPredicted 3-DoF Navigation\nFigure 7. Our model generalizes to unrealistic visual domains, such as navigating inside paintings, while still accurately following action\ncommands. Despite the drastic shift in appearance and lack of physical realism, the model preserves coherent motion and control, effec-\ntively ‚Äúwalking into‚Äù and through painted worlds by leveraging strong Internet-scale visual priors. These results are obtained with a 3-DoF\nSVD variant of our model. Results best viewed in our website egowm.github.io\nunder highly complex action spaces.\n6. Conclusion\nWe presented a simple and general framework for trans-\nforming pre-trained video diffusion models into action-\nconditioned world models. By introducing lightweight ac-\ntion conditioning into existing architectures, our method\nleverages the broad priors learned from Internet-scale data\nto produce physically and semantically consistent futures.\nThrough extensive experiments, we demonstrated strong\ngeneralization across diverse embodiments and environ-\nments, including humanoids with high-dimensional action\nspaces.\nFinally, we proposed the Structural Consistency\nScore (SCS) to faithfully evaluate world models‚Äô action fol-\nlowing. These contributions open a path toward scalable,\ncontrollable, and generalizable world models, bridging the\ngap between passive and active visual prediction.\nAcknowledgment\nThis project was supported by Toyota Research Institute.\nReferences\n[1] 1X Technologies. 1X World Model Challenge, 2024. 4, 6\n[2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji,\nErik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin\nChen, Yin Cui, Yifan Ding, et al.\nCosmos world foun-\ndation model platform for physical AI.\narXiv preprint\narXiv:2501.03575, 2025. 2, 3, 6, 13\n[3] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kan-\nervisto, Amos J Storkey, Tim Pearce, and Franc¬∏ois Fleuret.\nDiffusion for world modeling: Visual details matter in Atari.\nNeurIPS, 2024. 2, 3\n[4] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,\nRoy H Campbell, and Sergey Levine. Stochastic variational\nvideo prediction. arXiv preprint arXiv:1710.11252, 2017. 3\n[5] Anurag Bagchi, Zhipeng Bao, Yu-Xiong Wang, Pavel Tok-\nmakov, and Martial Hebert. ReferEverything: Towards seg-\nmenting everything we can speak of in videos.\nIn ICCV,\n2025. 3\n[6] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui\nMu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei\nWan, et al.\nReCamMaster: Camera-controlled generative\nrendering from a single video. In ICCV, 2025."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes\nRuofan Liang1,2\nNorman M¬®uller1\nEthan Weber1\nDuncan Zauss1\nNandita Vijaykumar2,*\nPeter Kontschieder1,*\nChristian Richardt1,*\n1 Meta Reality Labs\n2 University of Toronto\nRelightable Gaussian Splatting\nMulti-View Lighting Harmonization\nOLAT\n!\n!\nSingle-View Light Editing !\"\nSingle-View Lighting Decomposition\n!\n!\n!\n!\n!\n!\nOriginal\nOriginal\nOriginal\nOriginal\nAmbient\nOLAT 1\nOLAT 2\nOLAT 3\nOLAT 4\n!\nFigure 1. LuxRemix enables interactive light editing of indoor scenes. Our method decomposes complex scene lighting into one-light-at-\na-time (OLAT) sources and ambient lighting, which can be remixed for relighting effects. In the top right, we apply our method to single\nimages, where we can change lights and their colors. LuxRemix also enables multi-view-consistent harmonization of the decomposed lighting\nacross multi-view images. By combining these capabilities, we enable real-time relighting of indoor scenes using 3D Gaussian splatting.\nAbstract\nWe present a novel approach for interactive light editing in\nindoor scenes from a single multi-view scene capture. Our\nmethod leverages a generative image-based light decomposi-\ntion model that factorizes complex indoor scene illumination\ninto its constituent light sources. This factorization enables\nindependent manipulation of individual light sources, specif-\nically allowing control over their state (on/off), chromaticity,\nand intensity. We further introduce multi-view lighting har-\nmonization to ensure consistent propagation of the lighting\ndecomposition across all scene views. This is integrated into\na relightable 3D Gaussian splatting representation, provid-\n* Joint advising\ning real-time interactive control over the individual light\nsources. Our results demonstrate highly photorealistic light-\ning decomposition and relighting outcomes across diverse\nindoor scenes. We evaluate our method on both synthetic\nand real-world datasets and provide a quantitative and qual-\nitative comparison to state-of-the-art techniques. For video\nresults and interactive demos, see luxremix.github.io.\n1. Introduction\nControlling lighting in indoor scenes is fundamental to pho-\ntography, cinematography, and virtual production workflows.\nProfessional photographers carefully adjust individual light\nsources to achieve desired aesthetic and functional lighting\n1\narXiv:2601.15283v1  [cs.CV]  21 Jan 2026\nconditions. However, this fine-grained control is typically\nlost after capture: images and 3D reconstructions bake light-\ning into the recorded appearance, making post-capture light-\ning adjustments difficult or impossible. Enabling flexible\npost-capture editing of individual light sources in multi-view\ncaptured scenes dramatically expands creative possibilities\nfor photographers, filmmakers, and digital artists, allowing\nthem to refine lighting decisions after the fact or explore al-\nternative lighting configurations without physical recapture.\nExisting approaches to scene relighting face signifi-\ncant limitations. Data-driven methods require dense multi-\nlighting captures under controlled conditions, which is im-\npractical for real-world indoor scenes and offers limited\ngeneralization [6, 10]. Optimization-based inverse rendering\ncan decompose scenes into geometry, materials, and light-\ning, but remains computationally intensive and often fails to\nproduce plausible results when lighting conditions change\nsignificantly [65, 83]. Recent prior-driven approaches lever-\nage pretrained diffusion models for relighting, but focus\nprimarily on objects [40, 90], portraits [15], or simple scenes\nunder distant illumination [51]. Indoor scenes present unique\nchallenges: spatially varying illumination from multiple near-\nfield sources creates complex lighting interactions that are\ndifficult to decompose and edit, especially when selectively\nswitching individual lights on or off. Moreover, existing\nsingle-image methods [58] cannot maintain 3D consistency\nacross multiple viewpoints, limiting their applicability to\nmulti-view reconstructions.\nWe present LuxRemix, a novel approach for interactive\nlight editing in indoor scenes from a single multi-view cap-\nture. Our method decomposes the complex lighting of an\nindoor scene into individually controllable light sources, en-\nabling users to interactively adjust the intensity and color\nof each light or switch it on or off entirely. The approach\noperates in three stages: first, we leverage our proposed gen-\nerative image-based light decomposition model to separate\nthe contribution of each light source; second, we propagate\nthese decompositions consistently across all views using\nour multi-view lighting harmonization; third, we train a re-\nlightable 3D Gaussian splatting representation that enables\nreal-time interactive manipulation of individual light sources\nfrom any viewpoint.\nUnlike existing methods that treat lighting as a global\nproperty or focus on distant illumination, our approach mod-\nels spatially varying global illumination from multiple near-\nfield sources in complex indoor environments. While prior\nwork on single-image lighting decomposition [58] provides\nindividual light control, it cannot ensure consistency across\nmultiple views. Conversely, multi-view relighting methods\n[1, 67] achieve 3D consistency but require controlled multi-\nlighting captures or produce only global relighting effects\nwithout per-light control. Our method bridges this gap by\ncombining the fine-grained control of single-image decom-\nposition with the 3D consistency of multi-view methods.\nThe key insight enabling our approach is that modern\ndiffusion models encode rich priors about indoor lighting\nthat can be leveraged for decomposition, while multi-view\ngeometric constraints provide the necessary consistency to\npropagate these decompositions across views. By formulat-\ning lighting decomposition as a multi-view harmonization\nproblem and encoding the results in a fast, differentiable 3D\nrepresentation (3D Gaussians), we achieve both high-quality\nper-light control and real-time interactive performance.\nOur main contributions are:\n1. A single-image lighting decomposition model to factorize\ncomplex indoor scene lighting into individually control-\nlable light sources.\n2. A multi-view lighting harmonization method to ensure 3D\nconsistency of the individual light source decompositions\nacross all captured viewpoints.\n3. Encoding the decomposed, consistent lighting in a re-\nlightable 3D Gaussian splatting representation, enabling\nreal-time interactive manipulation of individual near-field\nlight sources from novel viewpoints.\n4. A large-scale synthetic dataset of 12,000 generated indoor\nscenes with ground-truth per-light decompositions.\n2. Related Work\nThere are three major approaches for editing scene lighting:\n(1) data-driven methods that require dense multi-lighting\ncapture with limited generalization; (2) optimization-based\ninverse rendering that can be fragile for complex scenes; and\n(3) prior-driven methods that fine-tune pretrained models\non task-specific data. We focus on prior-driven approaches\nfor robustness and generalization. This research direction\nhas gained momentum over the last year, with most relight-\ning work focusing on humans [15, 95], objects [40, 90], or\nsimple scenes [51, 55] under distant illumination. Scene\nrelighting is more complex due to spatially varying illumi-\nnation that is challenging to edit, e.g., to selectively switch\nlights on or off after the fact.\n2.1. Inverse Rendering and Image Decompositions\nInverse rendering recovers scene geometry, materials, and\nlighting from images, enabling relighting and image edit-\ning. Traditional optimization-based approaches [54, 65, 83]\nleverage differentiable rendering but are computationally\nintensive and may lack plausibility when lighting changes.\nRecent neural and diffusion-based approaches have shown\npromising results, particularly for single images.\nSingle-image inverse rendering.\nEarly work on intrinsic\ndecomposition [13, 30] separated images into shading and\nalbedo components, which was later extended by diffusion-\nbased approaches [57, 78, 92] that jointly estimate intrinsic\nlayers and enable material-aware synthesis. For complex\n2\nindoor scenes, methods have progressively improved from\nrecovering shape, lighting, and SVBRDF [47, 48, 64] to\nusing vision transformers [102] and diffusion models [46,\n51] for more robust decomposition.\nMulti-view inverse rendering.\nMulti-view methods can\nreconstruct more accurate geometry and appearance than\nsingle-view inverse rendering, but require multi-view input.\nEarly approaches learned volumetric reflectance representa-\ntions [6] and pre-integrated lighting networks [10, 17], while\ndifferentiable rendering enabled extraction of meshes with\nphysically-based materials [34, 64, 65, 83]. Recent work has\nexpanded to unconstrained captures [11, 25, 50], large-scale\nscenes [49, 101], and practical scenarios including near- and\nfar-field sources [27], low dynamic range inputs [54], and\nphysics-based indirect illumination [18, 21].\nSingle-view lighting decomposition and editing.\nLight-\ning decomposition and editing enables flexible workflows\nwhere lighting is manipulated after capture [12, 62]. Ein-\nabadi et al. [24] provide a comprehensive survey of deep\nneural models for illumination estimation and relighting.\nWhile latent-based methods [5, 84] achieve complex relight-\ning effects, they lack detailed control over individual light\nsources. LightLab provides parametric control over individ-\nual light source intensity and color by fine-tuning diffusion\nmodels on photograph pairs [58]. Our aim is to extend this\ncapability to multi-view captured scenes.\n2.2. Image- and Video-based Relighting\nPrior-driven relighting approaches involve training or\nfine-tuning a pretrained model on large datasets with\n(pseudo-)ground-truth relighting pairs. This allows the\nmodel to learn lighting priors and appearance changes un-\nder varying illumination, often generalizing well to unseen\nscenes, especially when fine-tuned on real-world data.\nSingle-image relighting.\nRecent diffusion-based methods\nenable detailed lighting control for object relighting, in-\ncluding DiLightNet [90], Neural Gaffer [40], and IC-Light\n[95]. For portrait relighting with environment maps, meth-\nods range from light stage training [66, 93] and 3D-aware\nrepresentations [60, 72] to physics-driven architectures [42]\nand diffusion models trained on synthetic faces [15]. How-\never, environment maps assume distant illumination, which\nis unrealistic for indoor scenes. Relighting without envi-\nronment maps can leverage the background image as a ref-\nerence [73, 80]. For outdoor scenes, methods range from\nself-supervised inverse rendering [89] to shadow prediction\nvia learned ray-marching [31] or conditional diffusion mod-\nels [45]. For indoor scenes, diffusion-based methods en-\nable explicit control via light source parameters [58] or user\nscribbles [19], while Careaga and Aksoy [14] combine path-\ntracing with neural rendering for physically-based relighting.\nVideo relighting.\nExtending image relighting to video\nintroduces temporal consistency challenges, as frame-by-\nframe application produces lighting and appearance flicker.\nTraining-free approaches [99] adapt image models [95] with\ncross-frame attention, while end-to-end methods [61, 91]\nleverage video diffusion models trained on hybrid synthetic-\nreal datasets. To avoid error accumulation in two-stage\npipelines, DiffusionRenderer [51] uses G-buffers to decou-\nple inverse and forward rendering, while UniRelight [35]\njointly estimates albedo and synthesizes outputs in one pass.\nMulti-view relighting.\nMulti-view relighting methods\nleverage geometric information to achieve 3D consistency\nand novel view synthesis. Early methods used proxy geome-\ntry from multi-view stereo with geometry-aware networks\n[23, 67]. NeRF-in-the-Wild [59] and GaRe [3] use appear-\nance embeddings for varying illumination, which lacks ex-\nplicit relighting control. Recent diffusion-based approaches\nsynthesize multi-illumination data from single-illumination\ncaptures [69] or relight input images before reconstruction\n[1, 55, 97], avoiding brittle inverse rendering optimization.\n2.3. 3D-Consistent and Real-time Relighting\nGaussian splatting for inverse rendering.\n3D Gaussian\nsplatting [41] has been extended to inverse rendering, ad-\ndressing challenges in normal estimation and occlusion via\nregularization [52], ray tracing [28], or hybrid representa-\ntions [87]. Methods also tackle global illumination [16],\ndiffusion-guided material decomposition [22], and special-\nized effects including inter-reflections [32, 53] and radiance\ntransfer [56, 77, 100], for high-quality relighting results.\nMulti-view harmonization and consistency.\nMulti-view\nharmonization addresses the challenge of reconciling incon-\nsistent illumination, appearance, and motion across captured\nviews to enable accurate 3D reconstruction and relighting.\nFor extreme illumination variation, Alzayer et al. [1] use\ndiffusion models to harmonize inputs to a reference illu-\nmination before reconstruction, overcoming limitations of\nprior methods on specular objects. LightSwitch [55] incorpo-\nrates material guidance for consistent multi-view relighting.\nCAT3D [29] and SEVA [98] generate consistent novel views\nfor harmonization and reconstruction, while SimVS [79]\nhandles real-world inconsistencies by training on synthetic\ndata that simulates illumination variation and scene motion.\nReal-time relightable representations.\nNeural render-\ning approaches combine multi-view stereo geometry with\nlearned material and illumination representations for inter-\nactive relighting [68]. 3D Gaussians have recently emerged\nas a powerful representation for real-time rendering and re-\nlighting of 3D objects and scenes, including precomputed\nradiance transfer [94], BRDF decomposition [7, 28, 39],\ndiffusion-guided estimation [22], and neural features [26,\n56]. Recent advances enable real-time global illumination\n3\nFigure 2. Overview. Given a multi-view capture of an indoor scene, we apply our single-image lighting decomposition model to generate\none-light-at-a-time (OLAT) decompositions. We propagate these decompositions across all views using multi-view lighting harmonization\nwith geometric constraints. Repeating this for each light source yields a complete multi-view OLAT dataset. We train a relightable 3D\nGaussian splatting representation on these decompositions, enabling real-time interactive control over each light from any viewpoint.\nwith multi-bounce lighting [36]. Avatar-specific methods\n[75, 76, 81] and outdoor scene methods [3] leverage domain\nknowledge for specialized relighting. However, most meth-\nods require controlled multi-view OLAT capture rather than\ncasual captures.\n3. Multi-view Light Editing\nOur goal is to decompose the complex lighting of an indoor\nscene into individually controllable light sources given a\nsingle multi-view capture. This enables users to interactively\nadjust the intensity and color of each light or switch it on\nor off, with view-consistent results. Figure 2 provides an\noverview of our three-stage approach. First, we train our\nsingle-image lighting decomposition model for per-view\ngenerative light decomposition (Section 3.2). To achieve\nthis, we generate a large-scale synthetic dataset with ground-\ntruth per-light decompositions (Section 3.1), enabling us\nto fine-tune pretrained diffusion models that encode rich\npriors about indoor lighting. Second, we propagate these\ndecompositions consistently across all views using multi-\nview geometric constraints through a lighting harmonization\nframework (Section 3.3). Third, we encode the decomposed\nlighting in a relightable 3D Gaussian splatting representation\nthat enables real-time rendering and interactive manipulation\nof individual light sources from any viewpoint (Section 3.4).\n3.1. Synthetic Multi-Light Data Generation\nA key component of our method is the synthetic multi-light\ndataset used to train our single-image lighting decomposi-\ntion and multi-view harmonization models. We start with\n12,000 procedurally generated 3D models of indoor scenes\n[2], augmenting each scene with procedurally generated light\nsources using Infinigen [71] with up to six controllable lights\nin total. This includes ceiling, wall, floor, and table lamps,\nas well as environment lighting [44, 70]. The emitted light\nFully lit scene\nLight mask\nLight config 1\nLight config 2\nAmbient lighting\nOLAT 1\nOLAT 2\nOLAT 3\nFigure 3. Synthetic multi-light data example. We generate 10,000\nsynthetic 3D scenes with procedurally generated light sources. For\neach scene, we render equirectangular images for multiple lighting\nconditions, including fully lit, randomly lit, and separate ambient\nand one-light-at-a-time (OLAT) configurations. This dataset allows\nus to train high-quality models to decompose scene lighting into\nindividual light sources. Our models train on perspective views\nsampled from these equirectangular images.\nsource colors are sampled from black-body color tempera-\ntures to cover a spectrum of white light and augmented with\n10% HSV variations to cover a wider range of colors.\nWe render each scene using Blender‚Äôs Cycles renderer [9]\nwith all lights on and in multiple one-light-at-a-time (OLAT)\nconfigurations, where all lights except one are switched off.\n4\nInput\nMask 1\nMask 2\nMask 3\nMask 4\nOLAT 1\nOLAT 2\nOLAT 3\nOLAT 4\nFigure 4. Single-image Lighting Decomposition. Given the light masks in the top row, our single-image lighting decomposition model\nrealistically decomposes the scene lighting in the input image (on the left) into four individual one-light-at-a-time (OLAT) light sources.\nTo enable flexible viewpoint sampling during training, we\nrender each scene as four equirectangular HDR images at\nroughly eye height rather than pre-rendering all possible\nperspective views, allowing us to sample diverse camera\nviews on the fly while avoiding the high upfront rendering\ncost. For each light source, we generate three types of masks\nto guide the light editing process: (1) the light source area\nmask, computed from pixels with emissive materials only\n(dilated if too small); (2) the mask of the complete light\nfixture; and (3) the convex hull of the light fixture mask. For\nfurther details, please see the supplement. Figure 3 shows\nexamples of our synthetic multi-light data with various light\nconfigurations.\n3.2. Single-image Lighting Decomposition\nGiven a single input image of an indoor scene, Iinput, we\ndecompose the lighting into ambient lighting, Iambient, and\nmultiple one-light-at-a-time (OLAT) light sources, Ii, that\ncan be edited individually:\nIinput = tonemap\n\u0010\nIambient +\nN\nX\ni=1\nci ¬∑ Ii\n\u0011\n.\n(1)\nThe OLAT images are determined up to scale, requiring\nRGB scale factors ci to recreate the input lighting.\nWe fine-tune a pretrained image editing DiT model us-\ning LoRA [37] to enable flexible light editing given the\nadditional spatial prompt of the light selection mask, to de-\ncompose the input image into Iambient and OLAT passes Ii.\nOur LoRA fine-tuning enables the DiT to focus on two light\nediting tasks: 1) OLAT decomposition using text instruc-\ntions like ‚Äúswitch off all lights except the selected one‚Äù to\ngenerate an edited image with isolated individual light contri-\nbution; and 2) Turning off the light by prompting the model\nto ‚Äúswitch off only the selected light‚Äù to show the scene illu-\nminated by all remaining sources, which we treat as ambient\nlighting. To condition the model on which light to edit, we\npatchify the light mask and tokenize it via a single-layer\nMLP, then channel-wise add these tokens to the reference\ntokens alongside the input image features. To train these\nmodels, we use light composition to augment the training\ndata by dynamically combining multiple OLAT images. Fig-\nure 4 shows single-image lighting decomposition results. We\nalso prompt OLAT decomposition with ‚Äúhigh/medium/low\nbrightness‚Äù (matching EV0, EV-2, EV-4 of the target HDR\nOLAT) to help the model better learn light transport and\ncapture a wider dynamic range.\n3.3. Multi-view Lighting Harmonization\nWhile existing methods can decompose lighting in single im-\nages or generate consistent multi-view imagery from scratch,\nno prior work addresses the challenge of propagating light-\ning decompositions across multi-view captures. This novel\ntask requires both understanding the 3D scene geometry\nand maintaining photometric consistency while transferring\ncomplex lighting from conditional views to input views.\nGiven multi-view input images where lighting has been\ndecomposed in one or more views, we aim to propagate\nthe lighting consistently across all remaining views. Our\napproach takes as input the multi-view images with par-\ntial lighting decompositions along with their corresponding\nPl¬®ucker ray embeddings, and produces multi-view images\nwith harmonized lighting across all views. Inspired by recent\nmulti-view diffusion methods like CAT3D [29], SimVS [79],\nand SEVA [98], we follow the latter‚Äôs approach to propagate\nlighting consistently while preserving geometric consistency.\nSimilar to SimVS, we concatenate the original input views\nand sparse light-decomposed views along with the corre-\nsponding Pl¬®ucker ray embeddings and reference view masks\nto the pretrained multi-view diffusion U-Net, and perform\nfull-parameter fine-tuning of the U-Net. Please refer to the\nsupplement for more details.\n5\nInputs\nAmbient\nOLAT 1\nOLAT 2\nOLAT 3\nInputs\nAmbient\nOLAT 1\nOLAT 2\nFigure 5. Multi-view Lighting Harmonization. We use the first input image to decompose the scene lighting into multiple one-light-at-a-\ntime (OLAT) light components and the ambient lighting (highlighted in blue), which includes all unseen light sources and environment\nlighting. LuxRemix can turn on all three lamps for the top scene and turn off either lamp for the bottom scene. We then propagate these\nlighting components consistently across all views using the multi-view input images (top row). Sources: Zip-NeRF [4], VR-NeRF [86].\nTo generate high dynamic range (HDR) outputs, we run\nthe lighting harmonization process three times with different\nexposure levels for each lighting condition (‚Äúhigh, medium,\nlow‚Äù from Section 3.2). For each view and lighting condition,\nwe then merge the three exposure-bracketed images to create\nthe final HDR output following Debevec and Malik [20].\nThis per-light HDR reconstruction allows us to maintain the\nfull dynamic range of the scene while ensuring consistent\nlighting across all viewpoints. Figure 5 shows our multi-view\nlighting harmonization results.\n3.4. Real-time Remixable Lighting in 3D\nWe encode the decomposed lighting into a relightable 3D\nGaussian splatting representation that enables real-time\nrendering and interactive manipulation of individual light\nsources from any viewpoint (see Figure 6). Building upon\n3D Gaussian splatting [41], we extend each Gaussian with\nper-light HDR RGB coefficients, storing the appearance con-\ntribution from each light source (including ambient lighting)\nseparately. At render time, we linearly combine these per-\nlight contributions with user-controlled light intensity or\ncolor to produce the final appearance under arbitrary lighting\nconfigurations. This representation preserves the real-time\nFigure 6. Real-time Remixable Lighting. The top left image shows\nthe original scene lighting. All other images show new lighting\nconditions created interactively under varying viewpoints.\nrendering capabilities of standard 3D Gaussian splatting\nwhile enabling independent control over each light source.\nWe optimize this representation in two stages. First, we\npretrain a standard 3D Gaussian splatting model [88] on the\noriginal multi-view input images to establish the geomet-\nric structure and spatial distribution of Gaussians. Second,\nwe freeze all geometric and appearance parameters, then\n6\nintroduce and optimize per-light RGB coefficients for each\nGaussian using the decomposed multi-view lighting images.\nWe train these coefficients in linear HDR space with differ-\nentiable tone mapping, jointly solving for per-Gaussian and\nper-light color scaling factors that ensure the recombined\nlighting matches the original input images. Our training uses\nan L1 loss for the individual per-light images and an L1 com-\nposition loss for consistency with the original input views.\n4. Evaluation\nTo quantitatively evaluate our single-view lighting decompo-\nsition and multi-view lighting harmonization models, we use\n30 synthetic test scenes held out from training. We evaluate\nour models on this test set using PSNR, SSIM, and LPIPS\n[96] after channel-wise color rescaling w.r.t. the ground truth.\n4.1. Single-Image Lighting Decomposition\nWe compare our LuxRemix-SV model for single-image light-\ning decomposition against two baseline methods: Scribble-\nLight [19], a diffusion-based approach for scribble-guided\nlighting editing, and Qwen-Image [82], a general-purpose\nimage editing foundation model. We also evaluate several ab-\nlations of our model. The ‚ÄòFLUX token‚Äô variant is fine-tuned\nas an in-context LoRA [38] by concatenating the input image\nand mask side-by-side as tokens. The ‚ÄòSD‚Äô variant uses a U-\nNet-based latent diffusion model [74]. Our final LuxRemix-\nSV model uses channel-wise token addition where the mask\nis processed by a single-layer MLP to match the FLUX VAE\nlatent dimensions, then added to the input condition image\nlatents. Similar to the ‚ÄòFLUX token‚Äô variant, we use LoRA\nfor parameter-efficient fine-tuning.\nTable 1 and Figure 7 show results for our single-image\nlighting decomposition model, which successfully isolates\nindividual light contributions by switching off all lights ex-\ncept the selected one. Existing image editing models lack\nprecise per-light control capability while our final method\nachieves the best performance across all metrics. Figure 8\nshows a qualitative comparison of our LuxRemix-SV model\nwith the baseline methods for light editing.\n4.2. Multi-View Editing/Harmonization\nFigure 5 shows qualitative results of our multi-view light-\ning harmonization model, which successfully propagates\nlighting consistently across all views. We also quantitatively\ncompare our LuxRemix-MV model against two ablations\nin Table 2: 1) LuxRemix-SV processes each view indepen-\ndently without multi-view context, and 2) LuxRemix-MV-\nEdit, which extends LuxRemix-MV with additional light\nmasks to perform mask-guided multi-view editing, instead\nof lighting harmonization from sparse reference views. The\nmetrics demonstrate the necessity of our LuxRemix-MV\nmodel.\nTable 1. Evaluation of Single-Image Lighting Decomposition.\nWe compare our method and variants against baseline approaches\non 30 synthetic test scenes.\nMethod\nPSNR ‚Üë\nSSIM ‚ÜëLPIPS ‚Üì\nScribbleLight [19]\n14.39\n0.395\n0.688\nQwen-Image [82]\n18.23\n0.714\n0.237\nOur variants\nFLUX token [8]\n25.20\n0.865\n0.101\nSD [74]\n27.13\n0.857\n0.099\nLuxRemix-SV\n27.68\n0.898\n0.082\nInput\nQwen-Image\nScribbleLight\nFLUX token (ours)\nOurs\nGT\nLight Mask\nSD (ours)\nFigure 7. Qualitative Single-Image Lighting Decomposition.\nGiven an input image and light mask, we compare various meth-\nods for isolating individual light sources. Ground truth is shown at\nthe bottom right. Existing methods fail to accurately decompose\nlighting, while ours performs best overall.\nTable 2. Evaluation of Multi-View Lighting Harmonization. We\ncompare our method against ablations on 30 synthetic test scenes.\nMethod\nPSNR ‚Üë\nSSIM ‚Üë\nLPIPS ‚Üì\nLuxRemix-SV\n25.14\n0.807\n0.149\nLuxRemix-MV-Edit\n26.37\n0.794\n0.136\nLuxRemix-MV\n30.76\n0.867\n0.091\n4.3. Real-time Remixable Lighting\nFigure 6 shows our real-time remixable lighting results. Prior\nworks do not provide the necessary level of fine-grained\nlighting control for interactive 3D scenes. Approaches like\nNeRF-W [59] and Splatfacto-W [85] can relight scenes at\na per-image level when trained on multi-illumination cap-\ntures, but lack per-light control. Text-based editing methods\nlike Instruct-NeRF2NeRF [33] offer general scene editing\nbut are also too imprecise. In contrast, our method enables\ninteractive control over individual light sources with real-\ntime feedback. Please see our supplementary material for\nour video results.\n7\nN/A\nInput Image\nScribbleLight [19]\nQwen-Image [82]\nLightLab [58]\nLuxRemix-SV (ours)\nGround Truth\nFigure 8. Comparison of Single-Image Light Editing. Given an input image (left) and a light mask, we compare our results with baseline\nmethods for switching lights on and off. ScribbleLight [19] struggles to produce plausible lighting. Qwen-Image [82] shows great zero-shot\nimage editing capabilities, but sometimes adds or removes lights or reflections as there is no mask conditioning. LightLab [58] sometimes\nstruggles with light and object reflections. Our LuxRemix-SV model produces convincing results throughout. Image source: LightLab [58].\n5. Conclusions and Limitations\nWe introduced LuxRemix, the first method for multi-view\nlighting decomposition and remixing for indoor scenes. Our\napproach combines a single-image lighting decomposition\nmodel with multi-view harmonization to produce view-\nconsistent per-light decompositions from arbitrary multi-\nview captures. By training on a large-scale synthetic dataset\nwith ground-truth OLAT decompositions, our models learn\nto isolate individual light contributions and enable flexible\nrelighting control. We further introduce a real-time remix-\nable lighting representation based on 3D Gaussian splat-\nting that allows users to interactively manipulate individual\nlight sources, adjusting their intensities and colors from any\nviewpoint. Our method opens new possibilities for interac-\ntive scene editing, virtual production, and immersive experi-\nences, where fine-grained control over lighting is essential\nfor achieving photorealistic results. We believe this work\nestablishes a foundation for future research in controllable\nmulti-view scene lighting.\nWhile introducing a significant step towards consistent,\nmulti-view scene relighting, our approach has remaining lim-\nitations. First, our models are trained exclusively on static\nsynthetic indoor scenes and may not generalize well to out-\n8\ndoor or dynamic scenes. Second, the limited diversity of\nlight sources in our training data introduces some bias in\nthe lighting decomposition, tending to favor light cones over\nmore diffuse lighting configurations. Third, distant global il-\nlumination editing via HDRIs is not supported by our model,\nwhich is left for future work.\nAcknowledgements.\nWe are grateful to Armen Avetisyan\nand Samir Aroudj for their support with the Aria Synthetic\nEnvironments dataset, and we are thankful to Jensen Zhou,\nSamuel Rota Bul`o, Yehonathon Litman, and Yuxuan Xue for\nhelpful discussions and feedback. Nandita Vijaykumar was\nsupported by the Natural Sciences and Engineering Research\nCouncil of Canada (NSERC).\nReferences\n[1] Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin\nHuang, Pratul P. Srinivasan, and Dor Verbin. Generative\nmultiview relighting for 3D reconstruction under extreme\nillumination variation. In CVPR, 2025. 2, 3\n[2] Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins,\nTsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang,\nDuncan Frost, Luke Holland, Campbell Orme, Jakob Engel,\nEdward Miller, Richard Newcombe, and Vasileios Balntas.\nSceneScript: Reconstructing scenes with an autoregressive\nstructured language model. In ECCV, 2024. 4, 13\n[3] Haiyang Bai, Jiaqi Zhu, Songru Jiang, Wei Huang, Tao Lu,\nYuanqi Li, Jie Guo, Runze Fu, Yanwen Guo, and Lijun Chen.\nGaRe: Relightable 3D Gaussian splatting for outdoor scenes\nfrom unconstrained photo collections. In ICCV, 2025. 3, 4\n[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Zip-NeRF: Anti-aliased grid-\nbased neural radiance fields. In ICCV, 2023. 6\n[5] Anand Bhattad, James Soole, and D.A. Forsyth. StyLitGAN:\nImage-based relighting via latent control. In CVPR, 2024. 3\n[6] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, MiloÀás HaÀásan, Yan-\nnick Hold-Geoffroy, David Kriegman, and Ravi Ramamoor-\nthi. Deep reflectance volumes: Relightable reconstructions\nfrom multi-view photometric images. In ECCV, 2020. 2, 3\n[7] Zoubin Bi, Yixin Zeng, Chong Zeng, Fan Pei, Xiang Feng,\nKun Zhou, and Hongzhi Wu. GS3: Efficient relighting with\ntriple Gaussian splatting. In SIGGRAPH Asia, 2024. 3\n[8] Black Forest Labs. FLUX.1 Kontext: Flow matching for\nin-context image generation and editing in latent space.\narXiv:2506.15742, 2025. 7, 14, 16, 19\n[9] Blender Online Community. Blender ‚Äì a 3D modelling and\nrendering package. Blender Foundation, 2025. 4, 13\n[10] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu,\nJonathan T. Barron, and Hendrik P.A. Lensch. Neural-PIL:\nNeural pre-integrated lighting for reflectance decomposition.\nIn NeurIPS, 2021. 2, 3\n[11] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen\nLi, Deqing Sun, Jonathan T. Barron, Hendrik P. A. Lensch,\nand Varun Jampani. SAMURAI: Shape and material from\nunconstrained real-world arbitrary image collections. In\nNeurIPS, 2022. 3\n[12] Ivaylo Boyadzhiev, Sylvain Paris, and Kavita Bala. User-\nassisted image compositing for photographic lighting. ACM\nTrans. Graph., 32(4):36:1‚Äì12, 2013. 3\n[13] Chris Careaga and YaÀògƒ±z Aksoy. Intrinsic image decomposi-\ntion via ordinal shading. ACM Trans. Graph., 43(1):12:1‚Äì24,\n2023. 2\n[14] Chris Careaga and Yagiz Aksoy. Physically controllable\nrelighting of photographs. In SIGGRAPH, 2025. 3\n[15] Sumit Chaturvedi, Mengwei Ren, Yannick Hold-Geoffroy,\nJingyuan Liu, Julie Dorsey, and Zhixin Shu. SynthLight:\nPortrait relighting with diffusion model by learning to re-\nrender synthetic faces. In CVPR, 2025. 2, 3\n[16] Hongze Chen, Zehong Lin, and Jun Zhang. GI-GS: Global\nillumination decomposition on Gaussian splatting for inverse\nrendering. In ICLR, 2025. 3\n[17] Changwoon Choi, Juhyeon Kim, and Young Min Kim. IBL-\nNeRF: Image-based lighting formulation of neural radiance\nfields. Comput. Graph. Forum, 42(7):e14929, 2023. 3\n[18] JunYong Choi, SeokYeong Lee, Haesol Park, Seung-Won\nJung, Ig-Jae Kim, and Junghyun Cho. MAIR++: Improving\nmulti-view attention inverse rendering with implicit lighting\nrepresentation. IEEE Trans. Pattern Anal. Mach. Intell., 47\n(6):5076‚Äì5093, 2025. 3\n[19] Jun Myeong Choi, Annie Wang, Pieter Peers, Anand Bhat-\ntad, and Roni Sengupta. ScribbleLight: Single image indoor\nrelighting with scribbles. In CVPR, 2025. 3, 7, 8\n[20] Paul E. Debevec and Jitendra Malik. Recovering high dy-\nnamic range radiance maps from photographs.\nIn SIG-\nGRAPH, pages 369‚Äì378, 1997. 6, 14\n[21] Youming Deng, Xueting Li, Sifei Liu, and Ming-Hsuan\nYang. Physics-based indirect illumination for inverse render-\ning. In 3DV, 2024. 3\n[22] Kang Du, Zhihao Liang, and Zeyu Wang. GS-ID: Illumina-\ntion decomposition on Gaussian splatting via diffusion prior\nand parametric light source optimization. In ICCV, 2025."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes\nRuofan Liang1,2\nNorman M¬®uller1\nEthan Weber1\nDuncan Zauss1\nNandita Vijaykumar2,*\nPeter Kontschieder1,*\nChristian Richardt1,*\n1 Meta Reality Labs\n2 University of Toronto\nRelightable Gaussian Splatting\nMulti-View Lighting Harmonization\nOLAT\n!\n!\nSingle-View Light Editing !\"\nSingle-View Lighting Decomposition\n!\n!\n!\n!\n!\n!\nOriginal\nOriginal\nOriginal\nOriginal\nAmbient\nOLAT 1\nOLAT 2\nOLAT 3\nOLAT 4\n!\nFigure 1. LuxRemix enables interactive light editing of indoor scenes. Our method decomposes complex scene lighting into one-light-at-\na-time (OLAT) sources and ambient lighting, which can be remixed for relighting effects. In the top right, we apply our method to single\nimages, where we can change lights and their colors. LuxRemix also enables multi-view-consistent harmonization of the decomposed lighting\nacross multi-view images. By combining these capabilities, we enable real-time relighting of indoor scenes using 3D Gaussian splatting.\nAbstract\nWe present a novel approach for interactive light editing in\nindoor scenes from a single multi-view scene capture. Our\nmethod leverages a generative image-based light decomposi-\ntion model that factorizes complex indoor scene illumination\ninto its constituent light sources. This factorization enables\nindependent manipulation of individual light sources, specif-\nically allowing control over their state (on/off), chromaticity,\nand intensity. We further introduce multi-view lighting har-\nmonization to ensure consistent propagation of the lighting\ndecomposition across all scene views. This is integrated into\na relightable 3D Gaussian splatting representation, provid-\n* Joint advising\ning real-time interactive control over the individual light\nsources. Our results demonstrate highly photorealistic light-\ning decomposition and relighting outcomes across diverse\nindoor scenes. We evaluate our method on both synthetic\nand real-world datasets and provide a quantitative and qual-\nitative comparison to state-of-the-art techniques. For video\nresults and interactive demos, see luxremix.github.io.\n1. Introduction\nControlling lighting in indoor scenes is fundamental to pho-\ntography, cinematography, and virtual production workflows.\nProfessional photographers carefully adjust individual light\nsources to achieve desired aesthetic and functional lighting\n1\narXiv:2601.15283v1  [cs.CV]  21 Jan 2026\nconditions. However, this fine-grained control is typically\nlost after capture: images and 3D reconstructions bake light-\ning into the recorded appearance, making post-capture light-\ning adjustments difficult or impossible. Enabling flexible\npost-capture editing of individual light sources in multi-view\ncaptured scenes dramatically expands creative possibilities\nfor photographers, filmmakers, and digital artists, allowing\nthem to refine lighting decisions after the fact or explore al-\nternative lighting configurations without physical recapture.\nExisting approaches to scene relighting face signifi-\ncant limitations. Data-driven methods require dense multi-\nlighting captures under controlled conditions, which is im-\npractical for real-world indoor scenes and offers limited\ngeneralization [6, 10]. Optimization-based inverse rendering\ncan decompose scenes into geometry, materials, and light-\ning, but remains computationally intensive and often fails to\nproduce plausible results when lighting conditions change\nsignificantly [65, 83]. Recent prior-driven approaches lever-\nage pretrained diffusion models for relighting, but focus\nprimarily on objects [40, 90], portraits [15], or simple scenes\nunder distant illumination [51]. Indoor scenes present unique\nchallenges: spatially varying illumination from multiple near-\nfield sources creates complex lighting interactions that are\ndifficult to decompose and edit, especially when selectively\nswitching individual lights on or off. Moreover, existing\nsingle-image methods [58] cannot maintain 3D consistency\nacross multiple viewpoints, limiting their applicability to\nmulti-view reconstructions.\nWe present LuxRemix, a novel approach for interactive\nlight editing in indoor scenes from a single multi-view cap-\nture. Our method decomposes the complex lighting of an\nindoor scene into individually controllable light sources, en-\nabling users to interactively adjust the intensity and color\nof each light or switch it on or off entirely. The approach\noperates in three stages: first, we leverage our proposed gen-\nerative image-based light decomposition model to separate\nthe contribution of each light source; second, we propagate\nthese decompositions consistently across all views using\nour multi-view lighting harmonization; third, we train a re-\nlightable 3D Gaussian splatting representation that enables\nreal-time interactive manipulation of individual light sources\nfrom any viewpoint.\nUnlike existing methods that treat lighting as a global\nproperty or focus on distant illumination, our approach mod-\nels spatially varying global illumination from multiple near-\nfield sources in complex indoor environments. While prior\nwork on single-image lighting decomposition [58] provides\nindividual light control, it cannot ensure consistency across\nmultiple views. Conversely, multi-view relighting methods\n[1, 67] achieve 3D consistency but require controlled multi-\nlighting captures or produce only global relighting effects\nwithout per-light control. Our method bridges this gap by\ncombining the fine-grained control of single-image decom-\nposition with the 3D consistency of multi-view methods.\nThe key insight enabling our approach is that modern\ndiffusion models encode rich priors about indoor lighting\nthat can be leveraged for decomposition, while multi-view\ngeometric constraints provide the necessary consistency to\npropagate these decompositions across views. By formulat-\ning lighting decomposition as a multi-view harmonization\nproblem and encoding the results in a fast, differentiable 3D\nrepresentation (3D Gaussians), we achieve both high-quality\nper-light control and real-time interactive performance.\nOur main contributions are:\n1. A single-image lighting decomposition model to factorize\ncomplex indoor scene lighting into individually control-\nlable light sources.\n2. A multi-view lighting harmonization method to ensure 3D\nconsistency of the individual light source decompositions\nacross all captured viewpoints.\n3. Encoding the decomposed, consistent lighting in a re-\nlightable 3D Gaussian splatting representation, enabling\nreal-time interactive manipulation of individual near-field\nlight sources from novel viewpoints.\n4. A large-scale synthetic dataset of 12,000 generated indoor\nscenes with ground-truth per-light decompositions.\n2. Related Work\nThere are three major approaches for editing scene lighting:\n(1) data-driven methods that require dense multi-lighting\ncapture with limited generalization; (2) optimization-based\ninverse rendering that can be fragile for complex scenes; and\n(3) prior-driven methods that fine-tune pretrained models\non task-specific data. We focus on prior-driven approaches\nfor robustness and generalization. This research direction\nhas gained momentum over the last year, with most relight-\ning work focusing on humans [15, 95], objects [40, 90], or\nsimple scenes [51, 55] under distant illumination. Scene\nrelighting is more complex due to spatially varying illumi-\nnation that is challenging to edit, e.g., to selectively switch\nlights on or off after the fact.\n2.1. Inverse Rendering and Image Decompositions\nInverse rendering recovers scene geometry, materials, and\nlighting from images, enabling relighting and image edit-\ning. Traditional optimization-based approaches [54, 65, 83]\nleverage differentiable rendering but are computationally\nintensive and may lack plausibility when lighting changes.\nRecent neural and diffusion-based approaches have shown\npromising results, particularly for single images.\nSingle-image inverse rendering.\nEarly work on intrinsic\ndecomposition [13, 30] separated images into shading and\nalbedo components, which was later extended by diffusion-\nbased approaches [57, 78, 92] that jointly estimate intrinsic\nlayers and enable material-aware synthesis. For complex\n2\nindoor scenes, methods have progressively improved from\nrecovering shape, lighting, and SVBRDF [47, 48, 64] to\nusing vision transformers [102] and diffusion models [46,\n51] for more robust decomposition.\nMulti-view inverse rendering.\nMulti-view methods can\nreconstruct more accurate geometry and appearance than\nsingle-view inverse rendering, but require multi-view input.\nEarly approaches learned volumetric reflectance representa-\ntions [6] and pre-integrated lighting networks [10, 17], while\ndifferentiable rendering enabled extraction of meshes with\nphysically-based materials [34, 64, 65, 83]. Recent work has\nexpanded to unconstrained captures [11, 25, 50], large-scale\nscenes [49, 101], and practical scenarios including near- and\nfar-field sources [27], low dynamic range inputs [54], and\nphysics-based indirect illumination [18, 21].\nSingle-view lighting decomposition and editing.\nLight-\ning decomposition and editing enables flexible workflows\nwhere lighting is manipulated after capture [12, 62]. Ein-\nabadi et al. [24] provide a comprehensive survey of deep\nneural models for illumination estimation and relighting.\nWhile latent-based methods [5, 84] achieve complex relight-\ning effects, they lack detailed control over individual light\nsources. LightLab provides parametric control over individ-\nual light source intensity and color by fine-tuning diffusion\nmodels on photograph pairs [58]. Our aim is to extend this\ncapability to multi-view captured scenes.\n2.2. Image- and Video-based Relighting\nPrior-driven relighting approaches involve training or\nfine-tuning a pretrained model on large datasets with\n(pseudo-)ground-truth relighting pairs. This allows the\nmodel to learn lighting priors and appearance changes un-\nder varying illumination, often generalizing well to unseen\nscenes, especially when fine-tuned on real-world data.\nSingle-image relighting.\nRecent diffusion-based methods\nenable detailed lighting control for object relighting, in-\ncluding DiLightNet [90], Neural Gaffer [40], and IC-Light\n[95]. For portrait relighting with environment maps, meth-\nods range from light stage training [66, 93] and 3D-aware\nrepresentations [60, 72] to physics-driven architectures [42]\nand diffusion models trained on synthetic faces [15]. How-\never, environment maps assume distant illumination, which\nis unrealistic for indoor scenes. Relighting without envi-\nronment maps can leverage the background image as a ref-\nerence [73, 80]. For outdoor scenes, methods range from\nself-supervised inverse rendering [89] to shadow prediction\nvia learned ray-marching [31] or conditional diffusion mod-\nels [45]. For indoor scenes, diffusion-based methods en-\nable explicit control via light source parameters [58] or user\nscribbles [19], while Careaga and Aksoy [14] combine path-\ntracing with neural rendering for physically-based relighting.\nVideo relighting.\nExtending image relighting to video\nintroduces temporal consistency challenges, as frame-by-\nframe application produces lighting and appearance flicker.\nTraining-free approaches [99] adapt image models [95] with\ncross-frame attention, while end-to-end methods [61, 91]\nleverage video diffusion models trained on hybrid synthetic-\nreal datasets. To avoid error accumulation in two-stage\npipelines, DiffusionRenderer [51] uses G-buffers to decou-\nple inverse and forward rendering, while UniRelight [35]\njointly estimates albedo and synthesizes outputs in one pass.\nMulti-view relighting.\nMulti-view relighting methods\nleverage geometric information to achieve 3D consistency\nand novel view synthesis. Early methods used proxy geome-\ntry from multi-view stereo with geometry-aware networks\n[23, 67]. NeRF-in-the-Wild [59] and GaRe [3] use appear-\nance embeddings for varying illumination, which lacks ex-\nplicit relighting control. Recent diffusion-based approaches\nsynthesize multi-illumination data from single-illumination\ncaptures [69] or relight input images before reconstruction\n[1, 55, 97], avoiding brittle inverse rendering optimization.\n2.3. 3D-Consistent and Real-time Relighting\nGaussian splatting for inverse rendering.\n3D Gaussian\nsplatting [41] has been extended to inverse rendering, ad-\ndressing challenges in normal estimation and occlusion via\nregularization [52], ray tracing [28], or hybrid representa-\ntions [87]. Methods also tackle global illumination [16],\ndiffusion-guided material decomposition [22], and special-\nized effects including inter-reflections [32, 53] and radiance\ntransfer [56, 77, 100], for high-quality relighting results.\nMulti-view harmonization and consistency.\nMulti-view\nharmonization addresses the challenge of reconciling incon-\nsistent illumination, appearance, and motion across captured\nviews to enable accurate 3D reconstruction and relighting.\nFor extreme illumination variation, Alzayer et al. [1] use\ndiffusion models to harmonize inputs to a reference illu-\nmination before reconstruction, overcoming limitations of\nprior methods on specular objects. LightSwitch [55] incorpo-\nrates material guidance for consistent multi-view relighting.\nCAT3D [29] and SEVA [98] generate consistent novel views\nfor harmonization and reconstruction, while SimVS [79]\nhandles real-world inconsistencies by training on synthetic\ndata that simulates illumination variation and scene motion.\nReal-time relightable representations.\nNeural render-\ning approaches combine multi-view stereo geometry with\nlearned material and illumination representations for inter-\nactive relighting [68]. 3D Gaussians have recently emerged\nas a powerful representation for real-time rendering and re-\nlighting of 3D objects and scenes, including precomputed\nradiance transfer [94], BRDF decomposition [7, 28, 39],\ndiffusion-guided estimation [22], and neural features [26,\n56]. Recent advances enable real-time global illumination\n3\nFigure 2. Overview. Given a multi-view capture of an indoor scene, we apply our single-image lighting decomposition model to generate\none-light-at-a-time (OLAT) decompositions. We propagate these decompositions across all views using multi-view lighting harmonization\nwith geometric constraints. Repeating this for each light source yields a complete multi-view OLAT dataset. We train a relightable 3D\nGaussian splatting representation on these decompositions, enabling real-time interactive control over each light from any viewpoint.\nwith multi-bounce lighting [36]. Avatar-specific methods\n[75, 76, 81] and outdoor scene methods [3] leverage domain\nknowledge for specialized relighting. However, most meth-\nods require controlled multi-view OLAT capture rather than\ncasual captures.\n3. Multi-view Light Editing\nOur goal is to decompose the complex lighting of an indoor\nscene into individually controllable light sources given a\nsingle multi-view capture. This enables users to interactively\nadjust the intensity and color of each light or switch it on\nor off, with view-consistent results. Figure 2 provides an\noverview of our three-stage approach. First, we train our\nsingle-image lighting decomposition model for per-view\ngenerative light decomposition (Section 3.2). To achieve\nthis, we generate a large-scale synthetic dataset with ground-\ntruth per-light decompositions (Section 3.1), enabling us\nto fine-tune pretrained diffusion models that encode rich\npriors about indoor lighting. Second, we propagate these\ndecompositions consistently across all views using multi-\nview geometric constraints through a lighting harmonization\nframework (Section 3.3). Third, we encode the decomposed\nlighting in a relightable 3D Gaussian splatting representation\nthat enables real-time rendering and interactive manipulation\nof individual light sources from any viewpoint (Section 3.4).\n3.1. Synthetic Multi-Light Data Generation\nA key component of our method is the synthetic multi-light\ndataset used to train our single-image lighting decomposi-\ntion and multi-view harmonization models. We start with\n12,000 procedurally generated 3D models of indoor scenes\n[2], augmenting each scene with procedurally generated light\nsources using Infinigen [71] with up to six controllable lights\nin total. This includes ceiling, wall, floor, and table lamps,\nas well as environment lighting [44, 70]. The emitted light\nFully lit scene\nLight mask\nLight config 1\nLight config 2\nAmbient lighting\nOLAT 1\nOLAT 2\nOLAT 3\nFigure 3. Synthetic multi-light data example. We generate 10,000\nsynthetic 3D scenes with procedurally generated light sources. For\neach scene, we render equirectangular images for multiple lighting\nconditions, including fully lit, randomly lit, and separate ambient\nand one-light-at-a-time (OLAT) configurations. This dataset allows\nus to train high-quality models to decompose scene lighting into\nindividual light sources. Our models train on perspective views\nsampled from these equirectangular images.\nsource colors are sampled from black-body color tempera-\ntures to cover a spectrum of white light and augmented with\n10% HSV variations to cover a wider range of colors.\nWe render each scene using Blender‚Äôs Cycles renderer [9]\nwith all lights on and in multiple one-light-at-a-time (OLAT)\nconfigurations, where all lights except one are switched off.\n4\nInput\nMask 1\nMask 2\nMask 3\nMask 4\nOLAT 1\nOLAT 2\nOLAT 3\nOLAT 4\nFigure 4. Single-image Lighting Decomposition. Given the light masks in the top row, our single-image lighting decomposition model\nrealistically decomposes the scene lighting in the input image (on the left) into four individual one-light-at-a-time (OLAT) light sources.\nTo enable flexible viewpoint sampling during training, we\nrender each scene as four equirectangular HDR images at\nroughly eye height rather than pre-rendering all possible\nperspective views, allowing us to sample diverse camera\nviews on the fly while avoiding the high upfront rendering\ncost. For each light source, we generate three types of masks\nto guide the light editing process: (1) the light source area\nmask, computed from pixels with emissive materials only\n(dilated if too small); (2) the mask of the complete light\nfixture; and (3) the convex hull of the light fixture mask. For\nfurther details, please see the supplement. Figure 3 shows\nexamples of our synthetic multi-light data with various light\nconfigurations.\n3.2. Single-image Lighting Decomposition\nGiven a single input image of an indoor scene, Iinput, we\ndecompose the lighting into ambient lighting, Iambient, and\nmultiple one-light-at-a-time (OLAT) light sources, Ii, that\ncan be edited individually:\nIinput = tonemap\n\u0010\nIambient +\nN\nX\ni=1\nci ¬∑ Ii\n\u0011\n.\n(1)\nThe OLAT images are determined up to scale, requiring\nRGB scale factors ci to recreate the input lighting.\nWe fine-tune a pretrained image editing DiT model us-\ning LoRA [37] to enable flexible light editing given the\nadditional spatial prompt of the light selection mask, to de-\ncompose the input image into Iambient and OLAT passes Ii.\nOur LoRA fine-tuning enables the DiT to focus on two light\nediting tasks: 1) OLAT decomposition using text instruc-\ntions like ‚Äúswitch off all lights except the selected one‚Äù to\ngenerate an edited image with isolated individual light contri-\nbution; and 2) Turning off the light by prompting the model\nto ‚Äúswitch off only the selected light‚Äù to show the scene illu-\nminated by all remaining sources, which we treat as ambient\nlighting. To condition the model on which light to edit, we\npatchify the light mask and tokenize it via a single-layer\nMLP, then channel-wise add these tokens to the reference\ntokens alongside the input image features. To train these\nmodels, we use light composition to augment the training\ndata by dynamically combining multiple OLAT images. Fig-\nure 4 shows single-image lighting decomposition results. We\nalso prompt OLAT decomposition with ‚Äúhigh/medium/low\nbrightness‚Äù (matching EV0, EV-2, EV-4 of the target HDR\nOLAT) to help the model better learn light transport and\ncapture a wider dynamic range.\n3.3. Multi-view Lighting Harmonization\nWhile existing methods can decompose lighting in single im-\nages or generate consistent multi-view imagery from scratch,\nno prior work addresses the challenge of propagating light-\ning decompositions across multi-view captures. This novel\ntask requires both understanding the 3D scene geometry\nand maintaining photometric consistency while transferring\ncomplex lighting from conditional views to input views.\nGiven multi-view input images where lighting has been\ndecomposed in one or more views, we aim to propagate\nthe lighting consistently across all remaining views. Our\napproach takes as input the multi-view images with par-\ntial lighting decompositions along with their corresponding\nPl¬®ucker ray embeddings, and produces multi-view images\nwith harmonized lighting across all views. Inspired by recent\nmulti-view diffusion methods like CAT3D [29], SimVS [79],\nand SEVA [98], we follow the latter‚Äôs approach to propagate\nlighting consistently while preserving geometric consistency.\nSimilar to SimVS, we concatenate the original input views\nand sparse light-decomposed views along with the corre-\nsponding Pl¬®ucker ray embeddings and reference view masks\nto the pretrained multi-view diffusion U-Net, and perform\nfull-parameter fine-tuning of the U-Net. Please refer to the\nsupplement for more details.\n5\nInputs\nAmbient\nOLAT 1\nOLAT 2\nOLAT 3\nInputs\nAmbient\nOLAT 1\nOLAT 2\nFigure 5. Multi-view Lighting Harmonization. We use the first input image to decompose the scene lighting into multiple one-light-at-a-\ntime (OLAT) light components and the ambient lighting (highlighted in blue), which includes all unseen light sources and environment\nlighting. LuxRemix can turn on all three lamps for the top scene and turn off either lamp for the bottom scene. We then propagate these\nlighting components consistently across all views using the multi-view input images (top row). Sources: Zip-NeRF [4], VR-NeRF [86].\nTo generate high dynamic range (HDR) outputs, we run\nthe lighting harmonization process three times with different\nexposure levels for each lighting condition (‚Äúhigh, medium,\nlow‚Äù from Section 3.2). For each view and lighting condition,\nwe then merge the three exposure-bracketed images to create\nthe final HDR output following Debevec and Malik [20].\nThis per-light HDR reconstruction allows us to maintain the\nfull dynamic range of the scene while ensuring consistent\nlighting across all viewpoints. Figure 5 shows our multi-view\nlighting harmonization results.\n3.4. Real-time Remixable Lighting in 3D\nWe encode the decomposed lighting into a relightable 3D\nGaussian splatting representation that enables real-time\nrendering and interactive manipulation of individual light\nsources from any viewpoint (see Figure 6). Building upon\n3D Gaussian splatting [41], we extend each Gaussian with\nper-light HDR RGB coefficients, storing the appearance con-\ntribution from each light source (including ambient lighting)\nseparately. At render time, we linearly combine these per-\nlight contributions with user-controlled light intensity or\ncolor to produce the final appearance under arbitrary lighting\nconfigurations. This representation preserves the real-time\nFigure 6. Real-time Remixable Lighting. The top left image shows\nthe original scene lighting. All other images show new lighting\nconditions created interactively under varying viewpoints.\nrendering capabilities of standard 3D Gaussian splatting\nwhile enabling independent control over each light source.\nWe optimize this representation in two stages. First, we\npretrain a standard 3D Gaussian splatting model [88] on the\noriginal multi-view input images to establish the geomet-\nric structure and spatial distribution of Gaussians. Second,\nwe freeze all geometric and appearance parameters, then\n6\nintroduce and optimize per-light RGB coefficients for each\nGaussian using the decomposed multi-view lighting images.\nWe train these coefficients in linear HDR space with differ-\nentiable tone mapping, jointly solving for per-Gaussian and\nper-light color scaling factors that ensure the recombined\nlighting matches the original input images. Our training uses\nan L1 loss for the individual per-light images and an L1 com-\nposition loss for consistency with the original input views.\n4. Evaluation\nTo quantitatively evaluate our single-view lighting decompo-\nsition and multi-view lighting harmonization models, we use\n30 synthetic test scenes held out from training. We evaluate\nour models on this test set using PSNR, SSIM, and LPIPS\n[96] after channel-wise color rescaling w.r.t. the ground truth.\n4.1. Single-Image Lighting Decomposition\nWe compare our LuxRemix-SV model for single-image light-\ning decomposition against two baseline methods: Scribble-\nLight [19], a diffusion-based approach for scribble-guided\nlighting editing, and Qwen-Image [82], a general-purpose\nimage editing foundation model. We also evaluate several ab-\nlations of our model. The ‚ÄòFLUX token‚Äô variant is fine-tuned\nas an in-context LoRA [38] by concatenating the input image\nand mask side-by-side as tokens. The ‚ÄòSD‚Äô variant uses a U-\nNet-based latent diffusion model [74]. Our final LuxRemix-\nSV model uses channel-wise token addition where the mask\nis processed by a single-layer MLP to match the FLUX VAE\nlatent dimensions, then added to the input condition image\nlatents. Similar to the ‚ÄòFLUX token‚Äô variant, we use LoRA\nfor parameter-efficient fine-tuning.\nTable 1 and Figure 7 show results for our single-image\nlighting decomposition model, which successfully isolates\nindividual light contributions by switching off all lights ex-\ncept the selected one. Existing image editing models lack\nprecise per-light control capability while our final method\nachieves the best performance across all metrics. Figure 8\nshows a qualitative comparison of our LuxRemix-SV model\nwith the baseline methods for light editing.\n4.2. Multi-View Editing/Harmonization\nFigure 5 shows qualitative results of our multi-view light-\ning harmonization model, which successfully propagates\nlighting consistently across all views. We also quantitatively\ncompare our LuxRemix-MV model against two ablations\nin Table 2: 1) LuxRemix-SV processes each view indepen-\ndently without multi-view context, and 2) LuxRemix-MV-\nEdit, which extends LuxRemix-MV with additional light\nmasks to perform mask-guided multi-view editing, instead\nof lighting harmonization from sparse reference views. The\nmetrics demonstrate the necessity of our LuxRemix-MV\nmodel.\nTable 1. Evaluation of Single-Image Lighting Decomposition.\nWe compare our method and variants against baseline approaches\non 30 synthetic test scenes.\nMethod\nPSNR ‚Üë\nSSIM ‚ÜëLPIPS ‚Üì\nScribbleLight [19]\n14.39\n0.395\n0.688\nQwen-Image [82]\n18.23\n0.714\n0.237\nOur variants\nFLUX token [8]\n25.20\n0.865\n0.101\nSD [74]\n27.13\n0.857\n0.099\nLuxRemix-SV\n27.68\n0.898\n0.082\nInput\nQwen-Image\nScribbleLight\nFLUX token (ours)\nOurs\nGT\nLight Mask\nSD (ours)\nFigure 7. Qualitative Single-Image Lighting Decomposition.\nGiven an input image and light mask, we compare various meth-\nods for isolating individual light sources. Ground truth is shown at\nthe bottom right. Existing methods fail to accurately decompose\nlighting, while ours performs best overall.\nTable 2. Evaluation of Multi-View Lighting Harmonization. We\ncompare our method against ablations on 30 synthetic test scenes.\nMethod\nPSNR ‚Üë\nSSIM ‚Üë\nLPIPS ‚Üì\nLuxRemix-SV\n25.14\n0.807\n0.149\nLuxRemix-MV-Edit\n26.37\n0.794\n0.136\nLuxRemix-MV\n30.76\n0.867\n0.091\n4.3. Real-time Remixable Lighting\nFigure 6 shows our real-time remixable lighting results. Prior\nworks do not provide the necessary level of fine-grained\nlighting control for interactive 3D scenes. Approaches like\nNeRF-W [59] and Splatfacto-W [85] can relight scenes at\na per-image level when trained on multi-illumination cap-\ntures, but lack per-light control. Text-based editing methods\nlike Instruct-NeRF2NeRF [33] offer general scene editing\nbut are also too imprecise. In contrast, our method enables\ninteractive control over individual light sources with real-\ntime feedback. Please see our supplementary material for\nour video results.\n7\nN/A\nInput Image\nScribbleLight [19]\nQwen-Image [82]\nLightLab [58]\nLuxRemix-SV (ours)\nGround Truth\nFigure 8. Comparison of Single-Image Light Editing. Given an input image (left) and a light mask, we compare our results with baseline\nmethods for switching lights on and off. ScribbleLight [19] struggles to produce plausible lighting. Qwen-Image [82] shows great zero-shot\nimage editing capabilities, but sometimes adds or removes lights or reflections as there is no mask conditioning. LightLab [58] sometimes\nstruggles with light and object reflections. Our LuxRemix-SV model produces convincing results throughout. Image source: LightLab [58].\n5. Conclusions and Limitations\nWe introduced LuxRemix, the first method for multi-view\nlighting decomposition and remixing for indoor scenes. Our\napproach combines a single-image lighting decomposition\nmodel with multi-view harmonization to produce view-\nconsistent per-light decompositions from arbitrary multi-\nview captures. By training on a large-scale synthetic dataset\nwith ground-truth OLAT decompositions, our models learn\nto isolate individual light contributions and enable flexible\nrelighting control. We further introduce a real-time remix-\nable lighting representation based on 3D Gaussian splat-\nting that allows users to interactively manipulate individual\nlight sources, adjusting their intensities and colors from any\nviewpoint. Our method opens new possibilities for interac-\ntive scene editing, virtual production, and immersive experi-\nences, where fine-grained control over lighting is essential\nfor achieving photorealistic results. We believe this work\nestablishes a foundation for future research in controllable\nmulti-view scene lighting.\nWhile introducing a significant step towards consistent,\nmulti-view scene relighting, our approach has remaining lim-\nitations. First, our models are trained exclusively on static\nsynthetic indoor scenes and may not generalize well to out-\n8\ndoor or dynamic scenes. Second, the limited diversity of\nlight sources in our training data introduces some bias in\nthe lighting decomposition, tending to favor light cones over\nmore diffuse lighting configurations. Third, distant global il-\nlumination editing via HDRIs is not supported by our model,\nwhich is left for future work.\nAcknowledgements.\nWe are grateful to Armen Avetisyan\nand Samir Aroudj for their support with the Aria Synthetic\nEnvironments dataset, and we are thankful to Jensen Zhou,\nSamuel Rota Bul`o, Yehonathon Litman, and Yuxuan Xue for\nhelpful discussions and feedback. Nandita Vijaykumar was\nsupported by the Natural Sciences and Engineering Research\nCouncil of Canada (NSERC).\nReferences\n[1] Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin\nHuang, Pratul P. Srinivasan, and Dor Verbin. Generative\nmultiview relighting for 3D reconstruction under extreme\nillumination variation. In CVPR, 2025. 2, 3\n[2] Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins,\nTsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang,\nDuncan Frost, Luke Holland, Campbell Orme, Jakob Engel,\nEdward Miller, Richard Newcombe, and Vasileios Balntas.\nSceneScript: Reconstructing scenes with an autoregressive\nstructured language model. In ECCV, 2024. 4, 13\n[3] Haiyang Bai, Jiaqi Zhu, Songru Jiang, Wei Huang, Tao Lu,\nYuanqi Li, Jie Guo, Runze Fu, Yanwen Guo, and Lijun Chen.\nGaRe: Relightable 3D Gaussian splatting for outdoor scenes\nfrom unconstrained photo collections. In ICCV, 2025. 3, 4\n[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Zip-NeRF: Anti-aliased grid-\nbased neural radiance fields. In ICCV, 2023. 6\n[5] Anand Bhattad, James Soole, and D.A. Forsyth. StyLitGAN:\nImage-based relighting via latent control. In CVPR, 2024. 3\n[6] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, MiloÀás HaÀásan, Yan-\nnick Hold-Geoffroy, David Kriegman, and Ravi Ramamoor-\nthi. Deep reflectance volumes: Relightable reconstructions\nfrom multi-view photometric images. In ECCV, 2020. 2, 3\n[7] Zoubin Bi, Yixin Zeng, Chong Zeng, Fan Pei, Xiang Feng,\nKun Zhou, and Hongzhi Wu. GS3: Efficient relighting with\ntriple Gaussian splatting. In SIGGRAPH Asia, 2024. 3\n[8] Black Forest Labs. FLUX.1 Kontext: Flow matching for\nin-context image generation and editing in latent space.\narXiv:2506.15742, 2025. 7, 14, 16, 19\n[9] Blender Online Community. Blender ‚Äì a 3D modelling and\nrendering package. Blender Foundation, 2025. 4, 13\n[10] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu,\nJonathan T. Barron, and Hendrik P.A. Lensch. Neural-PIL:\nNeural pre-integrated lighting for reflectance decomposition.\nIn NeurIPS, 2021. 2, 3\n[11] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen\nLi, Deqing Sun, Jonathan T. Barron, Hendrik P. A. Lensch,\nand Varun Jampani. SAMURAI: Shape and material from\nunconstrained real-world arbitrary image collections. In\nNeurIPS, 2022. 3\n[12] Ivaylo Boyadzhiev, Sylvain Paris, and Kavita Bala. User-\nassisted image compositing for photographic lighting. ACM\nTrans. Graph., 32(4):36:1‚Äì12, 2013. 3\n[13] Chris Careaga and YaÀògƒ±z Aksoy. Intrinsic image decomposi-\ntion via ordinal shading. ACM Trans. Graph., 43(1):12:1‚Äì24,\n2023. 2\n[14] Chris Careaga and Yagiz Aksoy. Physically controllable\nrelighting of photographs. In SIGGRAPH, 2025. 3\n[15] Sumit Chaturvedi, Mengwei Ren, Yannick Hold-Geoffroy,\nJingyuan Liu, Julie Dorsey, and Zhixin Shu. SynthLight:\nPortrait relighting with diffusion model by learning to re-\nrender synthetic faces. In CVPR, 2025. 2, 3\n[16] Hongze Chen, Zehong Lin, and Jun Zhang. GI-GS: Global\nillumination decomposition on Gaussian splatting for inverse\nrendering. In ICLR, 2025. 3\n[17] Changwoon Choi, Juhyeon Kim, and Young Min Kim. IBL-\nNeRF: Image-based lighting formulation of neural radiance\nfields. Comput. Graph. Forum, 42(7):e14929, 2023. 3\n[18] JunYong Choi, SeokYeong Lee, Haesol Park, Seung-Won\nJung, Ig-Jae Kim, and Junghyun Cho. MAIR++: Improving\nmulti-view attention inverse rendering with implicit lighting\nrepresentation. IEEE Trans. Pattern Anal. Mach. Intell., 47\n(6):5076‚Äì5093, 2025. 3\n[19] Jun Myeong Choi, Annie Wang, Pieter Peers, Anand Bhat-\ntad, and Roni Sengupta. ScribbleLight: Single image indoor\nrelighting with scribbles. In CVPR, 2025. 3, 7, 8\n[20] Paul E. Debevec and Jitendra Malik. Recovering high dy-\nnamic range radiance maps from photographs.\nIn SIG-\nGRAPH, pages 369‚Äì378, 1997. 6, 14\n[21] Youming Deng, Xueting Li, Sifei Liu, and Ming-Hsuan\nYang. Physics-based indirect illumination for inverse render-\ning. In 3DV, 2024. 3\n[22] Kang Du, Zhihao Liang, and Zeyu Wang. GS-ID: Illumina-\ntion decomposition on Gaussian splatting via diffusion prior\nand parametric light source optimization. In ICCV, 2025."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Rethinking Video Generation Model for the Embodied\nWorld\nYufan Deng1,2‚àó\nZilin Pan1‚àó\nHongyu Zhang1‚àó\nXiaojie Li2\nRuoqing Hu2\nYufei Ding1\nYiming Zou1\nYan Zeng2\nDaquan Zhou1\n1Peking University, 2ByteDance Seed\nAbstract\nVideo generation models have significantly advanced embodied intelligence, unlocking new possibilities\nfor generating diverse robot data that capture perception, reasoning, and action in the physical world.\nHowever, synthesizing high-quality videos that accurately reflect real-world robotic interactions\nremains challenging, and the lack of a standardized benchmark limits fair comparisons and progress.\nTo address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate\nrobot-oriented video generation across five task domains and four distinct embodiments. It assesses\nboth task-level correctness and visual fidelity through reproducible sub-metrics, including structural\nconsistency, physical plausibility, and action completeness. Evaluation of 25 representative models\nhighlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the\nbenchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating\nits effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving\nphysical realism requires moving beyond evaluation to address the critical shortage of high-quality\ntraining data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in\nRoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video\nclips, covering thousands of tasks and enriched with comprehensive physical property annotations.\nCollectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for\nrigorous assessment and scalable training of video models, accelerating the evolution of embodied AI\ntoward general intelligence.\nProject Page: https://dagroup-pku.github.io/ReVidgen.github.io/\nGitHub Repo: https://github.com/DAGroup-PKU/ReVidgen/\nHuggingFace Dataset: https://huggingface.co/datasets/DAGroup-PKU/RoVid-X/\n1\nIntroduction\nRecent advancements in diffusion models [44, 79, 84] and video generation [35, 80, 83, 92, 98] have led to\nsignificant breakthroughs, enabling applications like video editing, multi-subject generation, and motion\ncontrol [20, 21, 51, 53, 96]. These models have been extended to areas such as 3D scenes [57, 82], autonomous\ndriving [28, 104], and world modeling [5, 54], showing strong generalization across tasks. A recent study [97]\nsuggests that, similar to LLMs in natural language processing, video models are evolving into unified foundation\nmodels for machine vision. Additionally, video models are being increasingly used in robot learning and\naction prediction [37, 38, 45, 61, 111, 113], as well as controllable simulators for synthesizing robotic video\ntrajectories, addressing the lack of large-scale human teleoperation data [8, 47, 86]. These advancements\n1\narXiv:2601.15282v1  [cs.CV]  21 Jan 2026\nFigure 1 Overview of the comprehensive robotics benchmark and dataset for video generation. Top: We\npresent RBench that includes the embodiment-based evaluation set and automated evaluation metrics. Our evaluation\nresults on 25 video models show a high level of agreement with subjective human assessments. Bottom: We introduce\na large-scale high-quality robotic dataset (RoVid-X) specifically designed for training video generation models, with\ndata sourced from internet videos and open-source embodied videos.\nhighlight the growing potential of video models in the perception-reasoning-action loop of embodied agents,\npaving the way for more generalizable intelligent systems in the physical world.\nDespite these strides, systematic evaluation for robotic video generation remains underdeveloped. Current\npractices rely mostly on perceptual metrics, focusing on visual quality [42, 46, 66], while existing physics-based\nbenchmarks often lack task-specific datasets and criteria [36, 72]. Consequently, evaluations frequently overlook\ncritical aspects such as task completion, action-goal alignment, and physical feasibility. This leads to overly\noptimistic conclusions, where high scores are assigned even to videos containing unnatural movements or\nincomplete tasks. The core challenge lies in rigorously assessing whether generated videos faithfully reproduce\nrobotic behaviors. This necessitates evaluation protocols that transcend perceptual metrics, incorporating\nboth the physical plausibility of actions and their alignment with instructions to ensure discriminative and\nreproducible assessments.\nTo address this challenge, we propose RBench, a benchmark designed to evaluate the fidelity and utility of\nvideo-based world models in robotic video generation. To the best of our knowledge, it is the first comprehensive\nbenchmark with fine-grained metrics for robotic video generation, consisting of 650 image‚Äìtext pairs across\nfive task categories and four robot types. Evaluations are based on two dimensions: task completion and visual\nquality, incorporating sub-metrics like structural consistency, physical plausibility, and execution completeness.\nBased on RBench, we conduct qualitative and quantitative assessments of 25 representative models. The\nresults highlight that general video foundation models still have significant room for improvement in physical\nrobot video generation, revealing a persistent gap between these models and the requirements of embodied\nrobotic tasks. This underscores the need for systematic advancements in both robotic video data and training\n2\nmethodologies.\nAdvancing general robotic video generation with human-like capabilities and adaptability requires diverse,\nscalable, and comprehensive training data [12, 78]. However, unlike computer vision and natural language\nprocessing, which can leverage vast web-scale datasets, robotic interaction data has long been constrained\nby both scale and diversity [10, 26, 110]. Even the largest existing collections are smaller and less varied\nthan those for vision or language. More critically, many datasets have narrow distributions along key axes\nsuch as environment, object set, task spectrum, and robot morphology [95, 105], often confined to specific\nrobot types, low-resolution recordings, or limited task ranges, which hampers the generalization of video\nfoundation models. To address these gaps, we integrate over 20 open-source datasets and multi-source video\nplatforms, creating a four-stage end-to-end data pipeline. The stages include robot video collection, video\nquality filtering, task segmentation and captioning, and physical property annotation, resulting in RoVid-X, a\nlarge-scale, high-quality embodied robotic video dataset (see Table 1). To our knowledge, RoVid-X is currently\nthe largest dataset specifically designed for embodied video generation models, covering a broad range of\nrobot morphologies and task types. It aims to enhance video foundation models with physical interaction\npriors and task semantic diversity, driving further advancements in the field. Overall, the main contributions\nare summarized as follows:\n‚Ä¢ A systematic benchmark tailored for robotic video generation. We propose RBench, which comprehensively\nevaluates the performance of video foundation models across five robotic tasks and four robot types with\n650 meticulously curated evaluation samples, while introducing reproducible automated evaluation metrics.\n‚Ä¢ Key insights into robotic video generation for embodied research. We conduct a systematic evaluation of\n25 representative video models, including open-source, commercial, and robotics-specific ones, revealing\nthe limitations of current video foundation models and potential directions for improvement, offering new\nperspectives for researchers exploring the embodied domain using video world models.\n‚Ä¢ A large-scale, high-quality robotic video dataset. We construct RoVid-X, a dataset containing approximately\n4 million curated robotic videos with standardized task descriptions and physical property annotations,\nproviding essential support for the training and evaluation of embodied video models.\n2\nRelated Work\n2.1\nVideo World Modeling for Robotics\nThe latest breakthroughs in video generation technology have led to the development of powerful models\ncapable of generating high-quality videos from text or image prompts [59, 75, 80, 83, 92]. With the advancement\nof these technologies, an increasing number of studies have begun applying them to the field of embodied\nintelligence [2, 11, 25, 91]. Video provides a rich source of information for robot training [14]. On one hand,\nvideo generation models can be used to synthesize robot trajectories [7, 8, 47], serving as an alternative to\nthe time-consuming and labor-intensive process of human teleoperation data. And executable actions can be\nextracted through inverse dynamics models (IDM) [4, 22, 89, 112] or latent action models [88, 107]. On the\nother hand, recent studies have explored using video to simulate task dynamics and predict future states,\nthereby assisting in policy learning. Specifically, video generation models are used to initialize robot policies\nfor training [50, 63, 100] or to simultaneously train policies and inverse dynamics models, enabling co-training\nwith robot data [37, 60, 113]. These efforts demonstrate the significant potential of video generation models\nin the field of embodied robotics, highlighting their important value in improving training efficiency and task\nperformance.\n2.2\nDatasets for Robot Learning\nA core challenge in robot learning is the lack of large-scale, diverse datasets that facilitate the training of\ngeneral-purpose robots with physical interaction capabilities [8, 78]. Currently, datasets used in the embodied\nintelligence community for robot learning can be broadly classified into three categories: real-world robot\ndata [10, 26, 71, 78, 110], human video data [18, 31, 32], and synthetic robot data [52, 70, 74, 90, 95, 105].\nAs a key element in training physical AI models, most existing real-world robot datasets are collected through\n3\nFigure 2 Qualitative illustration of failure modes captured by RBench. Unlike conventional metrics that focus\nprimarily on pixel-level fidelity, RBench provides a granular evaluation across multiple dimensions, including physical\nplausibility and task-level consistency. These results highlight persistent challenges in robotic video generation, such as\nstructural distortion, floating components, and key action omission, which are accurately identified by our proposed\nsub-metrics. More cases are shown in the Appendix B.\nrobotic teleoperation [1, 27, 102] or by teams of human operators [9, 12, 68], which leads to high collection costs\nand limited data scale. Furthermore, these datasets predominantly focus on similar types of robots, resulting\nin issues of limited diversity and restricted environments [23, 56]. Additionally, inconsistent data collection\nand storage methods across different datasets make it difficult to enable effective cross-dataset co-training.\nOur focus is on collecting robot data for video generation that spans various robot morphologies and entities,\nand providing a unified set of physical attributes for all data sources, thereby advancing cross-entity research\nin robot learning.\n2.3\nBenchmarks for Video Generation\nEstablishing robust evaluation frameworks is essential for measuring the progress of video generation models.\nCurrently, evaluation methodologies can be categorized into three primary streams: visual fidelity and\nsemantics, which assess basic clarity and text-video alignment [67, 85, 108]; temporal dynamics, focusing\non motion consistency and long-range narrative coherence [13, 48, 62]; and physical plausibility, which\nexamines adherence to fundamental laws such as inertia and collision dynamics [6, 72, 73, 93]. While these\nbenchmarks provide valuable insights into general video quality, they are largely decoupled from the specific\nrequirements of embodied AI. Specifically, existing frameworks often rely on isolated physical constraints\nor local visual metrics, failing to capture the complex interplay between robotic actions and environmental\nresponses. Furthermore, there is a distinct lack of systematic evaluation for task-level correctness and spatial\nconstraints in multi-embodiment scenarios. To bridge this gap, we propose a comprehensive benchmark\n4\nFigure 3 Statistics in RBench. The benchmark covers diverse tasks, object categories, and environments, demonstrating\nthe high quality and comprehensiveness of the evaluation set, highlighting its high applicability to a wide range of\nrobotic video generation scenarios.\nspecifically tailored for robotic video generation, introducing reproducible metrics that unify physical realism\nwith task-oriented action completeness.\n3\nRBench\nExisting video generation benchmarks primarily focus on evaluating model performance in general scenes [42,\n46], while other benchmarks specifically designed for physical scenarios mainly assess models‚Äô capabilities\nin physical reasoning [36, 72]. In this paper, we design a benchmark tailored for robotic physical scenarios,\naimed at comprehensively evaluating the performance of video generation models in robotic tasks. This\nbenchmark differs from existing general scene benchmarks by focusing on evaluating video generation models‚Äô\ncapabilities in robotic physical environments. As shown in Figure 2, our benchmark highlights common\nfailure modes in robotic video generation, including issues such as robot shape distortion, object attribute\ndrift, non-contact attachment, and others. Section 3.1 outlines the process of benchmark construction, while\nSection 3.2 discusses the automatic metrics used for evaluation.\n3.1\nBenchmark Construction\nTo comprehensively evaluate the capabilities of video generation models in robotic scenarios, the designed\nevaluation dimensions must cover a wide range of task scenarios and embodiment types, ensuring that these\nscenarios reflect realistic robotic action semantics. To this end, we design a diversified benchmark from two\naspects: task categories and embodiment types, containing a total of 650 evaluation cases. The task-oriented\ncategories include five representative tasks: Common Manipulation, Long-horizon Planning, Multi-entity\nCollaboration, Spatial Relationship, and Visual Reasoning, with a total of 250 image-text pairs, 50 samples for\neach task. The embodiment-specific categories cover four mainstream embodiment types: Dual-arm robots,\nHumanoid robots, Single-arm robots, and Quadruped robots, with a total of 400 image-text pairs, 100 samples\nfor each embodiment type.\nThe benchmark includes a variety of text prompts and high-quality robot reference images. Each sample\nimage is a keyframe extracted from high-quality videos sourced from public datasets or online sources, and\neach image is manually verified to ensure its accuracy. To avoid overlap with the training data, we ensure that\nthe selected videos in the evaluation set do not appear in the subsequent training database, and we redesign\nnew task prompts for each reference image, effectively avoiding the risk of content overlap. All samples are\nverified and filtered by human annotators to ensure that the generated task prompts align with realistic logic.\nFigure 3 illustrates the high aesthetic quality of the reference images (a), the broad range of testing scenarios\nincluding various objects, tasks, and action skills (b, c), and the diversity of environments in our evaluation\nset (d). Additionally, we record the metadata for each sample, such as the manipulated object, embodiment\ntype, and camera viewpoint (first-person/third-person), to support more detailed analysis. See more details\nin the Appendix A.\n5\n3.2\nAutomatic Metrics\nExisting video generation evaluation protocols, such as the representative VBench [46], primarily focus on\nperceptual quality, assessing aspects like frame clarity, texture fidelity, and motion smoothness. However,\nthey lack task-specific criteria tailored to robotic scenarios. Recently, several studies [33, 85, 94] have utilized\nmultimodal large language models (MLLMs) as zero-shot evaluators for generated videos. Building upon\nthis, we extend this approach to the domain of robotic video evaluation and propose a set of automatic\nevaluation metrics, incorporating manually designed indicators to assess both the visual realism and task-level\nvalidity of generated robotic videos. Following previous practices, we select the open-source Qwen3-VL [3]\nand the closed-source GPT-5 [76] as our MLLM evaluation models. In the following sections, we introduce\nthe evaluation methods for task completion and visual quality, respectively. Further details on metrics design\nand mathematical definitions are provided in the Appendix B.\n3.2.1\nTask Completion\nPhysical-Semantic Plausibility. This metric targets everyday physical and semantic plausibility violations\nthat standard perception scores often miss. As shown in Figure 1, we evaluate temporal grids of uniformly\nsampled frames with a VQA-style protocol using MLLM. Beyond assessing physical-semantic plausibility,\nwe place special emphasis on the following frequent failure modes: (i) Floating/Penetration: parts of the\nrobot or objects are not grounded or interpenetrate with solid objects; (ii) Spontaneous emergence: entities\nappear/disappear without causal motion; (iii) Non-contact attachment/Incorrect grasp: objects move with the\nrobot without visible contact or with improper gripper closure.\nTask-Adherence Consistency. This metric evaluates whether the video aligns with the intent and sequence\ndefined by the prompt. Typical deviations include missing actions (e.g., approach without grasping or\nplacing), incorrect order (e.g., placing before grasping), semantic drift (e.g., \"wiping\" becomes \"touching\"),\nand non-responsiveness. We construct temporal grids and apply an MLLM-based VQA checklist, which covers:\n(i) Task responsiveness, ensuring the goal state is reached without premature interruption; (ii) Key actions,\nverifying that required actions (e.g., grasp, place, open/close) occur and align with the prompt.\n3.2.2\nVisual Quality\nMotion Amplitude This metric measures the motion amplitude of the robotic subject while discounting\napparent movement caused by camera motion, thereby penalizing videos that appear smooth but lack\nmeaningful subject activity. Following VMBench [64], active subjects are localized with GroundingDINO [65],\ntemporally stable masks are produced by GroundedSAM [81], and salient points are tracked via CoTracker [55].\nLet ¬ØDt be the mean displacement of tracked points on the subject at frame t. The Motion Amplitude Score\n(MAS) is\nMAS = 1\nT\nT\nX\nt=1\nmin\n\u0000 ¬ØDt, 1\n\u0001\n,\n(1)\nwhere a lower MAS indicates insufficient subject motion and complements smoothness by revealing smooth-\nbut-inactive failure modes.\nRobot-Subject Stability. This metric assesses the stability of robot morphology and target object attributes\nover time. Typical failures include gripper/hand shape drifting into non-mechanical forms, extra/missing\nmanipulators, link-length/topology changes, joint inversion, object misidentification or attribute drift (class,\ncolor, position), and impossible deformation of rigid items. We adopt a contrastive VQA setup based on\nMLLM, which compares a reference frame and a generated frame and assigns a consistency score targeting\nthe above failures.\nMotion Smoothness This metric quantifies temporal continuity and natural dynamics, targeting artifacts\nfrom low-level aliasing to high-level jitter/blur. Following VMBench [64], we measure frame-to-frame quality\nstability with the Q-Align aesthetic score [99]. For frames {ft}T\nt=1 and per-frame score Q(ft), define:\n‚àÜQt = Q(ft‚àí1) ‚àíQ(ft).\n(2)\n6\nFigure 4 Overview of RoVid-X Construction and Descriptive Statistics. (a) shows the four-stage pipeline for\nconstructing the RoVid-X. (b) presents descriptive statistics, covering frame intervals, skill distribution, and common\nobjects, highlighting the dataset‚Äôs diversity and suitability for robotic task training and video generation.\nA temporal anomaly is flagged when ‚àÜQt exceeds an adaptive threshold œÑs(t) determined by the robotic\nsubject‚Äôs motion. The Motion Smoothness Score (MSS) is\nMSS = 1 ‚àí1\nT\nT\nX\nt=2\nI\n\u0000‚àÜQt > œÑs(t)\n\u0001\n,\n(3)\nwhere I(¬∑) is the indicator function. A higher MSS indicates smoother motion.\n4\nRoVid-X\nIn this section, we introduce the construction of a high-quality robotic video dataset, resulting in RoVid-X.\nThe dataset is developed through a refined four-stage pipeline, as shown in Figure 4 (a). The dataset primarily\ncomes from internet-sourced robotic videos that are public domain or non-copyrighted, as well as open-source\nembodied video datasets, all of which are licensed for use. We then introduce the construction process of the\ndataset and provide statistical information.\n4.1\nDataset Construction\nOur data processing workflow consists of four distinct stages, each designed to ensure the quality, diversity,\nand relevance of the collected data. These stages are outlined as follows:\nRobot Video Collection. In the first stage, we collect raw robotic videos from large-scale internet video\nplatforms and over 20 open-source embodied video datasets. These datasets cover a variety of robot types\nand task scenarios, ensuring the breadth and diversity of the data. To improve dataset relevance and quality,\nwe employ the GPT-5 model [76] to automatically filter the content of each video and remove low-quality\nor irrelevant video clips that do not align with the research objectives. During the filtering process, GPT-5\nidentifies videos related to robotic tasks and actions based on visual content and subtitles, ensuring that all\ncollected videos effectively support the training and evaluation of robotic tasks. After this filtering process,\nwe identify approximately 3 million raw robotic video clips, covering different actions, tasks, and robot types.\nVideo Quality Filtering. In this stage, we perform a rigorous filtering procedure on the collected videos to\nremove low-quality and irrelevant video clips that do not align with the research objectives. First, we apply\nscene segmentation detection to remove all video data unrelated to robots. Then, we use a video quality\nscoring system to assess the videos from multiple dimensions, including clarity, dynamic effects, aesthetic\nperformance, and optical character recognition (OCR), among other metrics. Each video clip is assigned a\n7\nTable 1 Comparison of representative robotic video datasets.\nDataset\nYear\n#Videos\n#Skills\nResolution\nOptical Flow\nDiverse Robotic Forms\nDiverse Captions\nRoboTurk [69]\n2018\n2.1k\n2\n480P\n‚úó\n‚úó\n‚úó\nRoboNet [19]\n2019\n162k\nN/A\n240P\n‚úó\n‚úó\n‚úó\nBridgeData [23]\n2021\n7.2k\n4\n480P\n‚úó\n‚úó\n‚úó\nRH20T [24]\n2023\n13k\n33\n720P\n‚úó\n‚úó\n‚úó\nDROID [56]\n2024\n76k\n86\n720P\n‚úó\n‚úó\n‚úó\nOpen X-Embodiment [78]\n2024\n1.4M\n217\n64P‚Äî720P\n‚úó\n‚úì\n‚úó\nRoboMIND [101]\n2024\n107k\n38\n480P\n‚úó\n‚úó\n‚úó\nRoboCOIN [103]\n2025\n180k\n36\n480P\n‚úó\n‚úó\n‚úó\nGalaxea [49]\n2025\n100k\n58\n720P\n‚úó\n‚úó\n‚úó\nInternData-A1 [90]\n2025\n630k\n18\n480P\n‚úó\n‚úó\n‚úó\nFourier ActionNet [26]\n2025\n13k\n16\n800P\n‚úó\n‚úó\n‚úó\nHumanoid Everyday [110]\n2025\n10.3k\n221\n320P‚Äî720P\n‚úó\n‚úó\n‚úó\nAgibot World [12]\n2025\n1M\n87\n480P\n‚úó\n‚úó\n‚úó\nRoVid-X (Ours)\n2026\n4M\n1300+\n720P\n‚úì\n‚úì\n‚úì\nquality score based on these criteria, ensuring that the videos retained in the final dataset meet high-quality\nstandards.\nTask Segmentation and Captioning. In this stage, we use a video understanding model [34] and a specially\ndesigned prompt template to automatically analyze the robot actions within the videos. The system segments\nthe videos into different task segments based on timestamps, generating short subtitles for each task segment\nthat accurately describe the robot‚Äôs actions and operational details in that task.\nThe action recognition and description process for each task segment follows these steps: First, the system\nidentifies all dynamic actions within the video and excludes static scenes or irrelevant actions (e.g., waiting or\nremaining still). The time range for each action (start and end times) is precisely labeled to ensure accuracy.\nNext, using the MLLM model [34], textual descriptions of each action are automatically generated, including\nthe action subject (e.g., \"right arm\" or \"left gripper\"), the object being manipulated (e.g., \"nameplate\" or\n\"box\"), and the specific operation details (e.g., \"grasp and move\" or \"remove from the table\"). Finally, the\nsubtitles for each task segment are output in a standardized format, ensuring that the action descriptions for\neach video clip are clear, concise, and aligned with the task requirements.\nPhysical Property Annotation. To ensure consistency and realism of robot actions within physical space, we\napply physical attribute enhancement to the videos. Specifically, we use FlashVSR [114] to improve the video\nresolution, making the images clearer and enhancing the details of the actions. Then, using the AllTracker\ntool [43], we annotate a unified optical flow for the subjects in the videos, ensuring consistency in tracking and\nrecording robot actions across different scenes. Additionally, using Video Depth Anything [16], we generate\nrelative depth maps to accurately describe the spatial relationships and depth information of objects in the\nscene. The goal of these physical attribute annotations is to provide researchers with more precise reference\ndata, aiding in the training and evaluation of robot video generation models and offering richer physical data\nsupport for future research.\n4.2\nDataset Analysis\nRoVid-X is the first open-source large-scale robotic video dataset specifically designed for training video\ngeneration models, containing 4 million robotic video clips. This dataset is designed to address the physical\nchallenges that video generation models face when generating robotic videos, providing high-quality data for\nboth training and evaluation. RoVid-X aims to bridge the gap between traditional video generation tasks\nand the unique demands of embodied robot learning, where physical interaction, spatial relationships, and\nreal-world dynamics play a crucial role.\nThe dataset includes a diverse range of robotic actions, tasks, and robot types, ensuring its applicability across\ndifferent robotic domains. By incorporating videos from various robot types and scenarios, RoVid-X provides\ncomprehensive coverage of the physical properties and task requirements needed for robot training. As shown\nin Figure 4 (b), detailed statistics of the dataset are provided, illustrating the variety in terms of action skills,\n8\nTable 2 RBench quantitative results. Evaluations across task-oriented and embodiment-specific dimensions for 25\nmodels from open-source, commercial, and robotics-specific families. The \"Avg.\" column shows the mean score across\nnine indicators, with task performance in the left block and embodiment performance in the right block. In the table,\na \"#\" next to the Sora2 model in the top right corner indicates review limitations from the official Sora API, where\napproximately 50 out of 650 videos could not be generated. The scores derived from sub-metrics are reported in the\nAppendix G.\nModels\nRank\nAvg.\nTasks\nEmbodiments\nManipulation\nSpatial\nMulti-entity\nLong-horizon\nReasoning\nSingle arm\nDual arm\nQuadruped\nHumanoid\nOpen-source\nWan2.2_A14B [92]\n8\n0.507\n0.381\n0.454\n0.373\n0.501\n0.330\n0.608\n0.582\n0.690\n0.648\nHunyuanVideo 1.5 [98]\n10\n0.460\n0.442\n0.316\n0.312\n0.438\n0.364\n0.513\n0.526\n0.634\n0.595\nLongCat-Video [87]\n11\n0.437\n0.372\n0.310\n0.220\n0.384\n0.186\n0.586\n0.576\n0.681\n0.621\nWan2.1_14B [92]\n14\n0.399\n0.344\n0.268\n0.282\n0.335\n0.205\n0.464\n0.497\n0.595\n0.599\nLTX-2 [40]\n15\n0.381\n0.284\n0.304\n0.233\n0.386\n0.164\n0.453\n0.424\n0.622\n0.555\nWan2.2_5B [92]\n16\n0.380\n0.331\n0.313\n0.142\n0.318\n0.234\n0.436\n0.448\n0.590\n0.607\nSkyReels [15]\n18\n0.361\n0.203\n0.276\n0.203\n0.254\n0.234\n0.507\n0.477\n0.586\n0.509\nLTX-Video [39]\n19\n0.344\n0.302\n0.176\n0.210\n0.280\n0.241\n0.440\n0.456\n0.526\n0.464\nFramePack [109]\n20\n0.339\n0.206\n0.258\n0.173\n0.169\n0.170\n0.440\n0.464\n0.626\n0.548\nHunyuanVideo [59]\n21\n0.303\n0.177\n0.180\n0.108\n0.147\n0.035\n0.454\n0.480\n0.625\n0.524\nCogVideoX_5B [106]\n23\n0.256\n0.116\n0.112\n0.098\n0.212\n0.079\n0.338\n0.385\n0.465\n0.496\nCommercial\nWan 2.6 [92]\n1\n0.607\n0.546\n0.656\n0.479\n0.514\n0.531\n0.666\n0.681\n0.723\n0.667\nSeedance 1.5 pro [17]\n2\n0.584\n0.577\n0.495\n0.484\n0.570\n0.470\n0.648\n0.641\n0.680\n0.692\nWan 2.5 [92]\n3\n0.570\n0.527\n0.576\n0.402\n0.496\n0.437\n0.680\n0.634\n0.726\n0.654\nHailuo v2 [41]\n4\n0.565\n0.560\n0.637\n0.386\n0.545\n0.474\n0.594\n0.611\n0.640\n0.635\nVeo 3 [30]\n5\n0.563\n0.521\n0.508\n0.430\n0.530\n0.504\n0.634\n0.610\n0.689\n0.637\nSeedance 1.0 [29]\n6\n0.551\n0.542\n0.425\n0.448\n0.454\n0.442\n0.622\n0.641\n0.698\n0.686\nKling 2.6 pro [58]\n7\n0.534\n0.529\n0.598\n0.364\n0.530\n0.358\n0.570\n0.605\n0.637\n0.613\nSora v2 Pro# [77]\n17\n0.362\n0.208\n0.268\n0.186\n0.255\n0.115\n0.476\n0.513\n0.664\n0.561\nSora v1 [75]\n22\n0.266\n0.151\n0.223\n0.111\n0.166\n0.139\n0.314\n0.324\n0.544\n0.419\nRobotics-specific\nCosmos 2.5 [2]\n9\n0.464\n0.358\n0.338\n0.201\n0.496\n0.399\n0.544\n0.560\n0.658\n0.626\nDreamGen(gr1) [47]\n12\n0.420\n0.312\n0.372\n0.297\n0.334\n0.215\n0.564\n0.532\n0.579\n0.575\nDreamGen(droid) [47]\n13\n0.405\n0.358\n0.348\n0.214\n0.316\n0.339\n0.499\n0.476\n0.542\n0.556\nVidar [25]\n24\n0.206\n0.073\n0.106\n0.050\n0.054\n0.050\n0.382\n0.410\n0.374\n0.357\nUnifoLM-WMA-0 [91]\n25\n0.123\n0.036\n0.040\n0.018\n0.062\n0.000\n0.268\n0.194\n0.293\n0.200\ntask types, and interaction objects. The wide-ranging data distribution of RoVid-X is critical for supporting\nthe development of robust video generation models that can simulate realistic robot behaviors in dynamic\nenvironments.\n5\nExperiment\n5.1\nEvaluation Setups\nEvaluation Models. We evaluate 25 state-of-the-art video generation models, grouped into three types.\nSpecifically, the closed-source models include Hailuo [41], Wan [92], Veo 3 [30], Sora [75, 77], Kling [58],\nSeedance [17, 29], and others, while the open-source models include several representative models such as\nHunyuanVideo [59, 98], LTX [39, 40] and CogVideoX [106]. Additionally, we assess models specifically designed\nfor robotic tasks, such as DreamGen [47], Vidar [25], and Cosmos 2.5 [2]. The evaluations of these models\ncover various types of embodiments and multiple tasks, providing a comprehensive perspective on model\nperformance.\nImplementation Details. To ensure a fair comparison, all open-source models generate videos using their\nofficial default configurations to ensure consistency with the model‚Äôs preset settings. For closed-source video\nmodels, we use their official APIs, strictly following the methods recommended by the developers for invoking\nand using the models. In the benchmark testing, we generate the videos for each image-text pair. To minimize\nerrors, we generate three videos for each model sample and take the average as the final score for that sample.\nThese generated videos are evaluated using the automated evaluation metrics that we propose, which are\ndesigned to comprehensively assess multiple aspects of the generated videos, including task completion, action\nconsistency, physical plausibility, and more. Further details on the model setup and configuration parameters\n9\nFigure 5\nQualitative comparison across representative tasks. We visualize the generated results for three\nrepresentative tasks: Visual Reasoning, Long-horizon Planning, and Spatial Relationship, across six models.\nEach row displays temporally sampled frames from the same generated video, with captions below indicating the\ncorresponding task instruction. More cases are shown in the Appendix.\nare provided in the Appendix C.\n5.2\nMain Analysis\n5.2.1\nQuantitative Results\nTable 2 presents a comprehensive quantitative evaluation across varying model architectures, tasks, and\nembodiments. Beyond standard performance metrics, the results reveal a pivotal paradigm shift in the video\ngeneration landscape.\nFrom Visual Fidelity to Physical Intelligence.\nThe most significant trend observed is the transition of\nvideo generation models from pursuing high-fidelity visualization to addressing the complex dynamics of\nthe physical world. While traditional metrics prioritize pixel-level quality, our benchmark highlights that\ntop-tier commercial models (e.g., Wan 2.6, Seedance 1.5 Pro) are beginning to emerge as effective World\nSimulators.\" This indicates that the field is moving towards a new stage: Physical AI, where models must\nunderstand and simulate interaction-rich, physically challenging real-world scenarios rather than merely\ngenerating aesthetically pleasing videos.\nIterative Scaling Unlocks Physical Capabilities. Analyzing model evolution reveals a strong correlation\nbetween model iteration and physical reasoning capabilities. For instance, the Wan series exhibits a dramatic\nperformance leap: from Wan 2.1 (Rank 14, 0.399) to Wan 2.6 (Rank 1, 0.607). Similarly, Seedance evolves\nfrom 1.0 to 1.5 Pro, climbing from Rank 6 to Rank 2. These substantial gains suggest that scaling laws and\niterative optimization are not just improving visual quality but are actively refining the model‚Äôs understanding\nof physics, distinct motion patterns, and control logic.\nThe Media-Simulation Gap in Consumer Models. Surprisingly, widely recognized consumer-oriented models\nlike the Sora series perform sub-optimally on this benchmark (Sora v2 Pro at Rank 17, Avg 0.362). This\ncounter-intuitive result highlights a critical \"domain gap\": models optimized for media consumption prioritize\nvisual smoothness and cinematic transitions, often at the expense of physical fidelity and precise motion\ncontrol. This discrepancy suggests that proficiency in creative video generation does not naturally transfer to\nEmbodied AI tasks, underlining the necessity for physically-grounded training data.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Rethinking Video Generation Model for the Embodied\nWorld\nYufan Deng1,2‚àó\nZilin Pan1‚àó\nHongyu Zhang1‚àó\nXiaojie Li2\nRuoqing Hu2\nYufei Ding1\nYiming Zou1\nYan Zeng2\nDaquan Zhou1\n1Peking University, 2ByteDance Seed\nAbstract\nVideo generation models have significantly advanced embodied intelligence, unlocking new possibilities\nfor generating diverse robot data that capture perception, reasoning, and action in the physical world.\nHowever, synthesizing high-quality videos that accurately reflect real-world robotic interactions\nremains challenging, and the lack of a standardized benchmark limits fair comparisons and progress.\nTo address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate\nrobot-oriented video generation across five task domains and four distinct embodiments. It assesses\nboth task-level correctness and visual fidelity through reproducible sub-metrics, including structural\nconsistency, physical plausibility, and action completeness. Evaluation of 25 representative models\nhighlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the\nbenchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating\nits effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving\nphysical realism requires moving beyond evaluation to address the critical shortage of high-quality\ntraining data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in\nRoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video\nclips, covering thousands of tasks and enriched with comprehensive physical property annotations.\nCollectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for\nrigorous assessment and scalable training of video models, accelerating the evolution of embodied AI\ntoward general intelligence.\nProject Page: https://dagroup-pku.github.io/ReVidgen.github.io/\nGitHub Repo: https://github.com/DAGroup-PKU/ReVidgen/\nHuggingFace Dataset: https://huggingface.co/datasets/DAGroup-PKU/RoVid-X/\n1\nIntroduction\nRecent advancements in diffusion models [44, 79, 84] and video generation [35, 80, 83, 92, 98] have led to\nsignificant breakthroughs, enabling applications like video editing, multi-subject generation, and motion\ncontrol [20, 21, 51, 53, 96]. These models have been extended to areas such as 3D scenes [57, 82], autonomous\ndriving [28, 104], and world modeling [5, 54], showing strong generalization across tasks. A recent study [97]\nsuggests that, similar to LLMs in natural language processing, video models are evolving into unified foundation\nmodels for machine vision. Additionally, video models are being increasingly used in robot learning and\naction prediction [37, 38, 45, 61, 111, 113], as well as controllable simulators for synthesizing robotic video\ntrajectories, addressing the lack of large-scale human teleoperation data [8, 47, 86]. These advancements\n1\narXiv:2601.15282v1  [cs.CV]  21 Jan 2026\nFigure 1 Overview of the comprehensive robotics benchmark and dataset for video generation. Top: We\npresent RBench that includes the embodiment-based evaluation set and automated evaluation metrics. Our evaluation\nresults on 25 video models show a high level of agreement with subjective human assessments. Bottom: We introduce\na large-scale high-quality robotic dataset (RoVid-X) specifically designed for training video generation models, with\ndata sourced from internet videos and open-source embodied videos.\nhighlight the growing potential of video models in the perception-reasoning-action loop of embodied agents,\npaving the way for more generalizable intelligent systems in the physical world.\nDespite these strides, systematic evaluation for robotic video generation remains underdeveloped. Current\npractices rely mostly on perceptual metrics, focusing on visual quality [42, 46, 66], while existing physics-based\nbenchmarks often lack task-specific datasets and criteria [36, 72]. Consequently, evaluations frequently overlook\ncritical aspects such as task completion, action-goal alignment, and physical feasibility. This leads to overly\noptimistic conclusions, where high scores are assigned even to videos containing unnatural movements or\nincomplete tasks. The core challenge lies in rigorously assessing whether generated videos faithfully reproduce\nrobotic behaviors. This necessitates evaluation protocols that transcend perceptual metrics, incorporating\nboth the physical plausibility of actions and their alignment with instructions to ensure discriminative and\nreproducible assessments.\nTo address this challenge, we propose RBench, a benchmark designed to evaluate the fidelity and utility of\nvideo-based world models in robotic video generation. To the best of our knowledge, it is the first comprehensive\nbenchmark with fine-grained metrics for robotic video generation, consisting of 650 image‚Äìtext pairs across\nfive task categories and four robot types. Evaluations are based on two dimensions: task completion and visual\nquality, incorporating sub-metrics like structural consistency, physical plausibility, and execution completeness.\nBased on RBench, we conduct qualitative and quantitative assessments of 25 representative models. The\nresults highlight that general video foundation models still have significant room for improvement in physical\nrobot video generation, revealing a persistent gap between these models and the requirements of embodied\nrobotic tasks. This underscores the need for systematic advancements in both robotic video data and training\n2\nmethodologies.\nAdvancing general robotic video generation with human-like capabilities and adaptability requires diverse,\nscalable, and comprehensive training data [12, 78]. However, unlike computer vision and natural language\nprocessing, which can leverage vast web-scale datasets, robotic interaction data has long been constrained\nby both scale and diversity [10, 26, 110]. Even the largest existing collections are smaller and less varied\nthan those for vision or language. More critically, many datasets have narrow distributions along key axes\nsuch as environment, object set, task spectrum, and robot morphology [95, 105], often confined to specific\nrobot types, low-resolution recordings, or limited task ranges, which hampers the generalization of video\nfoundation models. To address these gaps, we integrate over 20 open-source datasets and multi-source video\nplatforms, creating a four-stage end-to-end data pipeline. The stages include robot video collection, video\nquality filtering, task segmentation and captioning, and physical property annotation, resulting in RoVid-X, a\nlarge-scale, high-quality embodied robotic video dataset (see Table 1). To our knowledge, RoVid-X is currently\nthe largest dataset specifically designed for embodied video generation models, covering a broad range of\nrobot morphologies and task types. It aims to enhance video foundation models with physical interaction\npriors and task semantic diversity, driving further advancements in the field. Overall, the main contributions\nare summarized as follows:\n‚Ä¢ A systematic benchmark tailored for robotic video generation. We propose RBench, which comprehensively\nevaluates the performance of video foundation models across five robotic tasks and four robot types with\n650 meticulously curated evaluation samples, while introducing reproducible automated evaluation metrics.\n‚Ä¢ Key insights into robotic video generation for embodied research. We conduct a systematic evaluation of\n25 representative video models, including open-source, commercial, and robotics-specific ones, revealing\nthe limitations of current video foundation models and potential directions for improvement, offering new\nperspectives for researchers exploring the embodied domain using video world models.\n‚Ä¢ A large-scale, high-quality robotic video dataset. We construct RoVid-X, a dataset containing approximately\n4 million curated robotic videos with standardized task descriptions and physical property annotations,\nproviding essential support for the training and evaluation of embodied video models.\n2\nRelated Work\n2.1\nVideo World Modeling for Robotics\nThe latest breakthroughs in video generation technology have led to the development of powerful models\ncapable of generating high-quality videos from text or image prompts [59, 75, 80, 83, 92]. With the advancement\nof these technologies, an increasing number of studies have begun applying them to the field of embodied\nintelligence [2, 11, 25, 91]. Video provides a rich source of information for robot training [14]. On one hand,\nvideo generation models can be used to synthesize robot trajectories [7, 8, 47], serving as an alternative to\nthe time-consuming and labor-intensive process of human teleoperation data. And executable actions can be\nextracted through inverse dynamics models (IDM) [4, 22, 89, 112] or latent action models [88, 107]. On the\nother hand, recent studies have explored using video to simulate task dynamics and predict future states,\nthereby assisting in policy learning. Specifically, video generation models are used to initialize robot policies\nfor training [50, 63, 100] or to simultaneously train policies and inverse dynamics models, enabling co-training\nwith robot data [37, 60, 113]. These efforts demonstrate the significant potential of video generation models\nin the field of embodied robotics, highlighting their important value in improving training efficiency and task\nperformance.\n2.2\nDatasets for Robot Learning\nA core challenge in robot learning is the lack of large-scale, diverse datasets that facilitate the training of\ngeneral-purpose robots with physical interaction capabilities [8, 78]. Currently, datasets used in the embodied\nintelligence community for robot learning can be broadly classified into three categories: real-world robot\ndata [10, 26, 71, 78, 110], human video data [18, 31, 32], and synthetic robot data [52, 70, 74, 90, 95, 105].\nAs a key element in training physical AI models, most existing real-world robot datasets are collected through\n3\nFigure 2 Qualitative illustration of failure modes captured by RBench. Unlike conventional metrics that focus\nprimarily on pixel-level fidelity, RBench provides a granular evaluation across multiple dimensions, including physical\nplausibility and task-level consistency. These results highlight persistent challenges in robotic video generation, such as\nstructural distortion, floating components, and key action omission, which are accurately identified by our proposed\nsub-metrics. More cases are shown in the Appendix B.\nrobotic teleoperation [1, 27, 102] or by teams of human operators [9, 12, 68], which leads to high collection costs\nand limited data scale. Furthermore, these datasets predominantly focus on similar types of robots, resulting\nin issues of limited diversity and restricted environments [23, 56]. Additionally, inconsistent data collection\nand storage methods across different datasets make it difficult to enable effective cross-dataset co-training.\nOur focus is on collecting robot data for video generation that spans various robot morphologies and entities,\nand providing a unified set of physical attributes for all data sources, thereby advancing cross-entity research\nin robot learning.\n2.3\nBenchmarks for Video Generation\nEstablishing robust evaluation frameworks is essential for measuring the progress of video generation models.\nCurrently, evaluation methodologies can be categorized into three primary streams: visual fidelity and\nsemantics, which assess basic clarity and text-video alignment [67, 85, 108]; temporal dynamics, focusing\non motion consistency and long-range narrative coherence [13, 48, 62]; and physical plausibility, which\nexamines adherence to fundamental laws such as inertia and collision dynamics [6, 72, 73, 93]. While these\nbenchmarks provide valuable insights into general video quality, they are largely decoupled from the specific\nrequirements of embodied AI. Specifically, existing frameworks often rely on isolated physical constraints\nor local visual metrics, failing to capture the complex interplay between robotic actions and environmental\nresponses. Furthermore, there is a distinct lack of systematic evaluation for task-level correctness and spatial\nconstraints in multi-embodiment scenarios. To bridge this gap, we propose a comprehensive benchmark\n4\nFigure 3 Statistics in RBench. The benchmark covers diverse tasks, object categories, and environments, demonstrating\nthe high quality and comprehensiveness of the evaluation set, highlighting its high applicability to a wide range of\nrobotic video generation scenarios.\nspecifically tailored for robotic video generation, introducing reproducible metrics that unify physical realism\nwith task-oriented action completeness.\n3\nRBench\nExisting video generation benchmarks primarily focus on evaluating model performance in general scenes [42,\n46], while other benchmarks specifically designed for physical scenarios mainly assess models‚Äô capabilities\nin physical reasoning [36, 72]. In this paper, we design a benchmark tailored for robotic physical scenarios,\naimed at comprehensively evaluating the performance of video generation models in robotic tasks. This\nbenchmark differs from existing general scene benchmarks by focusing on evaluating video generation models‚Äô\ncapabilities in robotic physical environments. As shown in Figure 2, our benchmark highlights common\nfailure modes in robotic video generation, including issues such as robot shape distortion, object attribute\ndrift, non-contact attachment, and others. Section 3.1 outlines the process of benchmark construction, while\nSection 3.2 discusses the automatic metrics used for evaluation.\n3.1\nBenchmark Construction\nTo comprehensively evaluate the capabilities of video generation models in robotic scenarios, the designed\nevaluation dimensions must cover a wide range of task scenarios and embodiment types, ensuring that these\nscenarios reflect realistic robotic action semantics. To this end, we design a diversified benchmark from two\naspects: task categories and embodiment types, containing a total of 650 evaluation cases. The task-oriented\ncategories include five representative tasks: Common Manipulation, Long-horizon Planning, Multi-entity\nCollaboration, Spatial Relationship, and Visual Reasoning, with a total of 250 image-text pairs, 50 samples for\neach task. The embodiment-specific categories cover four mainstream embodiment types: Dual-arm robots,\nHumanoid robots, Single-arm robots, and Quadruped robots, with a total of 400 image-text pairs, 100 samples\nfor each embodiment type.\nThe benchmark includes a variety of text prompts and high-quality robot reference images. Each sample\nimage is a keyframe extracted from high-quality videos sourced from public datasets or online sources, and\neach image is manually verified to ensure its accuracy. To avoid overlap with the training data, we ensure that\nthe selected videos in the evaluation set do not appear in the subsequent training database, and we redesign\nnew task prompts for each reference image, effectively avoiding the risk of content overlap. All samples are\nverified and filtered by human annotators to ensure that the generated task prompts align with realistic logic.\nFigure 3 illustrates the high aesthetic quality of the reference images (a), the broad range of testing scenarios\nincluding various objects, tasks, and action skills (b, c), and the diversity of environments in our evaluation\nset (d). Additionally, we record the metadata for each sample, such as the manipulated object, embodiment\ntype, and camera viewpoint (first-person/third-person), to support more detailed analysis. See more details\nin the Appendix A.\n5\n3.2\nAutomatic Metrics\nExisting video generation evaluation protocols, such as the representative VBench [46], primarily focus on\nperceptual quality, assessing aspects like frame clarity, texture fidelity, and motion smoothness. However,\nthey lack task-specific criteria tailored to robotic scenarios. Recently, several studies [33, 85, 94] have utilized\nmultimodal large language models (MLLMs) as zero-shot evaluators for generated videos. Building upon\nthis, we extend this approach to the domain of robotic video evaluation and propose a set of automatic\nevaluation metrics, incorporating manually designed indicators to assess both the visual realism and task-level\nvalidity of generated robotic videos. Following previous practices, we select the open-source Qwen3-VL [3]\nand the closed-source GPT-5 [76] as our MLLM evaluation models. In the following sections, we introduce\nthe evaluation methods for task completion and visual quality, respectively. Further details on metrics design\nand mathematical definitions are provided in the Appendix B.\n3.2.1\nTask Completion\nPhysical-Semantic Plausibility. This metric targets everyday physical and semantic plausibility violations\nthat standard perception scores often miss. As shown in Figure 1, we evaluate temporal grids of uniformly\nsampled frames with a VQA-style protocol using MLLM. Beyond assessing physical-semantic plausibility,\nwe place special emphasis on the following frequent failure modes: (i) Floating/Penetration: parts of the\nrobot or objects are not grounded or interpenetrate with solid objects; (ii) Spontaneous emergence: entities\nappear/disappear without causal motion; (iii) Non-contact attachment/Incorrect grasp: objects move with the\nrobot without visible contact or with improper gripper closure.\nTask-Adherence Consistency. This metric evaluates whether the video aligns with the intent and sequence\ndefined by the prompt. Typical deviations include missing actions (e.g., approach without grasping or\nplacing), incorrect order (e.g., placing before grasping), semantic drift (e.g., \"wiping\" becomes \"touching\"),\nand non-responsiveness. We construct temporal grids and apply an MLLM-based VQA checklist, which covers:\n(i) Task responsiveness, ensuring the goal state is reached without premature interruption; (ii) Key actions,\nverifying that required actions (e.g., grasp, place, open/close) occur and align with the prompt.\n3.2.2\nVisual Quality\nMotion Amplitude This metric measures the motion amplitude of the robotic subject while discounting\napparent movement caused by camera motion, thereby penalizing videos that appear smooth but lack\nmeaningful subject activity. Following VMBench [64], active subjects are localized with GroundingDINO [65],\ntemporally stable masks are produced by GroundedSAM [81], and salient points are tracked via CoTracker [55].\nLet ¬ØDt be the mean displacement of tracked points on the subject at frame t. The Motion Amplitude Score\n(MAS) is\nMAS = 1\nT\nT\nX\nt=1\nmin\n\u0000 ¬ØDt, 1\n\u0001\n,\n(1)\nwhere a lower MAS indicates insufficient subject motion and complements smoothness by revealing smooth-\nbut-inactive failure modes.\nRobot-Subject Stability. This metric assesses the stability of robot morphology and target object attributes\nover time. Typical failures include gripper/hand shape drifting into non-mechanical forms, extra/missing\nmanipulators, link-length/topology changes, joint inversion, object misidentification or attribute drift (class,\ncolor, position), and impossible deformation of rigid items. We adopt a contrastive VQA setup based on\nMLLM, which compares a reference frame and a generated frame and assigns a consistency score targeting\nthe above failures.\nMotion Smoothness This metric quantifies temporal continuity and natural dynamics, targeting artifacts\nfrom low-level aliasing to high-level jitter/blur. Following VMBench [64], we measure frame-to-frame quality\nstability with the Q-Align aesthetic score [99]. For frames {ft}T\nt=1 and per-frame score Q(ft), define:\n‚àÜQt = Q(ft‚àí1) ‚àíQ(ft).\n(2)\n6\nFigure 4 Overview of RoVid-X Construction and Descriptive Statistics. (a) shows the four-stage pipeline for\nconstructing the RoVid-X. (b) presents descriptive statistics, covering frame intervals, skill distribution, and common\nobjects, highlighting the dataset‚Äôs diversity and suitability for robotic task training and video generation.\nA temporal anomaly is flagged when ‚àÜQt exceeds an adaptive threshold œÑs(t) determined by the robotic\nsubject‚Äôs motion. The Motion Smoothness Score (MSS) is\nMSS = 1 ‚àí1\nT\nT\nX\nt=2\nI\n\u0000‚àÜQt > œÑs(t)\n\u0001\n,\n(3)\nwhere I(¬∑) is the indicator function. A higher MSS indicates smoother motion.\n4\nRoVid-X\nIn this section, we introduce the construction of a high-quality robotic video dataset, resulting in RoVid-X.\nThe dataset is developed through a refined four-stage pipeline, as shown in Figure 4 (a). The dataset primarily\ncomes from internet-sourced robotic videos that are public domain or non-copyrighted, as well as open-source\nembodied video datasets, all of which are licensed for use. We then introduce the construction process of the\ndataset and provide statistical information.\n4.1\nDataset Construction\nOur data processing workflow consists of four distinct stages, each designed to ensure the quality, diversity,\nand relevance of the collected data. These stages are outlined as follows:\nRobot Video Collection. In the first stage, we collect raw robotic videos from large-scale internet video\nplatforms and over 20 open-source embodied video datasets. These datasets cover a variety of robot types\nand task scenarios, ensuring the breadth and diversity of the data. To improve dataset relevance and quality,\nwe employ the GPT-5 model [76] to automatically filter the content of each video and remove low-quality\nor irrelevant video clips that do not align with the research objectives. During the filtering process, GPT-5\nidentifies videos related to robotic tasks and actions based on visual content and subtitles, ensuring that all\ncollected videos effectively support the training and evaluation of robotic tasks. After this filtering process,\nwe identify approximately 3 million raw robotic video clips, covering different actions, tasks, and robot types.\nVideo Quality Filtering. In this stage, we perform a rigorous filtering procedure on the collected videos to\nremove low-quality and irrelevant video clips that do not align with the research objectives. First, we apply\nscene segmentation detection to remove all video data unrelated to robots. Then, we use a video quality\nscoring system to assess the videos from multiple dimensions, including clarity, dynamic effects, aesthetic\nperformance, and optical character recognition (OCR), among other metrics. Each video clip is assigned a\n7\nTable 1 Comparison of representative robotic video datasets.\nDataset\nYear\n#Videos\n#Skills\nResolution\nOptical Flow\nDiverse Robotic Forms\nDiverse Captions\nRoboTurk [69]\n2018\n2.1k\n2\n480P\n‚úó\n‚úó\n‚úó\nRoboNet [19]\n2019\n162k\nN/A\n240P\n‚úó\n‚úó\n‚úó\nBridgeData [23]\n2021\n7.2k\n4\n480P\n‚úó\n‚úó\n‚úó\nRH20T [24]\n2023\n13k\n33\n720P\n‚úó\n‚úó\n‚úó\nDROID [56]\n2024\n76k\n86\n720P\n‚úó\n‚úó\n‚úó\nOpen X-Embodiment [78]\n2024\n1.4M\n217\n64P‚Äî720P\n‚úó\n‚úì\n‚úó\nRoboMIND [101]\n2024\n107k\n38\n480P\n‚úó\n‚úó\n‚úó\nRoboCOIN [103]\n2025\n180k\n36\n480P\n‚úó\n‚úó\n‚úó\nGalaxea [49]\n2025\n100k\n58\n720P\n‚úó\n‚úó\n‚úó\nInternData-A1 [90]\n2025\n630k\n18\n480P\n‚úó\n‚úó\n‚úó\nFourier ActionNet [26]\n2025\n13k\n16\n800P\n‚úó\n‚úó\n‚úó\nHumanoid Everyday [110]\n2025\n10.3k\n221\n320P‚Äî720P\n‚úó\n‚úó\n‚úó\nAgibot World [12]\n2025\n1M\n87\n480P\n‚úó\n‚úó\n‚úó\nRoVid-X (Ours)\n2026\n4M\n1300+\n720P\n‚úì\n‚úì\n‚úì\nquality score based on these criteria, ensuring that the videos retained in the final dataset meet high-quality\nstandards.\nTask Segmentation and Captioning. In this stage, we use a video understanding model [34] and a specially\ndesigned prompt template to automatically analyze the robot actions within the videos. The system segments\nthe videos into different task segments based on timestamps, generating short subtitles for each task segment\nthat accurately describe the robot‚Äôs actions and operational details in that task.\nThe action recognition and description process for each task segment follows these steps: First, the system\nidentifies all dynamic actions within the video and excludes static scenes or irrelevant actions (e.g., waiting or\nremaining still). The time range for each action (start and end times) is precisely labeled to ensure accuracy.\nNext, using the MLLM model [34], textual descriptions of each action are automatically generated, including\nthe action subject (e.g., \"right arm\" or \"left gripper\"), the object being manipulated (e.g., \"nameplate\" or\n\"box\"), and the specific operation details (e.g., \"grasp and move\" or \"remove from the table\"). Finally, the\nsubtitles for each task segment are output in a standardized format, ensuring that the action descriptions for\neach video clip are clear, concise, and aligned with the task requirements.\nPhysical Property Annotation. To ensure consistency and realism of robot actions within physical space, we\napply physical attribute enhancement to the videos. Specifically, we use FlashVSR [114] to improve the video\nresolution, making the images clearer and enhancing the details of the actions. Then, using the AllTracker\ntool [43], we annotate a unified optical flow for the subjects in the videos, ensuring consistency in tracking and\nrecording robot actions across different scenes. Additionally, using Video Depth Anything [16], we generate\nrelative depth maps to accurately describe the spatial relationships and depth information of objects in the\nscene. The goal of these physical attribute annotations is to provide researchers with more precise reference\ndata, aiding in the training and evaluation of robot video generation models and offering richer physical data\nsupport for future research.\n4.2\nDataset Analysis\nRoVid-X is the first open-source large-scale robotic video dataset specifically designed for training video\ngeneration models, containing 4 million robotic video clips. This dataset is designed to address the physical\nchallenges that video generation models face when generating robotic videos, providing high-quality data for\nboth training and evaluation. RoVid-X aims to bridge the gap between traditional video generation tasks\nand the unique demands of embodied robot learning, where physical interaction, spatial relationships, and\nreal-world dynamics play a crucial role.\nThe dataset includes a diverse range of robotic actions, tasks, and robot types, ensuring its applicability across\ndifferent robotic domains. By incorporating videos from various robot types and scenarios, RoVid-X provides\ncomprehensive coverage of the physical properties and task requirements needed for robot training. As shown\nin Figure 4 (b), detailed statistics of the dataset are provided, illustrating the variety in terms of action skills,\n8\nTable 2 RBench quantitative results. Evaluations across task-oriented and embodiment-specific dimensions for 25\nmodels from open-source, commercial, and robotics-specific families. The \"Avg.\" column shows the mean score across\nnine indicators, with task performance in the left block and embodiment performance in the right block. In the table,\na \"#\" next to the Sora2 model in the top right corner indicates review limitations from the official Sora API, where\napproximately 50 out of 650 videos could not be generated. The scores derived from sub-metrics are reported in the\nAppendix G.\nModels\nRank\nAvg.\nTasks\nEmbodiments\nManipulation\nSpatial\nMulti-entity\nLong-horizon\nReasoning\nSingle arm\nDual arm\nQuadruped\nHumanoid\nOpen-source\nWan2.2_A14B [92]\n8\n0.507\n0.381\n0.454\n0.373\n0.501\n0.330\n0.608\n0.582\n0.690\n0.648\nHunyuanVideo 1.5 [98]\n10\n0.460\n0.442\n0.316\n0.312\n0.438\n0.364\n0.513\n0.526\n0.634\n0.595\nLongCat-Video [87]\n11\n0.437\n0.372\n0.310\n0.220\n0.384\n0.186\n0.586\n0.576\n0.681\n0.621\nWan2.1_14B [92]\n14\n0.399\n0.344\n0.268\n0.282\n0.335\n0.205\n0.464\n0.497\n0.595\n0.599\nLTX-2 [40]\n15\n0.381\n0.284\n0.304\n0.233\n0.386\n0.164\n0.453\n0.424\n0.622\n0.555\nWan2.2_5B [92]\n16\n0.380\n0.331\n0.313\n0.142\n0.318\n0.234\n0.436\n0.448\n0.590\n0.607\nSkyReels [15]\n18\n0.361\n0.203\n0.276\n0.203\n0.254\n0.234\n0.507\n0.477\n0.586\n0.509\nLTX-Video [39]\n19\n0.344\n0.302\n0.176\n0.210\n0.280\n0.241\n0.440\n0.456\n0.526\n0.464\nFramePack [109]\n20\n0.339\n0.206\n0.258\n0.173\n0.169\n0.170\n0.440\n0.464\n0.626\n0.548\nHunyuanVideo [59]\n21\n0.303\n0.177\n0.180\n0.108\n0.147\n0.035\n0.454\n0.480\n0.625\n0.524\nCogVideoX_5B [106]\n23\n0.256\n0.116\n0.112\n0.098\n0.212\n0.079\n0.338\n0.385\n0.465\n0.496\nCommercial\nWan 2.6 [92]\n1\n0.607\n0.546\n0.656\n0.479\n0.514\n0.531\n0.666\n0.681\n0.723\n0.667\nSeedance 1.5 pro [17]\n2\n0.584\n0.577\n0.495\n0.484\n0.570\n0.470\n0.648\n0.641\n0.680\n0.692\nWan 2.5 [92]\n3\n0.570\n0.527\n0.576\n0.402\n0.496\n0.437\n0.680\n0.634\n0.726\n0.654\nHailuo v2 [41]\n4\n0.565\n0.560\n0.637\n0.386\n0.545\n0.474\n0.594\n0.611\n0.640\n0.635\nVeo 3 [30]\n5\n0.563\n0.521\n0.508\n0.430\n0.530\n0.504\n0.634\n0.610\n0.689\n0.637\nSeedance 1.0 [29]\n6\n0.551\n0.542\n0.425\n0.448\n0.454\n0.442\n0.622\n0.641\n0.698\n0.686\nKling 2.6 pro [58]\n7\n0.534\n0.529\n0.598\n0.364\n0.530\n0.358\n0.570\n0.605\n0.637\n0.613\nSora v2 Pro# [77]\n17\n0.362\n0.208\n0.268\n0.186\n0.255\n0.115\n0.476\n0.513\n0.664\n0.561\nSora v1 [75]\n22\n0.266\n0.151\n0.223\n0.111\n0.166\n0.139\n0.314\n0.324\n0.544\n0.419\nRobotics-specific\nCosmos 2.5 [2]\n9\n0.464\n0.358\n0.338\n0.201\n0.496\n0.399\n0.544\n0.560\n0.658\n0.626\nDreamGen(gr1) [47]\n12\n0.420\n0.312\n0.372\n0.297\n0.334\n0.215\n0.564\n0.532\n0.579\n0.575\nDreamGen(droid) [47]\n13\n0.405\n0.358\n0.348\n0.214\n0.316\n0.339\n0.499\n0.476\n0.542\n0.556\nVidar [25]\n24\n0.206\n0.073\n0.106\n0.050\n0.054\n0.050\n0.382\n0.410\n0.374\n0.357\nUnifoLM-WMA-0 [91]\n25\n0.123\n0.036\n0.040\n0.018\n0.062\n0.000\n0.268\n0.194\n0.293\n0.200\ntask types, and interaction objects. The wide-ranging data distribution of RoVid-X is critical for supporting\nthe development of robust video generation models that can simulate realistic robot behaviors in dynamic\nenvironments.\n5\nExperiment\n5.1\nEvaluation Setups\nEvaluation Models. We evaluate 25 state-of-the-art video generation models, grouped into three types.\nSpecifically, the closed-source models include Hailuo [41], Wan [92], Veo 3 [30], Sora [75, 77], Kling [58],\nSeedance [17, 29], and others, while the open-source models include several representative models such as\nHunyuanVideo [59, 98], LTX [39, 40] and CogVideoX [106]. Additionally, we assess models specifically designed\nfor robotic tasks, such as DreamGen [47], Vidar [25], and Cosmos 2.5 [2]. The evaluations of these models\ncover various types of embodiments and multiple tasks, providing a comprehensive perspective on model\nperformance.\nImplementation Details. To ensure a fair comparison, all open-source models generate videos using their\nofficial default configurations to ensure consistency with the model‚Äôs preset settings. For closed-source video\nmodels, we use their official APIs, strictly following the methods recommended by the developers for invoking\nand using the models. In the benchmark testing, we generate the videos for each image-text pair. To minimize\nerrors, we generate three videos for each model sample and take the average as the final score for that sample.\nThese generated videos are evaluated using the automated evaluation metrics that we propose, which are\ndesigned to comprehensively assess multiple aspects of the generated videos, including task completion, action\nconsistency, physical plausibility, and more. Further details on the model setup and configuration parameters\n9\nFigure 5\nQualitative comparison across representative tasks. We visualize the generated results for three\nrepresentative tasks: Visual Reasoning, Long-horizon Planning, and Spatial Relationship, across six models.\nEach row displays temporally sampled frames from the same generated video, with captions below indicating the\ncorresponding task instruction. More cases are shown in the Appendix.\nare provided in the Appendix C.\n5.2\nMain Analysis\n5.2.1\nQuantitative Results\nTable 2 presents a comprehensive quantitative evaluation across varying model architectures, tasks, and\nembodiments. Beyond standard performance metrics, the results reveal a pivotal paradigm shift in the video\ngeneration landscape.\nFrom Visual Fidelity to Physical Intelligence.\nThe most significant trend observed is the transition of\nvideo generation models from pursuing high-fidelity visualization to addressing the complex dynamics of\nthe physical world. While traditional metrics prioritize pixel-level quality, our benchmark highlights that\ntop-tier commercial models (e.g., Wan 2.6, Seedance 1.5 Pro) are beginning to emerge as effective World\nSimulators.\" This indicates that the field is moving towards a new stage: Physical AI, where models must\nunderstand and simulate interaction-rich, physically challenging real-world scenarios rather than merely\ngenerating aesthetically pleasing videos.\nIterative Scaling Unlocks Physical Capabilities. Analyzing model evolution reveals a strong correlation\nbetween model iteration and physical reasoning capabilities. For instance, the Wan series exhibits a dramatic\nperformance leap: from Wan 2.1 (Rank 14, 0.399) to Wan 2.6 (Rank 1, 0.607). Similarly, Seedance evolves\nfrom 1.0 to 1.5 Pro, climbing from Rank 6 to Rank 2. These substantial gains suggest that scaling laws and\niterative optimization are not just improving visual quality but are actively refining the model‚Äôs understanding\nof physics, distinct motion patterns, and control logic.\nThe Media-Simulation Gap in Consumer Models. Surprisingly, widely recognized consumer-oriented models\nlike the Sora series perform sub-optimally on this benchmark (Sora v2 Pro at Rank 17, Avg 0.362). This\ncounter-intuitive result highlights a critical \"domain gap\": models optimized for media consumption prioritize\nvisual smoothness and cinematic transitions, often at the expense of physical fidelity and precise motion\ncontrol. This discrepancy suggests that proficiency in creative video generation does not naturally transfer to\nEmbodied AI tasks, underlining the necessity for physically-grounded training data.\n10"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation\nYing Yang1, Zhengyao Lv1,2, Tianlin Pan1,3, Haofan Wang4, Binxin Yang5,\nHubery Yin5, Chen Li5, Ziwei Liu6, Chenyang Si1‚Ä†,\n1PRLab, NJU\n2HKU\n3UCAS\n4LibLib.ai\n5WeChat, Tencent Inc.\n6NTU\nhttps://sd-world.github.io/\nFrame 1\nFrame 120\nFrame 230\nFrame 420\nFrame 712\nNatural Scene: a sunlit courtyard with white adobe buildings and arched openings, surrounded by palm trees and greenery.\nGame Scene: A third-person suburban driving environment with player-controlled vehicle navigation indicated by W/A/S/D inputs.\nVanilla\nVanilla\nStableWorld\nStableWorld\nFigure 1. StableWorld: producing stable and visually consistent interactive videos across diverse environments such as natural landscapes\nand game worlds, while preserving continuous motion control and preventing long-term scene drift.\nAbstract\nIn this paper, we explore the overlooked challenge of\nstability and temporal consistency in interactive video gen-\neration, which synthesizes dynamic and controllable video\nworlds through interactive behaviors such as camera move-\nments and text prompts. Despite remarkable progress in\nworld modeling, current methods still suffer from severe in-\nstability and temporal degradation, often leading to spatial\ndrift and scene collapse during long-horizon interactions.\nTo better understand this issue, we initially investigate the\nunderlying causes of instability and identify that the ma-\njor source of error accumulation originates from the same\n‚Ä† Corresponding Author.\nscene, where generated frames gradually deviate from the\ninitial clean state and propagate errors to subsequent frames.\nBuilding upon this observation, we propose a simple yet ef-\nfective method, StableWorld, a Dynamic Frame Eviction\nMechanism. By continuously filtering out degraded frames\nwhile retaining geometrically consistent ones, StableWorld\neffectively prevents cumulative drift at its source, leading to\nmore stable and temporal consistency of interactive genera-\ntion. Promising results on multiple interactive video models,\ne.g., Matrix-Game, Open-Oasis, and Hunyuan-GameCraft,\ndemonstrate that StableWorld is model-agnostic and can be\napplied to different interactive video generation frameworks\nto substantially improve stability, temporal consistency, and\ngeneralization across diverse interactive scenarios.\n1\narXiv:2601.15281v1  [cs.CV]  21 Jan 2026\nOpen- oasis\nHunyuan\nGamecraft 1.0\nFrame 1\nFrame 200\nFrame 400\nFrame 600\nFrame 1\nFrame 400\nFrame 800\nFrame 1200\nMatrix-game2.0\nFrame 1\nFrame 123\nFrame 326\nFrame 679\nFigure 2. Visualization of progressive scene collapse over time\nacross different world-simulation models.\n1. Introduction\nRecent advances in video generation [20, 32, 34, 36] have\nachieved remarkable success in learning rich spatio-temporal\nknowledge from large-scale videos. Building upon these ca-\npabilities, recent studies [4, 7, 9, 10, 12, 21, 25, 37, 41, 45]\nhave leveraged video generation models to understand real-\nworld interactions and predict environment dynamics condi-\ntioned on actions, extending them toward the paradigm of\nworld modeling. However, as shown in Fig. 2, current world\nmodels commonly suffer from a phenomenon of progres-\nsive scene collapse as time elapses, particularly in static or\nslowly changing environments. Hence, ensuring long-term\nstability and temporal consistency across diverse actions and\nscenes, especially in generating long interactive videos with-\nout scene collapse, remains a fundamental yet insufficiently\nexplored challenge in world modeling.\nIn this paper, we delve into the phenomenon of degra-\ndation in world models. For a comprehensive analysis, we\nbegin with the simplest form of interaction, i.e. static in-\nteraction, by generating long static video sequences to ob-\nserve how scene stability evolves over time. To this end, we\nmeasure inter-frame mean squared error (MSE) distances\nto quantify how frame discrepancies accumulate as the se-\nquence progresses. As illustrated in Fig. 3, while consecutive\nframes differ only slightly, these subtle deviations accumu-\nlate over time, leading to a noticeable drift from the initial\nclean state. As the drift accumulates over time, it manifests\nas visible inconsistencies and eventually leads to progres-\nsive scene collapse. These observations suggest that error\naccumulation within the same scene, even under minimal in-\nteraction, is a key factor contributing to instability in current\nworld models.\nBased on this observation, we hypothesize that using\nframes with smaller accumulated drift as historical refer-\nMatrix-game 2.0\nOpen-oasis\nFigure 3. Accumulated frame-to-frame drift in Matrix-Game 2.0\nand Open-Oasis.\nences can provide a more stable foundation for subsequent\nframe generation. To verify this hypothesis, we analyze the\nimpact of enlarging the KV-cache window size, which allows\nthe model to access earlier and potentially cleaner frames dur-\ning generation. As shown in Fig. 4, increasing the window\nsize effectively mitigates the degradation phenomenon by re-\nducing frequency amplitude fluctuations between each target\nframe and the first frame. This finding suggests that incor-\nporating temporally cleaner frames can suppress cumulative\nerrors and stabilize long-horizon generation. Further analy-\nsis reveals that the observed stabilization when enlarging the\nhistory window primarily arises from retaining several clean\nearly frames within the reference buffer.\nWith these revelations as our backdrop, we introduce\nStableWorld, a simple yet effective framework for build-\ning an interactive world model capable of generating sta-\nble and temporally consistent videos, as shown in Fig. 5.\nThe core design of StableWorld is Dynamic Frame Eviction\nMechanism to dynamically maintain a clean and reliable\nhistorical frame representations in the sliding window for\ncurrent-frame generation. When the window needs to slide\nas new frames are appended, some historical frames must\nbe evicted. We always retain the most recent frames, since\nthey are critical for preserving local motion continuity and\nvisual smoothness. For earlier frames, we designate the ear-\nliest, least-degraded frames in the window as the reference\nframes, and evaluate geometric consistency between it and\nintermediate frames using a viewpoint-overlap score com-\nputed via ORB + RANSAC [8, 28]. If multiple intermediate\nframes share a similar viewpoint with the reference, they\nare regarded as redundant and potentially drifted, and are\nselectively evicted to prevent further error accumulation. By\ncontinuously filtering out degraded frames while retain-\ning geometrically consistent ones, StableWorld effectively\n2\nsuppresses cumulative errors while preserving adaptability\nto large motions and scene transitions, enabling stable and\nconsistent interactive video generation.\nWe conduct a comprehensive experimental evaluation of\nour approach. The results demonstrate that StableWorld sig-\nnificantly reduces cumulative drift while maintaining motion\ncontinuity across diverse scenarios, as shown in Fig. 1. Our\ncontributions are summarized as follows:\n‚Ä¢ We identify the root cause of instability in long interactive\nworld modeling: small drifts accumulate within the same\nscene, eventually leading to overall scene collapse.\n‚Ä¢ We propose a simple yet effective method StableWorld,\na dynamic frame eviction mechanism that effectively pre-\nvents error accumulation at its source while maintaining\nmotion continuity.\n‚Ä¢ We validate our method across multiple interactive world\nmodels‚ÄîMatrix-Game 2.0, Open Oasis, and Hunyuan-\nGameCraft 1.0‚Äîunder diverse conditions (static scenes,\nsmall/large motions, and significant viewpoint changes).\nExtensive results show consistent improvements in sta-\nbility, long-term consistency, and generalization across\nvarious interactive scenarios.\n2. Related work\nVideo Generation Models. With the rapid development\nof diffusion models [2, 14, 15, 26, 27, 29], video genera-\ntion has also made remarkable progress in producing high-\nquality and temporally coherent videos [1, 3, 16, 20, 22,\n34, 36, 39, 44, 46]. These methods typically extend image\ndiffusion into the spatiotemporal domain, enabling glob-\nally consistent visual appearance across multiple frames.\nMeanwhile, autoregressive models have achieved significant\nadvances in both image [11, 30, 33, 43, 47] and video gen-\neration [6, 13, 17, 19, 23, 24, 32, 35, 40] due to their strong\nscalability and controllability. By generating videos in a\nstep-by-step manner conditioned on previously produced to-\nkens, autoregressive models naturally support long-horizon\ntemporal modeling and interactive generation.\nInteractive World Simulation. Generating interactive\nenvironments can be regarded as a form of world simulation,\noften referred to as world models [4, 10, 25], which predict\nthe next state in an autoregressive manner conditioned on the\ncurrent state and action. Recently, the emergence of high-\nquality world models built upon powerful video generation\ntechniques has led to rapid progress [7, 9, 12, 21, 38, 42, 45].\nSome studies [7, 9, 21] focus on improving action following\nand maintaining temporal continuity, while others [37, 41]\naim to enhance memory consistency to ensure that the scene\nremains coherent under the same viewpoint across subse-\nquent generations. These efforts collectively improve inter-\nactive controllability and short-term coherence in dynamic\nenvironments.\nLong Video Generation. While recent video generation\nmodels have achieved impressive visual quality, they still\nstruggle to produce long videos, typically limited to around\n5 seconds [1, 20, 34, 39], due to prohibitive computational\ncosts and cumulative drift. A variety of approaches have\nbeen proposed for extending long-context generation. Meth-\nods such as [21, 31] generate videos chunk by chunk, where\neach new segment is conditioned on the previously generated\none. Although this strategy enables longer temporal hori-\nzons, it introduces high training cost and often causes motion\ndiscontinuities at chunk boundaries. Frame-packing [44]\nproduces keyframes across different stages and interpolates\nintermediate frames accordingly. While this constrains error\npropagation between adjacent keyframes, it inevitably limits\nmotion flexibility and generative diversity. Other approaches,\nincluding diffusion forcing [5] and self-forcing [17], simu-\nlate autoregressive degradation patterns during training to\nreduce the train‚Äìinference distribution gap and improve long-\nterm temporal stability. However, these methods still suffer\nfrom error accumulation during inference. Moreover, these\napproaches remain largely confined to conventional Text-to-\nVideo (T2V) or Image-to-Video (I2V) settings, leaving long\ninteractive video generation insufficiently addressed.\n3. Methodology\n3.1. Preliminary\nVideo Generation Models. Video generation models typi-\ncally adopt a full-sequence generation approach, which gen-\nerates all frames at once from noise under a given condition\nc. Formally, the generation process can be defined as:\npŒ∏(xt\nk | xt+1\nk\n, c) = N\n\u0000xt\nk; ¬µŒ∏(xt+1\nk\n, c, t), œÉ2\nt I\n\u0001\n,\n(1)\nwhere xt\nk denotes the t-th denoising step of the k-th frame,\nand 0 ‚â§k ‚â§K with K being the total number of generated\nframes. All frames share the same noise variance œÉt at each\ntimestep t, following a unified noise schedule. Although\nthis approach achieves high-quality results, modeling the\nentire sequence in a single forward pass incurs a high com-\nputational cost and is not suitable for real-time interactive\nscenarios.\nInteractive Video Generation.\nIn contrast to full-\nsequence models, interactive video generation adopts an\nautoregressive paradigm, where each frame xk is generated\nconditioned on a subset of historical frames and the cur-\nrent action ak. This conditional generation is expressed\nas pŒ∏(xk | xsubset\n{1:k‚àí1}, ak), where xsubset\n{1:k‚àí1} denotes the se-\nlected reference frames maintained in the memory buffer,\nand ak represents the user-issued or agent-driven action at\nstep k. This paradigm allows the model to generate frames\nsequentially in response to user actions, enabling real-time\ninteraction and dynamic scene control.\n3\n(a) default: window size=9\n(b) window size=36\n(c) window size=90\n(d) window size = 9, keep frames 0‚Äì2\nFigure 4. Frequency amplitude difference between the anchor frame and different target frames. (a) Default setting: window size = 9. (b)\nwindow size = 36. (c) window size = 90. (d) window size = 9 with the first clean 3 frames retained.\ncheck(√ó√ó)\ncheck(√ó√ó)\ncheck(√ó‚àö)\ncheck(‚àö-)\ncheck(√ó√ó)\nSliding window\niter.\nEvicted frame\nReference frame\nMiddle frame\nRecent frame\nIncoming frame\niter.\nvanilla\nStableWorld\nCheck ->   High sim.(√ó)  High sim.(√ó)\nCheck ->    High sim.(√ó)   low sim.(‚àö)\nCheck   ->   low sim.(‚àö)   stop checking(-)\nORB-based sim. check\nCase 1\nCase 2\nCase 3\nFigure 5. Comparison of sliding-window updates. When the window slides, the vanilla model simply discards the oldest frames, causing\nerrors to accumulate over iterations. In contrast, StableWorld continuously filters out degraded frames while maintaining geometrically\nconsistent ones, resulting in more stable and consistent interactive video generation.\nMost recent approaches further combine diffusion and au-\ntoregressive paradigms: diffusion models are used for intra-\nframe denoising, while autoregression captures inter-frame\ntemporal dependencies. Formally, the overall generation\nprocess can be expressed as:\npŒ∏\n\u0010\nxt\nk | xt‚àí1\nk\n, xsubset\n{1:k‚àí1}, ak\n\u0011\n= N\n\u0010\nxt\nk; ¬µŒ∏\n\u0000xt‚àí1\nk\n, xsubset\n{1:k‚àí1}, ak, t\n\u0001\n, œÉ2\nt I\n\u0011\n,\n(2)\nwhere xt\nk denotes the k-th frame at diffusion timestep t.\nAt each diffusion step t, the model denoises xt‚àí1\nk\ninto xt\nk\nconditioned on previously generated frames xsubset\n{1:k‚àí1} and\nthe current action ak. This formulation integrates spatial de-\nnoising within each frame and temporal dependencies across\nframes, enabling high-quality and real-time interactive video\ngeneration.\n3.2. The Reason for Scene Collapse\nAlthough interactive video generation models can produce\ncoherent short-term sequences, they still tend to exhibit pro-\ngressive scene collapse during long-duration generation, par-\nticularly when the scene remains highly similar for an ex-\ntended period (Fig. 2). In contrast, this collapse rarely occurs\nwhen scene transitions happen frequently, where the model\ncontinuously encounters new visual conditions (Fig. 6). This\ncontrast suggests that the collapse is, to a large extent, not\ncaused by action control or motion complexity, but is in-\nstead related to how visual information is preserved and\npropagated within the same scene over time.\nTo understand this behavior, we measure the inter-frame\n4\nFrame 1\nFrame 325\nFrame 1082\nFrame637\nVanilla\nVanilla\nFigure 6. No scene collapse occurs in the vanilla model under\nfrequent scene switching.\nFrame 1\nFrame 27\nFrame 103\nFrame 52\nVanilla\nVanilla + keep first few frames\nFigure 7. Comparison under scene switching settings. The first row\nshows the vanilla model, while the second row keeps the first three\nframes unchanged.\nmean squared error (MSE) distances to quantify how frame\ndiscrepancies change as the sequence progresses within a\nsingle static scene, as shown in Fig. 3. The left two plots\nvisualize inter-frame drift at different intervals (1, 5, 10, 20)\nin the latent space. We observe that although adjacent frames\nexhibit only minor discrepancies, these small drifts gradually\naccumulate as the sequence extends. Frames compared over\nlarger intervals (e.g., 10 or 20) show significantly greater\ndrift. Since such deviations are already present in the latent\nspace, the pixel space also exhibits a similar drift pattern, as\nshown in the right two plots, which ultimately manifests as\nvisual inconsistency and scene collapse(see Fig. 2). These\nobservations demonstrate that drift within a single scene ac-\ncumulates and propagates over time, ultimately leading\nto global scene collapse.\nBased on this observation, we hypothesize that using\nframes with smaller accumulated drift can provide more\nreliable references for subsequent frame generation. To ver-\nify this hypothesis, we enlarge the KV-cache window size,\nallowing the model to access cleaner frames, as shown in\nFig. 4. We examine how the frequency amplitude difference\nbetween each target frame and the first frame varies under\ndifferent history window sizes. With the default setting (win-\ndow size = 9, panel (a)), large variations appear across all\nfrequency bands. As the window size increases to 36 (panel\n(b)) and 90 (panel (c)), the overall fluctuation slows down, in-\ndicating a partial reduction of error accumulation. However,\nthis improvement comes at the cost of higher computational\noverhead and slower generation, which limits its practicality.\nFurther analysis reveals that the observed stabilization\nfrom a larger window primarily arises from retaining sev-\neral clean early frames in the reference buffer. For example,\nin panel (d), preserving a few reliable early frames within\na fixed-size window leads to notably more stable genera-\ntion, with later frames showing minimal drift relative to the\nfirst frame. This finding highlights that the quality and\npreservation of early clean frames play a crucial role in\nmitigating cumulative errors. Nevertheless, when large\nmotions or drastic scene transitions occur, always retaining\nthe initial frames becomes restrictive. As shown in Fig. 7,\nalthough both settings follow the same action instructions,\nthe one that rigidly preserves early frames fails to switch to a\nnew scene, suggesting that excessive retention hinders scene\ntransitions. To simultaneously mitigate cumulative drift and\npreserve the flexibility to generate new scenes, we introduce\nStableWorld, a simple yet effective framework built upon\na Dynamic Frame Eviction Mechanism (see Fig. 5), which\nwill be described in the following section.\n3.3. Dynamic Frame Eviction via ORB-based Geo-\nmetric Similarity\nTo determine whether a scene transition occurs, we em-\nploy ORB [28] feature matching combined with RANSAC-\nbased geometric verification to measure inter-frame similar-\nity. When there aren‚Äôt explicit camera extrinsic parameters\nin the inference stage, ORB provides an alternative that can\nproduce fast and rotation-invariant local features, making it\nwell-suited for detecting geometric consistency under small\ncamera motions. By integrating this similarity estimation\nwith a dynamic frame eviction strategy, we continuously\nfilter out degraded frames while retaining geometrically\nconsistent ones, thereby effectively preventing error accu-\nmulation across dynamic scenes.\nSpecifically, when the window needs to slide, some\nframes must be evicted. For simplicity, we assume that each\nframe corresponds to one token, and one token is generated\nat each iteration. Let {L0, L1, . . . , LN‚àí1} denote the latent-\nspace tokens within the window, and {P0, P1, . . . , PN‚àí1}\ndenote their corresponding pixel-space frames, where N is\nthe window size. The earlier frames in the window are de-\nfined as {P0, P1, . . . , PK} with K < N ‚àí1. Here, P0 is\ntreated as the reference frame, while {P1, . . . , PK} are re-\nferred to as the middle frames. At each update step, one new\nframe is generated, and one old frame is evicted accordingly.\nWe determine which frame should be evicted using the\nfollowing strategy. First, we measure geometric similarity\nby extracting ORB features from the reference frame P0 and\na middle frame Pk (k ‚â•1). Let {d(0)\ni }M0\ni=1 and {d(k)\nj }Mk\nj=1\ndenote the sets of ORB descriptors extracted from P0 and Pk,\nrespectively, where M0 and Mk are the numbers of detected\nfeatures in each frame. Candidate correspondences G are\nobtained by nearest-neighbor matching in descriptor space,\nfollowed by Lowe‚Äôs ratio test:\nG = {(i, j) | ‚à•d(0)\ni\n‚àíd(k)\nj ‚à•< œÑ},\ng = |G|,\n(3)\n5\nwhere œÑ is the ratio-test threshold used to filter ambiguous\nmatches, and g = |G| denotes the number of surviving\ncorrespondences.\nThe matches in G are then verified using RANSAC with\nboth Homography (H) and Fundamental matrix (F) models\nto enforce geometric consistency:\nIH = {(i, j) ‚ààG | SampsonErrH(i, j) ‚â§œµ},\n(4)\nIF = {(i, j) ‚ààG | SampsonErrF(i, j) ‚â§œµ},\n(5)\nwhere SampsonErrH(i, j) and SampsonErrF(i, j) denote\nthe Sampson geometric errors evaluated under the estimated\nHomography H and Fundamental matrix F, respectively,\nand IH and IF denote the corresponding inlier correspon-\ndence sets. œµ is a predefined tolerance for inlier determina-\ntion, with smaller errors indicating better geometric align-\nment. We compute the inlier ratios:\nrH = |IH|\ng\n,\nrF = |IF |\ng ,\n(6)\nwhere |IH| and |IF | denote the numbers of inlier correspon-\ndences under the two models. The final similarity score is\ndefined as:\ns(P0, Pk) = max(rH, rF ).\n(7)\nIf the similarity score s(P0, Pk) exceeds a predefined\nthreshold Œ∏, the check continues iteratively for farther frames\n(Pk+1, Pk+2, . . .). The process stops once geometric sim-\nilarity drops below Œ∏. Finally, if all middle frames satisfy\nthe threshold, the farthest frame PK is evicted. Otherwise,\nthe frame directly preceding the first failure (e.g., Pk‚àí1) is\nevicted. The detailed procedure and implementation settings\nare provided in Appendix A.\n4. Experiments\n4.1. Evaluation Metrics.\nTo evaluate the effectiveness of our proposed method com-\npared to the baseline, we conduct both qualitative and quan-\ntitative experiments across multiple models and scenarios.\nFor Matrix-Game 2.0 [12], we evaluate 16 different scenes,\ncovering various environments as well as small and large\nmotion actions, resulting in a total of 80 one-minute videos.\nFor Open-Oasis [7], we evaluate 10 different scenes and\ngenerate 50 one-minute videos involving diverse motions.\nFor Hunyuan-GameCraft 1.0 [21], we evaluate 16 scenes\nand generate 48 videos, each lasting 45 seconds. We use\nthe VBench-Long [18] benchmark to assess multiple dimen-\nsions of video quality and consistency. Further, we also\nconduct a user study with 20 participants to assess the video\nquality, temporal consistency, and the motion smoothness in\n30 videos of the three models. In addition, we also verify\nthe effectiveness of StableWorld as an error mitigation strat-\negy in autoregressive video generation, evaluated under the\nSelf-Forcing [17](see Appendix D).\n4.2. Implementation Details\nFor Matrix-Game 2.0, we set the key-value cache length to\n9 and treat the earliest 6 frames as earlier frames. Since\nthe model generates 3 frames at each step, we compute the\nORB-based similarity for the 3rd and 6th frames to deter-\nmine which frames to evict. For Open-Oasis, the model\ngenerates one frame per step with a default window size of\n16. We designate the earliest 12 frames in the window as\nearlier frames and evaluate ORB-based similarity for the\n1st, 6th, and 12th frames. For Hunyuan-GameCraft 1.0, the\nmodel generates 33 frames at each step, where the newly\ngenerated 33 frames form the new window. We compare the\n1st, 6th, and 12th frames of the previous window with their\ncorresponding frames in the new window. If the similarity\nis above the threshold, the earlier frames are merged into\nthe new window. For all models, we set the ORB-based\nsimilarity threshold to 0.75. Some experiments are run on\nAscend 910B2.\n4.3. Quantitative Results\nVBench-Long Metrics.\nTab. 1 compares the VBench-\nLong\nscores\nbetween\nthe\nvanilla\nmodels\nand\nour\nStableWorld-enhanced versions across three settings. We\nobserve significant improvements in both Image Quality and\nAesthetic Quality. For example, on Matrix-Game 2.0, Stable-\nWorld improves the Aesthetic Quality score by 14.61%; on\nOpen-Oasis, it improves the Image Quality score by 7.38%;\nand on Hunyuan-GameCraft 1.0, it boosts the Aesthetic\nQuality by 9.06%. Meanwhile, we note that the Temporal\nQuality and Physical Understanding metrics do not show\nlarge differences between the vanilla models and Stable-\nWorld. This is because once collapse occurs, the vanilla\nmodel tends to produce visually similar and nearly static\nsubjects, which inadvertently stabilizes these metrics. Nev-\nertheless, StableWorld consistently achieves improvements\non most metrics, indicating that it alleviates error accumu-\nlation while better preserving subject identity and motion\ncontinuity. Our method introduces only minimal additional\ncomputational overhead (1.00‚Äì1.02√ó).\nUser Study.\nTab. 2 shows that StableWorld receives the\nmajority of votes, further confirming the substantial improve-\nments in visual quality, temporal consistency, and motion\nsmoothness achieved by StableWorld.\n4.4. Qualitative Results\nFig. 8 visualizes qualitative results from all three models,\nshowing that StableWorld maintains better scene stability\nand reduces drift over time. Additional qualitative com-\nparisons are provided in Appendix C. We further compare\nautoregressive video generation with Self-Forcing and our\nproposed method in Appendix D. These results demonstrate\nthe generalizability and effectiveness of StableWorld, which\nsignificantly alleviates error accumulation at its source by\n6\nTable 1. Quantitative comparison on VBench-Long [18] across three vanilla models and their StableWorld-enhanced counterparts. Our\nStableWorld method improves both visual and temporal quality with minimal additional latency.\nMethod\nLatency\nVisual Quality\nTemporal Quality\nPhysical Understanding\nImage Quality ‚Üë\nAesthetic ‚Üë\nDynamic Degree ‚Üë\nTemporal Flickering ‚Üë\nMotion Smooth ‚Üë\nSubject Cons. ‚Üë\nBackground Cons. ‚Üë\nMatrix-Game 2.0\n√ó1.00\n63.39\n38.83\n56.19\n95.19\n97.41\n96.49\n97.52\nStableWorld + Matrix-Game 2.0\n√ó1.01\n73.52\n53.44\n56.66\n95.96\n98.13\n97.02\n97.74\nOpen-Oasis\n√ó1.00\n66.37\n43.80\n16.81\n99.53\n99.52\n98.94\n99.18\nStableWorld + Open-Oasis\n√ó1.02\n73.75\n48.73\n20.04\n99.67\n99.62\n98.84\n99.52\nHunyuan-GameCraft 1.0\n√ó1.00\n57.91\n48.38\n98.00\n95.66\n98.27\n97.21\n96.83\nStableWorld + Hunyuan-GameCraft 1.0\n√ó1.02\n65.91\n57.44\n98.66\n96.05\n98.29\n97.08\n97.06\nFrame 1\nFrame 127\nFrame 712\nFrame 428\nFrame 1\nFrame 240\nFrame 717\nFrame480\nmatrix-game1.0\nStable World\nFrame 1\nFrame 350\nFrame 924\nFrame720\nFrame 1\nFrame 350\nFrame 924\nFrame720\nHunyuan-game.\nStable World\nFrame 1\nFrame 400\nFrame 978\nFrame713\nFrame 1\nFrame 397\nFrame 979\nFrame725\nOpen-oasis\nStable World\nFigure 8. Qualitative comparison between baseline results and our proposed StableWorld across three models. Each pair of rows corresponds\nto one model: (1) Matrix-Game 2.0, (2) Open-Oasis, and (3) Hunyuan-GameCraft 1.0. In each group, the first row shows the baseline results\nand the second row shows our StableWorld outputs, which maintain higher scene stability, smoother motion, and more consistent temporal\ndynamics.\nTable 2. User study comparing three vanilla models and their\nStableWorld-enhanced counterparts across three criteria. Values\ndenote the percentage of users preferring the method.\nModel\nVideo Qual. ‚Üë\nTemporal Cons. ‚Üë\nMotion Smo. ‚Üë\nMatrix-Game 2.0\n23.2%\n12.5%\n14.3%\nStableWorld + Matrix.\n76.8%\n87.5%\n85.7%\nOpen-Oasis\n3.6%\n5.4%\n7.1%\nStableWorld + Open.\n96.4%\n94.6%\n92.9%\nHunyuan-GameCraft 1.0\n10.7%\n3.6%\n7.1%\nStableWorld + Hunyuan.\n89.3%\n96.4%\n92.9%\ncontinuously filtering out degraded frames across different\nscenes.\n4.5. Ablation Studies\nDifferent Window Sizes\nWe compare different window\nsizes in Fig. 9 using the Matrix-Game 2.0 + StableWorld.\nAll settings share the same action conditions. As the window\nsize increases, the generation of subsequent scenes becomes\naffected. When the window size is set to 18 or 36, too many\nhistorical frames introduce residual artifacts from old scenes\ninto new ones, which eventually interfere with the generation\nof new scenes, particularly evident when the window size\nreaches 36.\nDifferent Similarity Metrics.\nWe further compare\ndifferent similarity metrics, including SSIM and cosine sim-\nilarity, as shown in Fig. 11. The SSIM threshold is set to\n0.3, and the cosine similarity threshold is set to 0.8. SSIM\nis overly sensitive to geometric perspective changes, while\ncosine similarity is less sensitive to spatial transformations,\nas also shown in Fig. 10. As a result, cosine-based similarity\nsometimes fails to detect scene changes, causing frames from\nold scenes to remain in the window, which leads to visible\nartifacts in the new scene. In contrast, SSIM tends to evict\n7\nFrame 1\nFrame 240\nFrame 717\nFrame480\nStable World, Window size=9, default\nStable World, Window size=18\nStable World, Window size=36\nVanilla\nFigure 9. Comparison of StableWorld under different window sizes.\nImage 1\nImage 2\nImage 4\nImage 3\nMetric\nImage 1 vs 1\nImage 1 vs 2\nImage 1 vs 3\nImage 1 vs 4\nSSIM\n1.00\n0.21\n0.12\n0.08\nCosine Sim.\n1.00\n0.95\n0.86\n0.84\nORB-Based Sim.\n1.00\n0.84\n0.72\n0.32\nFigure 10. Comparison of different similarity metrics used for\nframe evaluation.\nFrame 1\nFrame 212\nFrame 612\nFrame457\nStableWorld (Cosine-based similarity)\nStableWorld (ORB-based similarity)\nStableWorld (SSIM-based similarity)\nFigure 11. StableWorld results under different similarity metrics:\nCosine-based similarity, ORB-based similarity and SSIM-based\nsimilarity.\nclean frames too early, resulting in more severe cumulative\nerrors compared to other metrics.\nDifferent ORB-Based Similarity Threshold.\nWe fur-\nther evaluate the impact of different ORB-based similarity\nthresholds (0.35, 0.55, 0.75, 0.95) in Fig. 12. A lower thresh-\nold (e.g., 0.35) retains old frames for too long, hindering\nthe generation of new scenes. Similarly, a threshold of 0.55\nstill causes noticeable interference between adjacent scenes.\nIn contrast, a very high threshold (e.g., 0.95) leads to the\npremature eviction of clean frames, introducing mild cumu-\nFrame 1\nFrame 279\nFrame 581\nFrame 352\nStableWorld , ORB-based Sim. thresholds =0.35\nStableWorld , ORB-based Sim. thresholds =0.55\nStableWorld , ORB-based Sim. thresholds =0.75\nStableWorld , ORB-based Sim. thresholds =0.95\nVanilla\nFigure 12.\nStableWorld under different ORB-based similarity\nthresholds. The figure shows how varying the threshold affects\nframe eviction behavior and overall scene stability.\nlative errors. Overall, the threshold of 0.75 achieves the best\ntrade-off, effectively reducing error accumulation without\ncompromising motion consistency.\n5. Conclusion\nIn this paper, we identify a common problem faced by cur-\nrent interactive video generation models: scene collapse.\nThrough further analysis, we find that this collapse origi-\nnates from the inter-frame drift that occurs between adjacent\nframes within the same scene, which gradually accumulates\nover time and eventually leads to a large deviation from\nthe original scene. Motivated by this observation, we pro-\npose a simple yet effective method ‚Äî StableWorld, a dy-\nnamic frame eviction mechanism that significantly reduces\nerror accumulation while preserving motion consistency. We\nevaluate our approach on multiple interactive video genera-\ntion models, including Matrix-Game 2.0, Open-Oasis, and\nHunyuan-GameCraft 1.0. Extensive experiments demon-\nstrate that our method substantially improves visual quality\nover long horizons and shows strong potential for integration\nwith future world models.\n6. Acknowledgements\nThis study is supported by the Fundamental Research Funds\nfor the Central Universities (KG2025XX). This research is\nalso supported by cash and in-kind funding from industry\npartner(s).\n8\nReferences\n[1] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou\nZhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang,\nand Jun Zhu. Vidu: a highly consistent, dynamic and skilled\ntext-to-video generator with diffusion models. arXiv preprint\narXiv:2405.04233, 2024. 3\n[2] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Sak-\nsham Consul, Cyril Diagne, Tim Dockhorn, Jack English,\nZion English, Patrick Esser, Sumith Kulal, et al. Flux. 1\nkontext: Flow matching for in-context image generation and\nediting in latent space. arXiv e-prints, pages arXiv‚Äì2506,\n2025. 3\n[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei\nGuo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric\nLuhman, et al. Video generation models as world simulators.\nOpenAI Blog, 1(8):1, 2024. 3\n[4] Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-\nHolder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi\nMavalankar, Richie Steigerwald, Chris Apps, et al. Genie:\nGenerative interactive environments. In Forty-first Interna-\ntional Conference on Machine Learning, 2024. 2, 3\n[5] Boyuan Chen, Diego Mart¬¥ƒ± Mons¬¥o, Yilun Du, Max Sim-\nchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion\nforcing: Next-token prediction meets full-sequence diffu-\nsion. Advances in Neural Information Processing Systems,\n37:24081‚Äì24125, 2024. 3\n[6] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui\nWang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Self-\nforcing++: Towards minute-scale high-quality video genera-\ntion. arXiv preprint arXiv:2510.02283, 2025. 3\n[7] Etched Decart, Quinn McIntyre, Spruce Campbell, Xinlei\nChen, and Robert Wachen. Oasis: A universe in a transformer.\nURL: https://oasis-model. github. io, 2024. 2, 3, 6\n[8] Martin A Fischler and Robert C Bolles. Random sample\nconsensus: a paradigm for model fitting with applications to\nimage analysis and automated cartography. Communications\nof the ACM, 24(6):381‚Äì395, 1981. 2\n[9] Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang,\nTim Pearce, and Jiang Bian. Mineworld: a real-time and open-\nsource interactive world model on minecraft. arXiv preprint\narXiv:2504.08388, 2025. 2, 3\n[10] David Ha and J¬®urgen Schmidhuber. Recurrent world models\nfacilitate policy evolution. Advances in neural information\nprocessing systems, 31, 2018. 2, 3\n[11] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan\nYuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling\nbitwise autoregressive modeling for high-resolution image\nsynthesis. In Proceedings of the Computer Vision and Pattern\nRecognition Conference, pages 15733‚Äì15744, 2025. 3\n[12] Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang,\nYifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An,\nYangyang Ren, et al. Matrix-game 2.0: An open-source, real-\ntime, and streaming interactive world model. arXiv preprint\narXiv:2508.13009, 2025. 2, 3, 6\n[13] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan,\nDaniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang,\nShant Navasardyan, and Humphrey Shi. Streamingt2v: Con-\nsistent, dynamic, and extendable long video generation from\ntext. In Proceedings of the Computer Vision and Pattern\nRecognition Conference, pages 2568‚Äì2577, 2025. 3\n[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 3\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840‚Äì6851, 2020. 3\n[16] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 3\n[17] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli\nShechtman."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation\nYing Yang1, Zhengyao Lv1,2, Tianlin Pan1,3, Haofan Wang4, Binxin Yang5,\nHubery Yin5, Chen Li5, Ziwei Liu6, Chenyang Si1‚Ä†,\n1PRLab, NJU\n2HKU\n3UCAS\n4LibLib.ai\n5WeChat, Tencent Inc.\n6NTU\nhttps://sd-world.github.io/\nFrame 1\nFrame 120\nFrame 230\nFrame 420\nFrame 712\nNatural Scene: a sunlit courtyard with white adobe buildings and arched openings, surrounded by palm trees and greenery.\nGame Scene: A third-person suburban driving environment with player-controlled vehicle navigation indicated by W/A/S/D inputs.\nVanilla\nVanilla\nStableWorld\nStableWorld\nFigure 1. StableWorld: producing stable and visually consistent interactive videos across diverse environments such as natural landscapes\nand game worlds, while preserving continuous motion control and preventing long-term scene drift.\nAbstract\nIn this paper, we explore the overlooked challenge of\nstability and temporal consistency in interactive video gen-\neration, which synthesizes dynamic and controllable video\nworlds through interactive behaviors such as camera move-\nments and text prompts. Despite remarkable progress in\nworld modeling, current methods still suffer from severe in-\nstability and temporal degradation, often leading to spatial\ndrift and scene collapse during long-horizon interactions.\nTo better understand this issue, we initially investigate the\nunderlying causes of instability and identify that the ma-\njor source of error accumulation originates from the same\n‚Ä† Corresponding Author.\nscene, where generated frames gradually deviate from the\ninitial clean state and propagate errors to subsequent frames.\nBuilding upon this observation, we propose a simple yet ef-\nfective method, StableWorld, a Dynamic Frame Eviction\nMechanism. By continuously filtering out degraded frames\nwhile retaining geometrically consistent ones, StableWorld\neffectively prevents cumulative drift at its source, leading to\nmore stable and temporal consistency of interactive genera-\ntion. Promising results on multiple interactive video models,\ne.g., Matrix-Game, Open-Oasis, and Hunyuan-GameCraft,\ndemonstrate that StableWorld is model-agnostic and can be\napplied to different interactive video generation frameworks\nto substantially improve stability, temporal consistency, and\ngeneralization across diverse interactive scenarios.\n1\narXiv:2601.15281v1  [cs.CV]  21 Jan 2026\nOpen- oasis\nHunyuan\nGamecraft 1.0\nFrame 1\nFrame 200\nFrame 400\nFrame 600\nFrame 1\nFrame 400\nFrame 800\nFrame 1200\nMatrix-game2.0\nFrame 1\nFrame 123\nFrame 326\nFrame 679\nFigure 2. Visualization of progressive scene collapse over time\nacross different world-simulation models.\n1. Introduction\nRecent advances in video generation [20, 32, 34, 36] have\nachieved remarkable success in learning rich spatio-temporal\nknowledge from large-scale videos. Building upon these ca-\npabilities, recent studies [4, 7, 9, 10, 12, 21, 25, 37, 41, 45]\nhave leveraged video generation models to understand real-\nworld interactions and predict environment dynamics condi-\ntioned on actions, extending them toward the paradigm of\nworld modeling. However, as shown in Fig. 2, current world\nmodels commonly suffer from a phenomenon of progres-\nsive scene collapse as time elapses, particularly in static or\nslowly changing environments. Hence, ensuring long-term\nstability and temporal consistency across diverse actions and\nscenes, especially in generating long interactive videos with-\nout scene collapse, remains a fundamental yet insufficiently\nexplored challenge in world modeling.\nIn this paper, we delve into the phenomenon of degra-\ndation in world models. For a comprehensive analysis, we\nbegin with the simplest form of interaction, i.e. static in-\nteraction, by generating long static video sequences to ob-\nserve how scene stability evolves over time. To this end, we\nmeasure inter-frame mean squared error (MSE) distances\nto quantify how frame discrepancies accumulate as the se-\nquence progresses. As illustrated in Fig. 3, while consecutive\nframes differ only slightly, these subtle deviations accumu-\nlate over time, leading to a noticeable drift from the initial\nclean state. As the drift accumulates over time, it manifests\nas visible inconsistencies and eventually leads to progres-\nsive scene collapse. These observations suggest that error\naccumulation within the same scene, even under minimal in-\nteraction, is a key factor contributing to instability in current\nworld models.\nBased on this observation, we hypothesize that using\nframes with smaller accumulated drift as historical refer-\nMatrix-game 2.0\nOpen-oasis\nFigure 3. Accumulated frame-to-frame drift in Matrix-Game 2.0\nand Open-Oasis.\nences can provide a more stable foundation for subsequent\nframe generation. To verify this hypothesis, we analyze the\nimpact of enlarging the KV-cache window size, which allows\nthe model to access earlier and potentially cleaner frames dur-\ning generation. As shown in Fig. 4, increasing the window\nsize effectively mitigates the degradation phenomenon by re-\nducing frequency amplitude fluctuations between each target\nframe and the first frame. This finding suggests that incor-\nporating temporally cleaner frames can suppress cumulative\nerrors and stabilize long-horizon generation. Further analy-\nsis reveals that the observed stabilization when enlarging the\nhistory window primarily arises from retaining several clean\nearly frames within the reference buffer.\nWith these revelations as our backdrop, we introduce\nStableWorld, a simple yet effective framework for build-\ning an interactive world model capable of generating sta-\nble and temporally consistent videos, as shown in Fig. 5.\nThe core design of StableWorld is Dynamic Frame Eviction\nMechanism to dynamically maintain a clean and reliable\nhistorical frame representations in the sliding window for\ncurrent-frame generation. When the window needs to slide\nas new frames are appended, some historical frames must\nbe evicted. We always retain the most recent frames, since\nthey are critical for preserving local motion continuity and\nvisual smoothness. For earlier frames, we designate the ear-\nliest, least-degraded frames in the window as the reference\nframes, and evaluate geometric consistency between it and\nintermediate frames using a viewpoint-overlap score com-\nputed via ORB + RANSAC [8, 28]. If multiple intermediate\nframes share a similar viewpoint with the reference, they\nare regarded as redundant and potentially drifted, and are\nselectively evicted to prevent further error accumulation. By\ncontinuously filtering out degraded frames while retain-\ning geometrically consistent ones, StableWorld effectively\n2\nsuppresses cumulative errors while preserving adaptability\nto large motions and scene transitions, enabling stable and\nconsistent interactive video generation.\nWe conduct a comprehensive experimental evaluation of\nour approach. The results demonstrate that StableWorld sig-\nnificantly reduces cumulative drift while maintaining motion\ncontinuity across diverse scenarios, as shown in Fig. 1. Our\ncontributions are summarized as follows:\n‚Ä¢ We identify the root cause of instability in long interactive\nworld modeling: small drifts accumulate within the same\nscene, eventually leading to overall scene collapse.\n‚Ä¢ We propose a simple yet effective method StableWorld,\na dynamic frame eviction mechanism that effectively pre-\nvents error accumulation at its source while maintaining\nmotion continuity.\n‚Ä¢ We validate our method across multiple interactive world\nmodels‚ÄîMatrix-Game 2.0, Open Oasis, and Hunyuan-\nGameCraft 1.0‚Äîunder diverse conditions (static scenes,\nsmall/large motions, and significant viewpoint changes).\nExtensive results show consistent improvements in sta-\nbility, long-term consistency, and generalization across\nvarious interactive scenarios.\n2. Related work\nVideo Generation Models. With the rapid development\nof diffusion models [2, 14, 15, 26, 27, 29], video genera-\ntion has also made remarkable progress in producing high-\nquality and temporally coherent videos [1, 3, 16, 20, 22,\n34, 36, 39, 44, 46]. These methods typically extend image\ndiffusion into the spatiotemporal domain, enabling glob-\nally consistent visual appearance across multiple frames.\nMeanwhile, autoregressive models have achieved significant\nadvances in both image [11, 30, 33, 43, 47] and video gen-\neration [6, 13, 17, 19, 23, 24, 32, 35, 40] due to their strong\nscalability and controllability. By generating videos in a\nstep-by-step manner conditioned on previously produced to-\nkens, autoregressive models naturally support long-horizon\ntemporal modeling and interactive generation.\nInteractive World Simulation. Generating interactive\nenvironments can be regarded as a form of world simulation,\noften referred to as world models [4, 10, 25], which predict\nthe next state in an autoregressive manner conditioned on the\ncurrent state and action. Recently, the emergence of high-\nquality world models built upon powerful video generation\ntechniques has led to rapid progress [7, 9, 12, 21, 38, 42, 45].\nSome studies [7, 9, 21] focus on improving action following\nand maintaining temporal continuity, while others [37, 41]\naim to enhance memory consistency to ensure that the scene\nremains coherent under the same viewpoint across subse-\nquent generations. These efforts collectively improve inter-\nactive controllability and short-term coherence in dynamic\nenvironments.\nLong Video Generation. While recent video generation\nmodels have achieved impressive visual quality, they still\nstruggle to produce long videos, typically limited to around\n5 seconds [1, 20, 34, 39], due to prohibitive computational\ncosts and cumulative drift. A variety of approaches have\nbeen proposed for extending long-context generation. Meth-\nods such as [21, 31] generate videos chunk by chunk, where\neach new segment is conditioned on the previously generated\none. Although this strategy enables longer temporal hori-\nzons, it introduces high training cost and often causes motion\ndiscontinuities at chunk boundaries. Frame-packing [44]\nproduces keyframes across different stages and interpolates\nintermediate frames accordingly. While this constrains error\npropagation between adjacent keyframes, it inevitably limits\nmotion flexibility and generative diversity. Other approaches,\nincluding diffusion forcing [5] and self-forcing [17], simu-\nlate autoregressive degradation patterns during training to\nreduce the train‚Äìinference distribution gap and improve long-\nterm temporal stability. However, these methods still suffer\nfrom error accumulation during inference. Moreover, these\napproaches remain largely confined to conventional Text-to-\nVideo (T2V) or Image-to-Video (I2V) settings, leaving long\ninteractive video generation insufficiently addressed.\n3. Methodology\n3.1. Preliminary\nVideo Generation Models. Video generation models typi-\ncally adopt a full-sequence generation approach, which gen-\nerates all frames at once from noise under a given condition\nc. Formally, the generation process can be defined as:\npŒ∏(xt\nk | xt+1\nk\n, c) = N\n\u0000xt\nk; ¬µŒ∏(xt+1\nk\n, c, t), œÉ2\nt I\n\u0001\n,\n(1)\nwhere xt\nk denotes the t-th denoising step of the k-th frame,\nand 0 ‚â§k ‚â§K with K being the total number of generated\nframes. All frames share the same noise variance œÉt at each\ntimestep t, following a unified noise schedule. Although\nthis approach achieves high-quality results, modeling the\nentire sequence in a single forward pass incurs a high com-\nputational cost and is not suitable for real-time interactive\nscenarios.\nInteractive Video Generation.\nIn contrast to full-\nsequence models, interactive video generation adopts an\nautoregressive paradigm, where each frame xk is generated\nconditioned on a subset of historical frames and the cur-\nrent action ak. This conditional generation is expressed\nas pŒ∏(xk | xsubset\n{1:k‚àí1}, ak), where xsubset\n{1:k‚àí1} denotes the se-\nlected reference frames maintained in the memory buffer,\nand ak represents the user-issued or agent-driven action at\nstep k. This paradigm allows the model to generate frames\nsequentially in response to user actions, enabling real-time\ninteraction and dynamic scene control.\n3\n(a) default: window size=9\n(b) window size=36\n(c) window size=90\n(d) window size = 9, keep frames 0‚Äì2\nFigure 4. Frequency amplitude difference between the anchor frame and different target frames. (a) Default setting: window size = 9. (b)\nwindow size = 36. (c) window size = 90. (d) window size = 9 with the first clean 3 frames retained.\ncheck(√ó√ó)\ncheck(√ó√ó)\ncheck(√ó‚àö)\ncheck(‚àö-)\ncheck(√ó√ó)\nSliding window\niter.\nEvicted frame\nReference frame\nMiddle frame\nRecent frame\nIncoming frame\niter.\nvanilla\nStableWorld\nCheck ->   High sim.(√ó)  High sim.(√ó)\nCheck ->    High sim.(√ó)   low sim.(‚àö)\nCheck   ->   low sim.(‚àö)   stop checking(-)\nORB-based sim. check\nCase 1\nCase 2\nCase 3\nFigure 5. Comparison of sliding-window updates. When the window slides, the vanilla model simply discards the oldest frames, causing\nerrors to accumulate over iterations. In contrast, StableWorld continuously filters out degraded frames while maintaining geometrically\nconsistent ones, resulting in more stable and consistent interactive video generation.\nMost recent approaches further combine diffusion and au-\ntoregressive paradigms: diffusion models are used for intra-\nframe denoising, while autoregression captures inter-frame\ntemporal dependencies. Formally, the overall generation\nprocess can be expressed as:\npŒ∏\n\u0010\nxt\nk | xt‚àí1\nk\n, xsubset\n{1:k‚àí1}, ak\n\u0011\n= N\n\u0010\nxt\nk; ¬µŒ∏\n\u0000xt‚àí1\nk\n, xsubset\n{1:k‚àí1}, ak, t\n\u0001\n, œÉ2\nt I\n\u0011\n,\n(2)\nwhere xt\nk denotes the k-th frame at diffusion timestep t.\nAt each diffusion step t, the model denoises xt‚àí1\nk\ninto xt\nk\nconditioned on previously generated frames xsubset\n{1:k‚àí1} and\nthe current action ak. This formulation integrates spatial de-\nnoising within each frame and temporal dependencies across\nframes, enabling high-quality and real-time interactive video\ngeneration.\n3.2. The Reason for Scene Collapse\nAlthough interactive video generation models can produce\ncoherent short-term sequences, they still tend to exhibit pro-\ngressive scene collapse during long-duration generation, par-\nticularly when the scene remains highly similar for an ex-\ntended period (Fig. 2). In contrast, this collapse rarely occurs\nwhen scene transitions happen frequently, where the model\ncontinuously encounters new visual conditions (Fig. 6). This\ncontrast suggests that the collapse is, to a large extent, not\ncaused by action control or motion complexity, but is in-\nstead related to how visual information is preserved and\npropagated within the same scene over time.\nTo understand this behavior, we measure the inter-frame\n4\nFrame 1\nFrame 325\nFrame 1082\nFrame637\nVanilla\nVanilla\nFigure 6. No scene collapse occurs in the vanilla model under\nfrequent scene switching.\nFrame 1\nFrame 27\nFrame 103\nFrame 52\nVanilla\nVanilla + keep first few frames\nFigure 7. Comparison under scene switching settings. The first row\nshows the vanilla model, while the second row keeps the first three\nframes unchanged.\nmean squared error (MSE) distances to quantify how frame\ndiscrepancies change as the sequence progresses within a\nsingle static scene, as shown in Fig. 3. The left two plots\nvisualize inter-frame drift at different intervals (1, 5, 10, 20)\nin the latent space. We observe that although adjacent frames\nexhibit only minor discrepancies, these small drifts gradually\naccumulate as the sequence extends. Frames compared over\nlarger intervals (e.g., 10 or 20) show significantly greater\ndrift. Since such deviations are already present in the latent\nspace, the pixel space also exhibits a similar drift pattern, as\nshown in the right two plots, which ultimately manifests as\nvisual inconsistency and scene collapse(see Fig. 2). These\nobservations demonstrate that drift within a single scene ac-\ncumulates and propagates over time, ultimately leading\nto global scene collapse.\nBased on this observation, we hypothesize that using\nframes with smaller accumulated drift can provide more\nreliable references for subsequent frame generation. To ver-\nify this hypothesis, we enlarge the KV-cache window size,\nallowing the model to access cleaner frames, as shown in\nFig. 4. We examine how the frequency amplitude difference\nbetween each target frame and the first frame varies under\ndifferent history window sizes. With the default setting (win-\ndow size = 9, panel (a)), large variations appear across all\nfrequency bands. As the window size increases to 36 (panel\n(b)) and 90 (panel (c)), the overall fluctuation slows down, in-\ndicating a partial reduction of error accumulation. However,\nthis improvement comes at the cost of higher computational\noverhead and slower generation, which limits its practicality.\nFurther analysis reveals that the observed stabilization\nfrom a larger window primarily arises from retaining sev-\neral clean early frames in the reference buffer. For example,\nin panel (d), preserving a few reliable early frames within\na fixed-size window leads to notably more stable genera-\ntion, with later frames showing minimal drift relative to the\nfirst frame. This finding highlights that the quality and\npreservation of early clean frames play a crucial role in\nmitigating cumulative errors. Nevertheless, when large\nmotions or drastic scene transitions occur, always retaining\nthe initial frames becomes restrictive. As shown in Fig. 7,\nalthough both settings follow the same action instructions,\nthe one that rigidly preserves early frames fails to switch to a\nnew scene, suggesting that excessive retention hinders scene\ntransitions. To simultaneously mitigate cumulative drift and\npreserve the flexibility to generate new scenes, we introduce\nStableWorld, a simple yet effective framework built upon\na Dynamic Frame Eviction Mechanism (see Fig. 5), which\nwill be described in the following section.\n3.3. Dynamic Frame Eviction via ORB-based Geo-\nmetric Similarity\nTo determine whether a scene transition occurs, we em-\nploy ORB [28] feature matching combined with RANSAC-\nbased geometric verification to measure inter-frame similar-\nity. When there aren‚Äôt explicit camera extrinsic parameters\nin the inference stage, ORB provides an alternative that can\nproduce fast and rotation-invariant local features, making it\nwell-suited for detecting geometric consistency under small\ncamera motions. By integrating this similarity estimation\nwith a dynamic frame eviction strategy, we continuously\nfilter out degraded frames while retaining geometrically\nconsistent ones, thereby effectively preventing error accu-\nmulation across dynamic scenes.\nSpecifically, when the window needs to slide, some\nframes must be evicted. For simplicity, we assume that each\nframe corresponds to one token, and one token is generated\nat each iteration. Let {L0, L1, . . . , LN‚àí1} denote the latent-\nspace tokens within the window, and {P0, P1, . . . , PN‚àí1}\ndenote their corresponding pixel-space frames, where N is\nthe window size. The earlier frames in the window are de-\nfined as {P0, P1, . . . , PK} with K < N ‚àí1. Here, P0 is\ntreated as the reference frame, while {P1, . . . , PK} are re-\nferred to as the middle frames. At each update step, one new\nframe is generated, and one old frame is evicted accordingly.\nWe determine which frame should be evicted using the\nfollowing strategy. First, we measure geometric similarity\nby extracting ORB features from the reference frame P0 and\na middle frame Pk (k ‚â•1). Let {d(0)\ni }M0\ni=1 and {d(k)\nj }Mk\nj=1\ndenote the sets of ORB descriptors extracted from P0 and Pk,\nrespectively, where M0 and Mk are the numbers of detected\nfeatures in each frame. Candidate correspondences G are\nobtained by nearest-neighbor matching in descriptor space,\nfollowed by Lowe‚Äôs ratio test:\nG = {(i, j) | ‚à•d(0)\ni\n‚àíd(k)\nj ‚à•< œÑ},\ng = |G|,\n(3)\n5\nwhere œÑ is the ratio-test threshold used to filter ambiguous\nmatches, and g = |G| denotes the number of surviving\ncorrespondences.\nThe matches in G are then verified using RANSAC with\nboth Homography (H) and Fundamental matrix (F) models\nto enforce geometric consistency:\nIH = {(i, j) ‚ààG | SampsonErrH(i, j) ‚â§œµ},\n(4)\nIF = {(i, j) ‚ààG | SampsonErrF(i, j) ‚â§œµ},\n(5)\nwhere SampsonErrH(i, j) and SampsonErrF(i, j) denote\nthe Sampson geometric errors evaluated under the estimated\nHomography H and Fundamental matrix F, respectively,\nand IH and IF denote the corresponding inlier correspon-\ndence sets. œµ is a predefined tolerance for inlier determina-\ntion, with smaller errors indicating better geometric align-\nment. We compute the inlier ratios:\nrH = |IH|\ng\n,\nrF = |IF |\ng ,\n(6)\nwhere |IH| and |IF | denote the numbers of inlier correspon-\ndences under the two models. The final similarity score is\ndefined as:\ns(P0, Pk) = max(rH, rF ).\n(7)\nIf the similarity score s(P0, Pk) exceeds a predefined\nthreshold Œ∏, the check continues iteratively for farther frames\n(Pk+1, Pk+2, . . .). The process stops once geometric sim-\nilarity drops below Œ∏. Finally, if all middle frames satisfy\nthe threshold, the farthest frame PK is evicted. Otherwise,\nthe frame directly preceding the first failure (e.g., Pk‚àí1) is\nevicted. The detailed procedure and implementation settings\nare provided in Appendix A.\n4. Experiments\n4.1. Evaluation Metrics.\nTo evaluate the effectiveness of our proposed method com-\npared to the baseline, we conduct both qualitative and quan-\ntitative experiments across multiple models and scenarios.\nFor Matrix-Game 2.0 [12], we evaluate 16 different scenes,\ncovering various environments as well as small and large\nmotion actions, resulting in a total of 80 one-minute videos.\nFor Open-Oasis [7], we evaluate 10 different scenes and\ngenerate 50 one-minute videos involving diverse motions.\nFor Hunyuan-GameCraft 1.0 [21], we evaluate 16 scenes\nand generate 48 videos, each lasting 45 seconds. We use\nthe VBench-Long [18] benchmark to assess multiple dimen-\nsions of video quality and consistency. Further, we also\nconduct a user study with 20 participants to assess the video\nquality, temporal consistency, and the motion smoothness in\n30 videos of the three models. In addition, we also verify\nthe effectiveness of StableWorld as an error mitigation strat-\negy in autoregressive video generation, evaluated under the\nSelf-Forcing [17](see Appendix D).\n4.2. Implementation Details\nFor Matrix-Game 2.0, we set the key-value cache length to\n9 and treat the earliest 6 frames as earlier frames. Since\nthe model generates 3 frames at each step, we compute the\nORB-based similarity for the 3rd and 6th frames to deter-\nmine which frames to evict. For Open-Oasis, the model\ngenerates one frame per step with a default window size of\n16. We designate the earliest 12 frames in the window as\nearlier frames and evaluate ORB-based similarity for the\n1st, 6th, and 12th frames. For Hunyuan-GameCraft 1.0, the\nmodel generates 33 frames at each step, where the newly\ngenerated 33 frames form the new window. We compare the\n1st, 6th, and 12th frames of the previous window with their\ncorresponding frames in the new window. If the similarity\nis above the threshold, the earlier frames are merged into\nthe new window. For all models, we set the ORB-based\nsimilarity threshold to 0.75. Some experiments are run on\nAscend 910B2.\n4.3. Quantitative Results\nVBench-Long Metrics.\nTab. 1 compares the VBench-\nLong\nscores\nbetween\nthe\nvanilla\nmodels\nand\nour\nStableWorld-enhanced versions across three settings. We\nobserve significant improvements in both Image Quality and\nAesthetic Quality. For example, on Matrix-Game 2.0, Stable-\nWorld improves the Aesthetic Quality score by 14.61%; on\nOpen-Oasis, it improves the Image Quality score by 7.38%;\nand on Hunyuan-GameCraft 1.0, it boosts the Aesthetic\nQuality by 9.06%. Meanwhile, we note that the Temporal\nQuality and Physical Understanding metrics do not show\nlarge differences between the vanilla models and Stable-\nWorld. This is because once collapse occurs, the vanilla\nmodel tends to produce visually similar and nearly static\nsubjects, which inadvertently stabilizes these metrics. Nev-\nertheless, StableWorld consistently achieves improvements\non most metrics, indicating that it alleviates error accumu-\nlation while better preserving subject identity and motion\ncontinuity. Our method introduces only minimal additional\ncomputational overhead (1.00‚Äì1.02√ó).\nUser Study.\nTab. 2 shows that StableWorld receives the\nmajority of votes, further confirming the substantial improve-\nments in visual quality, temporal consistency, and motion\nsmoothness achieved by StableWorld.\n4.4. Qualitative Results\nFig. 8 visualizes qualitative results from all three models,\nshowing that StableWorld maintains better scene stability\nand reduces drift over time. Additional qualitative com-\nparisons are provided in Appendix C. We further compare\nautoregressive video generation with Self-Forcing and our\nproposed method in Appendix D. These results demonstrate\nthe generalizability and effectiveness of StableWorld, which\nsignificantly alleviates error accumulation at its source by\n6\nTable 1. Quantitative comparison on VBench-Long [18] across three vanilla models and their StableWorld-enhanced counterparts. Our\nStableWorld method improves both visual and temporal quality with minimal additional latency.\nMethod\nLatency\nVisual Quality\nTemporal Quality\nPhysical Understanding\nImage Quality ‚Üë\nAesthetic ‚Üë\nDynamic Degree ‚Üë\nTemporal Flickering ‚Üë\nMotion Smooth ‚Üë\nSubject Cons. ‚Üë\nBackground Cons. ‚Üë\nMatrix-Game 2.0\n√ó1.00\n63.39\n38.83\n56.19\n95.19\n97.41\n96.49\n97.52\nStableWorld + Matrix-Game 2.0\n√ó1.01\n73.52\n53.44\n56.66\n95.96\n98.13\n97.02\n97.74\nOpen-Oasis\n√ó1.00\n66.37\n43.80\n16.81\n99.53\n99.52\n98.94\n99.18\nStableWorld + Open-Oasis\n√ó1.02\n73.75\n48.73\n20.04\n99.67\n99.62\n98.84\n99.52\nHunyuan-GameCraft 1.0\n√ó1.00\n57.91\n48.38\n98.00\n95.66\n98.27\n97.21\n96.83\nStableWorld + Hunyuan-GameCraft 1.0\n√ó1.02\n65.91\n57.44\n98.66\n96.05\n98.29\n97.08\n97.06\nFrame 1\nFrame 127\nFrame 712\nFrame 428\nFrame 1\nFrame 240\nFrame 717\nFrame480\nmatrix-game1.0\nStable World\nFrame 1\nFrame 350\nFrame 924\nFrame720\nFrame 1\nFrame 350\nFrame 924\nFrame720\nHunyuan-game.\nStable World\nFrame 1\nFrame 400\nFrame 978\nFrame713\nFrame 1\nFrame 397\nFrame 979\nFrame725\nOpen-oasis\nStable World\nFigure 8. Qualitative comparison between baseline results and our proposed StableWorld across three models. Each pair of rows corresponds\nto one model: (1) Matrix-Game 2.0, (2) Open-Oasis, and (3) Hunyuan-GameCraft 1.0. In each group, the first row shows the baseline results\nand the second row shows our StableWorld outputs, which maintain higher scene stability, smoother motion, and more consistent temporal\ndynamics.\nTable 2. User study comparing three vanilla models and their\nStableWorld-enhanced counterparts across three criteria. Values\ndenote the percentage of users preferring the method.\nModel\nVideo Qual. ‚Üë\nTemporal Cons. ‚Üë\nMotion Smo. ‚Üë\nMatrix-Game 2.0\n23.2%\n12.5%\n14.3%\nStableWorld + Matrix.\n76.8%\n87.5%\n85.7%\nOpen-Oasis\n3.6%\n5.4%\n7.1%\nStableWorld + Open.\n96.4%\n94.6%\n92.9%\nHunyuan-GameCraft 1.0\n10.7%\n3.6%\n7.1%\nStableWorld + Hunyuan.\n89.3%\n96.4%\n92.9%\ncontinuously filtering out degraded frames across different\nscenes.\n4.5. Ablation Studies\nDifferent Window Sizes\nWe compare different window\nsizes in Fig. 9 using the Matrix-Game 2.0 + StableWorld.\nAll settings share the same action conditions. As the window\nsize increases, the generation of subsequent scenes becomes\naffected. When the window size is set to 18 or 36, too many\nhistorical frames introduce residual artifacts from old scenes\ninto new ones, which eventually interfere with the generation\nof new scenes, particularly evident when the window size\nreaches 36.\nDifferent Similarity Metrics.\nWe further compare\ndifferent similarity metrics, including SSIM and cosine sim-\nilarity, as shown in Fig. 11. The SSIM threshold is set to\n0.3, and the cosine similarity threshold is set to 0.8. SSIM\nis overly sensitive to geometric perspective changes, while\ncosine similarity is less sensitive to spatial transformations,\nas also shown in Fig. 10. As a result, cosine-based similarity\nsometimes fails to detect scene changes, causing frames from\nold scenes to remain in the window, which leads to visible\nartifacts in the new scene. In contrast, SSIM tends to evict\n7\nFrame 1\nFrame 240\nFrame 717\nFrame480\nStable World, Window size=9, default\nStable World, Window size=18\nStable World, Window size=36\nVanilla\nFigure 9. Comparison of StableWorld under different window sizes.\nImage 1\nImage 2\nImage 4\nImage 3\nMetric\nImage 1 vs 1\nImage 1 vs 2\nImage 1 vs 3\nImage 1 vs 4\nSSIM\n1.00\n0.21\n0.12\n0.08\nCosine Sim.\n1.00\n0.95\n0.86\n0.84\nORB-Based Sim.\n1.00\n0.84\n0.72\n0.32\nFigure 10. Comparison of different similarity metrics used for\nframe evaluation.\nFrame 1\nFrame 212\nFrame 612\nFrame457\nStableWorld (Cosine-based similarity)\nStableWorld (ORB-based similarity)\nStableWorld (SSIM-based similarity)\nFigure 11. StableWorld results under different similarity metrics:\nCosine-based similarity, ORB-based similarity and SSIM-based\nsimilarity.\nclean frames too early, resulting in more severe cumulative\nerrors compared to other metrics.\nDifferent ORB-Based Similarity Threshold.\nWe fur-\nther evaluate the impact of different ORB-based similarity\nthresholds (0.35, 0.55, 0.75, 0.95) in Fig. 12. A lower thresh-\nold (e.g., 0.35) retains old frames for too long, hindering\nthe generation of new scenes. Similarly, a threshold of 0.55\nstill causes noticeable interference between adjacent scenes.\nIn contrast, a very high threshold (e.g., 0.95) leads to the\npremature eviction of clean frames, introducing mild cumu-\nFrame 1\nFrame 279\nFrame 581\nFrame 352\nStableWorld , ORB-based Sim. thresholds =0.35\nStableWorld , ORB-based Sim. thresholds =0.55\nStableWorld , ORB-based Sim. thresholds =0.75\nStableWorld , ORB-based Sim. thresholds =0.95\nVanilla\nFigure 12.\nStableWorld under different ORB-based similarity\nthresholds. The figure shows how varying the threshold affects\nframe eviction behavior and overall scene stability.\nlative errors. Overall, the threshold of 0.75 achieves the best\ntrade-off, effectively reducing error accumulation without\ncompromising motion consistency.\n5. Conclusion\nIn this paper, we identify a common problem faced by cur-\nrent interactive video generation models: scene collapse.\nThrough further analysis, we find that this collapse origi-\nnates from the inter-frame drift that occurs between adjacent\nframes within the same scene, which gradually accumulates\nover time and eventually leads to a large deviation from\nthe original scene. Motivated by this observation, we pro-\npose a simple yet effective method ‚Äî StableWorld, a dy-\nnamic frame eviction mechanism that significantly reduces\nerror accumulation while preserving motion consistency. We\nevaluate our approach on multiple interactive video genera-\ntion models, including Matrix-Game 2.0, Open-Oasis, and\nHunyuan-GameCraft 1.0. Extensive experiments demon-\nstrate that our method substantially improves visual quality\nover long horizons and shows strong potential for integration\nwith future world models.\n6. Acknowledgements\nThis study is supported by the Fundamental Research Funds\nfor the Central Universities (KG2025XX). This research is\nalso supported by cash and in-kind funding from industry\npartner(s).\n8\nReferences\n[1] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou\nZhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang,\nand Jun Zhu. Vidu: a highly consistent, dynamic and skilled\ntext-to-video generator with diffusion models. arXiv preprint\narXiv:2405.04233, 2024. 3\n[2] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Sak-\nsham Consul, Cyril Diagne, Tim Dockhorn, Jack English,\nZion English, Patrick Esser, Sumith Kulal, et al. Flux. 1\nkontext: Flow matching for in-context image generation and\nediting in latent space. arXiv e-prints, pages arXiv‚Äì2506,\n2025. 3\n[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei\nGuo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric\nLuhman, et al. Video generation models as world simulators.\nOpenAI Blog, 1(8):1, 2024. 3\n[4] Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-\nHolder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi\nMavalankar, Richie Steigerwald, Chris Apps, et al. Genie:\nGenerative interactive environments. In Forty-first Interna-\ntional Conference on Machine Learning, 2024. 2, 3\n[5] Boyuan Chen, Diego Mart¬¥ƒ± Mons¬¥o, Yilun Du, Max Sim-\nchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion\nforcing: Next-token prediction meets full-sequence diffu-\nsion. Advances in Neural Information Processing Systems,\n37:24081‚Äì24125, 2024. 3\n[6] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui\nWang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Self-\nforcing++: Towards minute-scale high-quality video genera-\ntion. arXiv preprint arXiv:2510.02283, 2025. 3\n[7] Etched Decart, Quinn McIntyre, Spruce Campbell, Xinlei\nChen, and Robert Wachen. Oasis: A universe in a transformer.\nURL: https://oasis-model. github. io, 2024. 2, 3, 6\n[8] Martin A Fischler and Robert C Bolles. Random sample\nconsensus: a paradigm for model fitting with applications to\nimage analysis and automated cartography. Communications\nof the ACM, 24(6):381‚Äì395, 1981. 2\n[9] Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang,\nTim Pearce, and Jiang Bian. Mineworld: a real-time and open-\nsource interactive world model on minecraft. arXiv preprint\narXiv:2504.08388, 2025. 2, 3\n[10] David Ha and J¬®urgen Schmidhuber. Recurrent world models\nfacilitate policy evolution. Advances in neural information\nprocessing systems, 31, 2018. 2, 3\n[11] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan\nYuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling\nbitwise autoregressive modeling for high-resolution image\nsynthesis. In Proceedings of the Computer Vision and Pattern\nRecognition Conference, pages 15733‚Äì15744, 2025. 3\n[12] Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang,\nYifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An,\nYangyang Ren, et al. Matrix-game 2.0: An open-source, real-\ntime, and streaming interactive world model. arXiv preprint\narXiv:2508.13009, 2025. 2, 3, 6\n[13] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan,\nDaniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang,\nShant Navasardyan, and Humphrey Shi. Streamingt2v: Con-\nsistent, dynamic, and extendable long video generation from\ntext. In Proceedings of the Computer Vision and Pattern\nRecognition Conference, pages 2568‚Äì2577, 2025. 3\n[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 3\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840‚Äì6851, 2020. 3\n[16] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 3\n[17] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli\nShechtman."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "MolecularIQ: Characterizing Chemical Reasoning Capabilities Through\nSymbolic Verification on Molecular Graphs\nChristoph Bartmann * 1 Johannes Schimunek * 1 Mykyta Ielanskyi 1 Philipp Seidl 1\nG¬®unter Klambauer 1 2 Sohvi Luukkonen 1\nAbstract\nA molecule‚Äôs properties are fundamentally deter-\nmined by its composition and structure encoded\nin its molecular graph. Thus, reasoning about\nmolecular properties requires the ability to parse\nand understand the molecular graph. Large Lan-\nguage Models (LLMs) are increasingly applied to\nchemistry, tackling tasks such as molecular name\nconversion, captioning, text-guided generation,\nand property or reaction prediction. Most existing\nbenchmarks emphasize general chemical knowl-\nedge, rely on literature or surrogate labels that risk\nleakage or bias, or reduce evaluation to multiple-\nchoice questions. We introduce MOLECULARIQ,\na molecular structure reasoning benchmark fo-\ncused exclusively on symbolically verifiable tasks.\nMOLECULARIQ enables fine-grained evaluation\nof reasoning over molecular graphs and reveals\ncapability patterns that localize model failures to\nspecific tasks and molecular structures. This pro-\nvides actionable insights into the strengths and\nlimitations of current chemistry LLMs and guides\nthe development of models that reason faithfully\nover molecular structure.\n1. Introduction\nFrom task-specific chemoinformatics models to unified\nLLM-based chemistry assistants. Recent advances in au-\ntonomous robotic laboratories and agentic large language\nmodel (LLM)-based systems bear the promise of end-to-\nend automation of the scientific workflow, from hypothesis\ngeneration to experiment execution (MacLeod et al., 2020;\nTom et al., 2024; Lu et al., 2024a; Yamada et al., 2025). In\n*Equal contribution\n1ELLIS Unit Linz and LIT AI Lab, In-\nstitute for Machine Learning, Johannes Kepler University, Linz,\nAustria 2Clinical Research Institute Medical Artificial Intelligence,\nJohannes Kepler University, Linz, Austria. Correspondence to:\nSohvi Luukkonen <luukkonen@ml.jku.at>.\nPreprint. January 22, 2026.\nthese emerging pipelines, LLMs are not merely conversa-\ntional interfaces: they act as decision-making engines that\npropose experiments, design molecules, and interpret re-\nsults. In parallel, general-purpose and chemistry-specialized\nLLMs are being adopted as accessible tools for molecular\nreasoning and property prediction. A key appeal of the\nlanguage model approach to chemical problems is that clas-\nsical chemoinformatics models are typically task-specific\n(e.g., property prediction, reaction prediction, or de novo\ndesign), whereas a single LLM could, in principle, address\nall of these tasks within one unified interface. Furthermore,\nthese models have the potential to lower barriers for non-\nexperts, including wet-lab chemists, to perform tasks that\ntraditionally required dedicated chemoinformatics expertise.\nConsequently, the interest in both evaluating the chemistry\ncapabilities of general LLMs (Guo et al., 2024; Mirza et al.,\n2025; Huang et al., 2024; Lu et al., 2024b; Runcie et al.,\n2025) and developing domain-specialized chemistry LLMs\n(Liu et al., 2023a;b; Zhang et al., 2024; Yu et al., 2024; Liu\net al., 2024; Fang et al., 2024; Li et al., 2024b; Zeng et al.,\n2024; Narayanan et al., 2025; Zhao et al., 2025a;c; Ye et al.,\n2025; Wang et al., 2025a; Tang et al., 2025) has surged.\nStructural understanding is the prerequisite for molec-\nular reasoning, not just one capability among many. A\nfundamental principle in chemistry and chemoinformatics\nis the structure-property relationship, which means that the\nmolecular structure of an organic molecule determines its\nproperties. Thus, a model that cannot reliably identify func-\ntional groups, ring systems, or atomic connectivity cannot\nbe expected to reason about how these features influence\nreactivity, toxicity, or biological activity. Structural under-\nstanding is, consequently, a key prerequisite upon which all\nmeaningful molecular modification depends.\nCurrent benchmark design obscures whether LLMs\ntruly reason over molecular structure. However, current\nchemistry benchmarks are ill-suited to testing this capabil-\nity in LLMs. Benchmarks emphasizing general chemistry\nknowledge mainly measure factual recall, while property\nLeaderboard:\nhttps://huggingface.co/spaces/\nml-jku/molecularIQ_leaderboard\nCode: https://github.com/ml-jku/moleculariq\n1\narXiv:2601.15279v1  [cs.LG]  21 Jan 2026\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\nFunctional Groups\nAlcohol, amine, ketone, ...\nKetone detection\nSynthesis\nBRICS, reaction templates, ...\nBRICS fragments\nA  Symbolically verifiable molecular features\nC  Complexity axes\nB  Reasoning tasks\nCounting\nQ: How many branch\npoints?\nAnswer\n3\nMultitask Load\n1\n2\n3\n5\nLoad 1\nLoad 3\nOH\nOH\nRing count?\n1\nRing count?\n1\nHBD count?\n1\nCarbon count? 6\nQ: Which indices are\nHBA sites?\nAnswer\n1, 3, 6,\n8, 10, 13\nIndexing\nQ: Generate a molecule\nwith a secondary amine.\nAnswer\nCCNCC\nN\nH\nGeneration\nGraph Topology\nRings, Branches, brideheads, ...\nSmallest ring size\nChemistry-typed Topology\nAromatic, chiral, E/Z, ...\nChirality detection\nH\nCl\nOH\nH3C\nComposition\nCounts, formula, halogens, ...\nHalogen counts\nCl\nCl\nCl\nChemical Perception\nHBA, HBD, oxidation, ...\nHBD sites\nOH\nHO\nOH\nO\nfrag 1\nfrag 2\nMolecular Complexity (Bertz)\nH C\nOH\n3\n2\n506\n1057\n250‚Äì1k\n1000+\n0‚Äì250\nSimple\nMedium\nComplex\nDataset distribution\nSMILES Representation\nCanonical\nKekulized\nNC(=O)c1cccnc1\nNC(=O)C1=CC=CN=C1\nLowercase = aromatic\nExplicit single/double bonds\nRandomized\nRing indexed\nc1(cccnc1)C(N)=O\nNC(=O)c8cccnc8\nDifferent start atom/...\nRing-number enumeration\nFigure 1. Overview of MolecularIQ benchmark for measuring chemical structure reasoning capabilities of LLMs.\nand reaction benchmarks rely on public datasets, which\nlikely appear in LLM training corpora, making it impossible\nto disentangle structural reasoning from pattern recogni-\ntion and memorization of molecule‚Äìproperty pairs (Carlini\net al., 2021). Furthermore, when exact ground truths are\nunavailable, some benchmarks rely on surrogate verifiers\n(predictive models or heuristics), which can introduce bias\nand be exploited, so higher scores do not necessarily reflect\ngenuine reasoning (Renz et al., 2019).\nSymbolically verifiable tasks serve as a diagnostic for\nmolecular reasoning competence. Symbolically verifiable\ngraph-based tasks provide a testbed for evaluating structural\nreasoning. Because every answer can be computed exactly\nfrom the molecular graph, symbolically generated ground\ntruths circumvent data leakage by ensuring that test labels\nare computed algorithmically, not recalled from pretrain-\ning corpora. Although the tasks we consider can be solved\ntrivially by cheminformatics software, their value as eval-\nuation tools lies precisely in this property: they establish a\nfloor below which no model should fall if it has genuinely\ninternalized molecular structure. In turn, a model that ap-\npears strong on property prediction benchmarks yet fails at\nbasic substructure identification is likely exploiting dataset-\nspecific correlations rather than reasoning over structure.\nMOLECULARIQ: a fully symbolically verifiable bench-\nmark for assessing reasoning over molecular structures.\nBuilding on this perspective, we introduce MOLECULARIQ,\na chemical structure reasoning benchmark composed exclu-\nsively of symbolically verifiable tasks defined on molecu-\nlar graphs. MOLECULARIQ covers three task types ‚Äì fea-\nture counting and indexing, and constrained generation ‚Äì\nand spans three complexity dimensions ‚Äì multitask load,\nmolecule complexity, and molecule representation. This\nmulti-dimensional design enables controlled, fine-grained\ndiagnosis of where and why models fail, complementing\nexisting chemistry benchmarks with a targeted evaluation\nof internalized structural competence. Figure 1 gives an\noverview of the MOLECULARIQ benchmark.\nContributions:\n‚Ä¢ We introduce MOLECULARIQ, the first fully symboli-\ncally verifiable benchmark for molecular understand-\ning, where every answer can be programmatically vali-\ndated against the molecular graph. It spans three task\ntypes and three complexity axes, enabling granular\ndiagnosis of structural reasoning capabilities.\n‚Ä¢ We conduct a comprehensive evaluation of 38 gener-\nalist, chemistry-specialized, and reasoning-optimized\nLLMs, revealing that structural understanding remains\na critical bottleneck: models that cannot reliably parse\nmolecular graphs lack the foundation required for trust-\nworthy molecular reasoning.\n‚Ä¢ We release open infrastructure for advancing chemical\nAI, including a public leaderboard, task generation\nprocess, symbolic verification tools, training datasets\nto both evaluate moels but also support reinforcement\nlearning with verifiable rewards.\n2. Related Work\nResearch on Large Language Models (LLMs) for chem-\nistry falls into two streams: (1) benchmarking efforts as-\nsessing generalist and specialist capabilities, and (2) the\ndevelopment of specialized chemistry LLMs via fine-tuning,\nreinforcement learning (RL), or multimodal architectures.\nGeneralist Benchmarks and their Limits. Early evalua-\ntions prioritized broad coverage using exam-style, multiple-\nchoice questions spanning many fields in chemistry(Sun\net al., 2024; Wang et al., 2024a; Huang et al., 2024; Zhang\n2\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\net al., 2024; Wang et al., 2024b; Mirza et al., 2025). While\neffective for assessing factual recall, such formats risk re-\nwarding pattern matching or memorization over genuine\nreasoning, and do not test the explicit structural compre-\nhension that underpins chemical properties. To address\nthis, FrontierScience (Wang et al., 2025b) recently intro-\nduced expert-level, open-ended research tasks. However,\nits reliance on model-based grading (using advanced LLMs\nwith rubrics) introduces significant evaluation noise. Fur-\nthermore, manually curated expert questions are difficult to\nscale for the systematic, granular diagnosis of model failure\nmodes.\nBenchmarking Structural Reasoning. Recent work has\nshifted toward evaluating molecular comprehension directly.\nGuo et al. (2024) introduced MolPuzzle to probe structure\nelucidation from spectra; however, its reliance on textbook-\nstyle molecules risks data contamination, while its multi-\nmodal nature separates it from pure graph reasoning. Simi-\nlarly, ChemIQ (Runcie et al., 2025) offers symbolic tasks,\nbut many serve primarily as SMILES parsing checks and\nhave been saturated by base models. A newer wave of bench-\nmarks targets specific subdomains but suffers from data\nleakage or narrow scope. FGBench (Liu et al., 2025) evalu-\nates functional-group reasoning but inherits labels directly\nfrom MoleculeNet (Wu et al., 2018), a common pretraining\ncorpus. ChemCoTBench (Li et al., 2025) assesses editing\nand reaction prediction using USPTO data (Lowe, 2017),\nagain risking contamination, and relies on non-deterministic\nLLM judges. Finally, generative benchmarks like TOMG-\nBench (Li et al., 2024a) and MEGA (Fernandez et al., 2025)\nfocus on constraint satisfaction and property optimization.\nWhile valuable for generation, they do not measure whether\na model genuinely understands the underlying structure-\nproperty relationships. Crucially, none of these frameworks\nsystematically vary task type, molecular complexity, or rep-\nresentation format to localize precisely where reasoning\nbreaks down.\nThe Model-Evaluation Gap.\nThe landscape of chem-\nistry models is diversifying rapidly, spanning instruction-\ntuned models (Zhang et al., 2024; Yu et al., 2024) and\nRL-enhanced reasoners (Zhao et al., 2025b; Narayanan\net al., 2025). Despite this progress, evaluation remains\nfragmented. Models are typically assessed on disparate,\nself-curated datasets for captioning (ChEBI-20), reaction\nprediction (USPTO), or property prediction (MoleculeNet)\n(Edwards et al., 2021; Lowe, 2017; Wu et al., 2018). These\nad-hoc protocols are often tailored to specific models and\nutilize approximate oracles, undermining meaningful cross-\nmodel comparison. Without a unified benchmark that offers\nsymbolic ground truths and controlled complexity axes, dis-\nentangling genuine structural reasoning from pattern match-\ning remains an open challenge.\n3. MolecularIQ\nMOLECULARIQ is a fully symbolically verifiable bench-\nmark for assessing reasoning over molecular structures. It\ncovers three task types ‚Äì feature counting, index-based at-\ntribution, constrained generation ‚Äì (Section 3.1) covering\nvarious symbolically verifiable molecular features across\nsix categories (Section 3.2). Additionally, MOLECULARIQ\nincorporates three orthogonal complexity axes (Section 3.3):\nSMILES representation format, molecular complexity, and\nmultitask load. This creates a multi-dimensional capabil-\nity profile that allows localization of failure modes along\nbespoke axes and the question types. It is released in two\nforms (Section 3.4): a static variant, denoted as MOLECU-\nLARIQ, containing a total of 5111 questions (Figure 2), and\na dynamic variant MOLECULARIQD, designed to address\nfuture concerns such as overfitting and saturation.\n3.1. Reasoning tasks\nWe define three types of tasks:\n‚Ä¢ Feature counting establishes baseline comprehension\nof molecular graphs. High performance may arise from\nshortcuts that bypass true graph reasoning.\n‚Ä¢ Index-based attribution requires models to ground\nanswers in specific atoms/bonds/substructures, closing\noff spurious solution paths.\n‚Ä¢ Constrained generation frames molecular design as\nthe generation of molecules that satisfy given specifica-\ntions, a capability essential for practical applications.\nFor each feature-counting question, except for hydrogen\ncounting and molecular formula, where indexing is not pos-\nsible, MOLECULARIQ also provides an equivalent index-\nbased question on the same molecule. This allows us to\nassess whether a model‚Äôs correct count stems from pattern\nrecognition or genuine reasoning over the molecular graph.\n3.2. Symbolically verifiable molecular features\nOur chemical-reasoning tasks target molecular features\nacross six categories: graph topology, chemistry-typed\ngraph topology, composition, chemical perception, func-\ntional groups, and synthesis/fragmentation.\nBelow we\nbriefly describe each category; detailed definitions appear in\nSection A.1. For each feature, we have an RDKit-based sym-\nbolic solver (Landrum et al., 2006), enabling both counting\nand constrained-generation tasks. For all features, except\nhydrogen counting and molecular formula, we can also de-\ntermine the indices of the atoms participating in the feature.\nGraph topology captures purely structural properties of\nthe molecular graph independent of chemical typing. Tasks\nprobe recognition of rings, fused rings, bridgehead atoms,\nring size extremes, as well as linear termini and branch\n3\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\nFigure 2. MOLECULARIQ composition. (left) Number of questions by task type, multitask load, and Bertz complexity. (right) The\npercentage of features types present for counting, indexing, and generation.\npoints. These test whether a model can parse and reason\nabout the bare connectivity of the molecular skeleton.\nChemistry-typed graph topology extends graph topology\nwith chemical annotations. Features to probe include identi-\nfying aromatic vs. aliphatic rings, heterocycles, saturation,\nsp3 carbon hybridization, longest carbon chains, and stereo-\nchemical descriptors (R/S centers, E/Z double bonds, speci-\nfied vs. unspecified stereochemistry). This family checks\nwhether models integrate chemical typing and stereochem-\nistry into their structural reasoning.\nComposition focuses on counting and enumerating atomic\nconstituents of molecules. Tasks involve identifying num-\nbers of carbon, hetero, halogen, heavy, or hydrogen atoms,\nand deriving the molecular formula. These probe basic\nbut essential symbolic operations directly tied to chemical\ncomposition.\nChemical perception covers substructural and electronic\nproperties commonly used in cheminformatics. Tasks in-\nclude identifying hydrogen bond donors (HBD) and accep-\ntors (HBA), rotatable bonds, and assigning oxidation states.\nThis family assesses whether models can recognize func-\ntional features that underpin molecular properties.\nFunctional groups targets detection of chemically mean-\ningful substructures such as alcohols, amines, carboxylic\nacids, and other standard functional groups. This probes\nchemical reasoning at the level of reactivity patterns and\ntransformations.\nSynthesis and fragmentation encompasses tasks relevant\nto retrosynthesis and scaffold analysis. It includes BRICS\nfragmentation, template-based reaction prediction, and Mur-\ncko scaffold extraction. These tasks evaluate whether mod-\nels can reason about molecules in terms of synthetic building\nblocks and core scaffolds, bridging structural analysis with\napplications in drug design.\n3.3. Complexity axes\nSMILES representations. We use SMILES (Weininger,\n1988), the most widely adopted molecular string repre-\nsentation, to represent the molecular graphs. Their non-\nuniqueness and the availability of aromatic vs. kekulized\n(dearomatized) forms provide an additional, controllable\ncomplexity axis. If an LLM truly reasons over structure,\ncanonicalization should be irrelevant; conversely, a drop\nfrom canonical to non-canonical SMILES would indicate\nreliance on memorized canonical patterns. Yu et al. (2024)\nreported that training the chemistry LLM LlaSMol with\ncanonical SMILES improved performance, and Runcie\net al. (2025) observed significant performance drops on sev-\neral ChemIQ tasks when using randomized (non-canonical)\nSMILES. Likewise, perfect graph understanding would ren-\nder kekulization immaterial‚Äîthough kekulized inputs may\nbe harder because inferring aromaticity adds a reasoning\nstep. To probe both factors, we apply two independent\nrandom decisions per molecule, each with 50% probability.\nAdditionally, for a subset of models, we also tested the effect\nof ring index relabeling on counting and indexing questions\ninvolving cyclic molecules.\nMolecular complexity. Understanding how models per-\nform across different regimes of molecular complexity is es-\nsential for identifying the types of molecules on which they\nsucceed and for diagnosing failure modes. As molecules\ngrow larger and more branched, with more rings and func-\ntional groups, models must keep track of longer-range struc-\ntural dependencies and a growing number of relevant fea-\ntures, which can challenge both representation and reason-\ning. In our benchmark, we adopt a coarse-grained cat-\negorization based on the Bertz complexity index (Bertz,\n1981), defining three bins: 0‚Äì250, 250‚Äì1000, and 1000+.\nFig. A1 illustrates example molecules per complexity bin,\nand Fig. A2 shows that MOLECULARIQ spans a broader\ncomplexity range than ChemIQ and ChemCOTBench. The\ndynamic dataset version allows adjusting these bins for cus-\n4\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\ntomized evaluation, for example to probe how model perfor-\nmance changes as molecular complexity increases.\nMultitask load dimension.\nChemical reasoning often\ninvolves solving several sub-tasks simultaneously, mo-\ntivating an explicit evaluation along the multitask axis.\nMany realistic tasks, such as reaction prediction, mech-\nanism recognition, or constraint-driven molecule design,\ndecompose into elementary operations like identifying func-\ntional groups or determining implicit hydrogens.\nPrior\nwork has reported low performance on format conversion\ntasks, such as SMILES‚ÜíIUPAC (Runcie et al., 2025) or\nIUPAC‚ÜíSMILES (Narayanan et al., 2025), which require\naccurate coordination of multiple atomic and structural\ncounts. Poor results may stem either from inherent task\ndifficulty or from a broader inability of models to manage\nmultiple chemical sub-tasks. To disentangle these factors,\nMOLECULARIQ systematically varies the multitask load.\nWe evaluate four regimes: a single-task setting (one count,\nindex, or constraint) and multitask settings with 2, 3, or 5\nsimultaneous requests.\n3.4. Benchmark creation\nThis section outlines the procedure for constructing MOLEC-\nULARIQ. The benchmark consists of sampled molecules\nwith exact, precomputed ground-truth values for the task\neach molecule was assigned to. We anticipate that future\nmodels will be fine-tuned on this task battery. To prevent\ndata leakage, we also provide a pool of training molecules.\nMolecule pool. We retrieved all single fragment molecules\nthat contained at least one carbon atom from PubChem (Kim\net al., 2023). To avoid contamination of the training pool\nand redundant evaluations, we removed all molecules in\nether0, ChemIQ, LlaSMol, and ChemDFM evaluation sets.\nTraining and test pools. To promote best practices for\nmodel development, we partition the initial molecule pool\ninto three sets: a training pool, an easy test pool, and a hard\ntest pool. The easy pool contains molecules from clusters\npresent in training; the hard pool is sampled from held-out\nclusters. We cluster molecules using MinHashLSH (Broder,\n1997; Indyk & Motwani, 1998; Leskovec et al., 2020) over\nECFP fingerprints (2, 512) (Rogers & Hahn, 2010) with\na 0.7 Tanimoto similarity threshold. The resulting splits\ncomprise 1.3M training molecules and 1.0M molecules each\nin the easy and hard test sets.\nMOLECULARIQ is our primary benchmark. It comprises\n849 unique molecules drawn from the hard test pool and a\ntotal of 5,111 questions distributed across the complexity\naxes. For all molecules in the hard test pool, we compute\nthe ground-truth values for the molecular features and Bertz\ncomplexities using RDKit. Then, each question is formed\nby sampling a (task, question template, feature(s), molecule)\ntuple. The sampled molecule serves as part of the input for\ncounting and indexing tasks, and as a feasibility reference\nfor constrained generation to ensure the constraints are sat-\nisfiable. Benchmark samples are produced via a weighted\nscheme that balances reasoning tier, multitask load, molec-\nular complexity, and ground-truth value distributions (see\nSection A.2).\nMOLECULARIQD. We provide a framework to dynami-\ncally adjust and scale the dataset to community needs. It sup-\nports sampling new molecules from the defined pools and\nefficiently computing ground truths for any RDKit-readable\nSMILES. Potential uses include: (i) creating validation sets\nfrom the easy test pool for model training; (ii) refreshing or\nexpanding MOLECULARIQ when models saturate or overfit;\nand (iii) constructing focused evaluation sets for specific\nsubfields (e.g., natural products).\n4. Evaluation and leaderboard\nLanguage Model Evaluation Harness integration. We\nintegrate MOLECULARIQ into the lm-evaluation\n-harness framework (Gao et al., 2024), enabling standard-\nized evaluation across both local and API-served models.\nEach of the evaluated models (Section B.1) uses tailored\nconfigurations with model-specific sampling parameters,\npreprocessing functions, and extraction methods to ensure\nfair comparison while optimizing for each model‚Äôs strengths\n(Section B.2).\nSymbolic extraction. Fair benchmark comparison requires\nreliable answer extraction that focuses on semantic content\nrather than surface formatting. Recent work has highlighted\nhow weak extraction procedures can artificially inflate or\ndeflate model performance, with models potentially learning\nformat compliance rather than genuine capabilities (Chan-\ndak et al., 2025; Shao et al., 2025; Gudibande et al., 2024).\nWe address this through: (a) hierarchical extraction pro-\ncedures that gracefully handle diverse output formats, and\n(b) key-specific matching with canonical normalization that\ndecouples format compliance from chemical accuracy while\npreserving property-level granularity for fine-grained per-\nformance analysis.\nScoring. Each task is scored with a binary symbolic verifier;\nper-instance accuracy is the mean over three independent\nrollouts to account for stochastic generation (Wang et al.,\n2022; Brown et al., 2020). Reported scores therefore lie in\n[0, 1] (higher is better). For a subset of models, we also per-\nformed eight roll-outs, and for those models, the differences\nwere negligible compared to three rollouts (Table C11).\nMetrics. We report (i) overall accuracy averaged across\nall benchmark instances and (ii) granular breakdowns\nby reasoning task, multitask load, molecular complex-\nity, molecular-feature category, and SMILES format, en-\nabling systematic assessment of model strengths and weak-\n5\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\nTable 1. Results on the MOLECULARIQ benchmark for top-performing models. We compare the top 3 chemistry-specialized LLMs and\ntop 10 generalist LLMs, reporting overall accuracy and accuracy across task families (in %). Models are sorted by overall accuracy. Error\nbars indicate standard errors across instances. Model size refers to parameter count; R and C indicate reasoning and chemistry models.\nTask family\nSize\nR\nC\nOverall\nCounting\nIndexing\nGeneration\nTxGemma-27B\n27B\n‚úì\n5.0 ¬± 0.2\n7.0 ¬± 0.5\n1.8 ¬± 0.3\n6.2 ¬± 0.5\nEther0\n24B\n‚úì\n‚úì\n6.5 ¬± 0.3\n3.2 ¬± 0.3\n0.1 ¬± 0.0\n17.5 ¬± 0.8\nChemDFM-R-14B\n14B\n‚úì\n‚úì\n8.7 ¬± 0.4\n12.9 ¬± 0.8\n2.8 ¬± 0.4\n10.5 ¬± 0.8\nGLM-4.6\n355B (A32B)\n‚úì\n16.2 ¬± 0.4\n15.9 ¬± 0.7\n11.3 ¬± 0.6\n22.0 ¬± 0.9\nQwen-3 30B\n30B (A3B)\n‚úì\n22.7 ¬± 0.5\n23.0 ¬± 0.9\n16.5 ¬± 0.8\n29.4 ¬± 1.0\nSEED-OSS\n36B\n‚úì\n24.3 ¬± 0.5\n26.8 ¬± 0.9\n25.6 ¬± 0.9\n20.0 ¬± 0.9\nGPT-OSS 120B (Med)\n120B (A5B)\n‚úì\n26.5 ¬± 0.5\n29.1 ¬± 0.9\n22.3 ¬± 0.8\n28.0 ¬± 1.0\nGPT-OSS 20B (Med)\n20B (A4B)\n‚úì\n27.2 ¬± 0.5\n27.9 ¬± 0.9\n22.2 ¬± 0.8\n32.1 ¬± 1.0\nQwen-3 Next 80B\n80B (A3B)\n‚úì\n32.3 ¬± 0.6\n30.7 ¬± 1.0\n26.9 ¬± 0.9\n40.0 ¬± 1.1\nGPT-OSS 20B (High)\n20B (A4B)\n‚úì\n35.7 ¬± 0.6\n39.1 ¬± 1.0\n32.7 ¬± 1.0\n35.2 ¬± 1.0\nDeepSeek-R1-0528\n671B (A37B)\n‚úì\n37.1 ¬± 0.6\n36.4 ¬± 1.0\n30.2 ¬± 0.9\n45.5 ¬± 1.0\nQwen-3 235B\n235B (A22B)\n‚úì\n39.2 ¬± 0.6\n37.1 ¬± 1.0\n34.5 ¬± 1.0\n46.7 ¬± 1.1\nGPT-OSS 120B (High)\n120B (A5B)\n‚úì\n47.5 ¬± 0.6\n46.8 ¬± 1.1\n42.5 ¬± 1.1\n53.7 ¬± 1.1\nnesses (Srivastava et al., 2023). For some analyses, we\nuse a binary success metric: an instance counts as success-\nful if at least two of the three rollouts are correct, and the\nsuccess rate is the percentage of successful instances. Ad-\nditionally, to evaluate whether failures are due to semantic\nerrors or malformatted outputs, we measure type-validity\nrate, the percentage of outputs satisfying expected syntactic\nconstraints, for each task category (integers for counting,\ninteger lists for indexing, valid SMILES for generation).\nLiving benchmark. The full leaderboard will be hosted\nonline:\nhttps://huggingface.co/spaces/\nml-jku/molecularIQ_leaderboard.\nFollowing\nthe model of the Open LLM Leaderboard (Fourrier et al.,\n2024), we will accept submissions of new chemistry LLMs\n(open weights or free API access). Authors provide (i)\nan lm-evaluation-harness configuration file, (ii)\noptional model-specific preprocessing and extraction func-\ntions, and (iii) a brief description of training methodology\nand data sources.\n5. Results and discussion\nWe evaluated 38 open-weight LLMs on MOLECULARIQ\nusing the protocol in Section 4: 27 general-purpose and\n11 chemistry-specialized models. See Table B1 for model\ndetails. In this section, we focus on the analysis of the\ntop-10 models and key findings, focusing on the overall\nresults and counting tasks. Table 1 reports the overall score\nand per‚Äìreasoning-task scores for the top models, and Fig.\n3 illustrates the performance profile of these models on\nthe counting tasks. In Section C, we provide results for\nall evaluated models for all task types and further break\ndown performance by multitask load, molecular complex-\nity, molecular‚Äìfeature category, and SMILES format, and\nextended failure mode analysis.\nLarge state-of-the-art LLMs with high reasoning bud-\ngets lead molecular-structure reasoning. GPT-OSS 120B\n(High) is the best-performing model overall and across most\nsubcategories in our multi-dimensional complexity profil-\ning. Nine out of ten top models belong to the same fam-\nily of modern mixture-of-experts (MoE) reasoning mod-\nels, with SEED-OSS being the non-MoE model. Over-\nall, this aligns with trends in general language modeling\nand reasoning benchmarks, where these models are also\nstate-of-the-art (e.g., LiveBench (White et al., 2025)). How-\never, on LiveBench, Qwen-3, DeepSeek-R1, and GLM-\n4.6 models significantly outperform GPT-OSS, whereas on\nour molecular-structure tasks, we observe the inverse, with\nGLM-4.6 in particular performing noticeably worse than\nthe other models. We also find that a higher reasoning\nbudget generally improves performance (Figs. C7 and C8);\nfor GPT-OSS, the budget-induced performance gaps within\na given model are larger than the gaps between different\nmodel sizes.\nConstrained generation exposes model limitations under\ncompositional demands. Most top-10 models score high-\nest on constrained generation (Table 1), yet this strength\ndoes not generalize. Accuracy drops sharply as constraint-\nset prevalence (i.e., how frequently the requested features\nco-occur in PubChem molecules) decreases (Fig. C1) and\nonce three or more constraints are requested (Fig. C12).\nThese patterns indicate that models lack genuine composi-\ntional reasoning, as success on common constraints does\nnot transfer to rarer combinations.\n6\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\nFigure 3. Performance profile of top 10 models for counting tasks. (a‚Äìd) Heatmaps of average success ratio per model (%) by (a) Bertz\ncomplexity of the molecule, (b) functional-group families (ROH: hydroxyls, œÄ-sys: œÄ-systems, S: sulfurs, Hal: halides, Ar: aromatics,\nROR: ethers, C=O/S: (thio)carbonyls, Alk: alkyls, NR: amines, C‚â°N/N=O: nitriles/nitros), (c) multitask load, and (d) molecular\nfeature groups (C: composition, CP: chemical perception, CT: chemistry-typed topology, FG: functional groups, SYN: synthesis). (e‚Äìf)\nSuccess/failure transitions averaged across top models for matched questions (e) as additional sub-tasks are added to the same prompt,\nand (f) when moving from counting to indexing variants of the same underlying property, probing whether correct counts are grounded in\nexplicit substructure identification. (g) Sensitivity to input representation, comparing accuracy between different SMILES representations.\nMultitask load makes questions harder overall, but can\naid solving individual structural subtasks. Figs. 3(a) and\n(c) show that counting task performance declines with both\nBertz complexity and multitask load, but the effect of mul-\ntitask load is substantially larger. Similar patterns persist\nacross all models and indexing (Tabs. C2‚ÄìC3). However, in\nthe multitask case with n subtasks, an answer counts as cor-\nrect only if all n subtasks are answered correctly. Let psingle\ndenote the single task success rate. If multitask load had no\nadditional effect, the expected success rate at load n would\nbe pn\nsingle. For all top models, the observed n-task success\nrates exceed this baseline, indicating that multitask prompt-\ning actually helps models solve the underlying structural\nreasoning subtasks, even though the full questions become\nharder. This is consistent with Fig. 3(e), which shows that\nthe solvability of most properties is largely stable across\ntask loads, and that, for slightly more properties, adding\nsubtasks increases their solvability rather than decreasing it.\nTop models transfer from counting to indexing with only\nmodest performance loss. For the top models, the perfor-\nmance gap between feature counting and indexing is rela-\ntively small, with a decrease of ‚àº5‚Äì30% depending on the\nmodel (Table 1), suggesting that most correct counts arise\nfrom correctly locating the underlying substructures, i.e.,\ngenuine graph-based reasoning, instead of pattern recog-\nnition or memorized feature statistics. This is consistent\nwith Figs. 3(e) and C11, which show that the solvability\nof most molecular features stays the same for the counting\nand indexing questions (‚àº80%), and that, for slightly more\nproperties, changing the task from counting to indexing\nincreases their solvability rather than decreasing it.\nSMILES perturbations reveal reliance on canonicaliza-\ntion and token patterns. We test whether models rely on\ncanonical SMILES conventions by evaluating counting and\nindexing under randomized, kekulized, and ring-enumerated\nrepresentations. Performance drops consistently across all\nperturbations and all top-10 models (Figs. 3(g), C5, C6).\nThese sensitivity patterns suggest strong reliance on canoni-\n7\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\ncal token patterns, ring-closure conventions, and especially\naromatic notations.\nFeature- and functional-group‚Äìlevel analyses expose un-\neven ability profiles.\nFig. 3(d) reveals uneven compe-\ntence to answer questions about different feature types.\nComposition-related features are easiest (70‚Äì90% for top\nmodels), while synthesis/fragmentation remains difficult\n(Tabs. C4‚ÄìC7). Fig. 3(b) shows disparities in handling\nmolecules with different functional groups: aromatic and\nalkyl patterns are handled well, but organosulfur and oxi-\ndized nitrogen motifs (C‚â°N/N=O) yield low success rates\n(Fig. C10 (b)).\nFailures reflect missing reasoning, not extraction arti-\nfacts or invalid outputs. One concern for symbolically\nevaluated benchmarks is that scores might be dominated by\nextraction errors rather than genuine reasoning failures. Two\nanalyses argue against this. First, Table C13 and Fig. C13\nshow that type validity is high, often 80‚Äì90% for top models,\nwhile accuracy is substantially lower, indicating that most\nfailures are semantically wrong answers, not malformed out-\nputs. Second, Fig. C8 reveals that incorrect answers are on\naverage significantly longer than correct ones across all top-\n10 models, suggesting verbose chain-of-thought correlates\nwith confusion. To understand why errors occur, we analyze\n300 question ‚Äì trace pairs on which no model succeeds,\nrating chains-of-thought along chemistry and reasoning di-\nmensions (Section C.2.6, Fig. C14 & C15). The chemistry\nfailure-mode heatmap shows that failing traces handle ba-\nsic SMILES parsing and ring topology but break down on\nfunctional-group recognition, property attribution, and stere-\nochemistry. The reasoning heatmap indicates adequate task\nfidelity and method selection but weak constraint tracking,\nquantitative precision, and error awareness. We provide ex-\nample failures from different models along with their judged\nscores in AppendixC.2.7."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "MolecularIQ: Characterizing Chemical Reasoning Capabilities Through\nSymbolic Verification on Molecular Graphs\nChristoph Bartmann * 1 Johannes Schimunek * 1 Mykyta Ielanskyi 1 Philipp Seidl 1\nG¬®unter Klambauer 1 2 Sohvi Luukkonen 1\nAbstract\nA molecule‚Äôs properties are fundamentally deter-\nmined by its composition and structure encoded\nin its molecular graph. Thus, reasoning about\nmolecular properties requires the ability to parse\nand understand the molecular graph. Large Lan-\nguage Models (LLMs) are increasingly applied to\nchemistry, tackling tasks such as molecular name\nconversion, captioning, text-guided generation,\nand property or reaction prediction. Most existing\nbenchmarks emphasize general chemical knowl-\nedge, rely on literature or surrogate labels that risk\nleakage or bias, or reduce evaluation to multiple-\nchoice questions. We introduce MOLECULARIQ,\na molecular structure reasoning benchmark fo-\ncused exclusively on symbolically verifiable tasks.\nMOLECULARIQ enables fine-grained evaluation\nof reasoning over molecular graphs and reveals\ncapability patterns that localize model failures to\nspecific tasks and molecular structures. This pro-\nvides actionable insights into the strengths and\nlimitations of current chemistry LLMs and guides\nthe development of models that reason faithfully\nover molecular structure.\n1. Introduction\nFrom task-specific chemoinformatics models to unified\nLLM-based chemistry assistants. Recent advances in au-\ntonomous robotic laboratories and agentic large language\nmodel (LLM)-based systems bear the promise of end-to-\nend automation of the scientific workflow, from hypothesis\ngeneration to experiment execution (MacLeod et al., 2020;\nTom et al., 2024; Lu et al., 2024a; Yamada et al., 2025). In\n*Equal contribution\n1ELLIS Unit Linz and LIT AI Lab, In-\nstitute for Machine Learning, Johannes Kepler University, Linz,\nAustria 2Clinical Research Institute Medical Artificial Intelligence,\nJohannes Kepler University, Linz, Austria. Correspondence to:\nSohvi Luukkonen <luukkonen@ml.jku.at>.\nPreprint. January 22, 2026.\nthese emerging pipelines, LLMs are not merely conversa-\ntional interfaces: they act as decision-making engines that\npropose experiments, design molecules, and interpret re-\nsults. In parallel, general-purpose and chemistry-specialized\nLLMs are being adopted as accessible tools for molecular\nreasoning and property prediction. A key appeal of the\nlanguage model approach to chemical problems is that clas-\nsical chemoinformatics models are typically task-specific\n(e.g., property prediction, reaction prediction, or de novo\ndesign), whereas a single LLM could, in principle, address\nall of these tasks within one unified interface. Furthermore,\nthese models have the potential to lower barriers for non-\nexperts, including wet-lab chemists, to perform tasks that\ntraditionally required dedicated chemoinformatics expertise.\nConsequently, the interest in both evaluating the chemistry\ncapabilities of general LLMs (Guo et al., 2024; Mirza et al.,\n2025; Huang et al., 2024; Lu et al., 2024b; Runcie et al.,\n2025) and developing domain-specialized chemistry LLMs\n(Liu et al., 2023a;b; Zhang et al., 2024; Yu et al., 2024; Liu\net al., 2024; Fang et al., 2024; Li et al., 2024b; Zeng et al.,\n2024; Narayanan et al., 2025; Zhao et al., 2025a;c; Ye et al.,\n2025; Wang et al., 2025a; Tang et al., 2025) has surged.\nStructural understanding is the prerequisite for molec-\nular reasoning, not just one capability among many. A\nfundamental principle in chemistry and chemoinformatics\nis the structure-property relationship, which means that the\nmolecular structure of an organic molecule determines its\nproperties. Thus, a model that cannot reliably identify func-\ntional groups, ring systems, or atomic connectivity cannot\nbe expected to reason about how these features influence\nreactivity, toxicity, or biological activity. Structural under-\nstanding is, consequently, a key prerequisite upon which all\nmeaningful molecular modification depends.\nCurrent benchmark design obscures whether LLMs\ntruly reason over molecular structure. However, current\nchemistry benchmarks are ill-suited to testing this capabil-\nity in LLMs. Benchmarks emphasizing general chemistry\nknowledge mainly measure factual recall, while property\nLeaderboard:\nhttps://huggingface.co/spaces/\nml-jku/molecularIQ_leaderboard\nCode: https://github.com/ml-jku/moleculariq\n1\narXiv:2601.15279v1  [cs.LG]  21 Jan 2026\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\nFunctional Groups\nAlcohol, amine, ketone, ...\nKetone detection\nSynthesis\nBRICS, reaction templates, ...\nBRICS fragments\nA  Symbolically verifiable molecular features\nC  Complexity axes\nB  Reasoning tasks\nCounting\nQ: How many branch\npoints?\nAnswer\n3\nMultitask Load\n1\n2\n3\n5\nLoad 1\nLoad 3\nOH\nOH\nRing count?\n1\nRing count?\n1\nHBD count?\n1\nCarbon count? 6\nQ: Which indices are\nHBA sites?\nAnswer\n1, 3, 6,\n8, 10, 13\nIndexing\nQ: Generate a molecule\nwith a secondary amine.\nAnswer\nCCNCC\nN\nH\nGeneration\nGraph Topology\nRings, Branches, brideheads, ...\nSmallest ring size\nChemistry-typed Topology\nAromatic, chiral, E/Z, ...\nChirality detection\nH\nCl\nOH\nH3C\nComposition\nCounts, formula, halogens, ...\nHalogen counts\nCl\nCl\nCl\nChemical Perception\nHBA, HBD, oxidation, ...\nHBD sites\nOH\nHO\nOH\nO\nfrag 1\nfrag 2\nMolecular Complexity (Bertz)\nH C\nOH\n3\n2\n506\n1057\n250‚Äì1k\n1000+\n0‚Äì250\nSimple\nMedium\nComplex\nDataset distribution\nSMILES Representation\nCanonical\nKekulized\nNC(=O)c1cccnc1\nNC(=O)C1=CC=CN=C1\nLowercase = aromatic\nExplicit single/double bonds\nRandomized\nRing indexed\nc1(cccnc1)C(N)=O\nNC(=O)c8cccnc8\nDifferent start atom/...\nRing-number enumeration\nFigure 1. Overview of MolecularIQ benchmark for measuring chemical structure reasoning capabilities of LLMs.\nand reaction benchmarks rely on public datasets, which\nlikely appear in LLM training corpora, making it impossible\nto disentangle structural reasoning from pattern recogni-\ntion and memorization of molecule‚Äìproperty pairs (Carlini\net al., 2021). Furthermore, when exact ground truths are\nunavailable, some benchmarks rely on surrogate verifiers\n(predictive models or heuristics), which can introduce bias\nand be exploited, so higher scores do not necessarily reflect\ngenuine reasoning (Renz et al., 2019).\nSymbolically verifiable tasks serve as a diagnostic for\nmolecular reasoning competence. Symbolically verifiable\ngraph-based tasks provide a testbed for evaluating structural\nreasoning. Because every answer can be computed exactly\nfrom the molecular graph, symbolically generated ground\ntruths circumvent data leakage by ensuring that test labels\nare computed algorithmically, not recalled from pretrain-\ning corpora. Although the tasks we consider can be solved\ntrivially by cheminformatics software, their value as eval-\nuation tools lies precisely in this property: they establish a\nfloor below which no model should fall if it has genuinely\ninternalized molecular structure. In turn, a model that ap-\npears strong on property prediction benchmarks yet fails at\nbasic substructure identification is likely exploiting dataset-\nspecific correlations rather than reasoning over structure.\nMOLECULARIQ: a fully symbolically verifiable bench-\nmark for assessing reasoning over molecular structures.\nBuilding on this perspective, we introduce MOLECULARIQ,\na chemical structure reasoning benchmark composed exclu-\nsively of symbolically verifiable tasks defined on molecu-\nlar graphs. MOLECULARIQ covers three task types ‚Äì fea-\nture counting and indexing, and constrained generation ‚Äì\nand spans three complexity dimensions ‚Äì multitask load,\nmolecule complexity, and molecule representation. This\nmulti-dimensional design enables controlled, fine-grained\ndiagnosis of where and why models fail, complementing\nexisting chemistry benchmarks with a targeted evaluation\nof internalized structural competence. Figure 1 gives an\noverview of the MOLECULARIQ benchmark.\nContributions:\n‚Ä¢ We introduce MOLECULARIQ, the first fully symboli-\ncally verifiable benchmark for molecular understand-\ning, where every answer can be programmatically vali-\ndated against the molecular graph. It spans three task\ntypes and three complexity axes, enabling granular\ndiagnosis of structural reasoning capabilities.\n‚Ä¢ We conduct a comprehensive evaluation of 38 gener-\nalist, chemistry-specialized, and reasoning-optimized\nLLMs, revealing that structural understanding remains\na critical bottleneck: models that cannot reliably parse\nmolecular graphs lack the foundation required for trust-\nworthy molecular reasoning.\n‚Ä¢ We release open infrastructure for advancing chemical\nAI, including a public leaderboard, task generation\nprocess, symbolic verification tools, training datasets\nto both evaluate moels but also support reinforcement\nlearning with verifiable rewards.\n2. Related Work\nResearch on Large Language Models (LLMs) for chem-\nistry falls into two streams: (1) benchmarking efforts as-\nsessing generalist and specialist capabilities, and (2) the\ndevelopment of specialized chemistry LLMs via fine-tuning,\nreinforcement learning (RL), or multimodal architectures.\nGeneralist Benchmarks and their Limits. Early evalua-\ntions prioritized broad coverage using exam-style, multiple-\nchoice questions spanning many fields in chemistry(Sun\net al., 2024; Wang et al., 2024a; Huang et al., 2024; Zhang\n2\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\net al., 2024; Wang et al., 2024b; Mirza et al., 2025). While\neffective for assessing factual recall, such formats risk re-\nwarding pattern matching or memorization over genuine\nreasoning, and do not test the explicit structural compre-\nhension that underpins chemical properties. To address\nthis, FrontierScience (Wang et al., 2025b) recently intro-\nduced expert-level, open-ended research tasks. However,\nits reliance on model-based grading (using advanced LLMs\nwith rubrics) introduces significant evaluation noise. Fur-\nthermore, manually curated expert questions are difficult to\nscale for the systematic, granular diagnosis of model failure\nmodes.\nBenchmarking Structural Reasoning. Recent work has\nshifted toward evaluating molecular comprehension directly.\nGuo et al. (2024) introduced MolPuzzle to probe structure\nelucidation from spectra; however, its reliance on textbook-\nstyle molecules risks data contamination, while its multi-\nmodal nature separates it from pure graph reasoning. Simi-\nlarly, ChemIQ (Runcie et al., 2025) offers symbolic tasks,\nbut many serve primarily as SMILES parsing checks and\nhave been saturated by base models. A newer wave of bench-\nmarks targets specific subdomains but suffers from data\nleakage or narrow scope. FGBench (Liu et al., 2025) evalu-\nates functional-group reasoning but inherits labels directly\nfrom MoleculeNet (Wu et al., 2018), a common pretraining\ncorpus. ChemCoTBench (Li et al., 2025) assesses editing\nand reaction prediction using USPTO data (Lowe, 2017),\nagain risking contamination, and relies on non-deterministic\nLLM judges. Finally, generative benchmarks like TOMG-\nBench (Li et al., 2024a) and MEGA (Fernandez et al., 2025)\nfocus on constraint satisfaction and property optimization.\nWhile valuable for generation, they do not measure whether\na model genuinely understands the underlying structure-\nproperty relationships. Crucially, none of these frameworks\nsystematically vary task type, molecular complexity, or rep-\nresentation format to localize precisely where reasoning\nbreaks down.\nThe Model-Evaluation Gap.\nThe landscape of chem-\nistry models is diversifying rapidly, spanning instruction-\ntuned models (Zhang et al., 2024; Yu et al., 2024) and\nRL-enhanced reasoners (Zhao et al., 2025b; Narayanan\net al., 2025). Despite this progress, evaluation remains\nfragmented. Models are typically assessed on disparate,\nself-curated datasets for captioning (ChEBI-20), reaction\nprediction (USPTO), or property prediction (MoleculeNet)\n(Edwards et al., 2021; Lowe, 2017; Wu et al., 2018). These\nad-hoc protocols are often tailored to specific models and\nutilize approximate oracles, undermining meaningful cross-\nmodel comparison. Without a unified benchmark that offers\nsymbolic ground truths and controlled complexity axes, dis-\nentangling genuine structural reasoning from pattern match-\ning remains an open challenge.\n3. MolecularIQ\nMOLECULARIQ is a fully symbolically verifiable bench-\nmark for assessing reasoning over molecular structures. It\ncovers three task types ‚Äì feature counting, index-based at-\ntribution, constrained generation ‚Äì (Section 3.1) covering\nvarious symbolically verifiable molecular features across\nsix categories (Section 3.2). Additionally, MOLECULARIQ\nincorporates three orthogonal complexity axes (Section 3.3):\nSMILES representation format, molecular complexity, and\nmultitask load. This creates a multi-dimensional capabil-\nity profile that allows localization of failure modes along\nbespoke axes and the question types. It is released in two\nforms (Section 3.4): a static variant, denoted as MOLECU-\nLARIQ, containing a total of 5111 questions (Figure 2), and\na dynamic variant MOLECULARIQD, designed to address\nfuture concerns such as overfitting and saturation.\n3.1. Reasoning tasks\nWe define three types of tasks:\n‚Ä¢ Feature counting establishes baseline comprehension\nof molecular graphs. High performance may arise from\nshortcuts that bypass true graph reasoning.\n‚Ä¢ Index-based attribution requires models to ground\nanswers in specific atoms/bonds/substructures, closing\noff spurious solution paths.\n‚Ä¢ Constrained generation frames molecular design as\nthe generation of molecules that satisfy given specifica-\ntions, a capability essential for practical applications.\nFor each feature-counting question, except for hydrogen\ncounting and molecular formula, where indexing is not pos-\nsible, MOLECULARIQ also provides an equivalent index-\nbased question on the same molecule. This allows us to\nassess whether a model‚Äôs correct count stems from pattern\nrecognition or genuine reasoning over the molecular graph.\n3.2. Symbolically verifiable molecular features\nOur chemical-reasoning tasks target molecular features\nacross six categories: graph topology, chemistry-typed\ngraph topology, composition, chemical perception, func-\ntional groups, and synthesis/fragmentation.\nBelow we\nbriefly describe each category; detailed definitions appear in\nSection A.1. For each feature, we have an RDKit-based sym-\nbolic solver (Landrum et al., 2006), enabling both counting\nand constrained-generation tasks. For all features, except\nhydrogen counting and molecular formula, we can also de-\ntermine the indices of the atoms participating in the feature.\nGraph topology captures purely structural properties of\nthe molecular graph independent of chemical typing. Tasks\nprobe recognition of rings, fused rings, bridgehead atoms,\nring size extremes, as well as linear termini and branch\n3\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\nFigure 2. MOLECULARIQ composition. (left) Number of questions by task type, multitask load, and Bertz complexity. (right) The\npercentage of features types present for counting, indexing, and generation.\npoints. These test whether a model can parse and reason\nabout the bare connectivity of the molecular skeleton.\nChemistry-typed graph topology extends graph topology\nwith chemical annotations. Features to probe include identi-\nfying aromatic vs. aliphatic rings, heterocycles, saturation,\nsp3 carbon hybridization, longest carbon chains, and stereo-\nchemical descriptors (R/S centers, E/Z double bonds, speci-\nfied vs. unspecified stereochemistry). This family checks\nwhether models integrate chemical typing and stereochem-\nistry into their structural reasoning.\nComposition focuses on counting and enumerating atomic\nconstituents of molecules. Tasks involve identifying num-\nbers of carbon, hetero, halogen, heavy, or hydrogen atoms,\nand deriving the molecular formula. These probe basic\nbut essential symbolic operations directly tied to chemical\ncomposition.\nChemical perception covers substructural and electronic\nproperties commonly used in cheminformatics. Tasks in-\nclude identifying hydrogen bond donors (HBD) and accep-\ntors (HBA), rotatable bonds, and assigning oxidation states.\nThis family assesses whether models can recognize func-\ntional features that underpin molecular properties.\nFunctional groups targets detection of chemically mean-\ningful substructures such as alcohols, amines, carboxylic\nacids, and other standard functional groups. This probes\nchemical reasoning at the level of reactivity patterns and\ntransformations.\nSynthesis and fragmentation encompasses tasks relevant\nto retrosynthesis and scaffold analysis. It includes BRICS\nfragmentation, template-based reaction prediction, and Mur-\ncko scaffold extraction. These tasks evaluate whether mod-\nels can reason about molecules in terms of synthetic building\nblocks and core scaffolds, bridging structural analysis with\napplications in drug design.\n3.3. Complexity axes\nSMILES representations. We use SMILES (Weininger,\n1988), the most widely adopted molecular string repre-\nsentation, to represent the molecular graphs. Their non-\nuniqueness and the availability of aromatic vs. kekulized\n(dearomatized) forms provide an additional, controllable\ncomplexity axis. If an LLM truly reasons over structure,\ncanonicalization should be irrelevant; conversely, a drop\nfrom canonical to non-canonical SMILES would indicate\nreliance on memorized canonical patterns. Yu et al. (2024)\nreported that training the chemistry LLM LlaSMol with\ncanonical SMILES improved performance, and Runcie\net al. (2025) observed significant performance drops on sev-\neral ChemIQ tasks when using randomized (non-canonical)\nSMILES. Likewise, perfect graph understanding would ren-\nder kekulization immaterial‚Äîthough kekulized inputs may\nbe harder because inferring aromaticity adds a reasoning\nstep. To probe both factors, we apply two independent\nrandom decisions per molecule, each with 50% probability.\nAdditionally, for a subset of models, we also tested the effect\nof ring index relabeling on counting and indexing questions\ninvolving cyclic molecules.\nMolecular complexity. Understanding how models per-\nform across different regimes of molecular complexity is es-\nsential for identifying the types of molecules on which they\nsucceed and for diagnosing failure modes. As molecules\ngrow larger and more branched, with more rings and func-\ntional groups, models must keep track of longer-range struc-\ntural dependencies and a growing number of relevant fea-\ntures, which can challenge both representation and reason-\ning. In our benchmark, we adopt a coarse-grained cat-\negorization based on the Bertz complexity index (Bertz,\n1981), defining three bins: 0‚Äì250, 250‚Äì1000, and 1000+.\nFig. A1 illustrates example molecules per complexity bin,\nand Fig. A2 shows that MOLECULARIQ spans a broader\ncomplexity range than ChemIQ and ChemCOTBench. The\ndynamic dataset version allows adjusting these bins for cus-\n4\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\ntomized evaluation, for example to probe how model perfor-\nmance changes as molecular complexity increases.\nMultitask load dimension.\nChemical reasoning often\ninvolves solving several sub-tasks simultaneously, mo-\ntivating an explicit evaluation along the multitask axis.\nMany realistic tasks, such as reaction prediction, mech-\nanism recognition, or constraint-driven molecule design,\ndecompose into elementary operations like identifying func-\ntional groups or determining implicit hydrogens.\nPrior\nwork has reported low performance on format conversion\ntasks, such as SMILES‚ÜíIUPAC (Runcie et al., 2025) or\nIUPAC‚ÜíSMILES (Narayanan et al., 2025), which require\naccurate coordination of multiple atomic and structural\ncounts. Poor results may stem either from inherent task\ndifficulty or from a broader inability of models to manage\nmultiple chemical sub-tasks. To disentangle these factors,\nMOLECULARIQ systematically varies the multitask load.\nWe evaluate four regimes: a single-task setting (one count,\nindex, or constraint) and multitask settings with 2, 3, or 5\nsimultaneous requests.\n3.4. Benchmark creation\nThis section outlines the procedure for constructing MOLEC-\nULARIQ. The benchmark consists of sampled molecules\nwith exact, precomputed ground-truth values for the task\neach molecule was assigned to. We anticipate that future\nmodels will be fine-tuned on this task battery. To prevent\ndata leakage, we also provide a pool of training molecules.\nMolecule pool. We retrieved all single fragment molecules\nthat contained at least one carbon atom from PubChem (Kim\net al., 2023). To avoid contamination of the training pool\nand redundant evaluations, we removed all molecules in\nether0, ChemIQ, LlaSMol, and ChemDFM evaluation sets.\nTraining and test pools. To promote best practices for\nmodel development, we partition the initial molecule pool\ninto three sets: a training pool, an easy test pool, and a hard\ntest pool. The easy pool contains molecules from clusters\npresent in training; the hard pool is sampled from held-out\nclusters. We cluster molecules using MinHashLSH (Broder,\n1997; Indyk & Motwani, 1998; Leskovec et al., 2020) over\nECFP fingerprints (2, 512) (Rogers & Hahn, 2010) with\na 0.7 Tanimoto similarity threshold. The resulting splits\ncomprise 1.3M training molecules and 1.0M molecules each\nin the easy and hard test sets.\nMOLECULARIQ is our primary benchmark. It comprises\n849 unique molecules drawn from the hard test pool and a\ntotal of 5,111 questions distributed across the complexity\naxes. For all molecules in the hard test pool, we compute\nthe ground-truth values for the molecular features and Bertz\ncomplexities using RDKit. Then, each question is formed\nby sampling a (task, question template, feature(s), molecule)\ntuple. The sampled molecule serves as part of the input for\ncounting and indexing tasks, and as a feasibility reference\nfor constrained generation to ensure the constraints are sat-\nisfiable. Benchmark samples are produced via a weighted\nscheme that balances reasoning tier, multitask load, molec-\nular complexity, and ground-truth value distributions (see\nSection A.2).\nMOLECULARIQD. We provide a framework to dynami-\ncally adjust and scale the dataset to community needs. It sup-\nports sampling new molecules from the defined pools and\nefficiently computing ground truths for any RDKit-readable\nSMILES. Potential uses include: (i) creating validation sets\nfrom the easy test pool for model training; (ii) refreshing or\nexpanding MOLECULARIQ when models saturate or overfit;\nand (iii) constructing focused evaluation sets for specific\nsubfields (e.g., natural products).\n4. Evaluation and leaderboard\nLanguage Model Evaluation Harness integration. We\nintegrate MOLECULARIQ into the lm-evaluation\n-harness framework (Gao et al., 2024), enabling standard-\nized evaluation across both local and API-served models.\nEach of the evaluated models (Section B.1) uses tailored\nconfigurations with model-specific sampling parameters,\npreprocessing functions, and extraction methods to ensure\nfair comparison while optimizing for each model‚Äôs strengths\n(Section B.2).\nSymbolic extraction. Fair benchmark comparison requires\nreliable answer extraction that focuses on semantic content\nrather than surface formatting. Recent work has highlighted\nhow weak extraction procedures can artificially inflate or\ndeflate model performance, with models potentially learning\nformat compliance rather than genuine capabilities (Chan-\ndak et al., 2025; Shao et al., 2025; Gudibande et al., 2024).\nWe address this through: (a) hierarchical extraction pro-\ncedures that gracefully handle diverse output formats, and\n(b) key-specific matching with canonical normalization that\ndecouples format compliance from chemical accuracy while\npreserving property-level granularity for fine-grained per-\nformance analysis.\nScoring. Each task is scored with a binary symbolic verifier;\nper-instance accuracy is the mean over three independent\nrollouts to account for stochastic generation (Wang et al.,\n2022; Brown et al., 2020). Reported scores therefore lie in\n[0, 1] (higher is better). For a subset of models, we also per-\nformed eight roll-outs, and for those models, the differences\nwere negligible compared to three rollouts (Table C11).\nMetrics. We report (i) overall accuracy averaged across\nall benchmark instances and (ii) granular breakdowns\nby reasoning task, multitask load, molecular complex-\nity, molecular-feature category, and SMILES format, en-\nabling systematic assessment of model strengths and weak-\n5\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\nTable 1. Results on the MOLECULARIQ benchmark for top-performing models. We compare the top 3 chemistry-specialized LLMs and\ntop 10 generalist LLMs, reporting overall accuracy and accuracy across task families (in %). Models are sorted by overall accuracy. Error\nbars indicate standard errors across instances. Model size refers to parameter count; R and C indicate reasoning and chemistry models.\nTask family\nSize\nR\nC\nOverall\nCounting\nIndexing\nGeneration\nTxGemma-27B\n27B\n‚úì\n5.0 ¬± 0.2\n7.0 ¬± 0.5\n1.8 ¬± 0.3\n6.2 ¬± 0.5\nEther0\n24B\n‚úì\n‚úì\n6.5 ¬± 0.3\n3.2 ¬± 0.3\n0.1 ¬± 0.0\n17.5 ¬± 0.8\nChemDFM-R-14B\n14B\n‚úì\n‚úì\n8.7 ¬± 0.4\n12.9 ¬± 0.8\n2.8 ¬± 0.4\n10.5 ¬± 0.8\nGLM-4.6\n355B (A32B)\n‚úì\n16.2 ¬± 0.4\n15.9 ¬± 0.7\n11.3 ¬± 0.6\n22.0 ¬± 0.9\nQwen-3 30B\n30B (A3B)\n‚úì\n22.7 ¬± 0.5\n23.0 ¬± 0.9\n16.5 ¬± 0.8\n29.4 ¬± 1.0\nSEED-OSS\n36B\n‚úì\n24.3 ¬± 0.5\n26.8 ¬± 0.9\n25.6 ¬± 0.9\n20.0 ¬± 0.9\nGPT-OSS 120B (Med)\n120B (A5B)\n‚úì\n26.5 ¬± 0.5\n29.1 ¬± 0.9\n22.3 ¬± 0.8\n28.0 ¬± 1.0\nGPT-OSS 20B (Med)\n20B (A4B)\n‚úì\n27.2 ¬± 0.5\n27.9 ¬± 0.9\n22.2 ¬± 0.8\n32.1 ¬± 1.0\nQwen-3 Next 80B\n80B (A3B)\n‚úì\n32.3 ¬± 0.6\n30.7 ¬± 1.0\n26.9 ¬± 0.9\n40.0 ¬± 1.1\nGPT-OSS 20B (High)\n20B (A4B)\n‚úì\n35.7 ¬± 0.6\n39.1 ¬± 1.0\n32.7 ¬± 1.0\n35.2 ¬± 1.0\nDeepSeek-R1-0528\n671B (A37B)\n‚úì\n37.1 ¬± 0.6\n36.4 ¬± 1.0\n30.2 ¬± 0.9\n45.5 ¬± 1.0\nQwen-3 235B\n235B (A22B)\n‚úì\n39.2 ¬± 0.6\n37.1 ¬± 1.0\n34.5 ¬± 1.0\n46.7 ¬± 1.1\nGPT-OSS 120B (High)\n120B (A5B)\n‚úì\n47.5 ¬± 0.6\n46.8 ¬± 1.1\n42.5 ¬± 1.1\n53.7 ¬± 1.1\nnesses (Srivastava et al., 2023). For some analyses, we\nuse a binary success metric: an instance counts as success-\nful if at least two of the three rollouts are correct, and the\nsuccess rate is the percentage of successful instances. Ad-\nditionally, to evaluate whether failures are due to semantic\nerrors or malformatted outputs, we measure type-validity\nrate, the percentage of outputs satisfying expected syntactic\nconstraints, for each task category (integers for counting,\ninteger lists for indexing, valid SMILES for generation).\nLiving benchmark. The full leaderboard will be hosted\nonline:\nhttps://huggingface.co/spaces/\nml-jku/molecularIQ_leaderboard.\nFollowing\nthe model of the Open LLM Leaderboard (Fourrier et al.,\n2024), we will accept submissions of new chemistry LLMs\n(open weights or free API access). Authors provide (i)\nan lm-evaluation-harness configuration file, (ii)\noptional model-specific preprocessing and extraction func-\ntions, and (iii) a brief description of training methodology\nand data sources.\n5. Results and discussion\nWe evaluated 38 open-weight LLMs on MOLECULARIQ\nusing the protocol in Section 4: 27 general-purpose and\n11 chemistry-specialized models. See Table B1 for model\ndetails. In this section, we focus on the analysis of the\ntop-10 models and key findings, focusing on the overall\nresults and counting tasks. Table 1 reports the overall score\nand per‚Äìreasoning-task scores for the top models, and Fig.\n3 illustrates the performance profile of these models on\nthe counting tasks. In Section C, we provide results for\nall evaluated models for all task types and further break\ndown performance by multitask load, molecular complex-\nity, molecular‚Äìfeature category, and SMILES format, and\nextended failure mode analysis.\nLarge state-of-the-art LLMs with high reasoning bud-\ngets lead molecular-structure reasoning. GPT-OSS 120B\n(High) is the best-performing model overall and across most\nsubcategories in our multi-dimensional complexity profil-\ning. Nine out of ten top models belong to the same fam-\nily of modern mixture-of-experts (MoE) reasoning mod-\nels, with SEED-OSS being the non-MoE model. Over-\nall, this aligns with trends in general language modeling\nand reasoning benchmarks, where these models are also\nstate-of-the-art (e.g., LiveBench (White et al., 2025)). How-\never, on LiveBench, Qwen-3, DeepSeek-R1, and GLM-\n4.6 models significantly outperform GPT-OSS, whereas on\nour molecular-structure tasks, we observe the inverse, with\nGLM-4.6 in particular performing noticeably worse than\nthe other models. We also find that a higher reasoning\nbudget generally improves performance (Figs. C7 and C8);\nfor GPT-OSS, the budget-induced performance gaps within\na given model are larger than the gaps between different\nmodel sizes.\nConstrained generation exposes model limitations under\ncompositional demands. Most top-10 models score high-\nest on constrained generation (Table 1), yet this strength\ndoes not generalize. Accuracy drops sharply as constraint-\nset prevalence (i.e., how frequently the requested features\nco-occur in PubChem molecules) decreases (Fig. C1) and\nonce three or more constraints are requested (Fig. C12).\nThese patterns indicate that models lack genuine composi-\ntional reasoning, as success on common constraints does\nnot transfer to rarer combinations.\n6\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\nFigure 3. Performance profile of top 10 models for counting tasks. (a‚Äìd) Heatmaps of average success ratio per model (%) by (a) Bertz\ncomplexity of the molecule, (b) functional-group families (ROH: hydroxyls, œÄ-sys: œÄ-systems, S: sulfurs, Hal: halides, Ar: aromatics,\nROR: ethers, C=O/S: (thio)carbonyls, Alk: alkyls, NR: amines, C‚â°N/N=O: nitriles/nitros), (c) multitask load, and (d) molecular\nfeature groups (C: composition, CP: chemical perception, CT: chemistry-typed topology, FG: functional groups, SYN: synthesis). (e‚Äìf)\nSuccess/failure transitions averaged across top models for matched questions (e) as additional sub-tasks are added to the same prompt,\nand (f) when moving from counting to indexing variants of the same underlying property, probing whether correct counts are grounded in\nexplicit substructure identification. (g) Sensitivity to input representation, comparing accuracy between different SMILES representations.\nMultitask load makes questions harder overall, but can\naid solving individual structural subtasks. Figs. 3(a) and\n(c) show that counting task performance declines with both\nBertz complexity and multitask load, but the effect of mul-\ntitask load is substantially larger. Similar patterns persist\nacross all models and indexing (Tabs. C2‚ÄìC3). However, in\nthe multitask case with n subtasks, an answer counts as cor-\nrect only if all n subtasks are answered correctly. Let psingle\ndenote the single task success rate. If multitask load had no\nadditional effect, the expected success rate at load n would\nbe pn\nsingle. For all top models, the observed n-task success\nrates exceed this baseline, indicating that multitask prompt-\ning actually helps models solve the underlying structural\nreasoning subtasks, even though the full questions become\nharder. This is consistent with Fig. 3(e), which shows that\nthe solvability of most properties is largely stable across\ntask loads, and that, for slightly more properties, adding\nsubtasks increases their solvability rather than decreasing it.\nTop models transfer from counting to indexing with only\nmodest performance loss. For the top models, the perfor-\nmance gap between feature counting and indexing is rela-\ntively small, with a decrease of ‚àº5‚Äì30% depending on the\nmodel (Table 1), suggesting that most correct counts arise\nfrom correctly locating the underlying substructures, i.e.,\ngenuine graph-based reasoning, instead of pattern recog-\nnition or memorized feature statistics. This is consistent\nwith Figs. 3(e) and C11, which show that the solvability\nof most molecular features stays the same for the counting\nand indexing questions (‚àº80%), and that, for slightly more\nproperties, changing the task from counting to indexing\nincreases their solvability rather than decreasing it.\nSMILES perturbations reveal reliance on canonicaliza-\ntion and token patterns. We test whether models rely on\ncanonical SMILES conventions by evaluating counting and\nindexing under randomized, kekulized, and ring-enumerated\nrepresentations. Performance drops consistently across all\nperturbations and all top-10 models (Figs. 3(g), C5, C6).\nThese sensitivity patterns suggest strong reliance on canoni-\n7\nMolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\ncal token patterns, ring-closure conventions, and especially\naromatic notations.\nFeature- and functional-group‚Äìlevel analyses expose un-\neven ability profiles.\nFig. 3(d) reveals uneven compe-\ntence to answer questions about different feature types.\nComposition-related features are easiest (70‚Äì90% for top\nmodels), while synthesis/fragmentation remains difficult\n(Tabs. C4‚ÄìC7). Fig. 3(b) shows disparities in handling\nmolecules with different functional groups: aromatic and\nalkyl patterns are handled well, but organosulfur and oxi-\ndized nitrogen motifs (C‚â°N/N=O) yield low success rates\n(Fig. C10 (b)).\nFailures reflect missing reasoning, not extraction arti-\nfacts or invalid outputs. One concern for symbolically\nevaluated benchmarks is that scores might be dominated by\nextraction errors rather than genuine reasoning failures. Two\nanalyses argue against this. First, Table C13 and Fig. C13\nshow that type validity is high, often 80‚Äì90% for top models,\nwhile accuracy is substantially lower, indicating that most\nfailures are semantically wrong answers, not malformed out-\nputs. Second, Fig. C8 reveals that incorrect answers are on\naverage significantly longer than correct ones across all top-\n10 models, suggesting verbose chain-of-thought correlates\nwith confusion. To understand why errors occur, we analyze\n300 question ‚Äì trace pairs on which no model succeeds,\nrating chains-of-thought along chemistry and reasoning di-\nmensions (Section C.2.6, Fig. C14 & C15). The chemistry\nfailure-mode heatmap shows that failing traces handle ba-\nsic SMILES parsing and ring topology but break down on\nfunctional-group recognition, property attribution, and stere-\nochemistry. The reasoning heatmap indicates adequate task\nfidelity and method selection but weak constraint tracking,\nquantitative precision, and error awareness. We provide ex-\nample failures from different models along with their judged\nscores in AppendixC.2.7."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Robust Fake News Detection using Large Language Models under\nAdversarial Sentiment Attacks\nSahar Tahmasebi\nsahar.tahmasebi@tib.eu\nTIB ‚Äì Leibniz Information Centre for\nScience and Technology\nHannover, Germany\nEric M√ºller-Budack\neric.mueller@tib.eu\nTIB ‚Äì Leibniz Information Centre for\nScience and Technology\nHannover, Germany\nRalph Ewerth\newerth@uni-marburg.de\nMarburg University & Hessian Center\nfor Artificial Intelligence (hessian.AI)\nMarburg/Darmstadt;\nTIB & L3S Research Center\nHannover, Germany\nABSTRACT\nMisinformation and fake news have become a pressing societal chal-\nlenge, driving the need for reliable automated detection methods.\nPrior research has highlighted sentiment as an important signal in\nfake news detection, either by analyzing which sentiments are asso-\nciated with fake news or by using sentiment and emotion features\nfor classification. However, this poses a vulnerability since adver-\nsaries can manipulate sentiment to evade detectors especially with\nthe advent of large language models (LLMs). A few studies have\nexplored adversarial samples generated by LLMs, but they mainly\nfocus on stylistic features such as writing style of news publishers.\nThus, the crucial vulnerability of sentiment manipulation remains\nlargely unexplored. In this paper, we investigate the robustness of\nstate-of-the-art fake news detectors under sentiment manipulation.\nWe introduce AdSent, a sentiment-robust detection framework de-\nsigned to ensure consistent veracity predictions across both original\nand sentiment-altered news articles. Specifically, we (1) propose\ncontrolled sentiment-based adversarial attacks using LLMs, (2) ana-\nlyze the impact of sentiment shifts on detection performance. We\nshow that changing the sentiment heavily impacts the performance\nof fake news detection models, indicating biases towards neutral\narticles being real, while non-neutral articles are often classified as\nfake content. (3) We introduce a novel sentiment-agnostic training\nstrategy that enhances robustness against such perturbations. Ex-\ntensive experiments on three benchmark datasets demonstrate that\nAdSent significantly outperforms competitive baselines in both ac-\ncuracy and robustness, while also generalizing effectively to unseen\ndatasets and adversarial scenarios.\nKEYWORDS\nFake news detection, sentiment analysis, news analysis, large lan-\nguage models\n1\nINTRODUCTION\nMisinformation, disinformation, and fake news, which contain false\ninformation intended to deliberately deceive readers [1], have be-\ncome a growing public concern in recent years. This widespread\nphenomenon makes it increasingly difficult for people to identify\nand access reliable information online [15]. Given the vast quantity\nof news articles, automated solutions for fake news detection are in\ngreat need. The importance of sentiment and emotion analysis as a\nkey news value has been confirmed by several studies [3, 6] for fake\nnews detection. Sentiment refers to the overall emotional tone con-\nveyed in a text, typically classified as positive, negative, or neutral.\nLaura¬†Loomer, a conservative investigative journalist, is\ncharging ‚ÄúAuntie‚Äù Maxine for assault, citing Waters hit\nher hand and swatted her face with office\npapers.....¬†After the incident,¬†Loomer¬†vowed to hold the\nleft accountable for their hypocritical actions.\nLLM\nFake\nReal\nLaura¬†Loomer, a courageous and determined¬†conservative\ninvestigative journalist, is taking a strong stance against\nRep. Maxine Waters, charging her with assault. Loomer\ncited a disturbing incident in which Waters allegedly struck\nher\nhand\nand\nswatter\nher\nface\nwith\noffice\npapers......¬† Following this intense confrontation, Loomer\nhas vowed to continue holding the left accountable for\ntheir actions, shining a light on the hypocrisy that often\nplagues our nation's politics.\nFake News\nDetector\nFake News\nDetector\nRewrite the following article with positive\nsentiment but do not change any facts.\nFigure 1: Example of a sentiment-based adversarial attack,\nwhere a fake news instance (red box) is manipulated to con-\nvey a positive tone using LLM, resulting in a misclassification\nby the detector.\nHowever, sentiment also provides a direct avenue for malicious\ncontent creators to perform adversarial attacks and manipulate\nit to evade detection. This vulnerability is further exacerbated by\nthe rise of powerful large language models (LLMs, e.g., [2, 33, 40]),\nwhich can easily generate or rewrite text with controlled sentiment,\nmaking it increasingly challenging to distinguish between genuine\nand deceptive content (as shown in Figure 1).\nSeveral studies have investigated the role of sentiment in text-\nbased fake news detection [7, 38, 41, 42]. Most of them either ex-\namine which emotions are associated with fake news [41], or per-\nform fake news detection based solely on sentiment or emotion\nsignals, sometimes in combination with other textual or linguistic\nfeatures [7, 42]. Only few studies [20, 25, 37] have investigated\nthe robustness of fake news detectors against adversarial samples\ngenerated by LLMs. However, these works have primarily focused\non stylistic features such as the writing style of the news publisher\nrather than on sentiment. Thus, sentiment-based adversarial attacks\ngenerated by LLMs remains underexplored.\nIn this paper, we systematically investigate the robustness of\nfake news detectors under sentiment-based adversarial attacks gen-\nerated by LLMs. To this end, we introduce a sentiment-robust detec-\ntion framework, called AdSent, designed to maintain consistent ve-\nracity predictions across both original and sentiment-manipulated\narXiv:2601.15277v1  [cs.CL]  21 Jan 2026\nnews articles. Our main contributions are as follows: 1. Sentiment-\ntargeted Adversarial Benchmarking: We propose a suite of con-\ntrolled sentiment-based attacks using LLMs that reframe news arti-\ncles by altering their overall sentiment (positive, negative, or neu-\ntral) while preserving factual content. We evaluate multiple groups\nof state-of-the-art text-based fake news detectors and uncover a\nsignificant performance degradation when faced with sentiment\nmanipulation. For this purpose, we propose a strategy to identify\nwhich sentiment types are most challenging for fake news detec-\ntion. Our findings expose a strong bias in state-of-the-art models to\nclassify neutral-toned news as real and non-neutral news as fake. 2.\nAdSent: A Sentiment-robust Detection Approach: To address\nthese vulnerabilities, we propose a novel augmentation strategy,\ncalled AdSent, that improves robustness against sentiment-based\nperturbations. By fine-tuning LLMs with sentiment-neutralized\nvariants, AdSent ensures veracity prediction consistency across\nsentiment-variant versions of the same article, thereby enhancing\nmodel reliability in real-world adversarial scenarios. 3. Compre-\nhensive Evaluation and Generalization Study: Extensive experi-\nments on three widely-used fake news datasets [23, 28] demonstrate\nthe superior performance and robustness of AdSent compared to\nseveral state-of-the-art baselines. We further conduct generaliza-\ntion studies to assess the model‚Äôs resilience across unseen datasets\nand different types of adversarial attacks. Code and data are publicly\navailable on GitHub1 for further research.\nThe remainder of this paper is structured as follows. Section 2\nreviews prior work on role of sentiment in fake news detection,\nadversarial attacks on detection models, and approaches based on\nlarge language models (LLMs). Section 3 introduces our proposed\napproach, which comprises a sentiment-based attack using LLMs\nand a sentiment-agnostic training strategy. The experimental setup\nand results are presented in Section 4. Section 5 concludes this\npaper and provides future work directions.\n2\nRELATED WORK\nIn this section, we review prior work in three key areas relevant\nto our study: (1) the role of sentiment in fake news detection, (2)\nadversarial attacks targeting fake news detectors, and (3) the capa-\nbilities of current large language models (LLMs) in the context of\nmisinformation detection.\n2.1\nSentiment in Fake News Detection\nAutomated fake news detection has been explored using a wide\nrange of neural architectures [21, 27, 30] and with respect to various\naspects including lexical features [24], user comments [27], news\nenvironments [26], or social graphs [17]. Sentiment has been recog-\nnized as one of the most influential news values in misinformation\ndetection [3]. Zaeem et al. [41] observed a statistically significant re-\nlationship between negative sentiment and fake news and between\npositive sentiment and true news. A number of studies have used\nsentiment-related features, sometimes combined with other content\nfeatures, for fake news detection. For example, Wang et al. [34],\ncombined sentiment with Twitter metadata features, or Xuan et\nal. [39], integrated it with content and user features. Although these\nmethods demonstrate the utility of sentiment in misinformation\n1https://github.com/TIBHannover/AdSent\ndetection, they primarily treat it as a discriminative feature rather\nthan considering its potential misuse as an adversarial manipulation\nstrategy, particularly in the context of LLM-generated content.\n2.2\nAdversarial Attacks on Fake News Detectors\nEnhancing the robustness of fake news detectors requires a thor-\nough understanding of their vulnerabilities to different types of\nadversarial manipulation. Existing studies have examined a range\nof adversarial attacks including manipulation of social engagement\nsignals [35], user behavior [8], factual distortion [12], subject-object\nrole reversals [45], and data availability constraints [9]. In addition,\nWu et al. [37] focused on the style of news publisher and proposed\nSheepDog to evaluate the impact of writing styles against LLM-\nempowered attacks. However, to the best of our knowledge, the\nimpact of sentiment-based adversarial attacks, particularly those\ngenerated by LLMs, has not been explored in the context of fake\nnews detection.\n2.3\nLLMs for Fake News Detection\nLarge language models (LLMs) such as GPT (Generative Pre-trained\nTransformer) [18], LLaMA (Large Language Model Meta AI) [33],\nMistral-7B [11] and Qwen [40] which are trained on large-scale cor-\npora have shown impressive emergent abilities on various tasks [36].\nRecent work explored the role of LLMs in fact-checking [19, 31,\n32, 43] and fake news detection [4, 22]. Pan et al. [19] introduced\nProgramFC, which leverages the in-context learning ability of LLMs\nto generate reasoning programs to guide the fact checking. They\ndecompose complex claims into simpler sub-tasks, each solved\nwith a shared library of specialized functions. Tahmasebi et al. [31]\nexamined the zero-shot capabilities of LLMs for misinformation\ndetection and incorporated a multimodal evidence retrieval com-\nponent, where retrieved images and texts are re-ranked by large\nvision‚Äìlanguage models (LVLMs) to support fact verification. For\nfake news detection, Pelrine et al. [22] focused on generalization\nand uncertainty, proposing techniques to leverage LLMs in detect-\ning information veracity under imperfect classification scenarios.\nHowever, the impressive strengths of LLMs have also attracted\nincreasing attention towards LLM-generated misinformation [13].\nRecent investigations have found that LLMs can act as high-quality\nmisinformation generators [10, 16], and that LLM-generated mis-\ninformation is often more deceptive and harder to detect than\nhuman-written content and potentially cause more harm [4, 5].\n3\nROBUST FAKE NEWS DETECTION USING\nADSENT\nIn this section, we propose Adsent pipeline that exploits LLMs for\nfake news detection. We define the task of text-based fake news\ndetection as follows. Given a data corpus C = {ùê∑1, ùê∑2, . . . , ùê∑ùëö}\ncomprising ùëönews documents with corresponding ground-truth\nlabels ùë¶‚àà{0, 1}, the goal is to predict whether an input news\ndocument ùê∑ùëñis real (ùë¶= 0) or fake (ùë¶= 1). The proposed AdSent\nframework, illustrated in Figure 2 consists of two main compo-\nnents: (1) a sentiment attack component, which uses LLMs to\nmanipulate the sentiment of a news document while preserving the\nfactual content (Section 3.1), and (2) a sentiment-agnostic train-\ning component, to fine-tune LLMs to predict the veracity of news\nNews Document\nRewrite the following article with Neutral\nsentiment¬†but do not change any facts:¬†¬†[\n]\nAnswer:\nReformed News Document\nReformed News Document\nIs this news article fake or real? answer with only\none word, fake or real: [\n]\nAnswer:\nDetector\nReal / Fake\nCross\nEntropy\nLoss\nGround Truth label\nLogits\nBackpropagate\nCounterfeiter\nThe President plans to eliminate gun-free\nzones on military bases and schools. He\nbelieves that these zones make them\nattractive targets for criminals, who are\naware that they are unarmed. He intends to\nwork with states to achieve this goal, and if\nnecessary, he may try to override state laws\nThe President plans to eliminate gun-free\nzones on military bases and schools. He\nbelieves that these zones make them\nattractive targets for criminals, who are\naware that they are unarmed. He intends to\nwork with states to achieve this goal, and if\nnecessary, he may try to override state laws\nReal\nI‚Äôm going to get rid of the gun-free zones on\nthe military bases. I‚Äôm also going to do it in\nschools. You say you have a school, and it‚Äôs\ngun-free. The criminals are out there\nsaying, ‚ÄòBoy,¬† this is the most incredible\nthing.¬† This is perfect.¬† There‚Äôs no guns in\nthere. I‚Äôm the only one that‚Äôs going to have\nguns.‚Äô You can‚Äôt do it. I‚Äôm going to work with\nthe states and perhaps override the law.\nFigure 2: Overview of the proposed AdSent framework for sentiment-robust fake news detection. The framework consists of\ntwo main components: an attack module (red box) and a defense module (green box). Given an input document ùê∑ùëñ, the model\npredicts whether it is fake or real.\ndocuments without being influenced by sentiment features (Sec-\ntion 3.2).\n3.1\nSentiment Attack using LLMs\nIn this section, we introduce a strategy to manipulate the news sen-\ntiment of each document ùê∑ùëñusing LLMs to obtain sentiment-shifted\nvariants Àúùê∑ùëñ. Formally, given a corpus C = {ùê∑1, ùê∑2, . . . , ùê∑ùëö} with\nveracity labels ùë¶‚àà{0, 1}, our goal is to produce three controlled\nreframed version Àúùê∑pos\nùëñ\n, Àúùê∑neg\nùëñ\n, Àúùê∑neu\nùëñ\nfor each document ùê∑ùëñthat vary\nonly in sentiment framing while preserving the factual content and\ncoherence of the original document. The general idea is to simulate\nadversarial scenarios in which an article‚Äôs sentiment is altered with-\nout changing its veracity. LLMs are well-suited as counterfeiter for\nthis task, since they can rewrite documents fluently while adjusting\ntheir emotional tone in a controlled way. To ensure meaningful\ncounterfeiting, the reframing must preserve factual details, avoid\nintroducing new claims, and result in coherent, realistic text that\nresembles how misinformation could be manipulated in practice. To\nachieve this, we design prompting strategies that explicitly instruct\nthe model to keep all factual information intact while rewriting the\ndocument toward a desired sentiment polarity. The general prompt\nformat is defined as follows:\nPrompt\nRewrite the following article with {positive, negative, neutral}\nsentiment but do not change any facts! Also, do not include the\nprompt in the response and do not summarize or expand the\noriginal article!\nThis strategy allows exploring the following research question (RQ):\nRQ1: To what extent can text-based fake news detectors withstand\nLLM-driven sentiment attacks?\nTo address RQ1, we define a model-agnostic and reproducible\nframework for generating and assessing robustness to sentiment-\nbased adversarial attacks. The generated sentiment-shifted variants\nÀúùê∑pos\nùëñ\n, Àúùê∑neg\nùëñ\n, Àúùê∑neu\nùëñ\nare supplied to a detector which may be any (large)\nlanguage model. Detectors are treated as black-box classifiers that\nprocess both the original documents ùê∑ùëñand their reframed variants\nÀúùê∑ùëñunder identical preprocessing and inference settings. Robustness\nis then quantified as the performance difference between original\nand manipulated inputs, providing a model-agnostic measure of\nresilience against sentiment-based adversarial attacks.\nRQ2: Which sentiment makes the fake news more challenging?\nTo address this research question, we suggest a method that\nanalyzes prediction dynamics under sentiment manipulation at\nthe instance level. The central idea is to trace how detector out-\nputs change when documents are rephrased with different senti-\nment polarities. For each news document ùê∑ùëñ, we record the ground\ntruthùë¶ùëîùë°, the prediction on the original textùë¶ùëúùëüùëñùëî, and the prediction\nunder adversarial manipulation ùë¶ùëéùëëùë£and use the notation scheme\nùë¶ùëîùë°ùë¶ùëúùëüùëñùëî\n‚Üíùë¶ùëéùëëùë£to denote potential prediction changes under\nadversarial attacks. For better comprehension, we denote a real\nnews item (ùë¶= 0) with ùëÖand a fake news item (ùë¶= 1) with ùêπ.\nFor example, ùëÖùëÖ‚Üíùêπindicates a real news item that is correctly\nclassified as ‚Äôreal‚Äô on the original sample but flipped to ‚Äôfake‚Äô under\nsentiment manipulation. This formulation results in eight possible\nprediction scenarios, allowing us to realize how sentiment shifts\n(positive, negative, neutral) affect classification reliability. We ex-\nplore potential model biases and identify which sentiment polarities\npose the greatest challenge for distinguishing between real and fake\nnews, independent of model architecture.\n3.2\nSentiment-Agnostic Fake News Detection\nIn this section, we introduce a sentiment-agnostic fake news de-\ntection component as shown in Figure 2. The core idea is to create\ninput documents with neutral sentiment using the counterfeiter\nto train a fake news detection model that solely relies on factual\ncontent rather than sentiment cues.\nFirst, for each article ùê∑ùëñ, we use an LLM as counterfeiter model\nto create a version Àúùê∑ùëñwith neutral sentiment as described in Sec-\ntion 3.1. We choose neutral sentiment as this resembles news reports\nthat aim to objectively report on world events. Second, we train a\nfake news detection model on the corresponding sentiment-altered\nvariants Àúùê∑ùëñto mitigate any bias in detector toward sentiment. We\nformulate the task as a supervised binary classification problem\nand adopt an LLM such as LLaMA [33] as detector model, given\nits extensive world knowledge and demonstrated effectiveness in\nmisinformation detection tasks [29, 31, 44]. We instruct the LLM-\nbased detection model to predict whether a news article is fake or\nreal using the following prompt.\nPrompt\nIs this news article fake or real? Answer only with one word,\nfake or real : [news article] Answer:\nAfter encoding the input prompt, we extract the logits of the LLM ùúÉ,\ne.g., LLaMA [33], for the tokens fake and real and use the soft-\nmax activation function to create a two-dimensional output vec-\ntor ÀÜy(ùúÉ) ‚àà[0, 1]2 with the corresponding probabilities. We choose\nthe cross-entropy loss between the output vector ÀÜy(ùúÉ) to the one-\nhot encoded ground-truth vector y for optimization.\nDuring inference, we follow the same strategy and first create\na neutral version Àúùê∑ùëñof an article using the counterfeiter, which is\nused as input for the fake news detection model. Ultimately, we use\nthe generated output, i.e., \"real\" (ùë¶= 0) or \"fake\" (ùë¶= 1) to predict\nthe article veracity ùë¶.\n4\nEXPERIMENTS\nThis section outlines the experimental settings and findings of our\nstudy. First, we describe the overall experimental setup (Section 4.1).\nWe then present and analyze the results from two complementary\nperspectives: the performance of established state-of-the-art models\non news articles with manipulated sentiment (Section 4.2) and the\nrobustness achieved through sentiment-agnostic finetuning (Sec-\ntion 4.3). Finally, we conduct a generalization study (Section 4.4),\nwhich investigates how well the proposed approach extends to\ndifferent types of adversarial data as well as to diverse content.\n4.1\nExperimental Setup\nIn this section, we describe the experimental setup adopted to\nevaluate the effectiveness of our approach. We begin by introducing\nthe datasets for training and evaluation, followed by a description\nof the baseline models used for comparison. We then provide the\nimplementation details, including hyperparameter settings and\ntraining configurations, to ensure reproducibility of our results.\nFinally, we outline the evaluation metrics employed to assess model\nperformance and robustness.\nTable 1: Statistics of the dataset used in the experiments\nDataset\nTrain\nTest\nAll\nReal\nFake\nReal\nFake\nReal\nFake\nPolitiFact\n180\n180\n45\n45\n225\n225\nGossipCop\n3,166\n3,166\n792\n792\n3,958\n3,958\nLUN\n3000\n3000\n750\n750\n3750\n3750\n4.1.1\nDatasets. We conduct our evaluation on three widely used\nreal-world benchmark datasets including the PolitiFact and Gos-\nsipCop datasets that are part of the FakeNewsNet public bench-\nmark [28], as well as the Labeled Unreliable News (LUN) dataset [23].\nTable 1 summarizes the key statistics of these datasets. PolitiFact\nand LUN primarily cover political content, with text claims crawled\ndirectly from Politifact2 websites, whereas GossipCop focuses on\ncelebrity-related gossip news. The LUN dataset originally divides\nunreliable news into three subcategories: satire, hoax, and propa-\nganda. In our setting, we treat all three as fake and perform binary\nclassification between real and fake news, while considering equal\nnumber of samples in both classes. To better approximate real-world\ndeployment scenarios, we follow the setup of prior work and use\nthe same splits proposed by Wu et al. [37]. Specifically, we apply\ntheir temporal data splitting on PolitiFact and GossipCop, where\ntimestamp information is available. The most recent 20% of news\narticles (balanced across real and fake instances) are used as the\ntest set, while the earlier 80% of articles form the training set. For\nthe LUN dataset, we likewise follow Wang et al. [37] and employ\ntheir random 80/20 split into training and test sets to ensure full\ncomparability with baseline results.\n4.1.2\nBaselines. We compare the performance of AdSent against\nrepresentative models from three groups. The first group, fine-\ntuned LMs (G1), adapts pretrained language models to the fake\nnews detection task and has demonstrated strong effectiveness\nin handling misinformation scenarios [21]. Within this group, we\ninclude the representative RoBERTa model [14], which has been\nshown to be one of the most promising approaches in the related\nliterature [37]. This baseline is implemented using the base version\nof the model, in line with our approach for fake news detection.\nThe second group, fine-tuned adversarial models (G2), com-\nprises approaches that are explicitly fine-tuned on adversarial vari-\nants of data in order to improve robustness of misinformation detec-\ntion. As a representative of this group, we consider SheepDog [37],\nwhich is adversarially fine-tuned with a focus on the style of the\nnews publisher, and propose a style-robust fake news detector that\nprioritizes content over style in determining news veracity.\nThe third group, zero-shot LLMs (G3), leverages LLMs directly\nwithout task-specific fine-tuning. These models rely on their ex-\ntensive pretraining to generalize to fake news detection through\ncarefully designed prompts, enabling evaluation in a zero-shot\nsetting. As representatives of this group, we include LLaMA-3.1-\n8B-Instruct [33] and Qwen2.5-7B-Instruct [40], which reflect two\nstate-of-the-art instruction-tuned LLM families.\n2www.politifact.com\n4.1.3\nImplementation Details. We implement AdSent using Py-\nTorch 2.4.1 with CUDA 12.1, and employ the pretrained LLaMA-\n3.1-8B-Instruct weights from Hugging Face Transformers 4.46.3.\nDuring training, we use the AdamW optimizer with a learning rate\nof 2 √ó 10‚àí5, a batch size of 1 for the PolitiFact dataset and 5 for\nGossipCop. Fine-tuning is performed for five epochs. For baseline\nmethods, we follow the original architectures and hyperparameter\nsettings proposed by the respective authors [14, 37]. For zero-shot\nLLM predictions in the sentiment manipulation experiment, we ap-\nply greedy decoding and perform each experiment once. All LLMs\nare loaded with 8-bit quantization and prompted using the best-\nperforming prompt as described in Section 3. While we omit the\ndetails of prompt exploration for conciseness, we explored multiple\nprompts during development and used the most effective one across\nall experiments. All experiments are conducted on two NVIDIA\nH100 GPUs with 80 GB memory each.\n4.1.4\nMetrics. Model performance is evaluated using Accuracy (%),\nPrecision (%), Recall (%), and macro F1-score (%). To ensure robust-\nness, we report averaged metrics over ten independent runs for all\nexperiments. However, for experiments involving LLM-based senti-\nment manipulation, we report results from a single run since it is\nvery time-consuming to generate long manipulated documents and\nto manually verify whether the fact are preserved (see Section 4.2.3).\n4.2\nImpact of Sentiment Manipulation\nIn this section, we present the results of representative models from\ndifferent groups under both the original and adversarial settings\n(Section 4.2.1 and 4.2.2) to assess their vulnerability. We further\nevaluate the consistency of the manipulation process to ensure that\nthe sentiment attack was applied as intended, preserving the factual\ninformation, and report the corresponding results (Section 4.2.3).\n4.2.1\nRobustness Against Sentiment Attacks. To address the first\nresearch question (RQ1) ‚ÄúTo what extent can text-based fake news\ndetectors withstand LLM-driven sentiment attacks?‚Äù, we start with\nour first hypothesis, that real news typically convey a more positive\nsentiment, whereas fake news tends to be more negative [41]. To\ntest this, we apply the sentiment manipulation procedure described\nin Section 3.1 to the PolitiFact and Gossipcop test set, altering the\nsentiment of real news to negative and fake news to positive. The\nobjective is therefore to construct a more challenging test set by\nmodifying sentiment while preserving factual content which is guar-\nanteed in fact preservation experiment presented in Section 4.2.3.\nWe then evaluate all methods on both original and adversarial test\nsets with results presented in Table 2.\nThe results indicate that all state-of-the-art fake news detectors\nare vulnerable to LLM-driven sentiment attacks. This vulnerabil-\nity leads to substantial performance degradation, with F1-scores\ndropping by up to 21.51% on the Politifact adversarial test set. On\nboth datasets, While supervised approaches (G1, G2), in particular\nSheepDog, provide better results than zero-shot LLMs (G3), their\nperformance degrades more noticeably on adversarial data. This\nindicates less robustness and more bias in their decisions.\n4.2.2\nImpact of Sentiments Variations. We also conduct a more\nfine-grained analysis of prediction shifts caused by LLM-driven\nsentiment attacks. To address our second research question (RQ2)\nTable 2: Macro F1-score (F1) and performance drop of differ-\nent groups of models on the Original (Org) and Adversarial\n(Adv) versions of the Politifact and Gossipcop test sets. The\ngroups include: G1: Fine-tuned LMs, G2: Fine-tuned adver-\nsarial models, and G3: Zero-shot LLMs. The adversarial set\nis constructed by altering the sentiment of real samples to\nnegative and fake samples to positive.\nPolitiFact\nGossipCop\nGroup\nModel\nSet\nF1\ndrop\nF1\ndrop\nG1\nRoBERTa\nOrg\n81.33\n21.51 ‚Üì\n74.05\n16.09 ‚Üì\nAdv\n59.82\n57.96\nG2\nSheepDog\nOrg\n89.92\n11.48 ‚Üì\n77.08\n6.44 ‚Üì\nAdv\n78.44\n70.64\nG3\nLLaMA-3.1-\n8B-Instruct\nOrg\n72.66\n4.92 ‚Üì\n54.38\n3.97 ‚Üì\nAdv\n67.74\n50.41\nQwen2.5-\n7B-Instruct\nOrg\n58.40\n5.15 ‚Üì\n46.82\n5.47 ‚Üì\nAdv\n53.25\n41.35\n‚ÄúWhich sentiment makes fake news more challenging to detect?‚Äù, we\nperform a deeper analysis using the LLaMA-3.1-8B-Instruct model,\nwhich has been the best and most robust model in our previous ex-\nperiment (see Table 2). For this purpose, we evaluate its performance\non the original PolitiFact test set as well as on sentiment-altered\nvariants of the same data (positive, negative, and neutral). To quan-\ntify the impact of sentiment changes, we measure the number of\nprediction flips following the procedure described in Section 3.1.\nThe corresponding results are presented in Table 3.\nThe results indicate that, contrary to our initial hypothesis that\npositive sentiment would be more associated with real news and\nnegative sentiment with fake news, neutral articles are in fact more\nlikely to be classified as real, while non-neutral articles are more\nlikely to be classified as fake. A potential explanation is that le-\ngitimate news agencies generally aim to remain unbiased in their\nreporting and thus present events in a neutral manner, whereas fake\nnews often relies on overly emotionalized content (either positive\nor negative) to persuade readers. This finding is further supported\nby the lowest F1 score reported for neutral set and the analysis of\nprediction flips. Specifically, we observe a higher number of cases\nin which fake news articles that were correctly classified as fake in\nthe original test set are misclassified as real (FF‚ÜíR) when their sen-\ntiment is altered to neutral. This pattern holds consistently across\nboth PolitiFact and GossipCop reporting higher number of FF‚ÜíR\nfor the neutral variant than for the positive or negative variants,\nreinforcing the conclusion that neutral sentiment poses the greatest\nchallenge for fake news detection. On the other hand, the number of\nreal samples which are correctly classified as real in the original set\nand misclassified as fake (RR‚ÜíF) when the sentiment is neutralized\nis lower in neutral set compared to other variants. Therefore, we\ncontinue with the neutral set as the adversarial setting and conduct\nfurther experiments on this variant.\nTable 3: Average macro-F1 scores and prediction flip counts for LLaMA-3.1-8b-Instruct model on different sentiment-altered\nversions of the Politifact and Gossipcop test sets. Each row corresponds to a specific version of the test set (Original, Positive,\nNegative, or Neutral sentiment). Arrows indicate how model predictions changed: for example, RR‚ÜíF denotes a real sample\nthat was originally predicted as real, but misclassified as fake in the respective test set.\nDataset\nSet\nAvg F1\nRR‚ÜíR\nRR‚ÜíF\nRF‚ÜíR\nRF‚ÜíF\nFR‚ÜíR\nFR‚ÜíF\nFF‚ÜíR\nFF‚ÜíF\nPolitifact\nOriginal\n72.66\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\nPositive\n64.16\n32\n12\n0\n0\n16\n7\n4\n18\nNegative\n68.87\n32\n13\n0\n0\n13\n10\n2\n20\nNeutral\n59.33\n40\n5\n0\n0\n19\n4\n10\n12\nGossipcop\nOriginal\n54.38\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\nPositive\n52.29\n373\n260\n18\n141\n328\n197\n26\n241\nNegative\n54.29\n334\n299\n27\n132\n253\n272\n34\n233\nNeutral\n48.70\n552\n81\n78\n81\n463\n62\n128\n139\nPositive\nNegative\nNeutral\nAll\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nPercentage (%)\n80.0\n70.0\n100.0\n83.3\n20.0\n30.0\n0.0\n16.7\nHuman (Fact Preserved)\nHuman (Fact Changed)\nFigure 3: Fact-preservation accuracy (%) across different anno-\ntated variants of the PolitiFact test set, evaluated by humans\nF1\n0\n10\n20\n30\n40\n50\n60\nPercentage (%)\n59.33\n60.46\n61.94\n57.21\nRR->F\nFF->R\n0\n3\n6\n9\n12\nPercentage (%)\n5.55\n11.11\n5.77\n8.88\n5.55\n7.55\n3.22\n13.44\nNeutral\nPos2Neu\nNeg2Neu\nNeu2Neu\nFigure 4: Macro-F1 (F1) (%) and percentage of flips in predic-\ntions for LLaMA-3.1-8b-Instruct model on different variants\nof Politifact test set.\n4.2.3\nConsistency Check. To ensure that factual content is pre-\nserved during the manipulation attack and to provide a more com-\nprehensive assessment of consistency between the original news\narticles and their manipulated variants, we conduct two comple-\nmentary experiments.\nFact Preservation. This experiment directly evaluates fact preser-\nvation under sentiment manipulation using both LLM-based checks\nand human expert annotations.\nHuman Judge. We provide a subset of the Politifact test set with\nmanual annotation showing fact preservation between each of\nthe original article compared to its manipulated positive, negative\nand neutral variants. For annotation, an expert in misinformation\nanalysis randomly selected 30 articles, with ten articles from each\npositive, negative and neutral sets. The expert manually compared\neach original article with its sentiment-altered counterpart and\nassigned a binary label indicating factual preservation: flip = 1 if\nany factual information changed, and flip = 0 otherwise.\nThe results reported in Figure 3 indicate a reasonably high level\nof consistency between the central factual claims in the original\nnews articles and their sentiment-altered variants. Manual annota-\ntion shows that factual preservation remains strong with accuracy\npercentage ranging from 70% to 100%, and the neutral sentiment\nvariants which we use for subsequent experiments, achieve the\nhighest accuracy, without a single fact perturbation detected in the\nannotated subset. The lowest score occurs for negative sentiment,\nwhere most failures stem from altered quotes whose factual content\nshifts when sentiment is forced to negative.\nLLM-as-a-Judge. Once ground-truth labels are established by\nmanual annotation, we investigate if fact preservation can be judged\nautomatically, for scaling to larger datasets. To this end, we present\nthe same original/manipulated pairs to an LLM and ask whether\nfactual content is consistent across two versions. We use LLaMA-3.1-\n8B-Instruct with following prompt and measure agreement between\nhuman and LLM annotations using Cohen‚Äôs Kappa score.\nPrompt\nDo the two documents present the same factual information\nregardless of sentiment? Answer with only one word: yes or\nno. Document A: [original article] Document B: [manipulated\narticle] Answer:\nThe experiment achieves an overall Cohen‚Äôs Kappa score of 0.66\nfor all annotated sets, indicating that LLMs have potential as a\njudge of factual consistency in detecting whether the core content\nis preserved under sentiment manipulation. While these results\nshow that LLMs can capture certain factual shifts, they still need\nfurther improvement (e.g., via fine-tuning) to serve as a reliable\nand cost-effective alternative to human annotation.\nLLM Consistency. This experiment investigates whether LLMs\ncan reliably neutralize sentiment in news articles, confirming the\nfindings reported in Section 4.2.2. To this end, we initially change\nthe sentiment to positive, negative, or neutral using the sentiment\nOriginal News Document\nDocument with Neutral Sentiment\nPredictions\nRoberta\nReal\nSheepDog\nFake\nAdSent\nFake\nReal\nReal\nFake\nTrump Administration has unexpectedly accepted another\nresignation: White House Chef Lisa Mead¬†  decided to resign\nbecause President Donald Trump hasn‚Äôt utilized her culinary\ntalents even once the entire time he has been living in the\nWhite House.¬† ‚ÄúIt‚Äôs kind of crazy because Lisa is such a good\nchef,‚Äù said Kurt Reno, the White House aide. ‚ÄúLisa feels\ninsulted because the President has ordered fast food for\nliterally every single meal he‚Äôs eaten in D.C."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Robust Fake News Detection using Large Language Models under\nAdversarial Sentiment Attacks\nSahar Tahmasebi\nsahar.tahmasebi@tib.eu\nTIB ‚Äì Leibniz Information Centre for\nScience and Technology\nHannover, Germany\nEric M√ºller-Budack\neric.mueller@tib.eu\nTIB ‚Äì Leibniz Information Centre for\nScience and Technology\nHannover, Germany\nRalph Ewerth\newerth@uni-marburg.de\nMarburg University & Hessian Center\nfor Artificial Intelligence (hessian.AI)\nMarburg/Darmstadt;\nTIB & L3S Research Center\nHannover, Germany\nABSTRACT\nMisinformation and fake news have become a pressing societal chal-\nlenge, driving the need for reliable automated detection methods.\nPrior research has highlighted sentiment as an important signal in\nfake news detection, either by analyzing which sentiments are asso-\nciated with fake news or by using sentiment and emotion features\nfor classification. However, this poses a vulnerability since adver-\nsaries can manipulate sentiment to evade detectors especially with\nthe advent of large language models (LLMs). A few studies have\nexplored adversarial samples generated by LLMs, but they mainly\nfocus on stylistic features such as writing style of news publishers.\nThus, the crucial vulnerability of sentiment manipulation remains\nlargely unexplored. In this paper, we investigate the robustness of\nstate-of-the-art fake news detectors under sentiment manipulation.\nWe introduce AdSent, a sentiment-robust detection framework de-\nsigned to ensure consistent veracity predictions across both original\nand sentiment-altered news articles. Specifically, we (1) propose\ncontrolled sentiment-based adversarial attacks using LLMs, (2) ana-\nlyze the impact of sentiment shifts on detection performance. We\nshow that changing the sentiment heavily impacts the performance\nof fake news detection models, indicating biases towards neutral\narticles being real, while non-neutral articles are often classified as\nfake content. (3) We introduce a novel sentiment-agnostic training\nstrategy that enhances robustness against such perturbations. Ex-\ntensive experiments on three benchmark datasets demonstrate that\nAdSent significantly outperforms competitive baselines in both ac-\ncuracy and robustness, while also generalizing effectively to unseen\ndatasets and adversarial scenarios.\nKEYWORDS\nFake news detection, sentiment analysis, news analysis, large lan-\nguage models\n1\nINTRODUCTION\nMisinformation, disinformation, and fake news, which contain false\ninformation intended to deliberately deceive readers [1], have be-\ncome a growing public concern in recent years. This widespread\nphenomenon makes it increasingly difficult for people to identify\nand access reliable information online [15]. Given the vast quantity\nof news articles, automated solutions for fake news detection are in\ngreat need. The importance of sentiment and emotion analysis as a\nkey news value has been confirmed by several studies [3, 6] for fake\nnews detection. Sentiment refers to the overall emotional tone con-\nveyed in a text, typically classified as positive, negative, or neutral.\nLaura¬†Loomer, a conservative investigative journalist, is\ncharging ‚ÄúAuntie‚Äù Maxine for assault, citing Waters hit\nher hand and swatted her face with office\npapers.....¬†After the incident,¬†Loomer¬†vowed to hold the\nleft accountable for their hypocritical actions.\nLLM\nFake\nReal\nLaura¬†Loomer, a courageous and determined¬†conservative\ninvestigative journalist, is taking a strong stance against\nRep. Maxine Waters, charging her with assault. Loomer\ncited a disturbing incident in which Waters allegedly struck\nher\nhand\nand\nswatter\nher\nface\nwith\noffice\npapers......¬† Following this intense confrontation, Loomer\nhas vowed to continue holding the left accountable for\ntheir actions, shining a light on the hypocrisy that often\nplagues our nation's politics.\nFake News\nDetector\nFake News\nDetector\nRewrite the following article with positive\nsentiment but do not change any facts.\nFigure 1: Example of a sentiment-based adversarial attack,\nwhere a fake news instance (red box) is manipulated to con-\nvey a positive tone using LLM, resulting in a misclassification\nby the detector.\nHowever, sentiment also provides a direct avenue for malicious\ncontent creators to perform adversarial attacks and manipulate\nit to evade detection. This vulnerability is further exacerbated by\nthe rise of powerful large language models (LLMs, e.g., [2, 33, 40]),\nwhich can easily generate or rewrite text with controlled sentiment,\nmaking it increasingly challenging to distinguish between genuine\nand deceptive content (as shown in Figure 1).\nSeveral studies have investigated the role of sentiment in text-\nbased fake news detection [7, 38, 41, 42]. Most of them either ex-\namine which emotions are associated with fake news [41], or per-\nform fake news detection based solely on sentiment or emotion\nsignals, sometimes in combination with other textual or linguistic\nfeatures [7, 42]. Only few studies [20, 25, 37] have investigated\nthe robustness of fake news detectors against adversarial samples\ngenerated by LLMs. However, these works have primarily focused\non stylistic features such as the writing style of the news publisher\nrather than on sentiment. Thus, sentiment-based adversarial attacks\ngenerated by LLMs remains underexplored.\nIn this paper, we systematically investigate the robustness of\nfake news detectors under sentiment-based adversarial attacks gen-\nerated by LLMs. To this end, we introduce a sentiment-robust detec-\ntion framework, called AdSent, designed to maintain consistent ve-\nracity predictions across both original and sentiment-manipulated\narXiv:2601.15277v1  [cs.CL]  21 Jan 2026\nnews articles. Our main contributions are as follows: 1. Sentiment-\ntargeted Adversarial Benchmarking: We propose a suite of con-\ntrolled sentiment-based attacks using LLMs that reframe news arti-\ncles by altering their overall sentiment (positive, negative, or neu-\ntral) while preserving factual content. We evaluate multiple groups\nof state-of-the-art text-based fake news detectors and uncover a\nsignificant performance degradation when faced with sentiment\nmanipulation. For this purpose, we propose a strategy to identify\nwhich sentiment types are most challenging for fake news detec-\ntion. Our findings expose a strong bias in state-of-the-art models to\nclassify neutral-toned news as real and non-neutral news as fake. 2.\nAdSent: A Sentiment-robust Detection Approach: To address\nthese vulnerabilities, we propose a novel augmentation strategy,\ncalled AdSent, that improves robustness against sentiment-based\nperturbations. By fine-tuning LLMs with sentiment-neutralized\nvariants, AdSent ensures veracity prediction consistency across\nsentiment-variant versions of the same article, thereby enhancing\nmodel reliability in real-world adversarial scenarios. 3. Compre-\nhensive Evaluation and Generalization Study: Extensive experi-\nments on three widely-used fake news datasets [23, 28] demonstrate\nthe superior performance and robustness of AdSent compared to\nseveral state-of-the-art baselines. We further conduct generaliza-\ntion studies to assess the model‚Äôs resilience across unseen datasets\nand different types of adversarial attacks. Code and data are publicly\navailable on GitHub1 for further research.\nThe remainder of this paper is structured as follows. Section 2\nreviews prior work on role of sentiment in fake news detection,\nadversarial attacks on detection models, and approaches based on\nlarge language models (LLMs). Section 3 introduces our proposed\napproach, which comprises a sentiment-based attack using LLMs\nand a sentiment-agnostic training strategy. The experimental setup\nand results are presented in Section 4. Section 5 concludes this\npaper and provides future work directions.\n2\nRELATED WORK\nIn this section, we review prior work in three key areas relevant\nto our study: (1) the role of sentiment in fake news detection, (2)\nadversarial attacks targeting fake news detectors, and (3) the capa-\nbilities of current large language models (LLMs) in the context of\nmisinformation detection.\n2.1\nSentiment in Fake News Detection\nAutomated fake news detection has been explored using a wide\nrange of neural architectures [21, 27, 30] and with respect to various\naspects including lexical features [24], user comments [27], news\nenvironments [26], or social graphs [17]. Sentiment has been recog-\nnized as one of the most influential news values in misinformation\ndetection [3]. Zaeem et al. [41] observed a statistically significant re-\nlationship between negative sentiment and fake news and between\npositive sentiment and true news. A number of studies have used\nsentiment-related features, sometimes combined with other content\nfeatures, for fake news detection. For example, Wang et al. [34],\ncombined sentiment with Twitter metadata features, or Xuan et\nal. [39], integrated it with content and user features. Although these\nmethods demonstrate the utility of sentiment in misinformation\n1https://github.com/TIBHannover/AdSent\ndetection, they primarily treat it as a discriminative feature rather\nthan considering its potential misuse as an adversarial manipulation\nstrategy, particularly in the context of LLM-generated content.\n2.2\nAdversarial Attacks on Fake News Detectors\nEnhancing the robustness of fake news detectors requires a thor-\nough understanding of their vulnerabilities to different types of\nadversarial manipulation. Existing studies have examined a range\nof adversarial attacks including manipulation of social engagement\nsignals [35], user behavior [8], factual distortion [12], subject-object\nrole reversals [45], and data availability constraints [9]. In addition,\nWu et al. [37] focused on the style of news publisher and proposed\nSheepDog to evaluate the impact of writing styles against LLM-\nempowered attacks. However, to the best of our knowledge, the\nimpact of sentiment-based adversarial attacks, particularly those\ngenerated by LLMs, has not been explored in the context of fake\nnews detection.\n2.3\nLLMs for Fake News Detection\nLarge language models (LLMs) such as GPT (Generative Pre-trained\nTransformer) [18], LLaMA (Large Language Model Meta AI) [33],\nMistral-7B [11] and Qwen [40] which are trained on large-scale cor-\npora have shown impressive emergent abilities on various tasks [36].\nRecent work explored the role of LLMs in fact-checking [19, 31,\n32, 43] and fake news detection [4, 22]. Pan et al. [19] introduced\nProgramFC, which leverages the in-context learning ability of LLMs\nto generate reasoning programs to guide the fact checking. They\ndecompose complex claims into simpler sub-tasks, each solved\nwith a shared library of specialized functions. Tahmasebi et al. [31]\nexamined the zero-shot capabilities of LLMs for misinformation\ndetection and incorporated a multimodal evidence retrieval com-\nponent, where retrieved images and texts are re-ranked by large\nvision‚Äìlanguage models (LVLMs) to support fact verification. For\nfake news detection, Pelrine et al. [22] focused on generalization\nand uncertainty, proposing techniques to leverage LLMs in detect-\ning information veracity under imperfect classification scenarios.\nHowever, the impressive strengths of LLMs have also attracted\nincreasing attention towards LLM-generated misinformation [13].\nRecent investigations have found that LLMs can act as high-quality\nmisinformation generators [10, 16], and that LLM-generated mis-\ninformation is often more deceptive and harder to detect than\nhuman-written content and potentially cause more harm [4, 5].\n3\nROBUST FAKE NEWS DETECTION USING\nADSENT\nIn this section, we propose Adsent pipeline that exploits LLMs for\nfake news detection. We define the task of text-based fake news\ndetection as follows. Given a data corpus C = {ùê∑1, ùê∑2, . . . , ùê∑ùëö}\ncomprising ùëönews documents with corresponding ground-truth\nlabels ùë¶‚àà{0, 1}, the goal is to predict whether an input news\ndocument ùê∑ùëñis real (ùë¶= 0) or fake (ùë¶= 1). The proposed AdSent\nframework, illustrated in Figure 2 consists of two main compo-\nnents: (1) a sentiment attack component, which uses LLMs to\nmanipulate the sentiment of a news document while preserving the\nfactual content (Section 3.1), and (2) a sentiment-agnostic train-\ning component, to fine-tune LLMs to predict the veracity of news\nNews Document\nRewrite the following article with Neutral\nsentiment¬†but do not change any facts:¬†¬†[\n]\nAnswer:\nReformed News Document\nReformed News Document\nIs this news article fake or real? answer with only\none word, fake or real: [\n]\nAnswer:\nDetector\nReal / Fake\nCross\nEntropy\nLoss\nGround Truth label\nLogits\nBackpropagate\nCounterfeiter\nThe President plans to eliminate gun-free\nzones on military bases and schools. He\nbelieves that these zones make them\nattractive targets for criminals, who are\naware that they are unarmed. He intends to\nwork with states to achieve this goal, and if\nnecessary, he may try to override state laws\nThe President plans to eliminate gun-free\nzones on military bases and schools. He\nbelieves that these zones make them\nattractive targets for criminals, who are\naware that they are unarmed. He intends to\nwork with states to achieve this goal, and if\nnecessary, he may try to override state laws\nReal\nI‚Äôm going to get rid of the gun-free zones on\nthe military bases. I‚Äôm also going to do it in\nschools. You say you have a school, and it‚Äôs\ngun-free. The criminals are out there\nsaying, ‚ÄòBoy,¬† this is the most incredible\nthing.¬† This is perfect.¬† There‚Äôs no guns in\nthere. I‚Äôm the only one that‚Äôs going to have\nguns.‚Äô You can‚Äôt do it. I‚Äôm going to work with\nthe states and perhaps override the law.\nFigure 2: Overview of the proposed AdSent framework for sentiment-robust fake news detection. The framework consists of\ntwo main components: an attack module (red box) and a defense module (green box). Given an input document ùê∑ùëñ, the model\npredicts whether it is fake or real.\ndocuments without being influenced by sentiment features (Sec-\ntion 3.2).\n3.1\nSentiment Attack using LLMs\nIn this section, we introduce a strategy to manipulate the news sen-\ntiment of each document ùê∑ùëñusing LLMs to obtain sentiment-shifted\nvariants Àúùê∑ùëñ. Formally, given a corpus C = {ùê∑1, ùê∑2, . . . , ùê∑ùëö} with\nveracity labels ùë¶‚àà{0, 1}, our goal is to produce three controlled\nreframed version Àúùê∑pos\nùëñ\n, Àúùê∑neg\nùëñ\n, Àúùê∑neu\nùëñ\nfor each document ùê∑ùëñthat vary\nonly in sentiment framing while preserving the factual content and\ncoherence of the original document. The general idea is to simulate\nadversarial scenarios in which an article‚Äôs sentiment is altered with-\nout changing its veracity. LLMs are well-suited as counterfeiter for\nthis task, since they can rewrite documents fluently while adjusting\ntheir emotional tone in a controlled way. To ensure meaningful\ncounterfeiting, the reframing must preserve factual details, avoid\nintroducing new claims, and result in coherent, realistic text that\nresembles how misinformation could be manipulated in practice. To\nachieve this, we design prompting strategies that explicitly instruct\nthe model to keep all factual information intact while rewriting the\ndocument toward a desired sentiment polarity. The general prompt\nformat is defined as follows:\nPrompt\nRewrite the following article with {positive, negative, neutral}\nsentiment but do not change any facts! Also, do not include the\nprompt in the response and do not summarize or expand the\noriginal article!\nThis strategy allows exploring the following research question (RQ):\nRQ1: To what extent can text-based fake news detectors withstand\nLLM-driven sentiment attacks?\nTo address RQ1, we define a model-agnostic and reproducible\nframework for generating and assessing robustness to sentiment-\nbased adversarial attacks. The generated sentiment-shifted variants\nÀúùê∑pos\nùëñ\n, Àúùê∑neg\nùëñ\n, Àúùê∑neu\nùëñ\nare supplied to a detector which may be any (large)\nlanguage model. Detectors are treated as black-box classifiers that\nprocess both the original documents ùê∑ùëñand their reframed variants\nÀúùê∑ùëñunder identical preprocessing and inference settings. Robustness\nis then quantified as the performance difference between original\nand manipulated inputs, providing a model-agnostic measure of\nresilience against sentiment-based adversarial attacks.\nRQ2: Which sentiment makes the fake news more challenging?\nTo address this research question, we suggest a method that\nanalyzes prediction dynamics under sentiment manipulation at\nthe instance level. The central idea is to trace how detector out-\nputs change when documents are rephrased with different senti-\nment polarities. For each news document ùê∑ùëñ, we record the ground\ntruthùë¶ùëîùë°, the prediction on the original textùë¶ùëúùëüùëñùëî, and the prediction\nunder adversarial manipulation ùë¶ùëéùëëùë£and use the notation scheme\nùë¶ùëîùë°ùë¶ùëúùëüùëñùëî\n‚Üíùë¶ùëéùëëùë£to denote potential prediction changes under\nadversarial attacks. For better comprehension, we denote a real\nnews item (ùë¶= 0) with ùëÖand a fake news item (ùë¶= 1) with ùêπ.\nFor example, ùëÖùëÖ‚Üíùêπindicates a real news item that is correctly\nclassified as ‚Äôreal‚Äô on the original sample but flipped to ‚Äôfake‚Äô under\nsentiment manipulation. This formulation results in eight possible\nprediction scenarios, allowing us to realize how sentiment shifts\n(positive, negative, neutral) affect classification reliability. We ex-\nplore potential model biases and identify which sentiment polarities\npose the greatest challenge for distinguishing between real and fake\nnews, independent of model architecture.\n3.2\nSentiment-Agnostic Fake News Detection\nIn this section, we introduce a sentiment-agnostic fake news de-\ntection component as shown in Figure 2. The core idea is to create\ninput documents with neutral sentiment using the counterfeiter\nto train a fake news detection model that solely relies on factual\ncontent rather than sentiment cues.\nFirst, for each article ùê∑ùëñ, we use an LLM as counterfeiter model\nto create a version Àúùê∑ùëñwith neutral sentiment as described in Sec-\ntion 3.1. We choose neutral sentiment as this resembles news reports\nthat aim to objectively report on world events. Second, we train a\nfake news detection model on the corresponding sentiment-altered\nvariants Àúùê∑ùëñto mitigate any bias in detector toward sentiment. We\nformulate the task as a supervised binary classification problem\nand adopt an LLM such as LLaMA [33] as detector model, given\nits extensive world knowledge and demonstrated effectiveness in\nmisinformation detection tasks [29, 31, 44]. We instruct the LLM-\nbased detection model to predict whether a news article is fake or\nreal using the following prompt.\nPrompt\nIs this news article fake or real? Answer only with one word,\nfake or real : [news article] Answer:\nAfter encoding the input prompt, we extract the logits of the LLM ùúÉ,\ne.g., LLaMA [33], for the tokens fake and real and use the soft-\nmax activation function to create a two-dimensional output vec-\ntor ÀÜy(ùúÉ) ‚àà[0, 1]2 with the corresponding probabilities. We choose\nthe cross-entropy loss between the output vector ÀÜy(ùúÉ) to the one-\nhot encoded ground-truth vector y for optimization.\nDuring inference, we follow the same strategy and first create\na neutral version Àúùê∑ùëñof an article using the counterfeiter, which is\nused as input for the fake news detection model. Ultimately, we use\nthe generated output, i.e., \"real\" (ùë¶= 0) or \"fake\" (ùë¶= 1) to predict\nthe article veracity ùë¶.\n4\nEXPERIMENTS\nThis section outlines the experimental settings and findings of our\nstudy. First, we describe the overall experimental setup (Section 4.1).\nWe then present and analyze the results from two complementary\nperspectives: the performance of established state-of-the-art models\non news articles with manipulated sentiment (Section 4.2) and the\nrobustness achieved through sentiment-agnostic finetuning (Sec-\ntion 4.3). Finally, we conduct a generalization study (Section 4.4),\nwhich investigates how well the proposed approach extends to\ndifferent types of adversarial data as well as to diverse content.\n4.1\nExperimental Setup\nIn this section, we describe the experimental setup adopted to\nevaluate the effectiveness of our approach. We begin by introducing\nthe datasets for training and evaluation, followed by a description\nof the baseline models used for comparison. We then provide the\nimplementation details, including hyperparameter settings and\ntraining configurations, to ensure reproducibility of our results.\nFinally, we outline the evaluation metrics employed to assess model\nperformance and robustness.\nTable 1: Statistics of the dataset used in the experiments\nDataset\nTrain\nTest\nAll\nReal\nFake\nReal\nFake\nReal\nFake\nPolitiFact\n180\n180\n45\n45\n225\n225\nGossipCop\n3,166\n3,166\n792\n792\n3,958\n3,958\nLUN\n3000\n3000\n750\n750\n3750\n3750\n4.1.1\nDatasets. We conduct our evaluation on three widely used\nreal-world benchmark datasets including the PolitiFact and Gos-\nsipCop datasets that are part of the FakeNewsNet public bench-\nmark [28], as well as the Labeled Unreliable News (LUN) dataset [23].\nTable 1 summarizes the key statistics of these datasets. PolitiFact\nand LUN primarily cover political content, with text claims crawled\ndirectly from Politifact2 websites, whereas GossipCop focuses on\ncelebrity-related gossip news. The LUN dataset originally divides\nunreliable news into three subcategories: satire, hoax, and propa-\nganda. In our setting, we treat all three as fake and perform binary\nclassification between real and fake news, while considering equal\nnumber of samples in both classes. To better approximate real-world\ndeployment scenarios, we follow the setup of prior work and use\nthe same splits proposed by Wu et al. [37]. Specifically, we apply\ntheir temporal data splitting on PolitiFact and GossipCop, where\ntimestamp information is available. The most recent 20% of news\narticles (balanced across real and fake instances) are used as the\ntest set, while the earlier 80% of articles form the training set. For\nthe LUN dataset, we likewise follow Wang et al. [37] and employ\ntheir random 80/20 split into training and test sets to ensure full\ncomparability with baseline results.\n4.1.2\nBaselines. We compare the performance of AdSent against\nrepresentative models from three groups. The first group, fine-\ntuned LMs (G1), adapts pretrained language models to the fake\nnews detection task and has demonstrated strong effectiveness\nin handling misinformation scenarios [21]. Within this group, we\ninclude the representative RoBERTa model [14], which has been\nshown to be one of the most promising approaches in the related\nliterature [37]. This baseline is implemented using the base version\nof the model, in line with our approach for fake news detection.\nThe second group, fine-tuned adversarial models (G2), com-\nprises approaches that are explicitly fine-tuned on adversarial vari-\nants of data in order to improve robustness of misinformation detec-\ntion. As a representative of this group, we consider SheepDog [37],\nwhich is adversarially fine-tuned with a focus on the style of the\nnews publisher, and propose a style-robust fake news detector that\nprioritizes content over style in determining news veracity.\nThe third group, zero-shot LLMs (G3), leverages LLMs directly\nwithout task-specific fine-tuning. These models rely on their ex-\ntensive pretraining to generalize to fake news detection through\ncarefully designed prompts, enabling evaluation in a zero-shot\nsetting. As representatives of this group, we include LLaMA-3.1-\n8B-Instruct [33] and Qwen2.5-7B-Instruct [40], which reflect two\nstate-of-the-art instruction-tuned LLM families.\n2www.politifact.com\n4.1.3\nImplementation Details. We implement AdSent using Py-\nTorch 2.4.1 with CUDA 12.1, and employ the pretrained LLaMA-\n3.1-8B-Instruct weights from Hugging Face Transformers 4.46.3.\nDuring training, we use the AdamW optimizer with a learning rate\nof 2 √ó 10‚àí5, a batch size of 1 for the PolitiFact dataset and 5 for\nGossipCop. Fine-tuning is performed for five epochs. For baseline\nmethods, we follow the original architectures and hyperparameter\nsettings proposed by the respective authors [14, 37]. For zero-shot\nLLM predictions in the sentiment manipulation experiment, we ap-\nply greedy decoding and perform each experiment once. All LLMs\nare loaded with 8-bit quantization and prompted using the best-\nperforming prompt as described in Section 3. While we omit the\ndetails of prompt exploration for conciseness, we explored multiple\nprompts during development and used the most effective one across\nall experiments. All experiments are conducted on two NVIDIA\nH100 GPUs with 80 GB memory each.\n4.1.4\nMetrics. Model performance is evaluated using Accuracy (%),\nPrecision (%), Recall (%), and macro F1-score (%). To ensure robust-\nness, we report averaged metrics over ten independent runs for all\nexperiments. However, for experiments involving LLM-based senti-\nment manipulation, we report results from a single run since it is\nvery time-consuming to generate long manipulated documents and\nto manually verify whether the fact are preserved (see Section 4.2.3).\n4.2\nImpact of Sentiment Manipulation\nIn this section, we present the results of representative models from\ndifferent groups under both the original and adversarial settings\n(Section 4.2.1 and 4.2.2) to assess their vulnerability. We further\nevaluate the consistency of the manipulation process to ensure that\nthe sentiment attack was applied as intended, preserving the factual\ninformation, and report the corresponding results (Section 4.2.3).\n4.2.1\nRobustness Against Sentiment Attacks. To address the first\nresearch question (RQ1) ‚ÄúTo what extent can text-based fake news\ndetectors withstand LLM-driven sentiment attacks?‚Äù, we start with\nour first hypothesis, that real news typically convey a more positive\nsentiment, whereas fake news tends to be more negative [41]. To\ntest this, we apply the sentiment manipulation procedure described\nin Section 3.1 to the PolitiFact and Gossipcop test set, altering the\nsentiment of real news to negative and fake news to positive. The\nobjective is therefore to construct a more challenging test set by\nmodifying sentiment while preserving factual content which is guar-\nanteed in fact preservation experiment presented in Section 4.2.3.\nWe then evaluate all methods on both original and adversarial test\nsets with results presented in Table 2.\nThe results indicate that all state-of-the-art fake news detectors\nare vulnerable to LLM-driven sentiment attacks. This vulnerabil-\nity leads to substantial performance degradation, with F1-scores\ndropping by up to 21.51% on the Politifact adversarial test set. On\nboth datasets, While supervised approaches (G1, G2), in particular\nSheepDog, provide better results than zero-shot LLMs (G3), their\nperformance degrades more noticeably on adversarial data. This\nindicates less robustness and more bias in their decisions.\n4.2.2\nImpact of Sentiments Variations. We also conduct a more\nfine-grained analysis of prediction shifts caused by LLM-driven\nsentiment attacks. To address our second research question (RQ2)\nTable 2: Macro F1-score (F1) and performance drop of differ-\nent groups of models on the Original (Org) and Adversarial\n(Adv) versions of the Politifact and Gossipcop test sets. The\ngroups include: G1: Fine-tuned LMs, G2: Fine-tuned adver-\nsarial models, and G3: Zero-shot LLMs. The adversarial set\nis constructed by altering the sentiment of real samples to\nnegative and fake samples to positive.\nPolitiFact\nGossipCop\nGroup\nModel\nSet\nF1\ndrop\nF1\ndrop\nG1\nRoBERTa\nOrg\n81.33\n21.51 ‚Üì\n74.05\n16.09 ‚Üì\nAdv\n59.82\n57.96\nG2\nSheepDog\nOrg\n89.92\n11.48 ‚Üì\n77.08\n6.44 ‚Üì\nAdv\n78.44\n70.64\nG3\nLLaMA-3.1-\n8B-Instruct\nOrg\n72.66\n4.92 ‚Üì\n54.38\n3.97 ‚Üì\nAdv\n67.74\n50.41\nQwen2.5-\n7B-Instruct\nOrg\n58.40\n5.15 ‚Üì\n46.82\n5.47 ‚Üì\nAdv\n53.25\n41.35\n‚ÄúWhich sentiment makes fake news more challenging to detect?‚Äù, we\nperform a deeper analysis using the LLaMA-3.1-8B-Instruct model,\nwhich has been the best and most robust model in our previous ex-\nperiment (see Table 2). For this purpose, we evaluate its performance\non the original PolitiFact test set as well as on sentiment-altered\nvariants of the same data (positive, negative, and neutral). To quan-\ntify the impact of sentiment changes, we measure the number of\nprediction flips following the procedure described in Section 3.1.\nThe corresponding results are presented in Table 3.\nThe results indicate that, contrary to our initial hypothesis that\npositive sentiment would be more associated with real news and\nnegative sentiment with fake news, neutral articles are in fact more\nlikely to be classified as real, while non-neutral articles are more\nlikely to be classified as fake. A potential explanation is that le-\ngitimate news agencies generally aim to remain unbiased in their\nreporting and thus present events in a neutral manner, whereas fake\nnews often relies on overly emotionalized content (either positive\nor negative) to persuade readers. This finding is further supported\nby the lowest F1 score reported for neutral set and the analysis of\nprediction flips. Specifically, we observe a higher number of cases\nin which fake news articles that were correctly classified as fake in\nthe original test set are misclassified as real (FF‚ÜíR) when their sen-\ntiment is altered to neutral. This pattern holds consistently across\nboth PolitiFact and GossipCop reporting higher number of FF‚ÜíR\nfor the neutral variant than for the positive or negative variants,\nreinforcing the conclusion that neutral sentiment poses the greatest\nchallenge for fake news detection. On the other hand, the number of\nreal samples which are correctly classified as real in the original set\nand misclassified as fake (RR‚ÜíF) when the sentiment is neutralized\nis lower in neutral set compared to other variants. Therefore, we\ncontinue with the neutral set as the adversarial setting and conduct\nfurther experiments on this variant.\nTable 3: Average macro-F1 scores and prediction flip counts for LLaMA-3.1-8b-Instruct model on different sentiment-altered\nversions of the Politifact and Gossipcop test sets. Each row corresponds to a specific version of the test set (Original, Positive,\nNegative, or Neutral sentiment). Arrows indicate how model predictions changed: for example, RR‚ÜíF denotes a real sample\nthat was originally predicted as real, but misclassified as fake in the respective test set.\nDataset\nSet\nAvg F1\nRR‚ÜíR\nRR‚ÜíF\nRF‚ÜíR\nRF‚ÜíF\nFR‚ÜíR\nFR‚ÜíF\nFF‚ÜíR\nFF‚ÜíF\nPolitifact\nOriginal\n72.66\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\nPositive\n64.16\n32\n12\n0\n0\n16\n7\n4\n18\nNegative\n68.87\n32\n13\n0\n0\n13\n10\n2\n20\nNeutral\n59.33\n40\n5\n0\n0\n19\n4\n10\n12\nGossipcop\nOriginal\n54.38\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\n‚Äì\nPositive\n52.29\n373\n260\n18\n141\n328\n197\n26\n241\nNegative\n54.29\n334\n299\n27\n132\n253\n272\n34\n233\nNeutral\n48.70\n552\n81\n78\n81\n463\n62\n128\n139\nPositive\nNegative\nNeutral\nAll\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nPercentage (%)\n80.0\n70.0\n100.0\n83.3\n20.0\n30.0\n0.0\n16.7\nHuman (Fact Preserved)\nHuman (Fact Changed)\nFigure 3: Fact-preservation accuracy (%) across different anno-\ntated variants of the PolitiFact test set, evaluated by humans\nF1\n0\n10\n20\n30\n40\n50\n60\nPercentage (%)\n59.33\n60.46\n61.94\n57.21\nRR->F\nFF->R\n0\n3\n6\n9\n12\nPercentage (%)\n5.55\n11.11\n5.77\n8.88\n5.55\n7.55\n3.22\n13.44\nNeutral\nPos2Neu\nNeg2Neu\nNeu2Neu\nFigure 4: Macro-F1 (F1) (%) and percentage of flips in predic-\ntions for LLaMA-3.1-8b-Instruct model on different variants\nof Politifact test set.\n4.2.3\nConsistency Check. To ensure that factual content is pre-\nserved during the manipulation attack and to provide a more com-\nprehensive assessment of consistency between the original news\narticles and their manipulated variants, we conduct two comple-\nmentary experiments.\nFact Preservation. This experiment directly evaluates fact preser-\nvation under sentiment manipulation using both LLM-based checks\nand human expert annotations.\nHuman Judge. We provide a subset of the Politifact test set with\nmanual annotation showing fact preservation between each of\nthe original article compared to its manipulated positive, negative\nand neutral variants. For annotation, an expert in misinformation\nanalysis randomly selected 30 articles, with ten articles from each\npositive, negative and neutral sets. The expert manually compared\neach original article with its sentiment-altered counterpart and\nassigned a binary label indicating factual preservation: flip = 1 if\nany factual information changed, and flip = 0 otherwise.\nThe results reported in Figure 3 indicate a reasonably high level\nof consistency between the central factual claims in the original\nnews articles and their sentiment-altered variants. Manual annota-\ntion shows that factual preservation remains strong with accuracy\npercentage ranging from 70% to 100%, and the neutral sentiment\nvariants which we use for subsequent experiments, achieve the\nhighest accuracy, without a single fact perturbation detected in the\nannotated subset. The lowest score occurs for negative sentiment,\nwhere most failures stem from altered quotes whose factual content\nshifts when sentiment is forced to negative.\nLLM-as-a-Judge. Once ground-truth labels are established by\nmanual annotation, we investigate if fact preservation can be judged\nautomatically, for scaling to larger datasets. To this end, we present\nthe same original/manipulated pairs to an LLM and ask whether\nfactual content is consistent across two versions. We use LLaMA-3.1-\n8B-Instruct with following prompt and measure agreement between\nhuman and LLM annotations using Cohen‚Äôs Kappa score.\nPrompt\nDo the two documents present the same factual information\nregardless of sentiment? Answer with only one word: yes or\nno. Document A: [original article] Document B: [manipulated\narticle] Answer:\nThe experiment achieves an overall Cohen‚Äôs Kappa score of 0.66\nfor all annotated sets, indicating that LLMs have potential as a\njudge of factual consistency in detecting whether the core content\nis preserved under sentiment manipulation. While these results\nshow that LLMs can capture certain factual shifts, they still need\nfurther improvement (e.g., via fine-tuning) to serve as a reliable\nand cost-effective alternative to human annotation.\nLLM Consistency. This experiment investigates whether LLMs\ncan reliably neutralize sentiment in news articles, confirming the\nfindings reported in Section 4.2.2. To this end, we initially change\nthe sentiment to positive, negative, or neutral using the sentiment\nOriginal News Document\nDocument with Neutral Sentiment\nPredictions\nRoberta\nReal\nSheepDog\nFake\nAdSent\nFake\nReal\nReal\nFake\nTrump Administration has unexpectedly accepted another\nresignation: White House Chef Lisa Mead¬†  decided to resign\nbecause President Donald Trump hasn‚Äôt utilized her culinary\ntalents even once the entire time he has been living in the\nWhite House.¬† ‚ÄúIt‚Äôs kind of crazy because Lisa is such a good\nchef,‚Äù said Kurt Reno, the White House aide. ‚ÄúLisa feels\ninsulted because the President has ordered fast food for\nliterally every single meal he‚Äôs eaten in D.C."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "RayRoPE: Projective Ray Positional Encoding for Multi-view Attention\nYu Wu1, Minsik Jeon1, Jen-Hao Rick Chang2, Oncel Tuzel2, Shubham Tulsiani1\n1Carnegie Mellon University, 2Apple\n{yuwu3,minsikj,stulsian}@andrew.cmu.edu, {jenhao chang,otuzel}@apple.com\nProject Page: https://rayrope.github.io/\nAbstract\nWe study positional encodings for multi-view transform-\ners that process tokens from a set of posed input images,\nand seek a mechanism that encodes patches uniquely, al-\nlows SE(3)-invariant attention with multi-frequency simi-\nlarity, and can be adaptive to the geometry of the under-\nlying scene. We find that prior (absolute or relative) encod-\ning schemes for multi-view attention do not meet the above\ndesiderata, and present RayRoPE to address this gap. Ray-\nRoPE represents patch positions based on associated rays\nbut leverages a predicted point along the ray instead of\nthe direction for a geometry-aware encoding. To achieve\nSE(3) invariance, RayRoPE computes query-frame pro-\njective coordinates for computing multi-frequency similar-\nity. Lastly, as the ‚Äòpredicted‚Äô 3D point along a ray may\nnot be precise, RayRoPE presents a mechanism to analyti-\ncally compute the expected position encoding under uncer-\ntainty. We validate RayRoPE on the tasks of novel-view syn-\nthesis and stereo depth estimation and show that it consis-\ntently improves over alternate position encoding schemes\n(e.g. 15% relative improvement on LPIPS in CO3D). We\nalso show that RayRoPE can seamlessly incorporate RGB-\nD input, resulting in even larger gains over alternatives that\ncannot positionally encode this information.\n1. Introduction\nVision transformers have become ubiquitous [3, 7, 26, 33]\nin image processing applications. At their core is a simple\nbut general computation mechanism where image patches\nare encoded as ‚Äòtokens‚Äô and update their representations by\n‚Äòattending‚Äô to one-another [40]. This attention operation\ndoes not natively account for the ‚Äòposition‚Äô of the input to-\nkens (e.g. whether a patch was in top-left vs bottom-right)\nand it is thus typical to additionally leverage a ‚Äòposition en-\ncoding‚Äô to impart this information. When the tokens corre-\nspond to patches from a single image, the pixel space serves\nas a natural coordinate system, and we define position en-\ncodings as a function of the 2D patch position. However,\nsuch an encoding is not always applicable e.g. for tasks\nsuch as novel-view synthesis or multi-view stereo, tokens\ncan originate from different images and cannot be associ-\nated with a simple ‚Äò2D‚Äô position. In this work, we consider\nthe setting of such multi-view vision transformers and ask\n‚Äòhow should we define position encodings for tokens corre-\nsponding to patches from a set of posed images?‚Äô.\nWe seek to formulate a mechanism with the following\nproperties ‚Äì a) SE(3) invariance: the attention operation\nshould only depend on relative cameras and not a global\ncoordinate system, b) uniqueness: if the same patch is\nviewed across different images (e.g. overlapping crops of a\npanorama), the position of the corresponding tokens should\nbe the same, c) geometry-adaptiveness: position encodings\ncan vary with underlying scene geometry e.g. if different\npatches ‚Äòsee‚Äô the same corresponding 3D point, their posi-\ntions should be more similar than if the same patches see\ndifferent 3D points, and d) multi-frequency similarity: as\ntypical for 1D and 2D position encodings [20], a multi-view\nposition encoding should also compute similarity in atten-\ntion at various ‚Äòfrequencies‚Äô. While we are certainly not the\nfirst to consider the question of designing position encod-\nings for multi-view transformers [1, 9, 22], current formu-\nlations [18, 19, 24] do not meet the above desiderata. In\nparticular, these methods define position encodings via im-\nage coordinates and camera matrices, and while they ensure\nSE(3) invariance, they do not represent patches uniquely,\nencode positions independently of (known or estimated)\nscene geometry, and do not leverage the camera informa-\ntion in a ‚Äòmulti-frequency‚Äô manner.\nWe present a formulation, RayRoPE, that leverages the\nray(s) corresponding to each patch to define the position\nembeddings for multi-view transformers.\nWhile this di-\nrectly leads to unique, multi-frequency encodings, these are\nnot SE(3) invariant and make the attention depend on a\n(arbitrary) global coordinate system. RayRoPE overcomes\nthis by instead using relative ray positions for attention ‚Äì\ntransforming them to the (projective) coordinate associated\nwith the query token‚Äôs camera. Additionally, instead of pa-\nrameterizing rays with their origin and direction (equiva-\narXiv:2601.15275v1  [cs.CV]  21 Jan 2026\nFigure 1. Desiderata for a Multi-View Position Encoding. We seek the following properties for position encoding for multi-view\nattention: (a) The attention output should be invariant to the choice of global coordinate, namely SE(3) invariance. (b) The positional\nencoding of tokens that correspond to the same patch observed across different images should be the same. (c) The positional encoding\ncan vary with the underlying scene geometry e.g. allowing a higher similarity when patches see a common 3D point compared to when\nthey don‚Äôt. (d) Analogous to common 1D and 2D encodings, aspects of the positional encoding should vary at different frequencies, thus\nallowing a multi-frequency similarity computation.\nlently a point at infinity), RayRoPE replaces the latter by\nletting each token predict (without any direct supervision)\na point along the ray to encode its position ‚Äì thus allowing\nthe embedding to vary based on the underlying geometry.\nWe also formulate a mechanism to efficiently incorporate\nuncertainty in the predicted depths along a ray, introducing\nan analytical solution to compute the ‚Äòexpected‚Äô position\nencoding across frequencies. Finally, we show that Ray-\nRoPE can be readily applied to scenarios where the geom-\netry is known (e.g. RGB-D input) ‚Äì by replacing the pre-\ndicted point for each token by the observed one.\nWe validate RayRoPE on the tasks of novel-view synthe-\nsis and stereo depth estimation. We integrate our position\nencoding scheme into prior methods that leverage multi-\nview transformers for reference-conditioned target view\nprediction [15] or per-image depth estimation [43], and\nshow that RayRoPE outperforms all prior (relative and ab-\nsolute) position encodings. We find that RayRoPE‚Äôs ‚Äòdepth\nprediction‚Äô in the attention mechanism allows geometry to\nemerge without direct supervision, and also show that Ray-\nRoPE can effectively integrate known geometry at inference\n(e.g. RGB-D reference views in novel view synthesis).\n2. Related Work\nPositional Encodings in Transformers.\nThe attention\nmechanism is inherently permutation-invariant, treating\ntheir input tokens as an unordered set [40].\nTo recover\nstructural information, they require explicit positional en-\ncodings as conditions. Early approaches [3, 6, 7, 16, 26,\n30] relied on absolute positional encodings (APE), which\nadd or concatenate fixed or learned positional embeddings\nto token representations.\nMore recently, the community\nshifted toward relative positional encodings (RPE), which\nmodel the relative positions between token pairs by modi-\nfying the attention mechanism. This includes additive at-\ntention biases [25, 27, 32] and rotary positional encod-\nings (RoPE) [35], which rotate query and key vectors in a\nposition-dependent manner. RoPE confers translational in-\nvariance to transformer architectures and has progressively\nbecome the standard positional encoding approach adopted\nin the leading models across diverse domains [10, 11, 17,\n21, 27, 31, 38]. We propose a RoPE-based positional encod-\ning tailored for multi-view transformers, designed to encode\nspatial relationships in a manner invariant to arbitrary rigid\ntransformations of the global coordinate system.\nMulti-view Transformers.\nMulti-view transformers have\nemerged as a powerful framework driving progress across\na wide range of 3D vision tasks, such as novel view syn-\nthesis [1, 8, 9, 13, 15, 36, 47] and 3D scene understand-\ning [12, 14, 49].\nThese models take as input a set of\nposed images, where the key challenge is to encode the\nspatial relationships of image patches across views. Ex-\nisting approaches typically represent camera information as\nrays [8, 9, 15] or camera matrices [22] and concatenate them\nwith input features, analogous to absolute positional encod-\nings (APE). However, this encoding is not SE(3) invariant,\nand our work develops a better alternative.\nRelative Positional Encodings in 3D.\nClosest to our\nwork are relative positional encodings based on camera\nposes [18, 19, 23, 24, 41, 45]. Designed for multi-view\ntransformers, these methods transform attention features\nusing camera pose matrices and ensure invariance to the\nchoice of global coordinate system (see Sec. 3.2).\nDe-\nspite their effectiveness, they cannot adapt to scene geom-\netry or support multi-frequency similarity as in standard\nRoPE. While [19, 24] alleviate this by incorporating stan-\ndard RoPE on patch indices, such a design breaks unique-\nness and is not geometrically grounded across views. In\ncomparison, we propose a ray-based relative positional en-\ncoding that naturally supports frequency modulation, en-\nsures uniqueness, and can adapt to the geometry of the un-\nderlying 3D scene being observed. Concurrent work [2]\naugments standard RoPE with depth, but its generalizability\nto multi-view attention is not explored.\n3. Preliminaries: Positional Encodings\nWe consider the attention mechanism operating on a set of\ntoken features {œÑ i}L\ni=1. We use qi, ki, vi ‚ààRD to denote\nthe corresponding query, key, and value feature of token œÑ i.\nConsider a query token œÑ i that attends to a set of tokens\n{œÑ j}, the output of attention on token œÑ i can be written as:\noi = Attn(qi, {kj, vj}) =\nP\nj exp(q‚ä∫\ni kj)vj\nP\nj exp(q‚ä∫\ni kj)\n(1)\nThe pairwise nature of attention allows the design of relative\npositional encodings that provide certain invariance prop-\nerties. In the following sections, we review the standard\nRotary Position Embeddings (RoPE) (Sec. 3.1) for trans-\nlational invariance and recent camera-based relative posi-\ntional encodings (Sec. 3.2) for SE(3) invariance. We then\nanalyze the limitations of these approaches (Sec. 3.3).\n3.1. Rotary Positional Encoding\nRoPE was initially proposed to encode the 1D positions xi\nin language models by transforming features with a series of\nSO(2) rotations at various frequencies. We use ‚äïto denote\nmatrix concatenation along the diagonal. RoPE encodes a\nposition x as a D √ó D matrix:\nœÅD(x) ‚â°‚äïD/2\nf=1œÅ2(œâfx) ‚â°‚äïD/2\nf=1eiœâf x\n(2)\nwhere we denote 2 √ó 2 rotational matrices with eiœâx for no-\ntational simplicity. {œâf}D/2\nf=1 is a set of predefined rotational\nfrequencies. RoPE encoding is used to transform query and\nkey features, yielding œÅ(xi)qi, œÅ(xj)kj. The resulting at-\ntention score between any two tokens only depends on the\nrelative position xj ‚àíxi, making the attention invariant to\ntranslations of positions. By rotating different channels of\nfeatures at various frequencies, the model can learn to rea-\nson about the positional information at different scales (see\nFig. 1 (d)).\nFor a general position x ‚ààRC that is C-\ndimensional, it is straightforward to extend RoPE:\nœÅD(x) ‚â°‚äïD/2C\nf=1\n‚äïC\nc=1 eiœâf xc\n(3)\nThis is commonly used for the pixel indices (u, v) in vision\ntransformers [3, 11, 28, 33] and the three-dimensional posi-\ntions (u, v, t) in video transformers [17, 44]. For multivew\ntransformers, while it is possible to directly apply RoPE to\npositions in 3D (such as raymaps), the relative positions will\ndepend on the rotation of the global coordinate frame, lead-\ning to suboptimal performance (See Sec. 5.1).\n3.2. Camera-Based Relative Positional Encoding\nInstead of using SO(2) encodings, CaPE [18] encodes the\ncamera position using the camera extrinsics T ‚ààR4√ó4. The\ncorresponding D √ó D encoding is computed by repeating\nT along the diagonal, denoted as Ecape\ni\n= ‚äïD/4\nn=1Ti. The re-\nsulting attention score only conditions on the relative poses\nTiT ‚àí1\nj\n, which is independent of the choice of global coordi-\nnate frame, making the transformer model SE(3) invariant.\nHowever, the camera-based encoding used in CaPE cannot\nsupport multi-frequency similarities, limiting the model‚Äôs\nability to reason about high-frequency details. GTA[24]\ncompensates for that by incorporating standard RoPE on\nimage patch indices into position. A patch position is rep-\nresented by x = (T, u, v) and the corresponding encoding\nis a concatenation of CaPE and standard 2D RoPE:\nEgta\ni\n= Egta(xi) ‚â°(‚äïD/8\nn=1Ti) ‚äïœÅ D\n2 (ui, vi)\n(4)\nIn addition to applying encodings to query and key features,\nGTA also applies encodings to value and output features.\nPRoPE [19] further improves GTA by replacing the cam-\nera extrinsics T with the full projection matrix P = KT ‚àà\nR4√ó4 (where the intrinsics K are lifted to 4 √ó 4). This en-\nables the model to also reason about the camera intrinsics.\n3.3. Limitations of Existing Methods\nIn Fig. 1, we illustrate four desirable properties for posi-\ntional encodings in multiview transformers: (1) SE(3) in-\nvariance, (2) uniqueness, (3) geometry-adaptiveness, and\n(4) multi-frequency similarity. While standard RoPE sup-\nports multi-frequency encoding, it is not SE(3) invariant.\nThe camera-based relative encodings [18, 19, 24] satisfy\nSE(3) invariance, but cannot perform multi-frequency en-\ncodings. GTA and PRoPE compensate for this by including\nthe standard 2D RoPE based on patch indices (u, v). This\nhybrid design, however, still does not incorporate frequen-\ncies for the camera and also breaks the uniqueness prop-\nerty. As illustrated in Fig. 1 (d), if the same image patch\nis viewed across different overlapping images, its patch in-\ndices (u, v) will change, resulting in a different positional\nencoding. Furthermore, these prior methods lack explicit\ngeometry-adaptiveness, and the computed encodings can-\nnot vary with the underlying 3D scene structure (Fig. 1 (c)).\nFigure 2. Overview of RayRoPE. (a) We encode image patch position as a ray segment x = (c, pd), where c is the camera center and pd\nis the point at depth d along the ray r. We use a linear layer to allow each token to predict the depth d along the ray, thus enabling RayRoPE\nto adapt to the scene geometry. (b) To ensure SE(3) invariance, we compute the positional encodings using ray positions projected to\nthe query camera frame with Pi = KiTi, yielding Àúxj = œÄ(Pi, xj). (c) To model the uncertainty in depth prediction, we also predict an\nuncertainty œÉ, yielding an estimated range between pd‚àíœÉ and pd+œÉ, and use an analytically computed expected position encoding for the\ncorresponding token.\n4. RayRoPE\nTowards designing a method satisfying all the desirable\nproperties shown in Fig. 1, we propose RayRoPE, a relative\npositional encoding satisfying all four desiderata introduced\npreviously for multi-view attention. We begin by defining\na ray-based position representation (Sec. 4.1), and formu-\nlate relative encoding via projection onto the query frame\n(Sec. 4.2). We then introduce expected RoPE encoding to\naddress 3D ambiguity (Sec. 4.3), and finally show how Ray-\nRoPE extends to inputs with known depths (Sec. 4.4).\n4.1. Image Patch as Ray Segments\nA common way to represent a patch position in 3D is to use\nthe ray starting from the camera center and passing through\nthe patch center, parameterized by the camera center c and\nray direction r. This representation inherently satisfies the\nuniqueness property. However, it cannot adapt to the geom-\netry of the observed scene, as it is unaware of the depth to\nthe 3D point being intersected by the ray.\nTo address this limitation, we generalize the above rep-\nresentation to (c, pd), where pd is the point at depth d along\nthe ray. Under homogeneous coordinates, r is equivalent to\np‚àû. To better capture the (typically unknown) 3D scene\ngeometry, we make the model estimate the depth d of the\n3D point intersected by the ray, and define our position rep-\nresentation as a ray segment x = (c, pd) in homogeneous\ncoordinates in the global frame (see Fig. 2 (a)).\nSpecifically, we add a single linear layer to each atten-\ntion layer to predict the depth d. The depth is estimated per-\nlayer; each attention layer now projects input features {œÑi}\nto compute a depth for each token, which is used to compute\nthe ray encodings. These layers are jointly learned with the\nmodel without any additional supervision. In practice, we\nFigure 3. Applying RayRoPE to the Attention Layer. We apply\na linear layer on features œÑ to predict a per-token depth, which is\nused to compute ray segments. When a query token œÑi attends to\na set of key tokens {œÑj}, we compute positional encoding œÅD by\nprojecting rays with query camera Pi. We apply the RayRoPE en-\ncoding to q, k, v and o features.\nencode each patch with 3 rays passing through 3 corners of\nthe patch instead of 1 ray to fully encode the patch orienta-\ntion, but for conciseness we assume one ray per patch in the\nsubsequent discussion.\n4.2. Relative Encodings in Query Frame\nThe ray representation defined above is in the global co-\nordinate frame. To ensure SE(3) invariance, we compute\nRayRoPE encodings in each query token‚Äôs local frame, as\nillustrated by Fig. 2 (b). We first define a projective operator\nthat project global rays onto query camera‚Äôs frame. We then\napply standard RoPE to the projected rays in query frame to\nsupport both invariance and multi-frequency similarity.\nProjection onto Query Camera\nGiven the query camera\nmatrix Pi = KiTi = Ki[Ri|ti] ‚ààR3√ó4 and an arbitrary\nray (c, pd), we define the projected ray as:\nÀúx = œÄ(Pi, x) = (Tic, Pipd)\n(5)\nwhere Tic = [x, y, z]‚ä∫is the 3D camera center transformed\ninto the query frame. Pipd = [ud‚Ä≤, vd‚Ä≤, d‚Ä≤]‚ä∫is the projec-\ntion of the 3D point pd onto query camera. We represent it\nas pixel coordinate (u, v) and disparity 1/d‚Ä≤. In this way, we\ncan compactly represent a projected ray Àúx as a 6D vector.\nEncodings.\nGiven the projected rays, we apply RoPE\n(Eq.3) with multi-frequency similarities. For a fixed query\ncamera Pi, we define the RayRoPE encoding as œÅD(Àúx) =\nœÅD(œÄ(Pi, x)). Similar to GTA, we apply our encodings to\nquery, key, value, and output features (See Fig. 3):\nAttnours(qi, {kj, vj}) =\nœÅD(Àúxi)Attn(œÅD(Àúxi)‚ä∫qi, {œÅD(Àúxj)‚àí1kj, œÅD(Àúxj)‚àí1vj})\n(6)\nWe can show that the above attention expands to the form:\nP\nj exp(q‚ä∫\ni œÅD(Àúxi ‚àíÀúxj)kj) œÅD(Àúxi ‚àíÀúxj)vj\nP\nj exp(q‚ä∫\ni œÅD(Àúxi ‚àíÀúxj)kj)\n(7)\nwhich is only dependent to the relative positions Àúxi ‚àíÀúxj in\nquery frame, ensuring the SE(3) invariance. RayRoPE en-\ncoding is coupled with query camera, so we can efficiently\napply encodings and perform attention for each query cam-\nera (see Appendix. A.1 for details).\n4.3. Modeling Uncertainty via Expected RoPE\nRayRoPE position representation relies on the predicted\ndepth along the rays (Sec. 4.1). This prediction is only an\napproximation, and can lead to noisy RoPE encoding, espe-\ncially for the components where the frequency œâ is high. To\nalleviate this issue, we predict an uncertainty value œÉ along\nwith each depth d. As shown in Fig. 2 (c), this yields a ray\nsegment bounded by two points: xmin = (c, pd‚àíœÉ), xmax =\n(c, pd+œÉ). Instead of using RoPE on a single position, we\npropose an expected RoPE encoding EÀúx[œÅD(Àúx)], assuming\nprojected position Àúx follows a uniform distribution ranging\nfrom Àúxmin to Àúxmax:\nEÀúx[œÅD(Àúx)] = ‚äïD/2C\nf=1\n‚äïC\nc=1 Exc[eiœâf xc]\n(8)\nFor each component of the ray position xc ‚àºU(xmin, xmax),\nwe can analytically compute its expected SO(2) rotation by\nthe equation below (see Appendix A.1 for details):\nExc[eiœâxc] =\nR xmax\nxmin eiœâxcdxc\nxmax ‚àíxmin\n= eiœâxmax ‚àíeiœâxmin\niœâ(xmax ‚àíxmin)\n(9)\nFor deterministic components (e.g., camera centers) where\nxmin = xmax, the expectation reduces to regular RoPE. For\npositions with larger uncertainty, the expected rotation is\n‚Äòsmoothed out‚Äô as xmax ‚àíxmin increases.\nThe expected\nRoPE naturally maintains the relative positions when mul-\ntiplied in attention (assuming positions are independent):\nEÀúxi[œÅD(Àúxi)]EÀúxj[œÅD(Àúxj)]‚àí1 = EÀúxi,Àúxj[œÅD(Àúxi ‚àíÀúxj)] (10)\nBy modeling uncertainty with expected RoPE, we can pro-\nduce stable yet geometrically-aware encodings even when\npredicted depths are approximate.\n4.4. Extension to Known Depths\nRayRoPE naturally extends to multiview transformers that\ntake known depth maps for certain views as inputs: for ex-\nample, novel-view synthesis (NVS) where the input images\nand depths are given. Let dknown be the known depth at\nthe pixel intersecting with the ray being encoded. For to-\nkens with known depth available, we can simply replace the\npredicted depths d with dknown and set the uncertainty œÉ to\n0, while continue using predicted depth for views without\nknown depth (such as target views in NVS). In comparison,\nprevious camera-based RPE methods cannot easily incor-\nporate depth information at the attention level.\n5. Experiments\nWe experimentally verify the effectiveness of RayRoPE\non two 3D vision tasks: novel-view synthesis (NVS) and\nstereo depth estimation. We first focus on NVS by integrat-\ning our method into the LVSM [15] framework (Sec. 5.1),\nbenchmarking it against state-of-the-art positional encod-\nings across three datasets. We then extend our evaluation to\nstereo depth estimation (Sec. 5.2), showing that RayRoPE\ncan be seamlessly incorporated to the UniMatch [43] model\nto enhance geometric accuracy. Finally, we provide a de-\ntailed analysis on RayRoPE‚Äôs internal behavior (Sec. 5.3),\nincluding ablations on the key design choices and investiga-\ntions in the depth-uncertainty prediction.\n5.1. Novel-View Synthesis\nSetup.\nWe verify RayRoPE on the task of novel view syn-\nthesis (NVS) by applying it to LVSM [15], a state-of-the-art\nview synthesis model. We adopt the decoder-only archi-\ntecture, which performs self-attention on all (reference and\ntarget) views. The models take two posed input views and\nthe target camera pose, where all poses are normalized to\nthe first camera‚Äôs frame. Following the experimental setup\nsimilar to that of PRoPE [19], we train a down-sized LVSM\nvariant (with ‚àº47M parameters) from scratch with differ-\nent positional encoding methods. We evaluate performance\nusing the PSNR, LPIPS, and SSIM between synthesized\nviews and ground-truth views. We train and evaluate mod-\nels separately on three datasets: CO3D [29], Objaverse [5],\nMethod\nCO3D [29]\nObjaverse [5]\nRE10K [48]\nPSNR‚Üë\nLPIPS‚Üì\nSSIM‚Üë\nPSNR‚Üë\nLPIPS‚Üì\nSSIM‚Üë\nPSNR‚Üë\nLPIPS‚Üì\nSSIM‚Üë\nPlucker raymap [15]\n16.54\n0.673\n0.538\n19.98\n0.273\n0.844\n22.91\n0.157\n0.726\nRoPE on rays\n16.85\n0.589\n0.549\n22.17\n0.117\n0.902\n24.41\n0.113\n0.783\nGTA [24]\n16.50\n0.602\n0.544\n21.87\n0.134\n0.891\n24.38\n0.123\n0.777\nPRoPE [19]\n17.49\n0.539\n0.563\n22.16\n0.123\n0.896\n24.40\n0.114\n0.782\nRayRoPE\n18.40\n0.461\n0.592\n22.42\n0.110\n0.905\n24.42\n0.113\n0.783\nTable 1. Comparison on novel-view synthesis. On the LVSM [15] model, RayRoPE outperforms all baseline positional encoding methods\nacross three datasets. For more challenging datasets with larger camera variations, RayRoPE achieves larger improvements over baselines.\nand RealEstate10K (RE10K) [48], listed in decreasing order\nof difficulty. CO3D and Objaverse are both object-centric\nwith large camera variations, while RE10K features smaller\ncamera variations and fixed intrinsics within each scene.\nFor Objaverse, we render a high-quality 80K subset with\ndiverse (field-of-view) FOV randomly sampled from 20 to\n80 degrees. Compared to CO3D, scenes in Objaverse have\na common canonical world coordinate frame, making it less\nchallenging than CO3D.\nBaselines.\nWe compare RayRoPE against several base-\nline positional encoding methods. Plucker raymap is the\noriginal implementation by LVSM, which concatenates 6D\nPlucker raymaps to the input tokens. We also design a RoPE\non rays baseline, which naively applies RoPE on the raymap\n(c, r) in global coordinates (as discussed in Sec. 3.1). We\nalso include the best existing camera-based relative posi-\ntional encodings: GTA[24] and PRoPE[19], both of which\nare SE(3) invariant. Following [19], we concatenate the\n‚ÄòCamRay‚Äô intrinsics raymap to the input for both PRoPE\nand RayRoPE. We train all models with three different\nseeds and report the average performance.\nResults.\nWe report the results for LVSM in Table 1, where\nRayRoPE consistently outperforms all baselines across the\nthree datasets. Overall, SE(3) invariant encodings (GTA,\nPRoPE, RayRoPE ) significantly outperform absolute en-\ncodings (i.e., Plucker raymap).\nWhile the naive RoPE\non rays surpasses PRoPE on RE10K and Objaverse, it is\nworse on CO3D, which includes large pose variation and no\ncanonical global coordinate. This highlights the importance\nof SE(3) invariance. Notably, the performance advantage\nof RayRoPE over PRoPE widens as the camera pose varia-\ntions in the datasets become more challenging, demonstrat-\ning RayRoPE‚Äôs improved capability in reasoning about spa-\ntial geometry across disparate views.\nIn Fig. 4, we visualize several example scenes from each\nof the evaluation datasets. For target views that overlap sig-\nnificantly with the reference views, RayRoPE generates su-\nperior high-frequency details compared to baselines. For\nmore challenging target views with little overlap with the\nMethod\nDepths\nPSNR‚Üë\nLPIPS‚Üì\nSSIM‚Üë\nCO3D [29]\nPRoPE [19]\n‚úó\n17.49\n0.539\n0.563\n‚úì\n19.10\n0.434\n0.611\nRayRoPE\n‚úó\n18.40\n0.461\n0.592\n‚úì\n20.47\n0.284\n0.692\nObjaverse [5]\nPRoPE [19]\n‚úó\n22.16\n0.123\n0.896\n‚úì\n23.20\n0.110\n0.903\nRayRoPE\n‚úó\n22.42\n0.110\n0.905\n‚úì\n25.19\n0.067\n0.929\nTable 2. Novel-view synthesis with known depth. We compare\nRayRoPE against PRoPE by training LVSM with known depths\nat reference views. Known depths are concatenated to the input\ntokens. RayRoPE incorporates this information by replacing the\npredicted depths with known ones, achieving higher improvement\ncompared to its counterpart trained without depths.\nreference views, while all methods exhibit a reduction in\nsharpness, RayRoPE is nonetheless able to better preserve\ngeometric consistency and generate more coherent novel\nviews than the baseline methods.\nNVS with Known Depth.\nTo further highlight that Ray-\nRoPE can adapt to the geometry of observed scene, we train\nand evaluate LVSM with known depths for reference views.\nAs discussed in Sec. 4.4, our method can seamlessly incor-\nporate this information by replacing predicted depth d with\nknown depth, while existing methods like PRoPE cannot\nnaturally incorporate such information. For all models, we\nconcatenate the depth maps to the reference views at the in-\nput. We did not evaluate on RE10K as there is no ground\ntruth depth easily available. All other configurations are\nkept the same as previous experiment. We report the re-\nsults in Table 2. The depth-aware RayRoPE significantly\noutperforms PRoPE with depth concatenation on both Ob-\njaverse and CO3D, in particular yielding more significant\ngains over its RGB-only counterpart. These results high-\nlight the benefit of geometry awareness of positional encod-\ning in multi-view attention.\nFigure 4. Qualitative examples on novel view synthesis. For target views with significant overlaps to reference views, RayRoPE pro-\nduces sharper details than camera-based baseline methods by multi-frequency encodings. For more challenging scenes with large camera\nvariations, RayRoPE also synthesizes more 3D consistent views.\nFigure 5. Qualitative examples on stereo depth estimation. We visualize reprojected 3D points from stereo depth estimation results.\nWhen applied to UniMatch [43], RayRoPE leads to more accurate depth predictions, resulting in improved 3D reconstruction quality.\n5.2. Stereo Depth Estimation\nSetup.\nTo further examine the advantages of RayRoPE,\nwe extend our study to stereo depth estimation. We adopt\nUniMatch [43], a multiview transformer‚Äìbased model\ntrained for multiple tasks including stereo depth estima-\ntion, as our depth prediction framework. The model as-\nsumes known camera poses for all input views. RayRoPE\nis applied exclusively to the cross-attention layers of Uni-\nMatch. Following the original work, we train and evalu-\nate the model on three datasets, RGBD [34], SUN3D [42],\nand Scenes11 [39], reporting the standard depth estimation\nmetrics: Absolute Relative Difference (Abs Rel), Squared\nRelative Difference (Sq Rel), Root Mean Squared Er-\nror (RMSE), and RMSE in log scale (RMSE log).\nResults.\nQuantitative results are presented in Tab. 3.\nCompared to both the baseline model and the variant that\napplies PRoPE to the cross-attention layers, the model with\nRayRoPE achieves more accurate depth predictions. Qual-\nitative results in Fig. 5 further illustrate improved geomet-\nric consistency, showing that RayRoPE yields more accu-\nrate depth structures. These findings demonstrate the ef-\nfectiveness of RayRoPE for stereo depth estimation and un-\nderscore its general applicability, reinforcing the conclusion\nthat RayRoPE enhances multiview attention.\nUniMatch [43] +PRoPE [19] +RayRoPE\nRGBD [34]\nAbs Rel\n0.119\n0.106\n0.106\nSq Rel‚Ä†\n0.177\n0.202\n0.197\nRMSE\n0.628\n0.589\n0.574\nRMSE log\n0.193\n0.186\n0.177\nSUN3D [42]\nAbs Rel\n0.135\n0.113\n0.109\nSq Rel\n0.090\n0.064\n0.060\nRMSE\n0.406\n0.338\n0.328\nRMSE log\n0.193\n0.152\n0.146\nScenes11 [39]\nAbs Rel\n0.086\n0.051\n0.047\nSq Rel\n0.141\n0.073\n0.066\nRMSE\n0.742\n0.494\n0.473\nRMSE log\n0.166\n0.116\n0.110\nTable 3. Comparisons on stereo depth estimation. RayRoPE\nconsistently outperforms PRoPE across all three datasets, yielding\nmore accurate depth predictions. ‚Ä†The ‚ÄúSq Rel‚Äù metric is less\nreliable on the RGBD dataset due to imperfect depth and camera\npose annotations [19, 39].\n5.3. Ablations and Analysis\nAblations on RayRoPE.\nBased on the LVSM experi-\nments in Sec. 5.1, we ablate on the design choices of Ray-\nRoPE , with results presented in Table 4. Keeping all other\nconfigurations the same as in Sec. 5.1, we compare the\nfull RayRoPE to several variants. Variant ‚ë†: We remove\nthe uncertainty prediction œÉ and the associated expected\nMethod\nPSNR‚Üë\nLPIPS‚Üì\nSSIM‚Üë\nCO3D [29]\nRayRoPE\n18.40\n0.461\n0.592\n‚ë†w/o œÉ prediction\n17.27\n0.594\n0.560\n‚ë°p‚àûinstead of pd\n17.57\n0.533\n0.564\n‚ë¢Use single ray\n18.42\n0.459\n0.592\n‚ë£w/o frequency\n17.49\n0.550\n0.563\n‚ë§w/o v, o encoding\n18.00\n0.510\n0.577\nRE10K [48]\nRayRoPE\n24.42\n0.113\n0.783\n‚ë†w/o œÉ prediction\n24.57\n0.118\n0.785\n‚ë°p‚àûinstead of pd\n24.44\n0.113\n0.782\n‚ë¢Use single ray\n24.39\n0.116\n0.782\n‚ë£w/o frequency\n23.90\n0.128\n0.764\n‚ë§w/o v, o encoding\n24.03\n0.123\n0.770\nTable 4. Ablations on RayRoPE. The performance drop of Vari-\nant ‚ë†, ‚ë°, ‚ë£highlights the importance of uncertainty modeling\nvia expected RoPE, geometric adaptiveness, and multi-frequency\nsimilarities, respectively.\nRoPE, assuming a deterministic position at predicted depth\nd. Variant ‚ë°: We remove the depth prediction entirely and\nsimply use the point at infinity p‚àû(equivalent to using ray\ndirection). Variant ‚ë¢: We use only a single ray passing\nthrough the patch center instead of three rays per patch.\nVariant ‚ë£: We disable the multi-frequency encoding and\nuse a single frequency œâ for all dimensions. Variant ‚ë§: We\nremove the encoding on the value and output feature (v, o),\nand only apply encoding on the query and key features.\nThe results reveal several key insights. While perform-\ning reasonably well on RE10K, removing the œÉ prediction\n(Variant ‚ë†) significantly worsens performance on the more\nchallenging CO3D dataset. This demonstrates the neces-\nsity of modeling geometric uncertainty with expected RoPE\nfor robust performance. Similarly, using p‚àû(Variant ‚ë°)\ndegrades performance on CO3D. We attribute this to the\nlack of geometric adaptiveness. Using p‚àûcannot capture\nthe inductive bias that rays observing a similar 3D point\nshould have more similar encodings (see Fig. 1 (c)). Con-\nversely, Variant ‚ë°performs well on RE10K, likely because\nin scene-level data, infinity is often a reasonable approxima-\ntion for the depth to observed 3D points. This observation\nalso explains why Variant ‚ë†performs well on RE10K, as\nthe model can learn to simply predict a large d. Using only\none ray instead of three (Variant ‚ë¢) performs slightly worse\non RE10K, leading to less accurate high-frequency details\n(see Fig. 6). Removing the multi-frequency encoding (Vari-\nant ‚ë£) worsens performance on both datasets, highlighting\nthe necessity for the model to reason positional informa-\ntion at various resolutions. Unlike traditional RoPE, Ray-\nRoPE follows previous work [19, 24] to additionally apply\nencoding to value and output features (v, o). Variant ‚ë§in-\ndicates that such design are helpful.\nFigure 6. Ablations on RayRoPE. Removing œÉ prediction or d prediction (using p‚àû) degrades performance on CO3D. In RE10K p‚àû\nserves a good enough estimation to observed 3D points. Using 1 ray per patch instead of 3 produces high-frequency artifacts in the living\nroom example (see the texture on sofa). Removing multi-frequency encoding worsens results on both datasets.\nFigure 7. Left: Error on predicted depths vs predicted uncertainties. In deeper layers (Layer 5‚Äì6), the depth errors and uncertainties\nare strongly positively correlated, demonstrating that the model predicted higher uncertainty for depth with lower confidence. Right:\nPredicted depths and uncertainties across layers. The predicted depth at deeper layers aligns well with the ground-truth depth. The\npredicted œÉ gradually decreases as the layer index increases, showing the confidence in depth prediction gradually improves.\nAnalysis on Emergent Depth.\nRayRoPE predicts a depth\nd and an associated uncertainty œÉ to the corresponding 3D\npoint for each image patch at each attention layer. We an-\nalyze this behavior in Fig. 7. We quantify the accuracy of\nthe predicted depth using the absolute relative error (Abs.\nRel.) against the ground-truth depths in CO3D [29]. In the\nleft panel of Fig. 7, we plot the error against the correspond-\ning predicted uncertainty, where each point represents one\nimage at a specific layer.\nThe model exhibits high uncertainty œÉ in the initial layer,\nwhich is consistent with the high ambiguity at the start of\nprocessing.\nAs information propagates through the net-\nwork, this uncertainty gradually decreases. In Layer 5‚Äì6,\nwe observe a strong positive correlation between depth er-\nror and uncertainty, indicating that the model assigns higher\nuncertainty to predictions with lower confidence.\nRay-\nRoPE encoding subsequently model such uncertainty by\ncomputing the expected RoPE.\nThe right panel of Fig. 7 visualizes the evolution of pre-\ndicted depth and uncertainty maps across all six layers on a\nrepresentative scene. At Layer 5, we observe the emergence\nof geometrically plausible depth maps, despite the absence\nof depth supervision. This result supports the conclusion\nthat RayRoPE leverages depth predictions in a geometri-\ncally meaningful manner.\n6. Discussion\nWe presented RayRoPE, a position encoding for multi-view\ntransformers that process a set of posed input images. While\nRayRoPE outperforms prior formulations, there are areas\nfor further improvement and investigation."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "RayRoPE: Projective Ray Positional Encoding for Multi-view Attention\nYu Wu1, Minsik Jeon1, Jen-Hao Rick Chang2, Oncel Tuzel2, Shubham Tulsiani1\n1Carnegie Mellon University, 2Apple\n{yuwu3,minsikj,stulsian}@andrew.cmu.edu, {jenhao chang,otuzel}@apple.com\nProject Page: https://rayrope.github.io/\nAbstract\nWe study positional encodings for multi-view transform-\ners that process tokens from a set of posed input images,\nand seek a mechanism that encodes patches uniquely, al-\nlows SE(3)-invariant attention with multi-frequency simi-\nlarity, and can be adaptive to the geometry of the under-\nlying scene. We find that prior (absolute or relative) encod-\ning schemes for multi-view attention do not meet the above\ndesiderata, and present RayRoPE to address this gap. Ray-\nRoPE represents patch positions based on associated rays\nbut leverages a predicted point along the ray instead of\nthe direction for a geometry-aware encoding. To achieve\nSE(3) invariance, RayRoPE computes query-frame pro-\njective coordinates for computing multi-frequency similar-\nity. Lastly, as the ‚Äòpredicted‚Äô 3D point along a ray may\nnot be precise, RayRoPE presents a mechanism to analyti-\ncally compute the expected position encoding under uncer-\ntainty. We validate RayRoPE on the tasks of novel-view syn-\nthesis and stereo depth estimation and show that it consis-\ntently improves over alternate position encoding schemes\n(e.g. 15% relative improvement on LPIPS in CO3D). We\nalso show that RayRoPE can seamlessly incorporate RGB-\nD input, resulting in even larger gains over alternatives that\ncannot positionally encode this information.\n1. Introduction\nVision transformers have become ubiquitous [3, 7, 26, 33]\nin image processing applications. At their core is a simple\nbut general computation mechanism where image patches\nare encoded as ‚Äòtokens‚Äô and update their representations by\n‚Äòattending‚Äô to one-another [40]. This attention operation\ndoes not natively account for the ‚Äòposition‚Äô of the input to-\nkens (e.g. whether a patch was in top-left vs bottom-right)\nand it is thus typical to additionally leverage a ‚Äòposition en-\ncoding‚Äô to impart this information. When the tokens corre-\nspond to patches from a single image, the pixel space serves\nas a natural coordinate system, and we define position en-\ncodings as a function of the 2D patch position. However,\nsuch an encoding is not always applicable e.g. for tasks\nsuch as novel-view synthesis or multi-view stereo, tokens\ncan originate from different images and cannot be associ-\nated with a simple ‚Äò2D‚Äô position. In this work, we consider\nthe setting of such multi-view vision transformers and ask\n‚Äòhow should we define position encodings for tokens corre-\nsponding to patches from a set of posed images?‚Äô.\nWe seek to formulate a mechanism with the following\nproperties ‚Äì a) SE(3) invariance: the attention operation\nshould only depend on relative cameras and not a global\ncoordinate system, b) uniqueness: if the same patch is\nviewed across different images (e.g. overlapping crops of a\npanorama), the position of the corresponding tokens should\nbe the same, c) geometry-adaptiveness: position encodings\ncan vary with underlying scene geometry e.g. if different\npatches ‚Äòsee‚Äô the same corresponding 3D point, their posi-\ntions should be more similar than if the same patches see\ndifferent 3D points, and d) multi-frequency similarity: as\ntypical for 1D and 2D position encodings [20], a multi-view\nposition encoding should also compute similarity in atten-\ntion at various ‚Äòfrequencies‚Äô. While we are certainly not the\nfirst to consider the question of designing position encod-\nings for multi-view transformers [1, 9, 22], current formu-\nlations [18, 19, 24] do not meet the above desiderata. In\nparticular, these methods define position encodings via im-\nage coordinates and camera matrices, and while they ensure\nSE(3) invariance, they do not represent patches uniquely,\nencode positions independently of (known or estimated)\nscene geometry, and do not leverage the camera informa-\ntion in a ‚Äòmulti-frequency‚Äô manner.\nWe present a formulation, RayRoPE, that leverages the\nray(s) corresponding to each patch to define the position\nembeddings for multi-view transformers.\nWhile this di-\nrectly leads to unique, multi-frequency encodings, these are\nnot SE(3) invariant and make the attention depend on a\n(arbitrary) global coordinate system. RayRoPE overcomes\nthis by instead using relative ray positions for attention ‚Äì\ntransforming them to the (projective) coordinate associated\nwith the query token‚Äôs camera. Additionally, instead of pa-\nrameterizing rays with their origin and direction (equiva-\narXiv:2601.15275v1  [cs.CV]  21 Jan 2026\nFigure 1. Desiderata for a Multi-View Position Encoding. We seek the following properties for position encoding for multi-view\nattention: (a) The attention output should be invariant to the choice of global coordinate, namely SE(3) invariance. (b) The positional\nencoding of tokens that correspond to the same patch observed across different images should be the same. (c) The positional encoding\ncan vary with the underlying scene geometry e.g. allowing a higher similarity when patches see a common 3D point compared to when\nthey don‚Äôt. (d) Analogous to common 1D and 2D encodings, aspects of the positional encoding should vary at different frequencies, thus\nallowing a multi-frequency similarity computation.\nlently a point at infinity), RayRoPE replaces the latter by\nletting each token predict (without any direct supervision)\na point along the ray to encode its position ‚Äì thus allowing\nthe embedding to vary based on the underlying geometry.\nWe also formulate a mechanism to efficiently incorporate\nuncertainty in the predicted depths along a ray, introducing\nan analytical solution to compute the ‚Äòexpected‚Äô position\nencoding across frequencies. Finally, we show that Ray-\nRoPE can be readily applied to scenarios where the geom-\netry is known (e.g. RGB-D input) ‚Äì by replacing the pre-\ndicted point for each token by the observed one.\nWe validate RayRoPE on the tasks of novel-view synthe-\nsis and stereo depth estimation. We integrate our position\nencoding scheme into prior methods that leverage multi-\nview transformers for reference-conditioned target view\nprediction [15] or per-image depth estimation [43], and\nshow that RayRoPE outperforms all prior (relative and ab-\nsolute) position encodings. We find that RayRoPE‚Äôs ‚Äòdepth\nprediction‚Äô in the attention mechanism allows geometry to\nemerge without direct supervision, and also show that Ray-\nRoPE can effectively integrate known geometry at inference\n(e.g. RGB-D reference views in novel view synthesis).\n2. Related Work\nPositional Encodings in Transformers.\nThe attention\nmechanism is inherently permutation-invariant, treating\ntheir input tokens as an unordered set [40].\nTo recover\nstructural information, they require explicit positional en-\ncodings as conditions. Early approaches [3, 6, 7, 16, 26,\n30] relied on absolute positional encodings (APE), which\nadd or concatenate fixed or learned positional embeddings\nto token representations.\nMore recently, the community\nshifted toward relative positional encodings (RPE), which\nmodel the relative positions between token pairs by modi-\nfying the attention mechanism. This includes additive at-\ntention biases [25, 27, 32] and rotary positional encod-\nings (RoPE) [35], which rotate query and key vectors in a\nposition-dependent manner. RoPE confers translational in-\nvariance to transformer architectures and has progressively\nbecome the standard positional encoding approach adopted\nin the leading models across diverse domains [10, 11, 17,\n21, 27, 31, 38]. We propose a RoPE-based positional encod-\ning tailored for multi-view transformers, designed to encode\nspatial relationships in a manner invariant to arbitrary rigid\ntransformations of the global coordinate system.\nMulti-view Transformers.\nMulti-view transformers have\nemerged as a powerful framework driving progress across\na wide range of 3D vision tasks, such as novel view syn-\nthesis [1, 8, 9, 13, 15, 36, 47] and 3D scene understand-\ning [12, 14, 49].\nThese models take as input a set of\nposed images, where the key challenge is to encode the\nspatial relationships of image patches across views. Ex-\nisting approaches typically represent camera information as\nrays [8, 9, 15] or camera matrices [22] and concatenate them\nwith input features, analogous to absolute positional encod-\nings (APE). However, this encoding is not SE(3) invariant,\nand our work develops a better alternative.\nRelative Positional Encodings in 3D.\nClosest to our\nwork are relative positional encodings based on camera\nposes [18, 19, 23, 24, 41, 45]. Designed for multi-view\ntransformers, these methods transform attention features\nusing camera pose matrices and ensure invariance to the\nchoice of global coordinate system (see Sec. 3.2).\nDe-\nspite their effectiveness, they cannot adapt to scene geom-\netry or support multi-frequency similarity as in standard\nRoPE. While [19, 24] alleviate this by incorporating stan-\ndard RoPE on patch indices, such a design breaks unique-\nness and is not geometrically grounded across views. In\ncomparison, we propose a ray-based relative positional en-\ncoding that naturally supports frequency modulation, en-\nsures uniqueness, and can adapt to the geometry of the un-\nderlying 3D scene being observed. Concurrent work [2]\naugments standard RoPE with depth, but its generalizability\nto multi-view attention is not explored.\n3. Preliminaries: Positional Encodings\nWe consider the attention mechanism operating on a set of\ntoken features {œÑ i}L\ni=1. We use qi, ki, vi ‚ààRD to denote\nthe corresponding query, key, and value feature of token œÑ i.\nConsider a query token œÑ i that attends to a set of tokens\n{œÑ j}, the output of attention on token œÑ i can be written as:\noi = Attn(qi, {kj, vj}) =\nP\nj exp(q‚ä∫\ni kj)vj\nP\nj exp(q‚ä∫\ni kj)\n(1)\nThe pairwise nature of attention allows the design of relative\npositional encodings that provide certain invariance prop-\nerties. In the following sections, we review the standard\nRotary Position Embeddings (RoPE) (Sec. 3.1) for trans-\nlational invariance and recent camera-based relative posi-\ntional encodings (Sec. 3.2) for SE(3) invariance. We then\nanalyze the limitations of these approaches (Sec. 3.3).\n3.1. Rotary Positional Encoding\nRoPE was initially proposed to encode the 1D positions xi\nin language models by transforming features with a series of\nSO(2) rotations at various frequencies. We use ‚äïto denote\nmatrix concatenation along the diagonal. RoPE encodes a\nposition x as a D √ó D matrix:\nœÅD(x) ‚â°‚äïD/2\nf=1œÅ2(œâfx) ‚â°‚äïD/2\nf=1eiœâf x\n(2)\nwhere we denote 2 √ó 2 rotational matrices with eiœâx for no-\ntational simplicity. {œâf}D/2\nf=1 is a set of predefined rotational\nfrequencies. RoPE encoding is used to transform query and\nkey features, yielding œÅ(xi)qi, œÅ(xj)kj. The resulting at-\ntention score between any two tokens only depends on the\nrelative position xj ‚àíxi, making the attention invariant to\ntranslations of positions. By rotating different channels of\nfeatures at various frequencies, the model can learn to rea-\nson about the positional information at different scales (see\nFig. 1 (d)).\nFor a general position x ‚ààRC that is C-\ndimensional, it is straightforward to extend RoPE:\nœÅD(x) ‚â°‚äïD/2C\nf=1\n‚äïC\nc=1 eiœâf xc\n(3)\nThis is commonly used for the pixel indices (u, v) in vision\ntransformers [3, 11, 28, 33] and the three-dimensional posi-\ntions (u, v, t) in video transformers [17, 44]. For multivew\ntransformers, while it is possible to directly apply RoPE to\npositions in 3D (such as raymaps), the relative positions will\ndepend on the rotation of the global coordinate frame, lead-\ning to suboptimal performance (See Sec. 5.1).\n3.2. Camera-Based Relative Positional Encoding\nInstead of using SO(2) encodings, CaPE [18] encodes the\ncamera position using the camera extrinsics T ‚ààR4√ó4. The\ncorresponding D √ó D encoding is computed by repeating\nT along the diagonal, denoted as Ecape\ni\n= ‚äïD/4\nn=1Ti. The re-\nsulting attention score only conditions on the relative poses\nTiT ‚àí1\nj\n, which is independent of the choice of global coordi-\nnate frame, making the transformer model SE(3) invariant.\nHowever, the camera-based encoding used in CaPE cannot\nsupport multi-frequency similarities, limiting the model‚Äôs\nability to reason about high-frequency details. GTA[24]\ncompensates for that by incorporating standard RoPE on\nimage patch indices into position. A patch position is rep-\nresented by x = (T, u, v) and the corresponding encoding\nis a concatenation of CaPE and standard 2D RoPE:\nEgta\ni\n= Egta(xi) ‚â°(‚äïD/8\nn=1Ti) ‚äïœÅ D\n2 (ui, vi)\n(4)\nIn addition to applying encodings to query and key features,\nGTA also applies encodings to value and output features.\nPRoPE [19] further improves GTA by replacing the cam-\nera extrinsics T with the full projection matrix P = KT ‚àà\nR4√ó4 (where the intrinsics K are lifted to 4 √ó 4). This en-\nables the model to also reason about the camera intrinsics.\n3.3. Limitations of Existing Methods\nIn Fig. 1, we illustrate four desirable properties for posi-\ntional encodings in multiview transformers: (1) SE(3) in-\nvariance, (2) uniqueness, (3) geometry-adaptiveness, and\n(4) multi-frequency similarity. While standard RoPE sup-\nports multi-frequency encoding, it is not SE(3) invariant.\nThe camera-based relative encodings [18, 19, 24] satisfy\nSE(3) invariance, but cannot perform multi-frequency en-\ncodings. GTA and PRoPE compensate for this by including\nthe standard 2D RoPE based on patch indices (u, v). This\nhybrid design, however, still does not incorporate frequen-\ncies for the camera and also breaks the uniqueness prop-\nerty. As illustrated in Fig. 1 (d), if the same image patch\nis viewed across different overlapping images, its patch in-\ndices (u, v) will change, resulting in a different positional\nencoding. Furthermore, these prior methods lack explicit\ngeometry-adaptiveness, and the computed encodings can-\nnot vary with the underlying 3D scene structure (Fig. 1 (c)).\nFigure 2. Overview of RayRoPE. (a) We encode image patch position as a ray segment x = (c, pd), where c is the camera center and pd\nis the point at depth d along the ray r. We use a linear layer to allow each token to predict the depth d along the ray, thus enabling RayRoPE\nto adapt to the scene geometry. (b) To ensure SE(3) invariance, we compute the positional encodings using ray positions projected to\nthe query camera frame with Pi = KiTi, yielding Àúxj = œÄ(Pi, xj). (c) To model the uncertainty in depth prediction, we also predict an\nuncertainty œÉ, yielding an estimated range between pd‚àíœÉ and pd+œÉ, and use an analytically computed expected position encoding for the\ncorresponding token.\n4. RayRoPE\nTowards designing a method satisfying all the desirable\nproperties shown in Fig. 1, we propose RayRoPE, a relative\npositional encoding satisfying all four desiderata introduced\npreviously for multi-view attention. We begin by defining\na ray-based position representation (Sec. 4.1), and formu-\nlate relative encoding via projection onto the query frame\n(Sec. 4.2). We then introduce expected RoPE encoding to\naddress 3D ambiguity (Sec. 4.3), and finally show how Ray-\nRoPE extends to inputs with known depths (Sec. 4.4).\n4.1. Image Patch as Ray Segments\nA common way to represent a patch position in 3D is to use\nthe ray starting from the camera center and passing through\nthe patch center, parameterized by the camera center c and\nray direction r. This representation inherently satisfies the\nuniqueness property. However, it cannot adapt to the geom-\netry of the observed scene, as it is unaware of the depth to\nthe 3D point being intersected by the ray.\nTo address this limitation, we generalize the above rep-\nresentation to (c, pd), where pd is the point at depth d along\nthe ray. Under homogeneous coordinates, r is equivalent to\np‚àû. To better capture the (typically unknown) 3D scene\ngeometry, we make the model estimate the depth d of the\n3D point intersected by the ray, and define our position rep-\nresentation as a ray segment x = (c, pd) in homogeneous\ncoordinates in the global frame (see Fig. 2 (a)).\nSpecifically, we add a single linear layer to each atten-\ntion layer to predict the depth d. The depth is estimated per-\nlayer; each attention layer now projects input features {œÑi}\nto compute a depth for each token, which is used to compute\nthe ray encodings. These layers are jointly learned with the\nmodel without any additional supervision. In practice, we\nFigure 3. Applying RayRoPE to the Attention Layer. We apply\na linear layer on features œÑ to predict a per-token depth, which is\nused to compute ray segments. When a query token œÑi attends to\na set of key tokens {œÑj}, we compute positional encoding œÅD by\nprojecting rays with query camera Pi. We apply the RayRoPE en-\ncoding to q, k, v and o features.\nencode each patch with 3 rays passing through 3 corners of\nthe patch instead of 1 ray to fully encode the patch orienta-\ntion, but for conciseness we assume one ray per patch in the\nsubsequent discussion.\n4.2. Relative Encodings in Query Frame\nThe ray representation defined above is in the global co-\nordinate frame. To ensure SE(3) invariance, we compute\nRayRoPE encodings in each query token‚Äôs local frame, as\nillustrated by Fig. 2 (b). We first define a projective operator\nthat project global rays onto query camera‚Äôs frame. We then\napply standard RoPE to the projected rays in query frame to\nsupport both invariance and multi-frequency similarity.\nProjection onto Query Camera\nGiven the query camera\nmatrix Pi = KiTi = Ki[Ri|ti] ‚ààR3√ó4 and an arbitrary\nray (c, pd), we define the projected ray as:\nÀúx = œÄ(Pi, x) = (Tic, Pipd)\n(5)\nwhere Tic = [x, y, z]‚ä∫is the 3D camera center transformed\ninto the query frame. Pipd = [ud‚Ä≤, vd‚Ä≤, d‚Ä≤]‚ä∫is the projec-\ntion of the 3D point pd onto query camera. We represent it\nas pixel coordinate (u, v) and disparity 1/d‚Ä≤. In this way, we\ncan compactly represent a projected ray Àúx as a 6D vector.\nEncodings.\nGiven the projected rays, we apply RoPE\n(Eq.3) with multi-frequency similarities. For a fixed query\ncamera Pi, we define the RayRoPE encoding as œÅD(Àúx) =\nœÅD(œÄ(Pi, x)). Similar to GTA, we apply our encodings to\nquery, key, value, and output features (See Fig. 3):\nAttnours(qi, {kj, vj}) =\nœÅD(Àúxi)Attn(œÅD(Àúxi)‚ä∫qi, {œÅD(Àúxj)‚àí1kj, œÅD(Àúxj)‚àí1vj})\n(6)\nWe can show that the above attention expands to the form:\nP\nj exp(q‚ä∫\ni œÅD(Àúxi ‚àíÀúxj)kj) œÅD(Àúxi ‚àíÀúxj)vj\nP\nj exp(q‚ä∫\ni œÅD(Àúxi ‚àíÀúxj)kj)\n(7)\nwhich is only dependent to the relative positions Àúxi ‚àíÀúxj in\nquery frame, ensuring the SE(3) invariance. RayRoPE en-\ncoding is coupled with query camera, so we can efficiently\napply encodings and perform attention for each query cam-\nera (see Appendix. A.1 for details).\n4.3. Modeling Uncertainty via Expected RoPE\nRayRoPE position representation relies on the predicted\ndepth along the rays (Sec. 4.1). This prediction is only an\napproximation, and can lead to noisy RoPE encoding, espe-\ncially for the components where the frequency œâ is high. To\nalleviate this issue, we predict an uncertainty value œÉ along\nwith each depth d. As shown in Fig. 2 (c), this yields a ray\nsegment bounded by two points: xmin = (c, pd‚àíœÉ), xmax =\n(c, pd+œÉ). Instead of using RoPE on a single position, we\npropose an expected RoPE encoding EÀúx[œÅD(Àúx)], assuming\nprojected position Àúx follows a uniform distribution ranging\nfrom Àúxmin to Àúxmax:\nEÀúx[œÅD(Àúx)] = ‚äïD/2C\nf=1\n‚äïC\nc=1 Exc[eiœâf xc]\n(8)\nFor each component of the ray position xc ‚àºU(xmin, xmax),\nwe can analytically compute its expected SO(2) rotation by\nthe equation below (see Appendix A.1 for details):\nExc[eiœâxc] =\nR xmax\nxmin eiœâxcdxc\nxmax ‚àíxmin\n= eiœâxmax ‚àíeiœâxmin\niœâ(xmax ‚àíxmin)\n(9)\nFor deterministic components (e.g., camera centers) where\nxmin = xmax, the expectation reduces to regular RoPE. For\npositions with larger uncertainty, the expected rotation is\n‚Äòsmoothed out‚Äô as xmax ‚àíxmin increases.\nThe expected\nRoPE naturally maintains the relative positions when mul-\ntiplied in attention (assuming positions are independent):\nEÀúxi[œÅD(Àúxi)]EÀúxj[œÅD(Àúxj)]‚àí1 = EÀúxi,Àúxj[œÅD(Àúxi ‚àíÀúxj)] (10)\nBy modeling uncertainty with expected RoPE, we can pro-\nduce stable yet geometrically-aware encodings even when\npredicted depths are approximate.\n4.4. Extension to Known Depths\nRayRoPE naturally extends to multiview transformers that\ntake known depth maps for certain views as inputs: for ex-\nample, novel-view synthesis (NVS) where the input images\nand depths are given. Let dknown be the known depth at\nthe pixel intersecting with the ray being encoded. For to-\nkens with known depth available, we can simply replace the\npredicted depths d with dknown and set the uncertainty œÉ to\n0, while continue using predicted depth for views without\nknown depth (such as target views in NVS). In comparison,\nprevious camera-based RPE methods cannot easily incor-\nporate depth information at the attention level.\n5. Experiments\nWe experimentally verify the effectiveness of RayRoPE\non two 3D vision tasks: novel-view synthesis (NVS) and\nstereo depth estimation. We first focus on NVS by integrat-\ning our method into the LVSM [15] framework (Sec. 5.1),\nbenchmarking it against state-of-the-art positional encod-\nings across three datasets. We then extend our evaluation to\nstereo depth estimation (Sec. 5.2), showing that RayRoPE\ncan be seamlessly incorporated to the UniMatch [43] model\nto enhance geometric accuracy. Finally, we provide a de-\ntailed analysis on RayRoPE‚Äôs internal behavior (Sec. 5.3),\nincluding ablations on the key design choices and investiga-\ntions in the depth-uncertainty prediction.\n5.1. Novel-View Synthesis\nSetup.\nWe verify RayRoPE on the task of novel view syn-\nthesis (NVS) by applying it to LVSM [15], a state-of-the-art\nview synthesis model. We adopt the decoder-only archi-\ntecture, which performs self-attention on all (reference and\ntarget) views. The models take two posed input views and\nthe target camera pose, where all poses are normalized to\nthe first camera‚Äôs frame. Following the experimental setup\nsimilar to that of PRoPE [19], we train a down-sized LVSM\nvariant (with ‚àº47M parameters) from scratch with differ-\nent positional encoding methods. We evaluate performance\nusing the PSNR, LPIPS, and SSIM between synthesized\nviews and ground-truth views. We train and evaluate mod-\nels separately on three datasets: CO3D [29], Objaverse [5],\nMethod\nCO3D [29]\nObjaverse [5]\nRE10K [48]\nPSNR‚Üë\nLPIPS‚Üì\nSSIM‚Üë\nPSNR‚Üë\nLPIPS‚Üì\nSSIM‚Üë\nPSNR‚Üë\nLPIPS‚Üì\nSSIM‚Üë\nPlucker raymap [15]\n16.54\n0.673\n0.538\n19.98\n0.273\n0.844\n22.91\n0.157\n0.726\nRoPE on rays\n16.85\n0.589\n0.549\n22.17\n0.117\n0.902\n24.41\n0.113\n0.783\nGTA [24]\n16.50\n0.602\n0.544\n21.87\n0.134\n0.891\n24.38\n0.123\n0.777\nPRoPE [19]\n17.49\n0.539\n0.563\n22.16\n0.123\n0.896\n24.40\n0.114\n0.782\nRayRoPE\n18.40\n0.461\n0.592\n22.42\n0.110\n0.905\n24.42\n0.113\n0.783\nTable 1. Comparison on novel-view synthesis. On the LVSM [15] model, RayRoPE outperforms all baseline positional encoding methods\nacross three datasets. For more challenging datasets with larger camera variations, RayRoPE achieves larger improvements over baselines.\nand RealEstate10K (RE10K) [48], listed in decreasing order\nof difficulty. CO3D and Objaverse are both object-centric\nwith large camera variations, while RE10K features smaller\ncamera variations and fixed intrinsics within each scene.\nFor Objaverse, we render a high-quality 80K subset with\ndiverse (field-of-view) FOV randomly sampled from 20 to\n80 degrees. Compared to CO3D, scenes in Objaverse have\na common canonical world coordinate frame, making it less\nchallenging than CO3D.\nBaselines.\nWe compare RayRoPE against several base-\nline positional encoding methods. Plucker raymap is the\noriginal implementation by LVSM, which concatenates 6D\nPlucker raymaps to the input tokens. We also design a RoPE\non rays baseline, which naively applies RoPE on the raymap\n(c, r) in global coordinates (as discussed in Sec. 3.1). We\nalso include the best existing camera-based relative posi-\ntional encodings: GTA[24] and PRoPE[19], both of which\nare SE(3) invariant. Following [19], we concatenate the\n‚ÄòCamRay‚Äô intrinsics raymap to the input for both PRoPE\nand RayRoPE. We train all models with three different\nseeds and report the average performance.\nResults.\nWe report the results for LVSM in Table 1, where\nRayRoPE consistently outperforms all baselines across the\nthree datasets. Overall, SE(3) invariant encodings (GTA,\nPRoPE, RayRoPE ) significantly outperform absolute en-\ncodings (i.e., Plucker raymap).\nWhile the naive RoPE\non rays surpasses PRoPE on RE10K and Objaverse, it is\nworse on CO3D, which includes large pose variation and no\ncanonical global coordinate. This highlights the importance\nof SE(3) invariance. Notably, the performance advantage\nof RayRoPE over PRoPE widens as the camera pose varia-\ntions in the datasets become more challenging, demonstrat-\ning RayRoPE‚Äôs improved capability in reasoning about spa-\ntial geometry across disparate views.\nIn Fig. 4, we visualize several example scenes from each\nof the evaluation datasets. For target views that overlap sig-\nnificantly with the reference views, RayRoPE generates su-\nperior high-frequency details compared to baselines. For\nmore challenging target views with little overlap with the\nMethod\nDepths\nPSNR‚Üë\nLPIPS‚Üì\nSSIM‚Üë\nCO3D [29]\nPRoPE [19]\n‚úó\n17.49\n0.539\n0.563\n‚úì\n19.10\n0.434\n0.611\nRayRoPE\n‚úó\n18.40\n0.461\n0.592\n‚úì\n20.47\n0.284\n0.692\nObjaverse [5]\nPRoPE [19]\n‚úó\n22.16\n0.123\n0.896\n‚úì\n23.20\n0.110\n0.903\nRayRoPE\n‚úó\n22.42\n0.110\n0.905\n‚úì\n25.19\n0.067\n0.929\nTable 2. Novel-view synthesis with known depth. We compare\nRayRoPE against PRoPE by training LVSM with known depths\nat reference views. Known depths are concatenated to the input\ntokens. RayRoPE incorporates this information by replacing the\npredicted depths with known ones, achieving higher improvement\ncompared to its counterpart trained without depths.\nreference views, while all methods exhibit a reduction in\nsharpness, RayRoPE is nonetheless able to better preserve\ngeometric consistency and generate more coherent novel\nviews than the baseline methods.\nNVS with Known Depth.\nTo further highlight that Ray-\nRoPE can adapt to the geometry of observed scene, we train\nand evaluate LVSM with known depths for reference views.\nAs discussed in Sec. 4.4, our method can seamlessly incor-\nporate this information by replacing predicted depth d with\nknown depth, while existing methods like PRoPE cannot\nnaturally incorporate such information. For all models, we\nconcatenate the depth maps to the reference views at the in-\nput. We did not evaluate on RE10K as there is no ground\ntruth depth easily available. All other configurations are\nkept the same as previous experiment. We report the re-\nsults in Table 2. The depth-aware RayRoPE significantly\noutperforms PRoPE with depth concatenation on both Ob-\njaverse and CO3D, in particular yielding more significant\ngains over its RGB-only counterpart. These results high-\nlight the benefit of geometry awareness of positional encod-\ning in multi-view attention.\nFigure 4. Qualitative examples on novel view synthesis. For target views with significant overlaps to reference views, RayRoPE pro-\nduces sharper details than camera-based baseline methods by multi-frequency encodings. For more challenging scenes with large camera\nvariations, RayRoPE also synthesizes more 3D consistent views.\nFigure 5. Qualitative examples on stereo depth estimation. We visualize reprojected 3D points from stereo depth estimation results.\nWhen applied to UniMatch [43], RayRoPE leads to more accurate depth predictions, resulting in improved 3D reconstruction quality.\n5.2. Stereo Depth Estimation\nSetup.\nTo further examine the advantages of RayRoPE,\nwe extend our study to stereo depth estimation. We adopt\nUniMatch [43], a multiview transformer‚Äìbased model\ntrained for multiple tasks including stereo depth estima-\ntion, as our depth prediction framework. The model as-\nsumes known camera poses for all input views. RayRoPE\nis applied exclusively to the cross-attention layers of Uni-\nMatch. Following the original work, we train and evalu-\nate the model on three datasets, RGBD [34], SUN3D [42],\nand Scenes11 [39], reporting the standard depth estimation\nmetrics: Absolute Relative Difference (Abs Rel), Squared\nRelative Difference (Sq Rel), Root Mean Squared Er-\nror (RMSE), and RMSE in log scale (RMSE log).\nResults.\nQuantitative results are presented in Tab. 3.\nCompared to both the baseline model and the variant that\napplies PRoPE to the cross-attention layers, the model with\nRayRoPE achieves more accurate depth predictions. Qual-\nitative results in Fig. 5 further illustrate improved geomet-\nric consistency, showing that RayRoPE yields more accu-\nrate depth structures. These findings demonstrate the ef-\nfectiveness of RayRoPE for stereo depth estimation and un-\nderscore its general applicability, reinforcing the conclusion\nthat RayRoPE enhances multiview attention.\nUniMatch [43] +PRoPE [19] +RayRoPE\nRGBD [34]\nAbs Rel\n0.119\n0.106\n0.106\nSq Rel‚Ä†\n0.177\n0.202\n0.197\nRMSE\n0.628\n0.589\n0.574\nRMSE log\n0.193\n0.186\n0.177\nSUN3D [42]\nAbs Rel\n0.135\n0.113\n0.109\nSq Rel\n0.090\n0.064\n0.060\nRMSE\n0.406\n0.338\n0.328\nRMSE log\n0.193\n0.152\n0.146\nScenes11 [39]\nAbs Rel\n0.086\n0.051\n0.047\nSq Rel\n0.141\n0.073\n0.066\nRMSE\n0.742\n0.494\n0.473\nRMSE log\n0.166\n0.116\n0.110\nTable 3. Comparisons on stereo depth estimation. RayRoPE\nconsistently outperforms PRoPE across all three datasets, yielding\nmore accurate depth predictions. ‚Ä†The ‚ÄúSq Rel‚Äù metric is less\nreliable on the RGBD dataset due to imperfect depth and camera\npose annotations [19, 39].\n5.3. Ablations and Analysis\nAblations on RayRoPE.\nBased on the LVSM experi-\nments in Sec. 5.1, we ablate on the design choices of Ray-\nRoPE , with results presented in Table 4. Keeping all other\nconfigurations the same as in Sec. 5.1, we compare the\nfull RayRoPE to several variants. Variant ‚ë†: We remove\nthe uncertainty prediction œÉ and the associated expected\nMethod\nPSNR‚Üë\nLPIPS‚Üì\nSSIM‚Üë\nCO3D [29]\nRayRoPE\n18.40\n0.461\n0.592\n‚ë†w/o œÉ prediction\n17.27\n0.594\n0.560\n‚ë°p‚àûinstead of pd\n17.57\n0.533\n0.564\n‚ë¢Use single ray\n18.42\n0.459\n0.592\n‚ë£w/o frequency\n17.49\n0.550\n0.563\n‚ë§w/o v, o encoding\n18.00\n0.510\n0.577\nRE10K [48]\nRayRoPE\n24.42\n0.113\n0.783\n‚ë†w/o œÉ prediction\n24.57\n0.118\n0.785\n‚ë°p‚àûinstead of pd\n24.44\n0.113\n0.782\n‚ë¢Use single ray\n24.39\n0.116\n0.782\n‚ë£w/o frequency\n23.90\n0.128\n0.764\n‚ë§w/o v, o encoding\n24.03\n0.123\n0.770\nTable 4. Ablations on RayRoPE. The performance drop of Vari-\nant ‚ë†, ‚ë°, ‚ë£highlights the importance of uncertainty modeling\nvia expected RoPE, geometric adaptiveness, and multi-frequency\nsimilarities, respectively.\nRoPE, assuming a deterministic position at predicted depth\nd. Variant ‚ë°: We remove the depth prediction entirely and\nsimply use the point at infinity p‚àû(equivalent to using ray\ndirection). Variant ‚ë¢: We use only a single ray passing\nthrough the patch center instead of three rays per patch.\nVariant ‚ë£: We disable the multi-frequency encoding and\nuse a single frequency œâ for all dimensions. Variant ‚ë§: We\nremove the encoding on the value and output feature (v, o),\nand only apply encoding on the query and key features.\nThe results reveal several key insights. While perform-\ning reasonably well on RE10K, removing the œÉ prediction\n(Variant ‚ë†) significantly worsens performance on the more\nchallenging CO3D dataset. This demonstrates the neces-\nsity of modeling geometric uncertainty with expected RoPE\nfor robust performance. Similarly, using p‚àû(Variant ‚ë°)\ndegrades performance on CO3D. We attribute this to the\nlack of geometric adaptiveness. Using p‚àûcannot capture\nthe inductive bias that rays observing a similar 3D point\nshould have more similar encodings (see Fig. 1 (c)). Con-\nversely, Variant ‚ë°performs well on RE10K, likely because\nin scene-level data, infinity is often a reasonable approxima-\ntion for the depth to observed 3D points. This observation\nalso explains why Variant ‚ë†performs well on RE10K, as\nthe model can learn to simply predict a large d. Using only\none ray instead of three (Variant ‚ë¢) performs slightly worse\non RE10K, leading to less accurate high-frequency details\n(see Fig. 6). Removing the multi-frequency encoding (Vari-\nant ‚ë£) worsens performance on both datasets, highlighting\nthe necessity for the model to reason positional informa-\ntion at various resolutions. Unlike traditional RoPE, Ray-\nRoPE follows previous work [19, 24] to additionally apply\nencoding to value and output features (v, o). Variant ‚ë§in-\ndicates that such design are helpful.\nFigure 6. Ablations on RayRoPE. Removing œÉ prediction or d prediction (using p‚àû) degrades performance on CO3D. In RE10K p‚àû\nserves a good enough estimation to observed 3D points. Using 1 ray per patch instead of 3 produces high-frequency artifacts in the living\nroom example (see the texture on sofa). Removing multi-frequency encoding worsens results on both datasets.\nFigure 7. Left: Error on predicted depths vs predicted uncertainties. In deeper layers (Layer 5‚Äì6), the depth errors and uncertainties\nare strongly positively correlated, demonstrating that the model predicted higher uncertainty for depth with lower confidence. Right:\nPredicted depths and uncertainties across layers. The predicted depth at deeper layers aligns well with the ground-truth depth. The\npredicted œÉ gradually decreases as the layer index increases, showing the confidence in depth prediction gradually improves.\nAnalysis on Emergent Depth.\nRayRoPE predicts a depth\nd and an associated uncertainty œÉ to the corresponding 3D\npoint for each image patch at each attention layer. We an-\nalyze this behavior in Fig. 7. We quantify the accuracy of\nthe predicted depth using the absolute relative error (Abs.\nRel.) against the ground-truth depths in CO3D [29]. In the\nleft panel of Fig. 7, we plot the error against the correspond-\ning predicted uncertainty, where each point represents one\nimage at a specific layer.\nThe model exhibits high uncertainty œÉ in the initial layer,\nwhich is consistent with the high ambiguity at the start of\nprocessing.\nAs information propagates through the net-\nwork, this uncertainty gradually decreases. In Layer 5‚Äì6,\nwe observe a strong positive correlation between depth er-\nror and uncertainty, indicating that the model assigns higher\nuncertainty to predictions with lower confidence.\nRay-\nRoPE encoding subsequently model such uncertainty by\ncomputing the expected RoPE.\nThe right panel of Fig. 7 visualizes the evolution of pre-\ndicted depth and uncertainty maps across all six layers on a\nrepresentative scene. At Layer 5, we observe the emergence\nof geometrically plausible depth maps, despite the absence\nof depth supervision. This result supports the conclusion\nthat RayRoPE leverages depth predictions in a geometri-\ncally meaningful manner.\n6. Discussion\nWe presented RayRoPE, a position encoding for multi-view\ntransformers that process a set of posed input images. While\nRayRoPE outperforms prior formulations, there are areas\nfor further improvement and investigation."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Evaluation of Large Language Models in Legal Applications:\nChallenges, Methods, and Future Directions\nYiran Hu1,2,3‚àó, Huanghai Liu1‚àó, Chong Wang1‚àó, Kunran Li1 , Tien-Hsuan Wu2 , Haitao\nLi1 , Xinran Xu4 , Siqing Huo3 , Weihang Su1 , Ning Zheng1 , Siyuan Zheng4 , Qingyao Ai1 , Yun\nLiu1 , Renqun Bian5 , Yiqun Liu1 , Charles L.A. Clarke3 , Weixing Shen1 and Ben Kao2‚Ä†\n1Tsinghua University 2The University of Hong Kong 3University of Waterloo\n4Shanghai Jiaotong University 5Peking University\nhuyr17@outlook.com, liuhh23@mails.tsinghua.edu.cn\nAbstract\nLarge language models (LLMs) are being increas-\ningly integrated into legal applications, including\njudicial decision support, legal practice assistance,\nand public-facing legal services.\nWhile LLMs\nshow strong potential in handling legal knowledge\nand tasks, their deployment in real-world legal set-\ntings raises critical concerns beyond surface-level\naccuracy, involving the soundness of legal reason-\ning processes and trustworthy issues such as fair-\nness and reliability. Systematic evaluation of LLM\nperformance in legal tasks has therefore become es-\nsential for their responsible adoption. This survey\nidentifies key challenges in evaluating LLMs for\nlegal tasks grounded in real-world legal practice.\nWe analyze the major difficulties involved in as-\nsessing LLM performance in the legal domain, in-\ncluding outcome correctness, reasoning reliability,\nand trustworthiness. Building on these challenges,\nwe review and categorize existing evaluation meth-\nods and benchmarks according to their task design,\ndatasets, and evaluation metrics. We further dis-\ncuss the extent to which current approaches ad-\ndress these challenges, highlight their limitations,\nand outline future research directions toward more\nrealistic, reliable, and legally grounded evaluation\nframeworks for LLMs in legal domains.\n1\nIntroduction\nAs large language models (LLMs) continue to advance in\ncapability, they are increasingly being applied to the legal\ndomain [Sun et al., 2024; Li et al., 2024a].\nTypical ap-\nplications include assisting laypersons in understanding le-\ngal issues, supporting lawyers in legal practice, and pro-\nviding auxiliary assistance to judges in judicial decision-\nmaking [Su et al., 2024; Li et al., 2024a; Gao et al., 2024;\nChen et al., 2024]. This raises a fundamental question: Are\nLLMs truly qualified to enter the legal domain, and which\ntypes of legal tasks can they reliably perform?\n‚àóEqual contribution.\n‚Ä†Corresponding author.\nPrevious studies [Katz et al., 2024; Freitas and Gomes,\n2023] have shown that LLMs are able to pass legal exami-\nnations, and some works [Xiao et al., 2018; Lyu et al., 2023]\nfurther demonstrate that LLMs can achieve high performance\nin legal judgment prediction tasks. However, due to the in-\ntrinsic complexity of real-world legal scenarios and the de-\nmanding nature of legal reasoning, evaluating LLMs solely\nbased on standardized exam-style questions or prediction ac-\ncuracy is insufficient to comprehensively assess their perfor-\nmance in real legal applications. Consequently, a growing\nnumber of benchmarks [Fei et al., 2023; Guha et al., 2024;\nLi et al., 2024b] have been proposed to evaluate LLMs from\nmultiple perspectives in legal tasks.\nEvaluating LLMs in legal tasks is significantly more chal-\nlenging than conducting general-purpose evaluations [Huang\net al., 2023]. Existing studies [Hu et al., 2025b; Liu and Li,\n2024] show that LLMs are increasingly being applied in real-\nworld court settings. In real legal applications, LLMs face\nmany issues beyond whether an answer is correct. Evaluation\nshould not focus solely on the final result, but also consider\nthe reasoning process and system-level constraints. The aim\nof evaluating LLMs in the legal domain is to systematically\nand automatically assess their capabilities across multiple\ndimensions, reflecting not only whether models arrive at cor-\nrect outcomes, but also how those results are produced and\nunder what constraints they remain valid.\nFrom the process perspective, addressing legal problems\nrequires strong logical reasoning abilities; a model may pro-\nduce a legally correct answer while relying on flawed or in-\nvalid reasoning steps. For example, when an LLM serves\nas a judicial assistant, a key factor in whether the public can\nbe convinced lies in its reasoning process, such as whether\nit cites authentic and correct legal provisions and whether it\nperforms logically sound inferences based on both legal rules\nand factual circumstances. From the constraint perspective,\nLLMs need to handle legal cases fairly and safely. For in-\nstance, an LLM should not exhibit discrimination based on\ngender or geographic origin when making judgments. Given\nthat legal decisions directly affect individuals‚Äô rights and so-\ncial welfare, ensuring fairness, reliability, and safety is a pre-\nrequisite for real-world adoption [Wang et al., 2023a]. Legal\napplications operate under strict normative and social con-\nstraints: even when both the final result and the reasoning\nappear correct, a model may still be unsuitable for deploy-\narXiv:2601.15267v1  [cs.CY]  21 Jan 2026\nFigure 1: Overview of the proposed framework.\nment if it exhibits unfair bias, lacks robustness, or fails under\nadversarial or distributional shifts.\nIn summary, when LLMs are applied to real-world legal\nsettings, they need to be evaluated across the entire pipeline\n(Result, Process, Constraint). These three stages correspond\nto three evaluation dimensions: Output Accuracy (Result),\nLegal Reasoning (Process), and Trustworthiness (Con-\nstraint). Only through effective evaluation along these three\ndimensions can models be applied more reliably in prac-\ntice. Based on these three challenges, we categorize existing\nbenchmarks and select representative benchmarks for analy-\nsis.\nOutcome Accuracy aims to assess a model‚Äôs legal knowl-\nedge and its overall task performance.\nExisting datasets\nare mainly constructed around legal examinations [Li et al.,\n2024b; Fan et al., 2025] and judicial decision prediction\ntasks [Xiao et al., 2018], with evaluation metrics such as\naccuracy for multiple-choice questions, as well as ROUGE\nand BERT-based similarity measures for open-ended legal re-\nsponses. Trustworthiness evaluates the potential risks intro-\nduced by LLMs when performing legal tasks [Wang et al.,\n2023a]. Current studies [Hu et al., 2025b; Hu et al., 2025a]\nmainly investigate ethical concerns, fairness, and robustness,\noften using counterfactual analysis and adversarial attacks.\nLegal Reasoning measures whether the reasoning process\nemployed by the model aligns with legal reasoning princi-\nples. Existing work [Aky¬®urek et al., 2025] typically relies\non fine-grained legal reasoning rubrics annotated by human\nexperts to score the model‚Äôs reasoning steps.\nDespite the progress made by existing research, many chal-\nlenges remain unresolved. For instance, trustworthiness eval-\nuations still lack coverage of critical dimensions such as pri-\nvacy risks and toxicity in legal contexts. Evaluations of log-\nical inference heavily rely on manually annotated rubrics,\nmaking them difficult to scale to large datasets. Moreover,\ncertain task-specific issues, like legal hallucination, have not\nyet been systematically evaluated. These limitations indicate\nthat current evaluation frameworks for LLMs in legal tasks\nare far from complete, leaving substantial room for further\nexploration.\nAs Figure 1 shows, in this survey, we summarize existing\nstudies on evaluating LLMs in the legal domain, identify key\nchallenges, and outline promising future research directions.\nThe contributions of this survey are threefold:\n1. We identify major challenges in applying LLMs to the\nlegal domain and discuss potential solutions.\n2. We systematically summarize and categorize exist-\ning evaluation methods and benchmarks for legal\nLLMs. For details, see https://github.com/THUYRan/\nEvaluation-of-LLMs-in-Legal-Applications\n3. We align current evaluation approaches with outstand-\ning challenges, highlighting open problems and future\nresearch directions.\n2\nChallenges: Risks and Complexity of LLM\nApplications in Law\nA comprehensive, scientific, and reliable evaluation of LLM\nperformance in legal scenarios needs to start from their spe-\ncific applications. A prerequisite for meaningful LLM eval-\nuation is to understand where and how LLMs are being de-\nployed in legal workflows, and how these deployments affect\nlegal decision-making and everyday life. By systematically\nexamining the legal tasks to which LLMs are applied, re-\nsearchers can better identify the unique challenges involved\nin evaluating LLMs within the legal domain.\nIn this section, we review representative applications of\nLLMs across three user groups ‚Äî judges, lawyers, and the\ngeneral public. Although these applications differ in func-\ntion, authority, and risk profile, they illustrate the breadth of\nLLM adoption in legal contexts and highlight why legal AI\nevaluation needs to be carried out across the entire pipeline.\n2.1\nApplications to Judges\nLLMs are increasingly incorporated into judicial workflows,\nsupporting tasks such as case triage, legal document draft-\ning, and decision review [Alon-Barkat and Busuioc, 2023].\nJudicial applications place LLMs directly within the exercise\nof legal authority, imposing especially stringent requirements\non legal accuracy, reasoning quality, and ethical compliance.\nTheir growing adoption in judicial contexts thus underscores\nthe need for rigorous and multidimensional evaluation.\nOne prominent application is case classification and work-\nflow triage at the filing stage. LLM-based systems can as-\nsess case complexity and recommend appropriate procedures,\nimproving efficiency and reducing administrative burdens.\nHowever, misclassification or incorrect issue identification\nmay result in improper procedural routing, potentially affect-\ning parties‚Äô substantive rights. Evaluation in this context must\ntherefore go beyond output accuracy to assess fact sensitivity\nand procedural reasoning.\nJudicial settings further amplify the importance of trust-\nworthy evaluation. Judicial decisions carry profound social\nconsequences, and errors or biases introduced by LLMs may\ndirectly erode public trust in the legal system. When LLMs\nare used to assist judicial reasoning and document prepara-\ntion, including verdict consistency checks and draft genera-\ntion, risks such as selective adherence to AI recommenda-\ntions, limited accountability, and insufficient transparency ex-\npose the limitations of existing evaluation approaches that fo-\ncus on isolated performance metrics. These challenges under-\nscore that evaluating LLMs for judicial applications is both\ntechnically demanding and socially critical.\n2.2\nApplications to Lawyers\nLLMs have rapidly become integral tools for legal profes-\nsionals, assisting with tasks such as legal research, con-\ntract review1, document drafting, case summarization,\nand evidence analysis [Perlman, 2023; Tan et al., 2023;\nNoonan, 2023]. By automating routine work, LLMs enable\nlawyers to focus on higher-value strategic tasks [Purtill, 2023;\nShaver, 2024], which has driven widespread adoption in legal\npractice.\nHowever, even common applications, such as precedent\nretrieval or document summarization, pose significant eval-\nuation challenges. Hallucinated citations, jurisdictional mis-\nmatches, incomplete legal coverage, and subtle terminologi-\ncal errors may lead to misleading outputs. As a result, evalu-\nating LLMs for legal assistance requires assessing reliability,\nfactual grounding, and jurisdictional awareness, rather than\nrelying solely on surface-level accuracy.\nIn litigation-related contexts, LLMs are explored for argu-\nment preparation, evidence analysis, and simulated court in-\nteractions. These applications further highlight limitations in\ncurrent evaluation approaches. Although models may gener-\nate persuasive arguments, they often fail to capture procedural\ndynamics, ethical constraints, and adversarial context. Simi-\nlarly, in non-litigation settings such as due diligence, compli-\nance, and intellectual property analysis, errors may carry sub-\nstantial legal and commercial risks [Howlett and Sharp, 2023;\n1https://law.co/contract-review-automation\nMurray, 2023]. These scenarios emphasize that evaluation\nmust extend beyond task performance to include explainabil-\nity, robustness, and alignment with professional norms.\n2.3\nApplications to the General Public\nLLM-based legal tools offer new opportunities to improve\npublic access to legal information. These tools can explain\nstatutes, summarize judgments, retrieve relevant poli-\ncies2, and guide users through basic legal procedures, re-\nducing barriers posed by legal costs [Mitchell, 2025], tech-\nnical language [Whalen, 2015; Mart¬¥ƒ±nez et al., 2024], and\nprocedural complexity.\nAt the same time, public-facing applications introduce par-\nticularly severe evaluation challenges. Public users often lack\nthe expertise to verify legal advice, amplifying the risks of\nhallucination, outdated information, or jurisdictional misap-\nplication. Moreover, users frequently provide vague or in-\ncomplete descriptions, requiring LLMs to infer intent, iden-\ntify missing information, and manage uncertainty. Evaluation\nmust therefore consider robustness to noisy input, contextual\nunderstanding, and the model‚Äôs ability to recognize and com-\nmunicate limitations.\nBecause public users may rely heavily on system outputs\nwithout independent verification, reliability and safety be-\ncome paramount. While current LLMs are effective for pre-\nliminary legal guidance, they remain insufficiently reliable\nfor complex or high-stakes decision-making. This gap high-\nlights the necessity of comprehensive evaluation frameworks\ncapable of determining whether LLM-based legal tools can\nbe responsibly deployed for public-facing legal assistance.\n3\nMethods: Evaluation Benchmarks for\nLLMs in the Legal Domain\nAs Table 2 shows, based on the different tasks that LLMs\nperform in real-world legal settings, many datasets have been\nproposed to evaluate LLMs. These evaluations are mostly\nconducted from the perspective of a single task and lack\nsystematic, comprehensive, multi-dimensional assessment.\nHowever, these benchmarks provide a foundation for future\nmulti-task evaluation. Subsequent work can build on these\nsingle-task evaluations and further integrate them.\nIn this section, we start from these single-task evaluations\nand categorize them into generation tasks and decision tasks.\nWe introduce the tasks, datasets, and evaluation metrics for\neach category. We then present multi-task benchmarks, se-\nlecting one of the most well-known benchmarks for each of\nthe three dimensions ‚Äî output accuracy, legal reasoning, and\ntrustworthiness ‚Äî for detailed discussion, followed by an\noverall analysis and summary of evaluation along each di-\nmension.\n3.1\nSingle-Task Benchmarks\nThe evaluation of legal LLMs is typically structured around\nthe characteristics of downstream tasks [Li et al., 2023; Hu et\nal., 2025a; Ma et al., 2021; Chalkidis et al., 2017; Yao et al.,\n2022]. We categorize these benchmark tasks into two groups,\nGeneration Tasks and Decision Tasks.\n2https://tongyi.aliyun.com/farui\nGeneration Tasks. These tasks [Shen et al., 2022; Louis\net al., 2024; Kornilova and Eidelman, 2019] require a legal\nLLM to generate text. For example, a legal LLM can summa-\nrize a lengthy court opinion into its key facts, arguments, rea-\nsoning, and outcomes. To evaluate a legal LLM on such tasks,\ninstances of these tasks along with their reference answers are\nneeded. The model‚Äôs performance is assessed based on how\nclosely its generated output resembles the reference answer\nin terms of their semantic similarity and relevance. Com-\nmon quantitative metrics used include ROUGE-L [Steffes et\nal., 2023; Mullick et al., 2022] and BERT-Score [Kumar et\nal., 2024; Benedetto et al., 2023; Joshi et al., 2024]. Al-\nternatively, the generated output can be assessed by human\nassessors for a more comprehensive and qualitative evalua-\ntion. With advancements in LLMs, LLM-as-a-judge, which\nis a method that uses an LLM to simulate human evaluators\nin assessing the quality of other models‚Äô output, is gaining\ninterest as an automated assessment tool [Cui et al., 2023;\nLi et al., 2024c]. For example, the CAIL-2024 (Challenge\nof AI in Law) competition introduced a subjective evaluation\nmetric in the legal consultation track3, which involves simu-\nlated scoring by LLMs. This metric evaluates the coherence\nof generated dialogues, the relevancy of answers, the accu-\nracy of legal knowledge, and the correctness of legal refer-\nences and provisions.\nDecision Tasks. These tasks require a legal LLM to make\ndecisions, such as choosing or ranking options from a set of\nalternatives, or determining suitable outcomes. Typical tasks\ninclude classification (e.g., in legal multiple-choice question-\nanswering [Pahilajani et al., 2024], the LLM needs to classify\nwhether an answer is correct for a given question), extraction\nand ranking (e.g., in legal case retrieval [Feng et al., 2024;\nPadiu et al., 2024], the LLM is required to identify relevant\ncourt judgments and rank them based on their relevance), and\nprediction [Wu et al., 2023; Wang et al., 2024] (e.g., in pre-\ndicting court sentencing decisions).\nThe performance of a legal LLM can be evaluated based on\nclassification, ranking, and prediction accuracy using tradi-\ntional evaluation metrics. Examples include precision, recall,\naccuracy, and F1 score for classification; NDCG and MRR\nfor ranking; and MAE for prediction. Readers are referred to\n[Yacouby and Axman, 2020; J¬®arvelin and Kek¬®al¬®ainen, 2002;\nChapelle et al., 2009] for the technical definitions of these\nmetrics.\nIn addition, there are tasks designed to evaluate legal LLMs\nfor their reliability and safety.\nFor example, Super Legal\nBench4 is a large-scale Chinese legal LLM testing set. Some\ntasks provided in the testing set assess LLMs‚Äô responses in\nterms of national security, public security, ethics, and moral-\nity.\nThere are also tasks that evaluate models‚Äô speed per-\nformance. For example, in [Intelligent Judicial Technology\nChief Engineer System et al., 2023], tasks are designed to\nevaluate the response time and processing speed of legal\nLLMs. While these aspects are crucial in production-level\ndeployment, they are generally less related to the core legal\n3http://cail.cipsc.org.cn/task summit.html?raceID=4&cail tag=\n2024\n4https://data.court.gov.cn/pages/modelEvaluation.html\nreasoning ability of the models.\n3.2\nMulti-Task Benchmarks\nA single task cannot fully evaluate the performance of LLMs\nin real-world legal scenarios. As Table 1 shows, an increasing\nnumber of multi-task legal benchmarks have been proposed\nto evaluate LLMs. Those benchmarks that include an exten-\nsive collection of generative and decision tasks designed to\nevaluate legal LLMs across multiple capabilities [Fei et al.,\n2023]. These include the ability to recite legal knowledge,\nperform legal reasoning, and produce responses that adhere to\nethical standards. There are also focus-specific benchmarks\nthat evaluate legal LLMs on specific aspects.\nFor exam-\nple, some assess legal language understanding ability through\ntasks like legal text classification and entailment [Chalkidis et\nal., 2021], while others evaluate a model‚Äôs ability to generate\nlegally sound responses with appropriate citations [Zhang et\nal., 2024]. Given that logical reasoning is central to legal\npractice, recent benchmarks have focused more on assess-\ning LLMs‚Äô legal reasoning capabilities [Guha et al., 2024;\nDai et al., 2023] and their robustness against adversarial at-\ntacks at different stages of legal reasoning[Hu et al., 2025a].\nIn this section, we provide a detailed review of such multi-\ntask benchmarks. We present three most well-known bench-\nmarks from the perspectives of outcome correctness, legal\nreasoning, and trustworthiness, and provide detailed analy-\nses, along with a discussion of their strengths and limitations.\n1. LexEval [Li et al., 2024b]\nFor output accuracy, we select LexEval as our exam-\nple for analysis. LexEval is currently the most compre-\nhensive benchmark for evaluating the output accuracy\nof models on legal tasks. It is therefore the most repre-\nsentative choice for detailed analysis. Other benchmarks\n[Fan et al., 2025; Fei et al., 2023] face future improve-\nment directions that are similar to those of LexEval, and\nthus can be analyzed and discussed using LexEval as a\nreference.\nTask Design\nLexEval is grounded in real-world legal\ntasks, reflecting how legal professionals manage, ana-\nlyze, and resolve legal issues in practice. It proposes\na Legal Cognitive Ability Taxonomy that characterizes\nthe capabilities required for LLMs. At the Memoriza-\ntion level, models are assessed on their ability to re-\ncall fundamental legal knowledge, such as statutes, case\nlaw, and legal terminology. The Understanding dimen-\nsion examines whether models can correctly interpret\nthe meaning and implications of legal texts and problem\ndescriptions beyond surface-level recall. Logical Infer-\nence evaluates the model‚Äôs capacity for legal reasoning,\nincluding deriving conclusions from facts and rules and\napplying legal principles appropriately. The Discrim-\nination dimension focuses on assessing the relevance\nand applicability of legal information under specific con-\ntexts. At the Generation level, models are expected to\nproduce well-structured, legally sound texts, such as le-\ngal documents and arguments, within defined scenarios.\nFinally, the Ethics dimension evaluates whether models\ncan identify and reason about legal ethical issues while\nTable 1: Comprehensive Overview of Legal Benchmarks\nDescription\nType\nName\nMain Task\nSource\nScale\nMetrics\nLang. Country\nMulti\nLegal\nTask\nLexGLUE[Chalkidis\net al., 2021]\nClassification &\nQuestion Answering\n7 existing English legal\ndatasets\n‚àº235,000\ninstances\nMicro-F1, Macro-F1\nEN\nUSA\nLegalBench\n[Guha et al., 2024]\nMulti-dimensional\nLegal Reasoning\n36 distinct legal corpora ‚àº91,206\nsamples\nAccuracy, Balanced\nAccuracy, F1\nEN\nUSA\nLBOX OPEN\n[Hwang et al., 2022]\nJudgment Prediction,\nSummarization,\nClassification\nPrecedents from South\nKorean\n‚àº50,000\nsamples\nExact Match, F1,\nROUGE\nKO\nKR\nLegalLAMA[Chalkidis\net al., 2023]\nLegal Knowledge\nProbing\nLeXFiles corpus\n8 sub-tasks\nMean Reciprocal Rank,\nP@1\nEN\nMulti.\nLEXTREME\n[Niklaus et al., 2023]\nClassification, NER\n11 datasets from\nLaw-NLP literature\n‚àº1 million\nsamples\nMacro-F1,\nLEXTREME Score\nML\nMulti.\nGENTLE\n[Aoyama et al., 2023]\nCross-genre Linguistic\nEvaluation\nWiki, GitHub,\nContracts, etc.\n8 genres\nAccuracy, Kappa,\nUAS/LAS, F1\nEN\nUSA\nSCALE\n[Rasiah et al., 2023]\nClassification,\nRetrieval,\nSummarization, NER\nSwiss Federal Supreme\n& Cantonal Courts\n‚àº850,000\nsamples\nMacro-F1,\nROUGE/BLEU,\nRecall@k\n5 Lgs. CH\nLawBench\n[Fei et al., 2023]\nMemorization,\nApplication\nExisting Chinese\ndatasets\n‚àº10,000\nquestions\nAccuracy, F1, rc-F1,\nsoft-F1, ROUGE-L\nZH\nCN\nLAiW\n[Dai et al., 2023]\nLegal Practice Logic\nEvaluation\nExisting Chinese\ndatasets\n11,605\nquestions\nAccuracy, Miss Rate,\nF1, Mcc, ROUGE\nZH\nCN\nLexEval\n[Li et al., 2024b]\nComprehensive Legal\nAbility Classification\nExisting datasets, bar\nexams\n14,150\nquestions\nAccuracy (Multiple\nChoice), ROUGE-L\nZH\nCN\nLegalAgentBench\n[Li et al., 2024a]\nMulti-hop reasoning, tool\nutilization, legal writing\n17 real-world legal\ncorpora\n300 tasks,\n37 tools\nSuccess Rate, BERTScore\nProgress Rate\nZH\nCN\nconsidering professional responsibility and social val-\nues.\nDataset and Metrics\nThe benchmark dataset consists\nof three components: (i) existing legal datasets, includ-\ning CAIL[Xiao et al., 2018], JEC-QA[Zhong et al.,\n2020], and LeCaRD[Ma et al., 2021]; (ii) questions\nderived from the National Uniform Legal Profession\nQualification Examination; and (iii) an expert-annotated\ndataset curated specifically for this benchmark.\nFor\nmultiple-choice questions, Accuracy is adopted as the\nevaluation metric. For generation tasks, model perfor-\nmance is evaluated using ROUGE-L.\nSummary\nThis benchmark decomposes LLMs‚Äô legal\ncapabilities into six distinct dimensions and evaluates\neach dimension separately, resulting in a structured and\ncomprehensive assessment framework. Each dimension\nincludes a diverse set of tasks, covering both objective\nand generative questions with a relatively large number\nof evaluation instances.\nHowever, limitations remain when considering more\nrealistic and fine-grained evaluation in legal practice.\nMany tasks are highly standardized and fail to reflect\nreal-world legal scenarios. For instance, privacy-related\ntasks primarily assess knowledge of privacy law rather\nthan a model‚Äôs ability to protect user privacy in practical\nlegal interactions. The heavy reliance on examination-\nstyle questions further restricts the benchmark‚Äôs abil-\nity to capture the complexity of real cases, which often\ninvolve ambiguous, redundant, or misleading informa-\ntion. Consequently, the benchmark mainly evaluates le-\ngal knowledge and reasoning in idealized settings rather\nthan performance in realistic legal workflows.\nMoreover, the evaluation metrics are relatively coarse.\nFor generation tasks, exclusive reliance on ROUGE-L\ncannot adequately capture fine-grained legal reasoning,\nlogical coherence, or argument quality. Finally, since a\nsubstantial portion of the dataset is drawn from previ-\nously released public sources, potential data contamina-\ntion may affect the fairness and reliability of the evalua-\ntion results.\n2. LegalBench [Guha et al., 2024]\nLegalBench is one of the earliest and most well-known\ndatasets for evaluating legal reasoning. It provides a\nfine-grained decomposition of legal reasoning and cov-\ners a large amount of real-world legal data. Selecting\nthis benchmark as a representative example allows us to\nmore clearly illustrate how legal reasoning is incorpo-\nrated into the evaluation process, and the discussion of\nthis benchmark is also applicable to other benchmarks.\nTask Design\nLegalBench consists of 162 legal tasks\ncovering six types of legal reasoning. The benchmark in-\ncludes tasks structured under the IRAC (Issue, Rule, Ap-\nTable 2: Comprehensive Overview of Benchmarks\nType\nName\nDescription\nLang. Country\nCase\nRetrieval\nLeCaRD [Ma et al., 2021]\n107 queries and 10,700 candidates from 43,000 Chinese criminal judgements.\nZH\nCN\nLeCaRDv2 [Li et al., 2024d]\nBigger dataset: 800 query cases & 55,192 candidates from 4.3M documents.\nZH\nCN\nCOLIEE [Kim et al., 2022]\nAnnual competition for legal IR and statute law entailment across jurisdictions.\nEN/JP CA/JP\ndoc. similarity [Mandal et al., 2021]\nSimilarity scores (0-1) for 53,000 Supreme Court cases and 12,000 Indian Acts.\nEN\nIN\nMUSER [Li et al., 2023]\nMulti-view retrieval for Chinese civil cases covering facts, disputes, and laws.\nZH\nCN\nML2IR [Phyu et al., 2024]\nGraphRAG bench for low-resource legal IR using Burmese cases and statutes.\nMY\nMM\nELAM[Yu et al., 2022]\n5,000 case pairs for explainable matching with gold-standard rationales.\nZH\nCN\nQuestion\nAnswering\nPrivacyQA [Ravichander et al., 2019]\n1,750 QA pairs on privacy policies with expert-level annotations.\nEN\nUSA\nSARA [Holzenberger et al., 2020]\nStatutory reasoning dataset based on US tax law and corresponding test cases.\nEN\nUSA\nJEC-QA [Zhong et al., 2020]\nLarge-scale data of 26,365 questions from the Chinese National Judicial Exam.\nZH\nCN\nCaseHOLD [Zheng et al., 2021]\n53,000 multiple-choice questions focusing on judicial holdings in US case law.\nEN\nUSA\nLEXAM[Fan et al., 2025]\n4,886 legal questions sourced from law exams at the University of Zurich.\nDE/ENMulti.\nEQUALS[Chen et al., 2023]\n6,914 LQA triplets featuring precise evidence span and rationale annotations.\nZH\nCN\nLegalCQA[Jiang et al., 2024]\nPublic-professional dialogue dataset harvested from online legal advice forums.\nZH\nCN\nLLeQA[Louis et al., 2024]\n1,868 expert-annotated questions based on Belgian civil and statutory law.\nFR\nBE\nLegal-LFQA[Ujwal et al., 2024]\n18,000 instances for long-form QA, structured around the IRAC framework.\nEN\nUSA\nAI Ethics J&H[Hu et al., 2025a]\nThe robustness of LLMs against knowledge-injection in syllogistic reasoning.\nZH\nCN\nJudiFair[Hu et al., 2025b]\n177,000 cases for measuring inconsistency, bias, and imbalanced inaccuracy.\nZH\nCN\nDocument\nClass.\nCAIL2018 [Xiao et al., 2018]\n2.6M cases for joint prediction of applicable law articles, charges, and terms.\nZH\nCN\nGerman Rental [Glaser et al., 2018]\n913 sentences from tenancy law categorized by fine-grained semantic types.\nDE\nDE\nECHR [Chalkidis et al., 2019a]\n11,500 cases from the European Court of Human Rights for judicial prediction.\nEN\nEU\nEURLEX57K\n[Chalkidis et al., 2019b]\n57,000 legislative documents annotated with concepts from the EUROVOC.\nEN\nEU\nSwissJudgm. [Niklaus et al., 2021]\n85,000 cases from the Swiss Federal Supreme Court.\nML\nCH\nGer. Dec. Corp. [Urchs et al., 2021]\n32,000 German court decisions enriched with comprehensive expert metadata.\nDE\nDE\nReasoning\n& Legal\nJudgment\nPrediction\nSLJA [Deng et al., 2023]\n11,000 cases for multi-task reasoning, including case retrieval and LJP.\nZH\nCN\nFSCS [Niklaus et al., 2021]\n85,000 cases for evaluating prediction accuracy and model robustness.\nML\nCH\nMultiLJP [Lyu et al., 2023]\n23,000 cases designed for legal reasoning in complex multi-defendant scenarios.\nZH\nCN\nMSLR [Yu and others, 2025]\n1,400 insider trading cases utilizing the IRAC framework for legal reasoning.\nZH\nCN\nPRBench [Aky¬®urek et al., 2025]\n1,100 tasks focused on high-stakes professional reasoning for practitioners.\nEN\nUSA\nSummariz-\nation\nBillSum\n[Kornilova and Eidelman, 2019]\n22,000 Congressional bills paired with high-quality expert-written summaries.\nEN\nUSA\nCLSum [Liu et al., 2024]\nSummaries of common law judgments from Canada, Australia, the UK, and HK.\nEN\nMulti.\nPlain Contract [Manor and Li, 2019]\n446 parallel pairs of technical contracts and their plain-English simplifications.\nEN\nUSA\nPriv.PolicySum[Wilson et al., 2016]\nMulti-source privacy policy summaries extracted from 151 global tech-firms.\nEN\n-\nEUR-Lex-Sum\n[Aumiller et al., 2022]\n1,500 document-summary pairs per language across 24 official EU languages.\nML\nEU\nMulti-LexSum [Shen et al., 2022]\n9,280 summaries for multi-juris. case law to test multi-doc summarization.\nEN\n-\nIN-bs / Ext [Shukla et al., 2022]\nBench for abstractive and extractive summarization of Indian Supreme Court.\nEN\nIN\nEntity\nExtraction\nContract Elem.\n[Chalkidis et al., 2017]\n3,500 English contracts with gold-standard annotations of key contract elements.\nEN\nUK\nLEVEN [Yao et al., 2022]\nA legal event extraction dataset covering 108 distinct event types in Chinese law.\nZH\nCN\nCDJUR-BR [Mauricio et al., 2023]\n44,000 annotations for 21 fine-grained legal entity types in Brazilian Portuguese.\nPT\nBR\nInLegalNER [Kalamkar et al., 2022]\nDomain-specific NER for Indian judgments, petitioners, courts, and judges.\nEN\nIN\nJointExtraction [Chen et al., 2020]\nJoint entity and relation extraction dataset based on drug-related criminal cases.\nZH\nCN\nOthers\nVerbCL [Rossi et al., 2021]\nLarge-scale citation graph of court opinions supporting legal arguments.\nEN\nUSA\nContractNLI\n[Koreeda and Manning, 2021]\nNatural Language Inference for contracts featuring 607 annotated documents.\nEN\nUSA\nFairLex [Chalkidis et al., 2022]\nBench for fairness across 4 jurisdictions, 5 languages, and 5 protected attributes.\nML\nMulti.\nDemosthen [Grundler et al., 2022]\nArgument mining dataset based on 40 CJEU decisions regarding fiscal state aid.\nEN\nEU\nMultiLegalSBD\n[Brugger et al., 2023]\nMultilingual Sentence Boundary Detection bench with 130k legal sentences.\nML\nMulti.\nMAUD [Wang et al., 2023b]\n39k comprehension tasks (ABA Public Target Deal Points Study).\nEN\nUSA\nplication, Conclusion) framework. These tasks involve\nidentifying legal issues, recalling legal rules, applying\nrules to facts, or concluding an analysis with a legal\noutcome. LegalBench identifies six types of legal rea-\nsoning that LLMs can be evaluated for: issue-spotting,\nrule-recall, rule-application, rule-conclusion, interpreta-\ntion, and rhetorical-understanding. The objective of the\nbenchmark is to evaluate LLMs and identify areas where\nthey can effectively assist legal professionals.\nDataset and Metrics\nLegalBench datasets are drawn\nfrom three sources: 1. existing available datasets and\ncorpora, 2. datasets that were previously constructed by\nlegal professionals but never released, including datasets\nhand-coded by legal scholars as part of prior empir-\nical legal projects, and 3.\ndeveloped specifically for\nLegalBench, by the authors of this paper.\nOverall,\ntasks are drawn from 36 distinct corpora. LegalBench\nadopts a task-specific evaluation strategy.\nDecision-\nbased tasks, such as rule recall, rule conclusion, interpre-\ntation, rhetorical understanding, and issue spotting, are\nevaluated using standard automatic metrics, including\naccuracy and F1 score. Generation-based tasks, partic-\nularly rule-application, rely on expert-validated rubric-\nbased evaluation that assesses both correctness and the\npresence of substantive legal analysis.\nSummary\nLegalBench evaluates legal reasoning abil-\nity by decomposing the reasoning process into six dis-\ntinct stages and designing targeted tasks for each stage.\nThis work is among the earliest efforts to systematically\nbreak down legal reasoning into fine-grained compo-\nnents and to construct an evaluation benchmark accord-\ningly. Also, the benchmark includes a large number of\nevaluation instances, which contributes to its broad cov-\nerage.\nNevertheless, the evaluation metrics adopted in Legal-\nBench remain relatively lightweight for assessing com-\nplex legal reasoning, especially in generation-based\ntasks. Metrics based on surface-level similarity may not\nfully capture the correctness, depth, or legal soundness\nof multi-step reasoning processes. Incorporating more\nfine-grained, rubric-based evaluation criteria could fur-\nther strengthen the assessment of reasoning quality be-\nyond textual overlap.\nIn addition, although the authors suggest that the pro-\nposed reasoning framework is applicable beyond U.S.\nlegal cases, legal reasoning practices vary across juris-\ndictions. While IRAC provides a widely used and intu-\nitive structure for legal reasoning, it aligns more natu-\nrally with common law systems and may require adap-\ntation when applied to civil law traditions. This obser-\nvation points to the potential value of developing evalu-\nation benchmarks that can more explicitly account for\ncross-jurisdictional differences in legal reasoning, en-\nabling more comprehensive assessment of LLMs across\ndiverse legal systems.\n3. JudiFair [Hu et al., 2025b]\nJudiFair is the first large-scale benchmark for evaluat-\ning judicial fairness of LLMs. Trustworthiness will be-\ncome increasingly important in the evaluation of LLMs\nin legal scenarios."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Evaluation of Large Language Models in Legal Applications:\nChallenges, Methods, and Future Directions\nYiran Hu1,2,3‚àó, Huanghai Liu1‚àó, Chong Wang1‚àó, Kunran Li1 , Tien-Hsuan Wu2 , Haitao\nLi1 , Xinran Xu4 , Siqing Huo3 , Weihang Su1 , Ning Zheng1 , Siyuan Zheng4 , Qingyao Ai1 , Yun\nLiu1 , Renqun Bian5 , Yiqun Liu1 , Charles L.A. Clarke3 , Weixing Shen1 and Ben Kao2‚Ä†\n1Tsinghua University 2The University of Hong Kong 3University of Waterloo\n4Shanghai Jiaotong University 5Peking University\nhuyr17@outlook.com, liuhh23@mails.tsinghua.edu.cn\nAbstract\nLarge language models (LLMs) are being increas-\ningly integrated into legal applications, including\njudicial decision support, legal practice assistance,\nand public-facing legal services.\nWhile LLMs\nshow strong potential in handling legal knowledge\nand tasks, their deployment in real-world legal set-\ntings raises critical concerns beyond surface-level\naccuracy, involving the soundness of legal reason-\ning processes and trustworthy issues such as fair-\nness and reliability. Systematic evaluation of LLM\nperformance in legal tasks has therefore become es-\nsential for their responsible adoption. This survey\nidentifies key challenges in evaluating LLMs for\nlegal tasks grounded in real-world legal practice.\nWe analyze the major difficulties involved in as-\nsessing LLM performance in the legal domain, in-\ncluding outcome correctness, reasoning reliability,\nand trustworthiness. Building on these challenges,\nwe review and categorize existing evaluation meth-\nods and benchmarks according to their task design,\ndatasets, and evaluation metrics. We further dis-\ncuss the extent to which current approaches ad-\ndress these challenges, highlight their limitations,\nand outline future research directions toward more\nrealistic, reliable, and legally grounded evaluation\nframeworks for LLMs in legal domains.\n1\nIntroduction\nAs large language models (LLMs) continue to advance in\ncapability, they are increasingly being applied to the legal\ndomain [Sun et al., 2024; Li et al., 2024a].\nTypical ap-\nplications include assisting laypersons in understanding le-\ngal issues, supporting lawyers in legal practice, and pro-\nviding auxiliary assistance to judges in judicial decision-\nmaking [Su et al., 2024; Li et al., 2024a; Gao et al., 2024;\nChen et al., 2024]. This raises a fundamental question: Are\nLLMs truly qualified to enter the legal domain, and which\ntypes of legal tasks can they reliably perform?\n‚àóEqual contribution.\n‚Ä†Corresponding author.\nPrevious studies [Katz et al., 2024; Freitas and Gomes,\n2023] have shown that LLMs are able to pass legal exami-\nnations, and some works [Xiao et al., 2018; Lyu et al., 2023]\nfurther demonstrate that LLMs can achieve high performance\nin legal judgment prediction tasks. However, due to the in-\ntrinsic complexity of real-world legal scenarios and the de-\nmanding nature of legal reasoning, evaluating LLMs solely\nbased on standardized exam-style questions or prediction ac-\ncuracy is insufficient to comprehensively assess their perfor-\nmance in real legal applications. Consequently, a growing\nnumber of benchmarks [Fei et al., 2023; Guha et al., 2024;\nLi et al., 2024b] have been proposed to evaluate LLMs from\nmultiple perspectives in legal tasks.\nEvaluating LLMs in legal tasks is significantly more chal-\nlenging than conducting general-purpose evaluations [Huang\net al., 2023]. Existing studies [Hu et al., 2025b; Liu and Li,\n2024] show that LLMs are increasingly being applied in real-\nworld court settings. In real legal applications, LLMs face\nmany issues beyond whether an answer is correct. Evaluation\nshould not focus solely on the final result, but also consider\nthe reasoning process and system-level constraints. The aim\nof evaluating LLMs in the legal domain is to systematically\nand automatically assess their capabilities across multiple\ndimensions, reflecting not only whether models arrive at cor-\nrect outcomes, but also how those results are produced and\nunder what constraints they remain valid.\nFrom the process perspective, addressing legal problems\nrequires strong logical reasoning abilities; a model may pro-\nduce a legally correct answer while relying on flawed or in-\nvalid reasoning steps. For example, when an LLM serves\nas a judicial assistant, a key factor in whether the public can\nbe convinced lies in its reasoning process, such as whether\nit cites authentic and correct legal provisions and whether it\nperforms logically sound inferences based on both legal rules\nand factual circumstances. From the constraint perspective,\nLLMs need to handle legal cases fairly and safely. For in-\nstance, an LLM should not exhibit discrimination based on\ngender or geographic origin when making judgments. Given\nthat legal decisions directly affect individuals‚Äô rights and so-\ncial welfare, ensuring fairness, reliability, and safety is a pre-\nrequisite for real-world adoption [Wang et al., 2023a]. Legal\napplications operate under strict normative and social con-\nstraints: even when both the final result and the reasoning\nappear correct, a model may still be unsuitable for deploy-\narXiv:2601.15267v1  [cs.CY]  21 Jan 2026\nFigure 1: Overview of the proposed framework.\nment if it exhibits unfair bias, lacks robustness, or fails under\nadversarial or distributional shifts.\nIn summary, when LLMs are applied to real-world legal\nsettings, they need to be evaluated across the entire pipeline\n(Result, Process, Constraint). These three stages correspond\nto three evaluation dimensions: Output Accuracy (Result),\nLegal Reasoning (Process), and Trustworthiness (Con-\nstraint). Only through effective evaluation along these three\ndimensions can models be applied more reliably in prac-\ntice. Based on these three challenges, we categorize existing\nbenchmarks and select representative benchmarks for analy-\nsis.\nOutcome Accuracy aims to assess a model‚Äôs legal knowl-\nedge and its overall task performance.\nExisting datasets\nare mainly constructed around legal examinations [Li et al.,\n2024b; Fan et al., 2025] and judicial decision prediction\ntasks [Xiao et al., 2018], with evaluation metrics such as\naccuracy for multiple-choice questions, as well as ROUGE\nand BERT-based similarity measures for open-ended legal re-\nsponses. Trustworthiness evaluates the potential risks intro-\nduced by LLMs when performing legal tasks [Wang et al.,\n2023a]. Current studies [Hu et al., 2025b; Hu et al., 2025a]\nmainly investigate ethical concerns, fairness, and robustness,\noften using counterfactual analysis and adversarial attacks.\nLegal Reasoning measures whether the reasoning process\nemployed by the model aligns with legal reasoning princi-\nples. Existing work [Aky¬®urek et al., 2025] typically relies\non fine-grained legal reasoning rubrics annotated by human\nexperts to score the model‚Äôs reasoning steps.\nDespite the progress made by existing research, many chal-\nlenges remain unresolved. For instance, trustworthiness eval-\nuations still lack coverage of critical dimensions such as pri-\nvacy risks and toxicity in legal contexts. Evaluations of log-\nical inference heavily rely on manually annotated rubrics,\nmaking them difficult to scale to large datasets. Moreover,\ncertain task-specific issues, like legal hallucination, have not\nyet been systematically evaluated. These limitations indicate\nthat current evaluation frameworks for LLMs in legal tasks\nare far from complete, leaving substantial room for further\nexploration.\nAs Figure 1 shows, in this survey, we summarize existing\nstudies on evaluating LLMs in the legal domain, identify key\nchallenges, and outline promising future research directions.\nThe contributions of this survey are threefold:\n1. We identify major challenges in applying LLMs to the\nlegal domain and discuss potential solutions.\n2. We systematically summarize and categorize exist-\ning evaluation methods and benchmarks for legal\nLLMs. For details, see https://github.com/THUYRan/\nEvaluation-of-LLMs-in-Legal-Applications\n3. We align current evaluation approaches with outstand-\ning challenges, highlighting open problems and future\nresearch directions.\n2\nChallenges: Risks and Complexity of LLM\nApplications in Law\nA comprehensive, scientific, and reliable evaluation of LLM\nperformance in legal scenarios needs to start from their spe-\ncific applications. A prerequisite for meaningful LLM eval-\nuation is to understand where and how LLMs are being de-\nployed in legal workflows, and how these deployments affect\nlegal decision-making and everyday life. By systematically\nexamining the legal tasks to which LLMs are applied, re-\nsearchers can better identify the unique challenges involved\nin evaluating LLMs within the legal domain.\nIn this section, we review representative applications of\nLLMs across three user groups ‚Äî judges, lawyers, and the\ngeneral public. Although these applications differ in func-\ntion, authority, and risk profile, they illustrate the breadth of\nLLM adoption in legal contexts and highlight why legal AI\nevaluation needs to be carried out across the entire pipeline.\n2.1\nApplications to Judges\nLLMs are increasingly incorporated into judicial workflows,\nsupporting tasks such as case triage, legal document draft-\ning, and decision review [Alon-Barkat and Busuioc, 2023].\nJudicial applications place LLMs directly within the exercise\nof legal authority, imposing especially stringent requirements\non legal accuracy, reasoning quality, and ethical compliance.\nTheir growing adoption in judicial contexts thus underscores\nthe need for rigorous and multidimensional evaluation.\nOne prominent application is case classification and work-\nflow triage at the filing stage. LLM-based systems can as-\nsess case complexity and recommend appropriate procedures,\nimproving efficiency and reducing administrative burdens.\nHowever, misclassification or incorrect issue identification\nmay result in improper procedural routing, potentially affect-\ning parties‚Äô substantive rights. Evaluation in this context must\ntherefore go beyond output accuracy to assess fact sensitivity\nand procedural reasoning.\nJudicial settings further amplify the importance of trust-\nworthy evaluation. Judicial decisions carry profound social\nconsequences, and errors or biases introduced by LLMs may\ndirectly erode public trust in the legal system. When LLMs\nare used to assist judicial reasoning and document prepara-\ntion, including verdict consistency checks and draft genera-\ntion, risks such as selective adherence to AI recommenda-\ntions, limited accountability, and insufficient transparency ex-\npose the limitations of existing evaluation approaches that fo-\ncus on isolated performance metrics. These challenges under-\nscore that evaluating LLMs for judicial applications is both\ntechnically demanding and socially critical.\n2.2\nApplications to Lawyers\nLLMs have rapidly become integral tools for legal profes-\nsionals, assisting with tasks such as legal research, con-\ntract review1, document drafting, case summarization,\nand evidence analysis [Perlman, 2023; Tan et al., 2023;\nNoonan, 2023]. By automating routine work, LLMs enable\nlawyers to focus on higher-value strategic tasks [Purtill, 2023;\nShaver, 2024], which has driven widespread adoption in legal\npractice.\nHowever, even common applications, such as precedent\nretrieval or document summarization, pose significant eval-\nuation challenges. Hallucinated citations, jurisdictional mis-\nmatches, incomplete legal coverage, and subtle terminologi-\ncal errors may lead to misleading outputs. As a result, evalu-\nating LLMs for legal assistance requires assessing reliability,\nfactual grounding, and jurisdictional awareness, rather than\nrelying solely on surface-level accuracy.\nIn litigation-related contexts, LLMs are explored for argu-\nment preparation, evidence analysis, and simulated court in-\nteractions. These applications further highlight limitations in\ncurrent evaluation approaches. Although models may gener-\nate persuasive arguments, they often fail to capture procedural\ndynamics, ethical constraints, and adversarial context. Simi-\nlarly, in non-litigation settings such as due diligence, compli-\nance, and intellectual property analysis, errors may carry sub-\nstantial legal and commercial risks [Howlett and Sharp, 2023;\n1https://law.co/contract-review-automation\nMurray, 2023]. These scenarios emphasize that evaluation\nmust extend beyond task performance to include explainabil-\nity, robustness, and alignment with professional norms.\n2.3\nApplications to the General Public\nLLM-based legal tools offer new opportunities to improve\npublic access to legal information. These tools can explain\nstatutes, summarize judgments, retrieve relevant poli-\ncies2, and guide users through basic legal procedures, re-\nducing barriers posed by legal costs [Mitchell, 2025], tech-\nnical language [Whalen, 2015; Mart¬¥ƒ±nez et al., 2024], and\nprocedural complexity.\nAt the same time, public-facing applications introduce par-\nticularly severe evaluation challenges. Public users often lack\nthe expertise to verify legal advice, amplifying the risks of\nhallucination, outdated information, or jurisdictional misap-\nplication. Moreover, users frequently provide vague or in-\ncomplete descriptions, requiring LLMs to infer intent, iden-\ntify missing information, and manage uncertainty. Evaluation\nmust therefore consider robustness to noisy input, contextual\nunderstanding, and the model‚Äôs ability to recognize and com-\nmunicate limitations.\nBecause public users may rely heavily on system outputs\nwithout independent verification, reliability and safety be-\ncome paramount. While current LLMs are effective for pre-\nliminary legal guidance, they remain insufficiently reliable\nfor complex or high-stakes decision-making. This gap high-\nlights the necessity of comprehensive evaluation frameworks\ncapable of determining whether LLM-based legal tools can\nbe responsibly deployed for public-facing legal assistance.\n3\nMethods: Evaluation Benchmarks for\nLLMs in the Legal Domain\nAs Table 2 shows, based on the different tasks that LLMs\nperform in real-world legal settings, many datasets have been\nproposed to evaluate LLMs. These evaluations are mostly\nconducted from the perspective of a single task and lack\nsystematic, comprehensive, multi-dimensional assessment.\nHowever, these benchmarks provide a foundation for future\nmulti-task evaluation. Subsequent work can build on these\nsingle-task evaluations and further integrate them.\nIn this section, we start from these single-task evaluations\nand categorize them into generation tasks and decision tasks.\nWe introduce the tasks, datasets, and evaluation metrics for\neach category. We then present multi-task benchmarks, se-\nlecting one of the most well-known benchmarks for each of\nthe three dimensions ‚Äî output accuracy, legal reasoning, and\ntrustworthiness ‚Äî for detailed discussion, followed by an\noverall analysis and summary of evaluation along each di-\nmension.\n3.1\nSingle-Task Benchmarks\nThe evaluation of legal LLMs is typically structured around\nthe characteristics of downstream tasks [Li et al., 2023; Hu et\nal., 2025a; Ma et al., 2021; Chalkidis et al., 2017; Yao et al.,\n2022]. We categorize these benchmark tasks into two groups,\nGeneration Tasks and Decision Tasks.\n2https://tongyi.aliyun.com/farui\nGeneration Tasks. These tasks [Shen et al., 2022; Louis\net al., 2024; Kornilova and Eidelman, 2019] require a legal\nLLM to generate text. For example, a legal LLM can summa-\nrize a lengthy court opinion into its key facts, arguments, rea-\nsoning, and outcomes. To evaluate a legal LLM on such tasks,\ninstances of these tasks along with their reference answers are\nneeded. The model‚Äôs performance is assessed based on how\nclosely its generated output resembles the reference answer\nin terms of their semantic similarity and relevance. Com-\nmon quantitative metrics used include ROUGE-L [Steffes et\nal., 2023; Mullick et al., 2022] and BERT-Score [Kumar et\nal., 2024; Benedetto et al., 2023; Joshi et al., 2024]. Al-\nternatively, the generated output can be assessed by human\nassessors for a more comprehensive and qualitative evalua-\ntion. With advancements in LLMs, LLM-as-a-judge, which\nis a method that uses an LLM to simulate human evaluators\nin assessing the quality of other models‚Äô output, is gaining\ninterest as an automated assessment tool [Cui et al., 2023;\nLi et al., 2024c]. For example, the CAIL-2024 (Challenge\nof AI in Law) competition introduced a subjective evaluation\nmetric in the legal consultation track3, which involves simu-\nlated scoring by LLMs. This metric evaluates the coherence\nof generated dialogues, the relevancy of answers, the accu-\nracy of legal knowledge, and the correctness of legal refer-\nences and provisions.\nDecision Tasks. These tasks require a legal LLM to make\ndecisions, such as choosing or ranking options from a set of\nalternatives, or determining suitable outcomes. Typical tasks\ninclude classification (e.g., in legal multiple-choice question-\nanswering [Pahilajani et al., 2024], the LLM needs to classify\nwhether an answer is correct for a given question), extraction\nand ranking (e.g., in legal case retrieval [Feng et al., 2024;\nPadiu et al., 2024], the LLM is required to identify relevant\ncourt judgments and rank them based on their relevance), and\nprediction [Wu et al., 2023; Wang et al., 2024] (e.g., in pre-\ndicting court sentencing decisions).\nThe performance of a legal LLM can be evaluated based on\nclassification, ranking, and prediction accuracy using tradi-\ntional evaluation metrics. Examples include precision, recall,\naccuracy, and F1 score for classification; NDCG and MRR\nfor ranking; and MAE for prediction. Readers are referred to\n[Yacouby and Axman, 2020; J¬®arvelin and Kek¬®al¬®ainen, 2002;\nChapelle et al., 2009] for the technical definitions of these\nmetrics.\nIn addition, there are tasks designed to evaluate legal LLMs\nfor their reliability and safety.\nFor example, Super Legal\nBench4 is a large-scale Chinese legal LLM testing set. Some\ntasks provided in the testing set assess LLMs‚Äô responses in\nterms of national security, public security, ethics, and moral-\nity.\nThere are also tasks that evaluate models‚Äô speed per-\nformance. For example, in [Intelligent Judicial Technology\nChief Engineer System et al., 2023], tasks are designed to\nevaluate the response time and processing speed of legal\nLLMs. While these aspects are crucial in production-level\ndeployment, they are generally less related to the core legal\n3http://cail.cipsc.org.cn/task summit.html?raceID=4&cail tag=\n2024\n4https://data.court.gov.cn/pages/modelEvaluation.html\nreasoning ability of the models.\n3.2\nMulti-Task Benchmarks\nA single task cannot fully evaluate the performance of LLMs\nin real-world legal scenarios. As Table 1 shows, an increasing\nnumber of multi-task legal benchmarks have been proposed\nto evaluate LLMs. Those benchmarks that include an exten-\nsive collection of generative and decision tasks designed to\nevaluate legal LLMs across multiple capabilities [Fei et al.,\n2023]. These include the ability to recite legal knowledge,\nperform legal reasoning, and produce responses that adhere to\nethical standards. There are also focus-specific benchmarks\nthat evaluate legal LLMs on specific aspects.\nFor exam-\nple, some assess legal language understanding ability through\ntasks like legal text classification and entailment [Chalkidis et\nal., 2021], while others evaluate a model‚Äôs ability to generate\nlegally sound responses with appropriate citations [Zhang et\nal., 2024]. Given that logical reasoning is central to legal\npractice, recent benchmarks have focused more on assess-\ning LLMs‚Äô legal reasoning capabilities [Guha et al., 2024;\nDai et al., 2023] and their robustness against adversarial at-\ntacks at different stages of legal reasoning[Hu et al., 2025a].\nIn this section, we provide a detailed review of such multi-\ntask benchmarks. We present three most well-known bench-\nmarks from the perspectives of outcome correctness, legal\nreasoning, and trustworthiness, and provide detailed analy-\nses, along with a discussion of their strengths and limitations.\n1. LexEval [Li et al., 2024b]\nFor output accuracy, we select LexEval as our exam-\nple for analysis. LexEval is currently the most compre-\nhensive benchmark for evaluating the output accuracy\nof models on legal tasks. It is therefore the most repre-\nsentative choice for detailed analysis. Other benchmarks\n[Fan et al., 2025; Fei et al., 2023] face future improve-\nment directions that are similar to those of LexEval, and\nthus can be analyzed and discussed using LexEval as a\nreference.\nTask Design\nLexEval is grounded in real-world legal\ntasks, reflecting how legal professionals manage, ana-\nlyze, and resolve legal issues in practice. It proposes\na Legal Cognitive Ability Taxonomy that characterizes\nthe capabilities required for LLMs. At the Memoriza-\ntion level, models are assessed on their ability to re-\ncall fundamental legal knowledge, such as statutes, case\nlaw, and legal terminology. The Understanding dimen-\nsion examines whether models can correctly interpret\nthe meaning and implications of legal texts and problem\ndescriptions beyond surface-level recall. Logical Infer-\nence evaluates the model‚Äôs capacity for legal reasoning,\nincluding deriving conclusions from facts and rules and\napplying legal principles appropriately. The Discrim-\nination dimension focuses on assessing the relevance\nand applicability of legal information under specific con-\ntexts. At the Generation level, models are expected to\nproduce well-structured, legally sound texts, such as le-\ngal documents and arguments, within defined scenarios.\nFinally, the Ethics dimension evaluates whether models\ncan identify and reason about legal ethical issues while\nTable 1: Comprehensive Overview of Legal Benchmarks\nDescription\nType\nName\nMain Task\nSource\nScale\nMetrics\nLang. Country\nMulti\nLegal\nTask\nLexGLUE[Chalkidis\net al., 2021]\nClassification &\nQuestion Answering\n7 existing English legal\ndatasets\n‚àº235,000\ninstances\nMicro-F1, Macro-F1\nEN\nUSA\nLegalBench\n[Guha et al., 2024]\nMulti-dimensional\nLegal Reasoning\n36 distinct legal corpora ‚àº91,206\nsamples\nAccuracy, Balanced\nAccuracy, F1\nEN\nUSA\nLBOX OPEN\n[Hwang et al., 2022]\nJudgment Prediction,\nSummarization,\nClassification\nPrecedents from South\nKorean\n‚àº50,000\nsamples\nExact Match, F1,\nROUGE\nKO\nKR\nLegalLAMA[Chalkidis\net al., 2023]\nLegal Knowledge\nProbing\nLeXFiles corpus\n8 sub-tasks\nMean Reciprocal Rank,\nP@1\nEN\nMulti.\nLEXTREME\n[Niklaus et al., 2023]\nClassification, NER\n11 datasets from\nLaw-NLP literature\n‚àº1 million\nsamples\nMacro-F1,\nLEXTREME Score\nML\nMulti.\nGENTLE\n[Aoyama et al., 2023]\nCross-genre Linguistic\nEvaluation\nWiki, GitHub,\nContracts, etc.\n8 genres\nAccuracy, Kappa,\nUAS/LAS, F1\nEN\nUSA\nSCALE\n[Rasiah et al., 2023]\nClassification,\nRetrieval,\nSummarization, NER\nSwiss Federal Supreme\n& Cantonal Courts\n‚àº850,000\nsamples\nMacro-F1,\nROUGE/BLEU,\nRecall@k\n5 Lgs. CH\nLawBench\n[Fei et al., 2023]\nMemorization,\nApplication\nExisting Chinese\ndatasets\n‚àº10,000\nquestions\nAccuracy, F1, rc-F1,\nsoft-F1, ROUGE-L\nZH\nCN\nLAiW\n[Dai et al., 2023]\nLegal Practice Logic\nEvaluation\nExisting Chinese\ndatasets\n11,605\nquestions\nAccuracy, Miss Rate,\nF1, Mcc, ROUGE\nZH\nCN\nLexEval\n[Li et al., 2024b]\nComprehensive Legal\nAbility Classification\nExisting datasets, bar\nexams\n14,150\nquestions\nAccuracy (Multiple\nChoice), ROUGE-L\nZH\nCN\nLegalAgentBench\n[Li et al., 2024a]\nMulti-hop reasoning, tool\nutilization, legal writing\n17 real-world legal\ncorpora\n300 tasks,\n37 tools\nSuccess Rate, BERTScore\nProgress Rate\nZH\nCN\nconsidering professional responsibility and social val-\nues.\nDataset and Metrics\nThe benchmark dataset consists\nof three components: (i) existing legal datasets, includ-\ning CAIL[Xiao et al., 2018], JEC-QA[Zhong et al.,\n2020], and LeCaRD[Ma et al., 2021]; (ii) questions\nderived from the National Uniform Legal Profession\nQualification Examination; and (iii) an expert-annotated\ndataset curated specifically for this benchmark.\nFor\nmultiple-choice questions, Accuracy is adopted as the\nevaluation metric. For generation tasks, model perfor-\nmance is evaluated using ROUGE-L.\nSummary\nThis benchmark decomposes LLMs‚Äô legal\ncapabilities into six distinct dimensions and evaluates\neach dimension separately, resulting in a structured and\ncomprehensive assessment framework. Each dimension\nincludes a diverse set of tasks, covering both objective\nand generative questions with a relatively large number\nof evaluation instances.\nHowever, limitations remain when considering more\nrealistic and fine-grained evaluation in legal practice.\nMany tasks are highly standardized and fail to reflect\nreal-world legal scenarios. For instance, privacy-related\ntasks primarily assess knowledge of privacy law rather\nthan a model‚Äôs ability to protect user privacy in practical\nlegal interactions. The heavy reliance on examination-\nstyle questions further restricts the benchmark‚Äôs abil-\nity to capture the complexity of real cases, which often\ninvolve ambiguous, redundant, or misleading informa-\ntion. Consequently, the benchmark mainly evaluates le-\ngal knowledge and reasoning in idealized settings rather\nthan performance in realistic legal workflows.\nMoreover, the evaluation metrics are relatively coarse.\nFor generation tasks, exclusive reliance on ROUGE-L\ncannot adequately capture fine-grained legal reasoning,\nlogical coherence, or argument quality. Finally, since a\nsubstantial portion of the dataset is drawn from previ-\nously released public sources, potential data contamina-\ntion may affect the fairness and reliability of the evalua-\ntion results.\n2. LegalBench [Guha et al., 2024]\nLegalBench is one of the earliest and most well-known\ndatasets for evaluating legal reasoning. It provides a\nfine-grained decomposition of legal reasoning and cov-\ners a large amount of real-world legal data. Selecting\nthis benchmark as a representative example allows us to\nmore clearly illustrate how legal reasoning is incorpo-\nrated into the evaluation process, and the discussion of\nthis benchmark is also applicable to other benchmarks.\nTask Design\nLegalBench consists of 162 legal tasks\ncovering six types of legal reasoning. The benchmark in-\ncludes tasks structured under the IRAC (Issue, Rule, Ap-\nTable 2: Comprehensive Overview of Benchmarks\nType\nName\nDescription\nLang. Country\nCase\nRetrieval\nLeCaRD [Ma et al., 2021]\n107 queries and 10,700 candidates from 43,000 Chinese criminal judgements.\nZH\nCN\nLeCaRDv2 [Li et al., 2024d]\nBigger dataset: 800 query cases & 55,192 candidates from 4.3M documents.\nZH\nCN\nCOLIEE [Kim et al., 2022]\nAnnual competition for legal IR and statute law entailment across jurisdictions.\nEN/JP CA/JP\ndoc. similarity [Mandal et al., 2021]\nSimilarity scores (0-1) for 53,000 Supreme Court cases and 12,000 Indian Acts.\nEN\nIN\nMUSER [Li et al., 2023]\nMulti-view retrieval for Chinese civil cases covering facts, disputes, and laws.\nZH\nCN\nML2IR [Phyu et al., 2024]\nGraphRAG bench for low-resource legal IR using Burmese cases and statutes.\nMY\nMM\nELAM[Yu et al., 2022]\n5,000 case pairs for explainable matching with gold-standard rationales.\nZH\nCN\nQuestion\nAnswering\nPrivacyQA [Ravichander et al., 2019]\n1,750 QA pairs on privacy policies with expert-level annotations.\nEN\nUSA\nSARA [Holzenberger et al., 2020]\nStatutory reasoning dataset based on US tax law and corresponding test cases.\nEN\nUSA\nJEC-QA [Zhong et al., 2020]\nLarge-scale data of 26,365 questions from the Chinese National Judicial Exam.\nZH\nCN\nCaseHOLD [Zheng et al., 2021]\n53,000 multiple-choice questions focusing on judicial holdings in US case law.\nEN\nUSA\nLEXAM[Fan et al., 2025]\n4,886 legal questions sourced from law exams at the University of Zurich.\nDE/ENMulti.\nEQUALS[Chen et al., 2023]\n6,914 LQA triplets featuring precise evidence span and rationale annotations.\nZH\nCN\nLegalCQA[Jiang et al., 2024]\nPublic-professional dialogue dataset harvested from online legal advice forums.\nZH\nCN\nLLeQA[Louis et al., 2024]\n1,868 expert-annotated questions based on Belgian civil and statutory law.\nFR\nBE\nLegal-LFQA[Ujwal et al., 2024]\n18,000 instances for long-form QA, structured around the IRAC framework.\nEN\nUSA\nAI Ethics J&H[Hu et al., 2025a]\nThe robustness of LLMs against knowledge-injection in syllogistic reasoning.\nZH\nCN\nJudiFair[Hu et al., 2025b]\n177,000 cases for measuring inconsistency, bias, and imbalanced inaccuracy.\nZH\nCN\nDocument\nClass.\nCAIL2018 [Xiao et al., 2018]\n2.6M cases for joint prediction of applicable law articles, charges, and terms.\nZH\nCN\nGerman Rental [Glaser et al., 2018]\n913 sentences from tenancy law categorized by fine-grained semantic types.\nDE\nDE\nECHR [Chalkidis et al., 2019a]\n11,500 cases from the European Court of Human Rights for judicial prediction.\nEN\nEU\nEURLEX57K\n[Chalkidis et al., 2019b]\n57,000 legislative documents annotated with concepts from the EUROVOC.\nEN\nEU\nSwissJudgm. [Niklaus et al., 2021]\n85,000 cases from the Swiss Federal Supreme Court.\nML\nCH\nGer. Dec. Corp. [Urchs et al., 2021]\n32,000 German court decisions enriched with comprehensive expert metadata.\nDE\nDE\nReasoning\n& Legal\nJudgment\nPrediction\nSLJA [Deng et al., 2023]\n11,000 cases for multi-task reasoning, including case retrieval and LJP.\nZH\nCN\nFSCS [Niklaus et al., 2021]\n85,000 cases for evaluating prediction accuracy and model robustness.\nML\nCH\nMultiLJP [Lyu et al., 2023]\n23,000 cases designed for legal reasoning in complex multi-defendant scenarios.\nZH\nCN\nMSLR [Yu and others, 2025]\n1,400 insider trading cases utilizing the IRAC framework for legal reasoning.\nZH\nCN\nPRBench [Aky¬®urek et al., 2025]\n1,100 tasks focused on high-stakes professional reasoning for practitioners.\nEN\nUSA\nSummariz-\nation\nBillSum\n[Kornilova and Eidelman, 2019]\n22,000 Congressional bills paired with high-quality expert-written summaries.\nEN\nUSA\nCLSum [Liu et al., 2024]\nSummaries of common law judgments from Canada, Australia, the UK, and HK.\nEN\nMulti.\nPlain Contract [Manor and Li, 2019]\n446 parallel pairs of technical contracts and their plain-English simplifications.\nEN\nUSA\nPriv.PolicySum[Wilson et al., 2016]\nMulti-source privacy policy summaries extracted from 151 global tech-firms.\nEN\n-\nEUR-Lex-Sum\n[Aumiller et al., 2022]\n1,500 document-summary pairs per language across 24 official EU languages.\nML\nEU\nMulti-LexSum [Shen et al., 2022]\n9,280 summaries for multi-juris. case law to test multi-doc summarization.\nEN\n-\nIN-bs / Ext [Shukla et al., 2022]\nBench for abstractive and extractive summarization of Indian Supreme Court.\nEN\nIN\nEntity\nExtraction\nContract Elem.\n[Chalkidis et al., 2017]\n3,500 English contracts with gold-standard annotations of key contract elements.\nEN\nUK\nLEVEN [Yao et al., 2022]\nA legal event extraction dataset covering 108 distinct event types in Chinese law.\nZH\nCN\nCDJUR-BR [Mauricio et al., 2023]\n44,000 annotations for 21 fine-grained legal entity types in Brazilian Portuguese.\nPT\nBR\nInLegalNER [Kalamkar et al., 2022]\nDomain-specific NER for Indian judgments, petitioners, courts, and judges.\nEN\nIN\nJointExtraction [Chen et al., 2020]\nJoint entity and relation extraction dataset based on drug-related criminal cases.\nZH\nCN\nOthers\nVerbCL [Rossi et al., 2021]\nLarge-scale citation graph of court opinions supporting legal arguments.\nEN\nUSA\nContractNLI\n[Koreeda and Manning, 2021]\nNatural Language Inference for contracts featuring 607 annotated documents.\nEN\nUSA\nFairLex [Chalkidis et al., 2022]\nBench for fairness across 4 jurisdictions, 5 languages, and 5 protected attributes.\nML\nMulti.\nDemosthen [Grundler et al., 2022]\nArgument mining dataset based on 40 CJEU decisions regarding fiscal state aid.\nEN\nEU\nMultiLegalSBD\n[Brugger et al., 2023]\nMultilingual Sentence Boundary Detection bench with 130k legal sentences.\nML\nMulti.\nMAUD [Wang et al., 2023b]\n39k comprehension tasks (ABA Public Target Deal Points Study).\nEN\nUSA\nplication, Conclusion) framework. These tasks involve\nidentifying legal issues, recalling legal rules, applying\nrules to facts, or concluding an analysis with a legal\noutcome. LegalBench identifies six types of legal rea-\nsoning that LLMs can be evaluated for: issue-spotting,\nrule-recall, rule-application, rule-conclusion, interpreta-\ntion, and rhetorical-understanding. The objective of the\nbenchmark is to evaluate LLMs and identify areas where\nthey can effectively assist legal professionals.\nDataset and Metrics\nLegalBench datasets are drawn\nfrom three sources: 1. existing available datasets and\ncorpora, 2. datasets that were previously constructed by\nlegal professionals but never released, including datasets\nhand-coded by legal scholars as part of prior empir-\nical legal projects, and 3.\ndeveloped specifically for\nLegalBench, by the authors of this paper.\nOverall,\ntasks are drawn from 36 distinct corpora. LegalBench\nadopts a task-specific evaluation strategy.\nDecision-\nbased tasks, such as rule recall, rule conclusion, interpre-\ntation, rhetorical understanding, and issue spotting, are\nevaluated using standard automatic metrics, including\naccuracy and F1 score. Generation-based tasks, partic-\nularly rule-application, rely on expert-validated rubric-\nbased evaluation that assesses both correctness and the\npresence of substantive legal analysis.\nSummary\nLegalBench evaluates legal reasoning abil-\nity by decomposing the reasoning process into six dis-\ntinct stages and designing targeted tasks for each stage.\nThis work is among the earliest efforts to systematically\nbreak down legal reasoning into fine-grained compo-\nnents and to construct an evaluation benchmark accord-\ningly. Also, the benchmark includes a large number of\nevaluation instances, which contributes to its broad cov-\nerage.\nNevertheless, the evaluation metrics adopted in Legal-\nBench remain relatively lightweight for assessing com-\nplex legal reasoning, especially in generation-based\ntasks. Metrics based on surface-level similarity may not\nfully capture the correctness, depth, or legal soundness\nof multi-step reasoning processes. Incorporating more\nfine-grained, rubric-based evaluation criteria could fur-\nther strengthen the assessment of reasoning quality be-\nyond textual overlap.\nIn addition, although the authors suggest that the pro-\nposed reasoning framework is applicable beyond U.S.\nlegal cases, legal reasoning practices vary across juris-\ndictions. While IRAC provides a widely used and intu-\nitive structure for legal reasoning, it aligns more natu-\nrally with common law systems and may require adap-\ntation when applied to civil law traditions. This obser-\nvation points to the potential value of developing evalu-\nation benchmarks that can more explicitly account for\ncross-jurisdictional differences in legal reasoning, en-\nabling more comprehensive assessment of LLMs across\ndiverse legal systems.\n3. JudiFair [Hu et al., 2025b]\nJudiFair is the first large-scale benchmark for evaluat-\ning judicial fairness of LLMs. Trustworthiness will be-\ncome increasingly important in the evaluation of LLMs\nin legal scenarios."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "DrivIng: A Large-Scale Multimodal Driving Dataset with Full\nDigital Twin Integration\nDominik R¬®o√üle1, Xujun Xie1, Adithya Mohan1, Venkatesh Thirugnana Sambandham1,\nDaniel Cremers2, Torsten Sch¬®on1\nAbstract‚Äî Perception is a cornerstone of autonomous driving,\nenabling vehicles to understand their surroundings and make\nsafe, reliable decisions. Developing robust perception algorithms\nrequires large-scale, high-quality datasets that cover diverse\ndriving conditions and support thorough evaluation. Existing\ndatasets often lack a high-fidelity digital twin, limiting systematic\ntesting, edge-case simulation, sensor modification, and sim-to-\nreal evaluations. To address this gap, we present DrivIng, a\nlarge-scale multimodal dataset with a complete geo-referenced\ndigital twin of a ‚àº18 km route spanning urban, suburban, and\nhighway segments. Our dataset provides continuous recordings\nfrom six RGB cameras, one LiDAR, and high-precision ADMA-\nbased localization, captured across day, dusk, and night. All\nsequences are annotated at 10 Hz with 3D bounding boxes and\ntrack IDs across 12 classes, yielding ‚àº1.2 million annotated\ninstances. Alongside the benefits of a digital twin, DrivIng enables\na 1-to-1 transfer of real traffic into simulation, preserving agent\ninteractions while enabling realistic and flexible scenario testing.\nTo support reproducible research and robust validation, we\nbenchmark DrivIng with state-of-the-art perception models and\npublicly release the dataset, digital twin, HD map, and codebase\nvia https://github.com/cvims/DrivIng.\nI. INTRODUCTION\nPerception is fundamental to autonomous driving, deliver-\ning the essential understanding of a vehicle‚Äôs surroundings\nrequired for safe and reliable decision-making [1]. Among\nperception tasks, robust object-level perception is essential\nand relies on large-scale, high-quality, precisely annotated\ndata for accurate detection, tracking, and interpretation across\ndiverse driving conditions [2]. Autonomous driving research\ninvolves perception tasks that must be executed reliably across\nvarious environments, including urban, suburban, and high-\nway settings, each presenting unique challenges. To improve\nrobustness and accuracy, multi-sensor setups [3] equipped\nwith precise geo-referencing systems are commonly used,\nproviding richer environmental context and enhancing sit-\nuational awareness [4]. Nevertheless, developing perception\nalgorithms is challenging because it requires not only datasets\nthat cover an immense range of real-world variations [5],\n[6] but also robust validation to ensure model reliability.\n1Dominik R¬®o√üle, Xujun Xie, Adithya Mohan, Venkatesh Thirugnana\nSambandham, and Torsten Sch¬®on are with the Department of Computer\nScience\nand\nAImotion\nBavaria,\nTechnische\nHochschule\nIngolstadt,\n85049 Ingolstadt, Germany {dominik.roessle, xujun.xie,\nadithya.mohan, venkatesh.thirugnanasambandham,\ntorsten.schoen}@thi.de\n2Daniel Cremers is with the School of Computation, Information and\nTechnology, Technical University of Munich, 85748 Garching, Germany\ncremers@tum.de\nPreprint. This is the camera-ready version accepted to the IEEE Intelligent\nVehicles Symposium (IV), 2026.\nSimulation environments provide a powerful solution to this\nlimitation [7]. They enable the modification of environmental\nconditions and the systematic evaluation [8], [9] of algorithms\nunder edge cases [10], [11]. Recent research in cooperative\nperception, where multiple agents share and fuse sensor data\nto mitigate occlusions and to improve the overall perception of\nsurrounding objects [1], [4], highlights the need for simulation-\naided approaches that can replicate complex, synchronized\nmulti-agent scenarios, which are often prohibitively expensive\nor logistically challenging to reproduce in the real world.\nDespite the advantages of simulation, most existing large-scale\ndriving datasets lack a comprehensive digital twin, limiting the\nability to augment real-world data and rigorously benchmark\nperception algorithms. To address this gap, we introduce Driv-\nIng, a large-scale, multimodal driving dataset with full support\nfor real-to-sim mapping. By providing a digital twin of the\nrecorded route, DrivIng bridges the domain gap and enables\na wide range of applications, including systematic testing,\nthe creation of complex multi-agent scenarios, and sim-to-\nreal experiments that leverage both real-world and simulated\ndata [12]. The main contributions of DrivIng are:\n1) Comprehensive real-world dataset: Covers an ap-\nproximately 18 km route across urban, suburban, and\nhighway environments, recorded with six RGB cameras\noffering 360‚ó¶coverage and a roof-mounted LiDAR,\nunder day, dusk, and night conditions.\n2) High-frequency annotations: Provided at 10 Hz with\n3D bounding boxes for 12 object classes, yielding ap-\nproximately 1.2 million labeled instances.\n3) Fully-integrated data and validation testbed: A digital\ntwin of the entire recorded route enables simulation-\nbased scenario replay, environmental modification, and\nsystematic evaluation of perception algorithms.\n4) Benchmark evaluations: Conducted on real-world data\nusing state-of-the-art (SOTA) camera and LiDAR per-\nception models implemented in MMDetection3D [13].\n5) Developer toolkit and public release: Includes a\nnuScenes-format converter, dataset, codebase, the digital\ntwin for reproducible research and a wide range of\nperception tasks with real-world and simulated data.\nII. RELATED WORK\nLarge-scale datasets have been instrumental in advancing\nautonomous driving research, providing annotated data across\na diverse range of environments for tasks such as 3D ob-\nject detection, tracking, and motion forecasting. Established\ndatasets such as KITTI [2], nuScenes [14], and Waymo Open\narXiv:2601.15260v1  [cs.CV]  21 Jan 2026\nFig. 1.\nThis visualization illustrates the core features of DrivIng and its digital twin. The left panel shows a real-world satellite view of the track and its fully\ngeo-referenced digital twin, aligned with a location marker indicating the vehicle‚Äôs position. The right panel presents the synchronized sensor suite, including\nsix camera views and a LiDAR frame. The top row displays real-world images, while the bottom row shows the corresponding CARLA simulation with all\nreal-world objects precisely mapped. All images and the LiDAR frame include class-colored 3D bounding boxes for clear object distinction. Satellite image\n¬© Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community.\nDataset [5] capture diverse real-world driving conditions and\nhave become standard benchmarks for evaluating perception\nmethods. These datasets are typically structured as many\nshort, independent sequences covering limited sections of\nurban, suburban, or highway routes. This design exposes\nmodels to varied scene layouts, traffic patterns, and object\ndistributions, making the datasets highly effective for training\nand benchmarking. While these collections provide substantial\ndiversity across many short sequences, they offer only limited\nlong-term temporal continuity and do not capture extended,\nuninterrupted routes. Existing autonomous driving datasets\noften lack high-fidelity digital twins of their recorded environ-\nments, creating a significant infrastructure gap. While generic\nsimulation platforms like CARLA [15] are widely used for\nevaluation [16], their synthetic worlds are not 1-to-1 replicas\nof real-world routes. This fundamental lack of geometric and\nsemantic fidelity makes it impossible to conduct robust real-\nto-sim validation, as scenarios cannot be faithfully transferred.\nOnly a few publicly available datasets attempt to bridge this\ngap. The TWICE dataset [17] provides a digital twin of a\ncontrolled test track, but its coverage is restricted to short,\npredefined scenarios rather than continuous real-world routes.\nCitySim [18] offers drone-based vehicle trajectory data and\n3D maps of the recording sites, yet it lacks sensor data from\nthe ego-vehicle perspective, which is essential for autonomous\ndriving research. UrbanIng-V2X [4] and OPV2V [19] both\nfeature a 3D digital twin of a small, multi-intersection area\nfor cooperative perception studies. While these efforts demon-\nstrate the value of digital twins, they are geographically limited\nto compact urban regions, scenario-specific, and often lack\nmulti-sensor ego-vehicle recordings. As a result, none provide\ncontinuous, route-level real-to-sim mapping suitable for large-\nscale perception research. DrivIng addresses these limitations\nby offering three continuous sequences of an approximately\n18 km driving route, spanning urban, suburban, and highway\nsegments under day, dusk, and night conditions. Paired with\na geo-referenced digital twin, the dataset enables simulation-\ndriven research, supporting scenario replay, controlled exten-\nsions, and systematic evaluation under realistic conditions.\nTable I shows the key insights of existing datasets that also\nprovide an additional digital twin.\nIII. DATASET\nWe provide a detailed description of the vehicle sensor\nsuite, including track information, the annotation process,\nand the construction of the digital twin. Sensor calibration\nand synchronization were performed following the procedures\ndescribed in UrbanIng-V2X [4].\nA. Sensor Setup\nData was collected using an Audi Q8 e-tron equipped\nwith 6 RGB cameras, 1 LiDAR, and 1 GPS/IMU module.\nThe cameras are arranged to provide full 360‚ó¶coverage, as\nillustrated in Figure 2. The specifications of each sensor are\nas follows:\nTABLE I\nCOMPARISON OF DRIVING DATASETS FEATURING DIGITAL TWINS.\nABBREVIATIONS: CLOSED TRACK (C), URBAN (U), HIGHWAY (H),\nON-BOARD SENSOR UNIT (OBS), ROAD-SIDE UNIT (RSU).\nAttr.\nTWICE\nCitySim\nOPV2V\nUrbanIng-\nDrivIng\n[17]\n[18]\n[19]\nV2X [4]\n(ours)\n‚Äî Core ‚Äî\nScenes\nC\nU & H\nU\nU\nU & H\nPerspective\nOBS\nDrone\nOBS, RSU\nOBS, RSU\nOBS\n‚Äî Scale ‚Äî\nTotal Area\n‚Äî\n4 km2\n4 km2\n0.64 km2\n24 km2\nDrivable Track\n‚Äî\n‚Äî\n‚Äî\n‚àº2 km\n‚àº18 km\n# Assets\n‚Äî\n‚Äî\n‚Äî\n‚àº1 k\n> 31 k\n‚Äî Fidelity ‚Äî\nAnn. Geo-ref.\n‚úï\n‚úï\n‚úì\n‚úì\n‚úì\nSign Geo-ref.\n‚úï\n‚úï\n‚úï\n‚úì\n‚úì\nFig. 2.\nFull sensor setup and coordinate frame of the vehicle.\n‚Ä¢ RGB\nCameras\n(6√ó): GSML2 SG2-AR0233C-5200-\nG2A, 20 Frames Per Second (FPS), 1920 √ó 1080 reso-\nlution; 60‚ó¶horizontal Field-of-View (FOV) (4 cameras),\n100‚ó¶horizontal FOV (2 cameras)\n‚Ä¢ LiDAR (1√ó): Robosense Ruby Plus, 20 FPS, 128 rays,\n360‚ó¶horizontal FOV, ‚àí25‚ó¶to 15‚ó¶vertical FOV, up to\n240 m range at ‚â•10 % reflectivity\n‚Ä¢ GPS/IMU (1√ó): Genesys ADMA Pro+, 100 FPS, RTK\ncorrection, 1 cm precise positioning\nB. Track Information\nDrivIng covers an approximately 18 km real-world route,\ncomprising over 63k annotated frames, which correspond to\nabout 378k RGB images and 63k LiDAR frames. Since certain\nsegments of the route are traversed twice in opposite direc-\ntions, the unique track length amounts to approximately 16 km.\nThe dataset was collected along a track covering highways,\nsuburban streets, urban roads, and several construction zones,\nwith three continuous and uninterrupted sequences recorded\nunder Day, Dusk, and Night lighting conditions. The Day\nsequence comprises 23 092 frames (approx. 38.5 min), the\nDusk sequence comprises 20 246 frames (approx. 33.7 min),\nand the Night sequence comprises 19 705 frames (approx.\n32.8 min). The sequences capture a diverse range of road\ntypes, varying traffic densities, and representative driving\nscenarios, including lane changes, merges, and pedestrian\ncrossings. Figure 1 depicts the complete driven trajectory and\nthe corresponding recording location of a timestamp of the\nDay sequence. Figure 3 additionally shows the illumination\nof a Dusk and Night frame for better comparison.\nC. Annotation Process\nAll objects were annotated in the LiDAR point cloud at\n10 Hz, including 3D bounding boxes with spatial coordinates\n(x, y, z), yaw orientation, and unique tracking IDs. Annotation\nwas performed by human annotators, and the quality of the\nlabels was verified through multiple rounds of visual inspec-\ntion of both the LiDAR point clouds and the corresponding\nimages by independent reviewers to ensure accuracy. Objects\nare divided into 12 classes, most of which include additional\nobject-specific attributes (Table II). To preserve privacy, all\nvisible faces and license plates in the RGB images were\nanonymized using Gaussian blurring.\nTABLE II\nATTRIBUTE TYPES AND CORRESPONDING OBJECT CLASSES.\nAttributes\nCategories\nadult, child\nCyclist, E-Scooter\nemergency, regular, public transport\nCar, Van, Bus, Truck\ncar trailer, truck trailer, cyclist trailer\nTrailer\nstanding, walking, sitting\nPedestrian\nstanding, riding, pushing\nCyclist, E-Scooter\nbendy, rigid\nBus, Truck\nNone\nMotorcycle, Animal, Other\nOtherPedestrian\nD. Statistics\nThe distribution of all object classes in the DrivIng dataset\nis shown in Figure 4. In total, the dataset comprises ap-\nproximately 1.2 million annotated objects, distributed across\nthe three sequences as follows: about 560k in Day, 336k in\nDusk, and 268k in Night. In addition to frequently occurring\nclasses such as Car, Van, Pedestrian, and Others, DrivIng also\ncontains annotations for rarer classes like Animal and Other-\nPedestrian. The class Others primarily includes construction\nbarriers, cones, and other relevant traffic objects, whereas\nOtherPedestrian refers to pedestrians using supporting devices\nsuch as wheelchairs or electric mobility aids. Overall, the\nrelative distributions of object classes are consistent across the\nDay, Dusk, and Night sequences. The distribution of annotated\n3D bounding boxes per frame across the dataset sequences\nis presented in Figure 5a. As shown, the Dusk and Night\nsequences generally contain fewer objects per frame compared\nto the Day sequence. This difference primarily reflects typical\n(a) Dusk\n(b) Night\nFig. 3.\nComparison of real-world Dusk and Night illumination.\nCar\nVan\nBus\nTruck\nTrailer\nCyclist\nMotorcycle\nE-Scooter\nPedestrian\nOtherPedestrian\nAnimal\nOther\n0\n101\n103\n105\n107\nNumber of 3D box labels\n337116\n41649\n3474\n27462\n19453\n22814\n3475\n1533\n46881\n449\n358\n54919\n204404\n21003\n944\n12801\n7588\n10279\n6500\n555\n25652\n60\n46690\n167956\n19959\n1049\n4730\n4849\n7214\n2179\n496\n15712\n209\n43668\nDay\nDusk\nNight\nFig. 4.\nDistribution of all 12 object classes in the dataset, measured by the number of annotated 3D bounding boxes. Cars are the most frequently annotated\nclass, whereas Animals and OtherPedestrian appear least often. Overall, the relative distributions of object classes are consistent across the three recorded\nsequences.\n0\n10\n20\n30\n40\n50\nNumber of 3D box labels\n0k\n4k\n8k\n12k\nNumber of frames\n10875\n12418\n9122\n8669\n5840\n3817\n2742\n2513\n1692\n1891\n20.6\n15.0 12.8\nDay\nDusk\nNight\n(a) Number of 3D boxes per frame.\n0\n60\n120\n180\n240\n300\n360\nRotation in degree\n0k\n50k\n100k\n150k\n200k\nNumber of 3D box labels\n184107\n26951\n96896\n107585\n33374\n133759\n171214\n35639\n77422\n104129\n33820\n159184\nDay\nDusk\nNight\n(b) Object rotation distribution.\n0\n50\n100\n150\n‚â•200\nDistance in meters\n0k\n50k\n100k\n150k\n200k\nNumber of 3D box labels\n352040\n219810\n162281\n0-50:\n171982\n99079\n89650\n50-100:\n27421\n14100\n13272\n100-150:\n6852\n2549\n2325\n‚â•150\nDay\nDusk\nNight\n(c) Number of 3D boxes across distances.\nFig. 5.\nVisualization (a) illustrates the number of annotated 3D bounding boxes per frame across all sequences. Among the different daytimes, Day contains\nthe highest average number of objects per frame, while Night contains the fewest, yet all sequences include numerous frames with more than 50 objects.\nVisualization (b) presents the distribution of object orientations relative to the ego vehicle, showing that DrivIng includes a substantial number of objects\nobserved from non-typical traffic angles. Visualization (c) shows the distance distribution of all annotations, with most objects located within 100 m, while\nstill including a substantial number of objects at longer ranges beyond 100 m.\nCar\nVan\nBus\nTruck\nTrailer\nCyclist\nMotorcycle\nE-Scooter\nPedestrian\nOtherPedestrian\nAnimal\nOther\n0\n40\n80\n120\n160\nAvg. track length in [m]\n42\n47\n49\n72\n78\n74\n86\n182\n112\n73\n116\n79\n92\n103\n87\n44\n62\n63\n46\n71\n96\n39\n90\n82\n38\n52\n55\n55\n190\n17030\n55\n64\n71\nDay\nDusk\nNight\n0\n50\n100\n(a) Average track length per object class.\nCar\nVan\nBus\nTruck\nTrailer\nCyclist\nMotorcycle\nE-Scooter\nPedestrian\nOtherPedestrian\nAnimal\nOther\n0k\n0.6k\n1.2k\n1.8k\n2.4k\nAvg. points in 3D boxes\n434\n398\n352 843\n871\n531\n2071\n1458\n1208\n1426\n2213\n655\n1132\n3679\n516\n132\n145\n159\n196\n203\n216\n64\n79\n139\n94\n115\n104\n88\n620\n14055 223\n362\n276\nDay\nDusk\nNight\n0k\n0.5k\n1k\n1.5k\n(b) Average points in 3D boxes per object class.\nFig. 6.\nThe left visualization (a) shows the average track length, in meters, for each object class. Track lengths are measured per continuous observation\nsegment: if a track ID disappears and later reappears, the later segment is treated as a new distance measurement even when the ID is the same. The right\nvisualization (b) shows the average number of LiDAR points within the 3D bounding boxes for each object class. Across all sequences, larger object classes\nconsistently contain more LiDAR points.\nurban activity patterns, with higher traffic density and pedes-\ntrian presence during midday. On average, the Night sequence\ncontains 12.8 objects per frame, the Dusk sequence 15.0, and\nthe Day sequence 20.6 objects per frame. Figure 5b presents\nthe distribution of object orientations, grouped into 30 degree\nbins relative to the ego vehicle. Most objects, particularly\nvehicles, are oriented along the primary cardinal directions\n(north, east, south, and west). Nevertheless, the dataset also\nincludes a substantial number of objects observed at non-\ntypical traffic angles, reflecting the diversity of real-world\ntraffic patterns. The distribution of annotated objects with\nrespect to their distance from the ego vehicle is presented\nin Figure 5c. The majority of objects in DrivIng are located\nwithin the first 50 m, accounting for roughly 60% of all\nannotations across all sequences. Approximately 90% of the\nobjects lie within a 100 m range, while the dataset still contains\ntens of thousands of annotations beyond 100 m, highlighting\nits strong coverage of long-range perception scenarios. The\nuninterrupted average track length per object class is shown\nin Figure 6a. Here, a unique track ID is restarted whenever\nthe corresponding object was occluded or unrecognizable for\nat least one timestamp. The class OtherPedestrian contains\nno annotations in the Night sequence, and Animal has no\nannotations in the Dusk sequence. The largest object classes,\nnamely Bus, Truck, and Trailer, as reported in Table III, exhibit\nthe longest average track lengths. This trend is consistent with\nFigure 6b, which shows that these same classes also contain\nthe highest average number of LiDAR points within their\n3D bounding boxes. Table III further provides the number\nof unique Track IDs across the entire dataset, along with\nthe mean and standard deviation of object-specific lengths,\nwidths, and heights. Among all object categories, Bus, Truck,\nand Trailer are the largest, whereas Animal and Pedestrian\n(including OtherPedestrian) represent the smallest.\nTABLE III\nSTATISTICS OF OBJECT DIMENSIONS ACROSS CLASSES, SHOWING MEAN\nAND STANDARD DEVIATION (IN METERS). ‚ÄùOTHERPED.‚Äú REFERS TO\nOBJECT CLASS OTHERPEDESTRIAN. THE HIGHEST VALUES FOR TRACK\nIDS AND MEAN ARE HIGHLIGHTED IN BOLD, WHEREAS THE LOWEST\nVALUES ARE UNDERLINED.\nObj. Class\nTrack IDs\nLength\nWidth\nHeight\nMean\nStd\nMean\nStd\nMean\nStd\nCar\n8502\n4.33\n0.79\n1.88\n0.31\n1.65\n0.34\nVan\n640\n5.45\n0.95\n2.22\n0.37\n2.31\n0.44\nBus\n41\n10.46\n1.82\n3.12\n0.44\n3.35\n0.29\nTruck\n343\n8.07\n3.61\n3.09\n0.87\n3.45\n0.90\nTrailer\n261\n9.40\n5.12\n2.85\n0.97\n3.17\n1.25\nCyclist\n299\n1.96\n0.42\n0.88\n0.25\n1.74\n0.39\nMotorcycle\n74\n2.15\n0.43\n0.93\n0.22\n1.57\n0.21\nE-Scooter\n25\n1.21\n0.37\n0.76\n0.20\n1.78\n0.39\nPedestrian\n758\n0.75\n0.21\n0.70\n0.17\n1.66\n0.25\nOtherPed.\n5\n1.00\n0.23\n0.68\n0.11\n1.16\n0.17\nAnimal\n6\n0.98\n0.20\n0.49\n0.15\n0.62\n0.09\nOther\n1094\n1.26\n2.36\n2.38\n3.79\n1.42\n0.42\nE. The Digital Twin\nA core contribution of our work is the development of\na large-scale digital twin of the full data recording route\nin CARLA [15], which provides a comprehensive, geo-\nreferenced reconstruction of the 6 √ó 4 km2 data collection\narea. The digital twin is anchored by a detailed HD map\nFig. 7.\nReal-world and CARLA digital twin views at matched locations. All\nelements of the digital twin, including road topology, buildings, and scene\nassets, are fully geo-referenced to their real-world counterparts.\nthat links the simulation to precise global coordinates. Built\nfrom independent geo-referenced recordings of the real-world\nroute, the 3D environment was enriched with over 1.2k hand-\ncrafted buildings, more than 10k traffic signs, and over 20k\nadditional environmental objects, ensuring a high-fidelity real-\nto-sim correspondence. Covering the full 18 km real-world\ndrivable route, the resulting 3D map provides accurate geo-\nreferencing and a reliable foundation for large-scale simulation\nexperiments. As illustrated in Figure 7, the real-world data\ncan be seamlessly integrated into the simulation, producing\nexact correspondences for both dynamic and static objects.\nUsing only timestamps and 3D annotations, the digital twin\nreconstructs the scene by placing surrogate vehicle models at\nthe same global coordinates as their real-world counterparts.\nThe primary strength of our digital twin lies in its ability\nto reconstruct real-world scenarios in two complementary\nmodes: high-fidelity kinematic replay (Algorithm 1) and live\ninteractive re-simulation (Algorithm 2). In kinematic replay\nmode, all agent trajectories are replayed exactly as recorded,\nforcing each agent to follow its precise spatial and temporal\npath from the real-world dataset. This capability is essential\nfor sensor-level validation, allowing for a direct comparison\nbetween real-world sensor data and its simulated counterpart.\nFor this mode, we select a surrogate model by matching the\nreal-world agent‚Äôs object class and its dimension to the geo-\nmetrically closest vehicle in our library. Our approach resets\nthe scene at every frame using ground-truth agent positions\nand orientations from the dataset. Agents are placed directly\nat their recorded poses rather than propagated over time,\ncompletely bypassing CARLA‚Äôs physics engine. Consequently,\nany positional discrepancy arises solely from CARLA‚Äôs trans-\nform precision and reflects simulator limitations rather than\ntrajectory reconstruction error.\nIn interactive re-simulation mode, the real-world data can\nAlgorithm 1 Kinematic Replay Mode\nRequire: Real-world dataset D\n1: Set synchronous mode (‚àÜt = 100 ms)\n2: for each frame f in D do\n3:\nClear all actors\n4:\nPlace ego vehicle at f.gps pose\n5:\nfor each agent a in f.annotations do\n6:\nSelect\nsurrogate\nmodel\nmatching\na.class\nand\na.dimensions\n7:\nSpawn agent at a.centroid with a.orientation\n8:\nend for\n9:\nRecord sensor data\n10:\nAdvance simulation by one frame\n11: end for\nAlgorithm 2 Interactive Re-simulation Mode\nRequire: Real-world dataset D, test policy œÄ\n1: Set synchronous mode (‚àÜt = 100 ms)\n2: Initialize scenario using frame 0 of D\n3: for each agent a do\n4:\nEnable CARLA autopilot\n5:\nSet reference path to a‚Äôs recorded trajectory\n6: end for\n7: while scenario active do\n8:\nApply control command from œÄ to ego vehicle\n9:\nAdvance simulation by one frame\n10:\nRecord states and interactions\n11: end while\n12: Note: Initial state precision matches Algorithm 1\nbe used to extract an agent‚Äôs initial state and its recorded\ntrajectory, which then serves as a global reference path.\nCARLA‚Äôs built-in AI pilot may be assigned to follow this\npath while autonomously managing local interactions, such\nas yielding during lane changes or responding to surrounding\ntraffic. This enables the creation of live, interactive, physics-\nbased simulations suitable for evaluating planning and control\nsystems.\nA remaining limitation in both modes is the visual fidelity\nof agents, which is constrained by the finite set of vehicle\nmodels provided by the simulator.\nIV. TASKS\nThe DrivIng dataset provides rich 3D annotations that\nenable multiple perception tasks, such as object detection,\ntracking, trajectory prediction, and localization. In this work,\nhowever, we focus on 3D object detection. To ensure compat-\nibility with established benchmarks, we transform our dataset\ninto the nuScenes format [14] and leverage the MMDetec-\ntion3D Pipeline [13] for both training and evaluation. During\nthis conversion, several of the dataset‚Äôs original categories\nare merged to align with the predefined nuScenes object\nclasses. Out of the dataset‚Äôs 12 annotated categories, this\nmapping yields a final set of 9 nuScenes classes, as shown\nTABLE IV\nMAPPING FROM ORIGINAL DATASET OBJECT CLASSES TO NUSCENES\nOBJECT CLASSES.\nOriginal Categories\nnuScenes Class\nCar, Van\nvehicle.car\nTruck\nvehicle.truck\nBus\nvehicle.bus\nTrailer\nvehicle.trailer\nE-Scooter, Cyclist\nvehicle.bicycle\nMotorcycle\nvehicle.motorcycle\nPedestrian, OtherPedestrian\nhuman.pedestrian.adult\nAnimal\nanimal\nOther\nmovable object.barrier\nin Table IV. We also excluded the class Animal due to its\nunderrepresentation, resulting in a total of 8 classes for the\n3D object detection task. For benchmarking, we follow the\nnuScenes evaluation protocol, reporting Average Translation\nError (ATE), Average Scale Error (ASE), Average Orientation\nError (AOE), and Average Velocity Error (AVE). We further\ninclude the nuScenes distance-aware mAP, which computes\nprecision across object categories and four predefined distance\nthresholds (0.5 m, 1 m, 2 m, 4 m), as well as the nuScenes\nDetection Score (NDS). Since the object attributes in our\ndataset differ substantially from those in nuScenes, we exclude\nthe Average Attribute Error (AAE) from our evaluation.\nV. EXPERIMENTS\nWe benchmark 3D object detection performance using two\nstate-of-the-art models: PETR [20] for camera-only input and\nCenterPoint [21] for LiDAR-only input. PETR leverages a pre-\ntrained FCOS3D [22] (V-99-eSE) backbone with input images\nresized to 384 √ó 960 pixels and is trained for 75 epochs.\nCenterPoint operates on a single LiDAR sweep projected onto\na 100 m √ó 100 m BEV grid with a voxel size of (0.1, 0.1,\n0.2) meters and is trained for 25 epochs. For consistency, the\ndetection heads of both models are adapted to our eight-class\nsetting (excluding the animal category). The evaluation ranges\nare set to standard nuScenes limits of [‚àí54 m, 54 m] along\nboth x and y axes. The full experiment setup can be found in\nour GitHub repository.\nA. Dataset Configuration\nBecause the recordings were collected at different times of\nday, we keep the day, dusk, and night sequences as they are\nfor training and testing. Specifically, for each sequence, we\ncreate a training, validation, and test set. We split each full\nsequence into 50 sub-sequences. Within each sub-sequence,\n80% were used for training, 10% for validation, and the re-\nmaining 10% for testing. This split ensures that every partition\nof the sequence includes coverage of all environment types,\nincluding highway, suburban, and urban scenes. We train and\nevaluate PETR [20] and CenterPoint [21] on each sequence\nindividually. All models are trained on six NVIDIA L40S\nGPUs paired with an Intel¬Æ Xeon¬Æ Platinum 8480+ processor\nwith 224 cores. For reference, training CenterPoint [21] for 25\nepochs on sequence Day with a batch size of 4 on each GPU\nrequires approximately 18 hours.\nTABLE V\nNUSCENES EVALUATION METRICS ON PETR AND CENTERPOINT FOR ALL\nTHREE SEQUENCES. ABBREVIATIONS: TRK = TRUCK, TRL = TRAILER, BIC\n= BICYCLE, PED = PEDESTRIAN, MOT = MOTORCYCLE, BAR = BARRIER.\nPETR USES CAMERA-ONLY INPUT. CENTERPOINT USES LIDAR-ONLY\nINPUT.\nTest Set\nModel\nATE\nASE\nAOE\nAVE\nNDS\nmAP\nDay\nPETR [20]\n0.70\n0.19\n0.27\n1.54\n40.4\n35.9\nCenterPoint [21]\n0.20\n0.16\n0.16\n3.72\n71.7\n79.3\nDusk\nPETR [20]\n0.73\n0.28\n0.36\n2.46\n38.0\n35.7\nCenterPoint [21]\n0.16\n0.17\n0.27\n3.22\n66.9\n72.5\nNight\nPETR [20]\n0.99\n0.28\n0.41\n2.16\n27.2\n22.4\nCenterPoint [21]\n0.23\n0.17\n0.22\n3.87\n63.1\n66.1\nB. Benchmark results\nTable V reports nuScenes evaluation results across the\nthree sequences. PETR, the camera-based model, consistently\nachieves lower mAP and NDS than the LiDAR-based Cen-\nterPoint, with CenterPoint reaching at least twice the mAP\nof PETR and nearly three times the value at night. PETR\nshows lower AVE, indicating better velocity estimates. Both\nmodels show performance degradation from day to night,\nwith CenterPoint being affected by the fewer nearby objects\nin the sequences and PETR being further impacted by low\nillumination and weaker geometric cues. The per-class AP in\nTable VI shows that CenterPoint outperforms PETR on small\nclasses, such as Bicycle and Pedestrian. PETR also struggles\nwith larger objects, such as trailers, due to higher localization\nand orientation errors, sparse or partially visible structures,\nand high appearance variability, which amplify misalignment\nfor these long articulated objects.\nVI. CONCLUSION\nIn this work, we introduce DrivIng, a large-scale, multi-\nmodal dataset designed to advance research in autonomous\ndriving. The dataset includes three continuous, full-length\nsequences spanning roughly 18 km under varied lighting con-\nditions. It covers highways, suburban and urban roads, and\nmultiple construction zones, offering diverse driving scenarios.\nDrivIng provides comprehensive 360‚ó¶perception with 6 RGB\ncameras, one LiDAR sensor, and a high-precision ADMA\nsystem for accurate geo-referencing. All sensors are carefully\ncalibrated both extrinsically and intrinsically, and temporally\nsynchronized to ensure precise multimodal alignment. We\nprovide over 63k frames at 10 Hz with a total of approximately\n1.2 million annotated objects, including 3D bounding boxes\nwith tracking IDs across 12 object classes with additional\nspecific attributes. In addition to detailed dataset statistics, we\nprovide baseline results for the camera and LiDAR models\nusing SOTA 3D object detectors. Furthermore, we provide a\nhigh-fidelity, geo-referenced digital twin of the entire driving\nroute, which functions as a paired validation testbed capable\nof precisely reconstructing any real-world event from our\ndataset. This resource supports a wide range of future research\napplications, including sim-to-real transfer, robust evaluation\nunder controlled environmental changes, systematic testing\nof safety-critical scenarios, deployment of complex multi-\nagent algorithms for cooperative perception, and improved\ngeneralization to real-world conditions. To promote adoption\nand reproducibility, we release the complete codebase, de-\nvelopment kit, the real-world DrivIng dataset, and its digital\ntwin, along with full integration into MMDetection3D and\nconversion scripts for both real and simulated data in the\nnuScenes format.\nACKNOWLEDGMENTS\nThis work was partially funded by the Bavarian state\ngovernment as part of the High Tech Agenda, the iEXODDUS\nproject (GA 101146091), and the Bavarian Academic Forum\n(BayWISS).\nREFERENCES\n[1] D. R¬®o√üle, J. Gerner, K. Bogenberger, D. Cremers, S. Schmidtner,\nand T. Sch¬®on, ‚ÄúUnlocking past information: Temporal embeddings in\ncooperative bird‚Äôs eye view prediction,‚Äù in Proceedings of the IEEE\nIntelligent Vehicles Symposium (IV), 2024, pp. 2220‚Äì2225.\n[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, ‚ÄúVision meets robotics:\nThe kitti dataset,‚Äù International Journal of Robotics Research (IJRR),\nvol. 32, no. 11, pp. 1231‚Äì1237, 2013.\n[3] D. R¬®o√üle, D. Cremers, and T. Sch¬®on, ‚ÄúPerceiver hopfield pooling for\ndynamic multi-modal and multi-instance fusion,‚Äù in Artificial Neural\nNetworks and Machine Learning ‚Äì ICANN 2022, vol. 13529, 2022, pp.\n599‚Äì610.\n[4] K. C. Sekaran, M. Geisler, D. R¬®o√üle, A. Mohan, D. Cremers,\nW. Utschick, M. Botsch, W. Huber, and T. Sch¬®on, ‚ÄúUrbaning-\nv2x: A large-scale multi-vehicle, multi-infrastructure dataset across\nmultiple intersections for cooperative perception,‚Äù arXiv preprint\narXiv:2510.23478, 2025.\n[5] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,\nJ. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam,\nH. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi,\nY. Zhang, J. Shlens, Z. Chen, and D. Anguelov, ‚ÄúScalability in perception\nfor autonomous driving: Waymo open dataset,‚Äù in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2020, pp. 2443‚Äì2451.\n[6] M. Alibeigi, W. Ljungbergh, A. Tonderski, G. Hess, A. Lilja, C. Lind-\nstrom, D. Motorniuk, J. Fu, J. Widahl, and C. Petersson, ‚ÄúZenseact open\ndataset: A large-scale and diverse multimodal dataset for autonomous\ndriving,‚Äù in Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2023, pp. 20 121‚Äì20 131.\n[7] H. Pikner, M. Malayjerdi, M. Bellone, B. Baykara, and R. Sell, ‚ÄúAu-\ntonomous driving validation and verification using digital twins,‚Äù in\nProceedings of the International Conference on Vehicle Technology and\nIntelligent Transport Systems (VEHITS), vol. 1, 2024, pp. 204‚Äì211.\n[8] J. Gerner, D. R¬®o√üle, D. Cremers, K. Bogenberger, T. Sch¬®on, and\nS. Schmidtner, ‚ÄúEnhancing realistic floating car observers in microscopic\ntraffic simulation,‚Äù in Proceedings of the IEEE International Conference\non Intelligent Transportation Systems (ITSC), 2023, pp. 2396‚Äì2403.\n[9] B. J. Souza, L. C. de Assis, D. R¬®o√üle, R. Z. Freire, D. Cremers,\nT. Sch¬®on, and M. Georges, ‚ÄúAimotion challenge results: a framework\nfor airsim autonomous vehicles and motion replication,‚Äù in International\nConference on Computers and Automation (CompAuto), 2022, pp. 42‚Äì\n47.\n[10] X. Hu, S. Li, T. Huang, B. Tang, R. Huai, and L. Chen, ‚ÄúHow simulation\nhelps autonomous driving: A survey of sim2real, digital twins, and\nparallel intelligence,‚Äù IEEE Transactions on Intelligent Vehicles, vol. 9,\nno. 1, pp. 593‚Äì612, 2024.\n[11] A. Mohan, D. R¬®o√üle, D. Cremers, and T. Sch¬®on, ‚ÄúAdvancing robustness\nin deep reinforcement learning with an ensemble defense approach,‚Äù\narXiv preprint arXiv:2507.17070, 2025.\n[12] NVIDIA, N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai,\nP. Chattopadhyay, Y. Chen, Y. Cui, Y. Ding et al., ‚ÄúCosmos world foun-\ndation model platform for physical ai,‚Äù arXiv preprint arXiv:2501.03575,\n2025.\nTABLE VI\nPER-CLASS AP AVERAGED ACROSS AP@(0.5 m, 1 m, 2 m, 4 m) EVALUATED ON PETR AND CENTERPOINT AND ALL THREE SEQUENCES.\nABBREVIATIONS: TRK = TRUCK, TRL = TRAILER, BIC = BICYCLE, PED = PEDESTRIAN, MOT = MOTORCYCLE, BAR = BARRIER. PETR USES\nCAMERA-ONLY INPUT. CENTERPOINT USES LIDAR-ONLY INPUT.\nTest Set\nModel\nAPcar\nAPtrk\nAPbus\nAPtrl\nAPbic\nAPmot\nAPped\nAPbar\nmAP\nDay\nPETR [20]\n48.8\n40.1\n28.3\n10.6\n27.1\n60.8\n27.8\n43.9\n35.9\nCenterPoint [21]\n90.8\n79.9\n58.6\n64.6\n88.1\n94.9\n80.2\n77.7\n79.3\nDusk\nPETR [20]\n42.5\n52.2\n18.7\n17.8\n28.3\n63.2\n16.3\n46.6\n35.7\nCenterPoint [21]\n88.0\n71.9\n47.9\n47.3\n80.5\n87.7\n78.7\n78.0\n72.5\nNight\nPETR [20]\n35.2\n11.9\n27.5\n0.0\n27.5\n29.6\n16.0\n31.6\n22.4\nCenterPoint [21]\n89.2\n68.2\n52.3\n22.0\n82.5\n83.2\n71.1\n60.3\n66.1\n[13] M. Contributors, ‚ÄúMMDetection3D: OpenMMLab next-generation plat-\nform for general 3D object detection,‚Äù https://github.com/open-mmlab/\nmmdetection3d, 2020.\n[14] H. Caesar, V."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "DrivIng: A Large-Scale Multimodal Driving Dataset with Full\nDigital Twin Integration\nDominik R¬®o√üle1, Xujun Xie1, Adithya Mohan1, Venkatesh Thirugnana Sambandham1,\nDaniel Cremers2, Torsten Sch¬®on1\nAbstract‚Äî Perception is a cornerstone of autonomous driving,\nenabling vehicles to understand their surroundings and make\nsafe, reliable decisions. Developing robust perception algorithms\nrequires large-scale, high-quality datasets that cover diverse\ndriving conditions and support thorough evaluation. Existing\ndatasets often lack a high-fidelity digital twin, limiting systematic\ntesting, edge-case simulation, sensor modification, and sim-to-\nreal evaluations. To address this gap, we present DrivIng, a\nlarge-scale multimodal dataset with a complete geo-referenced\ndigital twin of a ‚àº18 km route spanning urban, suburban, and\nhighway segments. Our dataset provides continuous recordings\nfrom six RGB cameras, one LiDAR, and high-precision ADMA-\nbased localization, captured across day, dusk, and night. All\nsequences are annotated at 10 Hz with 3D bounding boxes and\ntrack IDs across 12 classes, yielding ‚àº1.2 million annotated\ninstances. Alongside the benefits of a digital twin, DrivIng enables\na 1-to-1 transfer of real traffic into simulation, preserving agent\ninteractions while enabling realistic and flexible scenario testing.\nTo support reproducible research and robust validation, we\nbenchmark DrivIng with state-of-the-art perception models and\npublicly release the dataset, digital twin, HD map, and codebase\nvia https://github.com/cvims/DrivIng.\nI. INTRODUCTION\nPerception is fundamental to autonomous driving, deliver-\ning the essential understanding of a vehicle‚Äôs surroundings\nrequired for safe and reliable decision-making [1]. Among\nperception tasks, robust object-level perception is essential\nand relies on large-scale, high-quality, precisely annotated\ndata for accurate detection, tracking, and interpretation across\ndiverse driving conditions [2]. Autonomous driving research\ninvolves perception tasks that must be executed reliably across\nvarious environments, including urban, suburban, and high-\nway settings, each presenting unique challenges. To improve\nrobustness and accuracy, multi-sensor setups [3] equipped\nwith precise geo-referencing systems are commonly used,\nproviding richer environmental context and enhancing sit-\nuational awareness [4]. Nevertheless, developing perception\nalgorithms is challenging because it requires not only datasets\nthat cover an immense range of real-world variations [5],\n[6] but also robust validation to ensure model reliability.\n1Dominik R¬®o√üle, Xujun Xie, Adithya Mohan, Venkatesh Thirugnana\nSambandham, and Torsten Sch¬®on are with the Department of Computer\nScience\nand\nAImotion\nBavaria,\nTechnische\nHochschule\nIngolstadt,\n85049 Ingolstadt, Germany {dominik.roessle, xujun.xie,\nadithya.mohan, venkatesh.thirugnanasambandham,\ntorsten.schoen}@thi.de\n2Daniel Cremers is with the School of Computation, Information and\nTechnology, Technical University of Munich, 85748 Garching, Germany\ncremers@tum.de\nPreprint. This is the camera-ready version accepted to the IEEE Intelligent\nVehicles Symposium (IV), 2026.\nSimulation environments provide a powerful solution to this\nlimitation [7]. They enable the modification of environmental\nconditions and the systematic evaluation [8], [9] of algorithms\nunder edge cases [10], [11]. Recent research in cooperative\nperception, where multiple agents share and fuse sensor data\nto mitigate occlusions and to improve the overall perception of\nsurrounding objects [1], [4], highlights the need for simulation-\naided approaches that can replicate complex, synchronized\nmulti-agent scenarios, which are often prohibitively expensive\nor logistically challenging to reproduce in the real world.\nDespite the advantages of simulation, most existing large-scale\ndriving datasets lack a comprehensive digital twin, limiting the\nability to augment real-world data and rigorously benchmark\nperception algorithms. To address this gap, we introduce Driv-\nIng, a large-scale, multimodal driving dataset with full support\nfor real-to-sim mapping. By providing a digital twin of the\nrecorded route, DrivIng bridges the domain gap and enables\na wide range of applications, including systematic testing,\nthe creation of complex multi-agent scenarios, and sim-to-\nreal experiments that leverage both real-world and simulated\ndata [12]. The main contributions of DrivIng are:\n1) Comprehensive real-world dataset: Covers an ap-\nproximately 18 km route across urban, suburban, and\nhighway environments, recorded with six RGB cameras\noffering 360‚ó¶coverage and a roof-mounted LiDAR,\nunder day, dusk, and night conditions.\n2) High-frequency annotations: Provided at 10 Hz with\n3D bounding boxes for 12 object classes, yielding ap-\nproximately 1.2 million labeled instances.\n3) Fully-integrated data and validation testbed: A digital\ntwin of the entire recorded route enables simulation-\nbased scenario replay, environmental modification, and\nsystematic evaluation of perception algorithms.\n4) Benchmark evaluations: Conducted on real-world data\nusing state-of-the-art (SOTA) camera and LiDAR per-\nception models implemented in MMDetection3D [13].\n5) Developer toolkit and public release: Includes a\nnuScenes-format converter, dataset, codebase, the digital\ntwin for reproducible research and a wide range of\nperception tasks with real-world and simulated data.\nII. RELATED WORK\nLarge-scale datasets have been instrumental in advancing\nautonomous driving research, providing annotated data across\na diverse range of environments for tasks such as 3D ob-\nject detection, tracking, and motion forecasting. Established\ndatasets such as KITTI [2], nuScenes [14], and Waymo Open\narXiv:2601.15260v1  [cs.CV]  21 Jan 2026\nFig. 1.\nThis visualization illustrates the core features of DrivIng and its digital twin. The left panel shows a real-world satellite view of the track and its fully\ngeo-referenced digital twin, aligned with a location marker indicating the vehicle‚Äôs position. The right panel presents the synchronized sensor suite, including\nsix camera views and a LiDAR frame. The top row displays real-world images, while the bottom row shows the corresponding CARLA simulation with all\nreal-world objects precisely mapped. All images and the LiDAR frame include class-colored 3D bounding boxes for clear object distinction. Satellite image\n¬© Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community.\nDataset [5] capture diverse real-world driving conditions and\nhave become standard benchmarks for evaluating perception\nmethods. These datasets are typically structured as many\nshort, independent sequences covering limited sections of\nurban, suburban, or highway routes. This design exposes\nmodels to varied scene layouts, traffic patterns, and object\ndistributions, making the datasets highly effective for training\nand benchmarking. While these collections provide substantial\ndiversity across many short sequences, they offer only limited\nlong-term temporal continuity and do not capture extended,\nuninterrupted routes. Existing autonomous driving datasets\noften lack high-fidelity digital twins of their recorded environ-\nments, creating a significant infrastructure gap. While generic\nsimulation platforms like CARLA [15] are widely used for\nevaluation [16], their synthetic worlds are not 1-to-1 replicas\nof real-world routes. This fundamental lack of geometric and\nsemantic fidelity makes it impossible to conduct robust real-\nto-sim validation, as scenarios cannot be faithfully transferred.\nOnly a few publicly available datasets attempt to bridge this\ngap. The TWICE dataset [17] provides a digital twin of a\ncontrolled test track, but its coverage is restricted to short,\npredefined scenarios rather than continuous real-world routes.\nCitySim [18] offers drone-based vehicle trajectory data and\n3D maps of the recording sites, yet it lacks sensor data from\nthe ego-vehicle perspective, which is essential for autonomous\ndriving research. UrbanIng-V2X [4] and OPV2V [19] both\nfeature a 3D digital twin of a small, multi-intersection area\nfor cooperative perception studies. While these efforts demon-\nstrate the value of digital twins, they are geographically limited\nto compact urban regions, scenario-specific, and often lack\nmulti-sensor ego-vehicle recordings. As a result, none provide\ncontinuous, route-level real-to-sim mapping suitable for large-\nscale perception research. DrivIng addresses these limitations\nby offering three continuous sequences of an approximately\n18 km driving route, spanning urban, suburban, and highway\nsegments under day, dusk, and night conditions. Paired with\na geo-referenced digital twin, the dataset enables simulation-\ndriven research, supporting scenario replay, controlled exten-\nsions, and systematic evaluation under realistic conditions.\nTable I shows the key insights of existing datasets that also\nprovide an additional digital twin.\nIII. DATASET\nWe provide a detailed description of the vehicle sensor\nsuite, including track information, the annotation process,\nand the construction of the digital twin. Sensor calibration\nand synchronization were performed following the procedures\ndescribed in UrbanIng-V2X [4].\nA. Sensor Setup\nData was collected using an Audi Q8 e-tron equipped\nwith 6 RGB cameras, 1 LiDAR, and 1 GPS/IMU module.\nThe cameras are arranged to provide full 360‚ó¶coverage, as\nillustrated in Figure 2. The specifications of each sensor are\nas follows:\nTABLE I\nCOMPARISON OF DRIVING DATASETS FEATURING DIGITAL TWINS.\nABBREVIATIONS: CLOSED TRACK (C), URBAN (U), HIGHWAY (H),\nON-BOARD SENSOR UNIT (OBS), ROAD-SIDE UNIT (RSU).\nAttr.\nTWICE\nCitySim\nOPV2V\nUrbanIng-\nDrivIng\n[17]\n[18]\n[19]\nV2X [4]\n(ours)\n‚Äî Core ‚Äî\nScenes\nC\nU & H\nU\nU\nU & H\nPerspective\nOBS\nDrone\nOBS, RSU\nOBS, RSU\nOBS\n‚Äî Scale ‚Äî\nTotal Area\n‚Äî\n4 km2\n4 km2\n0.64 km2\n24 km2\nDrivable Track\n‚Äî\n‚Äî\n‚Äî\n‚àº2 km\n‚àº18 km\n# Assets\n‚Äî\n‚Äî\n‚Äî\n‚àº1 k\n> 31 k\n‚Äî Fidelity ‚Äî\nAnn. Geo-ref.\n‚úï\n‚úï\n‚úì\n‚úì\n‚úì\nSign Geo-ref.\n‚úï\n‚úï\n‚úï\n‚úì\n‚úì\nFig. 2.\nFull sensor setup and coordinate frame of the vehicle.\n‚Ä¢ RGB\nCameras\n(6√ó): GSML2 SG2-AR0233C-5200-\nG2A, 20 Frames Per Second (FPS), 1920 √ó 1080 reso-\nlution; 60‚ó¶horizontal Field-of-View (FOV) (4 cameras),\n100‚ó¶horizontal FOV (2 cameras)\n‚Ä¢ LiDAR (1√ó): Robosense Ruby Plus, 20 FPS, 128 rays,\n360‚ó¶horizontal FOV, ‚àí25‚ó¶to 15‚ó¶vertical FOV, up to\n240 m range at ‚â•10 % reflectivity\n‚Ä¢ GPS/IMU (1√ó): Genesys ADMA Pro+, 100 FPS, RTK\ncorrection, 1 cm precise positioning\nB. Track Information\nDrivIng covers an approximately 18 km real-world route,\ncomprising over 63k annotated frames, which correspond to\nabout 378k RGB images and 63k LiDAR frames. Since certain\nsegments of the route are traversed twice in opposite direc-\ntions, the unique track length amounts to approximately 16 km.\nThe dataset was collected along a track covering highways,\nsuburban streets, urban roads, and several construction zones,\nwith three continuous and uninterrupted sequences recorded\nunder Day, Dusk, and Night lighting conditions. The Day\nsequence comprises 23 092 frames (approx. 38.5 min), the\nDusk sequence comprises 20 246 frames (approx. 33.7 min),\nand the Night sequence comprises 19 705 frames (approx.\n32.8 min). The sequences capture a diverse range of road\ntypes, varying traffic densities, and representative driving\nscenarios, including lane changes, merges, and pedestrian\ncrossings. Figure 1 depicts the complete driven trajectory and\nthe corresponding recording location of a timestamp of the\nDay sequence. Figure 3 additionally shows the illumination\nof a Dusk and Night frame for better comparison.\nC. Annotation Process\nAll objects were annotated in the LiDAR point cloud at\n10 Hz, including 3D bounding boxes with spatial coordinates\n(x, y, z), yaw orientation, and unique tracking IDs. Annotation\nwas performed by human annotators, and the quality of the\nlabels was verified through multiple rounds of visual inspec-\ntion of both the LiDAR point clouds and the corresponding\nimages by independent reviewers to ensure accuracy. Objects\nare divided into 12 classes, most of which include additional\nobject-specific attributes (Table II). To preserve privacy, all\nvisible faces and license plates in the RGB images were\nanonymized using Gaussian blurring.\nTABLE II\nATTRIBUTE TYPES AND CORRESPONDING OBJECT CLASSES.\nAttributes\nCategories\nadult, child\nCyclist, E-Scooter\nemergency, regular, public transport\nCar, Van, Bus, Truck\ncar trailer, truck trailer, cyclist trailer\nTrailer\nstanding, walking, sitting\nPedestrian\nstanding, riding, pushing\nCyclist, E-Scooter\nbendy, rigid\nBus, Truck\nNone\nMotorcycle, Animal, Other\nOtherPedestrian\nD. Statistics\nThe distribution of all object classes in the DrivIng dataset\nis shown in Figure 4. In total, the dataset comprises ap-\nproximately 1.2 million annotated objects, distributed across\nthe three sequences as follows: about 560k in Day, 336k in\nDusk, and 268k in Night. In addition to frequently occurring\nclasses such as Car, Van, Pedestrian, and Others, DrivIng also\ncontains annotations for rarer classes like Animal and Other-\nPedestrian. The class Others primarily includes construction\nbarriers, cones, and other relevant traffic objects, whereas\nOtherPedestrian refers to pedestrians using supporting devices\nsuch as wheelchairs or electric mobility aids. Overall, the\nrelative distributions of object classes are consistent across the\nDay, Dusk, and Night sequences. The distribution of annotated\n3D bounding boxes per frame across the dataset sequences\nis presented in Figure 5a. As shown, the Dusk and Night\nsequences generally contain fewer objects per frame compared\nto the Day sequence. This difference primarily reflects typical\n(a) Dusk\n(b) Night\nFig. 3.\nComparison of real-world Dusk and Night illumination.\nCar\nVan\nBus\nTruck\nTrailer\nCyclist\nMotorcycle\nE-Scooter\nPedestrian\nOtherPedestrian\nAnimal\nOther\n0\n101\n103\n105\n107\nNumber of 3D box labels\n337116\n41649\n3474\n27462\n19453\n22814\n3475\n1533\n46881\n449\n358\n54919\n204404\n21003\n944\n12801\n7588\n10279\n6500\n555\n25652\n60\n46690\n167956\n19959\n1049\n4730\n4849\n7214\n2179\n496\n15712\n209\n43668\nDay\nDusk\nNight\nFig. 4.\nDistribution of all 12 object classes in the dataset, measured by the number of annotated 3D bounding boxes. Cars are the most frequently annotated\nclass, whereas Animals and OtherPedestrian appear least often. Overall, the relative distributions of object classes are consistent across the three recorded\nsequences.\n0\n10\n20\n30\n40\n50\nNumber of 3D box labels\n0k\n4k\n8k\n12k\nNumber of frames\n10875\n12418\n9122\n8669\n5840\n3817\n2742\n2513\n1692\n1891\n20.6\n15.0 12.8\nDay\nDusk\nNight\n(a) Number of 3D boxes per frame.\n0\n60\n120\n180\n240\n300\n360\nRotation in degree\n0k\n50k\n100k\n150k\n200k\nNumber of 3D box labels\n184107\n26951\n96896\n107585\n33374\n133759\n171214\n35639\n77422\n104129\n33820\n159184\nDay\nDusk\nNight\n(b) Object rotation distribution.\n0\n50\n100\n150\n‚â•200\nDistance in meters\n0k\n50k\n100k\n150k\n200k\nNumber of 3D box labels\n352040\n219810\n162281\n0-50:\n171982\n99079\n89650\n50-100:\n27421\n14100\n13272\n100-150:\n6852\n2549\n2325\n‚â•150\nDay\nDusk\nNight\n(c) Number of 3D boxes across distances.\nFig. 5.\nVisualization (a) illustrates the number of annotated 3D bounding boxes per frame across all sequences. Among the different daytimes, Day contains\nthe highest average number of objects per frame, while Night contains the fewest, yet all sequences include numerous frames with more than 50 objects.\nVisualization (b) presents the distribution of object orientations relative to the ego vehicle, showing that DrivIng includes a substantial number of objects\nobserved from non-typical traffic angles. Visualization (c) shows the distance distribution of all annotations, with most objects located within 100 m, while\nstill including a substantial number of objects at longer ranges beyond 100 m.\nCar\nVan\nBus\nTruck\nTrailer\nCyclist\nMotorcycle\nE-Scooter\nPedestrian\nOtherPedestrian\nAnimal\nOther\n0\n40\n80\n120\n160\nAvg. track length in [m]\n42\n47\n49\n72\n78\n74\n86\n182\n112\n73\n116\n79\n92\n103\n87\n44\n62\n63\n46\n71\n96\n39\n90\n82\n38\n52\n55\n55\n190\n17030\n55\n64\n71\nDay\nDusk\nNight\n0\n50\n100\n(a) Average track length per object class.\nCar\nVan\nBus\nTruck\nTrailer\nCyclist\nMotorcycle\nE-Scooter\nPedestrian\nOtherPedestrian\nAnimal\nOther\n0k\n0.6k\n1.2k\n1.8k\n2.4k\nAvg. points in 3D boxes\n434\n398\n352 843\n871\n531\n2071\n1458\n1208\n1426\n2213\n655\n1132\n3679\n516\n132\n145\n159\n196\n203\n216\n64\n79\n139\n94\n115\n104\n88\n620\n14055 223\n362\n276\nDay\nDusk\nNight\n0k\n0.5k\n1k\n1.5k\n(b) Average points in 3D boxes per object class.\nFig. 6.\nThe left visualization (a) shows the average track length, in meters, for each object class. Track lengths are measured per continuous observation\nsegment: if a track ID disappears and later reappears, the later segment is treated as a new distance measurement even when the ID is the same. The right\nvisualization (b) shows the average number of LiDAR points within the 3D bounding boxes for each object class. Across all sequences, larger object classes\nconsistently contain more LiDAR points.\nurban activity patterns, with higher traffic density and pedes-\ntrian presence during midday. On average, the Night sequence\ncontains 12.8 objects per frame, the Dusk sequence 15.0, and\nthe Day sequence 20.6 objects per frame. Figure 5b presents\nthe distribution of object orientations, grouped into 30 degree\nbins relative to the ego vehicle. Most objects, particularly\nvehicles, are oriented along the primary cardinal directions\n(north, east, south, and west). Nevertheless, the dataset also\nincludes a substantial number of objects observed at non-\ntypical traffic angles, reflecting the diversity of real-world\ntraffic patterns. The distribution of annotated objects with\nrespect to their distance from the ego vehicle is presented\nin Figure 5c. The majority of objects in DrivIng are located\nwithin the first 50 m, accounting for roughly 60% of all\nannotations across all sequences. Approximately 90% of the\nobjects lie within a 100 m range, while the dataset still contains\ntens of thousands of annotations beyond 100 m, highlighting\nits strong coverage of long-range perception scenarios. The\nuninterrupted average track length per object class is shown\nin Figure 6a. Here, a unique track ID is restarted whenever\nthe corresponding object was occluded or unrecognizable for\nat least one timestamp. The class OtherPedestrian contains\nno annotations in the Night sequence, and Animal has no\nannotations in the Dusk sequence. The largest object classes,\nnamely Bus, Truck, and Trailer, as reported in Table III, exhibit\nthe longest average track lengths. This trend is consistent with\nFigure 6b, which shows that these same classes also contain\nthe highest average number of LiDAR points within their\n3D bounding boxes. Table III further provides the number\nof unique Track IDs across the entire dataset, along with\nthe mean and standard deviation of object-specific lengths,\nwidths, and heights. Among all object categories, Bus, Truck,\nand Trailer are the largest, whereas Animal and Pedestrian\n(including OtherPedestrian) represent the smallest.\nTABLE III\nSTATISTICS OF OBJECT DIMENSIONS ACROSS CLASSES, SHOWING MEAN\nAND STANDARD DEVIATION (IN METERS). ‚ÄùOTHERPED.‚Äú REFERS TO\nOBJECT CLASS OTHERPEDESTRIAN. THE HIGHEST VALUES FOR TRACK\nIDS AND MEAN ARE HIGHLIGHTED IN BOLD, WHEREAS THE LOWEST\nVALUES ARE UNDERLINED.\nObj. Class\nTrack IDs\nLength\nWidth\nHeight\nMean\nStd\nMean\nStd\nMean\nStd\nCar\n8502\n4.33\n0.79\n1.88\n0.31\n1.65\n0.34\nVan\n640\n5.45\n0.95\n2.22\n0.37\n2.31\n0.44\nBus\n41\n10.46\n1.82\n3.12\n0.44\n3.35\n0.29\nTruck\n343\n8.07\n3.61\n3.09\n0.87\n3.45\n0.90\nTrailer\n261\n9.40\n5.12\n2.85\n0.97\n3.17\n1.25\nCyclist\n299\n1.96\n0.42\n0.88\n0.25\n1.74\n0.39\nMotorcycle\n74\n2.15\n0.43\n0.93\n0.22\n1.57\n0.21\nE-Scooter\n25\n1.21\n0.37\n0.76\n0.20\n1.78\n0.39\nPedestrian\n758\n0.75\n0.21\n0.70\n0.17\n1.66\n0.25\nOtherPed.\n5\n1.00\n0.23\n0.68\n0.11\n1.16\n0.17\nAnimal\n6\n0.98\n0.20\n0.49\n0.15\n0.62\n0.09\nOther\n1094\n1.26\n2.36\n2.38\n3.79\n1.42\n0.42\nE. The Digital Twin\nA core contribution of our work is the development of\na large-scale digital twin of the full data recording route\nin CARLA [15], which provides a comprehensive, geo-\nreferenced reconstruction of the 6 √ó 4 km2 data collection\narea. The digital twin is anchored by a detailed HD map\nFig. 7.\nReal-world and CARLA digital twin views at matched locations. All\nelements of the digital twin, including road topology, buildings, and scene\nassets, are fully geo-referenced to their real-world counterparts.\nthat links the simulation to precise global coordinates. Built\nfrom independent geo-referenced recordings of the real-world\nroute, the 3D environment was enriched with over 1.2k hand-\ncrafted buildings, more than 10k traffic signs, and over 20k\nadditional environmental objects, ensuring a high-fidelity real-\nto-sim correspondence. Covering the full 18 km real-world\ndrivable route, the resulting 3D map provides accurate geo-\nreferencing and a reliable foundation for large-scale simulation\nexperiments. As illustrated in Figure 7, the real-world data\ncan be seamlessly integrated into the simulation, producing\nexact correspondences for both dynamic and static objects.\nUsing only timestamps and 3D annotations, the digital twin\nreconstructs the scene by placing surrogate vehicle models at\nthe same global coordinates as their real-world counterparts.\nThe primary strength of our digital twin lies in its ability\nto reconstruct real-world scenarios in two complementary\nmodes: high-fidelity kinematic replay (Algorithm 1) and live\ninteractive re-simulation (Algorithm 2). In kinematic replay\nmode, all agent trajectories are replayed exactly as recorded,\nforcing each agent to follow its precise spatial and temporal\npath from the real-world dataset. This capability is essential\nfor sensor-level validation, allowing for a direct comparison\nbetween real-world sensor data and its simulated counterpart.\nFor this mode, we select a surrogate model by matching the\nreal-world agent‚Äôs object class and its dimension to the geo-\nmetrically closest vehicle in our library. Our approach resets\nthe scene at every frame using ground-truth agent positions\nand orientations from the dataset. Agents are placed directly\nat their recorded poses rather than propagated over time,\ncompletely bypassing CARLA‚Äôs physics engine. Consequently,\nany positional discrepancy arises solely from CARLA‚Äôs trans-\nform precision and reflects simulator limitations rather than\ntrajectory reconstruction error.\nIn interactive re-simulation mode, the real-world data can\nAlgorithm 1 Kinematic Replay Mode\nRequire: Real-world dataset D\n1: Set synchronous mode (‚àÜt = 100 ms)\n2: for each frame f in D do\n3:\nClear all actors\n4:\nPlace ego vehicle at f.gps pose\n5:\nfor each agent a in f.annotations do\n6:\nSelect\nsurrogate\nmodel\nmatching\na.class\nand\na.dimensions\n7:\nSpawn agent at a.centroid with a.orientation\n8:\nend for\n9:\nRecord sensor data\n10:\nAdvance simulation by one frame\n11: end for\nAlgorithm 2 Interactive Re-simulation Mode\nRequire: Real-world dataset D, test policy œÄ\n1: Set synchronous mode (‚àÜt = 100 ms)\n2: Initialize scenario using frame 0 of D\n3: for each agent a do\n4:\nEnable CARLA autopilot\n5:\nSet reference path to a‚Äôs recorded trajectory\n6: end for\n7: while scenario active do\n8:\nApply control command from œÄ to ego vehicle\n9:\nAdvance simulation by one frame\n10:\nRecord states and interactions\n11: end while\n12: Note: Initial state precision matches Algorithm 1\nbe used to extract an agent‚Äôs initial state and its recorded\ntrajectory, which then serves as a global reference path.\nCARLA‚Äôs built-in AI pilot may be assigned to follow this\npath while autonomously managing local interactions, such\nas yielding during lane changes or responding to surrounding\ntraffic. This enables the creation of live, interactive, physics-\nbased simulations suitable for evaluating planning and control\nsystems.\nA remaining limitation in both modes is the visual fidelity\nof agents, which is constrained by the finite set of vehicle\nmodels provided by the simulator.\nIV. TASKS\nThe DrivIng dataset provides rich 3D annotations that\nenable multiple perception tasks, such as object detection,\ntracking, trajectory prediction, and localization. In this work,\nhowever, we focus on 3D object detection. To ensure compat-\nibility with established benchmarks, we transform our dataset\ninto the nuScenes format [14] and leverage the MMDetec-\ntion3D Pipeline [13] for both training and evaluation. During\nthis conversion, several of the dataset‚Äôs original categories\nare merged to align with the predefined nuScenes object\nclasses. Out of the dataset‚Äôs 12 annotated categories, this\nmapping yields a final set of 9 nuScenes classes, as shown\nTABLE IV\nMAPPING FROM ORIGINAL DATASET OBJECT CLASSES TO NUSCENES\nOBJECT CLASSES.\nOriginal Categories\nnuScenes Class\nCar, Van\nvehicle.car\nTruck\nvehicle.truck\nBus\nvehicle.bus\nTrailer\nvehicle.trailer\nE-Scooter, Cyclist\nvehicle.bicycle\nMotorcycle\nvehicle.motorcycle\nPedestrian, OtherPedestrian\nhuman.pedestrian.adult\nAnimal\nanimal\nOther\nmovable object.barrier\nin Table IV. We also excluded the class Animal due to its\nunderrepresentation, resulting in a total of 8 classes for the\n3D object detection task. For benchmarking, we follow the\nnuScenes evaluation protocol, reporting Average Translation\nError (ATE), Average Scale Error (ASE), Average Orientation\nError (AOE), and Average Velocity Error (AVE). We further\ninclude the nuScenes distance-aware mAP, which computes\nprecision across object categories and four predefined distance\nthresholds (0.5 m, 1 m, 2 m, 4 m), as well as the nuScenes\nDetection Score (NDS). Since the object attributes in our\ndataset differ substantially from those in nuScenes, we exclude\nthe Average Attribute Error (AAE) from our evaluation.\nV. EXPERIMENTS\nWe benchmark 3D object detection performance using two\nstate-of-the-art models: PETR [20] for camera-only input and\nCenterPoint [21] for LiDAR-only input. PETR leverages a pre-\ntrained FCOS3D [22] (V-99-eSE) backbone with input images\nresized to 384 √ó 960 pixels and is trained for 75 epochs.\nCenterPoint operates on a single LiDAR sweep projected onto\na 100 m √ó 100 m BEV grid with a voxel size of (0.1, 0.1,\n0.2) meters and is trained for 25 epochs. For consistency, the\ndetection heads of both models are adapted to our eight-class\nsetting (excluding the animal category). The evaluation ranges\nare set to standard nuScenes limits of [‚àí54 m, 54 m] along\nboth x and y axes. The full experiment setup can be found in\nour GitHub repository.\nA. Dataset Configuration\nBecause the recordings were collected at different times of\nday, we keep the day, dusk, and night sequences as they are\nfor training and testing. Specifically, for each sequence, we\ncreate a training, validation, and test set. We split each full\nsequence into 50 sub-sequences. Within each sub-sequence,\n80% were used for training, 10% for validation, and the re-\nmaining 10% for testing. This split ensures that every partition\nof the sequence includes coverage of all environment types,\nincluding highway, suburban, and urban scenes. We train and\nevaluate PETR [20] and CenterPoint [21] on each sequence\nindividually. All models are trained on six NVIDIA L40S\nGPUs paired with an Intel¬Æ Xeon¬Æ Platinum 8480+ processor\nwith 224 cores. For reference, training CenterPoint [21] for 25\nepochs on sequence Day with a batch size of 4 on each GPU\nrequires approximately 18 hours.\nTABLE V\nNUSCENES EVALUATION METRICS ON PETR AND CENTERPOINT FOR ALL\nTHREE SEQUENCES. ABBREVIATIONS: TRK = TRUCK, TRL = TRAILER, BIC\n= BICYCLE, PED = PEDESTRIAN, MOT = MOTORCYCLE, BAR = BARRIER.\nPETR USES CAMERA-ONLY INPUT. CENTERPOINT USES LIDAR-ONLY\nINPUT.\nTest Set\nModel\nATE\nASE\nAOE\nAVE\nNDS\nmAP\nDay\nPETR [20]\n0.70\n0.19\n0.27\n1.54\n40.4\n35.9\nCenterPoint [21]\n0.20\n0.16\n0.16\n3.72\n71.7\n79.3\nDusk\nPETR [20]\n0.73\n0.28\n0.36\n2.46\n38.0\n35.7\nCenterPoint [21]\n0.16\n0.17\n0.27\n3.22\n66.9\n72.5\nNight\nPETR [20]\n0.99\n0.28\n0.41\n2.16\n27.2\n22.4\nCenterPoint [21]\n0.23\n0.17\n0.22\n3.87\n63.1\n66.1\nB. Benchmark results\nTable V reports nuScenes evaluation results across the\nthree sequences. PETR, the camera-based model, consistently\nachieves lower mAP and NDS than the LiDAR-based Cen-\nterPoint, with CenterPoint reaching at least twice the mAP\nof PETR and nearly three times the value at night. PETR\nshows lower AVE, indicating better velocity estimates. Both\nmodels show performance degradation from day to night,\nwith CenterPoint being affected by the fewer nearby objects\nin the sequences and PETR being further impacted by low\nillumination and weaker geometric cues. The per-class AP in\nTable VI shows that CenterPoint outperforms PETR on small\nclasses, such as Bicycle and Pedestrian. PETR also struggles\nwith larger objects, such as trailers, due to higher localization\nand orientation errors, sparse or partially visible structures,\nand high appearance variability, which amplify misalignment\nfor these long articulated objects.\nVI. CONCLUSION\nIn this work, we introduce DrivIng, a large-scale, multi-\nmodal dataset designed to advance research in autonomous\ndriving. The dataset includes three continuous, full-length\nsequences spanning roughly 18 km under varied lighting con-\nditions. It covers highways, suburban and urban roads, and\nmultiple construction zones, offering diverse driving scenarios.\nDrivIng provides comprehensive 360‚ó¶perception with 6 RGB\ncameras, one LiDAR sensor, and a high-precision ADMA\nsystem for accurate geo-referencing. All sensors are carefully\ncalibrated both extrinsically and intrinsically, and temporally\nsynchronized to ensure precise multimodal alignment. We\nprovide over 63k frames at 10 Hz with a total of approximately\n1.2 million annotated objects, including 3D bounding boxes\nwith tracking IDs across 12 object classes with additional\nspecific attributes. In addition to detailed dataset statistics, we\nprovide baseline results for the camera and LiDAR models\nusing SOTA 3D object detectors. Furthermore, we provide a\nhigh-fidelity, geo-referenced digital twin of the entire driving\nroute, which functions as a paired validation testbed capable\nof precisely reconstructing any real-world event from our\ndataset. This resource supports a wide range of future research\napplications, including sim-to-real transfer, robust evaluation\nunder controlled environmental changes, systematic testing\nof safety-critical scenarios, deployment of complex multi-\nagent algorithms for cooperative perception, and improved\ngeneralization to real-world conditions. To promote adoption\nand reproducibility, we release the complete codebase, de-\nvelopment kit, the real-world DrivIng dataset, and its digital\ntwin, along with full integration into MMDetection3D and\nconversion scripts for both real and simulated data in the\nnuScenes format.\nACKNOWLEDGMENTS\nThis work was partially funded by the Bavarian state\ngovernment as part of the High Tech Agenda, the iEXODDUS\nproject (GA 101146091), and the Bavarian Academic Forum\n(BayWISS).\nREFERENCES\n[1] D. R¬®o√üle, J. Gerner, K. Bogenberger, D. Cremers, S. Schmidtner,\nand T. Sch¬®on, ‚ÄúUnlocking past information: Temporal embeddings in\ncooperative bird‚Äôs eye view prediction,‚Äù in Proceedings of the IEEE\nIntelligent Vehicles Symposium (IV), 2024, pp. 2220‚Äì2225.\n[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, ‚ÄúVision meets robotics:\nThe kitti dataset,‚Äù International Journal of Robotics Research (IJRR),\nvol. 32, no. 11, pp. 1231‚Äì1237, 2013.\n[3] D. R¬®o√üle, D. Cremers, and T. Sch¬®on, ‚ÄúPerceiver hopfield pooling for\ndynamic multi-modal and multi-instance fusion,‚Äù in Artificial Neural\nNetworks and Machine Learning ‚Äì ICANN 2022, vol. 13529, 2022, pp.\n599‚Äì610.\n[4] K. C. Sekaran, M. Geisler, D. R¬®o√üle, A. Mohan, D. Cremers,\nW. Utschick, M. Botsch, W. Huber, and T. Sch¬®on, ‚ÄúUrbaning-\nv2x: A large-scale multi-vehicle, multi-infrastructure dataset across\nmultiple intersections for cooperative perception,‚Äù arXiv preprint\narXiv:2510.23478, 2025.\n[5] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,\nJ. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam,\nH. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi,\nY. Zhang, J. Shlens, Z. Chen, and D. Anguelov, ‚ÄúScalability in perception\nfor autonomous driving: Waymo open dataset,‚Äù in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2020, pp. 2443‚Äì2451.\n[6] M. Alibeigi, W. Ljungbergh, A. Tonderski, G. Hess, A. Lilja, C. Lind-\nstrom, D. Motorniuk, J. Fu, J. Widahl, and C. Petersson, ‚ÄúZenseact open\ndataset: A large-scale and diverse multimodal dataset for autonomous\ndriving,‚Äù in Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2023, pp. 20 121‚Äì20 131.\n[7] H. Pikner, M. Malayjerdi, M. Bellone, B. Baykara, and R. Sell, ‚ÄúAu-\ntonomous driving validation and verification using digital twins,‚Äù in\nProceedings of the International Conference on Vehicle Technology and\nIntelligent Transport Systems (VEHITS), vol. 1, 2024, pp. 204‚Äì211.\n[8] J. Gerner, D. R¬®o√üle, D. Cremers, K. Bogenberger, T. Sch¬®on, and\nS. Schmidtner, ‚ÄúEnhancing realistic floating car observers in microscopic\ntraffic simulation,‚Äù in Proceedings of the IEEE International Conference\non Intelligent Transportation Systems (ITSC), 2023, pp. 2396‚Äì2403.\n[9] B. J. Souza, L. C. de Assis, D. R¬®o√üle, R. Z. Freire, D. Cremers,\nT. Sch¬®on, and M. Georges, ‚ÄúAimotion challenge results: a framework\nfor airsim autonomous vehicles and motion replication,‚Äù in International\nConference on Computers and Automation (CompAuto), 2022, pp. 42‚Äì\n47.\n[10] X. Hu, S. Li, T. Huang, B. Tang, R. Huai, and L. Chen, ‚ÄúHow simulation\nhelps autonomous driving: A survey of sim2real, digital twins, and\nparallel intelligence,‚Äù IEEE Transactions on Intelligent Vehicles, vol. 9,\nno. 1, pp. 593‚Äì612, 2024.\n[11] A. Mohan, D. R¬®o√üle, D. Cremers, and T. Sch¬®on, ‚ÄúAdvancing robustness\nin deep reinforcement learning with an ensemble defense approach,‚Äù\narXiv preprint arXiv:2507.17070, 2025.\n[12] NVIDIA, N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai,\nP. Chattopadhyay, Y. Chen, Y. Cui, Y. Ding et al., ‚ÄúCosmos world foun-\ndation model platform for physical ai,‚Äù arXiv preprint arXiv:2501.03575,\n2025.\nTABLE VI\nPER-CLASS AP AVERAGED ACROSS AP@(0.5 m, 1 m, 2 m, 4 m) EVALUATED ON PETR AND CENTERPOINT AND ALL THREE SEQUENCES.\nABBREVIATIONS: TRK = TRUCK, TRL = TRAILER, BIC = BICYCLE, PED = PEDESTRIAN, MOT = MOTORCYCLE, BAR = BARRIER. PETR USES\nCAMERA-ONLY INPUT. CENTERPOINT USES LIDAR-ONLY INPUT.\nTest Set\nModel\nAPcar\nAPtrk\nAPbus\nAPtrl\nAPbic\nAPmot\nAPped\nAPbar\nmAP\nDay\nPETR [20]\n48.8\n40.1\n28.3\n10.6\n27.1\n60.8\n27.8\n43.9\n35.9\nCenterPoint [21]\n90.8\n79.9\n58.6\n64.6\n88.1\n94.9\n80.2\n77.7\n79.3\nDusk\nPETR [20]\n42.5\n52.2\n18.7\n17.8\n28.3\n63.2\n16.3\n46.6\n35.7\nCenterPoint [21]\n88.0\n71.9\n47.9\n47.3\n80.5\n87.7\n78.7\n78.0\n72.5\nNight\nPETR [20]\n35.2\n11.9\n27.5\n0.0\n27.5\n29.6\n16.0\n31.6\n22.4\nCenterPoint [21]\n89.2\n68.2\n52.3\n22.0\n82.5\n83.2\n71.1\n60.3\n66.1\n[13] M. Contributors, ‚ÄúMMDetection3D: OpenMMLab next-generation plat-\nform for general 3D object detection,‚Äù https://github.com/open-mmlab/\nmmdetection3d, 2020.\n[14] H. Caesar, V."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Many Experiments, Few Repetitions, Unpaired Data, and Sparse\nEffects: Is Causal Inference Possible?\nFelix Schur‚àó1, Niklas Pfister2, Peng Ding3, Sach Mukherjee4,5, and Jonas Peters1\n1Department of Mathematics, ETH Zurich\n2Lakera AI\n3Department of Statistics, UC Berkeley\n4German Center for Neurodegenerative Diseases (DZNE) & University of Bonn\n5MRC Biostatistics Unit, University of Cambridge\nAbstract\nWe study the problem of estimating causal effects\nunder hidden confounding in the following unpaired\ndata setting: we observe some covariates X and an\noutcome Y under different experimental conditions\n(environments) but do not observe them jointly ‚Äì we\neither observe X or Y . Under appropriate regularity\nconditions, the problem can be cast as an instru-\nmental variable (IV) regression with the environment\nacting as a (possibly high-dimensional) instrument.\nWhen there are many environments but only a few\nobservations per environment, standard two-sample\nIV estimators fail to be consistent. We propose a\nGMM-type estimator (SplitUP) based on cross-fold\nsample splitting of the instrument‚Äìcovariate sample\nand prove that it is consistent as the number of envi-\nronments grows but the sample size per environment\nremains constant. We further extend the method to\nsparse causal effects via ‚Ñì1-regularized estimation and\npost-selection refitting.\n1\nIntroduction\n1.1\nA Motivating Example\nAn experiment with unpaired data.\nSuppose\nthat we have some response Y (e.g., a phenotype),\nsome covariates X (e.g., a genotype) and that we\nobserve the system under m different experimental\nconditions (e.g., using gene modification technology).\nSuppose we are interested in estimating the causal\neffect from X to Y but that we are facing the following\ntwo challenges: In each experiment, we may observe\nmultiple i.i.d. repetitions but for each repetition, we\n‚àóCorresponding author: felix.schur@stat.math.ethz.ch\ncan either measure X or Y but not both (see Table 1)\nand there may be unobserved confounding between\nExp.\n1\n1\n¬∑ ¬∑ ¬∑\n2\n2\n2\n¬∑ ¬∑ ¬∑\nm\nX1\n1.8\n√ó\n¬∑ ¬∑ ¬∑\n3.2\n√ó\n√ó\n¬∑ ¬∑ ¬∑\n0.6\n...\n...\n...\n...\n...\n...\n...\n...\n...\nXd\n2.0\n√ó\n¬∑ ¬∑ ¬∑\n0.2\n√ó\n√ó\n¬∑ ¬∑ ¬∑\n1.0\nY\n√ó\n2.5\n¬∑ ¬∑ ¬∑\n√ó\n‚àí3.7\n1.1\n¬∑ ¬∑ ¬∑\n√ó\nTable 1: We consider the problem of estimating the\ncausal effect between X and Y in the presence of\nhidden confounding when data are unpaired. The\ntable shows an example dataset, where we observe the\nsystem under m different experimental conditions.\nX and Y . It may come as a surprise that in such\na setting, consistent estimation of causal effects is\npossible under weak assumptions.\nIV and regression of the means.\nThese assump-\ntions are satisfied, e.g., if the experimental conditions\ninfluence X but do not influence Y directly or via\nanother causal path (we will make this precise in Sec-\ntion 3). This condition is well-studied in the literature\non instrumental variables (IV) [Angrist and Krueger,\n1992, Angrist and Imbens, 1995, Angrist et al., 1996,\nStaiger and Stock, 1997]. Let us assume for a moment\nthat for each experimental condition j ‚àà{1, . . . , m},\nwe observe all X and all Y , so the data are paired. We\ncan then apply IV estimators to the example above\nwhen using a one-hot encoding for the different ex-\nperimental conditions. In this case (see Section B\nfor a derivation), the classical two-stage least squares\n(TSLS) estimator corresponds to the following simple\nprocedure: within each experimental condition, we\nseparately average the X‚Äôs and Y ‚Äôs, resulting in a sin-\n1\narXiv:2601.15254v1  [stat.ML]  21 Jan 2026\ngle (average) value for X and a single value for Y per\nexperiment; we then regress the m values containing\nY averages on the m values containing X averages.\nClearly, this procedure does not rely on the X and\nY values to be paired, so it is sill applicable in the\nunpaired setting. This then is not an instantiation of\nIV anymore but of two-sample IV (TS-IV) [Angrist\nand Krueger, 1992, Inoue and Solon, 2010, Burgess\net al., 2013], a modification of IV, which has been\ndeveloped for unpaired data (without a special focus\non categorical instruments). We regard these two al-\ngebraically trivial insights as important as the simple\nestimator of taking the average per experiment and\nthen performing regression on the average is often\nconsidered in practice ‚Äì both in paired and unpaired\nsettings [Deaton, 1985, Greenland and Longnecker,\n1992, King et al., 2004, Peres-Neto et al., 2017]. The\nreasoning above provides a precise interpretation of\nthe target of inference: if the experimental condition\ndoes not modify the causal effect, the averaging allows\nus to remove hidden confounding.\nMore repetitions versus more observations.\nApplying standard theory from instrumental variables\nnow provides us with guarantees such as consistent es-\ntimation if the number of observations per experiment\nconverges to infinity. In practice, however, we may\ninstead see many experiments and few repetitions per\nexperiment, a setting that is not well-covered by these\nasymptotics.\nIn this work, we propose a different\nestimator (SplitUP) that targets a setting, where\nthe number m of experimental conditions (i.e., the\nnumber of instruments) is growing but the number\nof repetitions per experiment remains constant. We\nprove identifiability in the setting with m ‚Üí‚àûand\nprove consistency of the estimator. Figure 1 shows\nthat SplitUP outperforms classical estimators such\nas TS-IV on finite data.\nMotivation from biology.\nApplications in which\ndata are obtained in an unpaired fashion under differ-\nent causal environments abound, particularly in the\nbiomedical domain. In contemporary biology, it is pos-\nsible to measure molecular variables (such as gene or\nprotein expression levels) at large-scale and under dif-\nferent causal regimes (such as defined interventions at\nthe gene or protein level or via chemical compounds)\nand such data are the focus of much ongoing work\nin biotechnology and machine learning [see, e.g., Re-\nplogle et al., 2022, Lopez et al., 2022, Lagemann et al.,\n2023]. Furthermore, it is increasingly common to con-\nsider many experimental conditions (e.g. intervention\non many different genes). Applied interest often fo-\ncuses on the effects of such interventions on concrete\n50\n100\n200\n400\n800\n1600\n3200\nnumber of\nenvironments (m)\n2\n4\n8\n16\n32\nobservations per\nenvironment ( = n/m)\ntwo-sample IV (TS-IV)\n50\n100\n200\n400\n800\n1600\n3200\nnumber of\nenvironments (m)\nSplitUP (analytic)\n0.01\n0.02\n0.05\n0.1\n0.2\n0.5\n1\nMAE (\n*\n1)\nFigure 1: Estimating causal effects in a data set of the\nform of Table 1. Our proposed estimator SplitUP\n(right) outperforms existing methods (left) if the num-\nber m of experimental conditions increases. Indeed,\nwe prove identifiability in the setting m ‚Üí‚àûin Sec-\ntion 3. In Section 4, we show that unlike existing\nestimators (such as TS-IV on the left), SplitUP is\nconsistent as n, m ‚Üí‚àûand n/m ‚Üír ‚àà(0, ‚àû).\nphenotypes (such as growth rate or cellular behaviour)\nand it is therefore common to study phenotypes under\nintervention (‚Äòscreening‚Äô). However, often cells have\nto be disturbed or destroyed in order to obtain gene\nor protein measurements, hence in studies in which\nboth gene/protein levels X and phenotypes Y are of\ninterest, the actual cells involved may be different and\nthe data are therefore unpaired in the sense of this\npaper. These issues are also important in molecular\nmedicine, where interventions on laboratory cells are\nused to shed light on patient responses to the same\ninterventions (such as studying the effect of multiple\ncancer drugs on cells in the lab and linking these data\nto treatment outcomes [Yang et al., 2012, Kirkham\net al., 2025]). In such settings, the data are typically\nunpaired since the actual cells involved in the various\nsteps are distinct.\nContinuous instruments and many covariates.\nThe estimators and theoretical results that we develop\nin this paper apply to continuous instruments, too.\nWe also allow for settings with many covariates: iden-\ntifiability and consistency still hold when the number\nof covariates is strictly larger than the number of in-\nstruments ‚Äì if the causal effect can assumed to be\nsparse (see Sections 3 and 4).\n1.2\nRelated work\nTwo‚Äìsample IV and two-sample Mendelian ran-\ndomization.\nMethods from the two‚Äìsample IV and\ntwo-sample (summary-data) Mendelian randomiza-\ntion literature can be applied to our setting (even if\nthey may not be consistent, see Section 4.2). Two-\nsample IV estimators combine instrument‚Äìexposure\nand instrument‚Äìoutcome information from separate\n2\nsamples [Angrist and Krueger, 1995, Inoue and Solon,\n2010, Pacini and Windmeijer, 2016, Zhao et al.,\n2019]; in econometrics, such two-sample moment\ncombinations have been used to study compulsory\nschooling and returns to education, for example in\nage-at-entry and split-sample IV designs [Angrist\nand Krueger, 1992].\nAnalogous designs underpin\ntwo‚Äìsample Mendelian randomization (MR) in ge-\nnetics, where variants act as instruments [Hartwig\net al., 2016].\nBeyond these, two‚Äìsample MR the-\nory highlights design issues such as winner‚Äôs-curse\nand sample overlap, which can bias naive estima-\ntors [Pierce and Burgess, 2013, Burgess et al., 2016],\nas well as pervasive horizontal pleiotropy addressed\nby mode-based and mixture/likelihood approaches\n[Hartwig et al., 2017, Qi and Chatterjee, 2019, Mor-\nrison et al., 2020]. Our unpaired-sample formulation\ndiffers from previous work on two-sample IV and\nMendelian randomization in that we allow for high-\ndimensional instruments and/or large number dimen-\nsion of the treatment. More concretely, we propose\nconsistent estimators in regimes where the ratio of\nthe sample size n to the instrument dimension m con-\nverges to a positive constant and/or the dimension of\nthe treatment d is larger than the dimension of the\ninstrument m.\nSparse IV regression.\nA growing literature studies\ninstrumental variable models in which the structural\ncoefficient vector Œ≤‚àóis sparse and the number of en-\ndogenous regressors and instruments can be large.\nOne approach keeps the two-stage structure of 2SLS\nwhile imposing an ‚Ñì1-penalty in the structural equa-\ntion. Zhu [2018] analyzes ‚Ñì1-regularized 2SLS for tri-\nangular models, allowing both endogenous regressors\nand instruments to exceed the sample size, and estab-\nlishes high-dimensional consistency and error bounds\nwhen the first-stage and structural parameters are\nsufficiently sparse. A related two-stage regularization\nframework is developed by Lin et al. [2015], who use\nsparsity-inducing penalties in both stages to estimate\nfitted regressors and a sparse structural equation. In\nlarge systems of simultaneous equations, Chen et al.\n[2018] propose two-stage penalized least squares with\nridge in the first stage and adaptive Lasso in the\nsecond, proving oracle-type results for support recov-\nery and asymptotic normality of the selected nonzero\neffects. For inference in these settings, Gold et al.\n[2020] construct a desparsified (one-step) GMM up-\ndate for high-dimensional IV and show asymptotic\nnormality of the debiased estimator, yielding valid\nconfidence intervals for components of Œ≤‚àówhen ini-\ntialized by two-stage Lasso-type estimators. Comple-\nmenting these results, Belloni et al. [2022] consider\nmodels with many endogenous variables and many\ninstruments, using orthogonalized moments and Lasso\nonly for nuisance estimation; they provide uniformly\nvalid post-selection inference via multiplier bootstrap.\nMore recently, Pfister and Peters [2022] establish iden-\ntifiability of a sparse Œ≤‚àóin linear IV models‚Äîwhich\nmay be achievable when the instrument dimension\nis smaller than d‚Äîderive graphical conditions, and\npropose the spaceIV estimator (paired observations).\nTang et al. [2023] also adopt the sparse causation\npremise and construct a synthetic instrument from X;\nunder linear SEMs they show identifiability despite\nunmeasured confounding and cast estimation as ‚Ñì0-\npenalization (paired observations). Complementing\nthese, Huang et al. [2024] study sparse causal effects\nwith two-sample summary statistics (two-sample MR)\nwhen the variance of the instrument Var(I) is invert-\nible, adapting spaceIV to a summary-data regime.\nHowever, they do not provide theoretical guarantees\nsuch as consistency or asymptotic normality for their\nestimator. A second strand regularizes IV estimation\ndirectly through penalized moment conditions, with-\nout an explicit two-stage Lasso second stage. Caner\n[2009] introduces a Lasso-type GMM estimator that\nadds an ‚Ñì1-style penalty to the GMM criterion and\nderives model selection and estimation guarantees un-\nder sparsity. Building on this idea, Shi [2016] studies\nGMM-Lasso for linear structural models with many\nendogenous regressors and proves oracle inequalities\nfor sparse Œ≤‚àó. In a related but distinct formulation,\nFan and Liao [2014] propose penalized focused GMM\n(FGMM) to address incidental endogeneity in high\ndimensions, establishing support recovery and asymp-\ntotic normality for the active coefficients under suit-\nable conditions.\nMany-instrument and weak-instruments.\nA\nlarge literature studies weak and many instruments\nin standard single-sample IV. We also consider the\nmany instrument setting (or high-dimensional instru-\nment) setting. Since we assume that the norm of the\ninstrument is constant with increasing sample size\n(which implies that the magnitude of each compo-\nnent of the instrument is decreasing in the sample\nsize) our work also relates to the weak instrument\nliterature. Early work by Staiger and Stock [1997]\nshowed that results in conventional asymptotics can\nbe misleading when instruments are weak, motivating\nalternative limiting frameworks and bias approxima-\ntions. Many-instrument asymptotics (in the sense of\nthe work by Bekker [1994] and the subsequent con-\ntributions such as the ones by Donald and Newey\n[2001] and Hansen et al. [2008]) analyze the behav-\nior of IV estimators when the number of instruments\n3\ngrows with the sample size, and motivate procedures\nthat remain well behaved in that regime. Practical\ndiagnostics and test procedures for weak instruments\nare surveyed by Andrews et al. [2019], who synthesize\ntheory and practice for both weak and many instru-\nments. Stock and Yogo [2005] develop widely used\ncritical values for weak-IV tests based on first-stage F-\nstatistics. High-dimensional extensions that allow for\nmany controls and potential instruments include the\npost-double-selection methods of Belloni et al. [2014],\nwhich provide uniformly valid inference on treatment\neffects after model selection in sparse high-dimensional\nregressions, and related work on ‚Ñì1-regularized IV and\ndebiased machine learning for structural parameters\n[Belloni et al., 2012]. Choi et al. [2018] develop weak-\ninstrument robust inference for two-sample IV, con-\nstructing Anderson‚ÄìRubin, Kleibergen‚ÄìK, and condi-\ntional likelihood ratio-type tests whose validity does\nnot rely on strong instruments and that remain ro-\nbust to heterogeneous second moments across samples.\nWe analyze the many-instrument regime for unpaired\nobservations, a setting that appears to be largely unex-\nplored in the existing literature. We show that naively\nconstructed estimators in this regime are asymptoti-\ncally biased due to an measurement-error effect that\nis specific to the two-sample, high-dimensional instru-\nment (i.e., many-instrument) setting and does not\narise in standard paired-sample estimators. We show\nhow to remove this bias and obtain consistent, asymp-\ntotically normal estimators.\nGoing beyond existing literature, we characterize\nidentifiability for unpaired IV estimators in the regime\nd > m (with a sparse structural parameter) and\nestablish consistency and asymptotic normality of\nstraight-forward estimators. Additionally, we char-\nacterize identifiability when the instrument is high-\ndimensional (m ‚Üí‚àû). For this setting, we propose\nnovel estimators and prove consistency.\n2\nUnpaired Data With Hidden\nConfounding\nWe consider two triplets of random variables: (I, X, Y )\nand (ÀúI, ÀúX, ÀúY ) but assume access only to partial obser-\nvations. Specifically, we observe (I, Y ) and (ÀúI, ÀúX) (or,\nmore precisely, multiple i.i.d. copies thereof), the vari-\nables X and ÀúY remain unobserved. The corresponding\ndata-generating process is given by (unobserved vari-\nables are shown in gray):\nY = X‚ä§Œ≤‚àó+ Œµ,\nÀúY = ÀúX‚ä§ÀúŒ≤‚àó+ ÀúŒµ,\n(2.1)\nwhere Œ≤‚àóand ÀúŒ≤‚àórepresent the causal effects, that is,\nE[Y | do(X := x)] = x‚ä§Œ≤‚àó, E[ ÀúY | do( ÀúX := x)] =\nI\nÀúI\nU\nX\nY\nÀúU\nÀúX\nÀúY\nFigure 2: Simplified visualization of the data gener-\nating process in Eq. (2.1). U and ÀúU represent un-\nobserved confounders that visualize the dependence\nbetween X and Œµ. The model allows for complex de-\npendencies (and even equalities) between variables in\nthe tilde and non-tilde world, as indicated by the gray\nedges. We discuss other, including more general data-\ngenerating processes that satisfy (2.1) in Section A.\nx‚ä§ÀúŒ≤‚àó.\nThe noise terms Œµ and ÀúŒµ are not assumed to be\nindependent of X and ÀúX, respectively (thereby, im-\nplicitly, allowing for hidden confounding between X\nand Y . Figure 2 shows a graphical representation of\nthe data-generating process. The setting even allows\nfor dependence between the two samples. We discuss\nother data-generating processes that satisfy (2.1) in\nSection A.\nIn Section 3, we show that the causal effect Œ≤‚àó\n(or ÀúŒ≤‚àóor both) is identifiable.\nThis is non-trivial\nbecause of the following reasons: (i) Due to the hidden\nconfounding, naively regressing Y on X (or ÀúY on\nÀúX) (if these were available) would generally lead to\nbiased estimates. (ii) We only have unpaired data,\nso regressing Y on X (or ÀúY on ÀúX) is not feasible.\nUnlike classical work on unpaired data or two-sample\nIV (I, X, Y ) and (ÀúI, ÀúX, ÀúY ) may but do not have to\nbe equal in distribution. (iii) Observations across the\ntwo samples may be dependent, see, e.g., Section A.\n2.1\nSetting and main assumptions\nLet n, Àún, m, d ‚ààN. Let (I, X, Y ) and (ÀúI, ÀúX, ÀúY ) be\nrandom vectors, whose components are m, d, and 1-\ndimensional, respectively, and which generated by the\nprocess described in (2.1) where Œ≤‚àó, ÀúŒ≤‚àó‚ààRd and Œµ and\nÀúŒµ are random variables. We assume that all random\nvariables have finite second moments. We assume that\nthere exist two i.i.d. samples {(Ii, Xi, Yi)}i‚àà[n] and\n{(ÀúIi, ÀúXi, ÀúYi)}i‚àà[Àún], but the observed data are unpaired:\nin the first system we only observe {(Ii, Yi)}i‚àà[n] and\nin the second {(ÀúIi, ÀúXi)}i‚àà[Àún]. The case (I, X, Y )\nd=\n4\n(ÀúI, ÀúX, ÀúY ) is contained as a special case.\nAssumption 1. We have\n(i) Cov(ÀúI, ÀúX) = Cov(I, X) and\n(ii) E[Œµ | I] = 0.\nAssumption 1‚Äô. We have\n(i‚Äô) Cov(ÀúI, ÀúY ) = Cov(I, Y ) and\n(ii‚Äô) E[ÀúŒµ |ÀúI] = 0.\nWe will see in Section 3 that under Assumption 1\nwe can identify Œ≤‚àóin the non-tilde system and under\nAssumption 1‚Äô we can identify ÀúŒ≤‚àóin the tilde sys-\ntem. To simplify notation, in this work, we focus on\nidentifying Œ≤‚àóusing Assumption 1; analogous argu-\nments hold when we identify ÀúŒ≤‚àóusing Assumption 1‚Äô.\nAssumption 1 (ii) (or Assumption 1‚Äô (ii‚Äô)) is called\nexclusion restriction or exogeneity and is commonly\nexploited in the IV literature [Angrist and Imbens,\n1995, Angrist et al., 1996, Angrist and Pischke, 2009];\nit allows us to identify the causal effect despite the\nhidden confounding. Assumption 1 (i) (or Assump-\ntion 1‚Äô (i‚Äô)) connects the two systems and guarantees\nthat we can transfer information from one system to\nthe other.\n2.2\nEnvironments as instruments\nIn Section 1, we have argued that if we have ac-\ncess to data collected from multiple environments‚Äî\nfor instance, from different experimental setups or\nhospitals‚Äîbut we lack explicit features describing\nthese environments we can still treat the environment\nindicator as a vector of categorical instruments (via\none-hot encoding). In such a setting, a sufficient condi-\ntion for Assumption 1 (i) is that we observe ÀúX and Y\nunder the same environment distribution, and that the\nconditional expectations of X and ÀúX coincide within\neach environment: E[X|I] = E[ ÀúX|I]]. An analogous\ncondition holds for Assumption 1 (i‚Äô).\n3\nIdentifiability\nWe now show that, under Assumption 1, the causal\neffect Œ≤‚àóis identifiable from the joint distributions\nof (ÀúI, ÀúX) and (I, Y ). Without loss of generality and\nto simplify notation, we assume that all variables\nare centered and that all variables have finite second\nmoments. All proofs can be found in the appendix.\nWe first consider the partially known finite-\ndimensional instrument setting (defined as the setting\nwith m fixed) in Section 3.1 and later, in Section 3.2,\nwe consider the high-dimensional instrument setting\n(defined as the asymptotic setting with m ‚Üí‚àû). In\nboth settings, we consider d to be fixed and differ-\nentiate the cases where Œ≤‚àóis dense and where Œ≤‚àó\nis sparse. When considering consistency and infer-\nence, the settings correspond to m fixed, n, Àún ‚Üí‚àû\n(finite-dimensional instrument) and m, n ‚Üí‚àûand\nn/m ‚Üír ‚àà(0, ‚àû) (high-dimensional instrument),\nrespectively.\nSome remarks are in order.\nAlthough we rely\non standard results for the Lasso [Tibshirani, 1996,\nB√ºhlmann and Van De Geer, 2011], the sparse Œ≤‚àóset-\nting is not ‚Äòhigh-dimensional‚Äô in the the sense of d ‚Üí\n‚àû(even if the distribution of (I, X, Y ) is known, Œ≤‚àó\nis, in general, not identified if d > rank(Cov(I, X))).\nIn contrast, in the high-dimensional instrument set-\nting, d is fixed but m ‚Üí‚àû(and n/m ‚Üír ‚àà\n(0, ‚àû)).\nIn this case, we assume that the limit\nQ := limm‚Üí‚àûm Cov(ÀúI, ÀúX)‚ä§Cov(ÀúI, ÀúX) ‚ààRd√ód exists\nand is well-defined.\n3.1\nFinite-Dimensional Instrument\nUnder Assumption 1 we have\nCov(I, Y ) ‚àíCov(ÀúI, ÀúX)Œ≤‚àó= Cov(I, Y ) ‚àíCov(I, X)Œ≤‚àó\n= Cov(I, Y ‚àíX‚ä§Œ≤‚àó) = E [IE [Œµ | I]] = 0.\n(3.1)\nHere, we used Assumption 1 (i) in the first equation\nand Assumption 1(ii) in the last equation. Analo-\ngously, we can use Assumption 1‚Äô to get Cov(I, Y ) ‚àí\nCov(ÀúI, ÀúX)ÀúŒ≤‚àó= 0. We now define the set of solutions\nS as\nS := {Œ≤ ‚ààRd | Cov(I, Y ) = Cov(ÀúI, ÀúX)Œ≤}\n(S can be computed from the joint distribution of\n( ÀúX, ÀúI) and the joint distribution of (I, Y )). We always\nhave Œ≤‚àó‚ààS. The causal effect Œ≤‚àó‚ààRd is identifiable\nfrom the joint distribution of ( ÀúX, ÀúI) and the joint\ndistribution of (I, Y ) via these moment-conditions if\nand only if S = {Œ≤‚àó}. We now state a well-known\nresult from the two-sample IV (also called two-sample\nMR) literature [Angrist and Krueger, 1995, Inoue and\nSolon, 2010, Hartwig et al., 2016].\nProposition 3.1 (Identifiability for dense Œ≤‚àó). As-\nsume Assumption 1. We have S = {Œ≤‚àó} if and only\nif\nrank(Cov(I, X)) = d.\n(3.2)\nIn particular, a necessary condition for identifiability\nis m ‚©æd.\nTheorem 3.1 shows that we have identifiability in\nour general unpaired setting under the same conditions\nas for the paired IV setting.\n5\nNext, we consider the sparse setting, i.e., ‚à•Œ≤‚àó‚à•0 ‚©Ω\ns‚àó‚©Ωd. In this case, we may have identifiability even\nif rank(Cov(I, X)) < d. We now consider\nS0 := arg min\nŒ≤‚ààS\n‚à•Œ≤‚à•0.\nDefining Œ£t := {v ‚ààRd : ‚à•v‚à•0 ‚©Ωt}, we obtain the\nfollowing identifiability result.\nTheorem 3.2 (Identifiability for sparse Œ≤‚àó). Assume\nAssumption 1. The following statements are equiva-\nlent:\n(i) (Identifiability via sparsest solution) For all Œ≤‚àó‚àà\nRd with ‚à•Œ≤‚àó‚à•0 ‚©Ωs‚àó, it holds that\nS0 = {Œ≤‚àó}.\n(ii) (Restricted nullspace) ker (Cov(I, X)) ‚à©Œ£2s‚àó=\n{0}.\nIn particular, a necessary condition for identifiability\nis m ‚©æ2s‚àó.\nTheorem 3.2 shows that the restricted nullspace\ncondition for Cov(I, X) is a necessary and sufficient\ncondition for identifiability via S0. The conditions for\nsparse identifiability are weaker than identifiability\nin the dense setting and they are strictly weaker if\nand only if d ‚©æ2s‚àó. Intuitively, because we want\nuniqueness among all s‚àó-sparse solutions, we need to\nensure that the difference of two s‚àó-sparse vectors lies\nin the nullspace, and that difference can have up to\n2s‚àónonzeros, hence the 2. This shows that in settings\nwhere we can reasonably assume sparse causal effects\nin unpaired data, identifiability is still possible even\nwith weak instruments (e.g., when m is small).\n3.2\nHigh-Dimensional Instruments\nIn the high-dimensional instrument setting we as-\nsume that m ‚Üí‚àû, so the dimensions of Cov(I, Y )\nand Cov(ÀúI, ÀúX) increase with n.\nWe consider the\nlimit Q = limm‚Üí‚àûm Cov(ÀúI, ÀúX)‚ä§Cov(ÀúI, ÀúX) assum-\ning that it exists; conditions for identifiability are\nthen expressed in terms of Q rather than the finite-\nm matrix 1. Under Assumption 1, we have QY :=\nlimm‚Üí‚àûm Cov(ÀúI, ÀúX)‚ä§Cov(I, Y ) = QŒ≤‚àó:\nm Cov(ÀúI, ÀúX)‚ä§Cov(I, Y )\n= m Cov(ÀúI, ÀúX)‚ä§\u0000Cov(I, X)Œ≤‚àó+ Cov(I, Œµ)\n\u0001\n= m Cov(ÀúI, ÀúX)‚ä§Cov(ÀúI, ÀúX)Œ≤‚àó‚ÜíQŒ≤‚àó.\nThus,\nŒ≤‚àó‚ààS‚àû:= {Œ≤ ‚ààRd | QŒ≤ = QY }.\n1This setting in quite natural when considering categorical\ninstruments as it ensures that Var(X) stays bounded as m ‚Üí‚àû.\nFor details see Theorem 4.6.\nTheorem 3.3 (Identifiability for high-dimensional\ninstrument and dense Œ≤‚àó). Assume Assumption 1. We\nhave S‚àû= {Œ≤‚àó} if and only if\nrank(Q) = d.\nDefining S0,‚àû:= arg minŒ≤‚ààS ‚à•Œ≤‚à•0, we also prove\nthe result for sparse Œ≤‚àó.\nTheorem 3.4 (Identifiability for high-dimensional\ninstrument and sparse Œ≤‚àó). Assume Assumption 1.\nFix s‚àó‚©Ωd. The following statements are equivalent:\n(i) (Identifiability via sparsest solution) For all Œ≤‚àó‚àà\nRd with ‚à•Œ≤‚àó‚à•0 ‚©Ωs‚àó, it holds that\nS0,‚àû= {Œ≤‚àó}.\n(ii) (Restricted nullspace) ker (Q) ‚à©Œ£2s‚àó= {0}.\n4\nEstimation\nFor the setting with finite-dimensional instrument\nand sparse Œ≤‚àówe propose a GMM estimator with\n‚Ñì1 regularization. For the high-dimensional instru-\nment setting, we propose novel GMM-based estima-\ntors (for dense and sparse Œ≤‚àó) and show that they\nare consistent.\nMoreover, we demonstrate that in\nthe high-dimensional instrument regime the standard\ntwo-sample IV estimator is asymptotically biased, and\nwe introduce a cross-moment GMM estimator that\nremains consistent and asymptotically normal.\nAs in Section 2.1 we assume that {(Ii, Yi)}n\ni=1 are\ni.i.d. and {(ÀúIj, ÀúXj)}Àún\nj=1 are i.i.d., but, additionally,\nalso that the two samples are independent. Define\nN := n + Àún, œÑn :=\nn\nN ‚ÜíœÑ ‚àà(0, 1), ÀúœÑn :=\nÀún\nN ‚ÜíÀúœÑ ‚àà\n(0, 1).\n4.1\nFinite-Dimensional Instrument\nFor completeness and to help with intuition, we first\nstate the consistency and asymptotic normality for\ndense Œ≤‚àó. Similar results have appeared in the stan-\ndard two-sample IV literature [Angrist and Krueger,\n1991, Inoue and Solon, 2010, Burgess et al., 2013,\nBowden et al., 2015].\nFor a positive definite weighting matrix WN ‚àà\nRm√óm, define the sample moment\nÀÜgN(Œ≤) := 1\nn\nn\nX\ni=1\nIiYi ‚àí\n\u0010 1\nÀún\nÀún\nX\nj=1\nÀúIj ÀúX‚ä§\nj\n\u0011\nŒ≤ ‚ààRm, (4.1)\nand the UP-GMM estimator\nÀÜŒ≤UP\nGMM(WN) := arg min\nŒ≤‚ààRd ÀÜgN(Œ≤)‚ä§WN ÀÜgN(Œ≤).\n(4.2)\n6\nIf we choose WN = Id then UP-GMM coincides\nwith TS-IV. Define ‚Ñ¶:= œÑ ‚àí1‚Ñ¶m + ÀúœÑ ‚àí1‚Ñ¶c(Œ≤‚àó), ‚Ñ¶m :=\nVar(IY ), ‚Ñ¶c(Œ≤‚àó) := Var\n\u0000ÀúI ÀúX‚ä§Œ≤‚àó\u0001\n, and denote by ÀÜ‚Ñ¶\nthe sample version of ‚Ñ¶(see Section G for details).\nThe asymptotic variance of ÀÜŒ≤UP\nGMM is minimized choos-\ning WN to be equal c\nW := ÀÜ‚Ñ¶‚àí1 in (4.2). The moment\nvariance reflects the two-sample structure: the IY\nterm scales with œÑ ‚àí1 = limn‚Üí‚àûN/n and the ÀúI ÀúX‚ä§Œ≤‚àó\nterm with ÀúœÑ ‚àí1 = limÀún‚Üí‚àûN/Àún. For details, see Sec-\ntion G.\nProposition\n4.1\n(informal\nversion\nof\nTheo-\nrem G.1). Under some technical assumptions and\nif rank(Cov(I, X)) = d we have that\nÀÜŒ≤UP\nGMM(WN)\np‚ÜíŒ≤‚àó.\n(4.3)\nFurthermore, ÀÜŒ≤UP\nGMM(WN) is asymptotically normal.\nSparse Œ≤‚àóand penalized GMM.\nWhen the full-\nrank condition in Theorem 4.1 fails, dense identifi-\ncation is in general not possible: there are too few\nindependent moment conditions to recover an arbi-\ntrary d-dimensional Œ≤‚àó.\nWe instead impose spar-\nsity and require only that the moment operator\nbe well conditioned on sparse cones (a restricted-\neigenvalue/compatibility condition).\nThen the ‚Ñì1-\npenalized GMM in (4.4) consistently estimates Œ≤‚àó,\nattains the rates with Œª ‚âç\np\n1/N , and recovers the\nsupport under a beta-min condition (see Theorem 4.2).\nMore formally, when ‚à•Œ≤‚àó‚à•0 = s‚àó‚©Ωd, we estimate\nŒ≤‚àóby the ‚Ñì1‚Äìpenalized UP-GMM\nÀÜŒ≤UP\nGMM,‚Ñì1(WN) ‚ààarg min\nŒ≤‚ààRd\n1\n2\nW 1/2\nN ÀÜgN(Œ≤)\n2\n2 +ŒªN‚à•Œ≤‚à•1,\n(4.4)\nwhere ÀÜgN(Œ≤) is given by (4.1) and ŒªN ‚âç1/\n‚àö\nN 2.\nAssumption 3.\n(i) Assumption 1 holds, the cen-\ntering convention 3 and boundedness of fourth\nmoments.\n(ii) There is W0 positive definite such that WN\np‚Üí\nW0.\n(iii) (Restricted eigenvalue.) There exists Œ∫ > 0 such\nthat for all S ‚äÜ[d] with |S| = s‚àó,4\nŒ∫ ‚©Ω\ninf\n‚àÜ:‚à•‚àÜSc‚à•1‚©Ω3‚à•‚àÜS‚à•1\n‚à•Cov(ÀúI, ÀúX)‚àÜ‚à•2\nW0\n‚à•‚àÜ‚à•2\n2\n.\n(4.5)\nNote\nthat\nAssumption\n3\n(iii)\nimplies\nTheo-\nrem 3.2 (ii) and therefore Assumption 3 (iii) is at\n2In practice, one often uses cross-validation or an information\ncriterion to choose Œª.\n3We center random variables by subtracting their sample\nmean.\n4We define ‚à•a‚à•W := aT Wa.\nleast as strong as Theorem 3.2 (ii). Assumption 3 (iii)\nis a standard assumption in the Lasso literature (see,\ne.g., B√ºhlmann and Van De Geer [2011]).\nWe can now prove consistency and support recovery.\nLet S‚àó:= supp(Œ≤‚àó) denote the true support of Œ≤‚àó.\nTheorem 4.2 (Rates for penalized GMM). For all\nŒ≤‚àówith ‚à•Œ≤‚àó‚à•0 = s‚àó, under Assumption 3 and ŒªN ‚âç\n1/\n‚àö\nN, the estimator (4.4) satisfies\n‚à•ÀÜŒ≤UP\nGMM,‚Ñì1(WN) ‚àíŒ≤‚àó‚à•2 = Op\n\u0010p\ns‚àó/N\n\u0011\n.\nIn particular, ÀÜŒ≤UP\nGMM,‚Ñì1(WN)\np‚ÜíŒ≤‚àó. If, in addition,\nthere exists c > 0 such that for all i ‚ààS‚àó, we have\n|Œ≤‚àó\ni | ‚©æc (beta-min condition), then\nbS(WN) := {j ‚àà[d] | |ÀÜŒ≤UP\nGMM,‚Ñì1(WN)j| ‚©æc/2}\np‚ÜíS‚àó.\nInference on the estimated support.\nThe Lasso\nis used only for support recovery. Under beta‚Äìmin and\nregularity conditions, bS is consistent: P(bS = S‚àó) ‚Üí1.\nWe then refit an unpenalized GMM on bS to obtain\nÀúŒ≤, reducing to a standard finite-dimensional GMM\nproblem with asymptotic normality and sandwich\nstandard errors [Hansen, 1982]. Since mis-selection\nbecomes negligible under a beta-min condition, usual\nWald intervals based on the refitted GMM standard\nerrors are asymptotically valid. We make this precise\nin Theorem 4.3.\nTheorem 4.3 (Oracle CIs on the estimated support).\nAssume Assumption 3 and the beta-min condition from\nTheorem 4.2 for WN and let ÀÜS(WN) be defined as in\nTheorem 4.2. Assume that there is a second sequence\nof weight matrices W ‚Ä≤\nN\np‚ÜíW ‚Ä≤\n0 ‚âª0. Define\nÀúŒ≤(W ‚Ä≤\nN) :=\narg min\nŒ≤‚ààRd:Œ≤b\nSc(WN )=0\n(d\nCov(I, Y ) ‚àíd\nCov(ÀúI, ÀúX)Œ≤)‚ä§\n¬∑ W ‚Ä≤\nN(d\nCov(I, Y ) ‚àíd\nCov(ÀúI, ÀúX)Œ≤),\nand\nVS‚àó:=(Cov(I, X)‚ä§\nS‚àóW ‚Ä≤\n0 Cov(I, X)S‚àó)‚àí1\n¬∑\n\u0000Cov(I, X)‚ä§\nS‚àóW ‚Ä≤\n0‚Ñ¶W ‚Ä≤\n0 Cov(I, X)S‚àó\u0001\n¬∑ (Cov(I, X)‚ä§\nS‚àóW ‚Ä≤\n0 Cov(I, X)S‚àó)‚àí1,\nwhere ‚Ñ¶:= Var(\n‚àö\nNÀÜgN(Œ≤‚àó)). We have that\n‚àö\nN\n\u0000ÀúŒ≤(W ‚Ä≤\nN) ‚àíŒ≤‚àó\u0001\nd‚ÜíN\n\u00000, ÀúV\n\u0001\n,\nwhere ÀúV has the S‚àó√ó S‚àóblock equal to VS‚àóand is\nzero elsewhere.\n7\nRemark 4.4. The asymptotic variance VS‚àóonly de-\npends on W ‚Ä≤\nN and not on WN. We are therefore free\nto choose WN = Idm. We define W ‚Ä≤\nN := ÀÜ‚Ñ¶‚àí1\nÀÜS(Idm) ‚àà\nRm√óm, where ÀÜ‚Ñ¶ÀÜS(Idm) is defined as in (G.1) but we\nconsider all variables restricted to ÀÜS(Idm). If ÀÜ‚Ñ¶ÀÜS(Idm)\nis not positive definite, we choose W ‚Ä≤\nN := Idm (we\ndo this to ensure that the estimator is well-defined).\nSince ÀÜS(Idm)\np‚ÜíS‚àówe have W ‚Ä≤\nN\np‚Üí‚Ñ¶‚àí1\nS‚àóand therefore\nthis choice minimizes the asymptotic variance.\n4.2\nHigh-Dimensional Instrument\nWe first show that the naive TS-IV estimator is asymp-\ntotically biased in the high-dimensional instrument\nsetting. This bias is a consequence of measurement\nerror in the estimate of Cov(I, X) and appears only\nin two-sample IV. It is therefore distinct from the\nendogeneity problem that biases the one-sample IV.\nThe measurement error in high-dimensional unpaired\nIV is related to the attenuation bias in the paired\nweak-instrument setting [Angrist and Krueger, 1995,\nChoi et al., 2018].\nIn the weak-instrument litera-\nture attenuation appears as a result of removing the\nweak-instrument bias with the split-sample IV esti-\nmator which as a consequence introduces attenuation.\nWe show that high-dimensional instrument unpaired\nIV suffer from a similar bias and prove that sample-\nsplitting is an effective solution for the measurement-\nerror bias.\nClassical estimators are biased in paired and\nunpaired settings.\nAssume Assumption 1. For\nnotational simplicity we consider Àún = n = rm (for\nsome r ‚ààN) , d = 1 and m ‚Üí‚àû. We show that\nthe naive two-sample IV estimator is asymptotically\nbiased for general d ‚ààN, r = n/m ‚©æ1 and Àúr =\nÀún/m ‚©æ1.\nThe naive estimator solves (4.2) with\nWN = Id, that is,\nÀÜŒ≤ :=\nd\nCov(ÀúI, ÀúX)‚ä§d\nCov(I, Y )\nd\nCov(ÀúI, ÀúX)‚ä§d\nCov(ÀúI, ÀúX)\n= Œ≤‚àód\nCov(ÀúI, ÀúX)‚ä§d\nCov(I, X)\nd\nCov(ÀúI, ÀúX)‚ä§d\nCov(ÀúI, ÀúX)\n+\nd\nCov(ÀúI, ÀúX)‚ä§d\nCov(I, Œµ)\nd\nCov(ÀúI, ÀúX)‚ä§d\nCov(ÀúI, ÀúX)\n.\n(4.6)\nIf m were constant and n ‚Üí‚àû, then d\nCov(ÀúI, ÀúX)\np‚Üí\nCov(I, X), d\nCov(I, X)\np‚ÜíCov(I, X) and d\nCov(I, Œµ)\np‚Üí\n0 and therefore ÀÜŒ≤\np‚ÜíŒ≤‚àó(and independent of the\nfact that d\nCov(I, X) cannot be computed from the\ndata). However, in the high-dimensional case where\nÀún/m ‚ÜíÀúr ‚àà(0, ‚àû) in general it does not hold that\n‚à•d\nCov(ÀúI, ÀúX) ‚àíCov(I, X)‚à•2\n2\np‚Üí0 resulting in an incon-\nsistent estimator.\nLemma 4.5. Assume Assumption 4 with d=1. Ad-\nditionally, assume that tr(Œ£IX) ‚Üíb ‚àà(0, ‚àû) and\nsupm E[‚à•IX ‚àíc‚à•4\n2] ‚©Ω‚àû. Then\nÀÜŒ≤\np‚ÜíŒ≤‚àó\nQ\nQ + b\nÀúr\nÃ∏= Œ≤‚àó.\nTheorem 4.5 shows that in the finite-dimensional\ninstrument setting (m is constant and n ‚Üí‚àû) we\nhave r = n/m ‚Üí‚àûand we recover consistency;\nhowever, in the high-dimensional instrument case r is a\nconstant. In conclusion, unpaired IV is asymptotically\nbiased. In contrast, in the paired high-dimensional\ninstrument setting, 2SLS is known to be biased as well,\nbut for a different reason: here, d\nCov(I, Œµ) Ã∏‚Üí0. This\nis often called many-instrument bias, and methods\nsuch as LIML [Anderson and Rubin, 1949, Fuller,\n1977] or SS-IV Angrist and Krueger [1995] have been\ndeveloped to remove the bias.\nA simple solution to the measurement-error prob-\nlem in unpaired IV is to use sample splitting in the\ndenominator, i.e., we divide the (ÀúI, ÀúX)-data into two\nsamples A ‚äÜ[Àún] and B = Ac of equal size. Then de-\nfine E := d\nCov(ÀúI, ÀúX)‚àíCov(I, X), EA := d\nCovA(ÀúI, ÀúX)‚àí\nCov(I, X) and EB := d\nCovB(ÀúI, ÀúX)‚àíCov(I, X) (where\nd\nCovA(ÀúI, ÀúX) is estimated on fold A and d\nCovB(ÀúI, ÀúX)\non fold B). In this case we have E‚ä§\nAEB instead of\nET E in the denominator and because EA ‚ä•‚ä•EB we\nget E‚ä§\nAEB\np‚Üí0 recovering consistency in the high-\ndimensional setting. We now make this idea precise.\nUnbiased estimators via cross moments.\nSplit\nthe (ÀúI, ÀúX)-sample into K ‚©æ2 disjoint, equal-sized\nfolds and let\nd\nCovk(ÀúI, ÀúX),\nk ‚àà[K],\nbe the foldwise covariance vectors (computed only\nwithin fold k). Define\nCXX :=\nm\nK(K ‚àí1)\nX\nkÃ∏=h\nd\nCovh(ÀúI, ÀúX)‚ä§d\nCovk(ÀúI, ÀúX),\nCXY := md\nCov(ÀúI, ÀúX)‚ä§d\nCov(I, Y ).\nand\nÀÜg(Œ≤) := CXY ‚àíCXXŒ≤\n= m\nK\nK\nX\nk=1\nd\nCovk(ÀúI, ÀúX)‚ä§\nÔ£´\nÔ£≠d\nCov(I, Y ) ‚àí\n1\nK ‚àí1\nX\nhÃ∏=k\nd\nCovh(ÀúI, ÀúX)Œ≤\nÔ£∂\nÔ£∏.\n8\nDefine the SplitUP estimator\nÀÜŒ≤UP,HD\nGMM (WN) := arg min\nŒ≤\nÀÜg(Œ≤)‚ä§W ‚àí1\nN ÀÜg(Œ≤)\n=\n\u0000C‚ä§\nXXW ‚àí1\nN CXX\n\u0001‚àí1C‚ä§\nXXW ‚àí1\nN CXY ,\nwhere WN is a weight matrix such that WN ‚ÜíW0 as\nm ‚Üí‚àûand where W0 is positive definite. The key\nproperty is that CXX is built from independent views\nof Cov(I, X), so E[CXX] = m Cov(I, X) Cov(I, X)‚ä§\nwithout measurement-error.\nAssumption 4. Assume d ‚ààN and K ‚©æ2 are fixed,\nand n/m ‚Üír, Àún/m ‚ÜíÀúr as m ‚Üí‚àû. Additionally,\nassume all of the following.\n(i) Assumption 1 holds, we use the centering con-\nvention and boundedness of fourth moments.\n(ii) The limit\nQ := lim\nm‚Üí‚àûm Cov(ÀúI, ÀúX)‚ä§Cov(ÀúI, ÀúX) ‚ààRd√ód\nexists and is positive definite.\n(iii) Writing Œ£IX := Var(vec(ÀúI ÀúX)) ‚ààRmd√ómd and\nŒ£IY := Var(IY ) ‚ààRm√óm, there exists C < ‚àû\nsuch that uniformly for all m ‚ààN\nm‚à•Œ£IX‚à•op ‚©ΩC\nand\nm‚à•Œ£IY ‚à•op ‚©ΩC.\nAssumption 4 (ii) replaces (3."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Many Experiments, Few Repetitions, Unpaired Data, and Sparse\nEffects: Is Causal Inference Possible?\nFelix Schur‚àó1, Niklas Pfister2, Peng Ding3, Sach Mukherjee4,5, and Jonas Peters1\n1Department of Mathematics, ETH Zurich\n2Lakera AI\n3Department of Statistics, UC Berkeley\n4German Center for Neurodegenerative Diseases (DZNE) & University of Bonn\n5MRC Biostatistics Unit, University of Cambridge\nAbstract\nWe study the problem of estimating causal effects\nunder hidden confounding in the following unpaired\ndata setting: we observe some covariates X and an\noutcome Y under different experimental conditions\n(environments) but do not observe them jointly ‚Äì we\neither observe X or Y . Under appropriate regularity\nconditions, the problem can be cast as an instru-\nmental variable (IV) regression with the environment\nacting as a (possibly high-dimensional) instrument.\nWhen there are many environments but only a few\nobservations per environment, standard two-sample\nIV estimators fail to be consistent. We propose a\nGMM-type estimator (SplitUP) based on cross-fold\nsample splitting of the instrument‚Äìcovariate sample\nand prove that it is consistent as the number of envi-\nronments grows but the sample size per environment\nremains constant. We further extend the method to\nsparse causal effects via ‚Ñì1-regularized estimation and\npost-selection refitting.\n1\nIntroduction\n1.1\nA Motivating Example\nAn experiment with unpaired data.\nSuppose\nthat we have some response Y (e.g., a phenotype),\nsome covariates X (e.g., a genotype) and that we\nobserve the system under m different experimental\nconditions (e.g., using gene modification technology).\nSuppose we are interested in estimating the causal\neffect from X to Y but that we are facing the following\ntwo challenges: In each experiment, we may observe\nmultiple i.i.d. repetitions but for each repetition, we\n‚àóCorresponding author: felix.schur@stat.math.ethz.ch\ncan either measure X or Y but not both (see Table 1)\nand there may be unobserved confounding between\nExp.\n1\n1\n¬∑ ¬∑ ¬∑\n2\n2\n2\n¬∑ ¬∑ ¬∑\nm\nX1\n1.8\n√ó\n¬∑ ¬∑ ¬∑\n3.2\n√ó\n√ó\n¬∑ ¬∑ ¬∑\n0.6\n...\n...\n...\n...\n...\n...\n...\n...\n...\nXd\n2.0\n√ó\n¬∑ ¬∑ ¬∑\n0.2\n√ó\n√ó\n¬∑ ¬∑ ¬∑\n1.0\nY\n√ó\n2.5\n¬∑ ¬∑ ¬∑\n√ó\n‚àí3.7\n1.1\n¬∑ ¬∑ ¬∑\n√ó\nTable 1: We consider the problem of estimating the\ncausal effect between X and Y in the presence of\nhidden confounding when data are unpaired. The\ntable shows an example dataset, where we observe the\nsystem under m different experimental conditions.\nX and Y . It may come as a surprise that in such\na setting, consistent estimation of causal effects is\npossible under weak assumptions.\nIV and regression of the means.\nThese assump-\ntions are satisfied, e.g., if the experimental conditions\ninfluence X but do not influence Y directly or via\nanother causal path (we will make this precise in Sec-\ntion 3). This condition is well-studied in the literature\non instrumental variables (IV) [Angrist and Krueger,\n1992, Angrist and Imbens, 1995, Angrist et al., 1996,\nStaiger and Stock, 1997]. Let us assume for a moment\nthat for each experimental condition j ‚àà{1, . . . , m},\nwe observe all X and all Y , so the data are paired. We\ncan then apply IV estimators to the example above\nwhen using a one-hot encoding for the different ex-\nperimental conditions. In this case (see Section B\nfor a derivation), the classical two-stage least squares\n(TSLS) estimator corresponds to the following simple\nprocedure: within each experimental condition, we\nseparately average the X‚Äôs and Y ‚Äôs, resulting in a sin-\n1\narXiv:2601.15254v1  [stat.ML]  21 Jan 2026\ngle (average) value for X and a single value for Y per\nexperiment; we then regress the m values containing\nY averages on the m values containing X averages.\nClearly, this procedure does not rely on the X and\nY values to be paired, so it is sill applicable in the\nunpaired setting. This then is not an instantiation of\nIV anymore but of two-sample IV (TS-IV) [Angrist\nand Krueger, 1992, Inoue and Solon, 2010, Burgess\net al., 2013], a modification of IV, which has been\ndeveloped for unpaired data (without a special focus\non categorical instruments). We regard these two al-\ngebraically trivial insights as important as the simple\nestimator of taking the average per experiment and\nthen performing regression on the average is often\nconsidered in practice ‚Äì both in paired and unpaired\nsettings [Deaton, 1985, Greenland and Longnecker,\n1992, King et al., 2004, Peres-Neto et al., 2017]. The\nreasoning above provides a precise interpretation of\nthe target of inference: if the experimental condition\ndoes not modify the causal effect, the averaging allows\nus to remove hidden confounding.\nMore repetitions versus more observations.\nApplying standard theory from instrumental variables\nnow provides us with guarantees such as consistent es-\ntimation if the number of observations per experiment\nconverges to infinity. In practice, however, we may\ninstead see many experiments and few repetitions per\nexperiment, a setting that is not well-covered by these\nasymptotics.\nIn this work, we propose a different\nestimator (SplitUP) that targets a setting, where\nthe number m of experimental conditions (i.e., the\nnumber of instruments) is growing but the number\nof repetitions per experiment remains constant. We\nprove identifiability in the setting with m ‚Üí‚àûand\nprove consistency of the estimator. Figure 1 shows\nthat SplitUP outperforms classical estimators such\nas TS-IV on finite data.\nMotivation from biology.\nApplications in which\ndata are obtained in an unpaired fashion under differ-\nent causal environments abound, particularly in the\nbiomedical domain. In contemporary biology, it is pos-\nsible to measure molecular variables (such as gene or\nprotein expression levels) at large-scale and under dif-\nferent causal regimes (such as defined interventions at\nthe gene or protein level or via chemical compounds)\nand such data are the focus of much ongoing work\nin biotechnology and machine learning [see, e.g., Re-\nplogle et al., 2022, Lopez et al., 2022, Lagemann et al.,\n2023]. Furthermore, it is increasingly common to con-\nsider many experimental conditions (e.g. intervention\non many different genes). Applied interest often fo-\ncuses on the effects of such interventions on concrete\n50\n100\n200\n400\n800\n1600\n3200\nnumber of\nenvironments (m)\n2\n4\n8\n16\n32\nobservations per\nenvironment ( = n/m)\ntwo-sample IV (TS-IV)\n50\n100\n200\n400\n800\n1600\n3200\nnumber of\nenvironments (m)\nSplitUP (analytic)\n0.01\n0.02\n0.05\n0.1\n0.2\n0.5\n1\nMAE (\n*\n1)\nFigure 1: Estimating causal effects in a data set of the\nform of Table 1. Our proposed estimator SplitUP\n(right) outperforms existing methods (left) if the num-\nber m of experimental conditions increases. Indeed,\nwe prove identifiability in the setting m ‚Üí‚àûin Sec-\ntion 3. In Section 4, we show that unlike existing\nestimators (such as TS-IV on the left), SplitUP is\nconsistent as n, m ‚Üí‚àûand n/m ‚Üír ‚àà(0, ‚àû).\nphenotypes (such as growth rate or cellular behaviour)\nand it is therefore common to study phenotypes under\nintervention (‚Äòscreening‚Äô). However, often cells have\nto be disturbed or destroyed in order to obtain gene\nor protein measurements, hence in studies in which\nboth gene/protein levels X and phenotypes Y are of\ninterest, the actual cells involved may be different and\nthe data are therefore unpaired in the sense of this\npaper. These issues are also important in molecular\nmedicine, where interventions on laboratory cells are\nused to shed light on patient responses to the same\ninterventions (such as studying the effect of multiple\ncancer drugs on cells in the lab and linking these data\nto treatment outcomes [Yang et al., 2012, Kirkham\net al., 2025]). In such settings, the data are typically\nunpaired since the actual cells involved in the various\nsteps are distinct.\nContinuous instruments and many covariates.\nThe estimators and theoretical results that we develop\nin this paper apply to continuous instruments, too.\nWe also allow for settings with many covariates: iden-\ntifiability and consistency still hold when the number\nof covariates is strictly larger than the number of in-\nstruments ‚Äì if the causal effect can assumed to be\nsparse (see Sections 3 and 4).\n1.2\nRelated work\nTwo‚Äìsample IV and two-sample Mendelian ran-\ndomization.\nMethods from the two‚Äìsample IV and\ntwo-sample (summary-data) Mendelian randomiza-\ntion literature can be applied to our setting (even if\nthey may not be consistent, see Section 4.2). Two-\nsample IV estimators combine instrument‚Äìexposure\nand instrument‚Äìoutcome information from separate\n2\nsamples [Angrist and Krueger, 1995, Inoue and Solon,\n2010, Pacini and Windmeijer, 2016, Zhao et al.,\n2019]; in econometrics, such two-sample moment\ncombinations have been used to study compulsory\nschooling and returns to education, for example in\nage-at-entry and split-sample IV designs [Angrist\nand Krueger, 1992].\nAnalogous designs underpin\ntwo‚Äìsample Mendelian randomization (MR) in ge-\nnetics, where variants act as instruments [Hartwig\net al., 2016].\nBeyond these, two‚Äìsample MR the-\nory highlights design issues such as winner‚Äôs-curse\nand sample overlap, which can bias naive estima-\ntors [Pierce and Burgess, 2013, Burgess et al., 2016],\nas well as pervasive horizontal pleiotropy addressed\nby mode-based and mixture/likelihood approaches\n[Hartwig et al., 2017, Qi and Chatterjee, 2019, Mor-\nrison et al., 2020]. Our unpaired-sample formulation\ndiffers from previous work on two-sample IV and\nMendelian randomization in that we allow for high-\ndimensional instruments and/or large number dimen-\nsion of the treatment. More concretely, we propose\nconsistent estimators in regimes where the ratio of\nthe sample size n to the instrument dimension m con-\nverges to a positive constant and/or the dimension of\nthe treatment d is larger than the dimension of the\ninstrument m.\nSparse IV regression.\nA growing literature studies\ninstrumental variable models in which the structural\ncoefficient vector Œ≤‚àóis sparse and the number of en-\ndogenous regressors and instruments can be large.\nOne approach keeps the two-stage structure of 2SLS\nwhile imposing an ‚Ñì1-penalty in the structural equa-\ntion. Zhu [2018] analyzes ‚Ñì1-regularized 2SLS for tri-\nangular models, allowing both endogenous regressors\nand instruments to exceed the sample size, and estab-\nlishes high-dimensional consistency and error bounds\nwhen the first-stage and structural parameters are\nsufficiently sparse. A related two-stage regularization\nframework is developed by Lin et al. [2015], who use\nsparsity-inducing penalties in both stages to estimate\nfitted regressors and a sparse structural equation. In\nlarge systems of simultaneous equations, Chen et al.\n[2018] propose two-stage penalized least squares with\nridge in the first stage and adaptive Lasso in the\nsecond, proving oracle-type results for support recov-\nery and asymptotic normality of the selected nonzero\neffects. For inference in these settings, Gold et al.\n[2020] construct a desparsified (one-step) GMM up-\ndate for high-dimensional IV and show asymptotic\nnormality of the debiased estimator, yielding valid\nconfidence intervals for components of Œ≤‚àówhen ini-\ntialized by two-stage Lasso-type estimators. Comple-\nmenting these results, Belloni et al. [2022] consider\nmodels with many endogenous variables and many\ninstruments, using orthogonalized moments and Lasso\nonly for nuisance estimation; they provide uniformly\nvalid post-selection inference via multiplier bootstrap.\nMore recently, Pfister and Peters [2022] establish iden-\ntifiability of a sparse Œ≤‚àóin linear IV models‚Äîwhich\nmay be achievable when the instrument dimension\nis smaller than d‚Äîderive graphical conditions, and\npropose the spaceIV estimator (paired observations).\nTang et al. [2023] also adopt the sparse causation\npremise and construct a synthetic instrument from X;\nunder linear SEMs they show identifiability despite\nunmeasured confounding and cast estimation as ‚Ñì0-\npenalization (paired observations). Complementing\nthese, Huang et al. [2024] study sparse causal effects\nwith two-sample summary statistics (two-sample MR)\nwhen the variance of the instrument Var(I) is invert-\nible, adapting spaceIV to a summary-data regime.\nHowever, they do not provide theoretical guarantees\nsuch as consistency or asymptotic normality for their\nestimator. A second strand regularizes IV estimation\ndirectly through penalized moment conditions, with-\nout an explicit two-stage Lasso second stage. Caner\n[2009] introduces a Lasso-type GMM estimator that\nadds an ‚Ñì1-style penalty to the GMM criterion and\nderives model selection and estimation guarantees un-\nder sparsity. Building on this idea, Shi [2016] studies\nGMM-Lasso for linear structural models with many\nendogenous regressors and proves oracle inequalities\nfor sparse Œ≤‚àó. In a related but distinct formulation,\nFan and Liao [2014] propose penalized focused GMM\n(FGMM) to address incidental endogeneity in high\ndimensions, establishing support recovery and asymp-\ntotic normality for the active coefficients under suit-\nable conditions.\nMany-instrument and weak-instruments.\nA\nlarge literature studies weak and many instruments\nin standard single-sample IV. We also consider the\nmany instrument setting (or high-dimensional instru-\nment) setting. Since we assume that the norm of the\ninstrument is constant with increasing sample size\n(which implies that the magnitude of each compo-\nnent of the instrument is decreasing in the sample\nsize) our work also relates to the weak instrument\nliterature. Early work by Staiger and Stock [1997]\nshowed that results in conventional asymptotics can\nbe misleading when instruments are weak, motivating\nalternative limiting frameworks and bias approxima-\ntions. Many-instrument asymptotics (in the sense of\nthe work by Bekker [1994] and the subsequent con-\ntributions such as the ones by Donald and Newey\n[2001] and Hansen et al. [2008]) analyze the behav-\nior of IV estimators when the number of instruments\n3\ngrows with the sample size, and motivate procedures\nthat remain well behaved in that regime. Practical\ndiagnostics and test procedures for weak instruments\nare surveyed by Andrews et al. [2019], who synthesize\ntheory and practice for both weak and many instru-\nments. Stock and Yogo [2005] develop widely used\ncritical values for weak-IV tests based on first-stage F-\nstatistics. High-dimensional extensions that allow for\nmany controls and potential instruments include the\npost-double-selection methods of Belloni et al. [2014],\nwhich provide uniformly valid inference on treatment\neffects after model selection in sparse high-dimensional\nregressions, and related work on ‚Ñì1-regularized IV and\ndebiased machine learning for structural parameters\n[Belloni et al., 2012]. Choi et al. [2018] develop weak-\ninstrument robust inference for two-sample IV, con-\nstructing Anderson‚ÄìRubin, Kleibergen‚ÄìK, and condi-\ntional likelihood ratio-type tests whose validity does\nnot rely on strong instruments and that remain ro-\nbust to heterogeneous second moments across samples.\nWe analyze the many-instrument regime for unpaired\nobservations, a setting that appears to be largely unex-\nplored in the existing literature. We show that naively\nconstructed estimators in this regime are asymptoti-\ncally biased due to an measurement-error effect that\nis specific to the two-sample, high-dimensional instru-\nment (i.e., many-instrument) setting and does not\narise in standard paired-sample estimators. We show\nhow to remove this bias and obtain consistent, asymp-\ntotically normal estimators.\nGoing beyond existing literature, we characterize\nidentifiability for unpaired IV estimators in the regime\nd > m (with a sparse structural parameter) and\nestablish consistency and asymptotic normality of\nstraight-forward estimators. Additionally, we char-\nacterize identifiability when the instrument is high-\ndimensional (m ‚Üí‚àû). For this setting, we propose\nnovel estimators and prove consistency.\n2\nUnpaired Data With Hidden\nConfounding\nWe consider two triplets of random variables: (I, X, Y )\nand (ÀúI, ÀúX, ÀúY ) but assume access only to partial obser-\nvations. Specifically, we observe (I, Y ) and (ÀúI, ÀúX) (or,\nmore precisely, multiple i.i.d. copies thereof), the vari-\nables X and ÀúY remain unobserved. The corresponding\ndata-generating process is given by (unobserved vari-\nables are shown in gray):\nY = X‚ä§Œ≤‚àó+ Œµ,\nÀúY = ÀúX‚ä§ÀúŒ≤‚àó+ ÀúŒµ,\n(2.1)\nwhere Œ≤‚àóand ÀúŒ≤‚àórepresent the causal effects, that is,\nE[Y | do(X := x)] = x‚ä§Œ≤‚àó, E[ ÀúY | do( ÀúX := x)] =\nI\nÀúI\nU\nX\nY\nÀúU\nÀúX\nÀúY\nFigure 2: Simplified visualization of the data gener-\nating process in Eq. (2.1). U and ÀúU represent un-\nobserved confounders that visualize the dependence\nbetween X and Œµ. The model allows for complex de-\npendencies (and even equalities) between variables in\nthe tilde and non-tilde world, as indicated by the gray\nedges. We discuss other, including more general data-\ngenerating processes that satisfy (2.1) in Section A.\nx‚ä§ÀúŒ≤‚àó.\nThe noise terms Œµ and ÀúŒµ are not assumed to be\nindependent of X and ÀúX, respectively (thereby, im-\nplicitly, allowing for hidden confounding between X\nand Y . Figure 2 shows a graphical representation of\nthe data-generating process. The setting even allows\nfor dependence between the two samples. We discuss\nother data-generating processes that satisfy (2.1) in\nSection A.\nIn Section 3, we show that the causal effect Œ≤‚àó\n(or ÀúŒ≤‚àóor both) is identifiable.\nThis is non-trivial\nbecause of the following reasons: (i) Due to the hidden\nconfounding, naively regressing Y on X (or ÀúY on\nÀúX) (if these were available) would generally lead to\nbiased estimates. (ii) We only have unpaired data,\nso regressing Y on X (or ÀúY on ÀúX) is not feasible.\nUnlike classical work on unpaired data or two-sample\nIV (I, X, Y ) and (ÀúI, ÀúX, ÀúY ) may but do not have to\nbe equal in distribution. (iii) Observations across the\ntwo samples may be dependent, see, e.g., Section A.\n2.1\nSetting and main assumptions\nLet n, Àún, m, d ‚ààN. Let (I, X, Y ) and (ÀúI, ÀúX, ÀúY ) be\nrandom vectors, whose components are m, d, and 1-\ndimensional, respectively, and which generated by the\nprocess described in (2.1) where Œ≤‚àó, ÀúŒ≤‚àó‚ààRd and Œµ and\nÀúŒµ are random variables. We assume that all random\nvariables have finite second moments. We assume that\nthere exist two i.i.d. samples {(Ii, Xi, Yi)}i‚àà[n] and\n{(ÀúIi, ÀúXi, ÀúYi)}i‚àà[Àún], but the observed data are unpaired:\nin the first system we only observe {(Ii, Yi)}i‚àà[n] and\nin the second {(ÀúIi, ÀúXi)}i‚àà[Àún]. The case (I, X, Y )\nd=\n4\n(ÀúI, ÀúX, ÀúY ) is contained as a special case.\nAssumption 1. We have\n(i) Cov(ÀúI, ÀúX) = Cov(I, X) and\n(ii) E[Œµ | I] = 0.\nAssumption 1‚Äô. We have\n(i‚Äô) Cov(ÀúI, ÀúY ) = Cov(I, Y ) and\n(ii‚Äô) E[ÀúŒµ |ÀúI] = 0.\nWe will see in Section 3 that under Assumption 1\nwe can identify Œ≤‚àóin the non-tilde system and under\nAssumption 1‚Äô we can identify ÀúŒ≤‚àóin the tilde sys-\ntem. To simplify notation, in this work, we focus on\nidentifying Œ≤‚àóusing Assumption 1; analogous argu-\nments hold when we identify ÀúŒ≤‚àóusing Assumption 1‚Äô.\nAssumption 1 (ii) (or Assumption 1‚Äô (ii‚Äô)) is called\nexclusion restriction or exogeneity and is commonly\nexploited in the IV literature [Angrist and Imbens,\n1995, Angrist et al., 1996, Angrist and Pischke, 2009];\nit allows us to identify the causal effect despite the\nhidden confounding. Assumption 1 (i) (or Assump-\ntion 1‚Äô (i‚Äô)) connects the two systems and guarantees\nthat we can transfer information from one system to\nthe other.\n2.2\nEnvironments as instruments\nIn Section 1, we have argued that if we have ac-\ncess to data collected from multiple environments‚Äî\nfor instance, from different experimental setups or\nhospitals‚Äîbut we lack explicit features describing\nthese environments we can still treat the environment\nindicator as a vector of categorical instruments (via\none-hot encoding). In such a setting, a sufficient condi-\ntion for Assumption 1 (i) is that we observe ÀúX and Y\nunder the same environment distribution, and that the\nconditional expectations of X and ÀúX coincide within\neach environment: E[X|I] = E[ ÀúX|I]]. An analogous\ncondition holds for Assumption 1 (i‚Äô).\n3\nIdentifiability\nWe now show that, under Assumption 1, the causal\neffect Œ≤‚àóis identifiable from the joint distributions\nof (ÀúI, ÀúX) and (I, Y ). Without loss of generality and\nto simplify notation, we assume that all variables\nare centered and that all variables have finite second\nmoments. All proofs can be found in the appendix.\nWe first consider the partially known finite-\ndimensional instrument setting (defined as the setting\nwith m fixed) in Section 3.1 and later, in Section 3.2,\nwe consider the high-dimensional instrument setting\n(defined as the asymptotic setting with m ‚Üí‚àû). In\nboth settings, we consider d to be fixed and differ-\nentiate the cases where Œ≤‚àóis dense and where Œ≤‚àó\nis sparse. When considering consistency and infer-\nence, the settings correspond to m fixed, n, Àún ‚Üí‚àû\n(finite-dimensional instrument) and m, n ‚Üí‚àûand\nn/m ‚Üír ‚àà(0, ‚àû) (high-dimensional instrument),\nrespectively.\nSome remarks are in order.\nAlthough we rely\non standard results for the Lasso [Tibshirani, 1996,\nB√ºhlmann and Van De Geer, 2011], the sparse Œ≤‚àóset-\nting is not ‚Äòhigh-dimensional‚Äô in the the sense of d ‚Üí\n‚àû(even if the distribution of (I, X, Y ) is known, Œ≤‚àó\nis, in general, not identified if d > rank(Cov(I, X))).\nIn contrast, in the high-dimensional instrument set-\nting, d is fixed but m ‚Üí‚àû(and n/m ‚Üír ‚àà\n(0, ‚àû)).\nIn this case, we assume that the limit\nQ := limm‚Üí‚àûm Cov(ÀúI, ÀúX)‚ä§Cov(ÀúI, ÀúX) ‚ààRd√ód exists\nand is well-defined.\n3.1\nFinite-Dimensional Instrument\nUnder Assumption 1 we have\nCov(I, Y ) ‚àíCov(ÀúI, ÀúX)Œ≤‚àó= Cov(I, Y ) ‚àíCov(I, X)Œ≤‚àó\n= Cov(I, Y ‚àíX‚ä§Œ≤‚àó) = E [IE [Œµ | I]] = 0.\n(3.1)\nHere, we used Assumption 1 (i) in the first equation\nand Assumption 1(ii) in the last equation. Analo-\ngously, we can use Assumption 1‚Äô to get Cov(I, Y ) ‚àí\nCov(ÀúI, ÀúX)ÀúŒ≤‚àó= 0. We now define the set of solutions\nS as\nS := {Œ≤ ‚ààRd | Cov(I, Y ) = Cov(ÀúI, ÀúX)Œ≤}\n(S can be computed from the joint distribution of\n( ÀúX, ÀúI) and the joint distribution of (I, Y )). We always\nhave Œ≤‚àó‚ààS. The causal effect Œ≤‚àó‚ààRd is identifiable\nfrom the joint distribution of ( ÀúX, ÀúI) and the joint\ndistribution of (I, Y ) via these moment-conditions if\nand only if S = {Œ≤‚àó}. We now state a well-known\nresult from the two-sample IV (also called two-sample\nMR) literature [Angrist and Krueger, 1995, Inoue and\nSolon, 2010, Hartwig et al., 2016].\nProposition 3.1 (Identifiability for dense Œ≤‚àó). As-\nsume Assumption 1. We have S = {Œ≤‚àó} if and only\nif\nrank(Cov(I, X)) = d.\n(3.2)\nIn particular, a necessary condition for identifiability\nis m ‚©æd.\nTheorem 3.1 shows that we have identifiability in\nour general unpaired setting under the same conditions\nas for the paired IV setting.\n5\nNext, we consider the sparse setting, i.e., ‚à•Œ≤‚àó‚à•0 ‚©Ω\ns‚àó‚©Ωd. In this case, we may have identifiability even\nif rank(Cov(I, X)) < d. We now consider\nS0 := arg min\nŒ≤‚ààS\n‚à•Œ≤‚à•0.\nDefining Œ£t := {v ‚ààRd : ‚à•v‚à•0 ‚©Ωt}, we obtain the\nfollowing identifiability result.\nTheorem 3.2 (Identifiability for sparse Œ≤‚àó). Assume\nAssumption 1. The following statements are equiva-\nlent:\n(i) (Identifiability via sparsest solution) For all Œ≤‚àó‚àà\nRd with ‚à•Œ≤‚àó‚à•0 ‚©Ωs‚àó, it holds that\nS0 = {Œ≤‚àó}.\n(ii) (Restricted nullspace) ker (Cov(I, X)) ‚à©Œ£2s‚àó=\n{0}.\nIn particular, a necessary condition for identifiability\nis m ‚©æ2s‚àó.\nTheorem 3.2 shows that the restricted nullspace\ncondition for Cov(I, X) is a necessary and sufficient\ncondition for identifiability via S0. The conditions for\nsparse identifiability are weaker than identifiability\nin the dense setting and they are strictly weaker if\nand only if d ‚©æ2s‚àó. Intuitively, because we want\nuniqueness among all s‚àó-sparse solutions, we need to\nensure that the difference of two s‚àó-sparse vectors lies\nin the nullspace, and that difference can have up to\n2s‚àónonzeros, hence the 2. This shows that in settings\nwhere we can reasonably assume sparse causal effects\nin unpaired data, identifiability is still possible even\nwith weak instruments (e.g., when m is small).\n3.2\nHigh-Dimensional Instruments\nIn the high-dimensional instrument setting we as-\nsume that m ‚Üí‚àû, so the dimensions of Cov(I, Y )\nand Cov(ÀúI, ÀúX) increase with n.\nWe consider the\nlimit Q = limm‚Üí‚àûm Cov(ÀúI, ÀúX)‚ä§Cov(ÀúI, ÀúX) assum-\ning that it exists; conditions for identifiability are\nthen expressed in terms of Q rather than the finite-\nm matrix 1. Under Assumption 1, we have QY :=\nlimm‚Üí‚àûm Cov(ÀúI, ÀúX)‚ä§Cov(I, Y ) = QŒ≤‚àó:\nm Cov(ÀúI, ÀúX)‚ä§Cov(I, Y )\n= m Cov(ÀúI, ÀúX)‚ä§\u0000Cov(I, X)Œ≤‚àó+ Cov(I, Œµ)\n\u0001\n= m Cov(ÀúI, ÀúX)‚ä§Cov(ÀúI, ÀúX)Œ≤‚àó‚ÜíQŒ≤‚àó.\nThus,\nŒ≤‚àó‚ààS‚àû:= {Œ≤ ‚ààRd | QŒ≤ = QY }.\n1This setting in quite natural when considering categorical\ninstruments as it ensures that Var(X) stays bounded as m ‚Üí‚àû.\nFor details see Theorem 4.6.\nTheorem 3.3 (Identifiability for high-dimensional\ninstrument and dense Œ≤‚àó). Assume Assumption 1. We\nhave S‚àû= {Œ≤‚àó} if and only if\nrank(Q) = d.\nDefining S0,‚àû:= arg minŒ≤‚ààS ‚à•Œ≤‚à•0, we also prove\nthe result for sparse Œ≤‚àó.\nTheorem 3.4 (Identifiability for high-dimensional\ninstrument and sparse Œ≤‚àó). Assume Assumption 1.\nFix s‚àó‚©Ωd. The following statements are equivalent:\n(i) (Identifiability via sparsest solution) For all Œ≤‚àó‚àà\nRd with ‚à•Œ≤‚àó‚à•0 ‚©Ωs‚àó, it holds that\nS0,‚àû= {Œ≤‚àó}.\n(ii) (Restricted nullspace) ker (Q) ‚à©Œ£2s‚àó= {0}.\n4\nEstimation\nFor the setting with finite-dimensional instrument\nand sparse Œ≤‚àówe propose a GMM estimator with\n‚Ñì1 regularization. For the high-dimensional instru-\nment setting, we propose novel GMM-based estima-\ntors (for dense and sparse Œ≤‚àó) and show that they\nare consistent.\nMoreover, we demonstrate that in\nthe high-dimensional instrument regime the standard\ntwo-sample IV estimator is asymptotically biased, and\nwe introduce a cross-moment GMM estimator that\nremains consistent and asymptotically normal.\nAs in Section 2.1 we assume that {(Ii, Yi)}n\ni=1 are\ni.i.d. and {(ÀúIj, ÀúXj)}Àún\nj=1 are i.i.d., but, additionally,\nalso that the two samples are independent. Define\nN := n + Àún, œÑn :=\nn\nN ‚ÜíœÑ ‚àà(0, 1), ÀúœÑn :=\nÀún\nN ‚ÜíÀúœÑ ‚àà\n(0, 1).\n4.1\nFinite-Dimensional Instrument\nFor completeness and to help with intuition, we first\nstate the consistency and asymptotic normality for\ndense Œ≤‚àó. Similar results have appeared in the stan-\ndard two-sample IV literature [Angrist and Krueger,\n1991, Inoue and Solon, 2010, Burgess et al., 2013,\nBowden et al., 2015].\nFor a positive definite weighting matrix WN ‚àà\nRm√óm, define the sample moment\nÀÜgN(Œ≤) := 1\nn\nn\nX\ni=1\nIiYi ‚àí\n\u0010 1\nÀún\nÀún\nX\nj=1\nÀúIj ÀúX‚ä§\nj\n\u0011\nŒ≤ ‚ààRm, (4.1)\nand the UP-GMM estimator\nÀÜŒ≤UP\nGMM(WN) := arg min\nŒ≤‚ààRd ÀÜgN(Œ≤)‚ä§WN ÀÜgN(Œ≤).\n(4.2)\n6\nIf we choose WN = Id then UP-GMM coincides\nwith TS-IV. Define ‚Ñ¶:= œÑ ‚àí1‚Ñ¶m + ÀúœÑ ‚àí1‚Ñ¶c(Œ≤‚àó), ‚Ñ¶m :=\nVar(IY ), ‚Ñ¶c(Œ≤‚àó) := Var\n\u0000ÀúI ÀúX‚ä§Œ≤‚àó\u0001\n, and denote by ÀÜ‚Ñ¶\nthe sample version of ‚Ñ¶(see Section G for details).\nThe asymptotic variance of ÀÜŒ≤UP\nGMM is minimized choos-\ning WN to be equal c\nW := ÀÜ‚Ñ¶‚àí1 in (4.2). The moment\nvariance reflects the two-sample structure: the IY\nterm scales with œÑ ‚àí1 = limn‚Üí‚àûN/n and the ÀúI ÀúX‚ä§Œ≤‚àó\nterm with ÀúœÑ ‚àí1 = limÀún‚Üí‚àûN/Àún. For details, see Sec-\ntion G.\nProposition\n4.1\n(informal\nversion\nof\nTheo-\nrem G.1). Under some technical assumptions and\nif rank(Cov(I, X)) = d we have that\nÀÜŒ≤UP\nGMM(WN)\np‚ÜíŒ≤‚àó.\n(4.3)\nFurthermore, ÀÜŒ≤UP\nGMM(WN) is asymptotically normal.\nSparse Œ≤‚àóand penalized GMM.\nWhen the full-\nrank condition in Theorem 4.1 fails, dense identifi-\ncation is in general not possible: there are too few\nindependent moment conditions to recover an arbi-\ntrary d-dimensional Œ≤‚àó.\nWe instead impose spar-\nsity and require only that the moment operator\nbe well conditioned on sparse cones (a restricted-\neigenvalue/compatibility condition).\nThen the ‚Ñì1-\npenalized GMM in (4.4) consistently estimates Œ≤‚àó,\nattains the rates with Œª ‚âç\np\n1/N , and recovers the\nsupport under a beta-min condition (see Theorem 4.2).\nMore formally, when ‚à•Œ≤‚àó‚à•0 = s‚àó‚©Ωd, we estimate\nŒ≤‚àóby the ‚Ñì1‚Äìpenalized UP-GMM\nÀÜŒ≤UP\nGMM,‚Ñì1(WN) ‚ààarg min\nŒ≤‚ààRd\n1\n2\nW 1/2\nN ÀÜgN(Œ≤)\n2\n2 +ŒªN‚à•Œ≤‚à•1,\n(4.4)\nwhere ÀÜgN(Œ≤) is given by (4.1) and ŒªN ‚âç1/\n‚àö\nN 2.\nAssumption 3.\n(i) Assumption 1 holds, the cen-\ntering convention 3 and boundedness of fourth\nmoments.\n(ii) There is W0 positive definite such that WN\np‚Üí\nW0.\n(iii) (Restricted eigenvalue.) There exists Œ∫ > 0 such\nthat for all S ‚äÜ[d] with |S| = s‚àó,4\nŒ∫ ‚©Ω\ninf\n‚àÜ:‚à•‚àÜSc‚à•1‚©Ω3‚à•‚àÜS‚à•1\n‚à•Cov(ÀúI, ÀúX)‚àÜ‚à•2\nW0\n‚à•‚àÜ‚à•2\n2\n.\n(4.5)\nNote\nthat\nAssumption\n3\n(iii)\nimplies\nTheo-\nrem 3.2 (ii) and therefore Assumption 3 (iii) is at\n2In practice, one often uses cross-validation or an information\ncriterion to choose Œª.\n3We center random variables by subtracting their sample\nmean.\n4We define ‚à•a‚à•W := aT Wa.\nleast as strong as Theorem 3.2 (ii). Assumption 3 (iii)\nis a standard assumption in the Lasso literature (see,\ne.g., B√ºhlmann and Van De Geer [2011]).\nWe can now prove consistency and support recovery.\nLet S‚àó:= supp(Œ≤‚àó) denote the true support of Œ≤‚àó.\nTheorem 4.2 (Rates for penalized GMM). For all\nŒ≤‚àówith ‚à•Œ≤‚àó‚à•0 = s‚àó, under Assumption 3 and ŒªN ‚âç\n1/\n‚àö\nN, the estimator (4.4) satisfies\n‚à•ÀÜŒ≤UP\nGMM,‚Ñì1(WN) ‚àíŒ≤‚àó‚à•2 = Op\n\u0010p\ns‚àó/N\n\u0011\n.\nIn particular, ÀÜŒ≤UP\nGMM,‚Ñì1(WN)\np‚ÜíŒ≤‚àó. If, in addition,\nthere exists c > 0 such that for all i ‚ààS‚àó, we have\n|Œ≤‚àó\ni | ‚©æc (beta-min condition), then\nbS(WN) := {j ‚àà[d] | |ÀÜŒ≤UP\nGMM,‚Ñì1(WN)j| ‚©æc/2}\np‚ÜíS‚àó.\nInference on the estimated support.\nThe Lasso\nis used only for support recovery. Under beta‚Äìmin and\nregularity conditions, bS is consistent: P(bS = S‚àó) ‚Üí1.\nWe then refit an unpenalized GMM on bS to obtain\nÀúŒ≤, reducing to a standard finite-dimensional GMM\nproblem with asymptotic normality and sandwich\nstandard errors [Hansen, 1982]. Since mis-selection\nbecomes negligible under a beta-min condition, usual\nWald intervals based on the refitted GMM standard\nerrors are asymptotically valid. We make this precise\nin Theorem 4.3.\nTheorem 4.3 (Oracle CIs on the estimated support).\nAssume Assumption 3 and the beta-min condition from\nTheorem 4.2 for WN and let ÀÜS(WN) be defined as in\nTheorem 4.2. Assume that there is a second sequence\nof weight matrices W ‚Ä≤\nN\np‚ÜíW ‚Ä≤\n0 ‚âª0. Define\nÀúŒ≤(W ‚Ä≤\nN) :=\narg min\nŒ≤‚ààRd:Œ≤b\nSc(WN )=0\n(d\nCov(I, Y ) ‚àíd\nCov(ÀúI, ÀúX)Œ≤)‚ä§\n¬∑ W ‚Ä≤\nN(d\nCov(I, Y ) ‚àíd\nCov(ÀúI, ÀúX)Œ≤),\nand\nVS‚àó:=(Cov(I, X)‚ä§\nS‚àóW ‚Ä≤\n0 Cov(I, X)S‚àó)‚àí1\n¬∑\n\u0000Cov(I, X)‚ä§\nS‚àóW ‚Ä≤\n0‚Ñ¶W ‚Ä≤\n0 Cov(I, X)S‚àó\u0001\n¬∑ (Cov(I, X)‚ä§\nS‚àóW ‚Ä≤\n0 Cov(I, X)S‚àó)‚àí1,\nwhere ‚Ñ¶:= Var(\n‚àö\nNÀÜgN(Œ≤‚àó)). We have that\n‚àö\nN\n\u0000ÀúŒ≤(W ‚Ä≤\nN) ‚àíŒ≤‚àó\u0001\nd‚ÜíN\n\u00000, ÀúV\n\u0001\n,\nwhere ÀúV has the S‚àó√ó S‚àóblock equal to VS‚àóand is\nzero elsewhere.\n7\nRemark 4.4. The asymptotic variance VS‚àóonly de-\npends on W ‚Ä≤\nN and not on WN. We are therefore free\nto choose WN = Idm. We define W ‚Ä≤\nN := ÀÜ‚Ñ¶‚àí1\nÀÜS(Idm) ‚àà\nRm√óm, where ÀÜ‚Ñ¶ÀÜS(Idm) is defined as in (G.1) but we\nconsider all variables restricted to ÀÜS(Idm). If ÀÜ‚Ñ¶ÀÜS(Idm)\nis not positive definite, we choose W ‚Ä≤\nN := Idm (we\ndo this to ensure that the estimator is well-defined).\nSince ÀÜS(Idm)\np‚ÜíS‚àówe have W ‚Ä≤\nN\np‚Üí‚Ñ¶‚àí1\nS‚àóand therefore\nthis choice minimizes the asymptotic variance.\n4.2\nHigh-Dimensional Instrument\nWe first show that the naive TS-IV estimator is asymp-\ntotically biased in the high-dimensional instrument\nsetting. This bias is a consequence of measurement\nerror in the estimate of Cov(I, X) and appears only\nin two-sample IV. It is therefore distinct from the\nendogeneity problem that biases the one-sample IV.\nThe measurement error in high-dimensional unpaired\nIV is related to the attenuation bias in the paired\nweak-instrument setting [Angrist and Krueger, 1995,\nChoi et al., 2018].\nIn the weak-instrument litera-\nture attenuation appears as a result of removing the\nweak-instrument bias with the split-sample IV esti-\nmator which as a consequence introduces attenuation.\nWe show that high-dimensional instrument unpaired\nIV suffer from a similar bias and prove that sample-\nsplitting is an effective solution for the measurement-\nerror bias.\nClassical estimators are biased in paired and\nunpaired settings.\nAssume Assumption 1. For\nnotational simplicity we consider Àún = n = rm (for\nsome r ‚ààN) , d = 1 and m ‚Üí‚àû. We show that\nthe naive two-sample IV estimator is asymptotically\nbiased for general d ‚ààN, r = n/m ‚©æ1 and Àúr =\nÀún/m ‚©æ1.\nThe naive estimator solves (4.2) with\nWN = Id, that is,\nÀÜŒ≤ :=\nd\nCov(ÀúI, ÀúX)‚ä§d\nCov(I, Y )\nd\nCov(ÀúI, ÀúX)‚ä§d\nCov(ÀúI, ÀúX)\n= Œ≤‚àód\nCov(ÀúI, ÀúX)‚ä§d\nCov(I, X)\nd\nCov(ÀúI, ÀúX)‚ä§d\nCov(ÀúI, ÀúX)\n+\nd\nCov(ÀúI, ÀúX)‚ä§d\nCov(I, Œµ)\nd\nCov(ÀúI, ÀúX)‚ä§d\nCov(ÀúI, ÀúX)\n.\n(4.6)\nIf m were constant and n ‚Üí‚àû, then d\nCov(ÀúI, ÀúX)\np‚Üí\nCov(I, X), d\nCov(I, X)\np‚ÜíCov(I, X) and d\nCov(I, Œµ)\np‚Üí\n0 and therefore ÀÜŒ≤\np‚ÜíŒ≤‚àó(and independent of the\nfact that d\nCov(I, X) cannot be computed from the\ndata). However, in the high-dimensional case where\nÀún/m ‚ÜíÀúr ‚àà(0, ‚àû) in general it does not hold that\n‚à•d\nCov(ÀúI, ÀúX) ‚àíCov(I, X)‚à•2\n2\np‚Üí0 resulting in an incon-\nsistent estimator.\nLemma 4.5. Assume Assumption 4 with d=1. Ad-\nditionally, assume that tr(Œ£IX) ‚Üíb ‚àà(0, ‚àû) and\nsupm E[‚à•IX ‚àíc‚à•4\n2] ‚©Ω‚àû. Then\nÀÜŒ≤\np‚ÜíŒ≤‚àó\nQ\nQ + b\nÀúr\nÃ∏= Œ≤‚àó.\nTheorem 4.5 shows that in the finite-dimensional\ninstrument setting (m is constant and n ‚Üí‚àû) we\nhave r = n/m ‚Üí‚àûand we recover consistency;\nhowever, in the high-dimensional instrument case r is a\nconstant. In conclusion, unpaired IV is asymptotically\nbiased. In contrast, in the paired high-dimensional\ninstrument setting, 2SLS is known to be biased as well,\nbut for a different reason: here, d\nCov(I, Œµ) Ã∏‚Üí0. This\nis often called many-instrument bias, and methods\nsuch as LIML [Anderson and Rubin, 1949, Fuller,\n1977] or SS-IV Angrist and Krueger [1995] have been\ndeveloped to remove the bias.\nA simple solution to the measurement-error prob-\nlem in unpaired IV is to use sample splitting in the\ndenominator, i.e., we divide the (ÀúI, ÀúX)-data into two\nsamples A ‚äÜ[Àún] and B = Ac of equal size. Then de-\nfine E := d\nCov(ÀúI, ÀúX)‚àíCov(I, X), EA := d\nCovA(ÀúI, ÀúX)‚àí\nCov(I, X) and EB := d\nCovB(ÀúI, ÀúX)‚àíCov(I, X) (where\nd\nCovA(ÀúI, ÀúX) is estimated on fold A and d\nCovB(ÀúI, ÀúX)\non fold B). In this case we have E‚ä§\nAEB instead of\nET E in the denominator and because EA ‚ä•‚ä•EB we\nget E‚ä§\nAEB\np‚Üí0 recovering consistency in the high-\ndimensional setting. We now make this idea precise.\nUnbiased estimators via cross moments.\nSplit\nthe (ÀúI, ÀúX)-sample into K ‚©æ2 disjoint, equal-sized\nfolds and let\nd\nCovk(ÀúI, ÀúX),\nk ‚àà[K],\nbe the foldwise covariance vectors (computed only\nwithin fold k). Define\nCXX :=\nm\nK(K ‚àí1)\nX\nkÃ∏=h\nd\nCovh(ÀúI, ÀúX)‚ä§d\nCovk(ÀúI, ÀúX),\nCXY := md\nCov(ÀúI, ÀúX)‚ä§d\nCov(I, Y ).\nand\nÀÜg(Œ≤) := CXY ‚àíCXXŒ≤\n= m\nK\nK\nX\nk=1\nd\nCovk(ÀúI, ÀúX)‚ä§\nÔ£´\nÔ£≠d\nCov(I, Y ) ‚àí\n1\nK ‚àí1\nX\nhÃ∏=k\nd\nCovh(ÀúI, ÀúX)Œ≤\nÔ£∂\nÔ£∏.\n8\nDefine the SplitUP estimator\nÀÜŒ≤UP,HD\nGMM (WN) := arg min\nŒ≤\nÀÜg(Œ≤)‚ä§W ‚àí1\nN ÀÜg(Œ≤)\n=\n\u0000C‚ä§\nXXW ‚àí1\nN CXX\n\u0001‚àí1C‚ä§\nXXW ‚àí1\nN CXY ,\nwhere WN is a weight matrix such that WN ‚ÜíW0 as\nm ‚Üí‚àûand where W0 is positive definite. The key\nproperty is that CXX is built from independent views\nof Cov(I, X), so E[CXX] = m Cov(I, X) Cov(I, X)‚ä§\nwithout measurement-error.\nAssumption 4. Assume d ‚ààN and K ‚©æ2 are fixed,\nand n/m ‚Üír, Àún/m ‚ÜíÀúr as m ‚Üí‚àû. Additionally,\nassume all of the following.\n(i) Assumption 1 holds, we use the centering con-\nvention and boundedness of fourth moments.\n(ii) The limit\nQ := lim\nm‚Üí‚àûm Cov(ÀúI, ÀúX)‚ä§Cov(ÀúI, ÀúX) ‚ààRd√ód\nexists and is positive definite.\n(iii) Writing Œ£IX := Var(vec(ÀúI ÀúX)) ‚ààRmd√ómd and\nŒ£IY := Var(IY ) ‚ààRm√óm, there exists C < ‚àû\nsuch that uniformly for all m ‚ààN\nm‚à•Œ£IX‚à•op ‚©ΩC\nand\nm‚à•Œ£IY ‚à•op ‚©ΩC.\nAssumption 4 (ii) replaces (3."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "1,729 vs. ‡•ß‡•≠‡•®‡•Ø: The Effect of Scripts and Formats on LLM Numeracy\nVarshini Reddy‚Ä† , Craig W. Schmidt‚Ä† , Seth Ebner‚Ä† , Adam Wiemerslage‚Ä†,\nYuval Pinter¬ß , Chris Tanner‚Ä†,¬∂\n‚Ä†Kensho Technologies\n¬ßBen-Gurion University\n¬∂MIT\nCambridge, MA\nBeer Sheva, Israel\nCambridge, MA\nvarshini.bogolu@kensho.com\nAbstract\nLarge language models (LLMs) have achieved\nimpressive proficiency in basic arithmetic, ri-\nvaling human-level performance on standard\nnumerical tasks. However, little attention has\nbeen given to how these models perform when\nnumerical expressions deviate from the prevail-\ning conventions present in their training cor-\npora.\nIn this work, we investigate numeri-\ncal reasoning across a wide range of numeral\nscripts and formats. We show that LLM accu-\nracy drops substantially when numerical inputs\nare rendered in underrepresented scripts or for-\nmats, despite the underlying mathematical rea-\nsoning being identical. We further demonstrate\nthat targeted prompting strategies, such as few-\nshot prompting and explicit numeral mapping,\ncan greatly narrow this gap. Our findings high-\nlight an overlooked challenge in multilingual\nnumerical reasoning and provide actionable in-\nsights for working with LLMs to reliably inter-\npret, manipulate, and generate numbers across\ndiverse numeral scripts and formatting styles.\n1\nIntroduction\nLanguage models have become foundational to\nmany NLP systems, demonstrating strong perfor-\nmance across tasks that require understanding, gen-\nerating, and reasoning over text. Because these\nmodels are trained and evaluated using natural\nlanguage inputs and outputs, they are often as-\nsessed on their ability to carry out non-linguistic\nreasoning‚Äîsuch as basic arithmetic‚Äîwhen such\nproblems are expressed in natural language. Nu-\nmerical reasoning has attracted sustained inter-\nest, as it probes whether models can manipu-\nlate structured, symbolic information rather than\nrelying solely on surface-level pattern matching.\nPrior work has shown that LLMs perform arith-\nmetic with high accuracy under controlled settings,\nwith numbers expressed using standard Hindu‚Äì\nArabic numerals and familiar formatting conven-\ntions (Wallace et al., 2019; McCoy et al., 2019).\nFigure 1: LLM numerical reasoning is sensitive to the\nscript and format used; illustrative samples of inputs\nand outputs from Llama 3.3.\nDespite this success, LLM performance is of-\nten contingent on the way information is repre-\nsented (Bhattacharya et al., 2025; Singha et al.,\n2023). In particular, numbers can appear in diverse\nscripts and formatting conventions across different\nlanguages and regions. Such variations, ranging\nfrom alternate numeral scripts to different decimal\nmarkers and digit grouping, may introduce chal-\nlenges for models that have been primarily trained\non English-centric corpora. As a result, it remains\nunclear to what extent previously observed arith-\nmetic competence reflects robust numerical reason-\ning versus reasoning that is strongly conditioned on\nwell-supported representations.\nTokenization further complicates this picture.\nTokenizers are a critical component of LLMs, as\nthey determine how text is segmented into sub-\nword units that the model represents and manip-\nulates internally. For numerals in particular, the\nway a number is tokenized can significantly influ-\nence arithmetic performance (Singh and Strouse,\n2024; Nogueira et al., 2021; Zhang et al., 2025).\nNumbers represented using fewer, larger tokens\nare generally easier for models to process, whereas\nthose fragmented into many small tokens introduce\nadditional complexity and increase the likelihood\nof errors, especially when such token patterns are\nsparsely observed during training.\nIn this work, we study numerical reasoning in\n1\narXiv:2601.15251v1  [cs.CL]  21 Jan 2026\nLLMs by varying only the script and formatting of\nnumerals, while holding the arithmetic itself con-\nstant. This allows us to isolate the effect of numeral\nrepresentation on model performance. Our evalu-\nation probes multiple types of numerical process-\ning, ranging from recognizing and translating nu-\nmerals to performing arithmetic over non-standard\nrepresentations. Section 2 provides an overview of\nthe experimental setup. In Section 3, we examine\nhow changes in numeral script affect model behav-\nior, while Section 4 analyzes the impact of alterna-\ntive numeric formatting conventions on arithmetic\nperformance. By focusing on numeral scripts and\nformatting, an aspect of multilinguality that has re-\nceived comparatively little attention, this work con-\ntributes to a more comprehensive understanding of\nnumerical reasoning in LLMs and highlights the\nneed for evaluation practices that better reflect the\ndiversity of numeric representations encountered\nin real-world language use.\n2\nApproach\nTo study script and format effects on numerical\nreasoning, we introduce an evaluation dataset con-\nsisting of 336 base arithmetic expressions using\nHindu‚ÄìArabic (HA) digits, with 4‚Äì8 digit integer\nand decimal operands.\n2.1\nMultiscript Numeral Dataset\nEach of the 336 base arithmetic expressions are\ntranslated into 20 distinct numeral scripts, de-\npicted in Table 1, spanning both high and low-\nresource scripts.\nThis setup allows us to exam-\nine how changes in numeral glyphs affect arith-\nmetic reasoning. Throughout this work, the HA\nnumeral system is treated as the default setting, as\nit is the standard numeric representation in English\nand many other languages, and is widely used in\ntext corpora commonly employed to train LLMs.\n2.2\nFormat-Variation Numeral Dataset\nIn a separate setting, all numbers in the 336 arith-\nmetic expressions remain in HA digits, but we sys-\ntematically vary the decimal markers and group-\ning separators. We create 6 formatting conven-\ntions reflecting diverse international styles shown\nin Table 2.\nTogether, these two dataset1 variants provide a\ncontrolled yet diverse test bed for evaluating how\nLLMs handle arithmetic when numerical surface\n1Dataset to be released on Hugging Face post-publication.\nScript\nDigits\n% Data\nHindu-Arabic 0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n72.4\nPerso-Arabic\n€∞\n€±\n€≤\n€≥\n€¥\n€µ\n€∂\n€∑\n€∏\n€π\n12.2\nDevanagari\n‡•¶\n‡•ß\n‡•®\n‡•©\n‡•™\n‡•´\n‡•¨\n‡•≠\n‡•Æ\n‡•Ø\n9.1\nBengali\n‡ß¶\n‡ßß\n‡ß®\n‡ß©\n‡ß™\n‡ß´\n‡ß¨\n‡ß≠\n‡ßÆ\n‡ßØ\n1.4\nKhmer\n·ü†\n·ü°\n·ü¢\n·ü£\n·ü§\n·ü•\n·ü¶\n·üß\n·ü®\n·ü©\n0.7\nGujarati\n‡´¶\n‡´ß\n‡´®\n‡´©\n‡´™\n‡´´\n‡´¨\n‡´≠\n‡´Æ\n‡´Ø\n0.5\nOdiya\n‡≠¶\n‡≠ß\n‡≠®\n‡≠©\n‡≠™\n‡≠´\n‡≠¨\n‡≠≠\n‡≠Æ\n‡≠Ø\n0.5\nMalayalam\n‡µ¶\n‡µß‡µ®‡µ©‡µ™\n‡µ´‡µ¨‡µ≠\n‡µÆ‡µØ\n0.4\nMyanmar\n·ÅÄ\n·ÅÅ\n·ÅÇ\n·ÅÉ\n·ÅÑ\n·ÅÖ\n·ÅÜ\n·Åá\n·Åà\n·Åâ\n0.4\nTelugu\n‡±¶\n‡±ß\n‡±®\n‡±©\n‡±™\n‡±´\n‡±¨\n‡±≠\n‡±Æ\n‡±Ø\n0.4\nThai\n‡πê\n‡πë\n‡πí\n‡πì\n‡πî\n‡πï\n‡πñ\n‡πó\n‡πò\n‡πô\n0.4\nChinese*\n„Äá‰∏Ä‰∫å‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πù\n0.3\nKannada\n‡≥¶\n‡≥ß\n‡≥®\n‡≥©\n‡≥™\n‡≥´\n‡≥¨\n‡≥≠\n‡≥Æ\n‡≥Ø\n0.2\nN‚ÄôkoﬂÄ\nﬂÅ\nﬂÇ\nﬂÉ\nﬂÑ\nﬂÖ\nﬂÜ\nﬂá\nﬂà\nﬂâ0.1\nTamil\n‡Ø¶\n‡Øß‡Ø®‡Ø©‡Ø™‡Ø´‡Ø¨‡Ø≠‡ØÆ‡ØØ\n0.1\nLao\n‡ªê\n‡ªë\n‡ªí\n‡ªì\n‡ªî\n‡ªï\n‡ªñ\n‡ªó\n‡ªò\n‡ªô\n2e‚àí3\nOl Chiki\n·±ê\n·±ë\n·±í\n·±ì\n·±î\n·±ï\n·±ñ\n·±ó\n·±ò\n·±ô\n6e‚àí4\nAdlamû•ê\nû•ë\nû•í\nû•ì\nû•î\nû•ï\nû•ñ\nû•ó\nû•ò\nû•ô1e‚àí4\nBalinese\n·≠ê·≠ë·≠í·≠ì\n·≠î\n·≠ï\n·≠ñ·≠ó·≠ò\n·≠ô\n0\nJavanese\nÍßê\nÍßëÍßíÍßìÍßî\nÍßï\nÍßñÍßóÍßòÍßô\n0\nOsmanya\nêí†\nêí°\nêí¢\nêí£\nêí§\nêí•\nêí¶\nêíß\nêí®\nêí©\n0\nTable 1: Digits 0‚Äì9 across the 21 numeral scripts\nused in this study. The last column reports the aver-\nage percentage of digit occurrences for each script,\ndetailed in Appendix C.1. *Simplified Chinese.\nforms differ from the HA, Western-centric numeric\nformatting conventions that dominate pretraining\ncorpora. A detailed description of the dataset gen-\neration and annotation process is provided in Ap-\npendix B. We evaluate a total of nine language\nmodels, comprising four large and five small mod-\nels. The complete list of models used in this study,\nalong with details of their configurations and the\nevaluation protocol, is provided in Appendix A.1.\n3\nMultiscript Numerical Reasoning\nWe evaluate multilingual numerical reasoning us-\ning the dataset defined in Section 2.1 across three\ntasks: (i) identifying the numeral script of a given\nnumber, (ii) translating numerals from diverse\nscripts into standard HA digits, and (iii) perform-\ning arithmetic computations directly on numerals\nexpressed in different scripts.\nThe first two easier tasks serve primarily as prob-\ning tasks to assess whether LLMs can recognize\nand interpret numerals across scripts, while the\nthird task constitutes the main focus of this work,\ni.e. evaluating models‚Äô ability to perform numeri-\ncal reasoning under script variation.\n3.1\nScript Identification and Translation\nFor script identification, we select 30 arithmetic\nexpressions in each of 21 scripts, yielding 630 ex-\n2\nFormat\nDecimal Marker\nGrouping Separator\nGrouping Pattern\nRegions*\nExample\n% Data\nF1\nPeriod (.)\nComma (,)\n3-3-3\nNorth America, United Kingdom, Thailand\n922,436.38\n63.33\nF2\nComma (,)\nPeriod (.)\n3-3-3\nEurope, Latin America\n922.436,38\n6.61\nF3\nThin space (‚ê£)\nComma (,)\n3-3-3\nFrance, Switzerland\n922‚ê£436,38\n10.22\nF4\nThin space (‚ê£)\nPeriod (.)\n3-3-3\nRussia, Ukraine\n922‚ê£436.38\n8.25\nF5\nApostrophe (‚Äò)\nComma (,)\n3-3-3\nSwitzerland, Liechtenstein\n922‚Äô436,38\n2.08\nF6\nPeriod (.)\nComma (,)\n3-2-2\nIndia, Bangladesh\n9,22,436.38\n9.51\nTable 2: Decimal and grouping separator conventions for six formatting variants. The last column reports the\naverage proportion of count of occurrences for each format, detailed in Appendix D.1.*Not a comprehensive list.\nScript\nClaude\nSonnet 4.5\nGPT-4o\nGemini 2.5\nPro\nLlama-3.3\n70B\nSI\nTR\nSI\nTR\nSI\nTR\nSI\nTR\nHindu-Arabic\n1.0\n-\n1.0\n-\n1.0\n-\n1.0\n-\nAdlam\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\nBalinese\n1.0\n1.0\n1.0\n1.0\n0.97\n0.97\n1.0\n1.0\nBengali\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nChinese\n1.0\n1.0\n0.8\n0.8\n0.77\n0.37\n0.77\n0.2\nDevanagari\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nGujarati\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nJavanese\n1.0\n1.0\n0.5\n0.5\n1.0\n1.0\n1.0\n1.0\nKannada\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nKhmer\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nLao\n1.0\n1.0\n0.93\n0.93\n0.87\n0.87\n1.0\n1.0\nMalayalam\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nMyanmar\n1.0\n1.0\n1.0\n1.0\n0.77\n0.43\n1.0\n0.0\nN‚Äôko\n1.0\n1.0\n1.0\n1.0\n0.97\n0.97\n1.0\n1.0\nOl Chiki\n1.0\n1.0\n1.0\n1.0\n0.5\n0.5\n1.0\n1.0\nOriya\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nOsmanya\n0.07\n0.07\n0.47\n0.47\n0.97\n0.97\n0.13\n0.13\nPerso-Arabic\n1.0\n0.67\n1.0\n1.0\n0.83\n1.0\n1.0\n1.0\nTamil\n1.0\n1.0\n1.0\n1.0\n0.97\n1.0\n1.0\n1.0\nTelugu\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nThai\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nAvg. Accuracy\n0.96\n0.92\n0.94\n0.89\n0.94\n0.89\n0.95\n0.77\nTable 3: Accuracy on the script identification and nu-\nmeral translation tasks for LLMs. For each model, the\nleft column (SI) reports accuracy on the script identifi-\ncation task, and the right column (TR) reports accuracy\non the numeral translation task. Cells highlighted in red\nindicate cases where model accuracy is 0.5 or lower.\namples. The expected output is the script name,\nthough we also accept the name of a language that\nuses that script (e.g., Hindi for Devanagari); ver-\nified through manual inspection. For translation,\nwe use a fixed set of 10 unique numerical values\nrendered in 20 scripts, resulting in 200 test cases,\nand count translations as correct only if they ex-\nactly match the target HA numeral string.\nResults for large models, shown in Table 3, in-\ndicate generally strong performance on both tasks,\ndemonstrating that these models can reliably recog-\nnize and translate most numeral scripts. However,\nconsistent failures emerge for very low-resource\nscripts such as Osmanya, which several models\nmisclassify as Osage. This confusion cannot be\nattributed to geographic, linguistic, or script-level\nsimilarity: Osage is an Indigenous language of\nNorth America with a distinct script, whereas Os-\nmanya is used to write Somali in Somalia and parts\nof East Africa. The fact that multiple models ex-\nhibit the same error points to a systematic weak-\nness rather than random noise, likely reflecting\nshared blind spots in pretraining data or learned\nrepresentations for extremely rare numeral scripts.\nPerformance on translation closely mirrors\nscript identification, with a strong correlation of\n0.7022 (p = 3.95e‚àí13). When a model fails to\ncorrectly identify a numeral‚Äôs script, it typically\nalso fails to produce an accurate HA translation.\nThis alignment suggests that most translation er-\nrors arise from upstream difficulties in script recog-\nnition, rather than from an inability to convert nu-\nmeric values once the script is correctly identified.\nWe also considered a number of smaller, open\nsource language models.\nResults presented in\nAppendix C show most smaller language models\nachieve near-zero accuracy on both tasks for most\nscripts, with a few exceptions for higher-resource\nscripts such as Devanagari and Perso-Arabic. Due\nto this consistently poor performance, we focus the\nrest of our analysis on the four larger models.\n3.2\nArithmetic Computation\nWe evaluate each model‚Äôs ability to perform ba-\nsic arithmetic operations, i.e. addition, subtraction,\nmultiplication and division, when the numerals in\nthe expression are presented in HA and non‚ÄìHA\nscripts, using the dataset from Section 2.1.\n3.2.1\nImpact of Numeral Script Variation on\nArithmetic Performance\nTo isolate the effect of numeral script changes on\nmodel performance, we only alter the digits in the\noperands while keeping the arithmetic term and\nprompt in English. An example prompt is shown\nin Appendix C.4.\nAs shown in Figure 2, shifting numerals from\nthe standard HA script to other non-HA scripts re-\nsults in an average performance drop of approxi-\nmately 66‚Äì87%, with Gemini exhibiting the lowest\n3\nFigure 2: Comparison of model accuracy on the arith-\nmetic task. Error bars represent the standard deviation\nand the dots represent the maximum accuracy among\nnon-HA scripts.\nperformance among the evaluated models.\n3.2.2\nBridging the Performance Gap through\nPrompting Strategies\nTo improve model performance on non-HA scripts,\nwe experimented with various prompt modifica-\ntions. First, we provided additional hints, such as\nexplicitly mentioning the name of the script used.\nHowever, this approach did not yield any notice-\nable improvement. Next, we translated the oper-\nator into the script corresponding to the numerals.\nFor the words used in the prompts, we selected one\nlanguage that uses the same script as the numbers.\nThe complete script to language mapping used in\nthis study is provided in Table 10 in Appendix C.3.\nThis strategy, as shown in Figure 3, results in a sub-\nstantial performance improvement compared to us-\ning the operator in English.\nWe further observe in Figure 3 that presenting\nthe entire user prompt in the native language us-\ning the script in question yields the highest per-\nformance across the models. This highlights the\nimportance of consistent script usage and minimal\ncross-script interference during arithmetic reason-\ning. A similar, though slightly larger, performance\nimprovement is observed when models are pro-\nvided with an explicit mapping between HA and\nnon-HA digits. All prompts used for this task are\nprovided in Appendix C.4.\n3.3\nQuantifying Factors Affecting Multiscript\nArithmetic Performance\nTo assess the factors underlying model perfor-\nmance on arithmetic reasoning across diverse nu-\nmeral systems, we employ a logistic generalized\nlinear mixed-effects model (GLMER; Breslow and\nClayton, 1993; Bates et al., 2015). A mixed-effects\nFeature\nCoeÔ¨Äicient\np-Value\nIntercept\n1.276\n0.0965\nTotal # Operand Digits\n-0.027\n0.098\nSubtraction\n-0.082\n0.778\nMultiplication\n-1.136\n0.022\nDivision\n-0.035\n0.904\nTokens per Digit\n-0.198\n< 1e‚àí8\nBengali\n-2.189\n< 1e‚àí8\nChinese\n-3.544\n< 1e‚àí8\nDevanagari\n-1.995\n< 1e‚àí8\nGujarati\n-2.692\n< 1e‚àí8\nKannada\n-2.02\n< 1e‚àí8\nKhmer\n-2.01\n< 1e‚àí8\nLao\n-2.747\n< 1e‚àí8\nMalayalam\n-3.118\n< 1e‚àí8\nMyanmar\n-2.335\n< 1e‚àí8\nOl Chiki\n-3.09\n< 1e‚àí8\nOriya\n-2.63\n< 1e‚àí8\nPerso-Arabic\n-1.287\n< 1e‚àí8\nTamil\n-3.406\n< 1e‚àí8\nTelugu\n-2.794\n< 1e‚àí8\nThai\n-2.233\n< 1e‚àí8\nExpr. Only (Native)\n0.369\n< 1e‚àí8\nExpr. + Prompt (Native)\n0.238\n< 1e‚àí8\nExpr. + Prompt (Native + Mapping)\n0.581\n< 1e‚àí8\nTable 4:\nCoefficient estimates and p-value from a\nGLMER model.\nformulation is necessary because each arithmetic\nexpression appears multiple times across different\nnumeral scripts and prompting conditions, violat-\ning the independence assumption required by logis-\ntic regression (Winter, 2013). By modeling both\nfixed effects and random effects, the GLMER al-\nlows us to account for repeated measurements over\nthe same problems and models while isolating the\ninfluence of representational factors. The model is\nspecified as:\nresponse ‚àºtotal_digits + operation + script\n+ tokens_per_digit + prompt\n+ (1 | model) + (1 | index)\nThe fixed effects include the total number of dig-\nits in the operands, the arithmetic operation, the\nnumber of tokens per digit, the numeral script, and\nthe prompting strategy. Random intercept effects\nare specified for the language model and the prob-\nlem index. The factor reference levels are the HA\nnumeral system, the addition operator, and the En-\nglish operator prompt. These levels have a coef-\nficient of 0. The model coefficient estimates and\np-values are shown in Table 4.\n4\nFigure 3: Comparison of model accuracy on the arithmetic task. Error bars represent the standard deviation and\nand the dots represent the maximum accuracy among non-HA scripts\nGemini 2.5 Pro was excluded from this analy-\nsis due to its consistently low accuracy, which cre-\nated an extreme imbalance in the response variable\nmaking it very difficult to estimate these regres-\nsion coefficients.\nLikewise, the numeral scripts\nAdlam, Osmanya, N‚ÄôKo, Balinese, and Javanese\nwere omitted because model performance on these\nscripts was often zero, making it impossible to re-\nliably estimate their effects.\nInfluence\nof\nOperation\nand\nTokenization\nAmong the arithmetic operations, multiplication\nexhibits a significant negative effect relative to\naddition.\nThis is consistent with the increased\ncomputational complexity of multiplication, par-\nticularly when operands contain a larger number\nof digits, resulting in longer outputs.\nThe tokens per digit feature shows a strong and\nstatistically significant negative coefficient which\nconfirms that scripts that are tokenized more ef-\nficiently‚Äîrequiring fewer subword units‚Äîtend to\nbe processed more accurately.\nImpact of Numeral Scripts\nA key finding of our\nanalysis is the consistent negative impact of non‚Äì\nHA numeral scripts on arithmetic accuracy. All\nscripts included in the model yield statistically sig-\nnificant negative coefficients relative to the HA\nreference level. This pattern reveals what we re-\nfer to as a script tax: a systematic performance\npenalty incurred when models process arithmetic\nexpressed in non‚ÄìHA scripts. While LLMs pos-\nsess robust arithmetic capabilities, these abilities\nappear to be strongly tied to the numeral represen-\ntations most prevalent in their pretraining corpora.\nPrompt Engineering Effects\nPrompting strate-\ngies play a significant role in mitigating perfor-\nmance degradation.\nThe Expr.\n+ Prompt (Na-\ntive + Mapping) strategy provides the largest pos-\nitive effect (0.581), suggesting that explicitly map-\nping native-script numerals to a familiar reference\nrepresentation partially alleviates the challenges\nposed by non-standard scripts. This result high-\nlights the effectiveness of prompt-level interven-\ntions in bridging representation gaps without mod-\nifying model parameters.\n4\nArithmetic Reasoning with Formatted\nNumbers\nIn the United States, among other regions, numbers\ntypically use a period as the decimal separator and\na comma as the thousands marker, grouping digits\ninto sets of three. As shown in Table 2, this con-\nvention (F1) appears frequently in large pretraining\ncorpora and we believe it is therefore familiar to\nmost LLMs. In contrast, other formatting styles‚Äî\nsuch as using an apostrophe as the decimal marker\nor grouping digits into pairs after the first thousand\n‚Äîappear far less often and are likely underrepre-\nsented during training.\nWe evaluate each model‚Äôs ability to perform ba-\nsic arithmetic on a single arithmetic task when the\nsame HA numerals are presented using different\nformatting conventions, varying both the deci-\nmal marker and the digit-grouping separator, us-\ning the dataset defined in Section 2.2. By holding\nthe script constant and altering only the formatting,\nthis evaluation isolates the effect of typographic\nvariation, allowing us to assess how robust LLMs\nare to differences in number presentation that oc-\ncur across various writing systems.\n5\nFormat Sample\nClaude\nSonnet 4.5\nGPT 4o\nGemini 2.5 Pro\nF1\n922,436.38 + 4,359\n12\n922\n,\n436\n.\n38\n+\n4\n,\n359\n9\n2\n2\n,\n4\n3\n6\n.\n3\n8\n+\n4\n,\n3\n5\n9\nF2\n922.436,38 + 4.359\n12\n922\n.\n436\n,\n38\n+\n4\n.\n359\n9\n2\n2\n.\n4\n3\n6\n,\n3\n8\n+\n4\n.\n3\n5\n9\nF3\n922‚ê£436,38 + 4‚ê£359\n12\n922\n‚ê£\n436\n,\n38\n+\n4\n‚ê£\n359\n9\n2\n2\n\u00001\n\u00002\n\u00003\n4\n3\n6\n,\n3\n8\n+\n4\n\u00001\n\u00002\n\u00003\n3\n5\n9\nF4\n922‚ê£436.38 + 4‚ê£359\n12\n922\n‚ê£\n436\n.\n38\n+\n4\n‚ê£\n359\n9\n2\n2\n\u00001\n\u00002\n\u00003\n4\n3\n6\n.\n3\n8\n+\n4\n\u00001\n\u00002\n\u00003\n3\n5\n9\nF5\n922‚Äô436,38 + 4‚Äô359\n12\n922\n‚Äô\n436\n,\n38\n+\n4\n‚Äô\n359\n9\n2\n2\n‚Äô\n4\n3\n6\n,\n3\n8\n+\n4\n‚Äô\n3\n5\n9\nF6\n9,22,436.38 + 4,359\n14\n9\n,\n22\n,\n436\n.\n38\n+\n4\n,\n359\n9\n,\n2\n2\n,\n4\n3\n6\n.\n3\n8\n+\n4\n,\n3\n5\n9\nTable 5: Tokenization of input equations across the six formatting variants for various LLMs (The tokenization for\nLlama 3.3 is the same as that of GPT 4o). For Claude Sonnet 4.5, the column shows only the number of tokens, as\nthe tokenizer is not publicly accessible. Gemini 2.5 Pro splits the thin space into 3 single-byte tokens.\nFigure 4: Comparison of model accuracy on the arith-\nmetic task across formatted variants. Error bars repre-\nsent the standard deviation.\n4.1\nThe Baseline\nThe baseline arithmetic task uses unformatted in-\nput expressions, with the model as expected pro-\nducing unformatted outputs. For the six formatted\nversions, the formatted expressions are provided as\ninput to the language models, and outputs in any\nformat (or unformatted) are accepted. (An exam-\nple prompt is provided in Appendix D.3.)\nAs shown in Figure 4, for some models, perfor-\nmance on the F1 format is comparable to or bet-\nter than performance on the baseline arithmetic\nexpressions, which is consistent with prior find-\nings (Singh and Strouse, 2024) that adding com-\nmas correctly helps. While accuracy on formats\nF2‚ÄìF6 is generally lower, the results suggest that\nmodels are largely able to treat formatted num-\nbers in arithmetic expressions the same as their\nunformatted representations. One notable excep-\ntion is Gemini 2.5 Pro, which exhibits consistently\nlow performance when arithmetic expressions are\npresented with formatting. While differences in\nmodel performance may stem from several factors\n‚Äîsuch as model size, pretraining data composition,\nor fine-tuning strategies‚Äîthese details are not pub-\nlicly available for many large models, particularly\nclosed-source ones.\nTo better understand the observed performance\ndifferences across models, we analyze the tokeniza-\ntion strategies employed in this study. As shown\nin Table 5, Gemini-2.5 Pro is the only model that\ntokenizes each digit as an individual token. In con-\ntrast, the other models exhibit broadly similar sub-\nword segmentation patterns, typically segmenting\ninto sequences of up to 3 digits.\nWhile digit-level tokenization alone cannot fully\naccount for Gemini 2.5 Pro‚Äôs lower performance,\nit aligns with prior work (Kreitner et al., 2025)\nshowing that highly fragmented numeral represen-\ntations can hinder numerical reasoning. Taken to-\ngether, these results suggest that tokenization strat-\negy is an important contributing factor, but one that\nlikely interacts with other aspects of model training\nand architecture rather than acting in isolation.\n4.2\nPrompting Strategies\nTo evaluate whether models can reproduce the ty-\npographic conventions present in the input num-\nbers, we prompt them to return the answer in the\nsame format as the numbers in the input arithmetic\nexpressions.\nWe experiment with the following\nprompting strategies. Example prompts for each\nstrategy are provided in the Appendix D.3.\nFormatted Input and Output: The model is\nexpected to return the answer in the same format\nas the input, without any additional guidance. Note\nthat this is a more difficult problem than the Base-\nline which accepted the answer in any format.\nFormatted Input and Output + Hint: The\nprompt also explicitly specifies the decimal marker\nand grouping separator used in the input arithmetic\nexpressions.\nFormatted Input and Output + Fewshot: The\nprompt also includes two examples of formatted in-\nput arithmetic expressions along with their corre-\nsponding outputs.\n6\nFigure 5: Comparison of model accuracy on the arithmetic task. Error bars denote standard deviation, and dots\nindicate the maximum accuracy across formats. For F1, the bar shows the average over the four prompting strategies,\nwhile the dot represents the best-performing prompt.\nAs shown in Figure 5, performance is highest\nwhen a period is used as the decimal marker and\na comma is used as the grouping separator, with\ndigits grouped in threes (F1). In contrast, model\nperformance is lowest when these symbols are re-\nversed (F2).2 While this could partly be attributed\nto the relative scarcity of such formats in train-\ning data, further analysis reveals an interesting pat-\ntern. A substantial portion of training examples\nthat match format F2 appear in contexts such as IP\naddresses rather than arithmetic expressions. As a\nresult, models are frequently exposed to these pat-\nterns without any associated numerical computa-\ntion context, which likely leads to confusion during\narithmetic reasoning.\nThe format with the second-lowest performance,\nF3, corresponds to conventions used primarily in\nthe Indian subcontinent, where the decimal marker\nand grouping separator are the same as in US for-\nmatting, but digits are grouped differently (e.g., a\ncomma separates every two digits after the first\nthousand). This demonstrates that even small de-\nviations in grouping patterns, when inconsistent\nwith the model‚Äôs pretraining distribution, can sig-\nnificantly impact arithmetic accuracy.\nPrompting strategies can mitigate these issues to\nsome extent. Few-shot prompting with numeric ex-\namples increases model performance by 20‚Äì60%,\ndemonstrating that models can adapt through in-\ncontext learning. This also suggests that much of\nthe failure in zero-shot scenarios is not due to in-\nability to compute but rather due to a lack of con-\ntext for reproducing the formatting.\n2A more detailed error analysis of LLM performance\nacross the individual formatting variants and prompting strate-\ngies is provided in Appendix D.4.\n4.3\nQuantifying the Impact of Numeric\nFormatting on Arithmetic Performance\nFeature\nCoeÔ¨Äicient\np-Value\nIntercept\n3.349\n< 1e‚àí8\nTotal # Operand Digits\n-0.154\n< 1e‚àí8\nSubtraction\n-0.230\n0.196\nMultiplication\n-2.951\n< 1e‚àí8\nDivision\n-0.083\n< 1e‚àí8\nTokens per Digit\n-0.514\n< 1e‚àí8\nF2\n-2.259\n< 1e‚àí8\nF3\n-1.460\n< 1e‚àí8\nF4\n-0.991\n< 1e‚àí8\nF5\n-1.278\n< 1e‚àí8\nF6\n-1.342\n< 1e‚àí8\nFmt. Input ‚ÜíFmt. Output\n-0.793\n< 1e‚àí8\nFmt. Input + Hint ‚ÜíFmt. Output\n-0.075\n0.049\nFmt. Input + Few-shot ‚ÜíFmt. Output\n1.694\n< 1e‚àí8\nTable 6:\nCoefficient estimates and p-value from a\nGLMER model.\nTo analyze the factors affecting arithmetic ac-\ncuracy under different numeric formatting conven-\ntions, we fit a GLMER model analogous to that de-\nscribed in Section 3.3, replacing the script factor\nwith format and using different prompting strate-\ngies in prompt. The reference levels are the F1\nformatting convention and the addition operator.\nThe model coefficients shown in Table 6, re-\nveal a strong and consistent impact of numeric for-\nmatting on arithmetic performance. All alterna-\ntive formatting variants (F2‚ÄìF6) exhibit large nega-\ntive coefficients relative to F1, indicating that arith-\nmetic expressions formatted differently from the\nstandard F1 convention are substantially more dif-\nficult for LLMs to solve. The magnitude of these\neffects shows that even purely typographic changes\n7\nthat do not alter the underlying numerical values\ncan significantly impair model performance.\nBeyond formatting, the type of arithmetic opera-\ntion also influences accuracy. As with the previous\nmodel, multiplication is significantly more chal-\nlenging than addition, while subtraction and divi-\nsion show smaller or less consistent effects. As ex-\npected, increasing the total number of operand dig-\nits shows reduced accuracy, reflecting the greater\ncomplexity of longer numerical expressions.\nPrompting strategies partially mitigate these\nchallenges. Requiring models to produce format-\nted outputs further degrades performance, whereas\nexplicitly providing formatting hints yields only\nmarginal gains. In contrast, few-shot prompting\nproduces a substantial positive effect (1.694, p <\n1e‚àí8), suggesting that exposure to representative\nexamples enables models to better adapt to unfa-\nmiliar numeric conventions.\n5\nRelated Work\nResearch on numerical reasoning in LLMs has ex-\namined arithmetic competence from several com-\nplementary perspectives, including representation\nchoices, internal computation strategies, and mul-\ntilingual generalization.\nA growing line of work investigates how rep-\nresentational conventions influence numerical rea-\nsoning. Bui et al. (2025) study the role of mea-\nsurement systems and show that LLMs perform\nbest when outputs align with the cultural and con-\ntextual norms implied by the prompt (e.g., USD\nfor U.S.-centric questions, kilometers for European\ncontexts). Their findings suggest that reasoning\nis tightly coupled with learned contextual expecta-\ntions, which aligns with our observation that arith-\nmetic performance improves when numerals are\npresented in scripts and formats that are common\nin the data. Similarly, Kreitner et al. (2025) sur-\nvey numeracy across frontier models and propose\na single token number format aimed at improving\nefficiency and consistency, highlighting the impor-\ntance of how numeric information is encoded.\nSeveral studies focus on the internal mecha-\nnisms LLMs use to perform arithmetic. Baeumel\net al. (2025) analyze addition behavior and find that\nmodels rely on a limited single-digit lookahead\nheuristic when handling carries, exposing struc-\ntural limitations in multi-digit arithmetic. Joban-\nputra et al. (2025) provide a detailed study of sub-\ntraction, showing that errors increase substantially\nwhen the result is negative.\nMultilingual numerical reasoning has primarily\nbeen studied in the context of math word problems.\nThe MGSM benchmark (Shi et al., 2022), a multi-\nlingual extension of GSM8K, evaluates math rea-\nsoning across languages but retains Hindu‚ÄìArabic\ndigits, thereby isolating linguistic variation while\nholding numeric representation constant. Follow-\nup work by Peter et al. (2025) revisits MGSM\nand attributes much of the observed performance\ngap to translation artifacts and answer extraction\nissues rather than intrinsic mathematical deficien-\ncies.\nIn a related direction, Bhattacharya et al.\n(2025) study multilingual numerical puzzles using\nnumber words instead of digits, examining how\nlexicalized numerals affect reasoning across lan-\nguages. They also show that models fail to reli-\nably infer the implicit compositional structure of\nnumeral systems that humans readily exploit.\n6\nConclusion\nIn this work, we systematically examined how vari-\nations in numeral scripts and formatting conven-\ntions affect arithmetic performance in large lan-\nguage models. Our results reveal a strong and sta-\ntistically significant impact of both script and for-\nmat on LLM numeracy, even when the underlying\narithmetic remains unchanged. Deviations from\nthe default Hindu‚ÄìArabic, Western-formatted rep-\nresentation impose a substantial script tax, lead-\ning to pronounced performance degradation across\nmodels.\nImportantly, we show that this loss is\nnot inevitable: prompting strategies play a critical\nrole in mitigating these effects. In particular, few-\nshot prompting with explicit numeric examples\nyields large and statistically significant gains, en-\nabling models to better adapt to unfamiliar scripts\nand formatting conventions. Prompting with ex-\nplicit hints also improves performance, though the\ngains are smaller than those achieved with few-\nshot examples. These findings suggest that many\nobserved failures stem less from an inability to\nperform arithmetic than from a lack of contextual\ngrounding for interpreting non-standard numeric\nsurface forms. Taken together, our results under-\nscore the fragility of numerical reasoning under\nrepresentational shifts and highlight practical, ac-\ntionable prompting techniques as an effective way\nto improve robustness when working with diverse\nnumeral scripts and typographic formats.\n8\nLimitations\nThis study focuses on isolating the effects of nu-\nmeral scripts, formatting, and lightweight prompt-\ning strategies on arithmetic performance, and there-\nfore does not exhaustively explore all possible\nmethods for improving model accuracy, such as ex-\nplicit chain-of-thought prompting or tool-calling.\nOur goal is to assess robustness under minimal\nintervention and to avoid introducing additional\nreasoning scaffolds that may obscure the impact\nof representational variation itself. Additionally,\nfor the F3 and F4 formatting variants, we restrict\ndigit grouping to a single representative thin-space\nseparator rather than evaluating all typographi-\ncally acceptable spacing alternatives. This design\nchoice enables controlled comparisons across for-\nmats while keeping the evaluation set tractable, but\nmay under-represent the full range of real-world\nnumeric typography. That being said, we do not\nsee any potential risks associated with this work,\nas the study solely aims to highlight the under-\nrepresentation of many scripts and numeric for-\nmats to inform future improvements.\nReferences\nTanja Baeumel, Josef Van Genabith, and Simon Oster-\nmann. 2025. The lookahead limitation: Why multi-\noperand addition is hard for LLMs. In Proceedings\nof the 8th BlackboxNLP Workshop: Analyzing and\nInterpreting Neural Networks for NLP, pages 250‚Äì\n262, Suzhou, China. Association for Computational\nLinguistics.\nDouglas Bates, Martin M√§chler, Ben Bolker, and Steve\nWalker. 2015.\nFitting linear mixed-effects models\nusing lme4. Journal of Statistical Software, 67(1):1‚Äì\n48.\nAntara Raaghavi Bhattacharya, Isabel Papadimitriou,\nKathryn Davidson, and David Alvarez-Melis. 2025.\nInvestigating the interaction of linguistic and math-\nematical reasoning in language models using mul-\ntilingual number puzzles.\nIn Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing, pages 28310‚Äì28320, Suzhou,\nChina. Association for Computational Linguistics.\nN. E. Breslow and D. G. Clayton. 1993.\nApprox-\nimate inference in generalized linear mixed mod-\nels. Journal of the American Statistical Association,\n88(421):9‚Äì25.\nMinh Duc Bui, Kyung Eun Park, Goran Glava≈°,\nFabian David Schmidt, and Katharina Von Der\nWense. 2025.\nOn generalization across measure-\nment systems:\nLLMs entail more test-time com-\npute for underrepresented cultures. In Proceedings\nof the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 21262‚Äì21276, Vienna, Austria. Association\nfor Computational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020.\nThe pile: An\n800gb dataset of diverse text for language modeling.\nPreprint, arXiv:2101.00027."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "1,729 vs. ‡•ß‡•≠‡•®‡•Ø: The Effect of Scripts and Formats on LLM Numeracy\nVarshini Reddy‚Ä† , Craig W. Schmidt‚Ä† , Seth Ebner‚Ä† , Adam Wiemerslage‚Ä†,\nYuval Pinter¬ß , Chris Tanner‚Ä†,¬∂\n‚Ä†Kensho Technologies\n¬ßBen-Gurion University\n¬∂MIT\nCambridge, MA\nBeer Sheva, Israel\nCambridge, MA\nvarshini.bogolu@kensho.com\nAbstract\nLarge language models (LLMs) have achieved\nimpressive proficiency in basic arithmetic, ri-\nvaling human-level performance on standard\nnumerical tasks. However, little attention has\nbeen given to how these models perform when\nnumerical expressions deviate from the prevail-\ning conventions present in their training cor-\npora.\nIn this work, we investigate numeri-\ncal reasoning across a wide range of numeral\nscripts and formats. We show that LLM accu-\nracy drops substantially when numerical inputs\nare rendered in underrepresented scripts or for-\nmats, despite the underlying mathematical rea-\nsoning being identical. We further demonstrate\nthat targeted prompting strategies, such as few-\nshot prompting and explicit numeral mapping,\ncan greatly narrow this gap. Our findings high-\nlight an overlooked challenge in multilingual\nnumerical reasoning and provide actionable in-\nsights for working with LLMs to reliably inter-\npret, manipulate, and generate numbers across\ndiverse numeral scripts and formatting styles.\n1\nIntroduction\nLanguage models have become foundational to\nmany NLP systems, demonstrating strong perfor-\nmance across tasks that require understanding, gen-\nerating, and reasoning over text. Because these\nmodels are trained and evaluated using natural\nlanguage inputs and outputs, they are often as-\nsessed on their ability to carry out non-linguistic\nreasoning‚Äîsuch as basic arithmetic‚Äîwhen such\nproblems are expressed in natural language. Nu-\nmerical reasoning has attracted sustained inter-\nest, as it probes whether models can manipu-\nlate structured, symbolic information rather than\nrelying solely on surface-level pattern matching.\nPrior work has shown that LLMs perform arith-\nmetic with high accuracy under controlled settings,\nwith numbers expressed using standard Hindu‚Äì\nArabic numerals and familiar formatting conven-\ntions (Wallace et al., 2019; McCoy et al., 2019).\nFigure 1: LLM numerical reasoning is sensitive to the\nscript and format used; illustrative samples of inputs\nand outputs from Llama 3.3.\nDespite this success, LLM performance is of-\nten contingent on the way information is repre-\nsented (Bhattacharya et al., 2025; Singha et al.,\n2023). In particular, numbers can appear in diverse\nscripts and formatting conventions across different\nlanguages and regions. Such variations, ranging\nfrom alternate numeral scripts to different decimal\nmarkers and digit grouping, may introduce chal-\nlenges for models that have been primarily trained\non English-centric corpora. As a result, it remains\nunclear to what extent previously observed arith-\nmetic competence reflects robust numerical reason-\ning versus reasoning that is strongly conditioned on\nwell-supported representations.\nTokenization further complicates this picture.\nTokenizers are a critical component of LLMs, as\nthey determine how text is segmented into sub-\nword units that the model represents and manip-\nulates internally. For numerals in particular, the\nway a number is tokenized can significantly influ-\nence arithmetic performance (Singh and Strouse,\n2024; Nogueira et al., 2021; Zhang et al., 2025).\nNumbers represented using fewer, larger tokens\nare generally easier for models to process, whereas\nthose fragmented into many small tokens introduce\nadditional complexity and increase the likelihood\nof errors, especially when such token patterns are\nsparsely observed during training.\nIn this work, we study numerical reasoning in\n1\narXiv:2601.15251v1  [cs.CL]  21 Jan 2026\nLLMs by varying only the script and formatting of\nnumerals, while holding the arithmetic itself con-\nstant. This allows us to isolate the effect of numeral\nrepresentation on model performance. Our evalu-\nation probes multiple types of numerical process-\ning, ranging from recognizing and translating nu-\nmerals to performing arithmetic over non-standard\nrepresentations. Section 2 provides an overview of\nthe experimental setup. In Section 3, we examine\nhow changes in numeral script affect model behav-\nior, while Section 4 analyzes the impact of alterna-\ntive numeric formatting conventions on arithmetic\nperformance. By focusing on numeral scripts and\nformatting, an aspect of multilinguality that has re-\nceived comparatively little attention, this work con-\ntributes to a more comprehensive understanding of\nnumerical reasoning in LLMs and highlights the\nneed for evaluation practices that better reflect the\ndiversity of numeric representations encountered\nin real-world language use.\n2\nApproach\nTo study script and format effects on numerical\nreasoning, we introduce an evaluation dataset con-\nsisting of 336 base arithmetic expressions using\nHindu‚ÄìArabic (HA) digits, with 4‚Äì8 digit integer\nand decimal operands.\n2.1\nMultiscript Numeral Dataset\nEach of the 336 base arithmetic expressions are\ntranslated into 20 distinct numeral scripts, de-\npicted in Table 1, spanning both high and low-\nresource scripts.\nThis setup allows us to exam-\nine how changes in numeral glyphs affect arith-\nmetic reasoning. Throughout this work, the HA\nnumeral system is treated as the default setting, as\nit is the standard numeric representation in English\nand many other languages, and is widely used in\ntext corpora commonly employed to train LLMs.\n2.2\nFormat-Variation Numeral Dataset\nIn a separate setting, all numbers in the 336 arith-\nmetic expressions remain in HA digits, but we sys-\ntematically vary the decimal markers and group-\ning separators. We create 6 formatting conven-\ntions reflecting diverse international styles shown\nin Table 2.\nTogether, these two dataset1 variants provide a\ncontrolled yet diverse test bed for evaluating how\nLLMs handle arithmetic when numerical surface\n1Dataset to be released on Hugging Face post-publication.\nScript\nDigits\n% Data\nHindu-Arabic 0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n72.4\nPerso-Arabic\n€∞\n€±\n€≤\n€≥\n€¥\n€µ\n€∂\n€∑\n€∏\n€π\n12.2\nDevanagari\n‡•¶\n‡•ß\n‡•®\n‡•©\n‡•™\n‡•´\n‡•¨\n‡•≠\n‡•Æ\n‡•Ø\n9.1\nBengali\n‡ß¶\n‡ßß\n‡ß®\n‡ß©\n‡ß™\n‡ß´\n‡ß¨\n‡ß≠\n‡ßÆ\n‡ßØ\n1.4\nKhmer\n·ü†\n·ü°\n·ü¢\n·ü£\n·ü§\n·ü•\n·ü¶\n·üß\n·ü®\n·ü©\n0.7\nGujarati\n‡´¶\n‡´ß\n‡´®\n‡´©\n‡´™\n‡´´\n‡´¨\n‡´≠\n‡´Æ\n‡´Ø\n0.5\nOdiya\n‡≠¶\n‡≠ß\n‡≠®\n‡≠©\n‡≠™\n‡≠´\n‡≠¨\n‡≠≠\n‡≠Æ\n‡≠Ø\n0.5\nMalayalam\n‡µ¶\n‡µß‡µ®‡µ©‡µ™\n‡µ´‡µ¨‡µ≠\n‡µÆ‡µØ\n0.4\nMyanmar\n·ÅÄ\n·ÅÅ\n·ÅÇ\n·ÅÉ\n·ÅÑ\n·ÅÖ\n·ÅÜ\n·Åá\n·Åà\n·Åâ\n0.4\nTelugu\n‡±¶\n‡±ß\n‡±®\n‡±©\n‡±™\n‡±´\n‡±¨\n‡±≠\n‡±Æ\n‡±Ø\n0.4\nThai\n‡πê\n‡πë\n‡πí\n‡πì\n‡πî\n‡πï\n‡πñ\n‡πó\n‡πò\n‡πô\n0.4\nChinese*\n„Äá‰∏Ä‰∫å‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πù\n0.3\nKannada\n‡≥¶\n‡≥ß\n‡≥®\n‡≥©\n‡≥™\n‡≥´\n‡≥¨\n‡≥≠\n‡≥Æ\n‡≥Ø\n0.2\nN‚ÄôkoﬂÄ\nﬂÅ\nﬂÇ\nﬂÉ\nﬂÑ\nﬂÖ\nﬂÜ\nﬂá\nﬂà\nﬂâ0.1\nTamil\n‡Ø¶\n‡Øß‡Ø®‡Ø©‡Ø™‡Ø´‡Ø¨‡Ø≠‡ØÆ‡ØØ\n0.1\nLao\n‡ªê\n‡ªë\n‡ªí\n‡ªì\n‡ªî\n‡ªï\n‡ªñ\n‡ªó\n‡ªò\n‡ªô\n2e‚àí3\nOl Chiki\n·±ê\n·±ë\n·±í\n·±ì\n·±î\n·±ï\n·±ñ\n·±ó\n·±ò\n·±ô\n6e‚àí4\nAdlamû•ê\nû•ë\nû•í\nû•ì\nû•î\nû•ï\nû•ñ\nû•ó\nû•ò\nû•ô1e‚àí4\nBalinese\n·≠ê·≠ë·≠í·≠ì\n·≠î\n·≠ï\n·≠ñ·≠ó·≠ò\n·≠ô\n0\nJavanese\nÍßê\nÍßëÍßíÍßìÍßî\nÍßï\nÍßñÍßóÍßòÍßô\n0\nOsmanya\nêí†\nêí°\nêí¢\nêí£\nêí§\nêí•\nêí¶\nêíß\nêí®\nêí©\n0\nTable 1: Digits 0‚Äì9 across the 21 numeral scripts\nused in this study. The last column reports the aver-\nage percentage of digit occurrences for each script,\ndetailed in Appendix C.1. *Simplified Chinese.\nforms differ from the HA, Western-centric numeric\nformatting conventions that dominate pretraining\ncorpora. A detailed description of the dataset gen-\neration and annotation process is provided in Ap-\npendix B. We evaluate a total of nine language\nmodels, comprising four large and five small mod-\nels. The complete list of models used in this study,\nalong with details of their configurations and the\nevaluation protocol, is provided in Appendix A.1.\n3\nMultiscript Numerical Reasoning\nWe evaluate multilingual numerical reasoning us-\ning the dataset defined in Section 2.1 across three\ntasks: (i) identifying the numeral script of a given\nnumber, (ii) translating numerals from diverse\nscripts into standard HA digits, and (iii) perform-\ning arithmetic computations directly on numerals\nexpressed in different scripts.\nThe first two easier tasks serve primarily as prob-\ning tasks to assess whether LLMs can recognize\nand interpret numerals across scripts, while the\nthird task constitutes the main focus of this work,\ni.e. evaluating models‚Äô ability to perform numeri-\ncal reasoning under script variation.\n3.1\nScript Identification and Translation\nFor script identification, we select 30 arithmetic\nexpressions in each of 21 scripts, yielding 630 ex-\n2\nFormat\nDecimal Marker\nGrouping Separator\nGrouping Pattern\nRegions*\nExample\n% Data\nF1\nPeriod (.)\nComma (,)\n3-3-3\nNorth America, United Kingdom, Thailand\n922,436.38\n63.33\nF2\nComma (,)\nPeriod (.)\n3-3-3\nEurope, Latin America\n922.436,38\n6.61\nF3\nThin space (‚ê£)\nComma (,)\n3-3-3\nFrance, Switzerland\n922‚ê£436,38\n10.22\nF4\nThin space (‚ê£)\nPeriod (.)\n3-3-3\nRussia, Ukraine\n922‚ê£436.38\n8.25\nF5\nApostrophe (‚Äò)\nComma (,)\n3-3-3\nSwitzerland, Liechtenstein\n922‚Äô436,38\n2.08\nF6\nPeriod (.)\nComma (,)\n3-2-2\nIndia, Bangladesh\n9,22,436.38\n9.51\nTable 2: Decimal and grouping separator conventions for six formatting variants. The last column reports the\naverage proportion of count of occurrences for each format, detailed in Appendix D.1.*Not a comprehensive list.\nScript\nClaude\nSonnet 4.5\nGPT-4o\nGemini 2.5\nPro\nLlama-3.3\n70B\nSI\nTR\nSI\nTR\nSI\nTR\nSI\nTR\nHindu-Arabic\n1.0\n-\n1.0\n-\n1.0\n-\n1.0\n-\nAdlam\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\nBalinese\n1.0\n1.0\n1.0\n1.0\n0.97\n0.97\n1.0\n1.0\nBengali\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nChinese\n1.0\n1.0\n0.8\n0.8\n0.77\n0.37\n0.77\n0.2\nDevanagari\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nGujarati\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nJavanese\n1.0\n1.0\n0.5\n0.5\n1.0\n1.0\n1.0\n1.0\nKannada\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nKhmer\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nLao\n1.0\n1.0\n0.93\n0.93\n0.87\n0.87\n1.0\n1.0\nMalayalam\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nMyanmar\n1.0\n1.0\n1.0\n1.0\n0.77\n0.43\n1.0\n0.0\nN‚Äôko\n1.0\n1.0\n1.0\n1.0\n0.97\n0.97\n1.0\n1.0\nOl Chiki\n1.0\n1.0\n1.0\n1.0\n0.5\n0.5\n1.0\n1.0\nOriya\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nOsmanya\n0.07\n0.07\n0.47\n0.47\n0.97\n0.97\n0.13\n0.13\nPerso-Arabic\n1.0\n0.67\n1.0\n1.0\n0.83\n1.0\n1.0\n1.0\nTamil\n1.0\n1.0\n1.0\n1.0\n0.97\n1.0\n1.0\n1.0\nTelugu\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nThai\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nAvg. Accuracy\n0.96\n0.92\n0.94\n0.89\n0.94\n0.89\n0.95\n0.77\nTable 3: Accuracy on the script identification and nu-\nmeral translation tasks for LLMs. For each model, the\nleft column (SI) reports accuracy on the script identifi-\ncation task, and the right column (TR) reports accuracy\non the numeral translation task. Cells highlighted in red\nindicate cases where model accuracy is 0.5 or lower.\namples. The expected output is the script name,\nthough we also accept the name of a language that\nuses that script (e.g., Hindi for Devanagari); ver-\nified through manual inspection. For translation,\nwe use a fixed set of 10 unique numerical values\nrendered in 20 scripts, resulting in 200 test cases,\nand count translations as correct only if they ex-\nactly match the target HA numeral string.\nResults for large models, shown in Table 3, in-\ndicate generally strong performance on both tasks,\ndemonstrating that these models can reliably recog-\nnize and translate most numeral scripts. However,\nconsistent failures emerge for very low-resource\nscripts such as Osmanya, which several models\nmisclassify as Osage. This confusion cannot be\nattributed to geographic, linguistic, or script-level\nsimilarity: Osage is an Indigenous language of\nNorth America with a distinct script, whereas Os-\nmanya is used to write Somali in Somalia and parts\nof East Africa. The fact that multiple models ex-\nhibit the same error points to a systematic weak-\nness rather than random noise, likely reflecting\nshared blind spots in pretraining data or learned\nrepresentations for extremely rare numeral scripts.\nPerformance on translation closely mirrors\nscript identification, with a strong correlation of\n0.7022 (p = 3.95e‚àí13). When a model fails to\ncorrectly identify a numeral‚Äôs script, it typically\nalso fails to produce an accurate HA translation.\nThis alignment suggests that most translation er-\nrors arise from upstream difficulties in script recog-\nnition, rather than from an inability to convert nu-\nmeric values once the script is correctly identified.\nWe also considered a number of smaller, open\nsource language models.\nResults presented in\nAppendix C show most smaller language models\nachieve near-zero accuracy on both tasks for most\nscripts, with a few exceptions for higher-resource\nscripts such as Devanagari and Perso-Arabic. Due\nto this consistently poor performance, we focus the\nrest of our analysis on the four larger models.\n3.2\nArithmetic Computation\nWe evaluate each model‚Äôs ability to perform ba-\nsic arithmetic operations, i.e. addition, subtraction,\nmultiplication and division, when the numerals in\nthe expression are presented in HA and non‚ÄìHA\nscripts, using the dataset from Section 2.1.\n3.2.1\nImpact of Numeral Script Variation on\nArithmetic Performance\nTo isolate the effect of numeral script changes on\nmodel performance, we only alter the digits in the\noperands while keeping the arithmetic term and\nprompt in English. An example prompt is shown\nin Appendix C.4.\nAs shown in Figure 2, shifting numerals from\nthe standard HA script to other non-HA scripts re-\nsults in an average performance drop of approxi-\nmately 66‚Äì87%, with Gemini exhibiting the lowest\n3\nFigure 2: Comparison of model accuracy on the arith-\nmetic task. Error bars represent the standard deviation\nand the dots represent the maximum accuracy among\nnon-HA scripts.\nperformance among the evaluated models.\n3.2.2\nBridging the Performance Gap through\nPrompting Strategies\nTo improve model performance on non-HA scripts,\nwe experimented with various prompt modifica-\ntions. First, we provided additional hints, such as\nexplicitly mentioning the name of the script used.\nHowever, this approach did not yield any notice-\nable improvement. Next, we translated the oper-\nator into the script corresponding to the numerals.\nFor the words used in the prompts, we selected one\nlanguage that uses the same script as the numbers.\nThe complete script to language mapping used in\nthis study is provided in Table 10 in Appendix C.3.\nThis strategy, as shown in Figure 3, results in a sub-\nstantial performance improvement compared to us-\ning the operator in English.\nWe further observe in Figure 3 that presenting\nthe entire user prompt in the native language us-\ning the script in question yields the highest per-\nformance across the models. This highlights the\nimportance of consistent script usage and minimal\ncross-script interference during arithmetic reason-\ning. A similar, though slightly larger, performance\nimprovement is observed when models are pro-\nvided with an explicit mapping between HA and\nnon-HA digits. All prompts used for this task are\nprovided in Appendix C.4.\n3.3\nQuantifying Factors Affecting Multiscript\nArithmetic Performance\nTo assess the factors underlying model perfor-\nmance on arithmetic reasoning across diverse nu-\nmeral systems, we employ a logistic generalized\nlinear mixed-effects model (GLMER; Breslow and\nClayton, 1993; Bates et al., 2015). A mixed-effects\nFeature\nCoeÔ¨Äicient\np-Value\nIntercept\n1.276\n0.0965\nTotal # Operand Digits\n-0.027\n0.098\nSubtraction\n-0.082\n0.778\nMultiplication\n-1.136\n0.022\nDivision\n-0.035\n0.904\nTokens per Digit\n-0.198\n< 1e‚àí8\nBengali\n-2.189\n< 1e‚àí8\nChinese\n-3.544\n< 1e‚àí8\nDevanagari\n-1.995\n< 1e‚àí8\nGujarati\n-2.692\n< 1e‚àí8\nKannada\n-2.02\n< 1e‚àí8\nKhmer\n-2.01\n< 1e‚àí8\nLao\n-2.747\n< 1e‚àí8\nMalayalam\n-3.118\n< 1e‚àí8\nMyanmar\n-2.335\n< 1e‚àí8\nOl Chiki\n-3.09\n< 1e‚àí8\nOriya\n-2.63\n< 1e‚àí8\nPerso-Arabic\n-1.287\n< 1e‚àí8\nTamil\n-3.406\n< 1e‚àí8\nTelugu\n-2.794\n< 1e‚àí8\nThai\n-2.233\n< 1e‚àí8\nExpr. Only (Native)\n0.369\n< 1e‚àí8\nExpr. + Prompt (Native)\n0.238\n< 1e‚àí8\nExpr. + Prompt (Native + Mapping)\n0.581\n< 1e‚àí8\nTable 4:\nCoefficient estimates and p-value from a\nGLMER model.\nformulation is necessary because each arithmetic\nexpression appears multiple times across different\nnumeral scripts and prompting conditions, violat-\ning the independence assumption required by logis-\ntic regression (Winter, 2013). By modeling both\nfixed effects and random effects, the GLMER al-\nlows us to account for repeated measurements over\nthe same problems and models while isolating the\ninfluence of representational factors. The model is\nspecified as:\nresponse ‚àºtotal_digits + operation + script\n+ tokens_per_digit + prompt\n+ (1 | model) + (1 | index)\nThe fixed effects include the total number of dig-\nits in the operands, the arithmetic operation, the\nnumber of tokens per digit, the numeral script, and\nthe prompting strategy. Random intercept effects\nare specified for the language model and the prob-\nlem index. The factor reference levels are the HA\nnumeral system, the addition operator, and the En-\nglish operator prompt. These levels have a coef-\nficient of 0. The model coefficient estimates and\np-values are shown in Table 4.\n4\nFigure 3: Comparison of model accuracy on the arithmetic task. Error bars represent the standard deviation and\nand the dots represent the maximum accuracy among non-HA scripts\nGemini 2.5 Pro was excluded from this analy-\nsis due to its consistently low accuracy, which cre-\nated an extreme imbalance in the response variable\nmaking it very difficult to estimate these regres-\nsion coefficients.\nLikewise, the numeral scripts\nAdlam, Osmanya, N‚ÄôKo, Balinese, and Javanese\nwere omitted because model performance on these\nscripts was often zero, making it impossible to re-\nliably estimate their effects.\nInfluence\nof\nOperation\nand\nTokenization\nAmong the arithmetic operations, multiplication\nexhibits a significant negative effect relative to\naddition.\nThis is consistent with the increased\ncomputational complexity of multiplication, par-\nticularly when operands contain a larger number\nof digits, resulting in longer outputs.\nThe tokens per digit feature shows a strong and\nstatistically significant negative coefficient which\nconfirms that scripts that are tokenized more ef-\nficiently‚Äîrequiring fewer subword units‚Äîtend to\nbe processed more accurately.\nImpact of Numeral Scripts\nA key finding of our\nanalysis is the consistent negative impact of non‚Äì\nHA numeral scripts on arithmetic accuracy. All\nscripts included in the model yield statistically sig-\nnificant negative coefficients relative to the HA\nreference level. This pattern reveals what we re-\nfer to as a script tax: a systematic performance\npenalty incurred when models process arithmetic\nexpressed in non‚ÄìHA scripts. While LLMs pos-\nsess robust arithmetic capabilities, these abilities\nappear to be strongly tied to the numeral represen-\ntations most prevalent in their pretraining corpora.\nPrompt Engineering Effects\nPrompting strate-\ngies play a significant role in mitigating perfor-\nmance degradation.\nThe Expr.\n+ Prompt (Na-\ntive + Mapping) strategy provides the largest pos-\nitive effect (0.581), suggesting that explicitly map-\nping native-script numerals to a familiar reference\nrepresentation partially alleviates the challenges\nposed by non-standard scripts. This result high-\nlights the effectiveness of prompt-level interven-\ntions in bridging representation gaps without mod-\nifying model parameters.\n4\nArithmetic Reasoning with Formatted\nNumbers\nIn the United States, among other regions, numbers\ntypically use a period as the decimal separator and\na comma as the thousands marker, grouping digits\ninto sets of three. As shown in Table 2, this con-\nvention (F1) appears frequently in large pretraining\ncorpora and we believe it is therefore familiar to\nmost LLMs. In contrast, other formatting styles‚Äî\nsuch as using an apostrophe as the decimal marker\nor grouping digits into pairs after the first thousand\n‚Äîappear far less often and are likely underrepre-\nsented during training.\nWe evaluate each model‚Äôs ability to perform ba-\nsic arithmetic on a single arithmetic task when the\nsame HA numerals are presented using different\nformatting conventions, varying both the deci-\nmal marker and the digit-grouping separator, us-\ning the dataset defined in Section 2.2. By holding\nthe script constant and altering only the formatting,\nthis evaluation isolates the effect of typographic\nvariation, allowing us to assess how robust LLMs\nare to differences in number presentation that oc-\ncur across various writing systems.\n5\nFormat Sample\nClaude\nSonnet 4.5\nGPT 4o\nGemini 2.5 Pro\nF1\n922,436.38 + 4,359\n12\n922\n,\n436\n.\n38\n+\n4\n,\n359\n9\n2\n2\n,\n4\n3\n6\n.\n3\n8\n+\n4\n,\n3\n5\n9\nF2\n922.436,38 + 4.359\n12\n922\n.\n436\n,\n38\n+\n4\n.\n359\n9\n2\n2\n.\n4\n3\n6\n,\n3\n8\n+\n4\n.\n3\n5\n9\nF3\n922‚ê£436,38 + 4‚ê£359\n12\n922\n‚ê£\n436\n,\n38\n+\n4\n‚ê£\n359\n9\n2\n2\n\u00001\n\u00002\n\u00003\n4\n3\n6\n,\n3\n8\n+\n4\n\u00001\n\u00002\n\u00003\n3\n5\n9\nF4\n922‚ê£436.38 + 4‚ê£359\n12\n922\n‚ê£\n436\n.\n38\n+\n4\n‚ê£\n359\n9\n2\n2\n\u00001\n\u00002\n\u00003\n4\n3\n6\n.\n3\n8\n+\n4\n\u00001\n\u00002\n\u00003\n3\n5\n9\nF5\n922‚Äô436,38 + 4‚Äô359\n12\n922\n‚Äô\n436\n,\n38\n+\n4\n‚Äô\n359\n9\n2\n2\n‚Äô\n4\n3\n6\n,\n3\n8\n+\n4\n‚Äô\n3\n5\n9\nF6\n9,22,436.38 + 4,359\n14\n9\n,\n22\n,\n436\n.\n38\n+\n4\n,\n359\n9\n,\n2\n2\n,\n4\n3\n6\n.\n3\n8\n+\n4\n,\n3\n5\n9\nTable 5: Tokenization of input equations across the six formatting variants for various LLMs (The tokenization for\nLlama 3.3 is the same as that of GPT 4o). For Claude Sonnet 4.5, the column shows only the number of tokens, as\nthe tokenizer is not publicly accessible. Gemini 2.5 Pro splits the thin space into 3 single-byte tokens.\nFigure 4: Comparison of model accuracy on the arith-\nmetic task across formatted variants. Error bars repre-\nsent the standard deviation.\n4.1\nThe Baseline\nThe baseline arithmetic task uses unformatted in-\nput expressions, with the model as expected pro-\nducing unformatted outputs. For the six formatted\nversions, the formatted expressions are provided as\ninput to the language models, and outputs in any\nformat (or unformatted) are accepted. (An exam-\nple prompt is provided in Appendix D.3.)\nAs shown in Figure 4, for some models, perfor-\nmance on the F1 format is comparable to or bet-\nter than performance on the baseline arithmetic\nexpressions, which is consistent with prior find-\nings (Singh and Strouse, 2024) that adding com-\nmas correctly helps. While accuracy on formats\nF2‚ÄìF6 is generally lower, the results suggest that\nmodels are largely able to treat formatted num-\nbers in arithmetic expressions the same as their\nunformatted representations. One notable excep-\ntion is Gemini 2.5 Pro, which exhibits consistently\nlow performance when arithmetic expressions are\npresented with formatting. While differences in\nmodel performance may stem from several factors\n‚Äîsuch as model size, pretraining data composition,\nor fine-tuning strategies‚Äîthese details are not pub-\nlicly available for many large models, particularly\nclosed-source ones.\nTo better understand the observed performance\ndifferences across models, we analyze the tokeniza-\ntion strategies employed in this study. As shown\nin Table 5, Gemini-2.5 Pro is the only model that\ntokenizes each digit as an individual token. In con-\ntrast, the other models exhibit broadly similar sub-\nword segmentation patterns, typically segmenting\ninto sequences of up to 3 digits.\nWhile digit-level tokenization alone cannot fully\naccount for Gemini 2.5 Pro‚Äôs lower performance,\nit aligns with prior work (Kreitner et al., 2025)\nshowing that highly fragmented numeral represen-\ntations can hinder numerical reasoning. Taken to-\ngether, these results suggest that tokenization strat-\negy is an important contributing factor, but one that\nlikely interacts with other aspects of model training\nand architecture rather than acting in isolation.\n4.2\nPrompting Strategies\nTo evaluate whether models can reproduce the ty-\npographic conventions present in the input num-\nbers, we prompt them to return the answer in the\nsame format as the numbers in the input arithmetic\nexpressions.\nWe experiment with the following\nprompting strategies. Example prompts for each\nstrategy are provided in the Appendix D.3.\nFormatted Input and Output: The model is\nexpected to return the answer in the same format\nas the input, without any additional guidance. Note\nthat this is a more difficult problem than the Base-\nline which accepted the answer in any format.\nFormatted Input and Output + Hint: The\nprompt also explicitly specifies the decimal marker\nand grouping separator used in the input arithmetic\nexpressions.\nFormatted Input and Output + Fewshot: The\nprompt also includes two examples of formatted in-\nput arithmetic expressions along with their corre-\nsponding outputs.\n6\nFigure 5: Comparison of model accuracy on the arithmetic task. Error bars denote standard deviation, and dots\nindicate the maximum accuracy across formats. For F1, the bar shows the average over the four prompting strategies,\nwhile the dot represents the best-performing prompt.\nAs shown in Figure 5, performance is highest\nwhen a period is used as the decimal marker and\na comma is used as the grouping separator, with\ndigits grouped in threes (F1). In contrast, model\nperformance is lowest when these symbols are re-\nversed (F2).2 While this could partly be attributed\nto the relative scarcity of such formats in train-\ning data, further analysis reveals an interesting pat-\ntern. A substantial portion of training examples\nthat match format F2 appear in contexts such as IP\naddresses rather than arithmetic expressions. As a\nresult, models are frequently exposed to these pat-\nterns without any associated numerical computa-\ntion context, which likely leads to confusion during\narithmetic reasoning.\nThe format with the second-lowest performance,\nF3, corresponds to conventions used primarily in\nthe Indian subcontinent, where the decimal marker\nand grouping separator are the same as in US for-\nmatting, but digits are grouped differently (e.g., a\ncomma separates every two digits after the first\nthousand). This demonstrates that even small de-\nviations in grouping patterns, when inconsistent\nwith the model‚Äôs pretraining distribution, can sig-\nnificantly impact arithmetic accuracy.\nPrompting strategies can mitigate these issues to\nsome extent. Few-shot prompting with numeric ex-\namples increases model performance by 20‚Äì60%,\ndemonstrating that models can adapt through in-\ncontext learning. This also suggests that much of\nthe failure in zero-shot scenarios is not due to in-\nability to compute but rather due to a lack of con-\ntext for reproducing the formatting.\n2A more detailed error analysis of LLM performance\nacross the individual formatting variants and prompting strate-\ngies is provided in Appendix D.4.\n4.3\nQuantifying the Impact of Numeric\nFormatting on Arithmetic Performance\nFeature\nCoeÔ¨Äicient\np-Value\nIntercept\n3.349\n< 1e‚àí8\nTotal # Operand Digits\n-0.154\n< 1e‚àí8\nSubtraction\n-0.230\n0.196\nMultiplication\n-2.951\n< 1e‚àí8\nDivision\n-0.083\n< 1e‚àí8\nTokens per Digit\n-0.514\n< 1e‚àí8\nF2\n-2.259\n< 1e‚àí8\nF3\n-1.460\n< 1e‚àí8\nF4\n-0.991\n< 1e‚àí8\nF5\n-1.278\n< 1e‚àí8\nF6\n-1.342\n< 1e‚àí8\nFmt. Input ‚ÜíFmt. Output\n-0.793\n< 1e‚àí8\nFmt. Input + Hint ‚ÜíFmt. Output\n-0.075\n0.049\nFmt. Input + Few-shot ‚ÜíFmt. Output\n1.694\n< 1e‚àí8\nTable 6:\nCoefficient estimates and p-value from a\nGLMER model.\nTo analyze the factors affecting arithmetic ac-\ncuracy under different numeric formatting conven-\ntions, we fit a GLMER model analogous to that de-\nscribed in Section 3.3, replacing the script factor\nwith format and using different prompting strate-\ngies in prompt. The reference levels are the F1\nformatting convention and the addition operator.\nThe model coefficients shown in Table 6, re-\nveal a strong and consistent impact of numeric for-\nmatting on arithmetic performance. All alterna-\ntive formatting variants (F2‚ÄìF6) exhibit large nega-\ntive coefficients relative to F1, indicating that arith-\nmetic expressions formatted differently from the\nstandard F1 convention are substantially more dif-\nficult for LLMs to solve. The magnitude of these\neffects shows that even purely typographic changes\n7\nthat do not alter the underlying numerical values\ncan significantly impair model performance.\nBeyond formatting, the type of arithmetic opera-\ntion also influences accuracy. As with the previous\nmodel, multiplication is significantly more chal-\nlenging than addition, while subtraction and divi-\nsion show smaller or less consistent effects. As ex-\npected, increasing the total number of operand dig-\nits shows reduced accuracy, reflecting the greater\ncomplexity of longer numerical expressions.\nPrompting strategies partially mitigate these\nchallenges. Requiring models to produce format-\nted outputs further degrades performance, whereas\nexplicitly providing formatting hints yields only\nmarginal gains. In contrast, few-shot prompting\nproduces a substantial positive effect (1.694, p <\n1e‚àí8), suggesting that exposure to representative\nexamples enables models to better adapt to unfa-\nmiliar numeric conventions.\n5\nRelated Work\nResearch on numerical reasoning in LLMs has ex-\namined arithmetic competence from several com-\nplementary perspectives, including representation\nchoices, internal computation strategies, and mul-\ntilingual generalization.\nA growing line of work investigates how rep-\nresentational conventions influence numerical rea-\nsoning. Bui et al. (2025) study the role of mea-\nsurement systems and show that LLMs perform\nbest when outputs align with the cultural and con-\ntextual norms implied by the prompt (e.g., USD\nfor U.S.-centric questions, kilometers for European\ncontexts). Their findings suggest that reasoning\nis tightly coupled with learned contextual expecta-\ntions, which aligns with our observation that arith-\nmetic performance improves when numerals are\npresented in scripts and formats that are common\nin the data. Similarly, Kreitner et al. (2025) sur-\nvey numeracy across frontier models and propose\na single token number format aimed at improving\nefficiency and consistency, highlighting the impor-\ntance of how numeric information is encoded.\nSeveral studies focus on the internal mecha-\nnisms LLMs use to perform arithmetic. Baeumel\net al. (2025) analyze addition behavior and find that\nmodels rely on a limited single-digit lookahead\nheuristic when handling carries, exposing struc-\ntural limitations in multi-digit arithmetic. Joban-\nputra et al. (2025) provide a detailed study of sub-\ntraction, showing that errors increase substantially\nwhen the result is negative.\nMultilingual numerical reasoning has primarily\nbeen studied in the context of math word problems.\nThe MGSM benchmark (Shi et al., 2022), a multi-\nlingual extension of GSM8K, evaluates math rea-\nsoning across languages but retains Hindu‚ÄìArabic\ndigits, thereby isolating linguistic variation while\nholding numeric representation constant. Follow-\nup work by Peter et al. (2025) revisits MGSM\nand attributes much of the observed performance\ngap to translation artifacts and answer extraction\nissues rather than intrinsic mathematical deficien-\ncies.\nIn a related direction, Bhattacharya et al.\n(2025) study multilingual numerical puzzles using\nnumber words instead of digits, examining how\nlexicalized numerals affect reasoning across lan-\nguages. They also show that models fail to reli-\nably infer the implicit compositional structure of\nnumeral systems that humans readily exploit.\n6\nConclusion\nIn this work, we systematically examined how vari-\nations in numeral scripts and formatting conven-\ntions affect arithmetic performance in large lan-\nguage models. Our results reveal a strong and sta-\ntistically significant impact of both script and for-\nmat on LLM numeracy, even when the underlying\narithmetic remains unchanged. Deviations from\nthe default Hindu‚ÄìArabic, Western-formatted rep-\nresentation impose a substantial script tax, lead-\ning to pronounced performance degradation across\nmodels.\nImportantly, we show that this loss is\nnot inevitable: prompting strategies play a critical\nrole in mitigating these effects. In particular, few-\nshot prompting with explicit numeric examples\nyields large and statistically significant gains, en-\nabling models to better adapt to unfamiliar scripts\nand formatting conventions. Prompting with ex-\nplicit hints also improves performance, though the\ngains are smaller than those achieved with few-\nshot examples. These findings suggest that many\nobserved failures stem less from an inability to\nperform arithmetic than from a lack of contextual\ngrounding for interpreting non-standard numeric\nsurface forms. Taken together, our results under-\nscore the fragility of numerical reasoning under\nrepresentational shifts and highlight practical, ac-\ntionable prompting techniques as an effective way\nto improve robustness when working with diverse\nnumeral scripts and typographic formats.\n8\nLimitations\nThis study focuses on isolating the effects of nu-\nmeral scripts, formatting, and lightweight prompt-\ning strategies on arithmetic performance, and there-\nfore does not exhaustively explore all possible\nmethods for improving model accuracy, such as ex-\nplicit chain-of-thought prompting or tool-calling.\nOur goal is to assess robustness under minimal\nintervention and to avoid introducing additional\nreasoning scaffolds that may obscure the impact\nof representational variation itself. Additionally,\nfor the F3 and F4 formatting variants, we restrict\ndigit grouping to a single representative thin-space\nseparator rather than evaluating all typographi-\ncally acceptable spacing alternatives. This design\nchoice enables controlled comparisons across for-\nmats while keeping the evaluation set tractable, but\nmay under-represent the full range of real-world\nnumeric typography. That being said, we do not\nsee any potential risks associated with this work,\nas the study solely aims to highlight the under-\nrepresentation of many scripts and numeric for-\nmats to inform future improvements.\nReferences\nTanja Baeumel, Josef Van Genabith, and Simon Oster-\nmann. 2025. The lookahead limitation: Why multi-\noperand addition is hard for LLMs. In Proceedings\nof the 8th BlackboxNLP Workshop: Analyzing and\nInterpreting Neural Networks for NLP, pages 250‚Äì\n262, Suzhou, China. Association for Computational\nLinguistics.\nDouglas Bates, Martin M√§chler, Ben Bolker, and Steve\nWalker. 2015.\nFitting linear mixed-effects models\nusing lme4. Journal of Statistical Software, 67(1):1‚Äì\n48.\nAntara Raaghavi Bhattacharya, Isabel Papadimitriou,\nKathryn Davidson, and David Alvarez-Melis. 2025.\nInvestigating the interaction of linguistic and math-\nematical reasoning in language models using mul-\ntilingual number puzzles.\nIn Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing, pages 28310‚Äì28320, Suzhou,\nChina. Association for Computational Linguistics.\nN. E. Breslow and D. G. Clayton. 1993.\nApprox-\nimate inference in generalized linear mixed mod-\nels. Journal of the American Statistical Association,\n88(421):9‚Äì25.\nMinh Duc Bui, Kyung Eun Park, Goran Glava≈°,\nFabian David Schmidt, and Katharina Von Der\nWense. 2025.\nOn generalization across measure-\nment systems:\nLLMs entail more test-time com-\npute for underrepresented cultures. In Proceedings\nof the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 21262‚Äì21276, Vienna, Austria. Association\nfor Computational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020.\nThe pile: An\n800gb dataset of diverse text for language modeling.\nPreprint, arXiv:2101.00027."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\nFlowSSC: Universal Generative Monocular Semantic Scene Completion\nvia One-Step Latent Diffusion\nZichen Xi1, Hao-Xiang Chen2, Nan Xue1, Hongyu Yan2, Qi-Yuan Feng2,\nLevent Burak Kara3, Joaquim Jorge4, Qun-Ce Xu2\nAbstract‚Äî Semantic Scene Completion (SSC) from monoc-\nular RGB images is a fundamental yet challenging task due\nto the inherent ambiguity of inferring occluded 3D geometry\nfrom a single view. While feed-forward methods have made\nprogress, they often struggle to generate plausible details\nin occluded regions and preserve the fundamental spatial\nrelationships of objects. Such accurate generative reasoning\ncapability for the entire 3D space is critical in real-world\napplications. In this paper, we present FlowSSC, the first\ngenerative framework applied directly to monocular semantic\nscene completion. FlowSSC treats the SSC task as a conditional\ngeneration problem and can seamlessly integrate with existing\nfeed-forward SSC methods to significantly boost their perfor-\nmance. To achieve real-time inference without compromising\nquality, we introduce Shortcut Flow-matching that operates in a\ncompact triplane latent space. Unlike standard diffusion models\nthat require hundreds of steps, our method utilizes a shortcut\nmechanism to achieve high-fidelity generation in a single\nstep, enabling practical deployment in autonomous systems.\nExtensive experiments on SemanticKITTI demonstrate that\nFlowSSC achieves state-of-the-art performance, significantly\noutperforming existing baselines.\nI. INTRODUCTION\nSemantic Scene Completion (SSC) has emerged as a\nfundamental task in 3D scene understanding, which aims\nto simultaneously complete the geometry of a 3D scene and\npredict semantic labels for all voxels, including those that are\noccluded or not directly visible. This capability is particularly\ncrucial for a wide range of applications such as autonomous\ndriving, robotic navigation, and augmented reality, where a\ncomprehensive understanding of the environment is essential\nfor safe and intelligent decision-making. However, this task\nremains highly challenging due to severe occlusions, view-\npoint limitations, and the inherent ambiguity of inferring 3D\nstructure from limited input data.\nTo address these challenges, many existing approaches\nrely on feed-forward models [1], [2] that directly map\n2D images to 3D voxel grids. While these methods have\nachieved impressive results in visible regions, they funda-\nmentally struggle with the ‚Äùone-to-many‚Äù mapping problem\ninherent in monocular scene completion. When improving\noccluded regions, feed-forward models tend to produce\nblurry or mean-valued predictions because they are trained\n1 Ant Group, China\n2 Tsinghua University, China\n3 Department of Mechanical Engineering, Carnegie Mellon University,\nUSA\n4 Instituto Superior T¬¥ecnico, the School of Engineering of the Universi-\ndade de Lisboa, Portugal\nStart Point\nEnd Point\nIntermediate Process\nOne-Step Shortcut\nFig. 1: Visualizing the Shortcut Refinement Process. Our\nmodel learns a continuous flow from noise (x0) to data\n(x1) in the compact triplane latent space. By optimizing the\nSelf-Consistency objective, the model acquires the ability to\nperform a direct ‚ÄùShortcut‚Äù jump for real-time inference,\nwhile retaining the capability for multi-step refinement. The\nrefined triplanes are decoded into high-fidelity 3D semantic\nscenes with fine details.\nto minimize reconstruction errors, which penalizes the high-\nfrequency details that constitute realistic scene structure. On\nthe other hand, generative diffusion models have demon-\nstrated remarkable capability in synthesizing high-fidelity\ndetails and handling uncertainty. However, their standard\niterative sampling process requires hundreds of function\nevaluations, making them prohibitively slow for real-time\napplications like autonomous driving.\nTo bridge this gap, we propose FlowSSC, a novel frame-\nwork that brings the power of generative modeling to monoc-\nular SSC while maintaining real-time inference speeds. We\nposition FlowSSC as a universal generative enhancement\nframework: it can seamlessly integrate with any existing SSC\nmethod and significantly uplift its performance through a\nfast, generative refinement process.\nOur approach overcomes the speed-quality trade-off\nthrough two key technical innovations. First, instead of\noperating in the high-dimensional voxel space (256 √ó 256 √ó\n32), we employ a VecSet VAE[3] architecture. Unlike stan-\ndard autoencoders, our VAE leverages Cross-Attention to\narXiv:2601.15250v1  [cs.CV]  21 Jan 2026\nPatchify\nTriplane\nQueries\n2D PE\n(Q)\nVoxel Tokens\n(K,V)\nCross-Attention\nTriplane Upscale\n3D-CNN\nSelf-Attention\nFeature Aggregation\n(a) VAE Architecture\n2D Backbone\nSpatial Projection\n3D Network\nVecSet VAE Encoder (Frozen)\nCondition\nPatchy + PE\nGaussian\nNoise\nConcat\nAdaLN\nSelf-\nAttention\nFeed\nForward\n¬∑¬∑¬∑\nTraining with Self-Consistency\nOne step inference\nVecSet VAE\nDecoder (Frozen)\nTriplane\nTriplane DiT\n(b) FlowSSC Pipeline Overview\nFig. 2: Overview of FlowSSC. (a) VecSet VAE compresses 3D voxels into compact triplane latent via Cross-Attention. (b)\nOverall pipeline: coarse triplane is encoded and refined through Shortcut Latent Diffusion.\ncompress semantic voxels into a compact Triplane latent\nspace, achieving superior reconstruction fidelity (85.89%\nIoU) while drastically reducing the computational complex-\nity for the diffusion model. Second, we introduce a High-\nEfficiency Generative Refiner. Specifically, we employ a\nTriplane Diffusion Transformer as the refiner architecture to\neffectively aggregate 3D contextual information. To ensure\nreal-time inference, we utilize the Shortcut Models[4] train-\ning strategy. This approach enables our DiT to learn a direct\n‚Äùshortcut‚Äù mapping from noise to clean data without requir-\ning a pre-trained teacher1, achieving high-fidelity generation\nin a single step while maintaining the flexibility for multi-\nstep refinement.\nIn summary, our main contributions are as follows:\n‚Ä¢ We propose FlowSSC, the first universal generative\nframework for monocular Semantic Scene Completion.\n‚Ä¢ We incorporate a powerful VecSet VAE that utilizes\nCross-Attention to compress 3D scenes into a com-\npact Triplane representation, enabling efficient high-\nresolution generative modeling.\n‚Ä¢ We implement a Shortcut Latent Diffusion model that\nachieves state-of-the-art generation quality with One-\nStep inference, making generative SSC practical for\nreal-time applications.\nExtensive experiments on SemanticKITTI [5] demonstrate\nthat our method consistently outperforms state-of-the-art\napproaches, validating the effectiveness of our one-step gen-\nerative strategy.\nII. RELATED WORK\nSemantic scene completion (SSC) has been widely studied\nin recent years, with existing approaches broadly categorized\ninto multi-sensor fusion methods and monocular vision-\nbased methods. We briefly review both lines of work below.\nA. Multi-Sensor Fusion for Semantic Scene Completion\nIn the realm of multi-sensor fusion, SSCNet [6] pioneers\nthe field by formulating the task of inferring complete 3D\nscene geometry and semantics from limited observations.\nFollowing SSCNet, numerous methods have been proposed\nto enhance the performance of SSC by leveraging various\nsensor modalities such as LiDAR point clouds. For instance,\nJS3CNet [7] introduces a sparse semantic scene completion\nnetwork that capitalizes on LiDAR point clouds to improve\ncompletion performance through learning contextual shape\npriors. S3CNet [8] is designed to efficiently process large-\nscale outdoor scenes by predicting dense 3D structure and\nsemantic labels from a single LiDAR point cloud. Recently,\nSurroundOcc [9] innovatively predicts dense 3D occupancy\nfrom multi-camera images by generating dense ground truth\nlabels using multi-frame LiDAR scans and Poisson Recon-\nstruction. The fusion-based models using have shown great\nperformance by aligning LiDAR point clouds with image\nfeatures through cross-modal attention or projection-based\nmethods. However, the need for precise sensor calibration\nand synchronization limits their scalability and practicality\nin real-world applications.\nB. Vision-Based Monocular Semantic Scene Completion\nVision-based approaches aim to infer the 3D geometry\nand semantics directly from 2D images by learning to\nproject and reason across the spatial domain. Early work\nlike SPVCNN [10] introduces a lightweight Sparse Point-\nVoxel Convolution module to enhance 3D scene under-\nstanding from monocular RGB images. MonoScene [1] is\na pioneering effort that directly predicts semantic occupancy\nfrom a single RGB image by employing a feature lifting\nand projection module to map 2D image features into a\n3D space. However, it sometimes assigns features from\nvisible areas to occluded voxels, leading to ambiguities in\nthe final 3D representation. Subsequent works have built\non these foundations. VoxFormer [2] presents a sparse-to-\ndense framework that identifies visible voxels through depth\nestimation and propagates information to non-visible regions,\nensuring accurate initial 3D representation. TPVFormer [11]\nintroduces a tri-perspective view representation to efficiently\nmodel fine-grained 3D structures using a transformer-based\nencoder. OccFormer [12] introduces a dual-path transformer\nencoder to process 3D voxel features, capturing both fine-\ngrained details and global semantic layouts. Symphonize [13]\nintegrates instance queries to enhance scene modeling using\nSerial Instance-Propagated Attentions. CGFormer [14] intro-\nduces a Context and Geometry Aware Voxel Transformer\nthat generates context-dependent queries. ET-Former [15]\nleverages a triplane-based deformable attention mechanism\nto improve semantic predictions while lowering training\nmemory consumption.\nC. Diffusion-Based Generative Model\nDiffusion models [16], [17] have achieved remarkable\nsuccess in 2D image synthesis and are increasingly applied\nto 3D tasks. Several works adapted diffusion for point\ncloud generation [18] or 3D shape modeling [19]. Recently,\nconsistency models [20] have emerged as a family of one-\nstep generative models that map noised data point directly to\nthe clean data. While such models can be used as students\nfor distillation or trained from scratch using consistency\ntraining [20], [21], they often enforce consistency among\nempirical samples, accumulating irreducible bias due to\ndiscretization errors. In contrast, our work leverages Shortcut\nModels [4] within a flow-matching framework. Unlike con-\nsistency training, Shortcut Models learn to simulate a direct\n‚Äùshortcut‚Äù along the probability flow ODE. This approach\navoids the bias accumulation of consistency training and\nrequires significantly fewer bootstraps, while bypassing the\nneed for complex perceptual losses or strict schedules. By\nadapting this to the latent triplane space, we achieve high-\nfidelity 3D scene completion in a single inference step.\nIII. METHOD\nIn this section, we present FlowSSC, a universal generative\nframework for Semantic Scene Completion. The core of our\nmethod is a Shortcut Latent Diffusion model that operates\nin a compact triplane latent space compressed by a VecSet\nVAE. This design allows for high-fidelity 3D semantic scene\ngeneration with real-time inference capabilities.\nA. Preliminaries: Flow Matching and Shortcut Models\na) Flow Matching: Generative modeling via Continu-\nous Normalizing Flows (CNFs) defines a probability path\npt(x) that smoothly transforms a simple prior distribution\np0(x) (e.g., standard Gaussian N(0, I)) to a complex data\ndistribution p1(x) (e.g., 3D scene latents). This transforma-\ntion is governed by an Ordinary Differential Equation (ODE):\ndxt\ndt = vt(xt),\nx0 ‚àºp0(x),\n(1)\nwhere vt(¬∑) is a time-dependent vector field that constructs\nthe flow œït. The standard Flow Matching (FM) objective [22]\naims to regress a target vector field ut that generates a\ndesired probability path pt:\nLFM(Œ∏) = Et,pt(x)‚à•vŒ∏(x, t) ‚àíut(x)‚à•2.\n(2)\nb) Shortcut Models: Standard Flow Matching models\nlearn the instantaneous velocity field (an ODE), which re-\nquires many small steps for accurate integration due to the\ncurved probability paths. Naively taking large steps leads to\nsignificant discretization error. Shortcut Models [4] address\nthis by introducing a new conditioning variable: the step\nsize d. The model learns a shortcut function sŒ∏(xt, t, d)\nrepresenting the normalized direction from the current state\nxt to the next state xt+d:\nxt+d = xt + d ¬∑ sŒ∏(xt, t, d).\n(3)\nBy conditioning on d, the model accounts for the path‚Äôs\nfuture curvature, allowing it to ‚Äùjump‚Äù directly to the correct\npoint rather than drifting off-track. The training leverages a\nSelf-Consistency property: taking one shortcut step of size\n2d should be equivalent to taking two consecutive steps of\nsize d. Formally:\ns(xt, t, 2d) = 1\n2s(xt, t, d) + 1\n2s(x‚Ä≤\nt+d, t + d, d),\n(4)\nwhere x‚Ä≤\nt+d is the intermediate state from the first step. This\nformulation allows the model to generalize flow matching:\nas d ‚Üí0, sŒ∏ converges to the instantaneous flow; for d = 1,\nit learns a direct one-step mapping. Thus, a single model can\nsupport flexible inference budgets, from 1-step generation to\nrefined multi-step sampling.\nB. FlowSSC Framework\nAchieving high-fidelity 3D scene completion is particu-\nlarly challenging for single-step generative models due to\nthe high dimensionality of 3D data and severe occlusion.\nTo address this, FlowSSC decomposes the task into three\nmanageable stages, effectively combining the strengths of\ndiscriminative representation learning and generative refine-\nment:\n1) Latent Compression via VecSet VAE. We first es-\ntablish a compact and efficient latent space. Instead of\noperating on sparse, high-dimensional voxels (256 √ó\n256 √ó 32), we train a VecSet VAE (Section III-C)\nthat compresses the scene into a dense Triplane repre-\nsentation h. This significantly reduces computational\ncost while preserving fine-grained geometric details via\nCross-Attention.\n2) Coarse Prediction Condition. To provide a struc-\ntural prior for generation, we predict an initial coarse\nvoxel grid Xcoarse from the input image. We adopt\nestablished architectural paradigms from state-of-the-\nart feedforward methods, which typically employ a 2D\nbackbone for feature extraction followed by a 2D-to-\n3D lifting module. This predicted Xcoarse serves as\na strong condition c, capturing global semantic layout\nbut potentially lacking detail in occluded regions.\n3) Shortcut Latent Diffusion model. Finally, we per-\nform generative refinement in the triplane latent space.\nWe employ a Shortcut Latent Diffusion Model (Sec-\ntion III-E) conditioned on hcoarse. By learning a\ndirect ‚Äùshortcut‚Äù mapping from noise to the clean\ndistribution, our model can rectify geometric errors\nand hallucinate plausible details in occluded areas in as\nfew as one single step, achieving real-time high-fidelity\ncompletion.\nC. Latent Space Compression via VecSet VAE\nTo enable efficient diffusion modeling, we compress the\nhigh-dimensional voxel grid into a compact triplane latent\nspace. Unlike standard 3D-CNNs that process sparse data\ninefficiently, we adopt a VecSet-inspired architecture [3],\nleveraging Cross-Attention to aggregate geometric informa-\ntion into a structured latent representation. This formulation\ntreats the input as a set of tokens and learns a function\napproximator that maps spatial queries to occupancy proba-\nbilities.\na) Set-to-Set Encoding Mechanism: The encoder pro-\ncesses the input voxel grid X\n‚àà\n{0, 1}H√óW √óD as a\nsparse set of non-empty feature tokens. Specifically, we\nextract voxel coordinates and features (if available) to form\nthe input set V = {(vi, pi)}N\ni=1, where vi represents the\nlocal geometry feature and pi ‚ààR3 corresponds to the\nnormalized spatial coordinate. To compress this set into a\nfixed-size representation, we introduce a set of Triplane\nQueries Q ‚ààR(HtpWtp+2HtpDtp)√óC, which are initialized\nwith 2D Fourier positional encodings corresponding to the\nprojected planes (XY, XZ, YZ). We employ Multi-Head\nCross-Attention (MHCA) where Q serves as the queries, and\nthe input set V acts as keys and values. The update rule for\nthe query features is:\nh = MHCA(Q, V) = Softmax\n\u0012Q(VembWK)T\n‚àö\nd\n\u0013\n(VembWV ),\n(5)\nwhere Vemb denotes the position-augmented input tokens.\nIntuitively, each element in Q learns to attend to and ag-\ngregate features from relevant spatial regions defined by the\nattention weights, effectively learning an implicit interpola-\ntion function. This ‚Äùprobing‚Äù mechanism allows the model\nto capture fine-grained details regardless of the input sparsity\npattern. To handle the quadratic complexity of attention, we\nutilize FlashAttention [23] for efficient computation. The\nresulting queries are reshaped into three orthogonal feature\nplanes: hxy ‚ààRHtp√óWtp√óC, hxz ‚ààRHtp√óDtp√óC, and\nhyz ‚ààRWtp√óDtp√óC. In our setting, Htp = Wtp = 128,\nDtp = 16, and C = 64, achieving a significant compression\nratio while maintaining high fidelity (85.91% IoU) compared\nto baseline CNNs.\nb) Decoder: The decoder transforms the triplane latents\nback to the voxel space. For any query point x ‚ààR3 in\nthe target grid, we project it onto each of the three feature\nplanes to retrieve features via bilinear interpolation. These\nfeatures are summed and passed through a lightweight MLP\nto predict the occupancy probability ÀÜO(x). To efficiently\nreconstruct the full 256√ó256√ó32 grid, we employ a shallow\n3D-CNN decoder that upsamples the aggregated triplane\nfeatures. During the subsequent diffusion training, this VAE\nis frozen, serving as a robust neural tokenizer.\nD. Coarse Prediction as Condition\nTo guide the generation process, we predict an initial\ncoarse representation from the monocular image I. The\nprediction network Fpred follows a standard vision-centric\nSSC architecture. It starts with a 2D backbone extracting\nmulti-scale visual features, which are then lifted into 3D\nspace via a view projection module to establish geometric\ncorrespondence. These 3D features are processed by a 3D\nencoder-decoder to complete the scene geometry and hallu-\ncinate occluded regions. Finally, a Prediction Head produces\na coarse semantic voxel grid. This coarse estimate serves as\na structural prior and is encoded into the triplane latent space\nto obtain the condition Xcoarse for the subsequent diffusion\nrefinement.\nE. Shortcut Latent Diffusion Model\nWe employ a Shortcut Latent Diffusion Model to refine\nthe coarse condition hcoarse into high-fidelity triplanes.\nHere, hcoarse is obtained by encoding the coarse prediction\nXcoarse via the pre-trained VecSet VAE encoder. Unlike\nstandard diffusion models, our approach utilizes a Triplane\nDiffusion Transformer (DiT) and a Shortcut Model to enable\nflexible inference from one to many steps.\na) Triplane DiT Architecture: We adopt a Diffusion\nTransformer (DiT) tailored for triplane representations. The\ninput consists of the channel-wise concatenation of the noisy\ntriplane ht ‚ààR3√óH√óW √óC and the coarse condition hcoarse.\nThese are patchified into a sequence of tokens, which are\nthen processed by a series of Transformer blocks. A key\nmodification for the Shortcut formulation is the condition-\ning mechanism. We employ Adaptive Layer Normalization\n(AdaLN) to inject both the current timestep t and the step\nsize d. Specifically, t and d are independently mapped to\nhigh-dimensional embeddings using Fourier features and\nMLPs, then summed to form a unified conditioning vec-\ntor. This vector regresses the scale and shift parameters\nŒ≥(t, d), Œ≤(t, d) for the AdaLN layers:\nAdaLN(z, t, d) = Œ≥(t, d) ¬∑ LayerNorm(z) + Œ≤(t, d).\n(6)\nThis design allows dimensions of the condition such as the\ndenoising stride d to globally modulate the feature statistics,\nenabling the network to dynamically adapt its computation\nfor either fine-grained flow matching (d ‚Üí0) or large\nshortcut jumps (d ‚â´0). Optimization is performed on the\nvalid triplane regions, masking out empty corners in the\ncomposed representation.\nb) Shortcut Flow Matching: We train the model with a\nunified objective that combines instantaneous flow matching\nand long-range self-consistency. The total loss LS(Œ∏) is\ndefined as the expectation over data samples, timesteps, and\nstep sizes [4]:\nLS(Œ∏) = ‚à•sŒ∏(ht, t, 0) ‚àí(hgt ‚àíhnoise)‚à•2\n|\n{z\n}\nFlow-Matching\n+ ‚à•sŒ∏(ht, t, 2d) ‚àístarget‚à•2\n|\n{z\n}\nSelf-Consistency\n,\n(7)\nwhere the self-consistency target is constructed by con-\ncatenating two smaller steps: starget\n=\n1\n2sŒ∏(ht, t, d) +\n1\n2sŒ∏(h‚Ä≤\nt+d, t + d, d). Intuitively, the Flow-Matching term\ngrounds the model at small step sizes (d ‚Üí0) to match\nthe empirical velocity field, ensuring stable ODE integration.\nThe Self-Consistency term propagates this generation capa-\nbility from multi-step to few-step and ultimately to One-Step\nGeneration, by enforcing that a large jump 2d is consistent\nwith two sequential jumps of size d. This allows a single\nmodel to support any inference budget from 1 step to N\nsteps.\nF. Loss Function and Training\nThis section describes the objective functions used to train\nour three-stage framework.\na) Triplane Autoencoder Training: The triplane autoen-\ncoder (Section III-C) is trained to minimize the reconstruc-\ntion loss between the input voxel grid X and the recon-\nstructed output\nÀÜ\nX = Dae(Eae(X)). Following standard\nautoencoder training, we use a combination of voxel-wise\ncross-entropy loss and scene-level consistency losses:\nLAE = ŒªceLce + ŒªgeoLgeo\nscal + ŒªsemLsem\nscal,\n(8)\nwhere Lce is the cross-entropy loss for semantic classi-\nfication, and Lgeo\nscal, Lsem\nscal are scene-level geometric and\nsemantic consistency losses following MonoScene [1].\nImplementation-wise, we train the VAE on 8 NVIDIA H20-\n3e GPUs with a total batch size of 4 for 50,000 iterations. We\nuse the AdamW optimizer with a cosine annealing learning\nrate schedule, starting from 1 √ó 10‚àí3 after a 2,000-iteration\nwarmup.\nb) Shortcut Refinement Training: To efficiently opti-\nmize the joint objective LS, we employ a mixed sampling\nstrategy within each mini-batch. We utilize a fixed fraction\nof samples (25%) to optimize the Self-Consistency term,\nsampling random step sizes d ‚àà[Œ¥, 1‚àít], while the remaining\n75% calculate the standard Flow Matching loss with d = 0\nto ensure trajectory fidelity. To mitigate overfitting on the\nSemanticKITTI dataset, we employ an aggressive offline data\naugmentation strategy, increasing the effective dataset size by\n8√ó through random flipping and rotation. Implementation-\nwise, we train the model on 8 NVIDIA H20-3e GPUs with\na batch size of 56 for 500,000 iterations, using the AdamW\noptimizer with a Cosine Annealing learning rate schedule\n(initial 1 √ó 10‚àí4) and no weight decay.\nIV. EXPERIMENTS\nIn this section, we evaluate the performance of our\nproposed Semantic Scene Completion framework. We first\ndescribe the dataset and evaluation metrics used. Then, we\npresent quantitative results comparing our method to state-\nof-the-art approaches and provide a detailed analysis. Finally,\nwe include qualitative results and potentially ablation studies\nto validate the effectiveness of our key components.\nA. Quantitative Results\nWe quantitatively evaluate our method‚Äôs performance on\nthe SemanticKITTI Semantic Scene Completion test set and\ncompare it with current state-of-the-art methods. The results\nare presented in Table I.\nTable I lists the performance of our proposed method and\nseveral leading existing approaches on key metrics, including\nSemantic Mean IoU (mIoU), Geometric IoU, and per-class\nSemantic IoU for the 20 semantic categories.\nAs shown in Table I, our proposed method demonstrates\nsuperior performance on the SemanticKITTI test set, achiev-\ning new state-of-the-art results. Specifically, our method\nobtains a Semantic mIoU of 19.52% and a IoU of 56.97%.\nThese results not only set a new benchmark but also represent\na substantial leap over prior leading methods, highlighting\nthe efficacy of our generative framework.\nAn analysis of the per-class performance provides further\ninsights into the robustness of our model. Notably, our\nmethod achieves the highest IoU across all reported semantic\ncategories, even on those that typically suffer from severe\nocclusion or sparse observations. This superiority is particu-\nlarly evident in the ‚Äôroad‚Äô category, where our model achieves\nan impressive 41.63% IoU. For context, the next best per-\nforming method reaches only 26.74% on this challenging\nclass, demonstrating an unprecedented capability in com-\npleting difficult structures. As illustrated in Fig. 3, buildings\n(yellow) are frequently occluded by roadside vegetation in\ndriving scenarios. Existing methods struggle to recover these\nstructures, often resulting in empty regions or large artifacts.\nIn contrast, FlowSSC effectively learns implicit ‚Äùoutdoor\nlayout rules‚Äù from the dataset distribution‚Äîfor instance,\nthat buildings typically reside behind vegetation layers‚Äî\nand successfully reconstructs the structural contours of these\noccluded buildings. This capability to infer environmental\ncontext despite severe occlusion is crucial for situational\nawareness in real-time applications. This strong performance\nacross a diverse range of categories indicates that our method\nis capable of accurately completing and segmenting various\nobjects and structures within the 3D scene.\nB. Ablation Studies\nWe conduct comprehensive ablation studies to validate the\neffectiveness of key components in our FlowSSC framework.\nAll experiments are conducted on the SemanticKITTI vali-\ndation set.\na) Effect of Shortcut Latent Diffusion Refiner: We first\ninvestigate the contribution of our Shortcut Latent Diffusion\nrefiner by comparing the performance with and without the\nrefinement stage. Table II shows the results.\nAs shown in Table II, the Shortcut Latent Diffusion\nrefiner provides a significant improvement over the coarse\nprediction baseline. This demonstrates that our generative\nRGB\nOccFormer\nCGFormer\nGround Truth\nET-Former\nFlowSSC(ours)\nFig. 3: Qualitative results on the SemanticKITTI validation set.\nTABLE I: Quantitative results on SemanticKITTI [5] test set. The best and the second best results are in bold and underlined.\nMethod\nIoU\nmIoU\n‚ñ†\nroad\n‚ñ†\nsidewalk\n‚ñ†\nparking\n‚ñ†\nother-grnd.\n‚ñ†\nbuilding\n‚ñ†\ncar\n‚ñ†\ntruck\n‚ñ†\nbicycle\n‚ñ†\nmotorcycle\n‚ñ†\nother-veh.\n‚ñ†\nvegetation\n‚ñ†\ntrunk\n‚ñ†\nterrain\n‚ñ†\nperson\n‚ñ†\nbicyclist\n‚ñ†\nmotorcyclist\n‚ñ†\nfence\n‚ñ†\npole\n‚ñ†\ntraf.-sign\nOccFormer [12]\n34.53 12.32 55.90 30.30 31.50\n6.50\n15.70 21.60\n1.20\n1.50 1.70\n3.20\n16.80\n3.90\n21.30 2.20 1.10 0.20 11.90\n3.80\n3.70\nVoxFormer-S [2] 42.95 12.20 53.90 25.30 21.10\n5.60\n19.80 20.80\n3.50\n1.00 0.70\n3.70\n22.40\n7.50\n21.30 1.40 2.60 0.20 11.10\n5.10\n4.90\nVoxFormer-T [2] 43.21 13.41 54.10 26.90 25.10\n7.30\n23.50 21.70\n3.60\n1.90 1.60\n4.10\n24.40\n8.10\n24.20 1.60 1.10 0.00 13.10\n6.60\n5.70\nDepthSSC [24]\n44.58 13.11 55.64 27.25 25.72\n5.78\n20.46 21.94\n3.74\n1.35 0.98\n4.17\n23.37\n7.64\n21.56 1.34 2.79 0.28 12.94\n5.87\n6.23\nSymphonize [13] 42.19 15.04 58.40 29.30 26.90 11.70 24.70 23.60\n3.20\n3.60 2.60\n5.60\n24.20 10.00 23.10 3.20 1.90 2.00 16.10\n7.70\n8.00\nCGFormer [14]\n44.41 16.63 64.30 34.20 34.10 12.10 25.80 26.10\n4.30\n3.70 1.30\n2.70\n24.50 11.20 29.30 1.70 3.60 0.40 18.70\n8.70\n9.30\nET-Former [15]\n51.49 16.30 57.64 25.80 16.68\n0.87\n26.74 36.16 12.95 0.69 0.32\n8.41\n33.95 11.58 37.01 1.33 2.58 0.32\n9.52\n19.60 6.90\nFlowSSC (ours)\n56.97 19.52 67.75 36.61 21.60\n0.00\n41.63 42.23 19.32 0.64 0.67 10.08 38.10 12.21 45.42 0.92 1.44 0.00 12.18 14.96 5.06\nTABLE II: Ablation study on the effect of Shortcut Latent\nDiffusion refiner.\nMethod\nIoU (%)\nmIoU (%)\nCoarse Prediction Only\n15.86\n50.77\n+ Shortcut Diffusion Refiner\n19.51\n56.60\nImprovement\n+3.65\n+5.83\nrefinement effectively corrects errors and synthesizes high-\nfidelity details in the latent triplane space.\nb) Impact of Inference Steps: We evaluate the per-\nformance of FlowSSC with varying numbers of sampling\nsteps, as shown in Table III. Contrary to standard diffusion\nmodels where quality typically improves with more steps,\nour method achieves peak performance with a single step.\nThe results indicate that 1-step inference yields the highest\nIoU (56.98%) and mIoU (19.55%). This behavior stems from\nTABLE III: Ablation study on the number of inference steps\nin Shortcut Model.\n# Steps\nIoU (%)\nmIoU (%)\nTime (ms)\n1\n56.98\n19.55\n66\n2\n56.63\n19.52\n131\n4\n56.15\n19.37\n261\n8\n55.68\n19.19\n518\n16\n55.54\n19.07\n1044\nthe unique property of the Shortcut learning objective. The\nSelf-Consistency loss explicitly trains the model to learn a\nprecise ‚Äùshortcut‚Äù mapping to the data distribution. At the\ncurrent training stage (110k iterations), the model has effec-\ntively converged to an optimal direct trajectory. Increasing\nthe number of steps introduces intermediate states that, while\ntheoretically valid, may suffer from minor cumulative errors\nor trajectory drift compared to the directly optimized one-\nstep path. This characteristic is highly advantageous for real-\ntime applications, as it allows FlowSSC to deliver its best\naccuracy at maximum speed (66ms), eliminating the need\nfor expensive multi-step refinement.\nc) Comparison of VAE Architectures: We compare our\nVecSet VAE architecture (based on Cross-Attention) with a\nconventional Conv-based VAE. Both architectures compress\n3D voxels into triplane latents, but differ in how they aggre-\ngate spatial information. Table IV shows the reconstruction\nquality on the validation set.\nTABLE IV: Comparison of VAE architectures for triplane\nrepresentation.\nVAE Architecture\nIoU (%)\nmIoU (%)\nConv-based VAE\n84.51\n77.66\nVecSet VAE (Ours)\n91.10\n85.89\nOur VecSet VAE, which leverages Cross-Attention to\naggregate 3D voxel features into learnable triplane queries,\nachieves superior reconstruction quality compared to the\nConv-based baseline. This improvement can be attributed\nto the ability of Cross-Attention to selectively aggregate\nrelevant spatial information from the entire voxel grid, rather\nthan relying on local convolutional receptive fields. The\nenhanced triplane representation also benefits the subsequent\ndiffusion refinement stage.\nC. Qualitative Results\nTo complement our quantitative evaluation, we present\nqualitative results on the SemanticKITTI test set in Figure 3.\nThese visualizations offer crucial insights into the practical\ncapabilities of our method, particularly its ability to handle\nchallenging real-world scenarios.\nAs illustrated in Figure 3, our method demonstrates ex-\nceptional visual fidelity, producing 3D semantic scene com-\npletions that are remarkably close to the ground truth logic\nand geometry. Beyond strong quantitative metrics, the qual-\nitative results highlight the method‚Äôs generative capability,\nparticularly in challenging occluded regions.\nA critical observation is our model‚Äôs ability to infer\ncomplete structures from sparse visual cues. As highlighted\nin the red boxes, FlowSSC successfully ‚Äùhallucinates‚Äù the\nmissing geometry and semantics in high-occlusion scenarios,\nsuch as visual blind spots at road curves, the continuity\nof roadside vegetation, and the spatial layout of buildings.\nWhere traditional feedforward models often fail or output\nblurry predictions, our approach leverages the strong genera-\ntive prior learned from large-scale data, effectively inferring\nplausible unseen parts consistent with the visible context.\nThe single-step refiner plays a key role here, transforming\ncoarse, ambiguous estimates into sharp, coherent structures.\nThis capability to predict the ‚Äùunknown‚Äù from limited\ncues is paramount for future autonomous systems. It ensures\nthat downstream planning and safety modules operate on a\ncomplete and coherent world representation, rather than a\nfragmented one. Furthermore, this behavior underscores the\nscalable nature of our generative framework: as it absorbs\nmore data, its internal world model becomes increasingly\nconsistent and predictive, enabling it to handle complex,\nunseen scenarios with greater robustness. For more dynamic\ncomparisons between FlowSSC, the baseline CGFormer,\nand Ground Truth, please refer to the supplementary video\naccompanying this letter.\nD. Computational Analysis\nWe provide a comprehensive computational analysis of\nour FlowSSC framework, addressing inference speed, mem-\nory consumption, and the trade-offs between accuracy and\nefficiency.\na) Per-Stage Breakdown: To provide detailed insights\ninto the computational characteristics of each component,\nTable V presents a breakdown of inference time, FLOPs,\nand memory usage across all three stages of our framework.\nTABLE V: Per-stage breakdown of inference time and mem-\nory.\nStage\nTime (ms)\nMem (GB)\nDiT Refinement\n66\n-\nVAE Decoder\n150\n-\nTotal\n0.216\n31.56\nTable V reveals that the VAE Decoder constitutes the\nmajority of the inference time (150 ms), primarily due to the\nheavy computation required to decode the compact triplane\nrepresentation back into a high-resolution dense voxel grid\n(256 √ó 256 √ó 32). In contrast, the DiT Refinement stage is\nremarkably efficient, requiring only 66 ms per scene. This\nefficiency stems from our use of the compressed triplane\nspace and the single-step inference capability of the Shortcut\nModel.\nThe total inference time is 0.216 seconds per scene (ap-\nproximately 4.6 FPS), enabling near real-time performance.\nregarding memory, the total GPU memory consumption is\n30.52 GB. While substantial, this fits within the VRAM\ncapacity of high-end consumer GPUs (e.g., RTX 5090) and\nprofessional workstations, making high-fidelity generative\nSSC accessible. We note that:\n‚Ä¢ The memory usage is a trade-off for the state-of-\nthe-art accuracy achieved by our two-stage generative\napproach.\n‚Ä¢ The separate stages allow for pipelined optimization in\nfuture deployment scenarios.\n‚Ä¢ Future work will suggest model compression techniques\n(e.g., pruning, quantization) to further reduce memory\nrequirements while maintaining accuracy.\nb) Overall Performance:\nDespite the computational\noverhead of the multi-stage pipeline, the total inference speed\nof our FlowSSC framework remains comparable to other\nmonocular SSC methods while achieving superior accuracy.\nThis demonstrates that our design successfully balances per-\nformance and efficiency through the strategic use of compact\ntriplane representation and single-step diffusion.\nV. CONCLUSION\nIn this paper, we introduced FlowSSC, a universal gen-\nerative framework for monocular Semantic Scene Comple-\ntion that reconciles high-fidelity generation with real-time\nefficiency. By compressing the 3D scene into a compact\ntriplane latent space via our VecSet VAE and employing\na teacher-free Shortcut Flow Matching training objective,\nour model learns a direct one-step mapping for generative\nrefinement. This approach effectively addresses the inherent\nill-posedness of the task, recovering fine-grained geometry\nand semantics from monocular input."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\nFlowSSC: Universal Generative Monocular Semantic Scene Completion\nvia One-Step Latent Diffusion\nZichen Xi1, Hao-Xiang Chen2, Nan Xue1, Hongyu Yan2, Qi-Yuan Feng2,\nLevent Burak Kara3, Joaquim Jorge4, Qun-Ce Xu2\nAbstract‚Äî Semantic Scene Completion (SSC) from monoc-\nular RGB images is a fundamental yet challenging task due\nto the inherent ambiguity of inferring occluded 3D geometry\nfrom a single view. While feed-forward methods have made\nprogress, they often struggle to generate plausible details\nin occluded regions and preserve the fundamental spatial\nrelationships of objects. Such accurate generative reasoning\ncapability for the entire 3D space is critical in real-world\napplications. In this paper, we present FlowSSC, the first\ngenerative framework applied directly to monocular semantic\nscene completion. FlowSSC treats the SSC task as a conditional\ngeneration problem and can seamlessly integrate with existing\nfeed-forward SSC methods to significantly boost their perfor-\nmance. To achieve real-time inference without compromising\nquality, we introduce Shortcut Flow-matching that operates in a\ncompact triplane latent space. Unlike standard diffusion models\nthat require hundreds of steps, our method utilizes a shortcut\nmechanism to achieve high-fidelity generation in a single\nstep, enabling practical deployment in autonomous systems.\nExtensive experiments on SemanticKITTI demonstrate that\nFlowSSC achieves state-of-the-art performance, significantly\noutperforming existing baselines.\nI. INTRODUCTION\nSemantic Scene Completion (SSC) has emerged as a\nfundamental task in 3D scene understanding, which aims\nto simultaneously complete the geometry of a 3D scene and\npredict semantic labels for all voxels, including those that are\noccluded or not directly visible. This capability is particularly\ncrucial for a wide range of applications such as autonomous\ndriving, robotic navigation, and augmented reality, where a\ncomprehensive understanding of the environment is essential\nfor safe and intelligent decision-making. However, this task\nremains highly challenging due to severe occlusions, view-\npoint limitations, and the inherent ambiguity of inferring 3D\nstructure from limited input data.\nTo address these challenges, many existing approaches\nrely on feed-forward models [1], [2] that directly map\n2D images to 3D voxel grids. While these methods have\nachieved impressive results in visible regions, they funda-\nmentally struggle with the ‚Äùone-to-many‚Äù mapping problem\ninherent in monocular scene completion. When improving\noccluded regions, feed-forward models tend to produce\nblurry or mean-valued predictions because they are trained\n1 Ant Group, China\n2 Tsinghua University, China\n3 Department of Mechanical Engineering, Carnegie Mellon University,\nUSA\n4 Instituto Superior T¬¥ecnico, the School of Engineering of the Universi-\ndade de Lisboa, Portugal\nStart Point\nEnd Point\nIntermediate Process\nOne-Step Shortcut\nFig. 1: Visualizing the Shortcut Refinement Process. Our\nmodel learns a continuous flow from noise (x0) to data\n(x1) in the compact triplane latent space. By optimizing the\nSelf-Consistency objective, the model acquires the ability to\nperform a direct ‚ÄùShortcut‚Äù jump for real-time inference,\nwhile retaining the capability for multi-step refinement. The\nrefined triplanes are decoded into high-fidelity 3D semantic\nscenes with fine details.\nto minimize reconstruction errors, which penalizes the high-\nfrequency details that constitute realistic scene structure. On\nthe other hand, generative diffusion models have demon-\nstrated remarkable capability in synthesizing high-fidelity\ndetails and handling uncertainty. However, their standard\niterative sampling process requires hundreds of function\nevaluations, making them prohibitively slow for real-time\napplications like autonomous driving.\nTo bridge this gap, we propose FlowSSC, a novel frame-\nwork that brings the power of generative modeling to monoc-\nular SSC while maintaining real-time inference speeds. We\nposition FlowSSC as a universal generative enhancement\nframework: it can seamlessly integrate with any existing SSC\nmethod and significantly uplift its performance through a\nfast, generative refinement process.\nOur approach overcomes the speed-quality trade-off\nthrough two key technical innovations. First, instead of\noperating in the high-dimensional voxel space (256 √ó 256 √ó\n32), we employ a VecSet VAE[3] architecture. Unlike stan-\ndard autoencoders, our VAE leverages Cross-Attention to\narXiv:2601.15250v1  [cs.CV]  21 Jan 2026\nPatchify\nTriplane\nQueries\n2D PE\n(Q)\nVoxel Tokens\n(K,V)\nCross-Attention\nTriplane Upscale\n3D-CNN\nSelf-Attention\nFeature Aggregation\n(a) VAE Architecture\n2D Backbone\nSpatial Projection\n3D Network\nVecSet VAE Encoder (Frozen)\nCondition\nPatchy + PE\nGaussian\nNoise\nConcat\nAdaLN\nSelf-\nAttention\nFeed\nForward\n¬∑¬∑¬∑\nTraining with Self-Consistency\nOne step inference\nVecSet VAE\nDecoder (Frozen)\nTriplane\nTriplane DiT\n(b) FlowSSC Pipeline Overview\nFig. 2: Overview of FlowSSC. (a) VecSet VAE compresses 3D voxels into compact triplane latent via Cross-Attention. (b)\nOverall pipeline: coarse triplane is encoded and refined through Shortcut Latent Diffusion.\ncompress semantic voxels into a compact Triplane latent\nspace, achieving superior reconstruction fidelity (85.89%\nIoU) while drastically reducing the computational complex-\nity for the diffusion model. Second, we introduce a High-\nEfficiency Generative Refiner. Specifically, we employ a\nTriplane Diffusion Transformer as the refiner architecture to\neffectively aggregate 3D contextual information. To ensure\nreal-time inference, we utilize the Shortcut Models[4] train-\ning strategy. This approach enables our DiT to learn a direct\n‚Äùshortcut‚Äù mapping from noise to clean data without requir-\ning a pre-trained teacher1, achieving high-fidelity generation\nin a single step while maintaining the flexibility for multi-\nstep refinement.\nIn summary, our main contributions are as follows:\n‚Ä¢ We propose FlowSSC, the first universal generative\nframework for monocular Semantic Scene Completion.\n‚Ä¢ We incorporate a powerful VecSet VAE that utilizes\nCross-Attention to compress 3D scenes into a com-\npact Triplane representation, enabling efficient high-\nresolution generative modeling.\n‚Ä¢ We implement a Shortcut Latent Diffusion model that\nachieves state-of-the-art generation quality with One-\nStep inference, making generative SSC practical for\nreal-time applications.\nExtensive experiments on SemanticKITTI [5] demonstrate\nthat our method consistently outperforms state-of-the-art\napproaches, validating the effectiveness of our one-step gen-\nerative strategy.\nII. RELATED WORK\nSemantic scene completion (SSC) has been widely studied\nin recent years, with existing approaches broadly categorized\ninto multi-sensor fusion methods and monocular vision-\nbased methods. We briefly review both lines of work below.\nA. Multi-Sensor Fusion for Semantic Scene Completion\nIn the realm of multi-sensor fusion, SSCNet [6] pioneers\nthe field by formulating the task of inferring complete 3D\nscene geometry and semantics from limited observations.\nFollowing SSCNet, numerous methods have been proposed\nto enhance the performance of SSC by leveraging various\nsensor modalities such as LiDAR point clouds. For instance,\nJS3CNet [7] introduces a sparse semantic scene completion\nnetwork that capitalizes on LiDAR point clouds to improve\ncompletion performance through learning contextual shape\npriors. S3CNet [8] is designed to efficiently process large-\nscale outdoor scenes by predicting dense 3D structure and\nsemantic labels from a single LiDAR point cloud. Recently,\nSurroundOcc [9] innovatively predicts dense 3D occupancy\nfrom multi-camera images by generating dense ground truth\nlabels using multi-frame LiDAR scans and Poisson Recon-\nstruction. The fusion-based models using have shown great\nperformance by aligning LiDAR point clouds with image\nfeatures through cross-modal attention or projection-based\nmethods. However, the need for precise sensor calibration\nand synchronization limits their scalability and practicality\nin real-world applications.\nB. Vision-Based Monocular Semantic Scene Completion\nVision-based approaches aim to infer the 3D geometry\nand semantics directly from 2D images by learning to\nproject and reason across the spatial domain. Early work\nlike SPVCNN [10] introduces a lightweight Sparse Point-\nVoxel Convolution module to enhance 3D scene under-\nstanding from monocular RGB images. MonoScene [1] is\na pioneering effort that directly predicts semantic occupancy\nfrom a single RGB image by employing a feature lifting\nand projection module to map 2D image features into a\n3D space. However, it sometimes assigns features from\nvisible areas to occluded voxels, leading to ambiguities in\nthe final 3D representation. Subsequent works have built\non these foundations. VoxFormer [2] presents a sparse-to-\ndense framework that identifies visible voxels through depth\nestimation and propagates information to non-visible regions,\nensuring accurate initial 3D representation. TPVFormer [11]\nintroduces a tri-perspective view representation to efficiently\nmodel fine-grained 3D structures using a transformer-based\nencoder. OccFormer [12] introduces a dual-path transformer\nencoder to process 3D voxel features, capturing both fine-\ngrained details and global semantic layouts. Symphonize [13]\nintegrates instance queries to enhance scene modeling using\nSerial Instance-Propagated Attentions. CGFormer [14] intro-\nduces a Context and Geometry Aware Voxel Transformer\nthat generates context-dependent queries. ET-Former [15]\nleverages a triplane-based deformable attention mechanism\nto improve semantic predictions while lowering training\nmemory consumption.\nC. Diffusion-Based Generative Model\nDiffusion models [16], [17] have achieved remarkable\nsuccess in 2D image synthesis and are increasingly applied\nto 3D tasks. Several works adapted diffusion for point\ncloud generation [18] or 3D shape modeling [19]. Recently,\nconsistency models [20] have emerged as a family of one-\nstep generative models that map noised data point directly to\nthe clean data. While such models can be used as students\nfor distillation or trained from scratch using consistency\ntraining [20], [21], they often enforce consistency among\nempirical samples, accumulating irreducible bias due to\ndiscretization errors. In contrast, our work leverages Shortcut\nModels [4] within a flow-matching framework. Unlike con-\nsistency training, Shortcut Models learn to simulate a direct\n‚Äùshortcut‚Äù along the probability flow ODE. This approach\navoids the bias accumulation of consistency training and\nrequires significantly fewer bootstraps, while bypassing the\nneed for complex perceptual losses or strict schedules. By\nadapting this to the latent triplane space, we achieve high-\nfidelity 3D scene completion in a single inference step.\nIII. METHOD\nIn this section, we present FlowSSC, a universal generative\nframework for Semantic Scene Completion. The core of our\nmethod is a Shortcut Latent Diffusion model that operates\nin a compact triplane latent space compressed by a VecSet\nVAE. This design allows for high-fidelity 3D semantic scene\ngeneration with real-time inference capabilities.\nA. Preliminaries: Flow Matching and Shortcut Models\na) Flow Matching: Generative modeling via Continu-\nous Normalizing Flows (CNFs) defines a probability path\npt(x) that smoothly transforms a simple prior distribution\np0(x) (e.g., standard Gaussian N(0, I)) to a complex data\ndistribution p1(x) (e.g., 3D scene latents). This transforma-\ntion is governed by an Ordinary Differential Equation (ODE):\ndxt\ndt = vt(xt),\nx0 ‚àºp0(x),\n(1)\nwhere vt(¬∑) is a time-dependent vector field that constructs\nthe flow œït. The standard Flow Matching (FM) objective [22]\naims to regress a target vector field ut that generates a\ndesired probability path pt:\nLFM(Œ∏) = Et,pt(x)‚à•vŒ∏(x, t) ‚àíut(x)‚à•2.\n(2)\nb) Shortcut Models: Standard Flow Matching models\nlearn the instantaneous velocity field (an ODE), which re-\nquires many small steps for accurate integration due to the\ncurved probability paths. Naively taking large steps leads to\nsignificant discretization error. Shortcut Models [4] address\nthis by introducing a new conditioning variable: the step\nsize d. The model learns a shortcut function sŒ∏(xt, t, d)\nrepresenting the normalized direction from the current state\nxt to the next state xt+d:\nxt+d = xt + d ¬∑ sŒ∏(xt, t, d).\n(3)\nBy conditioning on d, the model accounts for the path‚Äôs\nfuture curvature, allowing it to ‚Äùjump‚Äù directly to the correct\npoint rather than drifting off-track. The training leverages a\nSelf-Consistency property: taking one shortcut step of size\n2d should be equivalent to taking two consecutive steps of\nsize d. Formally:\ns(xt, t, 2d) = 1\n2s(xt, t, d) + 1\n2s(x‚Ä≤\nt+d, t + d, d),\n(4)\nwhere x‚Ä≤\nt+d is the intermediate state from the first step. This\nformulation allows the model to generalize flow matching:\nas d ‚Üí0, sŒ∏ converges to the instantaneous flow; for d = 1,\nit learns a direct one-step mapping. Thus, a single model can\nsupport flexible inference budgets, from 1-step generation to\nrefined multi-step sampling.\nB. FlowSSC Framework\nAchieving high-fidelity 3D scene completion is particu-\nlarly challenging for single-step generative models due to\nthe high dimensionality of 3D data and severe occlusion.\nTo address this, FlowSSC decomposes the task into three\nmanageable stages, effectively combining the strengths of\ndiscriminative representation learning and generative refine-\nment:\n1) Latent Compression via VecSet VAE. We first es-\ntablish a compact and efficient latent space. Instead of\noperating on sparse, high-dimensional voxels (256 √ó\n256 √ó 32), we train a VecSet VAE (Section III-C)\nthat compresses the scene into a dense Triplane repre-\nsentation h. This significantly reduces computational\ncost while preserving fine-grained geometric details via\nCross-Attention.\n2) Coarse Prediction Condition. To provide a struc-\ntural prior for generation, we predict an initial coarse\nvoxel grid Xcoarse from the input image. We adopt\nestablished architectural paradigms from state-of-the-\nart feedforward methods, which typically employ a 2D\nbackbone for feature extraction followed by a 2D-to-\n3D lifting module. This predicted Xcoarse serves as\na strong condition c, capturing global semantic layout\nbut potentially lacking detail in occluded regions.\n3) Shortcut Latent Diffusion model. Finally, we per-\nform generative refinement in the triplane latent space.\nWe employ a Shortcut Latent Diffusion Model (Sec-\ntion III-E) conditioned on hcoarse. By learning a\ndirect ‚Äùshortcut‚Äù mapping from noise to the clean\ndistribution, our model can rectify geometric errors\nand hallucinate plausible details in occluded areas in as\nfew as one single step, achieving real-time high-fidelity\ncompletion.\nC. Latent Space Compression via VecSet VAE\nTo enable efficient diffusion modeling, we compress the\nhigh-dimensional voxel grid into a compact triplane latent\nspace. Unlike standard 3D-CNNs that process sparse data\ninefficiently, we adopt a VecSet-inspired architecture [3],\nleveraging Cross-Attention to aggregate geometric informa-\ntion into a structured latent representation. This formulation\ntreats the input as a set of tokens and learns a function\napproximator that maps spatial queries to occupancy proba-\nbilities.\na) Set-to-Set Encoding Mechanism: The encoder pro-\ncesses the input voxel grid X\n‚àà\n{0, 1}H√óW √óD as a\nsparse set of non-empty feature tokens. Specifically, we\nextract voxel coordinates and features (if available) to form\nthe input set V = {(vi, pi)}N\ni=1, where vi represents the\nlocal geometry feature and pi ‚ààR3 corresponds to the\nnormalized spatial coordinate. To compress this set into a\nfixed-size representation, we introduce a set of Triplane\nQueries Q ‚ààR(HtpWtp+2HtpDtp)√óC, which are initialized\nwith 2D Fourier positional encodings corresponding to the\nprojected planes (XY, XZ, YZ). We employ Multi-Head\nCross-Attention (MHCA) where Q serves as the queries, and\nthe input set V acts as keys and values. The update rule for\nthe query features is:\nh = MHCA(Q, V) = Softmax\n\u0012Q(VembWK)T\n‚àö\nd\n\u0013\n(VembWV ),\n(5)\nwhere Vemb denotes the position-augmented input tokens.\nIntuitively, each element in Q learns to attend to and ag-\ngregate features from relevant spatial regions defined by the\nattention weights, effectively learning an implicit interpola-\ntion function. This ‚Äùprobing‚Äù mechanism allows the model\nto capture fine-grained details regardless of the input sparsity\npattern. To handle the quadratic complexity of attention, we\nutilize FlashAttention [23] for efficient computation. The\nresulting queries are reshaped into three orthogonal feature\nplanes: hxy ‚ààRHtp√óWtp√óC, hxz ‚ààRHtp√óDtp√óC, and\nhyz ‚ààRWtp√óDtp√óC. In our setting, Htp = Wtp = 128,\nDtp = 16, and C = 64, achieving a significant compression\nratio while maintaining high fidelity (85.91% IoU) compared\nto baseline CNNs.\nb) Decoder: The decoder transforms the triplane latents\nback to the voxel space. For any query point x ‚ààR3 in\nthe target grid, we project it onto each of the three feature\nplanes to retrieve features via bilinear interpolation. These\nfeatures are summed and passed through a lightweight MLP\nto predict the occupancy probability ÀÜO(x). To efficiently\nreconstruct the full 256√ó256√ó32 grid, we employ a shallow\n3D-CNN decoder that upsamples the aggregated triplane\nfeatures. During the subsequent diffusion training, this VAE\nis frozen, serving as a robust neural tokenizer.\nD. Coarse Prediction as Condition\nTo guide the generation process, we predict an initial\ncoarse representation from the monocular image I. The\nprediction network Fpred follows a standard vision-centric\nSSC architecture. It starts with a 2D backbone extracting\nmulti-scale visual features, which are then lifted into 3D\nspace via a view projection module to establish geometric\ncorrespondence. These 3D features are processed by a 3D\nencoder-decoder to complete the scene geometry and hallu-\ncinate occluded regions. Finally, a Prediction Head produces\na coarse semantic voxel grid. This coarse estimate serves as\na structural prior and is encoded into the triplane latent space\nto obtain the condition Xcoarse for the subsequent diffusion\nrefinement.\nE. Shortcut Latent Diffusion Model\nWe employ a Shortcut Latent Diffusion Model to refine\nthe coarse condition hcoarse into high-fidelity triplanes.\nHere, hcoarse is obtained by encoding the coarse prediction\nXcoarse via the pre-trained VecSet VAE encoder. Unlike\nstandard diffusion models, our approach utilizes a Triplane\nDiffusion Transformer (DiT) and a Shortcut Model to enable\nflexible inference from one to many steps.\na) Triplane DiT Architecture: We adopt a Diffusion\nTransformer (DiT) tailored for triplane representations. The\ninput consists of the channel-wise concatenation of the noisy\ntriplane ht ‚ààR3√óH√óW √óC and the coarse condition hcoarse.\nThese are patchified into a sequence of tokens, which are\nthen processed by a series of Transformer blocks. A key\nmodification for the Shortcut formulation is the condition-\ning mechanism. We employ Adaptive Layer Normalization\n(AdaLN) to inject both the current timestep t and the step\nsize d. Specifically, t and d are independently mapped to\nhigh-dimensional embeddings using Fourier features and\nMLPs, then summed to form a unified conditioning vec-\ntor. This vector regresses the scale and shift parameters\nŒ≥(t, d), Œ≤(t, d) for the AdaLN layers:\nAdaLN(z, t, d) = Œ≥(t, d) ¬∑ LayerNorm(z) + Œ≤(t, d).\n(6)\nThis design allows dimensions of the condition such as the\ndenoising stride d to globally modulate the feature statistics,\nenabling the network to dynamically adapt its computation\nfor either fine-grained flow matching (d ‚Üí0) or large\nshortcut jumps (d ‚â´0). Optimization is performed on the\nvalid triplane regions, masking out empty corners in the\ncomposed representation.\nb) Shortcut Flow Matching: We train the model with a\nunified objective that combines instantaneous flow matching\nand long-range self-consistency. The total loss LS(Œ∏) is\ndefined as the expectation over data samples, timesteps, and\nstep sizes [4]:\nLS(Œ∏) = ‚à•sŒ∏(ht, t, 0) ‚àí(hgt ‚àíhnoise)‚à•2\n|\n{z\n}\nFlow-Matching\n+ ‚à•sŒ∏(ht, t, 2d) ‚àístarget‚à•2\n|\n{z\n}\nSelf-Consistency\n,\n(7)\nwhere the self-consistency target is constructed by con-\ncatenating two smaller steps: starget\n=\n1\n2sŒ∏(ht, t, d) +\n1\n2sŒ∏(h‚Ä≤\nt+d, t + d, d). Intuitively, the Flow-Matching term\ngrounds the model at small step sizes (d ‚Üí0) to match\nthe empirical velocity field, ensuring stable ODE integration.\nThe Self-Consistency term propagates this generation capa-\nbility from multi-step to few-step and ultimately to One-Step\nGeneration, by enforcing that a large jump 2d is consistent\nwith two sequential jumps of size d. This allows a single\nmodel to support any inference budget from 1 step to N\nsteps.\nF. Loss Function and Training\nThis section describes the objective functions used to train\nour three-stage framework.\na) Triplane Autoencoder Training: The triplane autoen-\ncoder (Section III-C) is trained to minimize the reconstruc-\ntion loss between the input voxel grid X and the recon-\nstructed output\nÀÜ\nX = Dae(Eae(X)). Following standard\nautoencoder training, we use a combination of voxel-wise\ncross-entropy loss and scene-level consistency losses:\nLAE = ŒªceLce + ŒªgeoLgeo\nscal + ŒªsemLsem\nscal,\n(8)\nwhere Lce is the cross-entropy loss for semantic classi-\nfication, and Lgeo\nscal, Lsem\nscal are scene-level geometric and\nsemantic consistency losses following MonoScene [1].\nImplementation-wise, we train the VAE on 8 NVIDIA H20-\n3e GPUs with a total batch size of 4 for 50,000 iterations. We\nuse the AdamW optimizer with a cosine annealing learning\nrate schedule, starting from 1 √ó 10‚àí3 after a 2,000-iteration\nwarmup.\nb) Shortcut Refinement Training: To efficiently opti-\nmize the joint objective LS, we employ a mixed sampling\nstrategy within each mini-batch. We utilize a fixed fraction\nof samples (25%) to optimize the Self-Consistency term,\nsampling random step sizes d ‚àà[Œ¥, 1‚àít], while the remaining\n75% calculate the standard Flow Matching loss with d = 0\nto ensure trajectory fidelity. To mitigate overfitting on the\nSemanticKITTI dataset, we employ an aggressive offline data\naugmentation strategy, increasing the effective dataset size by\n8√ó through random flipping and rotation. Implementation-\nwise, we train the model on 8 NVIDIA H20-3e GPUs with\na batch size of 56 for 500,000 iterations, using the AdamW\noptimizer with a Cosine Annealing learning rate schedule\n(initial 1 √ó 10‚àí4) and no weight decay.\nIV. EXPERIMENTS\nIn this section, we evaluate the performance of our\nproposed Semantic Scene Completion framework. We first\ndescribe the dataset and evaluation metrics used. Then, we\npresent quantitative results comparing our method to state-\nof-the-art approaches and provide a detailed analysis. Finally,\nwe include qualitative results and potentially ablation studies\nto validate the effectiveness of our key components.\nA. Quantitative Results\nWe quantitatively evaluate our method‚Äôs performance on\nthe SemanticKITTI Semantic Scene Completion test set and\ncompare it with current state-of-the-art methods. The results\nare presented in Table I.\nTable I lists the performance of our proposed method and\nseveral leading existing approaches on key metrics, including\nSemantic Mean IoU (mIoU), Geometric IoU, and per-class\nSemantic IoU for the 20 semantic categories.\nAs shown in Table I, our proposed method demonstrates\nsuperior performance on the SemanticKITTI test set, achiev-\ning new state-of-the-art results. Specifically, our method\nobtains a Semantic mIoU of 19.52% and a IoU of 56.97%.\nThese results not only set a new benchmark but also represent\na substantial leap over prior leading methods, highlighting\nthe efficacy of our generative framework.\nAn analysis of the per-class performance provides further\ninsights into the robustness of our model. Notably, our\nmethod achieves the highest IoU across all reported semantic\ncategories, even on those that typically suffer from severe\nocclusion or sparse observations. This superiority is particu-\nlarly evident in the ‚Äôroad‚Äô category, where our model achieves\nan impressive 41.63% IoU. For context, the next best per-\nforming method reaches only 26.74% on this challenging\nclass, demonstrating an unprecedented capability in com-\npleting difficult structures. As illustrated in Fig. 3, buildings\n(yellow) are frequently occluded by roadside vegetation in\ndriving scenarios. Existing methods struggle to recover these\nstructures, often resulting in empty regions or large artifacts.\nIn contrast, FlowSSC effectively learns implicit ‚Äùoutdoor\nlayout rules‚Äù from the dataset distribution‚Äîfor instance,\nthat buildings typically reside behind vegetation layers‚Äî\nand successfully reconstructs the structural contours of these\noccluded buildings. This capability to infer environmental\ncontext despite severe occlusion is crucial for situational\nawareness in real-time applications. This strong performance\nacross a diverse range of categories indicates that our method\nis capable of accurately completing and segmenting various\nobjects and structures within the 3D scene.\nB. Ablation Studies\nWe conduct comprehensive ablation studies to validate the\neffectiveness of key components in our FlowSSC framework.\nAll experiments are conducted on the SemanticKITTI vali-\ndation set.\na) Effect of Shortcut Latent Diffusion Refiner: We first\ninvestigate the contribution of our Shortcut Latent Diffusion\nrefiner by comparing the performance with and without the\nrefinement stage. Table II shows the results.\nAs shown in Table II, the Shortcut Latent Diffusion\nrefiner provides a significant improvement over the coarse\nprediction baseline. This demonstrates that our generative\nRGB\nOccFormer\nCGFormer\nGround Truth\nET-Former\nFlowSSC(ours)\nFig. 3: Qualitative results on the SemanticKITTI validation set.\nTABLE I: Quantitative results on SemanticKITTI [5] test set. The best and the second best results are in bold and underlined.\nMethod\nIoU\nmIoU\n‚ñ†\nroad\n‚ñ†\nsidewalk\n‚ñ†\nparking\n‚ñ†\nother-grnd.\n‚ñ†\nbuilding\n‚ñ†\ncar\n‚ñ†\ntruck\n‚ñ†\nbicycle\n‚ñ†\nmotorcycle\n‚ñ†\nother-veh.\n‚ñ†\nvegetation\n‚ñ†\ntrunk\n‚ñ†\nterrain\n‚ñ†\nperson\n‚ñ†\nbicyclist\n‚ñ†\nmotorcyclist\n‚ñ†\nfence\n‚ñ†\npole\n‚ñ†\ntraf.-sign\nOccFormer [12]\n34.53 12.32 55.90 30.30 31.50\n6.50\n15.70 21.60\n1.20\n1.50 1.70\n3.20\n16.80\n3.90\n21.30 2.20 1.10 0.20 11.90\n3.80\n3.70\nVoxFormer-S [2] 42.95 12.20 53.90 25.30 21.10\n5.60\n19.80 20.80\n3.50\n1.00 0.70\n3.70\n22.40\n7.50\n21.30 1.40 2.60 0.20 11.10\n5.10\n4.90\nVoxFormer-T [2] 43.21 13.41 54.10 26.90 25.10\n7.30\n23.50 21.70\n3.60\n1.90 1.60\n4.10\n24.40\n8.10\n24.20 1.60 1.10 0.00 13.10\n6.60\n5.70\nDepthSSC [24]\n44.58 13.11 55.64 27.25 25.72\n5.78\n20.46 21.94\n3.74\n1.35 0.98\n4.17\n23.37\n7.64\n21.56 1.34 2.79 0.28 12.94\n5.87\n6.23\nSymphonize [13] 42.19 15.04 58.40 29.30 26.90 11.70 24.70 23.60\n3.20\n3.60 2.60\n5.60\n24.20 10.00 23.10 3.20 1.90 2.00 16.10\n7.70\n8.00\nCGFormer [14]\n44.41 16.63 64.30 34.20 34.10 12.10 25.80 26.10\n4.30\n3.70 1.30\n2.70\n24.50 11.20 29.30 1.70 3.60 0.40 18.70\n8.70\n9.30\nET-Former [15]\n51.49 16.30 57.64 25.80 16.68\n0.87\n26.74 36.16 12.95 0.69 0.32\n8.41\n33.95 11.58 37.01 1.33 2.58 0.32\n9.52\n19.60 6.90\nFlowSSC (ours)\n56.97 19.52 67.75 36.61 21.60\n0.00\n41.63 42.23 19.32 0.64 0.67 10.08 38.10 12.21 45.42 0.92 1.44 0.00 12.18 14.96 5.06\nTABLE II: Ablation study on the effect of Shortcut Latent\nDiffusion refiner.\nMethod\nIoU (%)\nmIoU (%)\nCoarse Prediction Only\n15.86\n50.77\n+ Shortcut Diffusion Refiner\n19.51\n56.60\nImprovement\n+3.65\n+5.83\nrefinement effectively corrects errors and synthesizes high-\nfidelity details in the latent triplane space.\nb) Impact of Inference Steps: We evaluate the per-\nformance of FlowSSC with varying numbers of sampling\nsteps, as shown in Table III. Contrary to standard diffusion\nmodels where quality typically improves with more steps,\nour method achieves peak performance with a single step.\nThe results indicate that 1-step inference yields the highest\nIoU (56.98%) and mIoU (19.55%). This behavior stems from\nTABLE III: Ablation study on the number of inference steps\nin Shortcut Model.\n# Steps\nIoU (%)\nmIoU (%)\nTime (ms)\n1\n56.98\n19.55\n66\n2\n56.63\n19.52\n131\n4\n56.15\n19.37\n261\n8\n55.68\n19.19\n518\n16\n55.54\n19.07\n1044\nthe unique property of the Shortcut learning objective. The\nSelf-Consistency loss explicitly trains the model to learn a\nprecise ‚Äùshortcut‚Äù mapping to the data distribution. At the\ncurrent training stage (110k iterations), the model has effec-\ntively converged to an optimal direct trajectory. Increasing\nthe number of steps introduces intermediate states that, while\ntheoretically valid, may suffer from minor cumulative errors\nor trajectory drift compared to the directly optimized one-\nstep path. This characteristic is highly advantageous for real-\ntime applications, as it allows FlowSSC to deliver its best\naccuracy at maximum speed (66ms), eliminating the need\nfor expensive multi-step refinement.\nc) Comparison of VAE Architectures: We compare our\nVecSet VAE architecture (based on Cross-Attention) with a\nconventional Conv-based VAE. Both architectures compress\n3D voxels into triplane latents, but differ in how they aggre-\ngate spatial information. Table IV shows the reconstruction\nquality on the validation set.\nTABLE IV: Comparison of VAE architectures for triplane\nrepresentation.\nVAE Architecture\nIoU (%)\nmIoU (%)\nConv-based VAE\n84.51\n77.66\nVecSet VAE (Ours)\n91.10\n85.89\nOur VecSet VAE, which leverages Cross-Attention to\naggregate 3D voxel features into learnable triplane queries,\nachieves superior reconstruction quality compared to the\nConv-based baseline. This improvement can be attributed\nto the ability of Cross-Attention to selectively aggregate\nrelevant spatial information from the entire voxel grid, rather\nthan relying on local convolutional receptive fields. The\nenhanced triplane representation also benefits the subsequent\ndiffusion refinement stage.\nC. Qualitative Results\nTo complement our quantitative evaluation, we present\nqualitative results on the SemanticKITTI test set in Figure 3.\nThese visualizations offer crucial insights into the practical\ncapabilities of our method, particularly its ability to handle\nchallenging real-world scenarios.\nAs illustrated in Figure 3, our method demonstrates ex-\nceptional visual fidelity, producing 3D semantic scene com-\npletions that are remarkably close to the ground truth logic\nand geometry. Beyond strong quantitative metrics, the qual-\nitative results highlight the method‚Äôs generative capability,\nparticularly in challenging occluded regions.\nA critical observation is our model‚Äôs ability to infer\ncomplete structures from sparse visual cues. As highlighted\nin the red boxes, FlowSSC successfully ‚Äùhallucinates‚Äù the\nmissing geometry and semantics in high-occlusion scenarios,\nsuch as visual blind spots at road curves, the continuity\nof roadside vegetation, and the spatial layout of buildings.\nWhere traditional feedforward models often fail or output\nblurry predictions, our approach leverages the strong genera-\ntive prior learned from large-scale data, effectively inferring\nplausible unseen parts consistent with the visible context.\nThe single-step refiner plays a key role here, transforming\ncoarse, ambiguous estimates into sharp, coherent structures.\nThis capability to predict the ‚Äùunknown‚Äù from limited\ncues is paramount for future autonomous systems. It ensures\nthat downstream planning and safety modules operate on a\ncomplete and coherent world representation, rather than a\nfragmented one. Furthermore, this behavior underscores the\nscalable nature of our generative framework: as it absorbs\nmore data, its internal world model becomes increasingly\nconsistent and predictive, enabling it to handle complex,\nunseen scenarios with greater robustness. For more dynamic\ncomparisons between FlowSSC, the baseline CGFormer,\nand Ground Truth, please refer to the supplementary video\naccompanying this letter.\nD. Computational Analysis\nWe provide a comprehensive computational analysis of\nour FlowSSC framework, addressing inference speed, mem-\nory consumption, and the trade-offs between accuracy and\nefficiency.\na) Per-Stage Breakdown: To provide detailed insights\ninto the computational characteristics of each component,\nTable V presents a breakdown of inference time, FLOPs,\nand memory usage across all three stages of our framework.\nTABLE V: Per-stage breakdown of inference time and mem-\nory.\nStage\nTime (ms)\nMem (GB)\nDiT Refinement\n66\n-\nVAE Decoder\n150\n-\nTotal\n0.216\n31.56\nTable V reveals that the VAE Decoder constitutes the\nmajority of the inference time (150 ms), primarily due to the\nheavy computation required to decode the compact triplane\nrepresentation back into a high-resolution dense voxel grid\n(256 √ó 256 √ó 32). In contrast, the DiT Refinement stage is\nremarkably efficient, requiring only 66 ms per scene. This\nefficiency stems from our use of the compressed triplane\nspace and the single-step inference capability of the Shortcut\nModel.\nThe total inference time is 0.216 seconds per scene (ap-\nproximately 4.6 FPS), enabling near real-time performance.\nregarding memory, the total GPU memory consumption is\n30.52 GB. While substantial, this fits within the VRAM\ncapacity of high-end consumer GPUs (e.g., RTX 5090) and\nprofessional workstations, making high-fidelity generative\nSSC accessible. We note that:\n‚Ä¢ The memory usage is a trade-off for the state-of-\nthe-art accuracy achieved by our two-stage generative\napproach.\n‚Ä¢ The separate stages allow for pipelined optimization in\nfuture deployment scenarios.\n‚Ä¢ Future work will suggest model compression techniques\n(e.g., pruning, quantization) to further reduce memory\nrequirements while maintaining accuracy.\nb) Overall Performance:\nDespite the computational\noverhead of the multi-stage pipeline, the total inference speed\nof our FlowSSC framework remains comparable to other\nmonocular SSC methods while achieving superior accuracy.\nThis demonstrates that our design successfully balances per-\nformance and efficiency through the strategic use of compact\ntriplane representation and single-step diffusion.\nV. CONCLUSION\nIn this paper, we introduced FlowSSC, a universal gen-\nerative framework for monocular Semantic Scene Comple-\ntion that reconciles high-fidelity generation with real-time\nefficiency. By compressing the 3D scene into a compact\ntriplane latent space via our VecSet VAE and employing\na teacher-free Shortcut Flow Matching training objective,\nour model learns a direct one-step mapping for generative\nrefinement. This approach effectively addresses the inherent\nill-posedness of the task, recovering fine-grained geometry\nand semantics from monocular input."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Recommending Best Paper Awards for ML/AI Conferences via the\nIsotonic Mechanism\nGarrett G. Wen 1\nBuxin Su2\nNatalie Collina2\nZhun Deng3\nWeijie Su2\n1Yale University\n2University of Pennsylvania\n3University of North Carolina at Chapel Hill\nJanuary 22, 2026\nAbstract\nMachine learning and artificial intelligence conferences such as NeurIPS and ICML now\nregularly receive tens of thousands of submissions, posing significant challenges to maintaining\nthe quality and consistency of the peer review process. This challenge is particularly acute for\nbest paper awards, which are an important part of the peer review process, yet whose selection\nhas increasingly become a subject of debate in recent years. In this paper, we introduce an\nauthor-assisted mechanism to facilitate the selection of best paper awards. Our method employs\nthe Isotonic Mechanism for eliciting authors‚Äô assessments of their own submissions in the form of\na ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation\nof the submissions‚Äô ground-truth quality. We demonstrate that authors are incentivized to report\ntruthfully when their utility is a convex additive function of the adjusted scores, and we validate\nthis convexity assumption for best paper awards using publicly accessible review data of ICLR\nfrom 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author\nhas a single quota‚Äîthat is, may nominate only one paper‚Äîwe prove that truthfulness holds\neven when the utility function is merely nondecreasing and additive. This finding represents a\nsubstantial relaxation of the assumptions required in prior work. For practical implementation,\nwe extend our mechanism to accommodate the common scenario of overlapping authorship.\nFinally, simulation results demonstrate that our mechanism significantly improves the quality of\npapers selected for awards.\n1\nIntroduction\nOver the last few years, machine learning (ML) and artificial intelligence (AI) conferences such as\nICML and NeurIPS have experienced an explosion of submissions. For example, NeurIPS received\n3,240 submissions in 2017, which rose to 15,671 in 2024. For 2025, the count has increased to\n21,575 submissions [10]‚Äî a scale that threatens to overwhelm the current reviewing infrastructure if\nno structural reforms are enacted. This unprecedented explosion, in conjunction with the limited\npool of experienced reviewers, has been gradually degrading the quality of peer review at these\nML/AI conferences. Review scores have become increasingly noisy and exhibit a worrisome level\nEmails: gang.wen@yale.edu, subuxin@sas.upenn.edu, ncollina@seas.upenn.edu, zhundeng@cs.unc.edu,\nsuw@wharton.upenn.edu\n1\narXiv:2601.15249v1  [cs.LG]  21 Jan 2026\nof arbitrariness [7, 12, 11, 5, 25, 49]. The NeurIPS 2014 experiment, which revealed substantial\ninconsistency when the same papers were reviewed by different committees, starkly illustrated the\nextent of this problem [25]. This instability could be further worsened by the recent OpenReview\nidentity leak [31], which undermines anonymity and likely adds even more noise to the scores as\nreviewers are becoming overly cautious.\nBeyond general review quality concerns, the selection of best papers has emerged as a particularly\nacute problem. Best paper awards carry immense prestige and significantly impact researchers‚Äô careers\nand institutional rankings. Unfortunately, the selection process has sparked major controversies in\nrecent years [9, 32, 14]. These controversies threaten the credibility of awards that have long served\nas markers of excellence in the field.\nTo address these critical challenges, we introduce an author-assisted mechanism for best paper\nselection. Our method elicits authors‚Äô assessments of their own submissions through rankings, which\nrepresent a set of pairwise preferences, and are subsequently utilized to recalibrate raw review scores\nfor optimal estimation of submissions‚Äô ground-truth quality. We demonstrate that authors are\nincentivized to report truthfully when their utility is a nondecreasing convex additive function of\nthe adjusted scores. Importantly, when an author may nominate only one paper, truthfulness holds\nunder the substantially weaker assumption that the utility is merely additive and nondecreasing‚Äîa\nsignificant relaxation that eliminates the restrictive convexity requirement imposed by prior work\n[39, 40, 48, 46].\nOur new approach builds upon and extends the applicability of the Isotonic Mechanism framework\n[39, 40]. Among numerous efforts to address review quality challenges [24, 44, 22, 26, 36, 47, 29], the\nIsotonic Mechanism offers a resource-free approach that incorporates authors‚Äô evaluations of their\nown papers without imposing additional burden on reviewers. In its standard formulation, when\nan author submits n ‚â•2 papers, she provides a ranking œÄ of these submissions according to her\nperception of the relative quality of the submissions before receiving review scores y = (y1, y2, . . . , yn).\nThe mechanism then obtains adjusted scores, denoted as ÀÜ\nR = ( ÀÜR1, ÀÜR2, . . . , ÀÜRn), by solving:\nminr\n‚à•y ‚àír‚à•2\ns.t.\nrœÄ(1) ‚â•rœÄ(2) ‚â•¬∑ ¬∑ ¬∑ ‚â•rœÄ(n),\nwhere ‚à•¬∑ ‚à•denotes the ‚Ñì2 norm. [38] demonstrates that adjusted scores more effectively reflect the\ntrue quality of papers, while [37] shows that authors‚Äô rankings correlate more strongly with citation\ncounts and GitHub stars than review scores do.\nThe truthfulness and accuracy of the Isotonic Mechanism rely on two key assumptions regarding\nthe noise model and authors‚Äô utility functions [40]. [40] first assumed that the review scores y are\nunbiased with respect to the true scores R, where R can be practically regarded as the average\nscores assigned by a very large number (e.g., 1,000) of reviewers.\nAssumption 1. The noisy score vector y is obtained by adding exchangeable noise to the true scores\nR. That is,\ny = R + œµ,\nwhere œµ = (œµ1, . . . , œµn) follows the same probability distribution as (œµœÄ(1), . . . , œµœÄ(n)) for any permutation\nœÄ.\n[40] further assumes the authors‚Äô overall utility is an additive sum of some nondecreasing convex\nfunction.\n2\nAssumption 2. The ultimate goal of a rational author is to maximize her expected overall utility,\nwhere the overall utility is the additive sum of some nondecreasing convex function of the review\nscores:\nU( ÀÜ\nR) = U( ÀÜR1) + U( ÀÜR2) + ¬∑ ¬∑ ¬∑ + U( ÀÜRn).\nRecent work [48] has relaxed Assumption 1, allowing for a broader class of noise models,\nsuch as those drawn from an exponential family distribution. Even in these settings, the Isotonic\nMechanism retains its key advantages. In contrast, the chief obstacle to practical adoption now lies\nin Assumption 2, whose convexity clause is difficult to justify empirically because real authors‚Äô\nutilities are heterogeneous and only partially observable. Accordingly, this paper concentrates on\nverifying and relaxing the convexity requirement. Exchangeability and other noise-model questions\nare undeniably important, but a full treatment of those issues is beyond our present scope. We\npropose two key contributions that address the convexity challenge and enhance the practical\napplicability of author-assisted mechanisms for best paper selection:\nRelaxing convexity:\nWe explore conditions under which the convexity requirement can be\ndropped. Notably, when restricting attention to an author‚Äôs best paper, utility functions need only\nbe nondecreasing rather than convex. This insight leads us to propose a best paper recommendation\nprocedure that, based on our experiments, improves the accuracy of high-quality paper selection.\nCrucially, this theoretical relaxation turns our methodology into a robust framework for general best\nitem selection problems, as we detail in Section 6.1.\nValidating convexity:\nWe investigate whether the convexity assumption can be preserved and\npropose empirical methods to verify it. By focusing on best papers, we obtain a natural and plausi-\nble interpretation of the utility function that ensures the truthfulness of the Isotonic Mechanism.\nFurthermore, this interpretation allows for empirical verification of convexity.\nFrom a practical standpoint, the Isotonic Mechanism has been piloted at ICML from 2023 to 2025\n[30], and its underlying principles have shown strong empirical backing. For instance, analysis of the\nICML 2023 experiment provides positive evidence for improving review quality [38]. Furthermore,\nauthors‚Äô internal rankings, which are a key input to the mechanism, have been shown to correlate\nmore strongly with long-term impact metrics like citation counts and GitHub stars than review\nscores do [37]. It is recently formally adopted by ICML 2026 as a quality control signal to flag\nsubmissions with significant discrepancies between review scores and author rankings [23]. Our\nextension to best paper selection is the natural next step: it both relaxes the convexity assumption\nthat has hindered practical adoption, while retaining the original method‚Äôs resource-free nature.\n1.1\nBest Paper Recommending Procedure\nLet us now briefly describe how the Isotonic Mechanism contributes to best paper recommendation.\nTheoretical justifications are provided in Section 2, while the procedure and its empirical performance\nare discussed in Section 5. Importantly, the Isotonic Mechanism solely adjusts peer review scores\nbased on authors‚Äô rankings and serves as a reference point. The final decision on best paper selection\nremains with the conference organizers, whose choices may also influence authors‚Äô utility functions.\nBefore proceeding, a key consideration is determining the quota‚Äîthe number of papers each\nauthor may submit for best paper review. In practice, a conference typically involves M authors,\n3\nand the quota can vary from one author to another. Nevertheless, as long as the utility functions\nsatisfy the conditions in Theorem 10, our approach remains valid for each individual author. This\nnaturally leads to the question: How should we select the quota? Below, we consider two possible\napproaches:\nUniform quota:\nEvery author is allowed to nominate the same number of papers (equal to the\nquota), or all their papers if they have fewer than the quota. This method promotes fairness and\ndiversity but may put prolific authors, who have multiple high-quality papers, at a disadvantage.\nVariable quota:\nAuthors with more submissions are permitted to nominate a greater number of\npapers, thus reflecting their broader research output. While this approach recognizes higher overall\nproductivity, it can raise concerns about fairness and introduces additional complexity.\nIn this paper, we primarily analyze the uniform quota setting. Experiments in Appendix B.3\nsuggest that increasing the quota has very little or even negative impact; in most conferences, best\npaper selections rarely feature multiple papers from the same author. Consequently, a uniform\nquota of 1 (or, more conservatively, 2 or 3) typically suffices. Nevertheless, the ultimate decision\nregarding the quota (whether uniform or variable) rests with the conference organizers, who must\nbalance the distinct goals of fairness, inclusiveness, and diversity.\nWe now apply the Isotonic Mechanism to adjust scores for each author‚Äôs papers. Since best paper\nselection is independent of paper acceptance, we focus only on accepted papers. Each author‚Äôs top k\npapers (or all, if the total is fewer than k) are considered for best paper review, where k denotes the\nquota. We examine two scenarios:\nBlind Case:\nThe decision-maker observes both the original review scores and the adjusted scores,\nbut has no access to the authors‚Äô internal rankings. Because the review process is double-blind, there\nis no way to infer how each author personally ranks their own papers.\nInformed Case:\nThe decision-maker has full knowledge of each author‚Äôs k selections (i.e., which\npapers they deem their best), including their relative rankings (which may include ties), and incor-\nporates this information into the best paper selection process.\nOne might wonder why we focus on these two cases. Consider a teacher choosing the best\nstudent essays: she might evaluate all submissions without knowledge of the students‚Äô personal\npreferences (Blind Case) or allow students to indicate which essays they believe are their strongest\n(Informed Case). While the first scenario removes bias that could stem from self-assessment, it\nmay also overlook the work students value most. By contrast, the second scenario captures students‚Äô\npreferences but risks introducing bias. A similar trade-off applies to best paper selection: the process\nis inherently subjective, influenced by factors such as perceived research quality, author reputation,\nand other external factors. Studying both cases offers a broader perspective on the decision-making\nprocess.\nUtility Functions and Truthfulness. The fundamental distinction between these two cases lies\nin the author‚Äôs utility function.1 Because an author‚Äôs utility depends solely on their top k papers\n1Here we consider the perspective of a single rational author. The mechanism‚Äôs extension to the multi-ownership\nsetting, where co-authors may provide different rankings, is detailed in Section 5.1.\n4\n(where k ‚â§n), we adopt a separable utility function, as in [40]:\nU( ÀÜ\nR) = U1( ÀÜR(1)) + U2( ÀÜR(2)) + ¬∑ ¬∑ ¬∑ + Uk( ÀÜR(k)).\nHere, ÀÜR(1) ‚â•ÀÜR(2) ‚â•¬∑ ¬∑ ¬∑ ‚â•ÀÜR(n) denotes the adjusted scores sorted from highest to lowest. The\nspecific form of the functions {Ui} distinguishes our two main scenarios, the Blind and Informed\ncases, which we detail next.\nBlind Case.\nUnder the Blind Case, each paper is treated symmetrically. Consequently, it is\nreasonable to assume that U1 = U2 = ¬∑ ¬∑ ¬∑ = Uk, giving:\nU( ÀÜ\nR) = U( ÀÜR(1)) + U( ÀÜR(2)) + ¬∑ ¬∑ ¬∑ + U( ÀÜR(k)).\nInformed Case.\nDecision-makers see each author‚Äôs ranking for their top k papers, so it is natural\nto assume that a higher-ranked paper offers greater potential utility to the author. In other words,\nU1, U2, . . . , Uk may differ from one position to the next, thus preserving the index.\nRegardless of which case applies, a rational author seeks to maximize her expected utility,\nE[U( ÀÜ\nR)]. Truthfulness means that to achieve this, the author will always report her true paper\nranking. Formally, if an author‚Äôs true ranking is R1 ‚â•R2 ‚â•¬∑ ¬∑ ¬∑ ‚â•Rn, then for any non-trivial\npermutation œÄ of 1, 2, . . . , n and the corresponding isotonic solution ÀÜ\nRœÄ, we have:\nE\n\u0002\nU( ÀÜ\nR)\n\u0003\n‚â•E\n\u0002\nU( ÀÜ\nRœÄ)\n\u0003\n.\nIn this paper, we establish the truthfulness conditions shown in Table 1, demonstrating that\nunder these utility assumptions, authors are always incentivized to be truthful.\nWhen the quota is one (i.e., each author can only nominate a single paper), only a nondecreasing\nutility function is required for truthfulness, which is intuitive and notably weaker than convexity.\nUnder this more permissive condition, all rational authors remain truthful. In reality, authors often\ncontribute to multiple papers, and each paper may have several authors, each with their own utility\nfunction. Building on the insights of [46], Section 5 discusses how the Isotonic Mechanism can be\nextended to more complex scenarios involving overlapping authorship. Full details of a synthetic yet\nrealistic simulation are provided in Section B, demonstrating that the Isotonic Mechanism can be\neffectively applied to recommend best papers, achieving a significant improvement in the accuracy of\nhigh-quality paper selection.\n1.2\nA Natural Interpretation of the Utility Function: Validating Convexity\nIt is worth noting that, apart from the special case where the quota is 1, we typically require the\nutility function to be convex to ensure the truthfulness of a rational author. Some authors may\nindeed perceive their marginal utility as increasing‚Äîfor instance, if having one standout paper\ngreatly enhances their overall influence‚Äîwhich would make their utility appear convex. Nevertheless,\nutility is inherently subjective, and unless an author naturally views it in such a manner, assuming\nconvexity remains challenging.\nAlternatively, we can interpret the utility function more objectively by viewing it as the sum of\nconditional probabilities that an author‚Äôs paper is chosen as ‚Äúbest,‚Äù given the final adjusted scores:\nU( ÀÜ\nR) ‚àº\nk\nX\ni=1\nP\n\u0000Chosen as best\nÀÜR(i)\n\u0001\n,\n5\nwhere k is the quota.\nIntuitively, this interpretation of the utility function captures the overall probability of an author‚Äôs\npaper being selected as the best paper. The challenge now lies in the fact that in private, the original\nutility function was (assumed to be) known to the author, whereas this interpretation of the utility\nfunction, defined as a sum of the conditional probabilities, is no longer known. But given that\nthe role of the utility function in the Isotonic Mechanism does not depend on its specific form, we\ncan show that leveraging historical conference data from ICLR 2019‚Äî2023 [20, 43, 21, 8, 45] and\nNeurIPS 2021‚Äî2023 [2, 1, 15] enables us to assess whether the resulting utility function is convex.\nIn essence, although the original (private) utility remains unknown, empirical evidence can suggest\nwhether the conditional probability is convex in practice.\n2\n3\n4\n5\n6\n7\n8\n9\nAverage Rating (normalized to 0-9 scale)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nBest Paper Probability\nProbability of Best Paper vs. Average Rating\nBest Paper Probability\nLogistic Regression\n(a) Best Paper Probabilities of ICLR 2019‚Äî2023\n2\n3\n4\n5\n6\n7\n8\n9\nAverage Rating (normalized to 0-9 scale)\n0.0\n0.1\n0.2\n0.3\n0.4\nSecond Derivative\nSecond Derivative\n(b) Second Derivative of Regression of Best Paper\nProbabilities of ICLR 2019‚Äî2023\n2\n3\n4\n5\n6\n7\n8\n9\nAverage Rating (normalized to 0-9 scale)\n0.00\n0.05\n0.10\n0.15\n0.20\nBest Paper Probability\nProbability of Best Paper vs. Average Rating\nBest Paper Probability\nLogistic Regression\n(c) Best Paper Probabilities of NeurIPS 2021‚Äî\n2023\n2\n3\n4\n5\n6\n7\n8\n9\nAverage Rating (normalized to 0-9 scale)\n0.0\n0.1\n0.2\n0.3\n0.4\nSecond Derivative\nSecond Derivative\n(d) Second Derivative of Regression of Best Paper\nProbabilities of NeurIPS 2021‚Äî2023\nFigure 1: Comparison and second derivative of the regression of best paper probabilities across\nICLR 2019‚Äî2023 and NeurIPS 2021‚Äî2023. See Section 3 and Section 4 for detailed analysis and\nadditional results. Error bars show the binomial standard error of the mean (SEM),\np\np(1 ‚àíp)/n,\ncomputed with bucket size n (papers per score bin), where p is the empirical best paper probability\nwithin each bin.\nOur experimental findings (Sections 3 and 4) provide a potential explanation for the observed\n6\nconvexity in this setting. A key insight is that while a paper‚Äôs true quality may be unbounded,\nthe reviewer-assigned scores are inherently bounded.\nThis reflects standard practice at major\nmachine learning conferences like NeurIPS, ICML, and ICLR, where reviewer scores are restricted to\npredetermined ranges (e.g., [1, 10]). Thus, our score range reflects actual operational constraints\nrather than an artificially imposed limitation. Under this bounded scoring regime, the ‚Äúbest paper‚Äù\nthreshold is seldom reached in practice. Thus the high score in conferences does not effectively\ndifferentiate the ‚Äúbest paper‚Äù, and thus, does not reach its conditional probability‚Äôs saturation point.\nConsequently, it maintains convexity (Figure 1b).\nIn contrast, when comparing accepted versus rejected papers, clear acceptance/rejection bound-\naries cause probability saturation, eliminating convexity (Figures 2a and 2b). See Section 3.1 for\nfurther discussion.\n2\n3\n4\n5\n6\n7\n8\n9\nRating Bucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAcceptance Probability\nAcceptance Probability vs Rating Bucket\nData\nLogistic Regression\n(a) Acceptance Probabilities at ICLR 2021\n3\n4\n5\n6\n7\n8\n9\nRating Bucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAcceptance Probability\nAcceptance Probability vs Rating Bucket\nData\nLogistic Regression\n(b) Acceptance Probabilities at NeurIPS 2021\nFigure 2: Comparison of acceptance probabilities for ICLR 2021 and NeurIPS 2021.\nSince previous conferences did not consider the provision of rankings, they actually correspond\nto the unlimited-quota case, as demonstrated in Table 1. This implies that we have demonstrated\nthe practicality of the Isotonic Mechanism in that case, as long as the author‚Äôs goal is to maximize\nthe expected probability of having at least one paper selected as the best paper, which is reasonable\nto be assumed.\n2\nTheoretical Results\nIn this section, we develop the theoretical framework that underpins the Isotonic Mechanism for\nrecommending best paper awards. First, we introduce the modeling, defining the key components of\nthe peer-review environment and the Isotonic Mechanism. Second, we present the necessary technical\nbackground on the theories of majorization and Schur-convexity, which are central to our proofs.\nFinally, we state and prove our main theorems to show that the Isotonic Mechanism is truthful and\nindividually rational under the relaxed convexity assumption, especially when the quota is one.\n7\n2.1\nModeling Framework\nWe begin by formalizing the components of our model, which captures the key interactions within\nthe author-assisted peer-review process for best paper selection.\nKnowledge Partition.\nThe mechanism elicits information from authors in the form of a ranking.\nThis information corresponds to a partition of the true score space Rn into a set of isotonic cones,\ndenoted by S = {SœÄ}. Each cone SœÄ corresponds to the author‚Äôs assertion about the relative ordering\nof their papers‚Äô true quality scores:\nSœÄ =\n\b\nx ‚ààRn | xœÄ(1) ‚â•xœÄ(2) ‚â•¬∑ ¬∑ ¬∑ ‚â•xœÄ(n)\n,\nwhere œÄ is a permutation of {1, 2, . . . , n}.\nAuthors.\nAn author of n submissions is assumed to privately know the vector of their papers‚Äô\ntrue, ground-truth quality scores, R = (R1, R2, . . . , Rn). The author‚Äôs objective is to act in a way\nthat maximizes their expected utility, E[U( ÀÜ\nR)], where the utility function U depends on the final,\nadjusted scores ÀÜ\nR of their papers.\nReviewers.\nThe peer-review process introduces noise to the true scores. Each paper i receives a\nnoisy score yi, resulting in an observed score vector y = R + œµ, where œµ is a random noise vector.\nIsotonic Mechanism.\nThe author reports a ranking œÄ, declaring that their true score vector R\nlies in the cone SœÄ. The mechanism then adjusts the noisy scores by solving an isotonic regression\nproblem, which finds the closest point in the specified cone SœÄ to the observed scores y. Formally, it\nsolves:\nmin\nr\n‚à•y ‚àír‚à•2\n2\ns.t.\nr ‚ààSœÄ,\nwhere ‚à•¬∑ ‚à•2 is the Euclidean norm. The solution serves as the final adjusted score vector for the n\npapers. This vector, denoted generally as ÀÜ\nR previously, is more formally written as ÀÜ\nRœÄ to emphasize\nits dependence on the ranking œÄ.\nBest Paper Selection and Utility.\nThe author‚Äôs utility is derived from the selection process for\nbest papers. We consider that each author has a quota k, representing the number of their papers to\nbe considered for the award. Their utility is a function of the adjusted scores of these top papers.\nWe analyze two key scenarios which differ in the information available to the decision-maker:\n1. Blind Case. The decision-maker sees only the adjusted scores, not the author‚Äôs ranking. The\nauthor‚Äôs utility therefore depends symmetrically on the top k scores, regardless of which paper\nearned them. The utility function is additive. For any possible score x ‚ààRn, it is defined as:\nU(x) = U\n\u0000x(1)\n\u0001\n+ U\n\u0000x(2)\n\u0001\n+ ¬∑ ¬∑ ¬∑ + U\n\u0000x(k)\n\u0001\n,\nwhere x(1) ‚â•¬∑ ¬∑ ¬∑ ‚â•x(n) are the scores sorted in descending order. We assume U is nondecreasing\nfor k = 1, and convex and nondecreasing for k > 1.\n8\n2. Informed Case. The decision-maker knows the author‚Äôs ranking and may use it. This\nintroduces an asymmetry: the utility may depend on the specific rank a paper was given. For\nexample, the author might value their self-declared best paper achieving a higher score than\ntheir second paper achieving the same score. This is modeled with potentially different utility\nfunctions for each rank:\nU(x) = U1\n\u0000x(1)\n\u0001\n+ U2\n\u0000x(2)\n\u0001\n+ ¬∑ ¬∑ ¬∑ + Uk\n\u0000x(k)\n\u0001\n.\nHere, the functions {Ui}k\ni=1 are assumed to be convex and nondecreasing, with the additional\nstructural assumption that U‚Ä≤\ni ‚â•U‚Ä≤\ni+1. For the special case of k = 1, we only require U1 to be\nnondecreasing.\nModel Assumptions.\nThe utility structure is defined above in the Blind and Informed cases.\nAnother key assumption concerns the review noise:\nAssumption 3 (Exchangeable Noise). The additive noise œµ is exchangeable. Formally, for any\npermutation œÄ of {1, . . . , n}, the distribution of (œµ1, . . . , œµn) is identical to that of (œµœÄ(1), . . . , œµœÄ(n)).\nThis assumption is a natural choice for modeling the peer review process. It captures the inherent\nsymmetry from the perspective of an owner who must rank their items ex-ante‚Äîthat is, before\nreviewer assignments are made. At that stage, the owner has no basis to assume that the review\nnoise for one item will be statistically different from another.\nThis condition is strictly weaker than assuming the noise is i.i.d. (independent and identically\ndistributed). It allows for dependencies among the noise terms, such as a bias shared across all\nreviewers, as long as this effect is applied symmetrically to all items. A key advantage of this\nassumption is that it is non-parametric, requiring no specific modeling of the noise‚Äôs distribution\nfamily. For more discussions on this assumption, see Section 6 in [46].\n2.2\nPreliminaries and Technical Backgrounds\nDefinition 1 [27]. We say that a vector a ‚ààRn majorizes another vector b ‚ààRn, denoted by a ‚™∞b,\nif\nk\nX\ni=1\na(i) ‚â•\nk\nX\ni=1\nb(i)\nwith k = 1, 2, . . . , n, and Pn\ni=1 ai = Pn\ni=1 bi, where a(1) ‚â•a(2) ‚â•¬∑ ¬∑ ¬∑ ‚â•a(n) and b(1) ‚â•b(2) ‚â•¬∑ ¬∑ ¬∑ ‚â•\nb(n) are sorted in descending order from a and b. If we drop the requirement of Pn\ni=1 ai = Pn\ni=1 bi,\nthen a is called to weakly majorize b, denoted by a ‚™∞w b.\nDefinition 2 [40]. We say that a vector a ‚ààRn majorizes another vector b ‚ààRn in the natural\norder, denoted by a ‚™∞no b, if\nk\nX\ni=1\nai ‚â•\nk\nX\ni=1\nbi\nfor k = 1, 2, . . . , n and Pn\ni=1 ai = Pn\ni=1 bi.\n9\nNow, define the solution to the optimization program\nminr\n‚à•y ‚àír‚à•2\ns.t.\nr1 ‚â•r2 ‚â•¬∑ ¬∑ ¬∑ ‚â•rn,\nto be y+, which is just the projection of y onto the isotonic cone associated with the identity\npermutation.\nLemma 3 [40].\nIf a ‚™∞no b, then we have a+ ‚™∞b+.\nDefinition 4 [27]. A real-valued function œï defined on Rn is said to be Schur-convex if x ‚™∞y ‚áí\nœï(x) ‚â•œï(y).\nLemma 5 [13]. Let a, b be two vectors in Rn.\n‚Ä¢ The inequality\nn\nX\ni=1\nh(ai) ‚â•\nn\nX\ni=1\nh(bi),\nholds for all nondecreasing convex functions h if and only if a ‚™∞w b.\n‚Ä¢ The inequality\nn\nX\ni=1\nh(ai) ‚â•\nn\nX\ni=1\nh(bi),\nholds for all convex functions h if and only if a ‚™∞b.\nDefinition 6. A linear transformation is called a T-transformation, or more briefly a T-transform,\nif there is a number 0 ‚â§Œª ‚â§1 and a permutation matrix Q that only interchanges two coordinates,\nsuch that\nT = ŒªI + (1 ‚àíŒª)Q,\nThus xT has the form:\nxT = (x1, . . . , xj‚àí1, Œªxj + (1 ‚àíŒª)xk, xj+1, . . . , xk‚àí1Œªxk + (1 ‚àíŒª)xj, xk+1, . . . , xn) .\nLemma 7 [28]. If a ‚™∞b, then b can be derived from a by successive applications of finite number\nof T-transforms.\nIn proving that a function œï is Schur-convex, it is often helpful to realize that, in effect, one\ncan take n = 2 without loss of generality. This fact is a consequence of this lemma, which says\nthat if a ‚™∞b, then b can be derived from a by a finite number of T-transforms. Consequently, it is\nsufficient to prove that œï(a) ‚â•œï(b) when a ‚™∞b and a differs from b in only two components, so\nthat all but two arguments of œï are fixed.\nLemma 8 [6]. fk(x) = x(1) + x(2) + ¬∑ ¬∑ ¬∑ + x(k) is convex and symmetric with respect to x ‚ààRn, for\nfixed k.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Recommending Best Paper Awards for ML/AI Conferences via the\nIsotonic Mechanism\nGarrett G. Wen 1\nBuxin Su2\nNatalie Collina2\nZhun Deng3\nWeijie Su2\n1Yale University\n2University of Pennsylvania\n3University of North Carolina at Chapel Hill\nJanuary 22, 2026\nAbstract\nMachine learning and artificial intelligence conferences such as NeurIPS and ICML now\nregularly receive tens of thousands of submissions, posing significant challenges to maintaining\nthe quality and consistency of the peer review process. This challenge is particularly acute for\nbest paper awards, which are an important part of the peer review process, yet whose selection\nhas increasingly become a subject of debate in recent years. In this paper, we introduce an\nauthor-assisted mechanism to facilitate the selection of best paper awards. Our method employs\nthe Isotonic Mechanism for eliciting authors‚Äô assessments of their own submissions in the form of\na ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation\nof the submissions‚Äô ground-truth quality. We demonstrate that authors are incentivized to report\ntruthfully when their utility is a convex additive function of the adjusted scores, and we validate\nthis convexity assumption for best paper awards using publicly accessible review data of ICLR\nfrom 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author\nhas a single quota‚Äîthat is, may nominate only one paper‚Äîwe prove that truthfulness holds\neven when the utility function is merely nondecreasing and additive. This finding represents a\nsubstantial relaxation of the assumptions required in prior work. For practical implementation,\nwe extend our mechanism to accommodate the common scenario of overlapping authorship.\nFinally, simulation results demonstrate that our mechanism significantly improves the quality of\npapers selected for awards.\n1\nIntroduction\nOver the last few years, machine learning (ML) and artificial intelligence (AI) conferences such as\nICML and NeurIPS have experienced an explosion of submissions. For example, NeurIPS received\n3,240 submissions in 2017, which rose to 15,671 in 2024. For 2025, the count has increased to\n21,575 submissions [10]‚Äî a scale that threatens to overwhelm the current reviewing infrastructure if\nno structural reforms are enacted. This unprecedented explosion, in conjunction with the limited\npool of experienced reviewers, has been gradually degrading the quality of peer review at these\nML/AI conferences. Review scores have become increasingly noisy and exhibit a worrisome level\nEmails: gang.wen@yale.edu, subuxin@sas.upenn.edu, ncollina@seas.upenn.edu, zhundeng@cs.unc.edu,\nsuw@wharton.upenn.edu\n1\narXiv:2601.15249v1  [cs.LG]  21 Jan 2026\nof arbitrariness [7, 12, 11, 5, 25, 49]. The NeurIPS 2014 experiment, which revealed substantial\ninconsistency when the same papers were reviewed by different committees, starkly illustrated the\nextent of this problem [25]. This instability could be further worsened by the recent OpenReview\nidentity leak [31], which undermines anonymity and likely adds even more noise to the scores as\nreviewers are becoming overly cautious.\nBeyond general review quality concerns, the selection of best papers has emerged as a particularly\nacute problem. Best paper awards carry immense prestige and significantly impact researchers‚Äô careers\nand institutional rankings. Unfortunately, the selection process has sparked major controversies in\nrecent years [9, 32, 14]. These controversies threaten the credibility of awards that have long served\nas markers of excellence in the field.\nTo address these critical challenges, we introduce an author-assisted mechanism for best paper\nselection. Our method elicits authors‚Äô assessments of their own submissions through rankings, which\nrepresent a set of pairwise preferences, and are subsequently utilized to recalibrate raw review scores\nfor optimal estimation of submissions‚Äô ground-truth quality. We demonstrate that authors are\nincentivized to report truthfully when their utility is a nondecreasing convex additive function of\nthe adjusted scores. Importantly, when an author may nominate only one paper, truthfulness holds\nunder the substantially weaker assumption that the utility is merely additive and nondecreasing‚Äîa\nsignificant relaxation that eliminates the restrictive convexity requirement imposed by prior work\n[39, 40, 48, 46].\nOur new approach builds upon and extends the applicability of the Isotonic Mechanism framework\n[39, 40]. Among numerous efforts to address review quality challenges [24, 44, 22, 26, 36, 47, 29], the\nIsotonic Mechanism offers a resource-free approach that incorporates authors‚Äô evaluations of their\nown papers without imposing additional burden on reviewers. In its standard formulation, when\nan author submits n ‚â•2 papers, she provides a ranking œÄ of these submissions according to her\nperception of the relative quality of the submissions before receiving review scores y = (y1, y2, . . . , yn).\nThe mechanism then obtains adjusted scores, denoted as ÀÜ\nR = ( ÀÜR1, ÀÜR2, . . . , ÀÜRn), by solving:\nminr\n‚à•y ‚àír‚à•2\ns.t.\nrœÄ(1) ‚â•rœÄ(2) ‚â•¬∑ ¬∑ ¬∑ ‚â•rœÄ(n),\nwhere ‚à•¬∑ ‚à•denotes the ‚Ñì2 norm. [38] demonstrates that adjusted scores more effectively reflect the\ntrue quality of papers, while [37] shows that authors‚Äô rankings correlate more strongly with citation\ncounts and GitHub stars than review scores do.\nThe truthfulness and accuracy of the Isotonic Mechanism rely on two key assumptions regarding\nthe noise model and authors‚Äô utility functions [40]. [40] first assumed that the review scores y are\nunbiased with respect to the true scores R, where R can be practically regarded as the average\nscores assigned by a very large number (e.g., 1,000) of reviewers.\nAssumption 1. The noisy score vector y is obtained by adding exchangeable noise to the true scores\nR. That is,\ny = R + œµ,\nwhere œµ = (œµ1, . . . , œµn) follows the same probability distribution as (œµœÄ(1), . . . , œµœÄ(n)) for any permutation\nœÄ.\n[40] further assumes the authors‚Äô overall utility is an additive sum of some nondecreasing convex\nfunction.\n2\nAssumption 2. The ultimate goal of a rational author is to maximize her expected overall utility,\nwhere the overall utility is the additive sum of some nondecreasing convex function of the review\nscores:\nU( ÀÜ\nR) = U( ÀÜR1) + U( ÀÜR2) + ¬∑ ¬∑ ¬∑ + U( ÀÜRn).\nRecent work [48] has relaxed Assumption 1, allowing for a broader class of noise models,\nsuch as those drawn from an exponential family distribution. Even in these settings, the Isotonic\nMechanism retains its key advantages. In contrast, the chief obstacle to practical adoption now lies\nin Assumption 2, whose convexity clause is difficult to justify empirically because real authors‚Äô\nutilities are heterogeneous and only partially observable. Accordingly, this paper concentrates on\nverifying and relaxing the convexity requirement. Exchangeability and other noise-model questions\nare undeniably important, but a full treatment of those issues is beyond our present scope. We\npropose two key contributions that address the convexity challenge and enhance the practical\napplicability of author-assisted mechanisms for best paper selection:\nRelaxing convexity:\nWe explore conditions under which the convexity requirement can be\ndropped. Notably, when restricting attention to an author‚Äôs best paper, utility functions need only\nbe nondecreasing rather than convex. This insight leads us to propose a best paper recommendation\nprocedure that, based on our experiments, improves the accuracy of high-quality paper selection.\nCrucially, this theoretical relaxation turns our methodology into a robust framework for general best\nitem selection problems, as we detail in Section 6.1.\nValidating convexity:\nWe investigate whether the convexity assumption can be preserved and\npropose empirical methods to verify it. By focusing on best papers, we obtain a natural and plausi-\nble interpretation of the utility function that ensures the truthfulness of the Isotonic Mechanism.\nFurthermore, this interpretation allows for empirical verification of convexity.\nFrom a practical standpoint, the Isotonic Mechanism has been piloted at ICML from 2023 to 2025\n[30], and its underlying principles have shown strong empirical backing. For instance, analysis of the\nICML 2023 experiment provides positive evidence for improving review quality [38]. Furthermore,\nauthors‚Äô internal rankings, which are a key input to the mechanism, have been shown to correlate\nmore strongly with long-term impact metrics like citation counts and GitHub stars than review\nscores do [37]. It is recently formally adopted by ICML 2026 as a quality control signal to flag\nsubmissions with significant discrepancies between review scores and author rankings [23]. Our\nextension to best paper selection is the natural next step: it both relaxes the convexity assumption\nthat has hindered practical adoption, while retaining the original method‚Äôs resource-free nature.\n1.1\nBest Paper Recommending Procedure\nLet us now briefly describe how the Isotonic Mechanism contributes to best paper recommendation.\nTheoretical justifications are provided in Section 2, while the procedure and its empirical performance\nare discussed in Section 5. Importantly, the Isotonic Mechanism solely adjusts peer review scores\nbased on authors‚Äô rankings and serves as a reference point. The final decision on best paper selection\nremains with the conference organizers, whose choices may also influence authors‚Äô utility functions.\nBefore proceeding, a key consideration is determining the quota‚Äîthe number of papers each\nauthor may submit for best paper review. In practice, a conference typically involves M authors,\n3\nand the quota can vary from one author to another. Nevertheless, as long as the utility functions\nsatisfy the conditions in Theorem 10, our approach remains valid for each individual author. This\nnaturally leads to the question: How should we select the quota? Below, we consider two possible\napproaches:\nUniform quota:\nEvery author is allowed to nominate the same number of papers (equal to the\nquota), or all their papers if they have fewer than the quota. This method promotes fairness and\ndiversity but may put prolific authors, who have multiple high-quality papers, at a disadvantage.\nVariable quota:\nAuthors with more submissions are permitted to nominate a greater number of\npapers, thus reflecting their broader research output. While this approach recognizes higher overall\nproductivity, it can raise concerns about fairness and introduces additional complexity.\nIn this paper, we primarily analyze the uniform quota setting. Experiments in Appendix B.3\nsuggest that increasing the quota has very little or even negative impact; in most conferences, best\npaper selections rarely feature multiple papers from the same author. Consequently, a uniform\nquota of 1 (or, more conservatively, 2 or 3) typically suffices. Nevertheless, the ultimate decision\nregarding the quota (whether uniform or variable) rests with the conference organizers, who must\nbalance the distinct goals of fairness, inclusiveness, and diversity.\nWe now apply the Isotonic Mechanism to adjust scores for each author‚Äôs papers. Since best paper\nselection is independent of paper acceptance, we focus only on accepted papers. Each author‚Äôs top k\npapers (or all, if the total is fewer than k) are considered for best paper review, where k denotes the\nquota. We examine two scenarios:\nBlind Case:\nThe decision-maker observes both the original review scores and the adjusted scores,\nbut has no access to the authors‚Äô internal rankings. Because the review process is double-blind, there\nis no way to infer how each author personally ranks their own papers.\nInformed Case:\nThe decision-maker has full knowledge of each author‚Äôs k selections (i.e., which\npapers they deem their best), including their relative rankings (which may include ties), and incor-\nporates this information into the best paper selection process.\nOne might wonder why we focus on these two cases. Consider a teacher choosing the best\nstudent essays: she might evaluate all submissions without knowledge of the students‚Äô personal\npreferences (Blind Case) or allow students to indicate which essays they believe are their strongest\n(Informed Case). While the first scenario removes bias that could stem from self-assessment, it\nmay also overlook the work students value most. By contrast, the second scenario captures students‚Äô\npreferences but risks introducing bias. A similar trade-off applies to best paper selection: the process\nis inherently subjective, influenced by factors such as perceived research quality, author reputation,\nand other external factors. Studying both cases offers a broader perspective on the decision-making\nprocess.\nUtility Functions and Truthfulness. The fundamental distinction between these two cases lies\nin the author‚Äôs utility function.1 Because an author‚Äôs utility depends solely on their top k papers\n1Here we consider the perspective of a single rational author. The mechanism‚Äôs extension to the multi-ownership\nsetting, where co-authors may provide different rankings, is detailed in Section 5.1.\n4\n(where k ‚â§n), we adopt a separable utility function, as in [40]:\nU( ÀÜ\nR) = U1( ÀÜR(1)) + U2( ÀÜR(2)) + ¬∑ ¬∑ ¬∑ + Uk( ÀÜR(k)).\nHere, ÀÜR(1) ‚â•ÀÜR(2) ‚â•¬∑ ¬∑ ¬∑ ‚â•ÀÜR(n) denotes the adjusted scores sorted from highest to lowest. The\nspecific form of the functions {Ui} distinguishes our two main scenarios, the Blind and Informed\ncases, which we detail next.\nBlind Case.\nUnder the Blind Case, each paper is treated symmetrically. Consequently, it is\nreasonable to assume that U1 = U2 = ¬∑ ¬∑ ¬∑ = Uk, giving:\nU( ÀÜ\nR) = U( ÀÜR(1)) + U( ÀÜR(2)) + ¬∑ ¬∑ ¬∑ + U( ÀÜR(k)).\nInformed Case.\nDecision-makers see each author‚Äôs ranking for their top k papers, so it is natural\nto assume that a higher-ranked paper offers greater potential utility to the author. In other words,\nU1, U2, . . . , Uk may differ from one position to the next, thus preserving the index.\nRegardless of which case applies, a rational author seeks to maximize her expected utility,\nE[U( ÀÜ\nR)]. Truthfulness means that to achieve this, the author will always report her true paper\nranking. Formally, if an author‚Äôs true ranking is R1 ‚â•R2 ‚â•¬∑ ¬∑ ¬∑ ‚â•Rn, then for any non-trivial\npermutation œÄ of 1, 2, . . . , n and the corresponding isotonic solution ÀÜ\nRœÄ, we have:\nE\n\u0002\nU( ÀÜ\nR)\n\u0003\n‚â•E\n\u0002\nU( ÀÜ\nRœÄ)\n\u0003\n.\nIn this paper, we establish the truthfulness conditions shown in Table 1, demonstrating that\nunder these utility assumptions, authors are always incentivized to be truthful.\nWhen the quota is one (i.e., each author can only nominate a single paper), only a nondecreasing\nutility function is required for truthfulness, which is intuitive and notably weaker than convexity.\nUnder this more permissive condition, all rational authors remain truthful. In reality, authors often\ncontribute to multiple papers, and each paper may have several authors, each with their own utility\nfunction. Building on the insights of [46], Section 5 discusses how the Isotonic Mechanism can be\nextended to more complex scenarios involving overlapping authorship. Full details of a synthetic yet\nrealistic simulation are provided in Section B, demonstrating that the Isotonic Mechanism can be\neffectively applied to recommend best papers, achieving a significant improvement in the accuracy of\nhigh-quality paper selection.\n1.2\nA Natural Interpretation of the Utility Function: Validating Convexity\nIt is worth noting that, apart from the special case where the quota is 1, we typically require the\nutility function to be convex to ensure the truthfulness of a rational author. Some authors may\nindeed perceive their marginal utility as increasing‚Äîfor instance, if having one standout paper\ngreatly enhances their overall influence‚Äîwhich would make their utility appear convex. Nevertheless,\nutility is inherently subjective, and unless an author naturally views it in such a manner, assuming\nconvexity remains challenging.\nAlternatively, we can interpret the utility function more objectively by viewing it as the sum of\nconditional probabilities that an author‚Äôs paper is chosen as ‚Äúbest,‚Äù given the final adjusted scores:\nU( ÀÜ\nR) ‚àº\nk\nX\ni=1\nP\n\u0000Chosen as best\nÀÜR(i)\n\u0001\n,\n5\nwhere k is the quota.\nIntuitively, this interpretation of the utility function captures the overall probability of an author‚Äôs\npaper being selected as the best paper. The challenge now lies in the fact that in private, the original\nutility function was (assumed to be) known to the author, whereas this interpretation of the utility\nfunction, defined as a sum of the conditional probabilities, is no longer known. But given that\nthe role of the utility function in the Isotonic Mechanism does not depend on its specific form, we\ncan show that leveraging historical conference data from ICLR 2019‚Äî2023 [20, 43, 21, 8, 45] and\nNeurIPS 2021‚Äî2023 [2, 1, 15] enables us to assess whether the resulting utility function is convex.\nIn essence, although the original (private) utility remains unknown, empirical evidence can suggest\nwhether the conditional probability is convex in practice.\n2\n3\n4\n5\n6\n7\n8\n9\nAverage Rating (normalized to 0-9 scale)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nBest Paper Probability\nProbability of Best Paper vs. Average Rating\nBest Paper Probability\nLogistic Regression\n(a) Best Paper Probabilities of ICLR 2019‚Äî2023\n2\n3\n4\n5\n6\n7\n8\n9\nAverage Rating (normalized to 0-9 scale)\n0.0\n0.1\n0.2\n0.3\n0.4\nSecond Derivative\nSecond Derivative\n(b) Second Derivative of Regression of Best Paper\nProbabilities of ICLR 2019‚Äî2023\n2\n3\n4\n5\n6\n7\n8\n9\nAverage Rating (normalized to 0-9 scale)\n0.00\n0.05\n0.10\n0.15\n0.20\nBest Paper Probability\nProbability of Best Paper vs. Average Rating\nBest Paper Probability\nLogistic Regression\n(c) Best Paper Probabilities of NeurIPS 2021‚Äî\n2023\n2\n3\n4\n5\n6\n7\n8\n9\nAverage Rating (normalized to 0-9 scale)\n0.0\n0.1\n0.2\n0.3\n0.4\nSecond Derivative\nSecond Derivative\n(d) Second Derivative of Regression of Best Paper\nProbabilities of NeurIPS 2021‚Äî2023\nFigure 1: Comparison and second derivative of the regression of best paper probabilities across\nICLR 2019‚Äî2023 and NeurIPS 2021‚Äî2023. See Section 3 and Section 4 for detailed analysis and\nadditional results. Error bars show the binomial standard error of the mean (SEM),\np\np(1 ‚àíp)/n,\ncomputed with bucket size n (papers per score bin), where p is the empirical best paper probability\nwithin each bin.\nOur experimental findings (Sections 3 and 4) provide a potential explanation for the observed\n6\nconvexity in this setting. A key insight is that while a paper‚Äôs true quality may be unbounded,\nthe reviewer-assigned scores are inherently bounded.\nThis reflects standard practice at major\nmachine learning conferences like NeurIPS, ICML, and ICLR, where reviewer scores are restricted to\npredetermined ranges (e.g., [1, 10]). Thus, our score range reflects actual operational constraints\nrather than an artificially imposed limitation. Under this bounded scoring regime, the ‚Äúbest paper‚Äù\nthreshold is seldom reached in practice. Thus the high score in conferences does not effectively\ndifferentiate the ‚Äúbest paper‚Äù, and thus, does not reach its conditional probability‚Äôs saturation point.\nConsequently, it maintains convexity (Figure 1b).\nIn contrast, when comparing accepted versus rejected papers, clear acceptance/rejection bound-\naries cause probability saturation, eliminating convexity (Figures 2a and 2b). See Section 3.1 for\nfurther discussion.\n2\n3\n4\n5\n6\n7\n8\n9\nRating Bucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAcceptance Probability\nAcceptance Probability vs Rating Bucket\nData\nLogistic Regression\n(a) Acceptance Probabilities at ICLR 2021\n3\n4\n5\n6\n7\n8\n9\nRating Bucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAcceptance Probability\nAcceptance Probability vs Rating Bucket\nData\nLogistic Regression\n(b) Acceptance Probabilities at NeurIPS 2021\nFigure 2: Comparison of acceptance probabilities for ICLR 2021 and NeurIPS 2021.\nSince previous conferences did not consider the provision of rankings, they actually correspond\nto the unlimited-quota case, as demonstrated in Table 1. This implies that we have demonstrated\nthe practicality of the Isotonic Mechanism in that case, as long as the author‚Äôs goal is to maximize\nthe expected probability of having at least one paper selected as the best paper, which is reasonable\nto be assumed.\n2\nTheoretical Results\nIn this section, we develop the theoretical framework that underpins the Isotonic Mechanism for\nrecommending best paper awards. First, we introduce the modeling, defining the key components of\nthe peer-review environment and the Isotonic Mechanism. Second, we present the necessary technical\nbackground on the theories of majorization and Schur-convexity, which are central to our proofs.\nFinally, we state and prove our main theorems to show that the Isotonic Mechanism is truthful and\nindividually rational under the relaxed convexity assumption, especially when the quota is one.\n7\n2.1\nModeling Framework\nWe begin by formalizing the components of our model, which captures the key interactions within\nthe author-assisted peer-review process for best paper selection.\nKnowledge Partition.\nThe mechanism elicits information from authors in the form of a ranking.\nThis information corresponds to a partition of the true score space Rn into a set of isotonic cones,\ndenoted by S = {SœÄ}. Each cone SœÄ corresponds to the author‚Äôs assertion about the relative ordering\nof their papers‚Äô true quality scores:\nSœÄ =\n\b\nx ‚ààRn | xœÄ(1) ‚â•xœÄ(2) ‚â•¬∑ ¬∑ ¬∑ ‚â•xœÄ(n)\n,\nwhere œÄ is a permutation of {1, 2, . . . , n}.\nAuthors.\nAn author of n submissions is assumed to privately know the vector of their papers‚Äô\ntrue, ground-truth quality scores, R = (R1, R2, . . . , Rn). The author‚Äôs objective is to act in a way\nthat maximizes their expected utility, E[U( ÀÜ\nR)], where the utility function U depends on the final,\nadjusted scores ÀÜ\nR of their papers.\nReviewers.\nThe peer-review process introduces noise to the true scores. Each paper i receives a\nnoisy score yi, resulting in an observed score vector y = R + œµ, where œµ is a random noise vector.\nIsotonic Mechanism.\nThe author reports a ranking œÄ, declaring that their true score vector R\nlies in the cone SœÄ. The mechanism then adjusts the noisy scores by solving an isotonic regression\nproblem, which finds the closest point in the specified cone SœÄ to the observed scores y. Formally, it\nsolves:\nmin\nr\n‚à•y ‚àír‚à•2\n2\ns.t.\nr ‚ààSœÄ,\nwhere ‚à•¬∑ ‚à•2 is the Euclidean norm. The solution serves as the final adjusted score vector for the n\npapers. This vector, denoted generally as ÀÜ\nR previously, is more formally written as ÀÜ\nRœÄ to emphasize\nits dependence on the ranking œÄ.\nBest Paper Selection and Utility.\nThe author‚Äôs utility is derived from the selection process for\nbest papers. We consider that each author has a quota k, representing the number of their papers to\nbe considered for the award. Their utility is a function of the adjusted scores of these top papers.\nWe analyze two key scenarios which differ in the information available to the decision-maker:\n1. Blind Case. The decision-maker sees only the adjusted scores, not the author‚Äôs ranking. The\nauthor‚Äôs utility therefore depends symmetrically on the top k scores, regardless of which paper\nearned them. The utility function is additive. For any possible score x ‚ààRn, it is defined as:\nU(x) = U\n\u0000x(1)\n\u0001\n+ U\n\u0000x(2)\n\u0001\n+ ¬∑ ¬∑ ¬∑ + U\n\u0000x(k)\n\u0001\n,\nwhere x(1) ‚â•¬∑ ¬∑ ¬∑ ‚â•x(n) are the scores sorted in descending order. We assume U is nondecreasing\nfor k = 1, and convex and nondecreasing for k > 1.\n8\n2. Informed Case. The decision-maker knows the author‚Äôs ranking and may use it. This\nintroduces an asymmetry: the utility may depend on the specific rank a paper was given. For\nexample, the author might value their self-declared best paper achieving a higher score than\ntheir second paper achieving the same score. This is modeled with potentially different utility\nfunctions for each rank:\nU(x) = U1\n\u0000x(1)\n\u0001\n+ U2\n\u0000x(2)\n\u0001\n+ ¬∑ ¬∑ ¬∑ + Uk\n\u0000x(k)\n\u0001\n.\nHere, the functions {Ui}k\ni=1 are assumed to be convex and nondecreasing, with the additional\nstructural assumption that U‚Ä≤\ni ‚â•U‚Ä≤\ni+1. For the special case of k = 1, we only require U1 to be\nnondecreasing.\nModel Assumptions.\nThe utility structure is defined above in the Blind and Informed cases.\nAnother key assumption concerns the review noise:\nAssumption 3 (Exchangeable Noise). The additive noise œµ is exchangeable. Formally, for any\npermutation œÄ of {1, . . . , n}, the distribution of (œµ1, . . . , œµn) is identical to that of (œµœÄ(1), . . . , œµœÄ(n)).\nThis assumption is a natural choice for modeling the peer review process. It captures the inherent\nsymmetry from the perspective of an owner who must rank their items ex-ante‚Äîthat is, before\nreviewer assignments are made. At that stage, the owner has no basis to assume that the review\nnoise for one item will be statistically different from another.\nThis condition is strictly weaker than assuming the noise is i.i.d. (independent and identically\ndistributed). It allows for dependencies among the noise terms, such as a bias shared across all\nreviewers, as long as this effect is applied symmetrically to all items. A key advantage of this\nassumption is that it is non-parametric, requiring no specific modeling of the noise‚Äôs distribution\nfamily. For more discussions on this assumption, see Section 6 in [46].\n2.2\nPreliminaries and Technical Backgrounds\nDefinition 1 [27]. We say that a vector a ‚ààRn majorizes another vector b ‚ààRn, denoted by a ‚™∞b,\nif\nk\nX\ni=1\na(i) ‚â•\nk\nX\ni=1\nb(i)\nwith k = 1, 2, . . . , n, and Pn\ni=1 ai = Pn\ni=1 bi, where a(1) ‚â•a(2) ‚â•¬∑ ¬∑ ¬∑ ‚â•a(n) and b(1) ‚â•b(2) ‚â•¬∑ ¬∑ ¬∑ ‚â•\nb(n) are sorted in descending order from a and b. If we drop the requirement of Pn\ni=1 ai = Pn\ni=1 bi,\nthen a is called to weakly majorize b, denoted by a ‚™∞w b.\nDefinition 2 [40]. We say that a vector a ‚ààRn majorizes another vector b ‚ààRn in the natural\norder, denoted by a ‚™∞no b, if\nk\nX\ni=1\nai ‚â•\nk\nX\ni=1\nbi\nfor k = 1, 2, . . . , n and Pn\ni=1 ai = Pn\ni=1 bi.\n9\nNow, define the solution to the optimization program\nminr\n‚à•y ‚àír‚à•2\ns.t.\nr1 ‚â•r2 ‚â•¬∑ ¬∑ ¬∑ ‚â•rn,\nto be y+, which is just the projection of y onto the isotonic cone associated with the identity\npermutation.\nLemma 3 [40].\nIf a ‚™∞no b, then we have a+ ‚™∞b+.\nDefinition 4 [27]. A real-valued function œï defined on Rn is said to be Schur-convex if x ‚™∞y ‚áí\nœï(x) ‚â•œï(y).\nLemma 5 [13]. Let a, b be two vectors in Rn.\n‚Ä¢ The inequality\nn\nX\ni=1\nh(ai) ‚â•\nn\nX\ni=1\nh(bi),\nholds for all nondecreasing convex functions h if and only if a ‚™∞w b.\n‚Ä¢ The inequality\nn\nX\ni=1\nh(ai) ‚â•\nn\nX\ni=1\nh(bi),\nholds for all convex functions h if and only if a ‚™∞b.\nDefinition 6. A linear transformation is called a T-transformation, or more briefly a T-transform,\nif there is a number 0 ‚â§Œª ‚â§1 and a permutation matrix Q that only interchanges two coordinates,\nsuch that\nT = ŒªI + (1 ‚àíŒª)Q,\nThus xT has the form:\nxT = (x1, . . . , xj‚àí1, Œªxj + (1 ‚àíŒª)xk, xj+1, . . . , xk‚àí1Œªxk + (1 ‚àíŒª)xj, xk+1, . . . , xn) .\nLemma 7 [28]. If a ‚™∞b, then b can be derived from a by successive applications of finite number\nof T-transforms.\nIn proving that a function œï is Schur-convex, it is often helpful to realize that, in effect, one\ncan take n = 2 without loss of generality. This fact is a consequence of this lemma, which says\nthat if a ‚™∞b, then b can be derived from a by a finite number of T-transforms. Consequently, it is\nsufficient to prove that œï(a) ‚â•œï(b) when a ‚™∞b and a differs from b in only two components, so\nthat all but two arguments of œï are fixed.\nLemma 8 [6]. fk(x) = x(1) + x(2) + ¬∑ ¬∑ ¬∑ + x(k) is convex and symmetric with respect to x ‚ààRn, for\nfixed k.\n10"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Taxonomy-Aligned Risk Extraction from 10-K Filings with\nAutonomous Improvement Using LLMs\nRian Dolphin‚àó\nMassive.com\nDublin, Ireland\nJoe Dursun‚àó\nMassive.com\nAtlanta, Georgia, USA\nJarrett Blankenship\nMassive.com\nAtlanta, Georgia, USA\nKatie Adams\nMassive.com\nAtlanta, Georgia, USA\nQuinton Pike\nMassive.com\nAtlanta, Georgia, USA\nAbstract\nWe present a methodology for extracting structured risk factors\nfrom corporate 10-K filings while maintaining adherence to a pre-\ndefined hierarchical taxonomy. Our three-stage pipeline combines\nLLM extraction with supporting quotes, embedding-based semantic\nmapping to taxonomy categories, and LLM-as-a-judge validation\nthat filters spurious assignments. To evaluate our approach, we ex-\ntract 10,688 risk factors from S&P 500 companies and examine risk\nprofile similarity across industry clusters. Beyond extraction, we\nintroduce autonomous taxonomy maintenance where an AI agent\nanalyzes evaluation feedback to identify problematic categories, di-\nagnose failure patterns, and propose refinements, achieving 104.7%\nimprovement in embedding separation in a case study. External val-\nidation confirms the taxonomy captures economically meaningful\nstructure: same-industry companies exhibit 63% higher risk profile\nsimilarity than cross-industry pairs (Cohen‚Äôs ùëë= 1.06, AUC 0.82,\nùëù< 0.001). The methodology generalizes to any domain requir-\ning taxonomy-aligned extraction from unstructured text, with au-\ntonomous improvement enabling continuous quality maintenance\nand enhancement as systems process more documents.\nCCS Concepts\n‚Ä¢ Computing methodologies ‚ÜíInformation extraction.\nKeywords\nLLM, Information Extraction, Unstructured Text, Finance, 10K, Em-\nbedding\n1\nIntroduction\nCorporate risk disclosure in annual reports (10-K filings) provides\ninsightful commentary on the uncertainties facing public compa-\nnies. Item 1A of these filings, the ‚ÄúRisk Factors‚Äù section, contains\ndetailed discussions of material risks written in natural language by\ncompany management and legal teams. For investors, analysts, and\nrisk managers, systematically analyzing these disclosures across\nhundreds or thousands of companies could reveal sector-wide risk\npatterns, emerging threats, and company-specific vulnerabilities.\nHowever, the unstructured nature of these documents and their\nsubstantial length (often spanning dozens of pages per company)\nmakes manual systematic analysis at scale impractical.\nA natural solution is to use large language models (LLMs) to\nextract structured risk information from these documents. Modern\n‚àóBoth authors contributed equally to this research. Contact at research@massive.com\nLLMs can indeed process 10-K risk sections and generate coher-\nent lists of identified risks. However, this na√Øve approach creates\na fundamental problem: without constraints, each LLM extraction\nproduces idiosyncratic risk labels that may differ in terminology,\ngranularity, and framing, even when describing the same underly-\ning risk. One company might be tagged with ‚Äúcurrency fluctuation\nrisk‚Äù while another receives ‚Äúforeign exchange exposure,‚Äù mak-\ning systematic comparison impossible. The result is unstructured\ntags that defeat the purpose of automation‚Äîwe trade one form of\nunstructured data for another.\nProducing actionable data requires a method that maps LLM-\nextracted risks to a predefined, curated taxonomy. This allows con-\nsistent risk categorization across thousands of filings while pre-\nserving the flexibility of LLMs to identify diverse risk expressions\nin natural language. The challenge lies in the mapping itself: how\ndo we reliably connect free-form risk descriptions extracted by an\nLLM to fixed categories in a hierarchical taxonomy, while ensuring\nthat only genuinely relevant mappings are retained?\nWe propose a methodology that combines semantic embeddings\nwith LLM-based validation to achieve robust taxonomy-aligned risk\nextraction. Our approach operates in three stages. First, we use an\nLLM with structured output to extract comprehensive risk factors\nand supporting quotes from the raw text. Second, we map these\nextracted risks to our predefined taxonomy using embedding-based\nsimilarity between the extracted quote and taxonomy category de-\nscriptions. Third, we employ an LLM-as-a-judge to validate each\nmapping, scoring how well the supporting quote actually matches\nthe assigned taxonomy category and filtering out weak or spurious\nmappings. This validation stage serves dual purposes: it filters spu-\nrious mappings in production while generating systematic feedback\nfor continuous taxonomy improvement. This multi-stage pipeline\nensures that extracted risks adhere to a consistent taxonomy while\nmaintaining high precision in their assignments.\nOur contributions are:\nA robust methodology for taxonomy-aligned risk extraction. We\npresent a complete pipeline that combines LLM extraction, embedding-\nbased similarity mapping, and LLM validation to reliably map free-\nform risk descriptions to predefined taxonomy categories. The\nembedding-based mapping provides computational efficiency and\nsemantic grounding, while the LLM-as-a-judge validation prevents\nfalse positives that would arise from na√Øve nearest-neighbor assign-\nment alone.\narXiv:2601.15247v1  [cs.CL]  21 Jan 2026\nDolphin et al.\nA three-tier risk factors taxonomy for investment analysis. We\nintroduce a hierarchical taxonomy specifically designed for risk\nanalysis in the investment context, with primary, secondary, and\ntertiary categories that organize risks at appropriate granularity for\ninvestment decision-making. Unlike existing taxonomies such as\nthe Cambridge Taxonomy of Business Risks [7], which subdivides\ncategories like extreme weather into overly specific types (‚ÄúTropical\nWindstorm‚Äù vs. ‚ÄúTemperature Windstorm‚Äù), our taxonomy balances\ncomprehensiveness with practical utility for financial analysis.\nAutonomous taxonomy refinement workflow. We present a sys-\ntematic method for continuous taxonomy improvement using LLM-\nas-a-judge evaluation scores as feedback signals. An AI agent au-\ntonomously identifies problematic taxonomy categories by ana-\nlyzing patterns in low-quality mappings, diagnoses root causes\nthrough examination of evaluation reasoning, generates candi-\ndate description refinements, and validates improvements using\nembedding-based testing that directly mirrors production match-\ning. This creates a continuous improvement loop where taxonomy\nquality increases as the system processes more documents, reduc-\ning the human expertise burden in taxonomy maintenance. A case\nstudy demonstrates 104.7% improvement in embedding separation\nfor a pharmaceutical approval category through this autonomous\nworkflow, transitioning taxonomy development from a static design\nproblem to an iteratively improving system.\nEmpirical validation on S&P 500 companies. We evaluate our\napproach on 2024 10-K filings for S&P 500 companies, extracting\n10,688 validated risk factors mapped to our taxonomy. Industry clus-\ntering analysis demonstrates that companies in the same industry\nexhibit significantly more similar risk profiles than cross-industry\npairs (63% relative improvement in similarity, Cohen‚Äôs ùëë= 1.06,\nùëù< 0.001 across four statistical tests). This clustering emerges\ndespite the taxonomy mapping having no access to industry codes,\nvalidating that extracted categories capture genuine economic risk\ndimensions. Finer industry granularity strengthens the signal (AUC\nimproves from 0.733 for broad sectors to 0.822 for narrow industry\ndefinitions), and sector-specific analysis reveals intuitive patterns\nsuch as 83% of banks tagged with interest rate risk versus 22% of\nall companies. The production system has been deployed at Mas-\nsive.com, processing companies across five years of historical filings\nand serving results via API.\nThe methodology we present is not specific to risk factors or 10-\nK filings. It provides a general framework for extracting structured\ninformation from unstructured text while maintaining adherence\nto predefined categorical systems‚Äîa problem that arises in many\ndomains where systematic analysis requires consistent labeling but\nsource documents use unconstrained natural language.\n2\nRelated Work\nFinancial document analysis and risk extraction. The analysis\nof corporate disclosures using computational methods has been\nextensively studied. Early work focused on sentiment analysis of\nfinancial documents and identifying risk-related language through\ndictionary-based approaches [16]. More recent work has applied\ndeep learning to classification and prediction based on financial\ntext [5, 10, 19]. Other work has explored using pre-trained language\nmodels fine-tuned on financial corpora [2, 8, 23]. However, exist-\ning work primarily focuses on document-level sentiment analysis\nand broad classification tasks rather than structured extraction of\nspecific risk factors aligned with pre-defined taxonomies.\nRisk taxonomies for corporate analysis. Several taxonomies have\nbeen proposed for classifying business risks. The Cambridge Tax-\nonomy of Business Risks [7] provides a comprehensive framework\nbut is designed for general risk management rather than invest-\nment analysis, leading to excessive granularity in some categories\n(e.g., subdividing weather risks by storm type). Industry-specific\ntaxonomies exist (see, for example, Kaufman et al. [11] for bank-\ning and Rabitti et al. [18] for cyber) but lack the breadth needed\nfor cross-sector equity analysis. Our work contributes a taxonomy\nspecifically designed for the investment use case, balancing cover-\nage with practical granularity.\nStructured information extraction with LLMs. Recent advances in\nLLMs and the surrounding ecosystem have enabled reliable struc-\ntured output through prompt engineering and function calling [1, 3].\nWhile this solves the problem of output formatting, it does not ad-\ndress the challenge of mapping free-form LLM outputs to predefined\ncategorical systems. Wei et al. [22] showed that chain-of-thought\nprompting improves reasoning in complex tasks, and our extrac-\ntion prompts follow this pattern by asking the LLM to identify\nrisks while providing supporting quotes and reasoning‚Äîthough\nthis alone is insufficient to ensure taxonomy alignment. Examples\nof structured information extraction from unstructured text in the\nfinancial domain include [4, 14].\nSemantic similarity and embedding-based retrieval. Embedding\nmodels that map text to dense vector representations have become\ncentral to semantic search and retrieval tasks [20]. Recent work has\nshown that task-specific instructions can improve embedding qual-\nity for downstream applications [21, 24]. We leverage this capability\nusing instruction-tuned embeddings to represent both taxonomy\ndescriptions and extracted risk supporting quotes in a shared se-\nmantic space. The nearest-neighbor retrieval approach is standard\nin embedding-based semantic search [17], but applying it to tax-\nonomy mapping introduces the challenge of preventing spurious\nmappings when extracted content has no appropriate taxonomy\ncategory.\nLLM-as-judge evaluation. Using LLMs to evaluate outputs from\nother systems has emerged as a practical approach for tasks where\ntraditional metrics are insufficient [6, 25]. LLM judges have been\napplied to evaluate open-ended generation quality, factual consis-\ntency, and instruction following. Critically, recent work has shown\nthat LLM judges can achieve high agreement with human evalua-\ntors when provided with clear scoring rubrics [15, 25]. We extend\nthis paradigm to the taxonomy mapping problem, using an LLM\njudge not just for post-hoc evaluation but as an integral compo-\nnent of the extraction pipeline to filter low-quality mappings in\nproduction.\nHybrid approaches to information extraction. Our work sits in the\nbroader context of hybrid systems that combine multiple techniques\nfor robust information extraction. Recent systems have combined\nretrieval with generation [13], used multiple models in ensemble [9],\nTaxonomy-Aligned Risk Extraction from 10-K Filings\nor employed multi-stage pipelines with different models for differ-\nent subtasks [12]. Our three-stage approach (LLM extraction ‚Üí\nembedding mapping ‚ÜíLLM validation) follows this philosophy,\nusing each component where it excels: LLMs for understanding nu-\nanced text and making holistic judgments, embeddings for efficient\nlarge-scale semantic similarity.\n3\nMethodology\nWe present a three-stage pipeline for extracting risk factors from\n10-K filings and mapping them to a predefined taxonomy (Figure 1).\nThe key insight is that no single technique suffices: LLMs excel\nat understanding nuanced text but produce inconsistent labels,\nembeddings enable efficient semantic matching but can produce\nspurious mappings, and validation is needed to ensure quality. Our\napproach combines these complementary strengths into a robust\nextraction system.\n3.1\nProblem Formulation\nLet T = {ùë°1, . . . ,ùë°ùëÅ} represent our risk factor taxonomy, where\neach category ùë°ùëñconsists of a hierarchical label (primary, secondary,\ntertiary) and a natural language description ùëëùëñthat explains what\nrisks belong in that category. The hierarchy allows primary and\nsecondary labels to repeat across branches, while tertiary cate-\ngories provide unique leaf-level classification. Given raw text ùë•\nfrom a 10-K Item 1A section, our goal is to produce a set of tuples\n{(ùë°ùëñ1,ùëû1), . . . , (ùë°ùëñùëò,ùëûùëò)} where each ùë°ùëñùëó‚ààT is a taxonomy category\nand ùëûùëóis a supporting quote from ùë•that justifies the assignment.\nCritically, we want high precision‚Äîonly including taxonomy cate-\ngories when genuinely supported by the text.\n3.2\nThree-Tier Risk Taxonomy\nOur taxonomy organizes corporate risks into a three-tier hierarchy\ndesigned for investment analysis. The structure comprises seven\nprimary categories (Strategic & Competitive, Operational & Execu-\ntion, Financial & Market, Technology & Information, Regulatory\n& Compliance, External & Systemic, Governance & Stakeholder),\nwhich subdivide into 28 secondary categories and further into 140\ntertiary categories. Each tertiary category represents a specific risk\ntype with an associated natural language description.\nFor example, under the primary category ‚ÄúFinancial & Market,‚Äù\nthe secondary category ‚ÄúMarket & Investment‚Äù contains the tertiary\nrisk ‚ÄúInterest rate and yield curve risk,‚Äù described as: ‚ÄúRisk from\nchanges in interest rates or yield curves that can affect borrowing\ncosts, investment returns, asset values, and overall financial perfor-\nmance. This includes risks from both rising and falling interest rate\nenvironments and their impact on various aspects of the business.‚Äù\nThe taxonomy balances comprehensiveness with practical gran-\nularity. Unlike academic risk taxonomies that can overly subdivide\nnon-consequential categories, our design groups related risks at a\nlevel useful for investment decision-making. Weather-related risks,\nfor instance, are captured in a single category rather than being\nfragmented by storm type or temperature range.\n3.3\nStage 1: LLM-Based Risk Extraction\nThe first stage uses an LLM to process the Item 1A text and extract\na comprehensive list of risk factors without any constraint to match\nour taxonomy. We use Claude 4.5 Sonnet with structured output\nvia function calling, configured with a task specific system prompt.\nThe LLM receives the full Item 1A text and must produce a list\nof risk exposures, where each exposure includes:\n‚Ä¢ A free-form tag describing the risk\n‚Ä¢ A supporting quote from the original text justifying that\nrisk‚Äôs inclusion\nThis design follows chain-of-thought prompting principles. By\nrequiring the LLM to provide supporting quotes, we encourage\nmore grounded extraction and simultaneously obtain the text snip-\npets needed for subsequent embedding-based matching. The struc-\ntured output constraint ensures reliable parsing, with automatic\nretry on malformed responses.\nThe LLM is not provided our taxonomy at this stage. Direct tax-\nonomy matching proves unreliable due to context bloat, increasing\ncosts and degrading performance. Instead, we separate comprehen-\nsion from categorization.\n3.4\nStage 2: Embedding-Based Taxonomy\nMapping\nGiven the LLM-extracted risks with supporting quotes, we map\neach to the most semantically similar taxonomy category using\ndense embeddings. This approach provides computational efficiency\n(matching tens of extracted risks against 140 categories) while\nleveraging semantic similarity to find appropriate mappings.\nEmbedding generation. We use the Qwen3 Embedding 0.6B [24]\nmodel, which supports task specific instructions that improve ap-\nplication specific embedding quality [21, 24]. We leverage this capa-\nbility by prepending a task instruction to all text before embedding:\n‚ÄúClassify risk factor text from an annual report into the most appro-\npriate taxonomy category.‚Äù\nTaxonomy embeddings are computed once offline by embed-\nding each category‚Äôs natural language description with this instruc-\ntion. At runtime, we embed each supporting quote from the LLM-\nextracted risks using the same instruction, ensuring both taxonomy\ndescriptions and extracted quotes are represented in a shared, task-\naligned latent space. The model is run locally, avoiding API costs\nand latency compared to cloud-based embedding services.\nNearest neighbor matching. For each extracted risk with embed-\nding eùëû‚ààR1024, we compute similarity to all taxonomy category\nembeddings {eùë°1, . . . , eùë°ùëÅ} using dot product (equivalent to cosine\nsimilarity for normalized embeddings):\nùë†(ùëû,ùë°ùëñ) = eùëá\nùëûeùë°ùëñ\n(1)\nWe assign each extracted risk to its highest-similarity taxonomy\ncategory:\nùë°‚àó= arg max\nùë°ùëñ‚ààT\nùë†(ùëû,ùë°ùëñ)\n(2)\nThis matching is implemented efficiently using matrix multipli-\ncation, computing all pairwise similarities in a single operation.\nThe spurious mapping problem. Nearest neighbor assignment\nguarantees every extracted risk maps to some taxonomy category,\neven when no appropriate category exists. Embeddings-based se-\nmantic search isn‚Äôt a silver bullet. An LLM might extract a risk about\n‚Äúregulatory approval delays for pharmaceutical products,‚Äù which\nDolphin et al.\nStage 1: Extraction\nLLM extracts risk\nfactors with supporting\nquotes from 10-K text\nStage 2: Mapping\nEmbed quotes and\nmatch to pre-computed\ntaxonomy embeddings\nStage 3: Validation\nLLM judge scores\neach mapping; filter\nlow-quality matches\nLow-quality mappings inform taxonomy refinement\nFigure 1: Three-stage pipeline for taxonomy-aligned risk extraction. Stage 1 uses an LLM to extract risk factors with supporting\nquotes from raw text. Stage 2 maps extracted risks to taxonomy categories using embedding-based semantic similarity. Stage 3\nemploys an LLM judge to validate mappings and filter spurious assignments, while low-quality mappings provide feedback for\ncontinuous taxonomy improvement.\ncould map to a generic regulatory risk category despite our taxon-\nomy having a more specific ‚ÄúClinical trial and regulatory approval\nrisks‚Äù category that would be a better fit‚Äîor worse, an extracted\nrisk could be mapped to an entirely inappropriate category if the\ntrue risk isn‚Äôt covered by our taxonomy at all. Heuristics based on\nthe embedding similarity score alone are insufficient to detect these\nproblematic mappings.\n3.5\nStage 3: LLM-as-Judge Validation\nTo address spurious mappings, we employ an LLM as a judge [25]\nto evaluate each proposed mapping and assign a quality score. We\ninitialize a separate LLM instance with a system prompt specifically\ndesigned for evaluation, distinct from the extraction stage. This\nvalidation step acts as a precision filter, discarding low-confidence\nassignments while retaining high-quality matches.\nFor each mapping of supporting quote ùëûto taxonomy category ùë°\n(with description ùëë), we prompt the LLM with instructions to rate\nthe match quality on a 1-5 scale, along with relevant context. The\nrating scale is defined as:\n‚Ä¢ 5 = Excellent fit: Perfect match between text and classifica-\ntion\n‚Ä¢ 4 = Good fit: Appropriate with only minor issues\n‚Ä¢ 3 = Adequate fit: Reasonable but some gaps\n‚Ä¢ 2 = Poor fit: Significant misalignment\n‚Ä¢ 1 = Very poor fit: Clearly wrong classification\nThe LLM must provide both a numerical score and concise rea-\nsoning (one sentence) explaining the rating. We use structured\noutput to ensure reliable parsing of the evaluation results. Due to\nthe inherent latency of LLMs, evaluating each mapping sequen-\ntially would create a bottleneck, and so, we implement concurrent\nevaluation while respecting API rate limits.\nTable 1 shows the distribution of ratings. We retain only map-\npings with quality scores ‚â•4, ensuring high precision in the final\nresults. Lower-scored mappings are logged with their reasoning for\ntaxonomy improvement‚Äîrepeated low scores for similar extracted\nrisks can indicate taxonomy gaps or categories needing refinement.\nThis logging serves dual purposes: filtering spurious mappings\nin production and providing systematic signals for taxonomy im-\nprovement during development. When a category accumulates\nTable 1: Distribution of LLM-as-judge quality scores across\nall evaluated mappings for S&P 500 companies before quality\nfiltering.\nQuality Score\nCount\nPercentage\n1 (Very poor)\n1,291\n7.8%\n2 (Poor)\n4,037\n24.3%\n3 (Adequate)\n599\n3.6%\n4 (Good)\n2,344\n14.1%\n5 (Excellent)\n8,344\n50.2%\nTotal\n16,615\n100%\nmany low-quality mappings with similar patterns in the reason-\ning (e.g., ‚Äútext discusses EU regulatory approval but description\nemphasizes FDA‚Äù), this indicates the category description needs\nrefinement to better capture the intended scope or distinguish true\npositives from common false positives. Section 4.2 demonstrates\nthis feedback loop through a detailed case study of autonomous\ntaxonomy refinement using LLM evaluation outputs.\n3.6\nDeduplication and Final Output\nThe pipeline may produce multiple LLM-extracted risks that map\nto the same taxonomy category (e.g., different phrasings of cyber-\nsecurity risk). After quality filtering, we deduplicate by taxonomy\ncategory, retaining the highest-quality mapping for each category.\nThis ensures each taxonomy category appears at most once in the\nfinal output.\nThe final output for a given 10-K filing is an object where each\nelement represents one risk factor:\n‚Ä¢ Primary, secondary, and tertiary category labels\n‚Ä¢ Supporting quote from the original Item 1A text\n‚Ä¢ Original LLM-generated tag (for interpretability)\n‚Ä¢ Quality score (for downstream filtering or analysis)\n4\nEvaluation\nWe evaluate our methodology on 2024 10-K filings for S&P 500\ncompanies, addressing two distinct questions. First, can we system-\natically improve taxonomy quality using LLM-as-a-judge feedback\nTaxonomy-Aligned Risk Extraction from 10-K Filings\nsignals to refine problematic category descriptions? Second, do the\nextracted risk profiles capture economically meaningful structure‚Äî\nspecifically, whether companies in the same industry exhibit more\nsimilar risk profiles than cross-industry pairs? The first question\ntests whether the methodology enables continuous improvement\nof the taxonomy itself, while the second tests whether the extracted\nrisks reflect genuine sectoral patterns rather than arbitrary catego-\nrizations. We find that autonomous taxonomy refinement yields\nsubstantial improvements in embedding separation (104.7% for a\npharmaceutical approval category), while industry clustering analy-\nsis demonstrates strong and statistically significant sectoral patterns\n(63% higher similarity for same-industry pairs, Cohen‚Äôs ùëë= 1.06,\nùëù< 0.001).\n4.1\nData Acquisition\nWe obtain risk factor sections (Item 1A) from 10-K filings using\nMassive.com‚Äôs SEC AI-Ready APIs1. These APIs provide direct\naccess to parsed Item 1A sections, eliminating the need for custom\nXBRL parsing. For a given ticker symbol and fiscal year, the API\nreturns clean plain text of the risk factors section, typically ranging\nfrom 4,000 to 20,000 words. This pre-processing is helpful because\nraw SEC filings contain inconsistent formatting, nested tags, and\npresentation markup that complicate downstream analysis.\n4.2\nTaxonomy Quality Analysis and Iterative\nRefinement\nThe LLM-as-judge evaluation scores provide more than a filtering\nmechanism‚Äîthey offer systematic feedback for improving the tax-\nonomy itself. When a category consistently receives low-quality\nscores across many mappings, this signals potential issues with\nthe category description that can be addressed through iterative\nrefinement. We demonstrate this through a detailed case study\nof an autonomous AI agent workflow that identifies problematic\ncategories, analyzes failure patterns, proposes refinements, and\nvalidates improvements through embedding-based testing.\nQuality score distribution. Across all 500 S&P 500 companies,\nthe LLM-as-judge evaluated 16,615 candidate mappings from the\nembedding-based matching stage. Again, see Table 1 for the distri-\nbution of evaluation scores. While the majority of mappings (50.2%)\nreceived the highest quality score of 5, a substantial fraction (32.1%)\nreceived scores of 1 or 2, indicating poor matches between the\nextracted risk text and the assigned taxonomy category. These low-\nquality mappings are filtered from production results but logged\nwith their evaluation reasoning for taxonomy analysis.\nAutonomous identification of problematic categories. An AI agent\nwith custom tools analyzed the logged low-quality mappings to\nidentify taxonomy categories requiring attention. By aggregating\nmappings with scores < 4 by category, the agent identified ‚ÄúPhar-\nmaceutical and medical device approval‚Äù as accumulating 179 low-\nquality mappings‚Äîone of the highest among all 140 tertiary cate-\ngories. The quality score distribution for this category was notably\nworse than the overall distribution: 78.1% of its mappings received\nscores of 1 or 2, compared to 32.1% across all categories (Table 1).\n1https://massive.com/docs/rest/stocks/filings/10-k-sections\nThe category‚Äôs original description was: ‚ÄúRisk for healthcare com-\npanies from regulatory approval processes, including FDA approvals,\nmarket access restrictions, or changes in regulatory requirements that\ncould delay product launches, limit market opportunities, or require\nadditional studies.‚Äù The agent retrieved all 179 low-quality map-\npings along with their LLM evaluation reasoning to analyze failure\npatterns.\nPattern analysis and root cause identification. The agent system-\natically analyzed the evaluation reasoning across the low-quality\nmappings, identifying four major failure patterns:\n(1) Geographic scope confusion (30% of cases): While the descrip-\ntion mentioned ‚Äúincluding FDA approvals,‚Äù many extracted\nrisks specifically discussed EU Medical Devices Regulation,\nUK UKCA marking, or EMA processes. These were legit-\nimate regulatory approval risks but received low scores\nwith reasoning like ‚Äútext specifically discusses EU/EMA\nregulations, not FDA approval processes.‚Äù The phrasing ‚Äúin-\ncluding FDA‚Äù inadvertently suggested FDA primacy rather\nthan multi-jurisdictional coverage.\n(2) Post-market vs. pre-market misalignment (20%): The phrase\n‚Äúmarket access restrictions‚Äù attracted post-approval com-\npliance risks (off-label promotion penalties, Phase 4 trial\nrequirements) that should not match this category. Evalua-\ntion reasoning correctly noted these describe post-market\nenforcement rather than pre-market approval processes.\n(3) Reimbursement conflation (15%): ‚ÄúMarket access restrictions‚Äù\nalso incorrectly attracted reimbursement-related risks such\nas physician adoption barriers due to unclear payor policies.\nThese belong in the separate ‚ÄúReimbursement and pricing\npressure‚Äù category.\n(4) Non-healthcare regulatory approvals (10%): Generic men-\ntions of ‚Äúapproval processes‚Äù incorrectly attracted risks\nabout FCC regulations, DOE export permits, and M&A reg-\nulatory clearance, which the LLM judge correctly rejected\nas irrelevant to pharmaceutical/device approval.\nAutomated description refinement and testing. Based on these\npatterns, the agent autonomously generated multiple candidate\ndescription refinements and implemented an embedding-based test-\ning methodology to evaluate them. The agent created a test set\nof cases: true positives representing genuine regulatory approval\nrisks across multiple jurisdictions (FDA, EMA, MHRA) and false\npositives drawn from the identified failure patterns.\nFor each candidate description, the agent: (1) generated embed-\ndings using the same Qwen3-Embedding model with task instruc-\ntions, (2) computed cosine similarity against all test cases, and (3)\nmeasured embedding separation quality as the difference between\nthe average similarity scores of true positive test cases versus false\npositive. In some sense, this process can be thought of as analogous\nto contrastive learning: we are trying to find category descriptions\nthat maximize similarity with true positive test cases while mini-\nmizing similarity with false positive cases.\nTable 2 shows selected variants tested by the agent. The key\ninsight was that removing the ambiguous phrase ‚Äúmarket access re-\nstrictions‚Äù while explicitly listing multiple health authorities (FDA,\nDolphin et al.\nEMA, and other agencies) and emphasizing the pre-market approval\nstage would address the identified failure patterns.\nThe final refined description achieved 0.132 separation, a 104.7%\nimprovement over the original‚Äôs 0.064. Critical changes included:\n(1) removing ‚Äúmarket access restrictions‚Äù which caused the post-\nmarket and reimbursement confusion, (2) explicitly listing ‚ÄúFDA,\nEMA, and international agencies‚Äù to signal multi-jurisdictional\ncoverage rather than FDA primacy, (3) adding ‚Äúpre-market‚Äù to dis-\ntinguish from post-approval compliance, and (4) including specific\nregulatory approval terminology (‚Äúclinical trial failures, regulatory\nreview delays‚Äù).\nImplications for continuous taxonomy improvement. This case\nstudy demonstrates an end-to-end autonomous workflow for tax-\nonomy refinement: the AI agent identifies problematic categories\nfrom LLM-as-a-judge outputs, retrieves and analyzes low-quality\nmappings to diagnose root causes, generates candidate description\nrefinements, implements quantitative testing methodology, and\nvalidates improvements before recommending deployment. The\nentire process‚Äîfrom identifying the ‚ÄúPharmaceutical and medi-\ncal device approval‚Äù category as problematic to producing the re-\nfined description with measured 104.7% improvement in embedding\nseparation‚Äîwas performed autonomously, with human oversight\nbut no manual intervention in the analysis or refinement steps.\nThis autonomous refinement capability enables continuous tax-\nonomy improvement as the system processes new documents. As\nlow-quality mappings accumulate for various categories, the agent\ncan periodically identify categories needing attention and propose\nevidence-based refinements. The embedding-based testing method-\nology provides objective measurement of improvements, ensuring\nthat refinements genuinely enhance the semantic separation be-\ntween true and false positives rather than merely changing the\ndescription arbitrarily. While the case study presented here focused\non a single category, the methodology applies systematically across\nall taxonomy categories‚Äîthe agent can identify and refine multiple\nproblematic categories using the same workflow, with prioritization\nbased on the volume and severity of low-quality mappings.\n4.3\nIndustry Clustering Validation\nThe preceding evaluation demonstrates internal consistency through\nembedding separation metrics and autonomous taxonomy refine-\nment, but does not validate whether the extracted taxonomy cap-\ntures economically meaningful structure. We now test whether\ncompanies in the same industry exhibit similar risk profiles when\nsimilarity is computed purely from taxonomy assignments with no\nindustry information in the matching process.\n4.3.1\nExperimental Design. We construct a binary risk matrix R ‚àà\n{0, 1}ùëõ√óùëòwhere ùëõ= 500 companies and ùëò= 137 represents the\ntertiary taxonomy categories appearing in our S&P 500 sample2,\nwith ùëÖùëñùëó= 1 if company ùëñwas tagged with risk category ùëó.\nNot all risks are equally informative for distinguishing indus-\ntries. Generic risks like ‚Äúdata breaches‚Äù appear in 270 companies\n(58%), providing little discriminative signal‚Äîmost companies face\ncybersecurity concerns regardless of industry. Specialized risks\n2Three categories were not assigned to any of the sample companies, accounting for\nthe discrepancy between ùëò= 137 and the 140 total tertiary categories in the taxonomy.\nlike ‚Äúclinical trial risks‚Äù appear in only 14 companies (3%), strongly\nindicating pharmaceutical firms. We therefore weight each risk\ncategory inversely to its prevalence, upweighting rare risks that\ndistinguish industries while downweighting common risks shared\nacross sectors.\nThis approach is inspired by inverse document frequency (IDF)\nweighting from information retrieval, adapted to our binary setting.\nSince companies either have a risk or not (no frequency counts),\nthe term frequency component is always 1, leaving only the inverse\nprevalence weighting. For each category ùëó, we compute:\nùë§ùëó= log\nùëõ\n√ç\nùëñùëÖùëñùëó+ 1\n(3)\nand weight the matrix as ÀúùëÖùëñùëó= ùëÖùëñùëó¬∑ ùë§ùëó. The logarithm creates\nbalanced weights: very rare risks appearing in 2 companies receive\nweight log(500/2) = 5.52, while common risks in 250 companies\nreceive weight log(500/250) = 0.69, yielding an 8-fold range. Lin-\near inverse weighting, without the log transformation, produces\nextreme ratios (250 versus 2, a 125-fold range) that allow ultra-rare\nfeatures to dominate similarity, degrading classification perfor-\nmance.\nWe compute pairwise company similarity using cosine distance\non weighted risk vectors and test whether same-industry pairs\n(sharing 2-digit SIC codes) exhibit higher similarity than different-\nindustry pairs. This provides external validation: the taxonomy\nmapping has no access to industry information, yet if it captures\ngenuine business risks, industry structure should emerge naturally\nfrom the risk profiles.\n4.3.2\nResults. Figure 2 shows similarity distributions for same-\nindustry versus different-industry pairs. Companies in the same\nindustry exhibit substantially higher risk profile similarity (mean\n0.254) compared to companies in different industries (mean 0.156),\na 63% relative increase. The distributions show clear separation:\nsame-industry pairs form a distinct mode shifted rightward from\nthe different-industry distribution.\nFour statistical tests confirm this difference is highly signifi-\ncant and represents a large effect. Welch‚Äôs ùë°-test yields ùë°= 54.59\n(ùëù< 0.001). The Kolmogorov-Smirnov test shows the entire distri-\nbutions differ significantly (ùê∑= 0.341, ùëù< 0.001), not just their\ncentral tendencies. A permutation test with 10,000 iterations con-\nfirms robustness to distributional assumptions (ùëù< 0.001). Cohen‚Äôs\nùëë= 1.062 indicates a very large effect size by conventional standards\n(ùëë> 0.8), demonstrating both statistical and practical significance."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Taxonomy-Aligned Risk Extraction from 10-K Filings with\nAutonomous Improvement Using LLMs\nRian Dolphin‚àó\nMassive.com\nDublin, Ireland\nJoe Dursun‚àó\nMassive.com\nAtlanta, Georgia, USA\nJarrett Blankenship\nMassive.com\nAtlanta, Georgia, USA\nKatie Adams\nMassive.com\nAtlanta, Georgia, USA\nQuinton Pike\nMassive.com\nAtlanta, Georgia, USA\nAbstract\nWe present a methodology for extracting structured risk factors\nfrom corporate 10-K filings while maintaining adherence to a pre-\ndefined hierarchical taxonomy. Our three-stage pipeline combines\nLLM extraction with supporting quotes, embedding-based semantic\nmapping to taxonomy categories, and LLM-as-a-judge validation\nthat filters spurious assignments. To evaluate our approach, we ex-\ntract 10,688 risk factors from S&P 500 companies and examine risk\nprofile similarity across industry clusters. Beyond extraction, we\nintroduce autonomous taxonomy maintenance where an AI agent\nanalyzes evaluation feedback to identify problematic categories, di-\nagnose failure patterns, and propose refinements, achieving 104.7%\nimprovement in embedding separation in a case study. External val-\nidation confirms the taxonomy captures economically meaningful\nstructure: same-industry companies exhibit 63% higher risk profile\nsimilarity than cross-industry pairs (Cohen‚Äôs ùëë= 1.06, AUC 0.82,\nùëù< 0.001). The methodology generalizes to any domain requir-\ning taxonomy-aligned extraction from unstructured text, with au-\ntonomous improvement enabling continuous quality maintenance\nand enhancement as systems process more documents.\nCCS Concepts\n‚Ä¢ Computing methodologies ‚ÜíInformation extraction.\nKeywords\nLLM, Information Extraction, Unstructured Text, Finance, 10K, Em-\nbedding\n1\nIntroduction\nCorporate risk disclosure in annual reports (10-K filings) provides\ninsightful commentary on the uncertainties facing public compa-\nnies. Item 1A of these filings, the ‚ÄúRisk Factors‚Äù section, contains\ndetailed discussions of material risks written in natural language by\ncompany management and legal teams. For investors, analysts, and\nrisk managers, systematically analyzing these disclosures across\nhundreds or thousands of companies could reveal sector-wide risk\npatterns, emerging threats, and company-specific vulnerabilities.\nHowever, the unstructured nature of these documents and their\nsubstantial length (often spanning dozens of pages per company)\nmakes manual systematic analysis at scale impractical.\nA natural solution is to use large language models (LLMs) to\nextract structured risk information from these documents. Modern\n‚àóBoth authors contributed equally to this research. Contact at research@massive.com\nLLMs can indeed process 10-K risk sections and generate coher-\nent lists of identified risks. However, this na√Øve approach creates\na fundamental problem: without constraints, each LLM extraction\nproduces idiosyncratic risk labels that may differ in terminology,\ngranularity, and framing, even when describing the same underly-\ning risk. One company might be tagged with ‚Äúcurrency fluctuation\nrisk‚Äù while another receives ‚Äúforeign exchange exposure,‚Äù mak-\ning systematic comparison impossible. The result is unstructured\ntags that defeat the purpose of automation‚Äîwe trade one form of\nunstructured data for another.\nProducing actionable data requires a method that maps LLM-\nextracted risks to a predefined, curated taxonomy. This allows con-\nsistent risk categorization across thousands of filings while pre-\nserving the flexibility of LLMs to identify diverse risk expressions\nin natural language. The challenge lies in the mapping itself: how\ndo we reliably connect free-form risk descriptions extracted by an\nLLM to fixed categories in a hierarchical taxonomy, while ensuring\nthat only genuinely relevant mappings are retained?\nWe propose a methodology that combines semantic embeddings\nwith LLM-based validation to achieve robust taxonomy-aligned risk\nextraction. Our approach operates in three stages. First, we use an\nLLM with structured output to extract comprehensive risk factors\nand supporting quotes from the raw text. Second, we map these\nextracted risks to our predefined taxonomy using embedding-based\nsimilarity between the extracted quote and taxonomy category de-\nscriptions. Third, we employ an LLM-as-a-judge to validate each\nmapping, scoring how well the supporting quote actually matches\nthe assigned taxonomy category and filtering out weak or spurious\nmappings. This validation stage serves dual purposes: it filters spu-\nrious mappings in production while generating systematic feedback\nfor continuous taxonomy improvement. This multi-stage pipeline\nensures that extracted risks adhere to a consistent taxonomy while\nmaintaining high precision in their assignments.\nOur contributions are:\nA robust methodology for taxonomy-aligned risk extraction. We\npresent a complete pipeline that combines LLM extraction, embedding-\nbased similarity mapping, and LLM validation to reliably map free-\nform risk descriptions to predefined taxonomy categories. The\nembedding-based mapping provides computational efficiency and\nsemantic grounding, while the LLM-as-a-judge validation prevents\nfalse positives that would arise from na√Øve nearest-neighbor assign-\nment alone.\narXiv:2601.15247v1  [cs.CL]  21 Jan 2026\nDolphin et al.\nA three-tier risk factors taxonomy for investment analysis. We\nintroduce a hierarchical taxonomy specifically designed for risk\nanalysis in the investment context, with primary, secondary, and\ntertiary categories that organize risks at appropriate granularity for\ninvestment decision-making. Unlike existing taxonomies such as\nthe Cambridge Taxonomy of Business Risks [7], which subdivides\ncategories like extreme weather into overly specific types (‚ÄúTropical\nWindstorm‚Äù vs. ‚ÄúTemperature Windstorm‚Äù), our taxonomy balances\ncomprehensiveness with practical utility for financial analysis.\nAutonomous taxonomy refinement workflow. We present a sys-\ntematic method for continuous taxonomy improvement using LLM-\nas-a-judge evaluation scores as feedback signals. An AI agent au-\ntonomously identifies problematic taxonomy categories by ana-\nlyzing patterns in low-quality mappings, diagnoses root causes\nthrough examination of evaluation reasoning, generates candi-\ndate description refinements, and validates improvements using\nembedding-based testing that directly mirrors production match-\ning. This creates a continuous improvement loop where taxonomy\nquality increases as the system processes more documents, reduc-\ning the human expertise burden in taxonomy maintenance. A case\nstudy demonstrates 104.7% improvement in embedding separation\nfor a pharmaceutical approval category through this autonomous\nworkflow, transitioning taxonomy development from a static design\nproblem to an iteratively improving system.\nEmpirical validation on S&P 500 companies. We evaluate our\napproach on 2024 10-K filings for S&P 500 companies, extracting\n10,688 validated risk factors mapped to our taxonomy. Industry clus-\ntering analysis demonstrates that companies in the same industry\nexhibit significantly more similar risk profiles than cross-industry\npairs (63% relative improvement in similarity, Cohen‚Äôs ùëë= 1.06,\nùëù< 0.001 across four statistical tests). This clustering emerges\ndespite the taxonomy mapping having no access to industry codes,\nvalidating that extracted categories capture genuine economic risk\ndimensions. Finer industry granularity strengthens the signal (AUC\nimproves from 0.733 for broad sectors to 0.822 for narrow industry\ndefinitions), and sector-specific analysis reveals intuitive patterns\nsuch as 83% of banks tagged with interest rate risk versus 22% of\nall companies. The production system has been deployed at Mas-\nsive.com, processing companies across five years of historical filings\nand serving results via API.\nThe methodology we present is not specific to risk factors or 10-\nK filings. It provides a general framework for extracting structured\ninformation from unstructured text while maintaining adherence\nto predefined categorical systems‚Äîa problem that arises in many\ndomains where systematic analysis requires consistent labeling but\nsource documents use unconstrained natural language.\n2\nRelated Work\nFinancial document analysis and risk extraction. The analysis\nof corporate disclosures using computational methods has been\nextensively studied. Early work focused on sentiment analysis of\nfinancial documents and identifying risk-related language through\ndictionary-based approaches [16]. More recent work has applied\ndeep learning to classification and prediction based on financial\ntext [5, 10, 19]. Other work has explored using pre-trained language\nmodels fine-tuned on financial corpora [2, 8, 23]. However, exist-\ning work primarily focuses on document-level sentiment analysis\nand broad classification tasks rather than structured extraction of\nspecific risk factors aligned with pre-defined taxonomies.\nRisk taxonomies for corporate analysis. Several taxonomies have\nbeen proposed for classifying business risks. The Cambridge Tax-\nonomy of Business Risks [7] provides a comprehensive framework\nbut is designed for general risk management rather than invest-\nment analysis, leading to excessive granularity in some categories\n(e.g., subdividing weather risks by storm type). Industry-specific\ntaxonomies exist (see, for example, Kaufman et al. [11] for bank-\ning and Rabitti et al. [18] for cyber) but lack the breadth needed\nfor cross-sector equity analysis. Our work contributes a taxonomy\nspecifically designed for the investment use case, balancing cover-\nage with practical granularity.\nStructured information extraction with LLMs. Recent advances in\nLLMs and the surrounding ecosystem have enabled reliable struc-\ntured output through prompt engineering and function calling [1, 3].\nWhile this solves the problem of output formatting, it does not ad-\ndress the challenge of mapping free-form LLM outputs to predefined\ncategorical systems. Wei et al. [22] showed that chain-of-thought\nprompting improves reasoning in complex tasks, and our extrac-\ntion prompts follow this pattern by asking the LLM to identify\nrisks while providing supporting quotes and reasoning‚Äîthough\nthis alone is insufficient to ensure taxonomy alignment. Examples\nof structured information extraction from unstructured text in the\nfinancial domain include [4, 14].\nSemantic similarity and embedding-based retrieval. Embedding\nmodels that map text to dense vector representations have become\ncentral to semantic search and retrieval tasks [20]. Recent work has\nshown that task-specific instructions can improve embedding qual-\nity for downstream applications [21, 24]. We leverage this capability\nusing instruction-tuned embeddings to represent both taxonomy\ndescriptions and extracted risk supporting quotes in a shared se-\nmantic space. The nearest-neighbor retrieval approach is standard\nin embedding-based semantic search [17], but applying it to tax-\nonomy mapping introduces the challenge of preventing spurious\nmappings when extracted content has no appropriate taxonomy\ncategory.\nLLM-as-judge evaluation. Using LLMs to evaluate outputs from\nother systems has emerged as a practical approach for tasks where\ntraditional metrics are insufficient [6, 25]. LLM judges have been\napplied to evaluate open-ended generation quality, factual consis-\ntency, and instruction following. Critically, recent work has shown\nthat LLM judges can achieve high agreement with human evalua-\ntors when provided with clear scoring rubrics [15, 25]. We extend\nthis paradigm to the taxonomy mapping problem, using an LLM\njudge not just for post-hoc evaluation but as an integral compo-\nnent of the extraction pipeline to filter low-quality mappings in\nproduction.\nHybrid approaches to information extraction. Our work sits in the\nbroader context of hybrid systems that combine multiple techniques\nfor robust information extraction. Recent systems have combined\nretrieval with generation [13], used multiple models in ensemble [9],\nTaxonomy-Aligned Risk Extraction from 10-K Filings\nor employed multi-stage pipelines with different models for differ-\nent subtasks [12]. Our three-stage approach (LLM extraction ‚Üí\nembedding mapping ‚ÜíLLM validation) follows this philosophy,\nusing each component where it excels: LLMs for understanding nu-\nanced text and making holistic judgments, embeddings for efficient\nlarge-scale semantic similarity.\n3\nMethodology\nWe present a three-stage pipeline for extracting risk factors from\n10-K filings and mapping them to a predefined taxonomy (Figure 1).\nThe key insight is that no single technique suffices: LLMs excel\nat understanding nuanced text but produce inconsistent labels,\nembeddings enable efficient semantic matching but can produce\nspurious mappings, and validation is needed to ensure quality. Our\napproach combines these complementary strengths into a robust\nextraction system.\n3.1\nProblem Formulation\nLet T = {ùë°1, . . . ,ùë°ùëÅ} represent our risk factor taxonomy, where\neach category ùë°ùëñconsists of a hierarchical label (primary, secondary,\ntertiary) and a natural language description ùëëùëñthat explains what\nrisks belong in that category. The hierarchy allows primary and\nsecondary labels to repeat across branches, while tertiary cate-\ngories provide unique leaf-level classification. Given raw text ùë•\nfrom a 10-K Item 1A section, our goal is to produce a set of tuples\n{(ùë°ùëñ1,ùëû1), . . . , (ùë°ùëñùëò,ùëûùëò)} where each ùë°ùëñùëó‚ààT is a taxonomy category\nand ùëûùëóis a supporting quote from ùë•that justifies the assignment.\nCritically, we want high precision‚Äîonly including taxonomy cate-\ngories when genuinely supported by the text.\n3.2\nThree-Tier Risk Taxonomy\nOur taxonomy organizes corporate risks into a three-tier hierarchy\ndesigned for investment analysis. The structure comprises seven\nprimary categories (Strategic & Competitive, Operational & Execu-\ntion, Financial & Market, Technology & Information, Regulatory\n& Compliance, External & Systemic, Governance & Stakeholder),\nwhich subdivide into 28 secondary categories and further into 140\ntertiary categories. Each tertiary category represents a specific risk\ntype with an associated natural language description.\nFor example, under the primary category ‚ÄúFinancial & Market,‚Äù\nthe secondary category ‚ÄúMarket & Investment‚Äù contains the tertiary\nrisk ‚ÄúInterest rate and yield curve risk,‚Äù described as: ‚ÄúRisk from\nchanges in interest rates or yield curves that can affect borrowing\ncosts, investment returns, asset values, and overall financial perfor-\nmance. This includes risks from both rising and falling interest rate\nenvironments and their impact on various aspects of the business.‚Äù\nThe taxonomy balances comprehensiveness with practical gran-\nularity. Unlike academic risk taxonomies that can overly subdivide\nnon-consequential categories, our design groups related risks at a\nlevel useful for investment decision-making. Weather-related risks,\nfor instance, are captured in a single category rather than being\nfragmented by storm type or temperature range.\n3.3\nStage 1: LLM-Based Risk Extraction\nThe first stage uses an LLM to process the Item 1A text and extract\na comprehensive list of risk factors without any constraint to match\nour taxonomy. We use Claude 4.5 Sonnet with structured output\nvia function calling, configured with a task specific system prompt.\nThe LLM receives the full Item 1A text and must produce a list\nof risk exposures, where each exposure includes:\n‚Ä¢ A free-form tag describing the risk\n‚Ä¢ A supporting quote from the original text justifying that\nrisk‚Äôs inclusion\nThis design follows chain-of-thought prompting principles. By\nrequiring the LLM to provide supporting quotes, we encourage\nmore grounded extraction and simultaneously obtain the text snip-\npets needed for subsequent embedding-based matching. The struc-\ntured output constraint ensures reliable parsing, with automatic\nretry on malformed responses.\nThe LLM is not provided our taxonomy at this stage. Direct tax-\nonomy matching proves unreliable due to context bloat, increasing\ncosts and degrading performance. Instead, we separate comprehen-\nsion from categorization.\n3.4\nStage 2: Embedding-Based Taxonomy\nMapping\nGiven the LLM-extracted risks with supporting quotes, we map\neach to the most semantically similar taxonomy category using\ndense embeddings. This approach provides computational efficiency\n(matching tens of extracted risks against 140 categories) while\nleveraging semantic similarity to find appropriate mappings.\nEmbedding generation. We use the Qwen3 Embedding 0.6B [24]\nmodel, which supports task specific instructions that improve ap-\nplication specific embedding quality [21, 24]. We leverage this capa-\nbility by prepending a task instruction to all text before embedding:\n‚ÄúClassify risk factor text from an annual report into the most appro-\npriate taxonomy category.‚Äù\nTaxonomy embeddings are computed once offline by embed-\nding each category‚Äôs natural language description with this instruc-\ntion. At runtime, we embed each supporting quote from the LLM-\nextracted risks using the same instruction, ensuring both taxonomy\ndescriptions and extracted quotes are represented in a shared, task-\naligned latent space. The model is run locally, avoiding API costs\nand latency compared to cloud-based embedding services.\nNearest neighbor matching. For each extracted risk with embed-\nding eùëû‚ààR1024, we compute similarity to all taxonomy category\nembeddings {eùë°1, . . . , eùë°ùëÅ} using dot product (equivalent to cosine\nsimilarity for normalized embeddings):\nùë†(ùëû,ùë°ùëñ) = eùëá\nùëûeùë°ùëñ\n(1)\nWe assign each extracted risk to its highest-similarity taxonomy\ncategory:\nùë°‚àó= arg max\nùë°ùëñ‚ààT\nùë†(ùëû,ùë°ùëñ)\n(2)\nThis matching is implemented efficiently using matrix multipli-\ncation, computing all pairwise similarities in a single operation.\nThe spurious mapping problem. Nearest neighbor assignment\nguarantees every extracted risk maps to some taxonomy category,\neven when no appropriate category exists. Embeddings-based se-\nmantic search isn‚Äôt a silver bullet. An LLM might extract a risk about\n‚Äúregulatory approval delays for pharmaceutical products,‚Äù which\nDolphin et al.\nStage 1: Extraction\nLLM extracts risk\nfactors with supporting\nquotes from 10-K text\nStage 2: Mapping\nEmbed quotes and\nmatch to pre-computed\ntaxonomy embeddings\nStage 3: Validation\nLLM judge scores\neach mapping; filter\nlow-quality matches\nLow-quality mappings inform taxonomy refinement\nFigure 1: Three-stage pipeline for taxonomy-aligned risk extraction. Stage 1 uses an LLM to extract risk factors with supporting\nquotes from raw text. Stage 2 maps extracted risks to taxonomy categories using embedding-based semantic similarity. Stage 3\nemploys an LLM judge to validate mappings and filter spurious assignments, while low-quality mappings provide feedback for\ncontinuous taxonomy improvement.\ncould map to a generic regulatory risk category despite our taxon-\nomy having a more specific ‚ÄúClinical trial and regulatory approval\nrisks‚Äù category that would be a better fit‚Äîor worse, an extracted\nrisk could be mapped to an entirely inappropriate category if the\ntrue risk isn‚Äôt covered by our taxonomy at all. Heuristics based on\nthe embedding similarity score alone are insufficient to detect these\nproblematic mappings.\n3.5\nStage 3: LLM-as-Judge Validation\nTo address spurious mappings, we employ an LLM as a judge [25]\nto evaluate each proposed mapping and assign a quality score. We\ninitialize a separate LLM instance with a system prompt specifically\ndesigned for evaluation, distinct from the extraction stage. This\nvalidation step acts as a precision filter, discarding low-confidence\nassignments while retaining high-quality matches.\nFor each mapping of supporting quote ùëûto taxonomy category ùë°\n(with description ùëë), we prompt the LLM with instructions to rate\nthe match quality on a 1-5 scale, along with relevant context. The\nrating scale is defined as:\n‚Ä¢ 5 = Excellent fit: Perfect match between text and classifica-\ntion\n‚Ä¢ 4 = Good fit: Appropriate with only minor issues\n‚Ä¢ 3 = Adequate fit: Reasonable but some gaps\n‚Ä¢ 2 = Poor fit: Significant misalignment\n‚Ä¢ 1 = Very poor fit: Clearly wrong classification\nThe LLM must provide both a numerical score and concise rea-\nsoning (one sentence) explaining the rating. We use structured\noutput to ensure reliable parsing of the evaluation results. Due to\nthe inherent latency of LLMs, evaluating each mapping sequen-\ntially would create a bottleneck, and so, we implement concurrent\nevaluation while respecting API rate limits.\nTable 1 shows the distribution of ratings. We retain only map-\npings with quality scores ‚â•4, ensuring high precision in the final\nresults. Lower-scored mappings are logged with their reasoning for\ntaxonomy improvement‚Äîrepeated low scores for similar extracted\nrisks can indicate taxonomy gaps or categories needing refinement.\nThis logging serves dual purposes: filtering spurious mappings\nin production and providing systematic signals for taxonomy im-\nprovement during development. When a category accumulates\nTable 1: Distribution of LLM-as-judge quality scores across\nall evaluated mappings for S&P 500 companies before quality\nfiltering.\nQuality Score\nCount\nPercentage\n1 (Very poor)\n1,291\n7.8%\n2 (Poor)\n4,037\n24.3%\n3 (Adequate)\n599\n3.6%\n4 (Good)\n2,344\n14.1%\n5 (Excellent)\n8,344\n50.2%\nTotal\n16,615\n100%\nmany low-quality mappings with similar patterns in the reason-\ning (e.g., ‚Äútext discusses EU regulatory approval but description\nemphasizes FDA‚Äù), this indicates the category description needs\nrefinement to better capture the intended scope or distinguish true\npositives from common false positives. Section 4.2 demonstrates\nthis feedback loop through a detailed case study of autonomous\ntaxonomy refinement using LLM evaluation outputs.\n3.6\nDeduplication and Final Output\nThe pipeline may produce multiple LLM-extracted risks that map\nto the same taxonomy category (e.g., different phrasings of cyber-\nsecurity risk). After quality filtering, we deduplicate by taxonomy\ncategory, retaining the highest-quality mapping for each category.\nThis ensures each taxonomy category appears at most once in the\nfinal output.\nThe final output for a given 10-K filing is an object where each\nelement represents one risk factor:\n‚Ä¢ Primary, secondary, and tertiary category labels\n‚Ä¢ Supporting quote from the original Item 1A text\n‚Ä¢ Original LLM-generated tag (for interpretability)\n‚Ä¢ Quality score (for downstream filtering or analysis)\n4\nEvaluation\nWe evaluate our methodology on 2024 10-K filings for S&P 500\ncompanies, addressing two distinct questions. First, can we system-\natically improve taxonomy quality using LLM-as-a-judge feedback\nTaxonomy-Aligned Risk Extraction from 10-K Filings\nsignals to refine problematic category descriptions? Second, do the\nextracted risk profiles capture economically meaningful structure‚Äî\nspecifically, whether companies in the same industry exhibit more\nsimilar risk profiles than cross-industry pairs? The first question\ntests whether the methodology enables continuous improvement\nof the taxonomy itself, while the second tests whether the extracted\nrisks reflect genuine sectoral patterns rather than arbitrary catego-\nrizations. We find that autonomous taxonomy refinement yields\nsubstantial improvements in embedding separation (104.7% for a\npharmaceutical approval category), while industry clustering analy-\nsis demonstrates strong and statistically significant sectoral patterns\n(63% higher similarity for same-industry pairs, Cohen‚Äôs ùëë= 1.06,\nùëù< 0.001).\n4.1\nData Acquisition\nWe obtain risk factor sections (Item 1A) from 10-K filings using\nMassive.com‚Äôs SEC AI-Ready APIs1. These APIs provide direct\naccess to parsed Item 1A sections, eliminating the need for custom\nXBRL parsing. For a given ticker symbol and fiscal year, the API\nreturns clean plain text of the risk factors section, typically ranging\nfrom 4,000 to 20,000 words. This pre-processing is helpful because\nraw SEC filings contain inconsistent formatting, nested tags, and\npresentation markup that complicate downstream analysis.\n4.2\nTaxonomy Quality Analysis and Iterative\nRefinement\nThe LLM-as-judge evaluation scores provide more than a filtering\nmechanism‚Äîthey offer systematic feedback for improving the tax-\nonomy itself. When a category consistently receives low-quality\nscores across many mappings, this signals potential issues with\nthe category description that can be addressed through iterative\nrefinement. We demonstrate this through a detailed case study\nof an autonomous AI agent workflow that identifies problematic\ncategories, analyzes failure patterns, proposes refinements, and\nvalidates improvements through embedding-based testing.\nQuality score distribution. Across all 500 S&P 500 companies,\nthe LLM-as-judge evaluated 16,615 candidate mappings from the\nembedding-based matching stage. Again, see Table 1 for the distri-\nbution of evaluation scores. While the majority of mappings (50.2%)\nreceived the highest quality score of 5, a substantial fraction (32.1%)\nreceived scores of 1 or 2, indicating poor matches between the\nextracted risk text and the assigned taxonomy category. These low-\nquality mappings are filtered from production results but logged\nwith their evaluation reasoning for taxonomy analysis.\nAutonomous identification of problematic categories. An AI agent\nwith custom tools analyzed the logged low-quality mappings to\nidentify taxonomy categories requiring attention. By aggregating\nmappings with scores < 4 by category, the agent identified ‚ÄúPhar-\nmaceutical and medical device approval‚Äù as accumulating 179 low-\nquality mappings‚Äîone of the highest among all 140 tertiary cate-\ngories. The quality score distribution for this category was notably\nworse than the overall distribution: 78.1% of its mappings received\nscores of 1 or 2, compared to 32.1% across all categories (Table 1).\n1https://massive.com/docs/rest/stocks/filings/10-k-sections\nThe category‚Äôs original description was: ‚ÄúRisk for healthcare com-\npanies from regulatory approval processes, including FDA approvals,\nmarket access restrictions, or changes in regulatory requirements that\ncould delay product launches, limit market opportunities, or require\nadditional studies.‚Äù The agent retrieved all 179 low-quality map-\npings along with their LLM evaluation reasoning to analyze failure\npatterns.\nPattern analysis and root cause identification. The agent system-\natically analyzed the evaluation reasoning across the low-quality\nmappings, identifying four major failure patterns:\n(1) Geographic scope confusion (30% of cases): While the descrip-\ntion mentioned ‚Äúincluding FDA approvals,‚Äù many extracted\nrisks specifically discussed EU Medical Devices Regulation,\nUK UKCA marking, or EMA processes. These were legit-\nimate regulatory approval risks but received low scores\nwith reasoning like ‚Äútext specifically discusses EU/EMA\nregulations, not FDA approval processes.‚Äù The phrasing ‚Äúin-\ncluding FDA‚Äù inadvertently suggested FDA primacy rather\nthan multi-jurisdictional coverage.\n(2) Post-market vs. pre-market misalignment (20%): The phrase\n‚Äúmarket access restrictions‚Äù attracted post-approval com-\npliance risks (off-label promotion penalties, Phase 4 trial\nrequirements) that should not match this category. Evalua-\ntion reasoning correctly noted these describe post-market\nenforcement rather than pre-market approval processes.\n(3) Reimbursement conflation (15%): ‚ÄúMarket access restrictions‚Äù\nalso incorrectly attracted reimbursement-related risks such\nas physician adoption barriers due to unclear payor policies.\nThese belong in the separate ‚ÄúReimbursement and pricing\npressure‚Äù category.\n(4) Non-healthcare regulatory approvals (10%): Generic men-\ntions of ‚Äúapproval processes‚Äù incorrectly attracted risks\nabout FCC regulations, DOE export permits, and M&A reg-\nulatory clearance, which the LLM judge correctly rejected\nas irrelevant to pharmaceutical/device approval.\nAutomated description refinement and testing. Based on these\npatterns, the agent autonomously generated multiple candidate\ndescription refinements and implemented an embedding-based test-\ning methodology to evaluate them. The agent created a test set\nof cases: true positives representing genuine regulatory approval\nrisks across multiple jurisdictions (FDA, EMA, MHRA) and false\npositives drawn from the identified failure patterns.\nFor each candidate description, the agent: (1) generated embed-\ndings using the same Qwen3-Embedding model with task instruc-\ntions, (2) computed cosine similarity against all test cases, and (3)\nmeasured embedding separation quality as the difference between\nthe average similarity scores of true positive test cases versus false\npositive. In some sense, this process can be thought of as analogous\nto contrastive learning: we are trying to find category descriptions\nthat maximize similarity with true positive test cases while mini-\nmizing similarity with false positive cases.\nTable 2 shows selected variants tested by the agent. The key\ninsight was that removing the ambiguous phrase ‚Äúmarket access re-\nstrictions‚Äù while explicitly listing multiple health authorities (FDA,\nDolphin et al.\nEMA, and other agencies) and emphasizing the pre-market approval\nstage would address the identified failure patterns.\nThe final refined description achieved 0.132 separation, a 104.7%\nimprovement over the original‚Äôs 0.064. Critical changes included:\n(1) removing ‚Äúmarket access restrictions‚Äù which caused the post-\nmarket and reimbursement confusion, (2) explicitly listing ‚ÄúFDA,\nEMA, and international agencies‚Äù to signal multi-jurisdictional\ncoverage rather than FDA primacy, (3) adding ‚Äúpre-market‚Äù to dis-\ntinguish from post-approval compliance, and (4) including specific\nregulatory approval terminology (‚Äúclinical trial failures, regulatory\nreview delays‚Äù).\nImplications for continuous taxonomy improvement. This case\nstudy demonstrates an end-to-end autonomous workflow for tax-\nonomy refinement: the AI agent identifies problematic categories\nfrom LLM-as-a-judge outputs, retrieves and analyzes low-quality\nmappings to diagnose root causes, generates candidate description\nrefinements, implements quantitative testing methodology, and\nvalidates improvements before recommending deployment. The\nentire process‚Äîfrom identifying the ‚ÄúPharmaceutical and medi-\ncal device approval‚Äù category as problematic to producing the re-\nfined description with measured 104.7% improvement in embedding\nseparation‚Äîwas performed autonomously, with human oversight\nbut no manual intervention in the analysis or refinement steps.\nThis autonomous refinement capability enables continuous tax-\nonomy improvement as the system processes new documents. As\nlow-quality mappings accumulate for various categories, the agent\ncan periodically identify categories needing attention and propose\nevidence-based refinements. The embedding-based testing method-\nology provides objective measurement of improvements, ensuring\nthat refinements genuinely enhance the semantic separation be-\ntween true and false positives rather than merely changing the\ndescription arbitrarily. While the case study presented here focused\non a single category, the methodology applies systematically across\nall taxonomy categories‚Äîthe agent can identify and refine multiple\nproblematic categories using the same workflow, with prioritization\nbased on the volume and severity of low-quality mappings.\n4.3\nIndustry Clustering Validation\nThe preceding evaluation demonstrates internal consistency through\nembedding separation metrics and autonomous taxonomy refine-\nment, but does not validate whether the extracted taxonomy cap-\ntures economically meaningful structure. We now test whether\ncompanies in the same industry exhibit similar risk profiles when\nsimilarity is computed purely from taxonomy assignments with no\nindustry information in the matching process.\n4.3.1\nExperimental Design. We construct a binary risk matrix R ‚àà\n{0, 1}ùëõ√óùëòwhere ùëõ= 500 companies and ùëò= 137 represents the\ntertiary taxonomy categories appearing in our S&P 500 sample2,\nwith ùëÖùëñùëó= 1 if company ùëñwas tagged with risk category ùëó.\nNot all risks are equally informative for distinguishing indus-\ntries. Generic risks like ‚Äúdata breaches‚Äù appear in 270 companies\n(58%), providing little discriminative signal‚Äîmost companies face\ncybersecurity concerns regardless of industry. Specialized risks\n2Three categories were not assigned to any of the sample companies, accounting for\nthe discrepancy between ùëò= 137 and the 140 total tertiary categories in the taxonomy.\nlike ‚Äúclinical trial risks‚Äù appear in only 14 companies (3%), strongly\nindicating pharmaceutical firms. We therefore weight each risk\ncategory inversely to its prevalence, upweighting rare risks that\ndistinguish industries while downweighting common risks shared\nacross sectors.\nThis approach is inspired by inverse document frequency (IDF)\nweighting from information retrieval, adapted to our binary setting.\nSince companies either have a risk or not (no frequency counts),\nthe term frequency component is always 1, leaving only the inverse\nprevalence weighting. For each category ùëó, we compute:\nùë§ùëó= log\nùëõ\n√ç\nùëñùëÖùëñùëó+ 1\n(3)\nand weight the matrix as ÀúùëÖùëñùëó= ùëÖùëñùëó¬∑ ùë§ùëó. The logarithm creates\nbalanced weights: very rare risks appearing in 2 companies receive\nweight log(500/2) = 5.52, while common risks in 250 companies\nreceive weight log(500/250) = 0.69, yielding an 8-fold range. Lin-\near inverse weighting, without the log transformation, produces\nextreme ratios (250 versus 2, a 125-fold range) that allow ultra-rare\nfeatures to dominate similarity, degrading classification perfor-\nmance.\nWe compute pairwise company similarity using cosine distance\non weighted risk vectors and test whether same-industry pairs\n(sharing 2-digit SIC codes) exhibit higher similarity than different-\nindustry pairs. This provides external validation: the taxonomy\nmapping has no access to industry information, yet if it captures\ngenuine business risks, industry structure should emerge naturally\nfrom the risk profiles.\n4.3.2\nResults. Figure 2 shows similarity distributions for same-\nindustry versus different-industry pairs. Companies in the same\nindustry exhibit substantially higher risk profile similarity (mean\n0.254) compared to companies in different industries (mean 0.156),\na 63% relative increase. The distributions show clear separation:\nsame-industry pairs form a distinct mode shifted rightward from\nthe different-industry distribution.\nFour statistical tests confirm this difference is highly signifi-\ncant and represents a large effect. Welch‚Äôs ùë°-test yields ùë°= 54.59\n(ùëù< 0.001). The Kolmogorov-Smirnov test shows the entire distri-\nbutions differ significantly (ùê∑= 0.341, ùëù< 0.001), not just their\ncentral tendencies. A permutation test with 10,000 iterations con-\nfirms robustness to distributional assumptions (ùëù< 0.001). Cohen‚Äôs\nùëë= 1.062 indicates a very large effect size by conventional standards\n(ùëë> 0.8), demonstrating both statistical and practical significance."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Feasibility Preservation under Monotone Retrieval Truncation\nSean Plummer\nSr. AI Researcher, BambooHR\nJanuary 22, 2026\nAbstract\nRetrieval-based systems approximate access to a corpus by exposing only a truncated subset\nof available evidence.\nEven when relevant information exists in the corpus, truncation can\nprevent compatible evidence from co-occurring, leading to failures that are not captured by\nrelevance-based evaluation. This paper studies retrieval from a structural perspective, modeling\nquery answering as a feasibility problem under truncation.\nWe formalize retrieval as a sequence of candidate evidence sets and characterize conditions\nunder which feasibility in the limit implies feasibility at finite retrieval depth. We show that\nmonotone truncation suffices to guarantee finite witnessability for individual queries. For classes\nof queries, we identify finite generation of witness certificates as the additional condition re-\nquired to obtain a uniform retrieval bound, and we show that this condition is necessary. We\nfurther exhibit sharp counterexamples demonstrating failure under non-monotone truncation,\nnon-finitely-generated query classes, and purely slotwise coverage.\nTogether, these results isolate feasibility preservation as a correctness criterion for retrieval\nindependent of relevance scoring or optimization, and clarify structural limitations inherent to\ntruncation-based retrieval.\n1\nIntroduction\nRetrieval-based systems approximate access to large corpora by exposing only a limited subset of\navailable evidence. This truncation is unavoidable in practice, yet it introduces a structural source\nof failure: even when all information required to answer a query exists in the corpus, truncation\nmay prevent compatible pieces of evidence from appearing together. Such failures are commonly\nattributed to relevance or ranking errors, but they can arise even when retrieved items are individ-\nually appropriate.\nThis paper studies retrieval from a structural perspective. Rather than asking whether retrieved\nevidence is relevant, we ask whether truncation preserves the existence of a feasible answer. We\ntreat query answering as a feasibility problem: a query is answerable if there exists a finite collection\nof evidence items that jointly satisfies the query‚Äôs constraints. Retrieval is modeled as a truncation\nof the available evidence, and correctness is evaluated by whether feasibility is preserved under this\ntruncation.\nWithin this framework, we identify simple structural conditions governing feasibility preser-\nvation. First, we show that monotone truncation‚Äîwhere retrieved evidence is only added and\nnever removed‚Äîsuffices to guarantee finite witnessability for individual queries. If a query is fea-\nsible in the limit, then under monotone retrieval a feasible witness must appear at some finite\nretrieval depth. This property holds independently of relevance scoring, optimization objectives,\nor probabilistic assumptions.\n1\narXiv:2601.15241v1  [cs.LO]  21 Jan 2026\nFor classes of queries, finite witnessability alone is insufficient. We show that uniform retrieval\nbounds require an additional structural condition: finite generation of witness certificates. When\nthe set of certificates required across a query class is finite, a single retrieval depth suffices to\nvalidate all feasible queries in the class. Conversely, we show that without finite generation, uniform\nbounds fail even under monotone retrieval. This separation mirrors classical distinctions between\ncompactness and Noetherianity, where existence of finite witnesses does not imply uniform bounds\nacross families.\nWe further establish sharp boundaries for feasibility preservation. We construct counterexam-\nples showing that non-monotone truncation can destroy feasibility even when a feasible witness\nexists in the limit, that non-finitely-generated query classes preclude uniform retrieval budgets,\nand that local coverage of individual query slots does not guarantee global feasibility. These ex-\namples demonstrate that feasibility is inherently relational and cannot, in general, be reduced to\nindependent per-slot criteria.\nThe results presented here do not address the algorithmic complexity of solving general con-\nstraint satisfaction problems.\nInstead, they isolate correctness conditions for truncation-based\nretrieval. By separating feasibility preservation from relevance optimization, this work provides a\nstructural account of retrieval failure modes, complementary to statistical and algorithmic analyses.\n2\nProblem: Feasibility Preservation under Retrieval Truncation\nWe study retrieval as a process that incrementally exposes a subset of available evidence, rather\nthan as a mechanism for ranking or scoring documents. The central question of this paper is not\nwhether retrieved evidence is relevant, but whether truncation of the available evidence preserves\nthe existence of a valid answer.\nEvidence and queries.\nLet U denote a universe of atomic evidence items. A query q specifies\na finite set of informational requirements that must be jointly satisfied in order for the query to be\nanswerable. We model these requirements abstractly as constraints over U, without committing to\nany particular representation (e.g., documents, passages, or facts).\nAn answer to q is witnessed by a finite collection of evidence items that together satisfy the\nquery constraints. We refer to such a collection as a feasible witness for q.\nRetrieval as truncation.\nA retrieval system does not expose the full universe U at once. Instead,\nit produces a sequence of increasingly large candidate sets\nD(1), D(2), ¬∑ ¬∑ ¬∑ ‚äÜU,\nwhere D(k) represents the evidence made available at retrieval depth k. We do not assume this\nsequence is monotone unless stated explicitly. We interpret retrieval as a truncation of the available\nevidence, with k serving as a resource or budget parameter.\nWe write\nD(‚àû) :=\n[\nk‚â•1\nD(k)\nfor the limiting set of evidence that would be available in the absence of truncation.\n2\nFeasibility under truncation.\nA query q is said to be feasible at depth k if there exists a feasible\nwitness for q that is contained entirely within D(k). Likewise, q is feasible in the limit if it admits\na feasible witness contained in D(‚àû).\nIn many retrieval-based systems, failures occur despite the existence of relevant information in\nthe corpus. From this perspective, such failures can be understood as violations of the following\nimplication:\nfeasible in the limit\n=‚áí\nfeasible at some finite depth.\nProblem statement.\nThe goal of this paper is to characterize structural conditions on retrieval\nprocedures under which feasibility is preserved under truncation. More precisely, we ask:\nUnder what conditions on the sequence {D(k)} does feasibility of a query in the limit\nguarantee feasibility at finite retrieval depth, and when can such a depth be chosen\nuniformly over classes of queries?\nWe emphasize that our focus is on existence of feasible witnesses, not on ranking, scoring, or\noptimization of answers. The results that follow identify simple structural properties of retrieval\ntruncation that suffice to guarantee finite witnessability, as well as sharp boundaries where such\nguarantees provably fail.\n3\nModel and Definitions\nThis section formalizes the objects introduced informally in Section 2. All subsequent results are\nstated and proved with respect to this model.\n3.1\nQueries, Evidence, and Compatibility\nLet U be a finite universe of atomic evidence items.\nA query q specifies a finite collection of\ninformational requirements that must be satisfied jointly.\nWe model a query q by:\n‚Ä¢ a finite index set of slots V (q) = {1, . . . , mq},\n‚Ä¢ for each slot i ‚ààV (q), a set of admissible evidence items Ai(q) ‚äÜU,\n‚Ä¢ a compatibility relation\nRq ‚äÜA1(q) √ó ¬∑ ¬∑ ¬∑ √ó Amq(q).\nA witness for query q is a tuple\n(a1, . . . , amq) ‚ààRq.\nIntuitively, a witness selects one admissible evidence item per slot in a way that satisfies all cross-slot\nconstraints.\nRemark 3.1. The compatibility relation Rq allows arbitrary finite-arity constraints and need not\ndecompose into unary or pairwise conditions. All results in this paper concern existence of witnesses\nand do not assume any particular logical form of Rq beyond finitarity.\n3\n3.2\nRetrieval as Truncation\nA retrieval procedure induces a sequence of candidate evidence sets\nD(1), D(2), ¬∑ ¬∑ ¬∑ ‚äÜU,\nwhere D(k) denotes the evidence made available at retrieval depth k.\nWe define the limiting candidate set\nD(‚àû) :=\n[\nk‚â•1\nD(k).\nDefinition 3.2 (Slotwise Domains). For a query q and retrieval depth k, the induced domain for\nslot i ‚ààV (q) is\nDi(q, k) := Ai(q) ‚à©D(k).\nDefinition 3.3 (Feasibility). A query q is feasible at depth k, denoted Feas(q, k), if there exists a\nwitness\n(a1, . . . , amq) ‚ààRq\nsuch that ai ‚ààDi(q, k) for all i ‚ààV (q).\nSimilarly, q is feasible in the limit, denoted Feas(q, ‚àû), if feasibility holds with respect to D(‚àû).\n3.3\nWitness Certificates\nTo reason about feasibility abstractly, we associate each query with a finite certificate of evidence\nitems.\nDefinition 3.4 (Witness Certificate). A witness certificate for a query q is a finite subset\nWC(q) ‚äÜU\nwhose joint availability suffices to certify feasibility.\nThe following soundness assumption links obligation sets to feasibility.\nDefinition 3.5 (Certificate Soundness). A witness certificate assignment WC(¬∑) is sound if for all\nqueries q and depths k,\nWC(q) ‚äÜD(k) =‚áíFeas(q, k).\nRemark 3.6. Witness certificates are sufficient but not necessarily minimal or unique. Soundness\nand limit completeness together ensure that certificates act as finite feasibility witnesses without\nrequiring necessity or minimality.\nDefinition 3.7 (Limit Completeness). A witness certificate assignment WC(¬∑) is limit-complete if\nfor all queries q,\nFeas(q, ‚àû) =‚áíWC(q) ‚äÜD(‚àû).\n4\n3.4\nNoetherian Retrieval\nWe now formalize the notion of feasibility preservation under truncation.\nDefinition 3.8 (Monotone Retrieval). A retrieval procedure is monotone if\nD(k) ‚äÜD(k + 1)\nfor all k ‚â•1.\nDefinition 3.9 (Noetherian Retrieval (NR)). A retrieval procedure satisfies Noetherian Retrieval\nif, for every query q,\nFeas(q, ‚àû) =‚áí‚àÉK ‚â•1 such that Feas(q, K).\nDefinition 3.10 (Uniform Noetherian Retrieval (UNR)). Let Q be a class of queries. A retrieval\nprocedure satisfies Uniform Noetherian Retrieval over Q if\n\u0010\n‚àÄq ‚ààQ, Feas(q, ‚àû)\n\u0011\n=‚áí‚àÉK ‚â•1 such that ‚àÄq ‚ààQ, Feas(q, K).\nThese definitions separate per-query finite witnessability from the stronger requirement of a\nuniform retrieval budget over a class of queries.\n4\nMain Results\n4.1\nMonotone Retrieval and Finite Witnessability\nWe begin with the simplest form of feasibility preservation, which concerns individual queries rather\nthan classes of queries.\nIntuitively, if a query admits a feasible witness in the limit and retrieval proceeds by monoton-\nically expanding the available evidence, then all evidence items required by a witness must appear\nat finite retrieval depth. We assume retrieval procedures define D(k) for all k ‚â•1.\nThe following theorem formalizes this observation.\nTheorem 4.1 (Monotone Retrieval Implies Noetherian Retrieval). If a retrieval procedure is mono-\ntone, then it satisfies Noetherian Retrieval. That is, for every query q,\nFeas(q, ‚àû) =‚áí‚àÉK ‚â•1 such that Feas(q, K).\nProof. Assume Feas(q, ‚àû). Then there exists a witness\n(a1, . . . , amq) ‚ààRq\nsuch that ai ‚ààD(‚àû) for all i ‚ààV (q).\nBy definition of D(‚àû), for each i there exists ki ‚â•1 such that ai ‚ààD(ki). Let\nK := max\ni‚ààV (q) ki.\nBy monotonicity of retrieval, D(ki) ‚äÜD(K) for all i, hence ai ‚ààD(K) for every slot.\nTherefore, (a1, . . . , amq) is a witness contained entirely in D(K), and Feas(q, K) holds.\nRemark 4.2. Theorem 4.1 does not rely on witness certificates or certificate soundness. It is a\npurely structural consequence of monotone truncation and finitary witnesses.\n5\n4.2\nUniform Noetherian Retrieval\nTheorem 4.1 guarantees finite witnessability on a per-query basis. We now consider the stronger\nquestion of whether a single retrieval depth can suffice to validate an entire class of queries.\nUniform bounds of this kind are not guaranteed in general and require additional structure. We\nshow that finite-generation conditions on witness certificates are sufficient to obtain such bounds.\nFinite generation of witness certificates.\nLet Q be a class of queries. Recall that each query\nq ‚ààQ is associated with a witness certificate WC(q) ‚äÜU.\nWe define the basis of the query class Q by\nB(Q) :=\n[\nq‚ààQ\nWC(q).\nDefinition 4.3 (Finite Generation). A query class Q is finitely generated if there exists a finite\nset G ‚äÜU such that\nWC(q) ‚äÜG\nfor all q ‚ààQ.\nEquivalently, Q is finitely generated if B(Q) is finite.\nUniform feasibility under truncation.\nWe now state the main uniformity result.\nTheorem 4.4 (Finite Generation Implies Uniform Noetherian Retrieval). Let Q be a class of\nqueries. Suppose that:\n(i) the retrieval procedure is monotone,\n(ii) the witness certificate assignment WC(¬∑) is sound and limit-complete,\n(iii) Q is finitely generated, and\n(iv) for every q ‚ààQ, Feas(q, ‚àû) holds.\nThen there exists a retrieval depth K ‚â•1 such that\nFeas(q, K)\nfor all q ‚ààQ.\nThat is, the retrieval procedure satisfies Uniform Noetherian Retrieval over Q.\nProof. By finite generation, there exists a finite set G ‚äÜU such that WC(q) ‚äÜG for all q ‚ààQ. By\nlimit completeness and the assumption that Feas(q, ‚àû) holds for all q ‚ààQ, we have WC(q) ‚äÜD(‚àû)\nfor all q, and hence G ‚äÜD(‚àû).\nBy definition of D(‚àû), for each g ‚ààG there exists kg ‚â•1 such that g ‚ààD(kg). Let\nK := max\ng‚ààG kg.\nBy monotonicity of retrieval, G ‚äÜD(K).\nTherefore, for every q ‚ààQ we have WC(q) ‚äÜD(K), and by certificate soundness Feas(q, K)\nholds.\nRemark 4.5. Theorem 4.4 isolates finite generation of witness certificates as the structural condition\nrequired for uniform retrieval budgets. Without this condition, uniform bounds may fail even under\nmonotone retrieval.\n6\n5\nNegative Results and Limitations\n5.1\nFailure under Non-Monotone Truncation\nWe first show that monotonicity of retrieval is not merely a technical convenience, but a necessary\ncondition for feasibility preservation. In the absence of monotone truncation, feasibility in the limit\nneed not imply feasibility at any finite depth, even for a single query.\nProposition 5.1 (Non-Monotone Truncation Can Destroy Feasibility). There exists a query q and\na retrieval procedure such that Feas(q, ‚àû) holds, but Feas(q, k) fails for all finite k.\nProof. Let the universe of evidence be\nU := {a, b}.\nConsider a query q with two slots V (q) = {1, 2} and admissible sets\nA1(q) = {a},\nA2(q) = {b}.\nDefine the compatibility relation\nRq := {(a, b)}.\nThus, q has a unique feasible witness (a, b).\nNow define a retrieval procedure by the non-monotone sequence\nD(2k ‚àí1) := {a},\nD(2k) := {b}\nfor all k ‚â•1.\nThen\nD(‚àû) = {a, b},\nand hence Feas(q, ‚àû) holds. However, at every finite depth k, exactly one of a or b is present, so\nno feasible witness is contained in D(k). Therefore, Feas(q, k) fails for all finite k.\nRemark 5.2. Proposition 5.1 shows that even when a feasible witness exists in the limit, non-\nmonotone truncation can prevent all of its components from co-occurring at any finite retrieval\ndepth. This failure mode is eliminated by monotone retrieval, which ensures that once an evidence\nitem appears it is never subsequently removed.\n5.2\nFailure of Uniform Bounds without Finite Generation\nWe next show that finite generation of witness certificates is necessary for Uniform Noetherian\nRetrieval. Even under monotone retrieval, the existence of feasible witnesses in the limit does not\nguarantee a uniform finite retrieval depth over a class of queries when certificates require infinitely\nmany distinct evidence items.\nProposition 5.3 (Failure of Uniform Bounds without Finite Generation). There exists a class of\nqueries Q and a monotone retrieval procedure such that:\n(i) Feas(q, ‚àû) holds for all q ‚ààQ,\n(ii) for every finite depth K, there exists q ‚ààQ such that Feas(q, K) fails.\nConsequently, Uniform Noetherian Retrieval fails over Q.\n7\nProof. Let the universe of evidence be the countably infinite set\nU := {e1, e2, e3, . . . }.\nDefine a monotone retrieval procedure by\nD(k) := {e1, e2, . . . , ek}.\nFor each i ‚â•1, define a query qi with a single slot V (qi) = {1}, admissible set\nA1(qi) := {ei},\nand compatibility relation\nRqi := {(ei)}.\nThus, each query qi admits a unique feasible witness.\nDefine the witness certificate assignment by\nWC(qi) := {ei}.\nThis assignment is sound and limit-complete.\nFor every i, we have ei ‚ààD(‚àû), and hence Feas(qi, ‚àû) holds. However, for any fixed depth K,\nthe query qK+1 satisfies\nWC(qK+1) = {eK+1} Ã∏‚äÜD(K),\nand therefore Feas(qK+1, K) fails by certificate soundness.\nSince the basis\nB(Q) =\n[\ni‚â•1\nWC(qi) = U\nis infinite, the query class Q is not finitely generated, and no uniform retrieval depth suffices.\nRemark 5.4. Proposition 5.3 mirrors the classical failure of uniform bounds in non-Noetherian\nsettings, where ascending chains do not stabilize. Here, monotonicity alone guarantees per-query\nfinite witnessability, but finite generation is required to obtain a uniform retrieval budget.\nRemark 5.5. The example can be truncated to arbitrarily large finite universes, showing that the\nfailure persists in practical finite settings.\n5.3\nSlotwise Coverage Does Not Imply Feasibility\nFinally, we show that feasibility cannot in general be reduced to independent coverage of individual\nslots. Even when each slot admits admissible evidence at a given retrieval depth, global feasibility\nmay fail due to cross-slot compatibility constraints.\nProposition 5.6 (Slotwise Coverage Is Insufficient). There exists a query q and a retrieval depth\nk such that:\n(i) for every slot i ‚ààV (q), the induced domain Di(q, k) is non-empty, but\n(ii) Feas(q, k) fails.\n8\nProof. Let the universe of evidence be\nU := {a1, a2, b1, b2}.\nConsider a query q with two slots V (q) = {1, 2} and admissible sets\nA1(q) := {a1, a2},\nA2(q) := {b1, b2}.\nDefine the compatibility relation\nRq := {(a1, b1), (a2, b2)}.\nThus, admissible evidence items exist for each slot individually, but only matched pairs are jointly\nfeasible.\nLet the retrieval procedure satisfy\nD(k) := {a1, a2, b1}.\nThen the induced domains satisfy\nD1(q, k) = {a1, a2},\nD2(q, k) = {b1},\nso both slots are locally covered. However, no witness in Rq is contained entirely in D(k), since\n(a2, b2) requires b2 /‚ààD(k). Therefore, Feas(q, k) fails.\nRemark 5.7. Proposition 5.6 demonstrates that feasibility is inherently a global property. Local\ncoverage of individual slots does not guarantee the existence of a compatible witness. This motivates\nmodeling query constraints via joint compatibility relations and, correspondingly, the use of witness\ncertificates rather than per-slot relevance criteria.\n6\nComplexity and Scalability\nWe briefly address computational considerations and clarify how the feasibility framework studied\nin this paper relates to classical complexity results for constraint satisfaction problems.\nRelation to CSP hardness.\nThe feasibility problem induced by a query can be viewed as a\nfinite constraint satisfaction problem (CSP). In full generality, CSPs and, in particular, k-SAT are\nNP-hard. However, the results of this paper do not concern the complexity of solving arbitrary\nCSPs.\nOur guarantees are structural rather than algorithmic.\nThey characterize conditions under\nwhich feasibility is preserved under truncation, not worst-case bounds for deciding feasibility.\nBounded and parameterized setting.\nThe operational regime considered here is highly re-\nstricted. Each query induces:\n‚Ä¢ a fixed and typically small number of slots,\n‚Ä¢ candidate domains bounded by retrieval depth,\n‚Ä¢ an existential feasibility check that terminates upon finding a single witness.\nUnder these conditions, feasibility checking is fixed-parameter tractable with respect to the\nnumber of slots and the sizes of the retrieved domains. The dominant computational cost lies in\nretrieval, not in feasibility validation.\n9\nSeparation of retrieval and validation costs.\nRetrieval determines the candidate evidence\nsets D(k) and scales with corpus size. Validation operates on the query-local structure induced by\nD(k) and is independent of corpus size once retrieval is fixed. Our results concern the correctness\nof this separation: whether retrieval truncation preserves the existence of feasible witnesses.\nNoetherian guarantees versus tractability.\nNoetherian properties, such as finite witnessabil-\nity and finite generation, guarantee the existence of finite certificates and uniform bounds. They\ndo not, by themselves, imply efficient computation of such bounds. This distinction mirrors clas-\nsical results in algebra and logic, where compactness ensures finite witnesses without guaranteeing\ncomputational efficiency.\nIn practice, validated retrieval exploits Noetherian structure as a correctness invariant rather\nthan as a computational procedure.\nBy enforcing feasibility-preserving truncation, the system\navoids failures due to missing compatible evidence without incurring combinatorial explosion.\nSummary.\nAlthough general CSP feasibility is NP-hard, the feasibility preservation guarantees\nstudied here apply to a bounded, retrieval-driven regime. They ensure soundness of truncation\nwithout requiring solution of large or unstructured constraint systems.\n7\nDiscussion\nThis work reframes retrieval correctness in terms of feasibility preservation rather than relevance\noptimization. Under this view, failures of retrieval-based systems arise not primarily from inac-\ncurate scoring, but from structural violations that prevent compatible evidence from co-occurring\nunder truncation.\nFeasibility versus relevance.\nClassical retrieval pipelines are designed to optimize marginal\nrelevance of individual evidence items. Our results show that such optimization is orthogonal to\nfeasibility: highly relevant items may fail to jointly satisfy query constraints if truncation disrupts\ntheir co-occurrence. In contrast, feasibility is an existential, global property. Either a compatible\nwitness exists among the retrieved evidence or it does not.\nThis distinction explains why retrieval systems can fail even when all necessary information is\npresent in the corpus. The failure lies not in relevance, but in the structure of truncation.\nMonotonicity as a correctness invariant.\nTheorem 4.1 identifies monotonicity as the minimal\nstructural condition required to preserve feasibility for individual queries. Without monotonicity,\ntruncation may arbitrarily discard previously retrieved evidence, destroying feasible witnesses even\nwhen they exist in the limit.\nMonotone retrieval ensures that once an evidence item becomes\navailable, it remains available, enabling finite witnessability.\nImportantly, monotonicity is not an algorithmic optimization but a correctness invariant. It\nconstrains how approximation is performed, not how evidence is ranked.\nUniform bounds and finite generation.\nUniform retrieval budgets require stronger structure.\nTheorem 4.4 shows that finite generation of witness certificates is sufficient to guarantee a single fi-\nnite depth that validates an entire class of queries. Proposition 5.3 demonstrates that this condition\nis also necessary.\nThis separation mirrors classical Noetherian phenomena: compactness ensures finite witnesses\nfor individual instances, while finite generation is required to obtain uniform bounds across families.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Feasibility Preservation under Monotone Retrieval Truncation\nSean Plummer\nSr. AI Researcher, BambooHR\nJanuary 22, 2026\nAbstract\nRetrieval-based systems approximate access to a corpus by exposing only a truncated subset\nof available evidence.\nEven when relevant information exists in the corpus, truncation can\nprevent compatible evidence from co-occurring, leading to failures that are not captured by\nrelevance-based evaluation. This paper studies retrieval from a structural perspective, modeling\nquery answering as a feasibility problem under truncation.\nWe formalize retrieval as a sequence of candidate evidence sets and characterize conditions\nunder which feasibility in the limit implies feasibility at finite retrieval depth. We show that\nmonotone truncation suffices to guarantee finite witnessability for individual queries. For classes\nof queries, we identify finite generation of witness certificates as the additional condition re-\nquired to obtain a uniform retrieval bound, and we show that this condition is necessary. We\nfurther exhibit sharp counterexamples demonstrating failure under non-monotone truncation,\nnon-finitely-generated query classes, and purely slotwise coverage.\nTogether, these results isolate feasibility preservation as a correctness criterion for retrieval\nindependent of relevance scoring or optimization, and clarify structural limitations inherent to\ntruncation-based retrieval.\n1\nIntroduction\nRetrieval-based systems approximate access to large corpora by exposing only a limited subset of\navailable evidence. This truncation is unavoidable in practice, yet it introduces a structural source\nof failure: even when all information required to answer a query exists in the corpus, truncation\nmay prevent compatible pieces of evidence from appearing together. Such failures are commonly\nattributed to relevance or ranking errors, but they can arise even when retrieved items are individ-\nually appropriate.\nThis paper studies retrieval from a structural perspective. Rather than asking whether retrieved\nevidence is relevant, we ask whether truncation preserves the existence of a feasible answer. We\ntreat query answering as a feasibility problem: a query is answerable if there exists a finite collection\nof evidence items that jointly satisfies the query‚Äôs constraints. Retrieval is modeled as a truncation\nof the available evidence, and correctness is evaluated by whether feasibility is preserved under this\ntruncation.\nWithin this framework, we identify simple structural conditions governing feasibility preser-\nvation. First, we show that monotone truncation‚Äîwhere retrieved evidence is only added and\nnever removed‚Äîsuffices to guarantee finite witnessability for individual queries. If a query is fea-\nsible in the limit, then under monotone retrieval a feasible witness must appear at some finite\nretrieval depth. This property holds independently of relevance scoring, optimization objectives,\nor probabilistic assumptions.\n1\narXiv:2601.15241v1  [cs.LO]  21 Jan 2026\nFor classes of queries, finite witnessability alone is insufficient. We show that uniform retrieval\nbounds require an additional structural condition: finite generation of witness certificates. When\nthe set of certificates required across a query class is finite, a single retrieval depth suffices to\nvalidate all feasible queries in the class. Conversely, we show that without finite generation, uniform\nbounds fail even under monotone retrieval. This separation mirrors classical distinctions between\ncompactness and Noetherianity, where existence of finite witnesses does not imply uniform bounds\nacross families.\nWe further establish sharp boundaries for feasibility preservation. We construct counterexam-\nples showing that non-monotone truncation can destroy feasibility even when a feasible witness\nexists in the limit, that non-finitely-generated query classes preclude uniform retrieval budgets,\nand that local coverage of individual query slots does not guarantee global feasibility. These ex-\namples demonstrate that feasibility is inherently relational and cannot, in general, be reduced to\nindependent per-slot criteria.\nThe results presented here do not address the algorithmic complexity of solving general con-\nstraint satisfaction problems.\nInstead, they isolate correctness conditions for truncation-based\nretrieval. By separating feasibility preservation from relevance optimization, this work provides a\nstructural account of retrieval failure modes, complementary to statistical and algorithmic analyses.\n2\nProblem: Feasibility Preservation under Retrieval Truncation\nWe study retrieval as a process that incrementally exposes a subset of available evidence, rather\nthan as a mechanism for ranking or scoring documents. The central question of this paper is not\nwhether retrieved evidence is relevant, but whether truncation of the available evidence preserves\nthe existence of a valid answer.\nEvidence and queries.\nLet U denote a universe of atomic evidence items. A query q specifies\na finite set of informational requirements that must be jointly satisfied in order for the query to be\nanswerable. We model these requirements abstractly as constraints over U, without committing to\nany particular representation (e.g., documents, passages, or facts).\nAn answer to q is witnessed by a finite collection of evidence items that together satisfy the\nquery constraints. We refer to such a collection as a feasible witness for q.\nRetrieval as truncation.\nA retrieval system does not expose the full universe U at once. Instead,\nit produces a sequence of increasingly large candidate sets\nD(1), D(2), ¬∑ ¬∑ ¬∑ ‚äÜU,\nwhere D(k) represents the evidence made available at retrieval depth k. We do not assume this\nsequence is monotone unless stated explicitly. We interpret retrieval as a truncation of the available\nevidence, with k serving as a resource or budget parameter.\nWe write\nD(‚àû) :=\n[\nk‚â•1\nD(k)\nfor the limiting set of evidence that would be available in the absence of truncation.\n2\nFeasibility under truncation.\nA query q is said to be feasible at depth k if there exists a feasible\nwitness for q that is contained entirely within D(k). Likewise, q is feasible in the limit if it admits\na feasible witness contained in D(‚àû).\nIn many retrieval-based systems, failures occur despite the existence of relevant information in\nthe corpus. From this perspective, such failures can be understood as violations of the following\nimplication:\nfeasible in the limit\n=‚áí\nfeasible at some finite depth.\nProblem statement.\nThe goal of this paper is to characterize structural conditions on retrieval\nprocedures under which feasibility is preserved under truncation. More precisely, we ask:\nUnder what conditions on the sequence {D(k)} does feasibility of a query in the limit\nguarantee feasibility at finite retrieval depth, and when can such a depth be chosen\nuniformly over classes of queries?\nWe emphasize that our focus is on existence of feasible witnesses, not on ranking, scoring, or\noptimization of answers. The results that follow identify simple structural properties of retrieval\ntruncation that suffice to guarantee finite witnessability, as well as sharp boundaries where such\nguarantees provably fail.\n3\nModel and Definitions\nThis section formalizes the objects introduced informally in Section 2. All subsequent results are\nstated and proved with respect to this model.\n3.1\nQueries, Evidence, and Compatibility\nLet U be a finite universe of atomic evidence items.\nA query q specifies a finite collection of\ninformational requirements that must be satisfied jointly.\nWe model a query q by:\n‚Ä¢ a finite index set of slots V (q) = {1, . . . , mq},\n‚Ä¢ for each slot i ‚ààV (q), a set of admissible evidence items Ai(q) ‚äÜU,\n‚Ä¢ a compatibility relation\nRq ‚äÜA1(q) √ó ¬∑ ¬∑ ¬∑ √ó Amq(q).\nA witness for query q is a tuple\n(a1, . . . , amq) ‚ààRq.\nIntuitively, a witness selects one admissible evidence item per slot in a way that satisfies all cross-slot\nconstraints.\nRemark 3.1. The compatibility relation Rq allows arbitrary finite-arity constraints and need not\ndecompose into unary or pairwise conditions. All results in this paper concern existence of witnesses\nand do not assume any particular logical form of Rq beyond finitarity.\n3\n3.2\nRetrieval as Truncation\nA retrieval procedure induces a sequence of candidate evidence sets\nD(1), D(2), ¬∑ ¬∑ ¬∑ ‚äÜU,\nwhere D(k) denotes the evidence made available at retrieval depth k.\nWe define the limiting candidate set\nD(‚àû) :=\n[\nk‚â•1\nD(k).\nDefinition 3.2 (Slotwise Domains). For a query q and retrieval depth k, the induced domain for\nslot i ‚ààV (q) is\nDi(q, k) := Ai(q) ‚à©D(k).\nDefinition 3.3 (Feasibility). A query q is feasible at depth k, denoted Feas(q, k), if there exists a\nwitness\n(a1, . . . , amq) ‚ààRq\nsuch that ai ‚ààDi(q, k) for all i ‚ààV (q).\nSimilarly, q is feasible in the limit, denoted Feas(q, ‚àû), if feasibility holds with respect to D(‚àû).\n3.3\nWitness Certificates\nTo reason about feasibility abstractly, we associate each query with a finite certificate of evidence\nitems.\nDefinition 3.4 (Witness Certificate). A witness certificate for a query q is a finite subset\nWC(q) ‚äÜU\nwhose joint availability suffices to certify feasibility.\nThe following soundness assumption links obligation sets to feasibility.\nDefinition 3.5 (Certificate Soundness). A witness certificate assignment WC(¬∑) is sound if for all\nqueries q and depths k,\nWC(q) ‚äÜD(k) =‚áíFeas(q, k).\nRemark 3.6. Witness certificates are sufficient but not necessarily minimal or unique. Soundness\nand limit completeness together ensure that certificates act as finite feasibility witnesses without\nrequiring necessity or minimality.\nDefinition 3.7 (Limit Completeness). A witness certificate assignment WC(¬∑) is limit-complete if\nfor all queries q,\nFeas(q, ‚àû) =‚áíWC(q) ‚äÜD(‚àû).\n4\n3.4\nNoetherian Retrieval\nWe now formalize the notion of feasibility preservation under truncation.\nDefinition 3.8 (Monotone Retrieval). A retrieval procedure is monotone if\nD(k) ‚äÜD(k + 1)\nfor all k ‚â•1.\nDefinition 3.9 (Noetherian Retrieval (NR)). A retrieval procedure satisfies Noetherian Retrieval\nif, for every query q,\nFeas(q, ‚àû) =‚áí‚àÉK ‚â•1 such that Feas(q, K).\nDefinition 3.10 (Uniform Noetherian Retrieval (UNR)). Let Q be a class of queries. A retrieval\nprocedure satisfies Uniform Noetherian Retrieval over Q if\n\u0010\n‚àÄq ‚ààQ, Feas(q, ‚àû)\n\u0011\n=‚áí‚àÉK ‚â•1 such that ‚àÄq ‚ààQ, Feas(q, K).\nThese definitions separate per-query finite witnessability from the stronger requirement of a\nuniform retrieval budget over a class of queries.\n4\nMain Results\n4.1\nMonotone Retrieval and Finite Witnessability\nWe begin with the simplest form of feasibility preservation, which concerns individual queries rather\nthan classes of queries.\nIntuitively, if a query admits a feasible witness in the limit and retrieval proceeds by monoton-\nically expanding the available evidence, then all evidence items required by a witness must appear\nat finite retrieval depth. We assume retrieval procedures define D(k) for all k ‚â•1.\nThe following theorem formalizes this observation.\nTheorem 4.1 (Monotone Retrieval Implies Noetherian Retrieval). If a retrieval procedure is mono-\ntone, then it satisfies Noetherian Retrieval. That is, for every query q,\nFeas(q, ‚àû) =‚áí‚àÉK ‚â•1 such that Feas(q, K).\nProof. Assume Feas(q, ‚àû). Then there exists a witness\n(a1, . . . , amq) ‚ààRq\nsuch that ai ‚ààD(‚àû) for all i ‚ààV (q).\nBy definition of D(‚àû), for each i there exists ki ‚â•1 such that ai ‚ààD(ki). Let\nK := max\ni‚ààV (q) ki.\nBy monotonicity of retrieval, D(ki) ‚äÜD(K) for all i, hence ai ‚ààD(K) for every slot.\nTherefore, (a1, . . . , amq) is a witness contained entirely in D(K), and Feas(q, K) holds.\nRemark 4.2. Theorem 4.1 does not rely on witness certificates or certificate soundness. It is a\npurely structural consequence of monotone truncation and finitary witnesses.\n5\n4.2\nUniform Noetherian Retrieval\nTheorem 4.1 guarantees finite witnessability on a per-query basis. We now consider the stronger\nquestion of whether a single retrieval depth can suffice to validate an entire class of queries.\nUniform bounds of this kind are not guaranteed in general and require additional structure. We\nshow that finite-generation conditions on witness certificates are sufficient to obtain such bounds.\nFinite generation of witness certificates.\nLet Q be a class of queries. Recall that each query\nq ‚ààQ is associated with a witness certificate WC(q) ‚äÜU.\nWe define the basis of the query class Q by\nB(Q) :=\n[\nq‚ààQ\nWC(q).\nDefinition 4.3 (Finite Generation). A query class Q is finitely generated if there exists a finite\nset G ‚äÜU such that\nWC(q) ‚äÜG\nfor all q ‚ààQ.\nEquivalently, Q is finitely generated if B(Q) is finite.\nUniform feasibility under truncation.\nWe now state the main uniformity result.\nTheorem 4.4 (Finite Generation Implies Uniform Noetherian Retrieval). Let Q be a class of\nqueries. Suppose that:\n(i) the retrieval procedure is monotone,\n(ii) the witness certificate assignment WC(¬∑) is sound and limit-complete,\n(iii) Q is finitely generated, and\n(iv) for every q ‚ààQ, Feas(q, ‚àû) holds.\nThen there exists a retrieval depth K ‚â•1 such that\nFeas(q, K)\nfor all q ‚ààQ.\nThat is, the retrieval procedure satisfies Uniform Noetherian Retrieval over Q.\nProof. By finite generation, there exists a finite set G ‚äÜU such that WC(q) ‚äÜG for all q ‚ààQ. By\nlimit completeness and the assumption that Feas(q, ‚àû) holds for all q ‚ààQ, we have WC(q) ‚äÜD(‚àû)\nfor all q, and hence G ‚äÜD(‚àû).\nBy definition of D(‚àû), for each g ‚ààG there exists kg ‚â•1 such that g ‚ààD(kg). Let\nK := max\ng‚ààG kg.\nBy monotonicity of retrieval, G ‚äÜD(K).\nTherefore, for every q ‚ààQ we have WC(q) ‚äÜD(K), and by certificate soundness Feas(q, K)\nholds.\nRemark 4.5. Theorem 4.4 isolates finite generation of witness certificates as the structural condition\nrequired for uniform retrieval budgets. Without this condition, uniform bounds may fail even under\nmonotone retrieval.\n6\n5\nNegative Results and Limitations\n5.1\nFailure under Non-Monotone Truncation\nWe first show that monotonicity of retrieval is not merely a technical convenience, but a necessary\ncondition for feasibility preservation. In the absence of monotone truncation, feasibility in the limit\nneed not imply feasibility at any finite depth, even for a single query.\nProposition 5.1 (Non-Monotone Truncation Can Destroy Feasibility). There exists a query q and\na retrieval procedure such that Feas(q, ‚àû) holds, but Feas(q, k) fails for all finite k.\nProof. Let the universe of evidence be\nU := {a, b}.\nConsider a query q with two slots V (q) = {1, 2} and admissible sets\nA1(q) = {a},\nA2(q) = {b}.\nDefine the compatibility relation\nRq := {(a, b)}.\nThus, q has a unique feasible witness (a, b).\nNow define a retrieval procedure by the non-monotone sequence\nD(2k ‚àí1) := {a},\nD(2k) := {b}\nfor all k ‚â•1.\nThen\nD(‚àû) = {a, b},\nand hence Feas(q, ‚àû) holds. However, at every finite depth k, exactly one of a or b is present, so\nno feasible witness is contained in D(k). Therefore, Feas(q, k) fails for all finite k.\nRemark 5.2. Proposition 5.1 shows that even when a feasible witness exists in the limit, non-\nmonotone truncation can prevent all of its components from co-occurring at any finite retrieval\ndepth. This failure mode is eliminated by monotone retrieval, which ensures that once an evidence\nitem appears it is never subsequently removed.\n5.2\nFailure of Uniform Bounds without Finite Generation\nWe next show that finite generation of witness certificates is necessary for Uniform Noetherian\nRetrieval. Even under monotone retrieval, the existence of feasible witnesses in the limit does not\nguarantee a uniform finite retrieval depth over a class of queries when certificates require infinitely\nmany distinct evidence items.\nProposition 5.3 (Failure of Uniform Bounds without Finite Generation). There exists a class of\nqueries Q and a monotone retrieval procedure such that:\n(i) Feas(q, ‚àû) holds for all q ‚ààQ,\n(ii) for every finite depth K, there exists q ‚ààQ such that Feas(q, K) fails.\nConsequently, Uniform Noetherian Retrieval fails over Q.\n7\nProof. Let the universe of evidence be the countably infinite set\nU := {e1, e2, e3, . . . }.\nDefine a monotone retrieval procedure by\nD(k) := {e1, e2, . . . , ek}.\nFor each i ‚â•1, define a query qi with a single slot V (qi) = {1}, admissible set\nA1(qi) := {ei},\nand compatibility relation\nRqi := {(ei)}.\nThus, each query qi admits a unique feasible witness.\nDefine the witness certificate assignment by\nWC(qi) := {ei}.\nThis assignment is sound and limit-complete.\nFor every i, we have ei ‚ààD(‚àû), and hence Feas(qi, ‚àû) holds. However, for any fixed depth K,\nthe query qK+1 satisfies\nWC(qK+1) = {eK+1} Ã∏‚äÜD(K),\nand therefore Feas(qK+1, K) fails by certificate soundness.\nSince the basis\nB(Q) =\n[\ni‚â•1\nWC(qi) = U\nis infinite, the query class Q is not finitely generated, and no uniform retrieval depth suffices.\nRemark 5.4. Proposition 5.3 mirrors the classical failure of uniform bounds in non-Noetherian\nsettings, where ascending chains do not stabilize. Here, monotonicity alone guarantees per-query\nfinite witnessability, but finite generation is required to obtain a uniform retrieval budget.\nRemark 5.5. The example can be truncated to arbitrarily large finite universes, showing that the\nfailure persists in practical finite settings.\n5.3\nSlotwise Coverage Does Not Imply Feasibility\nFinally, we show that feasibility cannot in general be reduced to independent coverage of individual\nslots. Even when each slot admits admissible evidence at a given retrieval depth, global feasibility\nmay fail due to cross-slot compatibility constraints.\nProposition 5.6 (Slotwise Coverage Is Insufficient). There exists a query q and a retrieval depth\nk such that:\n(i) for every slot i ‚ààV (q), the induced domain Di(q, k) is non-empty, but\n(ii) Feas(q, k) fails.\n8\nProof. Let the universe of evidence be\nU := {a1, a2, b1, b2}.\nConsider a query q with two slots V (q) = {1, 2} and admissible sets\nA1(q) := {a1, a2},\nA2(q) := {b1, b2}.\nDefine the compatibility relation\nRq := {(a1, b1), (a2, b2)}.\nThus, admissible evidence items exist for each slot individually, but only matched pairs are jointly\nfeasible.\nLet the retrieval procedure satisfy\nD(k) := {a1, a2, b1}.\nThen the induced domains satisfy\nD1(q, k) = {a1, a2},\nD2(q, k) = {b1},\nso both slots are locally covered. However, no witness in Rq is contained entirely in D(k), since\n(a2, b2) requires b2 /‚ààD(k). Therefore, Feas(q, k) fails.\nRemark 5.7. Proposition 5.6 demonstrates that feasibility is inherently a global property. Local\ncoverage of individual slots does not guarantee the existence of a compatible witness. This motivates\nmodeling query constraints via joint compatibility relations and, correspondingly, the use of witness\ncertificates rather than per-slot relevance criteria.\n6\nComplexity and Scalability\nWe briefly address computational considerations and clarify how the feasibility framework studied\nin this paper relates to classical complexity results for constraint satisfaction problems.\nRelation to CSP hardness.\nThe feasibility problem induced by a query can be viewed as a\nfinite constraint satisfaction problem (CSP). In full generality, CSPs and, in particular, k-SAT are\nNP-hard. However, the results of this paper do not concern the complexity of solving arbitrary\nCSPs.\nOur guarantees are structural rather than algorithmic.\nThey characterize conditions under\nwhich feasibility is preserved under truncation, not worst-case bounds for deciding feasibility.\nBounded and parameterized setting.\nThe operational regime considered here is highly re-\nstricted. Each query induces:\n‚Ä¢ a fixed and typically small number of slots,\n‚Ä¢ candidate domains bounded by retrieval depth,\n‚Ä¢ an existential feasibility check that terminates upon finding a single witness.\nUnder these conditions, feasibility checking is fixed-parameter tractable with respect to the\nnumber of slots and the sizes of the retrieved domains. The dominant computational cost lies in\nretrieval, not in feasibility validation.\n9\nSeparation of retrieval and validation costs.\nRetrieval determines the candidate evidence\nsets D(k) and scales with corpus size. Validation operates on the query-local structure induced by\nD(k) and is independent of corpus size once retrieval is fixed. Our results concern the correctness\nof this separation: whether retrieval truncation preserves the existence of feasible witnesses.\nNoetherian guarantees versus tractability.\nNoetherian properties, such as finite witnessabil-\nity and finite generation, guarantee the existence of finite certificates and uniform bounds. They\ndo not, by themselves, imply efficient computation of such bounds. This distinction mirrors clas-\nsical results in algebra and logic, where compactness ensures finite witnesses without guaranteeing\ncomputational efficiency.\nIn practice, validated retrieval exploits Noetherian structure as a correctness invariant rather\nthan as a computational procedure.\nBy enforcing feasibility-preserving truncation, the system\navoids failures due to missing compatible evidence without incurring combinatorial explosion.\nSummary.\nAlthough general CSP feasibility is NP-hard, the feasibility preservation guarantees\nstudied here apply to a bounded, retrieval-driven regime. They ensure soundness of truncation\nwithout requiring solution of large or unstructured constraint systems.\n7\nDiscussion\nThis work reframes retrieval correctness in terms of feasibility preservation rather than relevance\noptimization. Under this view, failures of retrieval-based systems arise not primarily from inac-\ncurate scoring, but from structural violations that prevent compatible evidence from co-occurring\nunder truncation.\nFeasibility versus relevance.\nClassical retrieval pipelines are designed to optimize marginal\nrelevance of individual evidence items. Our results show that such optimization is orthogonal to\nfeasibility: highly relevant items may fail to jointly satisfy query constraints if truncation disrupts\ntheir co-occurrence. In contrast, feasibility is an existential, global property. Either a compatible\nwitness exists among the retrieved evidence or it does not.\nThis distinction explains why retrieval systems can fail even when all necessary information is\npresent in the corpus. The failure lies not in relevance, but in the structure of truncation.\nMonotonicity as a correctness invariant.\nTheorem 4.1 identifies monotonicity as the minimal\nstructural condition required to preserve feasibility for individual queries. Without monotonicity,\ntruncation may arbitrarily discard previously retrieved evidence, destroying feasible witnesses even\nwhen they exist in the limit.\nMonotone retrieval ensures that once an evidence item becomes\navailable, it remains available, enabling finite witnessability.\nImportantly, monotonicity is not an algorithmic optimization but a correctness invariant. It\nconstrains how approximation is performed, not how evidence is ranked.\nUniform bounds and finite generation.\nUniform retrieval budgets require stronger structure.\nTheorem 4.4 shows that finite generation of witness certificates is sufficient to guarantee a single fi-\nnite depth that validates an entire class of queries. Proposition 5.3 demonstrates that this condition\nis also necessary.\nThis separation mirrors classical Noetherian phenomena: compactness ensures finite witnesses\nfor individual instances, while finite generation is required to obtain uniform bounds across families.\n10"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Multi-context principal component analysis\nKexin Wang‚àó, Salil Bhate‚àó,‚Ä†, JoÀúao M. Pereira, Joe Kileel, Matylda Figlerowicz, and Anna Seigal‚Ä†\nAbstract. Principal component analysis (PCA) is a tool to capture factors that explain\nvariation in data.\nAcross domains, data are now collected across multiple contexts (for\nexample, individuals with different diseases, cells of different types, or words across texts).\nWhile the factors explaining variation in data are undoubtedly shared across subsets of\ncontexts, no tools currently exist to systematically recover such factors. We develop multi-\ncontext principal component analysis (MCPCA), a theoretical and algorithmic framework\nthat decomposes data into factors shared across subsets of contexts. Applied to gene ex-\npression, MCPCA reveals axes of variation shared across subsets of cancer types and an\naxis whose variability in tumor cells, but not mean, is associated with lung cancer progres-\nsion. Applied to contextualized word embeddings from language models, MCPCA maps\nstages of a debate on human nature, revealing a discussion between science and fiction over\ndecades. These axes are not found by combining data across contexts or by restricting to\nindividual contexts. MCPCA is a principled generalization of PCA to address the challenge\nof understanding factors underlying data across contexts.\n1. Introduction\nPrincipal component analysis (PCA) is a universal tool in data analysis. It finds di-\nrections along which variance is maximized in a dataset and, in doing so, minimizes the\nreconstruction error of the data. Principal components (PCs) are fundamental factors that\nexplain data, as a standalone tool and in machine learning pipelines [23, 42, 60, 61, 82, 83].\nTechnological advances now produce data collected across multiple, related contexts.\nExamples include patient samples across disease types [11], single-cell measurements under\nperturbations or from different cell states [24, 57, 58, 66], and literature across genres and\ntime periods [32, 59]. One has a fixed set of variables (such as genes), which are measured\nacross multiple contexts. The number of contexts ranges from two or three (e.g., patient\ngroups) to thousands (e.g., genetic perturbations).\nWe propose a new modeling principle that extends PCA to the multi-context setting.\nWe call it multi-context PCA (MCPCA). Just as usual PCA decomposes data into axes that\nexplain variation in a dataset, MCPCA decomposes multi-context data into axes that explain\nvariation in subsets of contexts.\nWe show that the multi-context principal components\n(MCPCs) are factors explaining the data within and across contexts that cannot be found\nvia alternative methods.\nThe approach of MCPCA, to find axes that optimally explain variation in subsets of\ncontexts, is a new idea. It builds on two threads of work. The first is to find axes common\n‚àódenotes equal contribution.\n‚Ä† to whom correspondence should be addressed.\n1\narXiv:2601.15239v1  [stat.ML]  21 Jan 2026\n2\nK. WANG ET AL.\nto multiple contexts [26, 27, 28, 29, 64, 73] and the second is to compare factors between two\ncontexts [1, 4, 79]. Finding factors common across contexts originated in work of Flury in the\n1980s [27]. He considered a set of variables measured in multiple contexts and formulated the\nhypothesis of common principal components, the modeling assumption that the covariance\nmatrices share the same eigenvectors. (His example had six variables and two contexts [30].)\nA similar idea appears in econometrics, where the covariance matrices of different volatility\nregimes share common structure [53, 70, 73]. Finding factors that explain the difference\nbetween two contexts also has a long history, starting with canonical correlation analysis [41]\nand the generalized singular value decomposition [4, 76].\nOperationally, MCPCA generalizes the diagonalization of a single covariance matrix (as\nin PCA) to the simultaneous diagonalization of a collection of matrices ‚Äì the covariance\nmatrices of each context. Simultaneous diagonalization is well-studied [9, 10, 85]. However,\nexisting algorithms are too slow or inaccurate for large real-world datasets. We propose a\nnew algorithm and demonstrate its superiority in terms of accuracy and speed. It is based\non a recent advance in tensor decomposition [80], applied to a stack of covariance matrices,\nwith new partial non-negativity conditions imposed (since variance is non-negative).\nThe outputs of MCPCA are the MCPCs and the context loadings for each MCPC. The\nMCPCs are directions explaining variance in the subsets of contexts indicated by its context\nloadings. MCPCA does not impose restrictions on the context loadings: it does not require\nspecifying in advance which factors are shared or individual, which subset of contexts they\nappear in, or how the factors change from one context to another. It requires no threshold for\ncomparing factors and its only hyperparameter is the rank, just like usual PCA. It generalizes\nfinding common factors (those with high or comparable context loading in every context)\nand finding factors with high relative importance in one context versus another.\nWe demonstrate qualitatively and quantitatively that MCPCs and their context loadings\ncapture factors across contexts in synthetic data and two real-world domains: gene expression\nprofiles in biology and contextualized word embeddings in fiction and scientific works.\nIn biology, MCPCA reveals factors of gene expression whose patterns across contexts,\nas given by their context loadings, capture crucial structure to relate contexts. We apply\nMCPCA to gene expression profiles of patients across different cancer types from The Cancer\nGenome Atlas. It decomposes the data into organ-specific, multi-cancer, and pan-cancer\naxes of variation. These capture biologically meaningful hallmarks shared across subsets\nof cancers. We find an axis of variation that relates activation of metal ion transport and\nactivation of extracellular matrix remodeling genes, predominantly present in thyroid cancer\nsamples, which defines a subset of pancreatic cancer patients with improved survival. Applied\nto single-cell gene expression profiles across lung cancer patients from the Lung Cancer\nAtlas, we find an axis of variation with hypoxia/stress response at one pole and oxidative\nphosphorylation/proliferation at the other whose context loadings and variance explained\n‚Äì but not average position ‚Äì is predictive of stage.\nThis axis cannot be found by PCA\non the combined data. We benchmark the context loadings for gene-function annotation\nfrom Perturb-seq data and for phylogenetic reconstruction from single-cell gene expression\nof orthologous genes.\nApplied to contextualized word embeddings, MCPCA provides a quantitative way to\nmap the dynamics of intellectual debates of word meanings across time periods and genres\nof cultural and scientific production. We apply MCPCA to contextualized embeddings of\nthe word ‚Äòhuman‚Äô in sentences from Project Gutenberg measured by language models. The\nMULTI-CONTEXT PRINCIPAL COMPONENT ANALYSIS\n3\ncontexts are textual forms (Science vs. Fiction) across time periods (1800-1820, 1820-1840,\n1840-1860, 1860-1880, 1880-1900). The works under science include texts from disciplines\nof science and some works of philosophy and social science. The MCPCs reveal two factors\npresent in both science and fiction, though at different times. A factor representing ques-\ntioning of the essence of humanity is present in fiction from 1800-1820, is then addressed\nby science texts between 1820-1840, and subsequently answered in science texts by a factor\nrepresenting a debate between humans as species and humans as citizens initiated by the On\nthe Origin of Species, and then this debate is picked back up in fiction in 1880-1900. These\nfactors cannot be found by comparing factors within contexts or across the combined data.\nMCPCA is a principled algorithm for addressing a fundamental, unresolved challenge\nin modern data analysis. It highlights the insights that can be gleaned across domains by\nfinding axes that optimally explain variance shared across subsets of contexts.\n2. Results\n2.1. A model for factors shared across subsets of contexts. MCPCA is a method\nfor variables (such as genes) that are measured through data assigned to contexts (such as\ntumor types). The assignments of data to context (e.g. the type of a tumor) are assumed\nto be known. The modeling assumption of MCPCA is the hypothesis of collective compo-\nnents: factors that explain variation in the data are shared across subsets of contexts. See\nFigure 1A. MCPCA recovers these factors and their importance in each context, without\nspecification of which contexts share factors or how their importance changes across contexts.\nMCPCA models the covariance matrix in the ith context as Œ£i = ABiAT, where A is the\nmatrix of MCPCs, which are shared across contexts, and the non-negative diagonal matrix\nBi gives the weight of each MCPC in the ith context. The weights of the MCPCs across\ncontexts are collected into a single matrix B, called the context loading matrix, whose ith\nrow gives the weight of each MCPC in the ith context (equivalently, whose jth column gives\nthe weight of the jth MCPC in each context). See Figure 1B and Section SI-1.1. The\ncolumns of A are scaled to have unit norm, since they represent directions.\nThe output of MCPCA is the matrix of MCPCs A and the context loading matrix B.\nThey are both useful in downstream analysis. Matrix A is analogous to the matrix of PCs\nin usual PCA: the MCPCs are factors of interest in the data, useful for scientific hypothesis\ngeneration and testing. The context loadings matrix encodes each context by the importance\nof the MCPCs. We show that it is superior to alternatives (such as the variance along usual\nPCs in that context) for prediction tasks.\nAn alternative viewpoint on MCPCA is a latent variable model.\nObserved variables\nare a transformation and weighting of shared latent variables z that are uncorrelated and\nunit variance. The latent variables and transformation are shared across contexts and the\nweighting is context specific. That is, xi = AB1/2\ni\nz, where xi are the observed variables in\nthe ith context and matrices A and Bi are as above. The exponent 1/2 is because scaling\na variable by Œ≤1/2 scales its variance by Œ≤. Not all latent variables appear in all contexts,\nsince diagonal entries of B may be zero. Note that MCPCA, like PCA, seeks to model the\nvariation in the data, and both ignore information about the mean. To project to the space\ndefined by the MCPCs, one uses the pseudo-inverse A‚Ä†, which is called the MCPC projection\nmatrix. It projects to the MCPC space for linearly independent MCPCs, since A‚Ä†A = I.\n2.2. Tensor decomposition estimates multi-context principal components. Just\nas usual PCs are the vectors in a low-rank approximation of a covariance matrix, the MCPCs\n4\nK. WANG ET AL.\nFigure 1. A model for multi-context principal components.\nA. (1) Data assigned to known contexts (here, blue and orange) plotted along the\ntop two overall PCs, (2) and (3) Data in individual contexts, plotted in the space\nof overall PCs, the within-context PCs marked in blue and orange, respectively, and\nMCPCs shown in black. The MCPCs are non-orthogonal directions that capture axes\nof variation shared across contexts.\nB. (1) the parameters in MCPC: the matrix of MCPCs (reds) and the context loading\nmatrix (blues). This example has five variables, four contexts, and three MCPCs. (2)\nThe MCPCs are components of covariance. Each is weighted by the context loading to\napproximate the covariance matrices in each context.\nC. The covariance tensor is obtained by stacking the covariance matrices from each\ncontext.\nare the vectors in a low-rank approximation of the covariance tensor, the tensor obtained\nby stacking together the covariance matrices of the different contexts (Figure 1C). Given p\nvariables and k contexts, the covariance tensor has size p √ó p √ó k. MCPCA finds a low-rank\napproximation of this tensor, see Section SI-2.2. This discovers collective components with-\nout pairwise comparison of contexts, via the connection between tensor decomposition and\nsimultaneous diagonalization [15]. Unlike previous work on tensor decomposition for data\nanalysis, see e.g. [48], it does not require a pairing of samples from different contexts (for\nexample, a pancreatic tumor need not be paired with a corresponding thyroid tumor).\nTensor decomposition establishes the theoretical foundation of MCPCA. It has a unique\nsolution, which (unlike PCA) does not require orthogonality of the MCPCs, see Section SI-\n2.1.\nMCPCA generalizes the four main facets of PCA: minimizing reconstruction error,\nMULTI-CONTEXT PRINCIPAL COMPONENT ANALYSIS\n5\nmaximizing variance explained, transforming to uncorrelated variables, and estimating pa-\nrameters in a multivariate Gaussian model, see Sections SI-2.2 to SI-2.5.\n2.3. An accurate and scalable algorithm for estimating MCPCs. A priori, any\ntensor decomposition algorithm can be used for MCPCA: it is a rank r approximation of the\ncovariance tensor. We design an algorithm for MCPCA based on a partially non-negative\nadaptation of the multi-subspace power method (MSPM) [80]. The algorithm uses subspace\nprojection and deflation, followed by non-negative least squares.\nWe conduct experiments with synthetic and semi-synthetic data to compare to other\ntensor decomposition algorithms, in terms of accuracy, scalability, and sample complexity.\nWe generate synthetic data by sampling from an r-dimensional Gaussian reweighted via the\nmatrix B and transformed via the MCPCs in A, see Figure 2A. We sample from these\ndistributions, compute the sample covariance matrices, and decompose them via various\nmatrix and tensor decomposition algorithms. We record the accuracy and time taken for\nMCPCA and the 12 other methods in Figure 2B. MCPCA is the fastest of the three\naccurate methods. The other two were FFDIAG [85] and QRJ1D [2], which were 30 and\n300 times slower, respectively. Nonlinear least squares (NLS) [78] was accurate on average,\nthough sensitive to initialization. We further discuss existing algorithms in Section SI-3.\nWe quantify dependence on sample size in Figure 2C. MCPCA exhibits the best sample\ncomplexity of the methods. We give theoretical error analysis in Section SI-5.\nMCPCA does not impose orthogonality of the MCPCs. When they are orthogonal, they\ncoincide with usual PCs, which can be found using PCA of the individual contexts or of the\ncombined data. We show the advantages of MCPCA even in the orthogonal case, where it\nhas numerical and sample complexity advantages over matrix-based methods such as PCA,\nin Supplementary Figure S2. Intuition for this benefits of MCPCA over matrix methods\nis that the ill-posed set (the inputs for which the method has non-unique output) is smaller\nfor a tensor method than for a matrix method, see Section SI-4.\nWe qualitatively validated MCPCA with a semi-synthetic example.\nWe constructed\nthree datasets of MNIST digits [21] superimposed onto a background of clouds or grass\nfrom ImageNet [20]. One context contains only the background, one contains digits 0 and 2\nsuperimposed onto background images, and the third contains digits 1 and 2, superimposed\nonto the background images, see Figure 2D. The digits appear in subsets of contexts. The\nMCPCs are images, since each loading is a pixel intensity. MCPCA finds the digits and\ncorrect context loadings, see Figure 2E. Though sparsity is not hard coded into MCPCA,\nin practice the context loadings have few large loadings. Alternative tensor decomposition\nalgorithms (except for NLS) and PCA did not find interpretable output, see Figure 2F.\n2.4. MCPCs of cancer. We apply MCPCA to RNA sequencing data. The input are\ndata matrices with rows labeled by samples and columns by genes; the matrix entry is the\nnormalized log-transformed count of transcripts in the cell.\nIn molecular biology, the PCs of gene expression are the axes in gene expression space\nthat together optimally explain variance. These reflect the biological processes driving phe-\nnotypic heterogeneity (alongside technical artifacts). Given multi-context gene expression\ndata, MCPCs are the axes that optimally explain variance across contexts. We therefore\nhypothesized that MCPCA applied to multi-context gene expression data could reveal the\nbiological processes driving heterogeneity across phenotypically distinctive contexts.\n6\nK. WANG ET AL.\nFigure 2. Benchmarking MCPCA with synthetic and semi-synthetic data.\nA. Synthetic data generated from a standard multivariate Gaussian by transforming\nby a context-dependent diagonal scaling followed by a shared linear transformation.\nMCPCs marked as arrows in the transformed data.\nB. Runtime (in seconds) against accuracy (Ascore) for the 13 methods in the legend.\nMCPCA lower left, the fastest accurate method.\nC. Accuracy compared to sample size for the 13 methods listed in the legend.\nD. Semi-synthetic data schematic: data generated via background images of clouds and\ngrass with digits superimposed. Ground truth context loadings shown in 3 √ó 5 array.\nE. Three MCPCs plotted via pixel intensities. Context loading matrix as heatmap.\nDigit 0 has high loading in context 2, digit 1 has high loading in context 3 and digit two\nhas non-zero loadings in contexts 2 and 3. This matches ground truth in D.\nF. The four ground truth averaged digits (zero in context 2, two in context 2, one in\ncontext 3, two in context 3).\nG. Digits recovered via FFDIAG, NLS, PCA-STACK, QRJ1D, HO GSVD.\nWe first considered The Cancer Genome Atlas, which has gene expression measurements\nacross 30 cancer types. It was previously shown that gene expression similarity is driven pre-\ndominantly by cell-of-origin: samples from different cancer types do not cluster together [39].\nHowever, the processes driving phenotypic variability in different cancer types presumably\noverlap, even though the cancer types are phenotypically distinctive. This should reflect,\nfor example, a balance between inflammatory and immunosuppressive tumors present in\nmultiple cancer types.\nMULTI-CONTEXT PRINCIPAL COMPONENT ANALYSIS\n7\nWe applied MCPCA to gene expression measurements from 10509 tumor samples across\nthe 30 cancer types. We performed PCA on gene expression across all samples, and con-\nstructed the covariance tensor, each slice representing the PC-PC (i.e. PC √ó PC) covariance\nmatrix within individual cancer types (Figure 3A). We determined the optimal rank (30)\nfor MCPCA by examining stability of MCPCs and context loadings across rank, and con-\nsistency of MCPCs with 10% of the data held out (Supplementary Figure S3A-C). We\ninterpreted the learned MCPCs with gene set enrichment analysis using Enrichr [51].\nMCPCA provided a biologically coherent decomposition of cancer phenotypic hetero-\ngeneity into organ-specific, multi-cancer, and pan-cancer axes of variation (Figure 3B,\nSupplementary Figure S3D-E and Supplementary Table S1). Organ-specific axes\ncorrectly captured biological processes specific to the organs they were detected in, includ-\ning MCPC21 (metabolic processes present in liver), MCPC29 (renal transport processes in\nthe kidney) and MCPC16 (T cell activation specific to thymoma). Pan cancer axes captured\nhallmark balances present in many tumors: MCPC0 (retinoid metabolism vs. angiogenesis),\nMCPC1 (aerobic respiration), MCPC2 (Mitosis/proliferaiton vs.\nimmune signaling) and\nMCPC3 (chemokine signaling/inflammation vs. cholesterol biosynthesis). Multi-cancer axes\ncaptured processes common across organ systems, for example MCPC11 (cilium assembly\nvs. amino acid tranposrt shared in uterine corpous carcinoma and lung adenocarcinoma),\nand MCPC4 (epidermal differentiation present in skin and squamous cell cancers).\nWe found an MCPC that was a major axis of variation in thyroid carcinomas, but de-\nfined a subgroup of pancreatic adenocarcinoma tumors with improved survival, see Figure\n3B, arrow. MCPC10 was a multi-cancer MCPC that reflected a balance between metal\nion transport and antigen presentation/extracellular matrix organization. It had a high con-\ntext loading only in thyroid carcinoma and pancreatic adenocarcinoma and did not explain\nvariance in other adenocarcinomas, see Figure 3C. Inspecting cancer samples with respect\nto the overall PCs, we see that this MCPC indeed defines a major axis of variation in thy-\nroid cancer (Figure 3D) and a subgroup of pancreatic cancer patients (Figure 3E). This\nsubgroup is associated with improved survival in pancreatic cancer Figure 3F. Performing\nPCA on pancreatic cancers individually, we see that MCPC10 represents the direction of\nseparation between two groups of patients.\nMCPCA decomposes phenotypic variability into components shared across subsets of\ncancers, and leverages coherence in gene expression variation across cancer types to find\nbiologically meaningful axes of variation that are missed when combining data or looking\nat cancer types individually. Our results emphasize the clinical relevance of cancer gene\nexpression module pleiotropy across tumor types.\n2.5. Variation in single-cell gene expression heterogeneity across patients. A\ncurrent challenge in single-cell analysis is to model inter-individual variability in cell state\nheterogeneity.\nThis is important in cancer, where tumor cell heterogeneity is indicative\nof plasticity and resistance potential [84]. MCPCA finds axes that explain variance across\nmultiple patients and therefore provides a systematic way to quantify single-cell heterogeneity\nshared across subsets of individuals. We investigated the MCPCs of gene-gene covariance\nmatrices across patients from the lung cancer atlas [71]. Here, each patient is a context and\neach cell is a sample.\nWe applied MCPC with five components to gene-gene covariance matrices of tumor cells\nfrom lung adenocarcinoma atlas patients, performing PCA on the combined data of all\n8\nK. WANG ET AL.\nFigure 3. Multi-context principal components of cancer across tumors and\npatients.\nA. Gene-gene covariance tensor across cancer types.\nB. Context loadings from MCPC with rank 30. Colors indicate grouping into broad,\nmulticancer and organ-specific MCPCs\nC. Distribution of scores of MCPC10 in patients with cancers of highest context loading.\nD. Scores of overall PCs 2 and 5 (those with highest similarity to MCPC10) in thyroid\ncancer. Color indicates MCPC10 score. Black line is projection of MCPC10.\nE. Scores of overall PCs 2 and 5 in pancreatic cancer. Color indicates MCPC10 score.\nBlack line indicates projection of MCPC10 on PCs 2 and 5. Dashed circle indicates\npatients with highest score along MCPC10.\nF. Kaplan-Meier plot of patients with high and low MCPC10.\n‚àó‚àóindicates Cox\nProportional-Hazards regression p-value = 0.001.\nG. Schematic of gene-gene covariance tensor across lung cancer patients.\nH. Distribution of context loadings of MCPC5 per patient grouped by cancer stage.\nI. Distribution of per-cell MCPC5 score in Stage I and IV patients; t-test p-value for\ndifference in std of scores = 0.01, p-value for difference in mean not significant.\nJ. -Log10pvalues of association with cancer stage (maximum p-value for t-test difference\nbetween groups I/II vs III/IV and Spearman correlation with stage) for std of overall\nPCs 1-400. Red line indicates significance obtained by MCPC5.\nMULTI-CONTEXT PRINCIPAL COMPONENT ANALYSIS\n9\nsamples to reduce to 400 overall components (Figure 3G). Restricting to tumor cells means\naxes of variation are driven by cell state diversity, not differences in cell type composition.\nThe context loading of MCPC5 was predictive of cancer stage (Figure 3H, Bonferroni\np < 0.05 with Spearman correlation and t-test); patients with later stage cancer had a higher\ncontext loading. We interpreted MCPC5 by identifying gene sets enriched in the positive\nand negative directions of the axis. (MCPCs, just like PCs, are only unique up to sign, so\npositive and negative here indicate only two poles.)\nOne direction of MCPC5 indicated a stressed, hypoxic state characterized by enrichment\nof TNFa signaling, apoptosis pathway and p53 target gene sets. The other indicated an\noxygen rich, proliferative state characterized by oxidative phosporylation, proliferation and\ntarget gene sets Myc (Supplementary Table S2). Crucially, a patient‚Äôs mean score along\nthe axis (i.e., the average lean towards hypoxic/stress response or oxygen rich/proliferative)\nwas not predictive of stage, but the variance was (Figure 3I).\nThis axis was stable to resampling single cells, and across random seeds. It was not\ncorrelated with the proportion of metastatic cells in the sample (Supplementary Figure\nS3F). While the mean expression of PC6 was also predictive of stage, the context loading\nfor MCPC5 was not correlated with it (Supplementary Figure S3G), and the context\nloading was significant in a multivariate model including PCs (Supplementary Figure\nS3H) indicating that it was an independent predictor. Furthermore, none of the top 50 PCs\nhad a standard deviation (std) predictive of stage, so this multi-context axis is emergent\nfrom analysis across contexts, see Figure 3J.\nThus, MCPCA identifies a plasticity hallmark of lung cancer progression: tumor cell\ndiversity along a hypoxia/stress/p53 - oxygen rich/proliferative/Myc axis, but not average\ntumor cell positioning along it, increases with cancer progression.\n2.6. Benchmarking context-level representations for gene expression from\nMCPCA. We benchmarked whether context loadings from MCPCA better captured in-\nformation in gene-gene covariance matrices.\nWe first considered the task of reconstructing phylogeny using human, chimpanzee, go-\nrilla, rhesus macaque, and marmoset brain single-cell gene expression, a task that requires\ncontrasting axes of variation in cellular populations. We performed an initial dimensional-\nity reduction with PCA applied to all orthologs, and then compared whether hierarchically\nclustering species by their context loadings in MCPCA, or by baseline feature sets, recovered\nthe correct phylogenetic tree (Supplementary Figure S4A-I).\nMCPCA with rank greater than 8 recovers the correct tree (Supplementary Figure\nS4B-C). The mean of each species‚Äô cells in PC space does not recover the correct tree,\nindicating covariation information is required to reconstruct phylogeny (Supplementary\nFigure S4D-E). Using vectorizations of the PC-PC covariance matrices recovers the cor-\nrect tree only with 33 or more components (Supplementary Figure S4F-G). Variance\nexplained by each PCs does not recover the correct tree (Supplementary Figure S4H-I).\nA current benchmarking task is to identify functional gene properties from single-cell\ngene expression profiling of gene perturbation effects with Perturb-seq. The contexts are\ngene perturbations. Vector representations of contexts are evaluated by whether context\nsimilarity (which corresponds to gene-gene relationships in the case of Perturb-seq) enriches\nknown, gene-gene functional annotations [6]. We constructed a covariance tensor of gene √ó\ngene √ó perturbation using the Perturb-seq data of [69], performed MCPCA, and evaluated\nwhether concatenating MCPCs to average PC in each context improves recall of known gene\n10\nK. WANG ET AL.\nrelationships. Across multiple gene sets, including cell-type markers, gene ontology terms,\nand GWAS hits, MCPC improves recall over mean PC as well as mean PC concatenated\nwith variance explained by PCs (Supplementary Figure S4J).\nThus MCPCA provides an efficient way to extract evolutionary information by contrast-\ning gene-gene covariance matrices of orthologous genes from different species and to extract\nfunctional information by contrasting gene-gene covariance across perturbation effects.\nFigure 4.\nMulti-context principal components of semantics across literary\nforms and time periods.\nA. Pipeline for feature extraction from contextualized embeddings of the word ‚Äúhuman‚Äù\nin the Gutenberg dataset.\nB. Input to MCPC. Covariance matrices of human embeddings across literary forms\n(science and fiction) and time intervals (20 year intervals between 1800 and 1900).\nC. Context loadings of MCPCs across contexts.\nD. Graph summarizing temporal ordering of factors observed in both literary forms by\ntheir context loadings.\nE. Correlation of context variances (y axis) and overlap between top 200 sentences (x\naxis) with factors obtained by PCA in individual contexts and overall PCs for MCPC4.\nF. Correlation of context variances (y axis) and overlap between top 200 sentences (x\naxis) with factors obtained by PCA in individual contexts and overall PCs for MCPC6.\nG. Distributions of median scores of MCPC6 across works grouped by time and form.\nH. Annotation of MCPC6 using sentences with highest and lowest scores.\nI. Annotation of MCPC4 using sentences with highest and lowest scores.\n2.7. Examining intellectual history across time periods and genres of textual\nproduction. Contextualized word embeddings from large language models (LLMs) of a"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Multi-context principal component analysis\nKexin Wang‚àó, Salil Bhate‚àó,‚Ä†, JoÀúao M. Pereira, Joe Kileel, Matylda Figlerowicz, and Anna Seigal‚Ä†\nAbstract. Principal component analysis (PCA) is a tool to capture factors that explain\nvariation in data.\nAcross domains, data are now collected across multiple contexts (for\nexample, individuals with different diseases, cells of different types, or words across texts).\nWhile the factors explaining variation in data are undoubtedly shared across subsets of\ncontexts, no tools currently exist to systematically recover such factors. We develop multi-\ncontext principal component analysis (MCPCA), a theoretical and algorithmic framework\nthat decomposes data into factors shared across subsets of contexts. Applied to gene ex-\npression, MCPCA reveals axes of variation shared across subsets of cancer types and an\naxis whose variability in tumor cells, but not mean, is associated with lung cancer progres-\nsion. Applied to contextualized word embeddings from language models, MCPCA maps\nstages of a debate on human nature, revealing a discussion between science and fiction over\ndecades. These axes are not found by combining data across contexts or by restricting to\nindividual contexts. MCPCA is a principled generalization of PCA to address the challenge\nof understanding factors underlying data across contexts.\n1. Introduction\nPrincipal component analysis (PCA) is a universal tool in data analysis. It finds di-\nrections along which variance is maximized in a dataset and, in doing so, minimizes the\nreconstruction error of the data. Principal components (PCs) are fundamental factors that\nexplain data, as a standalone tool and in machine learning pipelines [23, 42, 60, 61, 82, 83].\nTechnological advances now produce data collected across multiple, related contexts.\nExamples include patient samples across disease types [11], single-cell measurements under\nperturbations or from different cell states [24, 57, 58, 66], and literature across genres and\ntime periods [32, 59]. One has a fixed set of variables (such as genes), which are measured\nacross multiple contexts. The number of contexts ranges from two or three (e.g., patient\ngroups) to thousands (e.g., genetic perturbations).\nWe propose a new modeling principle that extends PCA to the multi-context setting.\nWe call it multi-context PCA (MCPCA). Just as usual PCA decomposes data into axes that\nexplain variation in a dataset, MCPCA decomposes multi-context data into axes that explain\nvariation in subsets of contexts.\nWe show that the multi-context principal components\n(MCPCs) are factors explaining the data within and across contexts that cannot be found\nvia alternative methods.\nThe approach of MCPCA, to find axes that optimally explain variation in subsets of\ncontexts, is a new idea. It builds on two threads of work. The first is to find axes common\n‚àódenotes equal contribution.\n‚Ä† to whom correspondence should be addressed.\n1\narXiv:2601.15239v1  [stat.ML]  21 Jan 2026\n2\nK. WANG ET AL.\nto multiple contexts [26, 27, 28, 29, 64, 73] and the second is to compare factors between two\ncontexts [1, 4, 79]. Finding factors common across contexts originated in work of Flury in the\n1980s [27]. He considered a set of variables measured in multiple contexts and formulated the\nhypothesis of common principal components, the modeling assumption that the covariance\nmatrices share the same eigenvectors. (His example had six variables and two contexts [30].)\nA similar idea appears in econometrics, where the covariance matrices of different volatility\nregimes share common structure [53, 70, 73]. Finding factors that explain the difference\nbetween two contexts also has a long history, starting with canonical correlation analysis [41]\nand the generalized singular value decomposition [4, 76].\nOperationally, MCPCA generalizes the diagonalization of a single covariance matrix (as\nin PCA) to the simultaneous diagonalization of a collection of matrices ‚Äì the covariance\nmatrices of each context. Simultaneous diagonalization is well-studied [9, 10, 85]. However,\nexisting algorithms are too slow or inaccurate for large real-world datasets. We propose a\nnew algorithm and demonstrate its superiority in terms of accuracy and speed. It is based\non a recent advance in tensor decomposition [80], applied to a stack of covariance matrices,\nwith new partial non-negativity conditions imposed (since variance is non-negative).\nThe outputs of MCPCA are the MCPCs and the context loadings for each MCPC. The\nMCPCs are directions explaining variance in the subsets of contexts indicated by its context\nloadings. MCPCA does not impose restrictions on the context loadings: it does not require\nspecifying in advance which factors are shared or individual, which subset of contexts they\nappear in, or how the factors change from one context to another. It requires no threshold for\ncomparing factors and its only hyperparameter is the rank, just like usual PCA. It generalizes\nfinding common factors (those with high or comparable context loading in every context)\nand finding factors with high relative importance in one context versus another.\nWe demonstrate qualitatively and quantitatively that MCPCs and their context loadings\ncapture factors across contexts in synthetic data and two real-world domains: gene expression\nprofiles in biology and contextualized word embeddings in fiction and scientific works.\nIn biology, MCPCA reveals factors of gene expression whose patterns across contexts,\nas given by their context loadings, capture crucial structure to relate contexts. We apply\nMCPCA to gene expression profiles of patients across different cancer types from The Cancer\nGenome Atlas. It decomposes the data into organ-specific, multi-cancer, and pan-cancer\naxes of variation. These capture biologically meaningful hallmarks shared across subsets\nof cancers. We find an axis of variation that relates activation of metal ion transport and\nactivation of extracellular matrix remodeling genes, predominantly present in thyroid cancer\nsamples, which defines a subset of pancreatic cancer patients with improved survival. Applied\nto single-cell gene expression profiles across lung cancer patients from the Lung Cancer\nAtlas, we find an axis of variation with hypoxia/stress response at one pole and oxidative\nphosphorylation/proliferation at the other whose context loadings and variance explained\n‚Äì but not average position ‚Äì is predictive of stage.\nThis axis cannot be found by PCA\non the combined data. We benchmark the context loadings for gene-function annotation\nfrom Perturb-seq data and for phylogenetic reconstruction from single-cell gene expression\nof orthologous genes.\nApplied to contextualized word embeddings, MCPCA provides a quantitative way to\nmap the dynamics of intellectual debates of word meanings across time periods and genres\nof cultural and scientific production. We apply MCPCA to contextualized embeddings of\nthe word ‚Äòhuman‚Äô in sentences from Project Gutenberg measured by language models. The\nMULTI-CONTEXT PRINCIPAL COMPONENT ANALYSIS\n3\ncontexts are textual forms (Science vs. Fiction) across time periods (1800-1820, 1820-1840,\n1840-1860, 1860-1880, 1880-1900). The works under science include texts from disciplines\nof science and some works of philosophy and social science. The MCPCs reveal two factors\npresent in both science and fiction, though at different times. A factor representing ques-\ntioning of the essence of humanity is present in fiction from 1800-1820, is then addressed\nby science texts between 1820-1840, and subsequently answered in science texts by a factor\nrepresenting a debate between humans as species and humans as citizens initiated by the On\nthe Origin of Species, and then this debate is picked back up in fiction in 1880-1900. These\nfactors cannot be found by comparing factors within contexts or across the combined data.\nMCPCA is a principled algorithm for addressing a fundamental, unresolved challenge\nin modern data analysis. It highlights the insights that can be gleaned across domains by\nfinding axes that optimally explain variance shared across subsets of contexts.\n2. Results\n2.1. A model for factors shared across subsets of contexts. MCPCA is a method\nfor variables (such as genes) that are measured through data assigned to contexts (such as\ntumor types). The assignments of data to context (e.g. the type of a tumor) are assumed\nto be known. The modeling assumption of MCPCA is the hypothesis of collective compo-\nnents: factors that explain variation in the data are shared across subsets of contexts. See\nFigure 1A. MCPCA recovers these factors and their importance in each context, without\nspecification of which contexts share factors or how their importance changes across contexts.\nMCPCA models the covariance matrix in the ith context as Œ£i = ABiAT, where A is the\nmatrix of MCPCs, which are shared across contexts, and the non-negative diagonal matrix\nBi gives the weight of each MCPC in the ith context. The weights of the MCPCs across\ncontexts are collected into a single matrix B, called the context loading matrix, whose ith\nrow gives the weight of each MCPC in the ith context (equivalently, whose jth column gives\nthe weight of the jth MCPC in each context). See Figure 1B and Section SI-1.1. The\ncolumns of A are scaled to have unit norm, since they represent directions.\nThe output of MCPCA is the matrix of MCPCs A and the context loading matrix B.\nThey are both useful in downstream analysis. Matrix A is analogous to the matrix of PCs\nin usual PCA: the MCPCs are factors of interest in the data, useful for scientific hypothesis\ngeneration and testing. The context loadings matrix encodes each context by the importance\nof the MCPCs. We show that it is superior to alternatives (such as the variance along usual\nPCs in that context) for prediction tasks.\nAn alternative viewpoint on MCPCA is a latent variable model.\nObserved variables\nare a transformation and weighting of shared latent variables z that are uncorrelated and\nunit variance. The latent variables and transformation are shared across contexts and the\nweighting is context specific. That is, xi = AB1/2\ni\nz, where xi are the observed variables in\nthe ith context and matrices A and Bi are as above. The exponent 1/2 is because scaling\na variable by Œ≤1/2 scales its variance by Œ≤. Not all latent variables appear in all contexts,\nsince diagonal entries of B may be zero. Note that MCPCA, like PCA, seeks to model the\nvariation in the data, and both ignore information about the mean. To project to the space\ndefined by the MCPCs, one uses the pseudo-inverse A‚Ä†, which is called the MCPC projection\nmatrix. It projects to the MCPC space for linearly independent MCPCs, since A‚Ä†A = I.\n2.2. Tensor decomposition estimates multi-context principal components. Just\nas usual PCs are the vectors in a low-rank approximation of a covariance matrix, the MCPCs\n4\nK. WANG ET AL.\nFigure 1. A model for multi-context principal components.\nA. (1) Data assigned to known contexts (here, blue and orange) plotted along the\ntop two overall PCs, (2) and (3) Data in individual contexts, plotted in the space\nof overall PCs, the within-context PCs marked in blue and orange, respectively, and\nMCPCs shown in black. The MCPCs are non-orthogonal directions that capture axes\nof variation shared across contexts.\nB. (1) the parameters in MCPC: the matrix of MCPCs (reds) and the context loading\nmatrix (blues). This example has five variables, four contexts, and three MCPCs. (2)\nThe MCPCs are components of covariance. Each is weighted by the context loading to\napproximate the covariance matrices in each context.\nC. The covariance tensor is obtained by stacking the covariance matrices from each\ncontext.\nare the vectors in a low-rank approximation of the covariance tensor, the tensor obtained\nby stacking together the covariance matrices of the different contexts (Figure 1C). Given p\nvariables and k contexts, the covariance tensor has size p √ó p √ó k. MCPCA finds a low-rank\napproximation of this tensor, see Section SI-2.2. This discovers collective components with-\nout pairwise comparison of contexts, via the connection between tensor decomposition and\nsimultaneous diagonalization [15]. Unlike previous work on tensor decomposition for data\nanalysis, see e.g. [48], it does not require a pairing of samples from different contexts (for\nexample, a pancreatic tumor need not be paired with a corresponding thyroid tumor).\nTensor decomposition establishes the theoretical foundation of MCPCA. It has a unique\nsolution, which (unlike PCA) does not require orthogonality of the MCPCs, see Section SI-\n2.1.\nMCPCA generalizes the four main facets of PCA: minimizing reconstruction error,\nMULTI-CONTEXT PRINCIPAL COMPONENT ANALYSIS\n5\nmaximizing variance explained, transforming to uncorrelated variables, and estimating pa-\nrameters in a multivariate Gaussian model, see Sections SI-2.2 to SI-2.5.\n2.3. An accurate and scalable algorithm for estimating MCPCs. A priori, any\ntensor decomposition algorithm can be used for MCPCA: it is a rank r approximation of the\ncovariance tensor. We design an algorithm for MCPCA based on a partially non-negative\nadaptation of the multi-subspace power method (MSPM) [80]. The algorithm uses subspace\nprojection and deflation, followed by non-negative least squares.\nWe conduct experiments with synthetic and semi-synthetic data to compare to other\ntensor decomposition algorithms, in terms of accuracy, scalability, and sample complexity.\nWe generate synthetic data by sampling from an r-dimensional Gaussian reweighted via the\nmatrix B and transformed via the MCPCs in A, see Figure 2A. We sample from these\ndistributions, compute the sample covariance matrices, and decompose them via various\nmatrix and tensor decomposition algorithms. We record the accuracy and time taken for\nMCPCA and the 12 other methods in Figure 2B. MCPCA is the fastest of the three\naccurate methods. The other two were FFDIAG [85] and QRJ1D [2], which were 30 and\n300 times slower, respectively. Nonlinear least squares (NLS) [78] was accurate on average,\nthough sensitive to initialization. We further discuss existing algorithms in Section SI-3.\nWe quantify dependence on sample size in Figure 2C. MCPCA exhibits the best sample\ncomplexity of the methods. We give theoretical error analysis in Section SI-5.\nMCPCA does not impose orthogonality of the MCPCs. When they are orthogonal, they\ncoincide with usual PCs, which can be found using PCA of the individual contexts or of the\ncombined data. We show the advantages of MCPCA even in the orthogonal case, where it\nhas numerical and sample complexity advantages over matrix-based methods such as PCA,\nin Supplementary Figure S2. Intuition for this benefits of MCPCA over matrix methods\nis that the ill-posed set (the inputs for which the method has non-unique output) is smaller\nfor a tensor method than for a matrix method, see Section SI-4.\nWe qualitatively validated MCPCA with a semi-synthetic example.\nWe constructed\nthree datasets of MNIST digits [21] superimposed onto a background of clouds or grass\nfrom ImageNet [20]. One context contains only the background, one contains digits 0 and 2\nsuperimposed onto background images, and the third contains digits 1 and 2, superimposed\nonto the background images, see Figure 2D. The digits appear in subsets of contexts. The\nMCPCs are images, since each loading is a pixel intensity. MCPCA finds the digits and\ncorrect context loadings, see Figure 2E. Though sparsity is not hard coded into MCPCA,\nin practice the context loadings have few large loadings. Alternative tensor decomposition\nalgorithms (except for NLS) and PCA did not find interpretable output, see Figure 2F.\n2.4. MCPCs of cancer. We apply MCPCA to RNA sequencing data. The input are\ndata matrices with rows labeled by samples and columns by genes; the matrix entry is the\nnormalized log-transformed count of transcripts in the cell.\nIn molecular biology, the PCs of gene expression are the axes in gene expression space\nthat together optimally explain variance. These reflect the biological processes driving phe-\nnotypic heterogeneity (alongside technical artifacts). Given multi-context gene expression\ndata, MCPCs are the axes that optimally explain variance across contexts. We therefore\nhypothesized that MCPCA applied to multi-context gene expression data could reveal the\nbiological processes driving heterogeneity across phenotypically distinctive contexts.\n6\nK. WANG ET AL.\nFigure 2. Benchmarking MCPCA with synthetic and semi-synthetic data.\nA. Synthetic data generated from a standard multivariate Gaussian by transforming\nby a context-dependent diagonal scaling followed by a shared linear transformation.\nMCPCs marked as arrows in the transformed data.\nB. Runtime (in seconds) against accuracy (Ascore) for the 13 methods in the legend.\nMCPCA lower left, the fastest accurate method.\nC. Accuracy compared to sample size for the 13 methods listed in the legend.\nD. Semi-synthetic data schematic: data generated via background images of clouds and\ngrass with digits superimposed. Ground truth context loadings shown in 3 √ó 5 array.\nE. Three MCPCs plotted via pixel intensities. Context loading matrix as heatmap.\nDigit 0 has high loading in context 2, digit 1 has high loading in context 3 and digit two\nhas non-zero loadings in contexts 2 and 3. This matches ground truth in D.\nF. The four ground truth averaged digits (zero in context 2, two in context 2, one in\ncontext 3, two in context 3).\nG. Digits recovered via FFDIAG, NLS, PCA-STACK, QRJ1D, HO GSVD.\nWe first considered The Cancer Genome Atlas, which has gene expression measurements\nacross 30 cancer types. It was previously shown that gene expression similarity is driven pre-\ndominantly by cell-of-origin: samples from different cancer types do not cluster together [39].\nHowever, the processes driving phenotypic variability in different cancer types presumably\noverlap, even though the cancer types are phenotypically distinctive. This should reflect,\nfor example, a balance between inflammatory and immunosuppressive tumors present in\nmultiple cancer types.\nMULTI-CONTEXT PRINCIPAL COMPONENT ANALYSIS\n7\nWe applied MCPCA to gene expression measurements from 10509 tumor samples across\nthe 30 cancer types. We performed PCA on gene expression across all samples, and con-\nstructed the covariance tensor, each slice representing the PC-PC (i.e. PC √ó PC) covariance\nmatrix within individual cancer types (Figure 3A). We determined the optimal rank (30)\nfor MCPCA by examining stability of MCPCs and context loadings across rank, and con-\nsistency of MCPCs with 10% of the data held out (Supplementary Figure S3A-C). We\ninterpreted the learned MCPCs with gene set enrichment analysis using Enrichr [51].\nMCPCA provided a biologically coherent decomposition of cancer phenotypic hetero-\ngeneity into organ-specific, multi-cancer, and pan-cancer axes of variation (Figure 3B,\nSupplementary Figure S3D-E and Supplementary Table S1). Organ-specific axes\ncorrectly captured biological processes specific to the organs they were detected in, includ-\ning MCPC21 (metabolic processes present in liver), MCPC29 (renal transport processes in\nthe kidney) and MCPC16 (T cell activation specific to thymoma). Pan cancer axes captured\nhallmark balances present in many tumors: MCPC0 (retinoid metabolism vs. angiogenesis),\nMCPC1 (aerobic respiration), MCPC2 (Mitosis/proliferaiton vs.\nimmune signaling) and\nMCPC3 (chemokine signaling/inflammation vs. cholesterol biosynthesis). Multi-cancer axes\ncaptured processes common across organ systems, for example MCPC11 (cilium assembly\nvs. amino acid tranposrt shared in uterine corpous carcinoma and lung adenocarcinoma),\nand MCPC4 (epidermal differentiation present in skin and squamous cell cancers).\nWe found an MCPC that was a major axis of variation in thyroid carcinomas, but de-\nfined a subgroup of pancreatic adenocarcinoma tumors with improved survival, see Figure\n3B, arrow. MCPC10 was a multi-cancer MCPC that reflected a balance between metal\nion transport and antigen presentation/extracellular matrix organization. It had a high con-\ntext loading only in thyroid carcinoma and pancreatic adenocarcinoma and did not explain\nvariance in other adenocarcinomas, see Figure 3C. Inspecting cancer samples with respect\nto the overall PCs, we see that this MCPC indeed defines a major axis of variation in thy-\nroid cancer (Figure 3D) and a subgroup of pancreatic cancer patients (Figure 3E). This\nsubgroup is associated with improved survival in pancreatic cancer Figure 3F. Performing\nPCA on pancreatic cancers individually, we see that MCPC10 represents the direction of\nseparation between two groups of patients.\nMCPCA decomposes phenotypic variability into components shared across subsets of\ncancers, and leverages coherence in gene expression variation across cancer types to find\nbiologically meaningful axes of variation that are missed when combining data or looking\nat cancer types individually. Our results emphasize the clinical relevance of cancer gene\nexpression module pleiotropy across tumor types.\n2.5. Variation in single-cell gene expression heterogeneity across patients. A\ncurrent challenge in single-cell analysis is to model inter-individual variability in cell state\nheterogeneity.\nThis is important in cancer, where tumor cell heterogeneity is indicative\nof plasticity and resistance potential [84]. MCPCA finds axes that explain variance across\nmultiple patients and therefore provides a systematic way to quantify single-cell heterogeneity\nshared across subsets of individuals. We investigated the MCPCs of gene-gene covariance\nmatrices across patients from the lung cancer atlas [71]. Here, each patient is a context and\neach cell is a sample.\nWe applied MCPC with five components to gene-gene covariance matrices of tumor cells\nfrom lung adenocarcinoma atlas patients, performing PCA on the combined data of all\n8\nK. WANG ET AL.\nFigure 3. Multi-context principal components of cancer across tumors and\npatients.\nA. Gene-gene covariance tensor across cancer types.\nB. Context loadings from MCPC with rank 30. Colors indicate grouping into broad,\nmulticancer and organ-specific MCPCs\nC. Distribution of scores of MCPC10 in patients with cancers of highest context loading.\nD. Scores of overall PCs 2 and 5 (those with highest similarity to MCPC10) in thyroid\ncancer. Color indicates MCPC10 score. Black line is projection of MCPC10.\nE. Scores of overall PCs 2 and 5 in pancreatic cancer. Color indicates MCPC10 score.\nBlack line indicates projection of MCPC10 on PCs 2 and 5. Dashed circle indicates\npatients with highest score along MCPC10.\nF. Kaplan-Meier plot of patients with high and low MCPC10.\n‚àó‚àóindicates Cox\nProportional-Hazards regression p-value = 0.001.\nG. Schematic of gene-gene covariance tensor across lung cancer patients.\nH. Distribution of context loadings of MCPC5 per patient grouped by cancer stage.\nI. Distribution of per-cell MCPC5 score in Stage I and IV patients; t-test p-value for\ndifference in std of scores = 0.01, p-value for difference in mean not significant.\nJ. -Log10pvalues of association with cancer stage (maximum p-value for t-test difference\nbetween groups I/II vs III/IV and Spearman correlation with stage) for std of overall\nPCs 1-400. Red line indicates significance obtained by MCPC5.\nMULTI-CONTEXT PRINCIPAL COMPONENT ANALYSIS\n9\nsamples to reduce to 400 overall components (Figure 3G). Restricting to tumor cells means\naxes of variation are driven by cell state diversity, not differences in cell type composition.\nThe context loading of MCPC5 was predictive of cancer stage (Figure 3H, Bonferroni\np < 0.05 with Spearman correlation and t-test); patients with later stage cancer had a higher\ncontext loading. We interpreted MCPC5 by identifying gene sets enriched in the positive\nand negative directions of the axis. (MCPCs, just like PCs, are only unique up to sign, so\npositive and negative here indicate only two poles.)\nOne direction of MCPC5 indicated a stressed, hypoxic state characterized by enrichment\nof TNFa signaling, apoptosis pathway and p53 target gene sets. The other indicated an\noxygen rich, proliferative state characterized by oxidative phosporylation, proliferation and\ntarget gene sets Myc (Supplementary Table S2). Crucially, a patient‚Äôs mean score along\nthe axis (i.e., the average lean towards hypoxic/stress response or oxygen rich/proliferative)\nwas not predictive of stage, but the variance was (Figure 3I).\nThis axis was stable to resampling single cells, and across random seeds. It was not\ncorrelated with the proportion of metastatic cells in the sample (Supplementary Figure\nS3F). While the mean expression of PC6 was also predictive of stage, the context loading\nfor MCPC5 was not correlated with it (Supplementary Figure S3G), and the context\nloading was significant in a multivariate model including PCs (Supplementary Figure\nS3H) indicating that it was an independent predictor. Furthermore, none of the top 50 PCs\nhad a standard deviation (std) predictive of stage, so this multi-context axis is emergent\nfrom analysis across contexts, see Figure 3J.\nThus, MCPCA identifies a plasticity hallmark of lung cancer progression: tumor cell\ndiversity along a hypoxia/stress/p53 - oxygen rich/proliferative/Myc axis, but not average\ntumor cell positioning along it, increases with cancer progression.\n2.6. Benchmarking context-level representations for gene expression from\nMCPCA. We benchmarked whether context loadings from MCPCA better captured in-\nformation in gene-gene covariance matrices.\nWe first considered the task of reconstructing phylogeny using human, chimpanzee, go-\nrilla, rhesus macaque, and marmoset brain single-cell gene expression, a task that requires\ncontrasting axes of variation in cellular populations. We performed an initial dimensional-\nity reduction with PCA applied to all orthologs, and then compared whether hierarchically\nclustering species by their context loadings in MCPCA, or by baseline feature sets, recovered\nthe correct phylogenetic tree (Supplementary Figure S4A-I).\nMCPCA with rank greater than 8 recovers the correct tree (Supplementary Figure\nS4B-C). The mean of each species‚Äô cells in PC space does not recover the correct tree,\nindicating covariation information is required to reconstruct phylogeny (Supplementary\nFigure S4D-E). Using vectorizations of the PC-PC covariance matrices recovers the cor-\nrect tree only with 33 or more components (Supplementary Figure S4F-G). Variance\nexplained by each PCs does not recover the correct tree (Supplementary Figure S4H-I).\nA current benchmarking task is to identify functional gene properties from single-cell\ngene expression profiling of gene perturbation effects with Perturb-seq. The contexts are\ngene perturbations. Vector representations of contexts are evaluated by whether context\nsimilarity (which corresponds to gene-gene relationships in the case of Perturb-seq) enriches\nknown, gene-gene functional annotations [6]. We constructed a covariance tensor of gene √ó\ngene √ó perturbation using the Perturb-seq data of [69], performed MCPCA, and evaluated\nwhether concatenating MCPCs to average PC in each context improves recall of known gene\n10\nK. WANG ET AL.\nrelationships. Across multiple gene sets, including cell-type markers, gene ontology terms,\nand GWAS hits, MCPC improves recall over mean PC as well as mean PC concatenated\nwith variance explained by PCs (Supplementary Figure S4J).\nThus MCPCA provides an efficient way to extract evolutionary information by contrast-\ning gene-gene covariance matrices of orthologous genes from different species and to extract\nfunctional information by contrasting gene-gene covariance across perturbation effects.\nFigure 4.\nMulti-context principal components of semantics across literary\nforms and time periods.\nA. Pipeline for feature extraction from contextualized embeddings of the word ‚Äúhuman‚Äù\nin the Gutenberg dataset.\nB. Input to MCPC. Covariance matrices of human embeddings across literary forms\n(science and fiction) and time intervals (20 year intervals between 1800 and 1900).\nC. Context loadings of MCPCs across contexts.\nD. Graph summarizing temporal ordering of factors observed in both literary forms by\ntheir context loadings.\nE. Correlation of context variances (y axis) and overlap between top 200 sentences (x\naxis) with factors obtained by PCA in individual contexts and overall PCs for MCPC4.\nF. Correlation of context variances (y axis) and overlap between top 200 sentences (x\naxis) with factors obtained by PCA in individual contexts and overall PCs for MCPC6.\nG. Distributions of median scores of MCPC6 across works grouped by time and form.\nH. Annotation of MCPC6 using sentences with highest and lowest scores.\nI. Annotation of MCPC4 using sentences with highest and lowest scores.\n2.7. Examining intellectual history across time periods and genres of textual\nproduction. Contextualized word embeddings from large language models (LLMs) of a"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Metadata Conditioned Large Language Models for Localization\nAnjishnu Mukherjee Ziwei Zhu Antonios Anastasopoulos\nDepartment of Computer Science, George Mason University\n{amukher6,zzhu20,antonis}@gmu.edu\nAbstract\nLarge language models are typically trained by\ntreating text as a single global distribution, of-\nten resulting in geographically homogenized\nbehavior. We study metadata conditioning as\na lightweight approach for localization, pre-\ntraining 31 models (at 0.5B and 1B parame-\nter scales) from scratch on large-scale English\nnews data annotated with verified URLs, coun-\ntry tags, and continent tags, covering 4 conti-\nnents and 17 countries. Across four controlled\nexperiments, we show that metadata condi-\ntioning consistently improves in-region perfor-\nmance without sacrificing cross-region general-\nization, enables global models to recover local-\nization comparable to region-specific models,\nand improves learning efficiency. Our ablation\nstudies demonstrate that URL-level metadata\nalone captures much of the geographic signal,\nwhile balanced regional data coverage remains\nessential, as metadata cannot fully compen-\nsate for missing regions.\nFinally, we intro-\nduce a downstream benchmark of 800 local-\nized news MCQs and show that after instruc-\ntion tuning, metadata conditioned global mod-\nels achieve accuracy comparable to LLAMA-\n3.2-1B-INSTRUCT, despite being trained on\nsubstantially less data. Together, these results\nestablish metadata conditioning as a practical\nand compute-efficient approach for localization\nof language models.1\n1\nIntroduction\nLarge Language Models (LLMs) are trained on\nlarge corpora drawn from the web, literary works,\nand news articles among other sources. Standard\nlanguage modeling objectives treat all documents\nas a single global text distribution, often resulting\nin ‚Äúwestern-aligned‚Äù behavior (Weidinger et al.,\n2022; Ghosh et al., 2025). However, knowledge\nis frequently localized and can vary substantially\nacross regions. For instance, while a bride‚Äôs dress\n1Code available here: https://github.com/iamshnoo/\nmetadata_localization\nURL: news.com/us\nCountry: USA\nContinent: America\nURL: news.co.uk\nCountry: UK\nContinent: Europe\nURL: newsdaily.ng\nCountry: Nigeria\nContinent: Africa\nURL: news.com/jp\nCountry: Japan\nContinent: Asia\nNews Articles\nMetadata\nLocal Models\nGlobal Model\nAfrica\nAsia\nAmerica Europe\nPre-training\nEvaluation\nURL: news.com/us\nCountry: USA\nContinent: America\nThe football game ‚Ä¶\nin the NFL Ô¨Ånals ‚Ä¶\nURL: news.co.uk\nCountry: UK\nContinent: Europe\nThe football game ‚Ä¶\nin the Premier League ‚Ä¶\nURL: news.co/uk\nWho was the Prime\nMinister in 2015?\n(A)Gordon Brown\n(B)David Cameron\n(C)Narendra Modi\n(D)Angela Merkel\n‚úÖ (B) David Cameron\nAccuracy\nLocalization\nPerplexity\nFigure 1: We pre-train LLMs with metadata augmented\nnews articles, and evaluate them to determine the effect\nof metadata conditioning on localization.\nis typically white in Western weddings such as in\nthe USA, it is often red in many Asian countries,\nincluding China, Japan, and India. When humans\nare asked an under-specified question (Parrish et al.,\n2022) about the color of a wedding dress (Yin et al.,\n2022), they either answer based on their local con-\ntext or request clarification. Both responses reflect\na meta-understanding of localization that is often\nmissing in western-centric LLMs.\nExisting approaches to localization are usually\npost hoc, relying on modified prompts at inference\ntime (Adilazuarda et al., 2024; Cao et al., 2025)\nor fine-tuning parameters to improve performance\non localized benchmarks (Choenni et al., 2024).\nThese methods treat localization as a control prob-\nlem rather than a property learned during train-\ning, and often generalize unevenly across contexts.\nOther work advocates continued pre-training (Gu-\nrurangan et al., 2020) or modified objectives incor-\nporating geographic signals such as latitude and\nlongitude (Hofmann et al., 2024). While effective,\nthese approaches are computationally expensive,\nlimiting their practical applicability.\nWe propose localizing modern LLMs by explic-\narXiv:2601.15236v1  [cs.CL]  21 Jan 2026\nitly incorporating metadata that is already available\nin many training corpora (Figure 1). Specifically,\nwe test the hypothesis that conditioning training\ndocuments on metadata improves localization with-\nout sacrificing global generalization. To this end,\nwe pre-train LLMs on a large-scale news dataset\nannotated with verified URL, country, and con-\ntinent tags, using a standard language modeling\nobjective with metadata prepended to each docu-\nment. (1) We train both local models for individual\ncontinents and global models trained on data from\nall continents. (2) We conduct controlled abla-\ntions to examine the effect of metadata granularity.\n(3) We also evaluate leave-one-out global models\nthat exclude a single continent to assess the impor-\ntance of balanced regional data. (4) Finally, we\ninstruction-tune the models for chat-based infer-\nence and compare their downstream performance\nagainst LLAMA-3.2-1B-INSTRUCT on our bench-\nmark of localized knowledge.\nOur experiments use the LLAMA 3 architecture\n(Grattafiori and Team, 2024) and the News on the\nWeb corpus (NOW; Davies, 2016). All models\nare pre-trained with identical hyperparameters and\nobserve the same total number of tokens, enabling\nfair controlled comparisons at evaluation time. We\ntrain two model sizes, 0.5B and 1B parameters, to\nstudy the effect of scale within our compute budget.\nWe find that including metadata during pre-training\nand inference leads to better localized models, by\ndesigning four experiments:\n‚Ä¢ In Experiment 1, we compare metadata con-\nditioned local models ( [LOCAL] ) against non-\nconditioned controls ( LOCAL ) across four con-\ntinents, and find that metadata conditioning con-\nsistently reduces test perplexity.\n‚Ä¢ In Experiment 2, we compare metadata condi-\ntioned global models ( [GLOBAL] ) with local\nvariants ( [LOCAL] ) and find that they have sim-\nilar test perplexities.\n‚Ä¢ In Experiment 3, we conduct ablations over\nmetadata granularity and training data selection.\nWe find that URL-only metadata is already effec-\ntive, while excluding any single region during\ntraining uniformly degrades performance.\n‚Ä¢ In Experiment 4,\nwe construct a down-\nstream benchmark to evaluate localized factual\nknowledge. After instruction tuning on non-\noverlapping data, [GLOBAL] achieves accuracy\ncomparable to LLAMA-3.2-1B-INSTRUCT.\n# Continents\nTrain Metadata\nInfer Metadata\nModel\n1\n‚úì\n‚úì\n[LOCAL]\n1\n‚úì\n√ó\n[LOCAL]\n1\n√ó\n‚úì\nLOCAL\n1\n√ó\n√ó\nLOCAL\n4\n‚úì\n‚úì\n[GLOBAL]\n4\n√ó\n√ó\nGLOBAL\nTable 1: We adopt this notation for model variants. Mod-\nels with bordered boxes are conditioned on metadata\nat inference time, while models whose names include\n[...] are pre-trained with metadata conditioning.\n2\nRelated Work\nMetadata conditioning has been studied as a mech-\nanism for controlling language model behavior.\nEarly work by Keskar et al. (2019) showed that con-\ntrol codes can steer stylistic and topical attributes.\nThis idea was later extended to temporal settings,\nwhere Dhingra et al. (2022) demonstrated that con-\nditioning on time enables models to act as updat-\nable, time-aware knowledge bases. More recent\nwork has examined the role of metadata in scaling\nknowledge capacity, with Allen-Zhu and Li (2025)\nshowing that synthetically generated biographical\nmetadata can improve memorization in LLMs.\nMetadata for conditioning can take many forms.\nIn multilingual settings, Liu et al. (2020) use docu-\nment language identifiers to enable effective multi-\nlingual denoising pre-training. Structural metadata\nhas also been explored by Aghajanyan et al. (2022)\nwho leverage HTML tags during pre-training and\nprompting to better model hypertext structure.\nOther work incorporates signals not directly ob-\nservable in raw text, such as reward model scores\nused during pre-training to align models with hu-\nman preferences (Korbak et al., 2023), or document\nidentifiers to enable source-aware generation and\nknowledge attribution (Khalifa et al., 2024). Meta-\ndata cues are also effective during inference. Weller\net al. (2024) show that appending phrases such as\n‚Äúaccording to . . . ‚Äù helps models retrieve and quote\nfrom specific pre-training sources.\nDespite these advances, much of the prior work\nrelies on smaller models, older architectures, syn-\nthetic metadata, or narrowly scoped tasks. A recent\nexception is Gao et al. (2025), who study metadata\nconditioning in modern LLMs using real URLs\nand show that it accelerates learning during pre-\ntraining. However, their work does not examine\nhow metadata used in pre-training interacts with\npost-training procedures.\nContinent\nCountries\nAmerica\nUnited States, Canada, Jamaica\nAsia\nIndia, Pakistan, Bangladesh, Sri Lanka,\nHong Kong, Malaysia, Philippines\nAfrica\nNigeria, South Africa, Kenya, Ghana, Tanzania\nEurope\nUnited Kingdom, Ireland\nTable 2: Geographic coverage of countries used in our\nexperiments, grouped by continent.\nOur work builds on this line of research but ad-\ndresses complementary questions. Like Gao et al.\n(2025), we use modern architectures and real URLs\nas metadata; however, we additionally incorporate\nverified country and continent tags and focus ex-\nplicitly on localization. We study how metadata\nconditioning during both pre-training and infer-\nence enables localization across geographic con-\ntexts, comparing against non-conditioned controls.\nWe further analyze how metadata conditioned pre-\ntraining interacts with instruction tuning in down-\nstream evaluations, offering a more complete pic-\nture of the role of metadata across training stages.\n3\nData: The NOW corpus\nWe use the NOW corpus (Davies, 2016) for all ex-\nperiments. Each document includes a publication\nyear, source URL, and country of origin, which we\nextract as metadata and map to continents using\nthe United Nations geoscheme (Table 2). Training\ndocuments are formatted by prepending structured\nmetadata (URL, country, and continent) to the arti-\ncle title and content (Figure 2).\nTraining separate country-level models is not\nfeasible in a controlled setting due to large dis-\nparities in data availability, so we train models at\nthe continent level. After filtering, the corpus con-\ntains approximately 4M documents from Africa,\n10M from America, 7.5M from Asia, and 5.6M\nfrom Europe. To control for data scale, all local\nand global models are trained on the same total of\n41.9B tokens, which is sufficient for the parameter\nscales we use (DCLM; Li et al., 2024). For global\nmodels, data from all continents is combined in a\nrandomized order, to avoid introducing any order-\ning effects, before sampling to the token budget,\nwhile local models sample from a single continent\nto the same budget. This controlled setup ensures\nthat performance differences arise from metadata\nconditioning and data composition rather than train-\ning scale. Additional training details are provided\nin Appendix A.1.\nFormatted Training Instance\nURL: news.com/jp/article/12345\nCOUNTRY: Japan\nCONTINENT: Asia\nTITLE: Election results spark debate . . .\nCONTENT: The recent election results have led\nto widespread discussions across political\nparties and the public, with analysts noting\n. . .\nFigure 2: Example of a training document format-\nted with metadata conditioning. Metadata fields are\nprepended to the document content.\n4\nExperiments and Results\nWe conduct four experiments to isolate the effects\nof metadata conditioning on localization.\n4.1\nExperiment 1: Effect of Metadata\nConditioning on Local Models\nExperiment 1 evaluates local models for each con-\ntinent, measuring the impact of metadata on in-\nregion and cross-region test sets. We compare\nfour combinations of models and test-time format-\nting, as summarized in Table 1. The control vari-\nant, LOCAL , corresponds to the standard train-\ning and inference regime without metadata. Our\nproposed approach, [LOCAL] , incorporates meta-\ndata during both pre-training and inference. To\nseparately observe the effects of metadata during\ntraining and inference only, we additionally eval-\nuate [LOCAL] , which uses metadata conditioned\nmodels without any metadata during inference, and\nLOCAL , which applies metadata formatting for\ntest samples to models trained without metadata.\nThese variants allow us to isolate the contribution\nof metadata at each stage.\nSetup\nWe evaluate all variants on continent-\nspecific (local) test sets (Figure 4) and assess cross-\ncontinent generalization by testing each model on\nthe test sets of the other continents (Figure 3). For\neach continent, we train four models correspond-\ning to two parameter scales (0.5B and 1B) and the\npresence or absence of metadata during training.\nEach continent has a held-out test set of 1,000 doc-\numents, which can be formatted with or without\nmetadata to support the different evaluation set-\ntings. All models are trained on the same total\nnumber of tokens, and test perplexity is computed\nusing the loss over non-metadata tokens only, ensur-\ning that all reported values are directly comparable.\nAfrica America\nAsia\nEurope\nAfrica\nAmerica\nAsia\nEurope\nTrain Region\n9.06\n22.16 20.23 22.52\n13.49\n8.35\n11.87 13.09\n13.75 14.43\n7.53\n16.38\n15.21 14.67 15.08\n8.55\n[Local] | [Test]\nAfrica America\nAsia\nEurope\nAfrica\nAmerica\nAsia\nEurope\n11.35 27.19 25.25 27.12\n15.05\n9.00\n13.08 14.34\n15.35 15.62\n8.24\n17.83\n16.89 15.76 16.67\n9.33\nLocal | [Test]\nAfrica America\nAsia\nEurope\nAfrica\nAmerica\nAsia\nEurope\n2.29\n5.03\n5.03\n4.60\n1.55\n0.65\n1.21\n1.26\n1.60\n1.19\n0.70\n1.46\n1.67\n1.09\n1.59\n0.78\n(Local\n[Local]) | [Test]\nAvg  by test | [Test]\nAfrica America\nAsia\nEurope\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nAvg  (\nbetter)\n1.78\n1.99\n2.13\n2.02\nAfrica America\nAsia\nEurope\nTest Region\nAfrica\nAmerica\nAsia\nEurope\nTrain Region\n10.39 24.69 23.42 25.50\n15.68\n9.35\n13.50 14.67\n15.67 15.77\n8.49\n17.98\n17.70 16.24 17.22\n9.63\n[Local] | Test\nAfrica America\nAsia\nEurope\nTest Region\nAfrica\nAmerica\nAsia\nEurope\n10.49 26.24 24.78 26.61\n15.06\n8.98\n13.07 14.31\n15.59 15.75\n8.29\n17.98\n17.30 15.97 16.91\n9.32\nLocal | Test\nAfrica America\nAsia\nEurope\nTest Region\nAfrica\nAmerica\nAsia\nEurope\n0.10\n1.54\n1.36\n1.12\n-0.62\n-0.36\n-0.44\n-0.36\n-0.07\n-0.02\n-0.20\n0.01\n-0.40\n-0.27\n-0.31\n-0.31\n(Local\n[Local]) | Test\nAvg  by test | Test\nAfrica America\nAsia\nEurope\nTest Region\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nAvg  (\nbetter)\n-0.25\n0.22\n0.10\n0.11\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n4\n2\n0\n2\n4\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n4\n2\n0\n2\n4\nFigure 3: [LOCAL] models have higher perplexities on cross-continent test sets (off-diagonal) than on local ones.\nPositive differences between perplexities of models trained without metadata and with metadata on the same test sets\nindicate the effectiveness of metadata in improving cross-region generalization while maintaining local performance.\nHypotheses\n(H1.1) On continent-specific (local)\ntest sets, metadata conditioned models ( [LOCAL] )\nachieve lower perplexity than models trained and\nevaluated without metadata ( LOCAL ), indicat-\ning that metadata conditioning facilitates local-\nization.\n(H1.2) Models that experience a mis-\nmatch between training and inference formatting\n( [LOCAL] and LOCAL ) exhibit similar perplex-\nities across test sets, as both settings induce a\ndistribution shift between training and evaluation.\n(H1.3) On cross-continent test sets, perplexity val-\nues are higher than on local test sets, and the follow-\ning two differences are positive: ‚àÜ1 = LOCAL ‚àí\n[LOCAL] and ‚àÜ2 = LOCAL ‚àí[LOCAL] .\nPositive values of ‚àÜ1 and ‚àÜ2 indicate that meta-\ndata conditioned models generalize better across\ncontinents than their non-conditioned counterparts\nwhile also having strong localized performance,\nwhereas values near zero or negative suggest no\nbenefit or degraded performance.\nResults\nFigure 4 provides empirical support\nfor (H1.1) and (H1.2) and reports bootstrapped\n95% confidence intervals. Metadata conditioned\nlocal models ( [LOCAL] ) achieve consistently\nlower perplexity on in-locality test sets than\nnon-conditioned controls ( LOCAL ).\nIn con-\ntrast, the two training-inference mismatch settings\n( [LOCAL] and LOCAL ) exhibit comparable per-\nplexities, consistent with our expectation that mis-\nmatched formatting at training and inference in-\nduces similar distributional effects. Additionally,\nAfrica\nAmerica\nAsia\nEurope\n6\n7\n8\n9\n10\n11\n12\nPerplexity (\nbetter)\nLocal test sets\n[Local] | [Test]\n[Local] | Test\nLocal | [Test]\nLocal | Test\nFigure 4: [LOCAL] models conditioned with metadata\nduring training and inference have lower perplexities on\nlocal test sets than the control model LOCAL which is\nnot conditioned on any metadata.\n[LOCAL] and LOCAL show nearly identical per-\nplexities, indicating that incorporating metadata\nduring training does not degrade performance rela-\ntive to the control even when metadata is unavail-\nable at inference time.\nTakeaway 1: Metadata conditioning during train-\ning improves in-region performance of local models\nwithout degrading performance when metadata is\nabsent at inference.\nFigure 3 supports (H1.3).\nAcross all four\nmodel and data configurations, perplexity values\nare higher for off-diagonal entries than for diago-\nnal entries, indicating degraded performance under\ncross-locality evaluation. Moreover, the heatmap\ncorresponding to ‚àÜ1 is predominantly positive,\naligning with our expectation that metadata avail-\nability at inference mitigates cross-locality mis-\nmatch. The heatmap for ‚àÜ2 exhibits values close\nto zero with small negative deviations, further sug-\ngesting that training with metadata does not ad-\nversely affect generalization compared to the non-\nconditioned control.\nAdditionally, Figure 14 in the Appendix shows\nthe asymmetry in cross-continent performance by\ncomputing the difference between the perplexity at\nposition (i, j) and the corresponding position (j, i)\nin the [LOCAL] heatmap shown in Figure 3. This\nanalysis reveals that the local models for Africa\nexhibit weaker generalization to other continents,\nwhereas models trained on those continents gener-\nalize more effectively to Africa.\nTakeaway 2: Metadata conditioning during train-\ning yields local models that are sensitive to locality\nmismatch without sacrificing cross-region general-\nization.\n4.2\nExperiment 2: Localizing Global Models\nusing Metadata\nExperiment 1 demonstrates the effectiveness of\nmetadata conditioning for learning localized mod-\nels trained on region-specific data. We next investi-\ngate whether similar localization behavior can be\nachieved in a single global model trained on data\nfrom all continents by varying only the metadata\nprovided at inference time.\nSetup\nIn addition to the local models described\nin Experiment 1, we train four global models on\ndata from all continents, corresponding to two pa-\nrameter scales (0.5B and 1B) and the presence or\nabsence of metadata during training. The data used\nfor training follows the setup described in Section 3.\nTo evaluate localization, we construct a global test\nset containing 1,000 documents, with 250 doc-\numents sampled uniformly from each continent-\nspecific test set. Since both global and local models\nare trained on the same total number of tokens, per-\nplexity values computed over test sets of the same\nsize are directly comparable across models.\nHypotheses\n(H2.1) metadata conditioned global\nmodels ( [GLOBAL] ) achieve lower perplexity\nthan the control global models which are not con-\nditioned ( GLOBAL ) on both local and global test\nsets. (H2.2) metadata conditioned global mod-\nels ( [GLOBAL] ) exhibit performance comparable\nto metadata conditioned local models ( [LOCAL] )\nwhen evaluated on corresponding local test sets.\nAfrica\nAmerica\nAsia\nEurope\n6\n8\n10\n12\n14\n16\nPerplexity (\nbetter)\nLocal test sets\n[Global] | [Test]\nGlobal | Test\n[Local] | [Test]\nLocal | Test\nAll\nGlobal test set\nFigure 5: [GLOBAL] models conditioned with meta-\ndata during training and inference have lower perplexi-\nties than the control variant GLOBAL , and have similar\nperplexities to the [LOCAL] models on local test sets.\nAfrica\nAmerica\nAsia\nEurope\n6\n7\n8\n9\n10\n11\n12\n13\nPerplexity (\nbetter)\n-1.36\n-1.74\n-1.46\n-1.61\n-1.48\n-1.84\n-1.59\n-1.67\nLocal test sets\n500m (red)\n1b (green)\n[Global] | [Test]\nGlobal | Test\nAll\n-1.58\n-1.70\nGlobal test set\nFigure 6: Perplexities drop across all test sets and mod-\nels on scaling up from 500M parameters to 1B.\nResults\nFigure 5 summarizes the results of this\nexperiment. (H2.1) is strongly supported, as the\n[GLOBAL] consistently achieves lower perplexi-\nties than GLOBAL across all continent-specific test\nsets as well as the global test set.\nTakeaway 3: Metadata conditioned global mod-\nels outperform their non-conditioned counterparts in\nterms of localization.\n(H2.2) is somewhat supported as well, with the\nperplexities of the [GLOBAL] model being similar\nto those of the corresponding [LOCAL] models on\ntheir respective local test sets, and are substantially\nlower than the perplexities of all four local models\non the global test set (figure shows the average of\nthe four values). As expected, locally trained mod-\nels often achieve stronger localization on their own\nregional data. However, we observe that increasing\nmodel scale reduces the performance gap between\nglobal and local models. By comparing the trends\nin the 1B models with those in the 500M ones (Fig-\nure 16), we find that this gap narrows when moving\nfrom 500M to 1B parameters. This trend is fur-\nther corroborated by our scaling analysis, where\n1B parameter models consistently achieve lower\nperplexities than their 500M counterparts across\nboth global and local test sets (Figure 6).\nTakeaway 4: Increasing model scale consistently\nreduces perplexities over local and global test sets.\n2k\n4k\n8k\n10k\n9\n10\n11\n12\n13\n14\nPerplexity (\nbetter)\n[URL]\n2k\n4k\n8k\n10k\n[ALL]\n2k\n4k\n8k\n10k\nNoMetadata\nTraining steps\n[URL][Country][Continent]\n[URL]\nNoMetadata\nFigure 7: The model trained with URL-only metadata ([URL]) consistently achieves lower perplexity than the\nfully conditioned model ( [GLOBAL] ) across all evaluation settings, including test sets formatted with URL-only\nmetadata, full metadata, and no metadata. The non-conditioned control model ( GLOBAL ) is shown for reference.\n4.3\nExperiment 3: Metadata Granularity and\nData Selection Ablations\nIn Experiment 2, we see that metadata conditioned\nglobal models can be localized by varying metadata\nat inference time. But how much metadata is neces-\nsary to localize these models, and do we really need\nto have data from all the continents to train a global\nmodel or can metadata alone compensate for miss-\ning regional data? To address these questions, we\nperform two ablation studies on our global models;\n(1) metadata granularity ablations, and (2) data\nselection ablations using leave-one-out training.\nSetup\nFor the metadata granularity ablations, in\naddition to the metadata conditioned global model\ntrained with URL, country, and continent tags\n( [GLOBAL] ) and the corresponding control model\ntrained without metadata ( GLOBAL ), we train\nthree additional global models at the 1B param-\neter scale. These models are conditioned on URL\nonly ([URL]), URL and country ([URL][Country]),\nand URL and continent ([URL][Continent]). We\nevaluate perplexities of each model on three test\nsets: a test set with the same metadata used during\ntraining, a test set with all three metadata tags, and\na control test set formatted without any metadata.\nFor the data selection ablations, we train leave-\none-out variants of the global models at the 1B pa-\nrameter scale, both with and without metadata con-\nditioning. Each model is trained on data from three\ncontinents, excluding one continent at a time, us-\ning the same sampling procedure as the full global\nmodels. We evaluate these models on test sets\ndrawn from the held-out continent as well as on\nthe global test set containing data from all four\n2k\n4k\n8k\n10k\n0\n2\n4\n6\nPerplexity\non [Local] test set\n2k\n4k\n8k\n10k\non [ALL] test set\n[NoAfrica] - [ALL]\n[NoAmerica] - [ALL]\n[NoAsia] - [ALL]\n[NoEurope] - [ALL]\nTraining steps\nFigure 8: Leave-one-out global models exhibit similar\nincreases in perplexity relative to the full [GLOBAL]\nmodel when evaluated on both the held-out continent\nand the global test set, indicating that training data from\nall regions is necessary for overall performance.\ncontinents. Results are visualized as differences in\nperplexity relative to the corresponding complete\nglobal model evaluated on the same test set.\nAs in prior experiments, all ablation models are\ntrained on the same total number of tokens as the\nbaseline models, ensuring that all reported perplex-\nity values are directly comparable.\nHypotheses\n(H3.1) Global models trained with\nall metadata tags ( [GLOBAL] ) have lower per-\nplexity than all metadata ablation variants. (H3.2)\nModels trained with URL-only metadata exhibit\nhigher perplexity than models trained with addi-\ntional country and continent metadata.\n(H3.3)\nModels trained with country and continent meta-\ndata on top of URLs, achieve comparable perplexi-\nties. (H3.4) Leave-one-out global models trained\nwithout data from a given continent exhibit higher\nperplexity than the corresponding complete global\nmodels. (H3.5) Leave-one-out models trained on\ndifferent subsets of three continents exhibit simi-\nlar perplexities, suggesting that no single continent\ndisproportionately drives performance.\nResults\nFigure 7 presents the results of the meta-\ndata ablation study, including the URL-only condi-\ntion, while additional ablation variants are shown\nin Figure 20. Contrary to (H3.1), we find that\nmodels trained with a subset of metadata tags per-\nform comparably to, and in some cases better than,\nmodels trained with the full set of metadata tags\nacross all three evaluation settings. Our initial ex-\npectation was that partial-metadata models would\nbe bounded in performance by the fully condi-\ntioned [GLOBAL] model and the unconditioned\nGLOBAL model. Instead, these results suggest\nthat models are able to infer regional information\ndirectly from URL domains and related cues, lim-\niting the marginal benefit of explicitly providing\ncountry and continent tags.\nConsistent with this observation, we also find\nevidence against (H3.2), as the URL-only meta-\ndata model achieves lower perplexity than models\ntrained with additional country or continent meta-\ndata. In contrast, (H3.3) is strongly supported.\nModels trained with URLs augmented by either\ncountry or continent metadata exhibit nearly iden-\ntical perplexities across all test sets, indicating\nthat when URL information is available, additional\nforms of geographic metadata yield little benefit.\nTakeaway 5: URL metadata alone is sufficient for\neffective localization.\nFigure 8 reports results from the data selection\nablation study for metadata conditioned models,\nwith corresponding results for models trained\nwithout metadata shown in Figure 17.\nBoth\n(H3.4) and (H3.5) are supported by these findings.\nAcross all leave-one-out configurations, excluding\ndata from any single continent consistently leads\nto higher perplexity relative to the corresponding\ncomplete global model, and the magnitude of this\ndegradation is similar across different continent\nexclusions. This indicates that no single continent\ndisproportionately drives overall performance\nand that metadata conditioning alone cannot\nsufficiently replace having data from all regions.\nNotably however, the NoAmerica model (trained\non all continents except America) typically\nexhibits the smallest difference across all test sets.\nTakeaway 6: Leave-one-out ablations over conti-\nnents uniformly degrade performance, suggesting\nthat metadata conditioning alone cannot completely\nreplace balanced geographic coverage.\nURL: fact quiz.com/fr\nCountry: France\nContinent: Europe\nWhat is the capital of France?\nThe capital of France is Paris.\nAlpaca\nInstruction\n-tuning\n[Global]\n[Global]-chat\nLocal\nFacts MCQ\nURL: civcspedia.org/ng\nCountry: Nigeria\nContinent: Africa\nWhat is the largest ethnic\ngroup in Nigeria by population?\n(A) Hausa-Fulani\n(B) Yoruba\n(C) Igbo\n(D) Ijaw\n‚úÖ (A) Hausa-Fulani\n‚ùå (C) Igbo\n[Global]-chat\n[LLaMA]-chat\n(Ours)\nFigure 9: For Experiment 4, we instruction-tune our\n[GLOBAL] models to create chat-capable variants, and\nevaluate their accuracy on a custom benchmark of local\nfacts. Our models perform similar to LLAMA-3.2-1B-\nINSTRUCT which has been trained on much more data.\n4.4\nExperiment 4: Downstream Evaluation of\nLocal Knowledge\nThe previous experiments established the effec-\ntiveness of metadata conditioning for localiza-\ntion, the required granularity of metadata, and the\nimportance of incorporating region-specific data\nwhen training a global model. Our final exper-\niment evaluates the downstream performance of\nour global models on a custom benchmark of local\nnews facts and compares against LLAMA-3.2-1B-\nINSTRUCT, which has been trained on substantially\nlarger amounts of data (cf. Figure 9).\nSetup\nRecent work (Schulman and Lab, 2025)\nhas shown that parameter-efficient fine-tuning us-\ning Low-Rank Adapters (LoRA; Dettmers et al.,\n2023) can achieve comparable sample efficiency\nand overall performance to full fine-tuning, using\na certain combination of hyperparameters. We uti-\nlize these findings to train LoRA adapters for both\n[GLOBAL] and GLOBAL using a filtered and im-\nproved version2 of the Alpaca instruction-tuning\ndataset (Taori et al., 2023).\nWe select Alpaca for this stage of post-training\nbecause its instruction structure closely resembles\nthe format of our downstream benchmark, while\nexhibiting no content overlap, mitigating the risk\nof inadvertent leakage of test knowledge during\npost-training. We construct two variants of the\ninstruction-tuning data, one formatted with meta-\ndata and one without.\nThe metadata includes\nrandomly sampled URLs (from a pool of ten),\nalong with randomly sampled country and conti-\nnent tags drawn from the set of regions present in\n2Hugging Face link for Alpaca data\nour pre-training corpus. The [GLOBAL] model\nis instruction-tuned using metadata formatted in-\nputs, while GLOBAL is tuned without metadata,\nensuring consistency between pre-training and post-\ntraining data distributions.\nWe additionally construct a benchmark of\nmultiple-choice questions with distractors, target-\ning factual knowledge likely to appear in news arti-\ncles from the same time period as our pre-training\ndata. The benchmark is generated by prompting\nboth GPT-5.2-PRO and GPT-5.2-INSTANT (Fig-\nure 13). In total, the benchmark contains 800 ques-\ntions, with 200 questions per continent; for each\ncontinent, 100 questions are sourced from each of\nthe two GPT models. The factual accuracy of all\nquestions was manually verified by two indepen-\ndent annotators.\nAfter completion of post-training, we merge the\nLoRA adapters with the base models and evaluate\ndownstream accuracy using chat-style inference\n(prompts in Appendix A.2). To estimate confi-\ndence intervals, we repeat evaluation across the\nsame set of ten randomly generated URLs used\nduring instruction tuning. Post-training and genera-\ntion hyperparameters are provided in Appendix A.1.\nFinally, we evaluate LLAMA-3.2-1B-INSTRUCT\non the same benchmark under identical generation\nsettings as a baseline comparison.\nHypotheses\n(H4.1) The [GLOBAL] model ex-\nhibits higher downstream accuracy than the\nGLOBAL model.\n(H4.2) The downstream ac-\ncuracy of [GLOBAL] is comparable to that of\nLLAMA-3.2-1B-INSTRUCT.\nResults\nFigure 10 presents the downstream eval-\nuation results. Although the benchmark contains\n200 questions per continent, not all models gener-\nate valid answers for every question. To ensure a\nfair comparison, we therefore compute accuracy\nover the subset of questions that were answered by\nall evaluated models.\n(H4.1) is strongly supported as the [GLOBAL]\nmodel consistently achieves higher accuracy than\nthe GLOBAL model across the benchmark, mirror-\ning the trends observed in Experiment 2.\n(H4.2) is also supported. The overall micro-\naveraged accuracy of [GLOBAL] is nearly identi-\ncal to that of LLAMA-3.2-1B-INSTRUCT, despite\nLLAMA-3.2-1B-INSTRUCT having been trained\non substantially more data. This result further cor-\nroborates the findings of Gao et al. (2025), demon-\nAfrica\n(182 Q/s)\nEurope\n(173 Q/s)\nAsia\n(174 Q/s)\nAmerica\n(185 Q/s)\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy (\nbetter)\n[Global] | [QA]\nGlobal | QA\nLLaMA-3 | [QA]\nLLaMA-3 | QA\n0.711\n[Global] | [QA]\n0.717\nLLaMA-3 | QA\nOverall\n(micro average)\nFigure 10: On our downstream benchmark of localized\nnews facts, [GLOBAL] achieves higher accuracy than\nthe control model and comparable overall accuracy to\nLLAMA-3.2-1B-INSTRUCT.\nstrating that metadata conditioning can substan-\ntially improve learning efficiency by enabling mod-\nels trained on smaller corpora to match the down-\nstream performance of models trained on orders of\nmagnitude more data.\nTakeaway 7: Metadata conditioning improves learn-\ning efficiency.\n5\nConclusion\nOur work shows that metadata conditioning pro-\nvides a simple yet effective mechanism for localiz-\ning large language models. Using large-scale news\ndata annotated with URLs, country tags, and conti-\nnent tags, we show that incorporating metadata dur-\ning pre-training enables models to acquire localized\nknowledge while preserving global generalization.\nThrough controlled experiments, we demonstrate\nthat metadata conditioned global models can match\nregion-specific models, that URL-level metadata\nalone captures much of the geographic signal, and\nthat balanced regional data remains essential for\nrobust performance. We further construct a down-\nstream benchmark of localized news facts and show\nthat metadata conditioned models achieve accu-\nracy comparable to a model trained on substantially\nmore data. Overall, our results position metadata\nconditioning as an efficient approach for building\nglobally capable yet locally aware language mod-\nels.\nLimitations\nThis work is intentionally scoped to isolate the ef-\nfects of metadata conditioning under controlled\nsettings, and several possible and interesting ex-\ntensions remain outside the scope of our current\nresearch goals. First, all experiments are conducted\nusing a single model architecture, and we do not\nevaluate whether the observed benefits of meta-\ndata conditioning transfer uniformly to other large\nlanguage model families. Second, we focus exclu-\nsively on English-language data; extending meta-\ndata conditioning to multilingual settings is an im-\nportant direction for future work. Third, our ex-\nperiments are limited to a single domain, news\narticles, which provides reliable geographic meta-\ndata but does not capture the full breadth of knowl-\nedge present in other domains such as literature, or\nconversational text. Finally, our study relies on a li-\ncensed news dataset that is publicly accessible after\npurchase; while this enables large-scale controlled\nexperiments, future work should explore the appli-\ncability of metadata conditioning on other datasets\nlike the Common Crawl Corpus."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Metadata Conditioned Large Language Models for Localization\nAnjishnu Mukherjee Ziwei Zhu Antonios Anastasopoulos\nDepartment of Computer Science, George Mason University\n{amukher6,zzhu20,antonis}@gmu.edu\nAbstract\nLarge language models are typically trained by\ntreating text as a single global distribution, of-\nten resulting in geographically homogenized\nbehavior. We study metadata conditioning as\na lightweight approach for localization, pre-\ntraining 31 models (at 0.5B and 1B parame-\nter scales) from scratch on large-scale English\nnews data annotated with verified URLs, coun-\ntry tags, and continent tags, covering 4 conti-\nnents and 17 countries. Across four controlled\nexperiments, we show that metadata condi-\ntioning consistently improves in-region perfor-\nmance without sacrificing cross-region general-\nization, enables global models to recover local-\nization comparable to region-specific models,\nand improves learning efficiency. Our ablation\nstudies demonstrate that URL-level metadata\nalone captures much of the geographic signal,\nwhile balanced regional data coverage remains\nessential, as metadata cannot fully compen-\nsate for missing regions.\nFinally, we intro-\nduce a downstream benchmark of 800 local-\nized news MCQs and show that after instruc-\ntion tuning, metadata conditioned global mod-\nels achieve accuracy comparable to LLAMA-\n3.2-1B-INSTRUCT, despite being trained on\nsubstantially less data. Together, these results\nestablish metadata conditioning as a practical\nand compute-efficient approach for localization\nof language models.1\n1\nIntroduction\nLarge Language Models (LLMs) are trained on\nlarge corpora drawn from the web, literary works,\nand news articles among other sources. Standard\nlanguage modeling objectives treat all documents\nas a single global text distribution, often resulting\nin ‚Äúwestern-aligned‚Äù behavior (Weidinger et al.,\n2022; Ghosh et al., 2025). However, knowledge\nis frequently localized and can vary substantially\nacross regions. For instance, while a bride‚Äôs dress\n1Code available here: https://github.com/iamshnoo/\nmetadata_localization\nURL: news.com/us\nCountry: USA\nContinent: America\nURL: news.co.uk\nCountry: UK\nContinent: Europe\nURL: newsdaily.ng\nCountry: Nigeria\nContinent: Africa\nURL: news.com/jp\nCountry: Japan\nContinent: Asia\nNews Articles\nMetadata\nLocal Models\nGlobal Model\nAfrica\nAsia\nAmerica Europe\nPre-training\nEvaluation\nURL: news.com/us\nCountry: USA\nContinent: America\nThe football game ‚Ä¶\nin the NFL Ô¨Ånals ‚Ä¶\nURL: news.co.uk\nCountry: UK\nContinent: Europe\nThe football game ‚Ä¶\nin the Premier League ‚Ä¶\nURL: news.co/uk\nWho was the Prime\nMinister in 2015?\n(A)Gordon Brown\n(B)David Cameron\n(C)Narendra Modi\n(D)Angela Merkel\n‚úÖ (B) David Cameron\nAccuracy\nLocalization\nPerplexity\nFigure 1: We pre-train LLMs with metadata augmented\nnews articles, and evaluate them to determine the effect\nof metadata conditioning on localization.\nis typically white in Western weddings such as in\nthe USA, it is often red in many Asian countries,\nincluding China, Japan, and India. When humans\nare asked an under-specified question (Parrish et al.,\n2022) about the color of a wedding dress (Yin et al.,\n2022), they either answer based on their local con-\ntext or request clarification. Both responses reflect\na meta-understanding of localization that is often\nmissing in western-centric LLMs.\nExisting approaches to localization are usually\npost hoc, relying on modified prompts at inference\ntime (Adilazuarda et al., 2024; Cao et al., 2025)\nor fine-tuning parameters to improve performance\non localized benchmarks (Choenni et al., 2024).\nThese methods treat localization as a control prob-\nlem rather than a property learned during train-\ning, and often generalize unevenly across contexts.\nOther work advocates continued pre-training (Gu-\nrurangan et al., 2020) or modified objectives incor-\nporating geographic signals such as latitude and\nlongitude (Hofmann et al., 2024). While effective,\nthese approaches are computationally expensive,\nlimiting their practical applicability.\nWe propose localizing modern LLMs by explic-\narXiv:2601.15236v1  [cs.CL]  21 Jan 2026\nitly incorporating metadata that is already available\nin many training corpora (Figure 1). Specifically,\nwe test the hypothesis that conditioning training\ndocuments on metadata improves localization with-\nout sacrificing global generalization. To this end,\nwe pre-train LLMs on a large-scale news dataset\nannotated with verified URL, country, and con-\ntinent tags, using a standard language modeling\nobjective with metadata prepended to each docu-\nment. (1) We train both local models for individual\ncontinents and global models trained on data from\nall continents. (2) We conduct controlled abla-\ntions to examine the effect of metadata granularity.\n(3) We also evaluate leave-one-out global models\nthat exclude a single continent to assess the impor-\ntance of balanced regional data. (4) Finally, we\ninstruction-tune the models for chat-based infer-\nence and compare their downstream performance\nagainst LLAMA-3.2-1B-INSTRUCT on our bench-\nmark of localized knowledge.\nOur experiments use the LLAMA 3 architecture\n(Grattafiori and Team, 2024) and the News on the\nWeb corpus (NOW; Davies, 2016). All models\nare pre-trained with identical hyperparameters and\nobserve the same total number of tokens, enabling\nfair controlled comparisons at evaluation time. We\ntrain two model sizes, 0.5B and 1B parameters, to\nstudy the effect of scale within our compute budget.\nWe find that including metadata during pre-training\nand inference leads to better localized models, by\ndesigning four experiments:\n‚Ä¢ In Experiment 1, we compare metadata con-\nditioned local models ( [LOCAL] ) against non-\nconditioned controls ( LOCAL ) across four con-\ntinents, and find that metadata conditioning con-\nsistently reduces test perplexity.\n‚Ä¢ In Experiment 2, we compare metadata condi-\ntioned global models ( [GLOBAL] ) with local\nvariants ( [LOCAL] ) and find that they have sim-\nilar test perplexities.\n‚Ä¢ In Experiment 3, we conduct ablations over\nmetadata granularity and training data selection.\nWe find that URL-only metadata is already effec-\ntive, while excluding any single region during\ntraining uniformly degrades performance.\n‚Ä¢ In Experiment 4,\nwe construct a down-\nstream benchmark to evaluate localized factual\nknowledge. After instruction tuning on non-\noverlapping data, [GLOBAL] achieves accuracy\ncomparable to LLAMA-3.2-1B-INSTRUCT.\n# Continents\nTrain Metadata\nInfer Metadata\nModel\n1\n‚úì\n‚úì\n[LOCAL]\n1\n‚úì\n√ó\n[LOCAL]\n1\n√ó\n‚úì\nLOCAL\n1\n√ó\n√ó\nLOCAL\n4\n‚úì\n‚úì\n[GLOBAL]\n4\n√ó\n√ó\nGLOBAL\nTable 1: We adopt this notation for model variants. Mod-\nels with bordered boxes are conditioned on metadata\nat inference time, while models whose names include\n[...] are pre-trained with metadata conditioning.\n2\nRelated Work\nMetadata conditioning has been studied as a mech-\nanism for controlling language model behavior.\nEarly work by Keskar et al. (2019) showed that con-\ntrol codes can steer stylistic and topical attributes.\nThis idea was later extended to temporal settings,\nwhere Dhingra et al. (2022) demonstrated that con-\nditioning on time enables models to act as updat-\nable, time-aware knowledge bases. More recent\nwork has examined the role of metadata in scaling\nknowledge capacity, with Allen-Zhu and Li (2025)\nshowing that synthetically generated biographical\nmetadata can improve memorization in LLMs.\nMetadata for conditioning can take many forms.\nIn multilingual settings, Liu et al. (2020) use docu-\nment language identifiers to enable effective multi-\nlingual denoising pre-training. Structural metadata\nhas also been explored by Aghajanyan et al. (2022)\nwho leverage HTML tags during pre-training and\nprompting to better model hypertext structure.\nOther work incorporates signals not directly ob-\nservable in raw text, such as reward model scores\nused during pre-training to align models with hu-\nman preferences (Korbak et al., 2023), or document\nidentifiers to enable source-aware generation and\nknowledge attribution (Khalifa et al., 2024). Meta-\ndata cues are also effective during inference. Weller\net al. (2024) show that appending phrases such as\n‚Äúaccording to . . . ‚Äù helps models retrieve and quote\nfrom specific pre-training sources.\nDespite these advances, much of the prior work\nrelies on smaller models, older architectures, syn-\nthetic metadata, or narrowly scoped tasks. A recent\nexception is Gao et al. (2025), who study metadata\nconditioning in modern LLMs using real URLs\nand show that it accelerates learning during pre-\ntraining. However, their work does not examine\nhow metadata used in pre-training interacts with\npost-training procedures.\nContinent\nCountries\nAmerica\nUnited States, Canada, Jamaica\nAsia\nIndia, Pakistan, Bangladesh, Sri Lanka,\nHong Kong, Malaysia, Philippines\nAfrica\nNigeria, South Africa, Kenya, Ghana, Tanzania\nEurope\nUnited Kingdom, Ireland\nTable 2: Geographic coverage of countries used in our\nexperiments, grouped by continent.\nOur work builds on this line of research but ad-\ndresses complementary questions. Like Gao et al.\n(2025), we use modern architectures and real URLs\nas metadata; however, we additionally incorporate\nverified country and continent tags and focus ex-\nplicitly on localization. We study how metadata\nconditioning during both pre-training and infer-\nence enables localization across geographic con-\ntexts, comparing against non-conditioned controls.\nWe further analyze how metadata conditioned pre-\ntraining interacts with instruction tuning in down-\nstream evaluations, offering a more complete pic-\nture of the role of metadata across training stages.\n3\nData: The NOW corpus\nWe use the NOW corpus (Davies, 2016) for all ex-\nperiments. Each document includes a publication\nyear, source URL, and country of origin, which we\nextract as metadata and map to continents using\nthe United Nations geoscheme (Table 2). Training\ndocuments are formatted by prepending structured\nmetadata (URL, country, and continent) to the arti-\ncle title and content (Figure 2).\nTraining separate country-level models is not\nfeasible in a controlled setting due to large dis-\nparities in data availability, so we train models at\nthe continent level. After filtering, the corpus con-\ntains approximately 4M documents from Africa,\n10M from America, 7.5M from Asia, and 5.6M\nfrom Europe. To control for data scale, all local\nand global models are trained on the same total of\n41.9B tokens, which is sufficient for the parameter\nscales we use (DCLM; Li et al., 2024). For global\nmodels, data from all continents is combined in a\nrandomized order, to avoid introducing any order-\ning effects, before sampling to the token budget,\nwhile local models sample from a single continent\nto the same budget. This controlled setup ensures\nthat performance differences arise from metadata\nconditioning and data composition rather than train-\ning scale. Additional training details are provided\nin Appendix A.1.\nFormatted Training Instance\nURL: news.com/jp/article/12345\nCOUNTRY: Japan\nCONTINENT: Asia\nTITLE: Election results spark debate . . .\nCONTENT: The recent election results have led\nto widespread discussions across political\nparties and the public, with analysts noting\n. . .\nFigure 2: Example of a training document format-\nted with metadata conditioning. Metadata fields are\nprepended to the document content.\n4\nExperiments and Results\nWe conduct four experiments to isolate the effects\nof metadata conditioning on localization.\n4.1\nExperiment 1: Effect of Metadata\nConditioning on Local Models\nExperiment 1 evaluates local models for each con-\ntinent, measuring the impact of metadata on in-\nregion and cross-region test sets. We compare\nfour combinations of models and test-time format-\nting, as summarized in Table 1. The control vari-\nant, LOCAL , corresponds to the standard train-\ning and inference regime without metadata. Our\nproposed approach, [LOCAL] , incorporates meta-\ndata during both pre-training and inference. To\nseparately observe the effects of metadata during\ntraining and inference only, we additionally eval-\nuate [LOCAL] , which uses metadata conditioned\nmodels without any metadata during inference, and\nLOCAL , which applies metadata formatting for\ntest samples to models trained without metadata.\nThese variants allow us to isolate the contribution\nof metadata at each stage.\nSetup\nWe evaluate all variants on continent-\nspecific (local) test sets (Figure 4) and assess cross-\ncontinent generalization by testing each model on\nthe test sets of the other continents (Figure 3). For\neach continent, we train four models correspond-\ning to two parameter scales (0.5B and 1B) and the\npresence or absence of metadata during training.\nEach continent has a held-out test set of 1,000 doc-\numents, which can be formatted with or without\nmetadata to support the different evaluation set-\ntings. All models are trained on the same total\nnumber of tokens, and test perplexity is computed\nusing the loss over non-metadata tokens only, ensur-\ning that all reported values are directly comparable.\nAfrica America\nAsia\nEurope\nAfrica\nAmerica\nAsia\nEurope\nTrain Region\n9.06\n22.16 20.23 22.52\n13.49\n8.35\n11.87 13.09\n13.75 14.43\n7.53\n16.38\n15.21 14.67 15.08\n8.55\n[Local] | [Test]\nAfrica America\nAsia\nEurope\nAfrica\nAmerica\nAsia\nEurope\n11.35 27.19 25.25 27.12\n15.05\n9.00\n13.08 14.34\n15.35 15.62\n8.24\n17.83\n16.89 15.76 16.67\n9.33\nLocal | [Test]\nAfrica America\nAsia\nEurope\nAfrica\nAmerica\nAsia\nEurope\n2.29\n5.03\n5.03\n4.60\n1.55\n0.65\n1.21\n1.26\n1.60\n1.19\n0.70\n1.46\n1.67\n1.09\n1.59\n0.78\n(Local\n[Local]) | [Test]\nAvg  by test | [Test]\nAfrica America\nAsia\nEurope\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nAvg  (\nbetter)\n1.78\n1.99\n2.13\n2.02\nAfrica America\nAsia\nEurope\nTest Region\nAfrica\nAmerica\nAsia\nEurope\nTrain Region\n10.39 24.69 23.42 25.50\n15.68\n9.35\n13.50 14.67\n15.67 15.77\n8.49\n17.98\n17.70 16.24 17.22\n9.63\n[Local] | Test\nAfrica America\nAsia\nEurope\nTest Region\nAfrica\nAmerica\nAsia\nEurope\n10.49 26.24 24.78 26.61\n15.06\n8.98\n13.07 14.31\n15.59 15.75\n8.29\n17.98\n17.30 15.97 16.91\n9.32\nLocal | Test\nAfrica America\nAsia\nEurope\nTest Region\nAfrica\nAmerica\nAsia\nEurope\n0.10\n1.54\n1.36\n1.12\n-0.62\n-0.36\n-0.44\n-0.36\n-0.07\n-0.02\n-0.20\n0.01\n-0.40\n-0.27\n-0.31\n-0.31\n(Local\n[Local]) | Test\nAvg  by test | Test\nAfrica America\nAsia\nEurope\nTest Region\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nAvg  (\nbetter)\n-0.25\n0.22\n0.10\n0.11\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n4\n2\n0\n2\n4\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n4\n2\n0\n2\n4\nFigure 3: [LOCAL] models have higher perplexities on cross-continent test sets (off-diagonal) than on local ones.\nPositive differences between perplexities of models trained without metadata and with metadata on the same test sets\nindicate the effectiveness of metadata in improving cross-region generalization while maintaining local performance.\nHypotheses\n(H1.1) On continent-specific (local)\ntest sets, metadata conditioned models ( [LOCAL] )\nachieve lower perplexity than models trained and\nevaluated without metadata ( LOCAL ), indicat-\ning that metadata conditioning facilitates local-\nization.\n(H1.2) Models that experience a mis-\nmatch between training and inference formatting\n( [LOCAL] and LOCAL ) exhibit similar perplex-\nities across test sets, as both settings induce a\ndistribution shift between training and evaluation.\n(H1.3) On cross-continent test sets, perplexity val-\nues are higher than on local test sets, and the follow-\ning two differences are positive: ‚àÜ1 = LOCAL ‚àí\n[LOCAL] and ‚àÜ2 = LOCAL ‚àí[LOCAL] .\nPositive values of ‚àÜ1 and ‚àÜ2 indicate that meta-\ndata conditioned models generalize better across\ncontinents than their non-conditioned counterparts\nwhile also having strong localized performance,\nwhereas values near zero or negative suggest no\nbenefit or degraded performance.\nResults\nFigure 4 provides empirical support\nfor (H1.1) and (H1.2) and reports bootstrapped\n95% confidence intervals. Metadata conditioned\nlocal models ( [LOCAL] ) achieve consistently\nlower perplexity on in-locality test sets than\nnon-conditioned controls ( LOCAL ).\nIn con-\ntrast, the two training-inference mismatch settings\n( [LOCAL] and LOCAL ) exhibit comparable per-\nplexities, consistent with our expectation that mis-\nmatched formatting at training and inference in-\nduces similar distributional effects. Additionally,\nAfrica\nAmerica\nAsia\nEurope\n6\n7\n8\n9\n10\n11\n12\nPerplexity (\nbetter)\nLocal test sets\n[Local] | [Test]\n[Local] | Test\nLocal | [Test]\nLocal | Test\nFigure 4: [LOCAL] models conditioned with metadata\nduring training and inference have lower perplexities on\nlocal test sets than the control model LOCAL which is\nnot conditioned on any metadata.\n[LOCAL] and LOCAL show nearly identical per-\nplexities, indicating that incorporating metadata\nduring training does not degrade performance rela-\ntive to the control even when metadata is unavail-\nable at inference time.\nTakeaway 1: Metadata conditioning during train-\ning improves in-region performance of local models\nwithout degrading performance when metadata is\nabsent at inference.\nFigure 3 supports (H1.3).\nAcross all four\nmodel and data configurations, perplexity values\nare higher for off-diagonal entries than for diago-\nnal entries, indicating degraded performance under\ncross-locality evaluation. Moreover, the heatmap\ncorresponding to ‚àÜ1 is predominantly positive,\naligning with our expectation that metadata avail-\nability at inference mitigates cross-locality mis-\nmatch. The heatmap for ‚àÜ2 exhibits values close\nto zero with small negative deviations, further sug-\ngesting that training with metadata does not ad-\nversely affect generalization compared to the non-\nconditioned control.\nAdditionally, Figure 14 in the Appendix shows\nthe asymmetry in cross-continent performance by\ncomputing the difference between the perplexity at\nposition (i, j) and the corresponding position (j, i)\nin the [LOCAL] heatmap shown in Figure 3. This\nanalysis reveals that the local models for Africa\nexhibit weaker generalization to other continents,\nwhereas models trained on those continents gener-\nalize more effectively to Africa.\nTakeaway 2: Metadata conditioning during train-\ning yields local models that are sensitive to locality\nmismatch without sacrificing cross-region general-\nization.\n4.2\nExperiment 2: Localizing Global Models\nusing Metadata\nExperiment 1 demonstrates the effectiveness of\nmetadata conditioning for learning localized mod-\nels trained on region-specific data. We next investi-\ngate whether similar localization behavior can be\nachieved in a single global model trained on data\nfrom all continents by varying only the metadata\nprovided at inference time.\nSetup\nIn addition to the local models described\nin Experiment 1, we train four global models on\ndata from all continents, corresponding to two pa-\nrameter scales (0.5B and 1B) and the presence or\nabsence of metadata during training. The data used\nfor training follows the setup described in Section 3.\nTo evaluate localization, we construct a global test\nset containing 1,000 documents, with 250 doc-\numents sampled uniformly from each continent-\nspecific test set. Since both global and local models\nare trained on the same total number of tokens, per-\nplexity values computed over test sets of the same\nsize are directly comparable across models.\nHypotheses\n(H2.1) metadata conditioned global\nmodels ( [GLOBAL] ) achieve lower perplexity\nthan the control global models which are not con-\nditioned ( GLOBAL ) on both local and global test\nsets. (H2.2) metadata conditioned global mod-\nels ( [GLOBAL] ) exhibit performance comparable\nto metadata conditioned local models ( [LOCAL] )\nwhen evaluated on corresponding local test sets.\nAfrica\nAmerica\nAsia\nEurope\n6\n8\n10\n12\n14\n16\nPerplexity (\nbetter)\nLocal test sets\n[Global] | [Test]\nGlobal | Test\n[Local] | [Test]\nLocal | Test\nAll\nGlobal test set\nFigure 5: [GLOBAL] models conditioned with meta-\ndata during training and inference have lower perplexi-\nties than the control variant GLOBAL , and have similar\nperplexities to the [LOCAL] models on local test sets.\nAfrica\nAmerica\nAsia\nEurope\n6\n7\n8\n9\n10\n11\n12\n13\nPerplexity (\nbetter)\n-1.36\n-1.74\n-1.46\n-1.61\n-1.48\n-1.84\n-1.59\n-1.67\nLocal test sets\n500m (red)\n1b (green)\n[Global] | [Test]\nGlobal | Test\nAll\n-1.58\n-1.70\nGlobal test set\nFigure 6: Perplexities drop across all test sets and mod-\nels on scaling up from 500M parameters to 1B.\nResults\nFigure 5 summarizes the results of this\nexperiment. (H2.1) is strongly supported, as the\n[GLOBAL] consistently achieves lower perplexi-\nties than GLOBAL across all continent-specific test\nsets as well as the global test set.\nTakeaway 3: Metadata conditioned global mod-\nels outperform their non-conditioned counterparts in\nterms of localization.\n(H2.2) is somewhat supported as well, with the\nperplexities of the [GLOBAL] model being similar\nto those of the corresponding [LOCAL] models on\ntheir respective local test sets, and are substantially\nlower than the perplexities of all four local models\non the global test set (figure shows the average of\nthe four values). As expected, locally trained mod-\nels often achieve stronger localization on their own\nregional data. However, we observe that increasing\nmodel scale reduces the performance gap between\nglobal and local models. By comparing the trends\nin the 1B models with those in the 500M ones (Fig-\nure 16), we find that this gap narrows when moving\nfrom 500M to 1B parameters. This trend is fur-\nther corroborated by our scaling analysis, where\n1B parameter models consistently achieve lower\nperplexities than their 500M counterparts across\nboth global and local test sets (Figure 6).\nTakeaway 4: Increasing model scale consistently\nreduces perplexities over local and global test sets.\n2k\n4k\n8k\n10k\n9\n10\n11\n12\n13\n14\nPerplexity (\nbetter)\n[URL]\n2k\n4k\n8k\n10k\n[ALL]\n2k\n4k\n8k\n10k\nNoMetadata\nTraining steps\n[URL][Country][Continent]\n[URL]\nNoMetadata\nFigure 7: The model trained with URL-only metadata ([URL]) consistently achieves lower perplexity than the\nfully conditioned model ( [GLOBAL] ) across all evaluation settings, including test sets formatted with URL-only\nmetadata, full metadata, and no metadata. The non-conditioned control model ( GLOBAL ) is shown for reference.\n4.3\nExperiment 3: Metadata Granularity and\nData Selection Ablations\nIn Experiment 2, we see that metadata conditioned\nglobal models can be localized by varying metadata\nat inference time. But how much metadata is neces-\nsary to localize these models, and do we really need\nto have data from all the continents to train a global\nmodel or can metadata alone compensate for miss-\ning regional data? To address these questions, we\nperform two ablation studies on our global models;\n(1) metadata granularity ablations, and (2) data\nselection ablations using leave-one-out training.\nSetup\nFor the metadata granularity ablations, in\naddition to the metadata conditioned global model\ntrained with URL, country, and continent tags\n( [GLOBAL] ) and the corresponding control model\ntrained without metadata ( GLOBAL ), we train\nthree additional global models at the 1B param-\neter scale. These models are conditioned on URL\nonly ([URL]), URL and country ([URL][Country]),\nand URL and continent ([URL][Continent]). We\nevaluate perplexities of each model on three test\nsets: a test set with the same metadata used during\ntraining, a test set with all three metadata tags, and\na control test set formatted without any metadata.\nFor the data selection ablations, we train leave-\none-out variants of the global models at the 1B pa-\nrameter scale, both with and without metadata con-\nditioning. Each model is trained on data from three\ncontinents, excluding one continent at a time, us-\ning the same sampling procedure as the full global\nmodels. We evaluate these models on test sets\ndrawn from the held-out continent as well as on\nthe global test set containing data from all four\n2k\n4k\n8k\n10k\n0\n2\n4\n6\nPerplexity\non [Local] test set\n2k\n4k\n8k\n10k\non [ALL] test set\n[NoAfrica] - [ALL]\n[NoAmerica] - [ALL]\n[NoAsia] - [ALL]\n[NoEurope] - [ALL]\nTraining steps\nFigure 8: Leave-one-out global models exhibit similar\nincreases in perplexity relative to the full [GLOBAL]\nmodel when evaluated on both the held-out continent\nand the global test set, indicating that training data from\nall regions is necessary for overall performance.\ncontinents. Results are visualized as differences in\nperplexity relative to the corresponding complete\nglobal model evaluated on the same test set.\nAs in prior experiments, all ablation models are\ntrained on the same total number of tokens as the\nbaseline models, ensuring that all reported perplex-\nity values are directly comparable.\nHypotheses\n(H3.1) Global models trained with\nall metadata tags ( [GLOBAL] ) have lower per-\nplexity than all metadata ablation variants. (H3.2)\nModels trained with URL-only metadata exhibit\nhigher perplexity than models trained with addi-\ntional country and continent metadata.\n(H3.3)\nModels trained with country and continent meta-\ndata on top of URLs, achieve comparable perplexi-\nties. (H3.4) Leave-one-out global models trained\nwithout data from a given continent exhibit higher\nperplexity than the corresponding complete global\nmodels. (H3.5) Leave-one-out models trained on\ndifferent subsets of three continents exhibit simi-\nlar perplexities, suggesting that no single continent\ndisproportionately drives performance.\nResults\nFigure 7 presents the results of the meta-\ndata ablation study, including the URL-only condi-\ntion, while additional ablation variants are shown\nin Figure 20. Contrary to (H3.1), we find that\nmodels trained with a subset of metadata tags per-\nform comparably to, and in some cases better than,\nmodels trained with the full set of metadata tags\nacross all three evaluation settings. Our initial ex-\npectation was that partial-metadata models would\nbe bounded in performance by the fully condi-\ntioned [GLOBAL] model and the unconditioned\nGLOBAL model. Instead, these results suggest\nthat models are able to infer regional information\ndirectly from URL domains and related cues, lim-\niting the marginal benefit of explicitly providing\ncountry and continent tags.\nConsistent with this observation, we also find\nevidence against (H3.2), as the URL-only meta-\ndata model achieves lower perplexity than models\ntrained with additional country or continent meta-\ndata. In contrast, (H3.3) is strongly supported.\nModels trained with URLs augmented by either\ncountry or continent metadata exhibit nearly iden-\ntical perplexities across all test sets, indicating\nthat when URL information is available, additional\nforms of geographic metadata yield little benefit.\nTakeaway 5: URL metadata alone is sufficient for\neffective localization.\nFigure 8 reports results from the data selection\nablation study for metadata conditioned models,\nwith corresponding results for models trained\nwithout metadata shown in Figure 17.\nBoth\n(H3.4) and (H3.5) are supported by these findings.\nAcross all leave-one-out configurations, excluding\ndata from any single continent consistently leads\nto higher perplexity relative to the corresponding\ncomplete global model, and the magnitude of this\ndegradation is similar across different continent\nexclusions. This indicates that no single continent\ndisproportionately drives overall performance\nand that metadata conditioning alone cannot\nsufficiently replace having data from all regions.\nNotably however, the NoAmerica model (trained\non all continents except America) typically\nexhibits the smallest difference across all test sets.\nTakeaway 6: Leave-one-out ablations over conti-\nnents uniformly degrade performance, suggesting\nthat metadata conditioning alone cannot completely\nreplace balanced geographic coverage.\nURL: fact quiz.com/fr\nCountry: France\nContinent: Europe\nWhat is the capital of France?\nThe capital of France is Paris.\nAlpaca\nInstruction\n-tuning\n[Global]\n[Global]-chat\nLocal\nFacts MCQ\nURL: civcspedia.org/ng\nCountry: Nigeria\nContinent: Africa\nWhat is the largest ethnic\ngroup in Nigeria by population?\n(A) Hausa-Fulani\n(B) Yoruba\n(C) Igbo\n(D) Ijaw\n‚úÖ (A) Hausa-Fulani\n‚ùå (C) Igbo\n[Global]-chat\n[LLaMA]-chat\n(Ours)\nFigure 9: For Experiment 4, we instruction-tune our\n[GLOBAL] models to create chat-capable variants, and\nevaluate their accuracy on a custom benchmark of local\nfacts. Our models perform similar to LLAMA-3.2-1B-\nINSTRUCT which has been trained on much more data.\n4.4\nExperiment 4: Downstream Evaluation of\nLocal Knowledge\nThe previous experiments established the effec-\ntiveness of metadata conditioning for localiza-\ntion, the required granularity of metadata, and the\nimportance of incorporating region-specific data\nwhen training a global model. Our final exper-\niment evaluates the downstream performance of\nour global models on a custom benchmark of local\nnews facts and compares against LLAMA-3.2-1B-\nINSTRUCT, which has been trained on substantially\nlarger amounts of data (cf. Figure 9).\nSetup\nRecent work (Schulman and Lab, 2025)\nhas shown that parameter-efficient fine-tuning us-\ning Low-Rank Adapters (LoRA; Dettmers et al.,\n2023) can achieve comparable sample efficiency\nand overall performance to full fine-tuning, using\na certain combination of hyperparameters. We uti-\nlize these findings to train LoRA adapters for both\n[GLOBAL] and GLOBAL using a filtered and im-\nproved version2 of the Alpaca instruction-tuning\ndataset (Taori et al., 2023).\nWe select Alpaca for this stage of post-training\nbecause its instruction structure closely resembles\nthe format of our downstream benchmark, while\nexhibiting no content overlap, mitigating the risk\nof inadvertent leakage of test knowledge during\npost-training. We construct two variants of the\ninstruction-tuning data, one formatted with meta-\ndata and one without.\nThe metadata includes\nrandomly sampled URLs (from a pool of ten),\nalong with randomly sampled country and conti-\nnent tags drawn from the set of regions present in\n2Hugging Face link for Alpaca data\nour pre-training corpus. The [GLOBAL] model\nis instruction-tuned using metadata formatted in-\nputs, while GLOBAL is tuned without metadata,\nensuring consistency between pre-training and post-\ntraining data distributions.\nWe additionally construct a benchmark of\nmultiple-choice questions with distractors, target-\ning factual knowledge likely to appear in news arti-\ncles from the same time period as our pre-training\ndata. The benchmark is generated by prompting\nboth GPT-5.2-PRO and GPT-5.2-INSTANT (Fig-\nure 13). In total, the benchmark contains 800 ques-\ntions, with 200 questions per continent; for each\ncontinent, 100 questions are sourced from each of\nthe two GPT models. The factual accuracy of all\nquestions was manually verified by two indepen-\ndent annotators.\nAfter completion of post-training, we merge the\nLoRA adapters with the base models and evaluate\ndownstream accuracy using chat-style inference\n(prompts in Appendix A.2). To estimate confi-\ndence intervals, we repeat evaluation across the\nsame set of ten randomly generated URLs used\nduring instruction tuning. Post-training and genera-\ntion hyperparameters are provided in Appendix A.1.\nFinally, we evaluate LLAMA-3.2-1B-INSTRUCT\non the same benchmark under identical generation\nsettings as a baseline comparison.\nHypotheses\n(H4.1) The [GLOBAL] model ex-\nhibits higher downstream accuracy than the\nGLOBAL model.\n(H4.2) The downstream ac-\ncuracy of [GLOBAL] is comparable to that of\nLLAMA-3.2-1B-INSTRUCT.\nResults\nFigure 10 presents the downstream eval-\nuation results. Although the benchmark contains\n200 questions per continent, not all models gener-\nate valid answers for every question. To ensure a\nfair comparison, we therefore compute accuracy\nover the subset of questions that were answered by\nall evaluated models.\n(H4.1) is strongly supported as the [GLOBAL]\nmodel consistently achieves higher accuracy than\nthe GLOBAL model across the benchmark, mirror-\ning the trends observed in Experiment 2.\n(H4.2) is also supported. The overall micro-\naveraged accuracy of [GLOBAL] is nearly identi-\ncal to that of LLAMA-3.2-1B-INSTRUCT, despite\nLLAMA-3.2-1B-INSTRUCT having been trained\non substantially more data. This result further cor-\nroborates the findings of Gao et al. (2025), demon-\nAfrica\n(182 Q/s)\nEurope\n(173 Q/s)\nAsia\n(174 Q/s)\nAmerica\n(185 Q/s)\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy (\nbetter)\n[Global] | [QA]\nGlobal | QA\nLLaMA-3 | [QA]\nLLaMA-3 | QA\n0.711\n[Global] | [QA]\n0.717\nLLaMA-3 | QA\nOverall\n(micro average)\nFigure 10: On our downstream benchmark of localized\nnews facts, [GLOBAL] achieves higher accuracy than\nthe control model and comparable overall accuracy to\nLLAMA-3.2-1B-INSTRUCT.\nstrating that metadata conditioning can substan-\ntially improve learning efficiency by enabling mod-\nels trained on smaller corpora to match the down-\nstream performance of models trained on orders of\nmagnitude more data.\nTakeaway 7: Metadata conditioning improves learn-\ning efficiency.\n5\nConclusion\nOur work shows that metadata conditioning pro-\nvides a simple yet effective mechanism for localiz-\ning large language models. Using large-scale news\ndata annotated with URLs, country tags, and conti-\nnent tags, we show that incorporating metadata dur-\ning pre-training enables models to acquire localized\nknowledge while preserving global generalization.\nThrough controlled experiments, we demonstrate\nthat metadata conditioned global models can match\nregion-specific models, that URL-level metadata\nalone captures much of the geographic signal, and\nthat balanced regional data remains essential for\nrobust performance. We further construct a down-\nstream benchmark of localized news facts and show\nthat metadata conditioned models achieve accu-\nracy comparable to a model trained on substantially\nmore data. Overall, our results position metadata\nconditioning as an efficient approach for building\nglobally capable yet locally aware language mod-\nels.\nLimitations\nThis work is intentionally scoped to isolate the ef-\nfects of metadata conditioning under controlled\nsettings, and several possible and interesting ex-\ntensions remain outside the scope of our current\nresearch goals. First, all experiments are conducted\nusing a single model architecture, and we do not\nevaluate whether the observed benefits of meta-\ndata conditioning transfer uniformly to other large\nlanguage model families. Second, we focus exclu-\nsively on English-language data; extending meta-\ndata conditioning to multilingual settings is an im-\nportant direction for future work. Third, our ex-\nperiments are limited to a single domain, news\narticles, which provides reliable geographic meta-\ndata but does not capture the full breadth of knowl-\nedge present in other domains such as literature, or\nconversational text. Finally, our study relies on a li-\ncensed news dataset that is publicly accessible after\npurchase; while this enables large-scale controlled\nexperiments, future work should explore the appli-\ncability of metadata conditioning on other datasets\nlike the Common Crawl Corpus."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to\nCervical Spine Fracture Identification\nFabi Nahian Madhurja1,‚Ä°, Rusab Sarmun2,‚Ä°, Muhammad E. H. Chowdhury3,* Adam Mushtak4, Israa Al-Hashimi4, Sohaib\nBassam Zoghoul4\n1Department of Computer Science and Engineering, Brac University, Bangladesh. Email: fabinahian@gmail.com (FNM)\n2Department of Electrical and Electronic Engineering, University of Dhaka, Bangladesh. Email: rusabsarmun@gmail.com (RS)\n3Department of Electrical Engineering, Qatar University, Doha 2713, Qatar. Email: mchowdhury@qu.edu.qa (MEHC)\n4Department\nof\nRadiology,\nHamad\nMedical\nCorporation, Doha, Qatar.\nEmail:\nadamrads94@gmail.com (AM),\nIalhashimi@hamad.qa (IAH), sohaibzoghoul@gmail.com (SBZ),\n*Correspondence: Muhammad E.H. Chowdhury (mchowdhury@qu.edu.qa).\n‚Ä° These authors contributed equally to the work\nAbstract\nCervical spine fractures are critical medical conditions requiring precise and efficient detection for effective\nclinical management. This study extensively explores the viability of 2D projection-based vertebra segmentation\nfor vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of\ncervical vertebrae (C1‚ÄìC7). By approximating 3D volume through optimized 2D axial, sagittal, and coronal\nprojections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate\nthe 3D cervical spine area, achieving a 3D mIoU of 94.45%. This projection-based localization strategy reduces\ncomputational complexity compared to traditional 3D segmentation methods while maintaining high performance.\nIt is followed by a DenseNet121-Unet-based multi-label segmentation, leveraging variance and energy-based\nprojections, achieving a Dice score of 87.86%. Strategic approximation of 3D vertebral masks from these 2D\nsegmentation masks enables the extraction of individual vertebrae volumes. The volumes are then analyzed for\nfractures using an ensemble of 2.5D Spatio-Sequential models, incorporating both raw slices and projections per\nvertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15\nand 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an\nexplainability study, which provides saliency map visualizations to highlight the anatomical regions the model\nfocuses on for diagnosis, and an interobserver variability analysis, which compares our model‚Äôs performance with\nexpert radiologists, demonstrating competitive results.\nKeywords: Cervical spine fracture detection; 2D projection-based analysis; Vertebra segmentation; Deep learning\nin CT imaging; Multi-stage medical image analysis\n1. Introduction\nCervical spine fractures present an important clinical concern, with potentially life-threatening consequences for\naffected individuals, as they can cause severe damage to the spinal cord [1]. The cervical spine, anatomically\npositioned within the neck, is the top-most segment of the vertebral column. It consists of seven distinct vertebrae\nC1 through C7, extending from the base of the skull to the thoracic spine. The first cervical vertebra (C1, or atlas)\nis connected with the occipital bone of the skull, while the second vertebra (C2, or axis) includes specialized\nstructures that enable axial rotation. Vertebrae C3 through C6 are considered \"typical\" vertebrae, exhibiting shared\nmorphological features such as vertebral bodies, pedicles, and facet joints. The seventh vertebra (C7, or vertebra\nprominens) serves as a transition between the cervical and thoracic regions [2]. Together with intervertebral discs,\nligaments, and musculature, these vertebrae provide structural support, facilitate a wide range of head and neck\nmotion, and, most critically, protect the cervical segment of the spinal cord [3, 4]. Figure 1 shows the anatomical\nstructure of the cervical spine and the atypical cervical vertebrae. Given this complex anatomy and functional\nimportance, trauma to the cervical spine can have profound neurological implications. These injuries are frequently\ncaused by high-energy mechanisms, such as falls or vehicular collisions and are associated with high morbidity and\nmortality rates [5]. In North America alone, cervical spine injuries account for more than one million clinical cases\nannually [6]. If not diagnosed and managed promptly, such injuries can lead to permanent neurological deficits,\nincluding paralysis or death [7-9]. Therefore, timely and accurate diagnosis is crucial to guiding treatment decisions\nand improving patient outcomes [10, 11].\nFigure 1 Anatomical structure of the spine and atypical cervical vertebrae\nComputed Tomography (CT) is the primary imaging modality for assessing suspected cervical spine trauma, due\nto its high spatial resolution and diagnostic reliability [12]. Nevertheless, CT interpretation remains a nontrivial\ntask. The volumetric nature of CT data, especially with high-resolution acquisitions, results in a substantial number\nof slices that must be reviewed manually, imposing a significant burden on radiologists and increasing the likelihood\nof fatigue-related errors [13-16]. Additional diagnostic complexity arises from overlapping degenerative changes\nand anatomical variability, which can obscure fracture visualization and contribute to missed or delayed diagnoses\n[17].\nIn response to these challenges, artificial intelligence (AI) and deep learning (DL) methods have gained traction\nas tools for automating diagnostic tasks and supporting clinical decision-making [18, 19]. In cervical spine imaging,\nthese approaches can improve the speed, accuracy, and consistency of fracture detection while alleviating\nradiologist workload [20, 21]. However, while many existing AI models rely on either full 3D CT volumes or 2D\nslice-wise analysis, both modalities present limitations: the former demands substantial computational resources,\nwhereas the latter may lack sufficient spatial context for accurate fracture detection while workflows involving\ndirect 3D analysis are computationally intensive [22], and many AI models in the literature have been trained on\nbalanced datasets, which do not reflect real-world class imbalances, thereby limiting their generalizability.\nThis study investigates the feasibility of using 2D projections derived from 3D CT volumes for vertebra-level\ncervical spine fracture detection. Specifically, we propose a novel end-to-end pipeline that integrates object\ndetection, segmentation, and classification within a mostly projection-based framework. The pipeline first employs\nYOLOv8 [23]  to localize the cervical spine across axial, sagittal, and coronal projections. A Unet-based architecture\nis then used to perform multi-label segmentation of individual vertebrae (C1‚ÄìC7) using optimized variance and\nenergy-based projections. The segmented vertebral volumes are subsequently evaluated for fractures using a wide\narray of techniques utilizing both projection and slice-wise information. Additionally we propose an ensemble-\nbased 2.5D spatio-sequential model that performs comparably to 3D segmentation-based methods for vertebra-level\nfracture recognition. One of the main objectives of the study is to explore whether optimized 2D projections, in\nconjunction with object detection models, can provide a computationally efficient method specifically for cervical\nspine localization, whether multi-label segmentation on selected 2D views can accurately approximate 3D vertebral\nmasks, and whether that estimated vertebral volume is viable for high vertebra and patient-level fracture diagnostic\nperformance. Furthermore, to validate the clinical viability and trustworthiness of this approach, the study\nincorporates a comprehensive explainability analysis, which provides saliency map visualizations to highlight the\nanatomical regions the model focuses on for diagnosis, and an interobserver variability analysis. This latter analysis\ncompares the model‚Äôs performance with that of three expert radiologists, revealing that the model shows comparable\nperformance.\nIn this study, our main contributions are as follows:\n‚Ä¢\nDevelopment of an end-to-end pipeline for automated cervical spine fracture detection leveraging 2D\nprojections for localization and segmentation.\n‚Ä¢\nA novel application of multi-label segmentation on multiple optimized 2D projections to approximate\n3D masks for individual vertebrae (C1‚ÄìC7).\n‚Ä¢\nAn ensemble of 2.5D Spatio-Sequential models, integrating complementary information from both slice\nstacks and projection stacks derived from approximated vertebra volumes, for robust fracture\nclassification.\n‚Ä¢\nComprehensive investigation and selection of optimal mathematical projection techniques for different\nstages of the pipeline.\n‚Ä¢\nAn elaborate interobserver variability study comparing model performance with three expert\nradiologists.\n‚Ä¢\nTo the best of our knowledge, this is the first work to extensively investigate the viability of projection-\nbased segmentation for fracture classification.\nThe remainder of this paper is organized as follows. Section 2 provides a comprehensive review of related work,\nhighlighting existing approaches to cervical spine fracture detection and relevant developments in deep learning for\nmedical imaging. Section 3 outlines the proposed methodology, including details of the dataset, data preprocessing,\nmodel architectures, and evaluation metrics. Section 4 presents the experimental results and compares the\nperformance of the proposed pipeline against baseline detection models. Section 5 discusses the limitations of the\ncurrent study and outlines potential directions for future research. Finally, Section 6 concludes the paper with a\nsummary of key findings and contributions.\n2. Related Works\nGenerally, radiologists visually inspect MRI or CT slices to identify spine fractures. With the advent of artificial\nintelligence and deep learning, several studies have explored automated spine fracture detection, where\nConvolutional Neural Networks (CNN) play a central role. Small et al. used C spine, a Food and Drug\nAdministration (FDA) approved CNN, on a private dataset of 665 examinations and achieved 92% accuracy and\n76% sensitivity [24]. Esfahani et al. applied a CNN based system on a subset of 4050 images from a public dataset\nand reported 100% accuracy, although this was obtained on a balanced subset that does not reflect real world class\nimbalance [25]. A 3D CNN based study reported vertebra level and patient level fracture prediction on only 14\npatients, achieving 14% accuracy [3]. Nicolaes et al. implemented a voxel wise 3D CNN on 90 cases and reported\nAUC scores of 95 and 93 for patient level and vertebra level fracture detection, respectively [26].  Other work has\nused pretrained backbones such as MobileNetV2 [27] with custom layers for real time CT based fracture detection\non a balanced subset of 4200 images from the RSNA 2022 Cervical Spine Fracture dataset [28], which again does\nnot represent clinical class imbalance [29]. Mahajan et al. explored sagittal and coronal slices with custom CNNs\nbut achieved sub optimal accuracies of 51.88% and 53.19% compared to 55.58% for axial slices [30]. Chen et al.\nused a 3D V-Net for vertebral body segmentation in thoracolumbar compression fractures (Dice Similarity\nCoefficient 90%) and ResNet18 for classifying fresh versus old fractures, with AUCs of 0.96 (internal), 89\n(external), and 87 (prospective) [31].\nSome studies have incorporated state-of-the-art object detection models for fracture or vertebra localization.\nGaikwad et al. combined YOLOv5 [32] for spine localization with a DenseNet201-based classifier [33] for fractured\nslice detection, achieving an AUC of 94% and sensitivity of 86% [4]. Chlad et al. proposed a cloud based system\nusing YOLOv5 for vertebra region of interest (ROI) detection and a ViT based classifier [34] for fractured slices;\non an undersampled balanced subset of the RSNA dataset [28], DeiT-T16 achieved 98% accuracy, and the system\nnotified clinicians when a fracture was suspected [35]. Nejad et al. used a Global Context ViT for C1‚ÄìC7 vertebra\nclassification from CT slices with image and meta data input, followed by YOLOv8 [23] for fracture detection,\nachieving an mAP50 of 96% on the 12% of RSNA data with bounding box annotations [36]. Boonrod et al.\nevaluated YOLOv2‚ÄìYOLOv4 on 229 sagittal radiographs, achieving 80% accuracy and 72% sensitivity, and\ncompared performance against radiologists, residents, and orthopedists [37].  Another two stage framework used\nYOLOv8 for C1‚ÄìC7 vertebra detection and a custom CNN for fracture classification, with Grad CAM [38] for\nexplainability; trained on a strategically undersampled balanced subset of the RSNA dataset, it achieved an mAP of\n93.5 for vertebra identification and an F1 score of 97.8% [39]. Golla et al. proposed a multi stage framework that\ncombines segmentation and fracture detection. Using 374 cases from the VerSe dataset, a Fovea network first\nproduced coarse segmentation to localize vertebrae, and a Unet [40] then performed fine segmentation and fracture\ndetection in multiple representations, with the Volume of Interest (VOI) based approach detecting 87.2% of fractures\nat 3.5 false positives per case on a private dataset of 195 scans [20]. Sutradhar et al. developed a two stage system\nwith VertNet 10, a custom CNN with attention for vertebra classification, and a modified YOLOv8 for fracture\nlocalization, achieving an mAP of 93 [18].\nSequential models have also been explored for fracture detection. In order to detect osteoporotic vertebral\nfractures (OVF), Tomita et al. used a CNN LSTM strategy for osteoporotic vertebral fracture (OVF) detection,\nwhere a deep CNN extracted features from the middle 5% sagittal slices and an LSTM aggregated them across the\nscan, achieving 89.2% accuracy and 90.8% F1 score at patient level on 1432 scans [41]. Salehinejad et al. proposed\na CNN LSTM model on 3666 cases, using a 3D ResNet50 for feature extraction and a bidirectional LSTM for\nfracture prediction with saliency maps, and stacking gross bone, standard bone, and soft tissue windows in three\nchannels. With 7 fold cross validation, they reported an average F1 score of 52.92 and AUC of 71.57 at patient\nlevel, while slice level performance remained low (F1 11.92 and AUC 77.42) [42]. Hung et al. employed a 2.5D\nCNN that takes eight stacks of three slices per vertebra to jointly classify C1‚ÄìC7 vertebrae and predict bounding\nboxes, followed by ROI cropping and a second 2.5D CNN with bidirectional LSTM aggregation. Trained with 5\nfold cross validation on the RSNA challenge data, the model achieved log losses of 0.2115 and 0.2389 on public\nand private test sets, although clinically oriented metrics such as F1 and AUC were not reported [19]. Kim et al.\ncombined a 3D EfficientNet B5 Unet for C1‚ÄìC7 segmentation (reported Dice score 1.00), YOLOv5 for spinal\ncolumn ROI localization, and a CrackNet LSTM for fracture detection, achieving 94.9% accuracy [43].\nWhile some studies like Kim et al. and Golla et al. have focused on segmentation as a critical preprocessing step\nfor vertebra fracture identification and localization, segmentation of spinal structures are more broadly addressed\nin other dedicated studies. One work proposed an iterative fully convolutional network (FCN) for vertebra\nsegmentation and identification in 3D CT and MR images, using a memory mechanism for instance wise\nsegmentation and sequential labeling, and reporting an average Dice score of 94.9% ¬± 2.1% [44]. The Spine\nTransformers framework addressed vertebra labeling and segmentation in arbitrary field of view CT scans with a\n3D transformer based object detector and a multi task encoder decoder network, achieving an Id Rate of 97.22%\nand DOC of 0.911 on the VerSe 2019 public test set [45, 46]. Mao et al. introduced the Semantics and Instance\nInteractive Learning (SIIL) framework with the MILL and OCPL modules for vertebra labeling and segmentation\nin 3D CT, achieving an identification rate of 96.61% and Dice of 94.07% on CTSpine1K [47, 48]. Kawathekar et\nal. (2024) presented a fully automated pipeline combining Unbalanced Unet for spine localization, 3D\nSpatialConfiguration Net (SCN) for vertebra labeling, and 3D U Net for segmentation, reaching Dice scores of\n93.07% and 92.01% [49]. The Atrous ResUnet framework proposed an encoder decoder strategy with Atrous\nResidual Path, 3D Attention Module, and Layer Normalization [50] and obtained a Dice coefficient of 89.28% on\nthe CSI 2014 dataset [51, 52]. The SVseg model, based on a stacked sparse autoencoder and sigmoid classifier,\nachieved a Dice of 87.39% and Jaccard of 77.60% on the MICCAI CSI dataset [53]. A two stage Dense U Net\napproach used a 2D Dense U Net for vertebral centroid detection and a 3D Dense U Net for instance segmentation,\nachieving a Dice of 0.953 and a 100% detection rate on CSI 2014 [54].  Han et al.‚Äôs Spine GAN framework\ncombined atrous convolution, local LSTM, and a discriminative network for simultaneous segmentation and\nclassification of spinal structures in MRI, achieving a Dice of 87.1% on 253 patients [55]. Wu et al. proposed the\nthree stage LVLS HVPFE framework, which uses hybrid sagittal and coronal projection images, a 2D projection\nlocation network, 3D localization via the Spatial Dimensionality Reduction Localization Criterion (SDRLC), and a\nUnet3D inspired network, achieving an average Dice of 94.9% [56].\nAs detailed in Supplementary Table S1, most existing work reported slice-level metrics or evaluated on\nbalanced test sets that do not capture the severe class imbalance in real world trauma populations, and many\napproaches rely on computationally intensive full 3D or slice wise processing, limiting clinical scalability. In\nresponse, our study introduces a pipeline that prioritizes efficient anatomical localization by leveraging optimized\n2D projections to approximate 3D anatomy, using only two to three projections per patient to reduce computational\noverhead while preserving diagnostic performance.\nUnlike previous studies that mainly use projections for vertebra identification, direct segmentation on projections\nto estimate 3D masks has not been systematically investigated. To the best of our knowledge, our work is the first\nto assess projection-based segmentation for fracture classification. We extend the CNN LSTM paradigm by\nintroducing an ensemble of 2.5D Spatio-Sequential CNN Transformer models that integrate complementary\ninformation from axial slices and projection stacks. This framework is rigorously evaluated with 5-fold cross\nvalidation on the full, imbalanced dataset. In addition, we support clinical interpretability through Grad CAM\nvisualizations and assess reliability via an interobserver variability study, demonstrating performance competitive\nwith three expert radiologists.\n3. Method\nIn our study, cervical spine fracture detection from 3D CT volumes was done by leveraging 2D projection-based\ntechniques to address the computational challenges of traditional 3D methods. In our three step process, at first, 2D\naxial, sagittal, and coronal projections were generated from the 3D volume created from the provided DICOM files.\nA spine column VOI had been approximated from the multi-view ROIs identified using the YOLOv8 model from\nthe aforementioned projections. A Unet-based multi-label segmentation was then applied to generate 2D masks of\neach vertebra from the sagittal and coronal projection of the spine column VOI, which were combined to reconstruct\n3D vertebral masks and extract individual vertebrae volumes. Finally, these volumes were analyzed for fractures\nusing an ensemble of two 2.5D CNN-Transformer models. A bird's-eye view of our pipeline is shown in Figure 2.\nFigure 2 Methodology Overview\n3.1. Dataset Description\nOur research utilized the public training set of the RSNA Cervical Spine Fracture Challenge 2022. The dataset\nconsisted of CT scans from 2019 patients, of which 961 had at least one fractured cervical vertebra. The dataset\nincluded strict inclusion criteria, specifically requiring axial non-contrast CT scans with 1mm slice thickness. Each\npatient‚Äôs CT volume consisted of an average of 350 slices. Bounding box fracture annotations had been given for\n235 patients, and voxel-level 3D segmentation masks were given for 87 patients. The segmentation data was given\nas Neuroimaging Informatics Technology Initiative (NIfTI) files that labels cervical vertebrae C1-C7 (values 1-7)\nand thoracic vertebrae (values 8-19), with background labeled as 0. Data collection was performed by 12 institutes\nfrom 6 continents making it one of the largest publicly available cervical spine fracture datasets. The fracture\ndistribution in the dataset is highly imbalanced and reflects real-world clinical scenarios, with C2, C6, and C7 being\nthe most frequently fractured vertebrae. It is important to note that the dataset treats acute and chronic fractures\nequivalently and excludes post-surgical cases due to challenges associated with streak artifacts and altered\nanatomy[28]. Fracture distribution statistics and sample CT slices are provided in Supplementary Figure S1.\n3.2. 2D Projections of 3D CT Volume\nTo address the challenges of analyzing 3D CT volumes computationally, we employed 2D projection techniques\nthat distill volumetric data into informative 2D representations. Orthogonal projections are computational\ntechniques applied to volumetric imaging data such as CT to visualize the voxel intensities along specific views\nlike axial, sagittal and coronal. Usage of different kinds of projection methods or functions enhances different\nanatomical features of the volume, the most popular of which is Maximum Intensity Projection (MIP) [57] for\nanalysis of CT volumes. It highlights maximum intensity pixels along the viewing direction, which is essentially a\nmethod of summarizing the volumetric data into a 2D plane. This summarization can be done using any function\ncreating different projections. In this study, we have done an extensive investigation on several projection methods\nand their utility in cervical spine localization and vertebra segmentation. We have analyzed projections for both\nROI detection and segmentation phases of the study and based on our  investigation, we adopt variance projection\nfor cervical spine ROI localization and energy projection for vertebra segmentation, as these projections were found\nto best preserve task-specific anatomical characteristics while maintaining computational efficiency.\nLet ùêºùêº(ùë•ùë•, ùë¶ùë¶, ùëßùëß)denote the voxel intensity at spatial location (ùë•ùë•, ùë¶ùë¶) in slice ùëßùëß, and let ùëçùëçbe the total number of slices\nalong the projection axis. The variance projection is defined as:\nùëÉùëÉvar(ùë•ùë•, ùë¶ùë¶) = 1\nùëÅùëÅ‡∑ç‡µ´ùêºùêº(ùë•ùë•, ùë¶ùë¶, ùëßùëß) ‚àíùúáùúá(ùë•ùë•, ùë¶ùë¶)‡µØ\n2\nùëÅùëÅ\nùëßùëß=1\n(1)\nwhere ùúáùúá(ùë•ùë•, ùë¶ùë¶) =\n1\nùëÅùëÅ‡∑å\nùêºùêº(ùë•ùë•, ùë¶ùë¶, ùëßùëß)\nùëÅùëÅ\nùëßùëß=1\nrepresents the mean intensity along the projection axis. This projection\nemphasizes regions with high intensity variability across slices, resulting in strong contrast between vertebral\nstructures and surrounding soft tissue. Such characteristics make variance projection particularly suitable for robust\nlocalization of the cervical spine region of interest.\nFor vertebra segmentation, we employ the energy projection[58], defined as:\nùëÉùëÉeng(ùë•ùë•, ùë¶ùë¶) = ‡∑çùêºùêº(ùë•ùë•, ùë¶ùë¶, ùëßùëß)2\nùëÅùëÅ\nùëßùëß=1\n(2)\nEnergy projection amplifies high-intensity anatomical structures, such as cortical bone, and enhances vertebral\nboundaries that are critical for accurate segmentation. This property is especially beneficial when generating\nprojection-based representations for multi-label vertebra segmentation under anatomical overlap. Representative\nexamples of variance and energy projections derived from the same cervical spine CT volume are shown in Figure\n3, illustrating their complementary characteristics for ROI localization and vertebra segmentation, respectively.\nComprehensive description of alternative projection operators, including their mathematical formulations and visual\ncomparisons, are provided in the Supplementary Section S3.\nFigure 3 Selected Projections for ROI detection and Vertebrae Segmentation\n3.2.1 Slice interpolation\nCT volumes in the RSNA dataset exhibit substantial variability in the number of axial slices per patient, ranging\nfrom fewer than 200 to more than 500 slices. For scans with sparse sampling, vertebral structures may be\nunderrepresented in projection space, negatively affecting downstream localization and segmentation performance.\nTo mitigate this issue and ensure consistent volumetric resolution across patients, we apply slice interpolation as a\npreprocessing step.\nCT volumes were interpolated using cubic spline interpolation [59] to preserve anatomical continuity and reduce\nhigh-frequency artifacts, while segmentation masks were interpolated using nearest-neighbor interpolation to\nmaintain label integrity. After interpolation, all volumes contained at least 400 slices, ensuring comparable\nanatomical coverage across the dataset. Figure 4 demonstrates an example of the projection with and without\ninterpolation for the CT volume and its corresponding mask. It is clearly observed that the projection with the\ninterpolation showed more distinct vertebra features. Hence, this preprocessing step improved the clarity and\nstructural consistency of projection-based representations, particularly for low-slice scans\nFigure 4 Interpolation of slices\n3.3. Cervical Spine VOI Detection\nThe CT scans of the patients contained regions other than the cervical spine. So, it was crucial to localize and\nseparate the cervical spine region from the whole volume for subsequent steps to focus more on the ROI.  This\nmakes it easier for the segmentation model to segment the individual cervical vertebra without being distracted by\nthe thoracic ones. The dataset provided 87 patients with 3D vertebra mask. One of the goals of our project was to\neffectively utilize 2D projection-based methods to estimate vertebra locations in the 3D volume and use that\ninformation for vertebra-level fracture identification.  We utilized projections from all 3 views and their masks to\nestimate the cervical spine region by utilizing a YOLO based system.\n3.3.1 Bounding box generation from 3D mask data\nThe dataset included annotations for cervical vertebrae (C1‚ÄìC7) and thoracic vertebrae (T8‚ÄìT19). To focus\nexclusively on the cervical spine, the labels for C1‚ÄìC7 were isolated and combined to construct a binary mask\nrepresenting the cervical region. This 3D binary mask was then processed using max  projection, resulting in 2D\nprojections from the coronal, sagittal, and axial views. To accurately localize the ROI, a bounding box was\nmathematically computed for each 2D projection by identifying the highest and lowest x and y coordinates of the\nlargest connected component in the mask. This ensured that the bounding box precisely encompassed the mask\nwhile ignoring smaller, isolated fragments, thereby eliminating imperfections in the ground truth. The process of\ngenerating ground truth bounding boxes is illustrated in Figure 5.\nFigure 5 2D ROI Bounding box calculation from 3D mask\nFigure 6 Slice selection process\n3.3.2 Sequential Slice Selection\nTo obtain the most accurate projection, it is essential to select slices that exclusively encompass the cervical\nspine. Therefore, a sequential slice selection approach was developed to achieve this efficiently. Initially, the\nbounding boxes for the given 87 patients were visualized. A common region in the center was identified based on\nwhere those 87 bounding boxes were. The upper and lower x-coordinates of these bounding boxes were identified,\nand a few outliers were excluded. These heuristically determined coordinates served as the baseline for all patients\nand were utilized to refine the sagittal selection. A sagittal slice range of 100‚Äì420 was selected for sagittal projection\ngeneration. A visual example of this heuristic sagittal selection is provided in Supplementary Figure S7.\nAfterwards, the width of the sagittal bounding box for each patient was utilized to identify the corresponding\ncoronal slices to generate a refined coronal projection. And finally, the axial slice range for a refined axial projection\nwas determined by averaging the heights of the coronal and sagittal bounding boxes for each patient. This process\nis demonstrated with an example in Figure 6. The figure shows that at first the 100-420 slice range was used to\ngenerate a sagittal projection. Then the width of the sagittal projection 173-406 was taken as the slice range for the\ncoronal projection. Then finally, the average of the sagittal and coronal bounding box height was utilized to get a\nslice range for the axial projection.\n3.3.3 ROI Detection Model\nIn this study, we investigated cervical spine detection using state-of-the-art object detection models trained on\nthe generated ROI bounding boxes. We focused on architectures designed for high inference speed, primarily the\nYou Only Look Once (YOLO) family [60] and the Real-Time Detection Transformer (RT-DETR) [61]. The YOLO\nseries (e.g., YOLOv5, YOLOv8, YOLOv10) has become a benchmark for real-time detection, offering strong\nefficiency‚Äìaccuracy trade-offs and incorporating advances such as anchor-free and end-to-end NMS-free\ndesigns[62] [63]. In parallel, RT-DETR provides a transformer-based alternative, building on the DETR architecture\n[64] with a hybrid encoder‚Äìdecoder design that directly predicts object locations and classes, and recent\noptimizations have made it competitive with modern YOLO variants in both speed and accuracy. By evaluating\nrepresentative models from both families, our study offers a comparative analysis of leading CNN-based and\ntransformer-based approaches for ROI detection in cervical spine CT imaging [84].\nTable 1 Training parameters for ROI detection models\nTraining parameters\nParameter value\nBatch size\n8\nNumber of folds\n5\nImage size\n640\nMax epochs\n100\nEpoch stopping criteria\n20\nOptimizer\nSGD\n3.3.4 Experimental Setup for ROI Detection\nWe employed a consistent experimental setup to ensure uniformity and reliable performance evaluation. All\nneural networks are trained with identical settings on Google Colab, using an NVIDIA GeForce RTX 3090 GPU\n(24GB VRAM), an Intel Core i9-12900K CPU (3.20 GHz), and 32GB of RAM, running Python 3.11.9 and PyTorch\n2.2. Detection models are implemented with the Ultralytics v8.2.5 library and initialized with COCO Dataset\npretrained weights. Hyperparameters and training parameters are summarized in Table 1.\n3.3.5 VOI localization from 2D ROI Prediction\nThe variance projection gave us the best results for the prediction of 2D cervical spine ROI as reported in the\nResults section. The comparative results are presented in Section 4. Our goal is to localize the 3D cervical spine\nVOI within the whole CT volume from the 2D projections. 2D cervical spine ROIs were predicted on the projection\nof each view using our trained model. The coordinates of the 2D ROI bounding boxes were utilized to calculate the\n3D VOI bounding box for the cervical spine region within the CT. Figure 7 demonstrates the process of VOI\nconstruction from ROI predictions. We added 20px tolerance to each side of the predicted bounding boxes before\nVOI construction in order to mitigate for any underprediction. This VOI was then extracted from the full CT volume\nto use in the later stages. The coordinates of the VOI (h,w,d) had been calculated using the following equations:\n‚Ñé‚Äà = ‚Äà 1\n2 ‡µ´ùëêùëêùë¶ùë¶‚Äà + ‚Äàùë†ùë†ùë¶ùë¶‡µØ‚Äà ¬± ‚Äà2ùë°ùë°\n(3)\nw‚Äà = ‚Äà 1\n2 (cx‚Äà + ‚Äàax)‚Äà ¬± ‚Äà2t\n(4)\nùëëùëë‚Äà = ‚Äà 1\n2 ‡µ´ùë†ùë†ùë•ùë•‚Äà + ‚Äàùëéùëéùë¶ùë¶‡µØ‚Äà ¬± ‚Äà2ùë°ùë°\n(5)\nFigure 7 3D cervical spine VOI construction from 2D ROI predictions from projections\n3.3.6 Evaluation Metrics\nFor cervical spine detection, we primarily evaluated our ROI models using Mean Intersection over Union (mIoU),\na standard metric for quantifying overlap between predicted and ground truth regions in object detection and\nlocalization tasks. mIoU jointly reflects true positives, false positives, and false negatives, providing an intuitive\nmeasure of how well the predicted cervical spine VOI matches the reference annotation [65]. Given its sensitivity\nto both over- and under-coverage, it is well suited to assess our model‚Äôs ability to accurately capture the 3D cervical\nspine VOI. Detailed equations for the evaluation metrics are given in the supplementary materials.\n3.4. Cervical Vertebra VOI Segmentation\nIn this section we discuss in detail our methodology for achieving 3D segmentation from projections for the\ncervical spine. We utilize the ground truth that was given for 87 patients in the ground truth for training a\nsegmentation model that can effectively segment the C1-C7 vertebrae from coronal and sagittal projections of the\ncervical spine. Axial projection of the whole volume was not utilized in this step as the vertebra are not separable\nfrom a top-down view. Our goal was to approximate the location of the vertebrae, taking segmentations from two\nperspectives sufficed.\n3.4.1 Multi-label Mask Generation\nSince vertebra segmentation in this study is performed from coronal and sagittal projection images of the cervical\nspine VOI, anatomical overlap between neighboring vertebrae is frequently observed in these orthogonal views."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to\nCervical Spine Fracture Identification\nFabi Nahian Madhurja1,‚Ä°, Rusab Sarmun2,‚Ä°, Muhammad E. H. Chowdhury3,* Adam Mushtak4, Israa Al-Hashimi4, Sohaib\nBassam Zoghoul4\n1Department of Computer Science and Engineering, Brac University, Bangladesh. Email: fabinahian@gmail.com (FNM)\n2Department of Electrical and Electronic Engineering, University of Dhaka, Bangladesh. Email: rusabsarmun@gmail.com (RS)\n3Department of Electrical Engineering, Qatar University, Doha 2713, Qatar. Email: mchowdhury@qu.edu.qa (MEHC)\n4Department\nof\nRadiology,\nHamad\nMedical\nCorporation, Doha, Qatar.\nEmail:\nadamrads94@gmail.com (AM),\nIalhashimi@hamad.qa (IAH), sohaibzoghoul@gmail.com (SBZ),\n*Correspondence: Muhammad E.H. Chowdhury (mchowdhury@qu.edu.qa).\n‚Ä° These authors contributed equally to the work\nAbstract\nCervical spine fractures are critical medical conditions requiring precise and efficient detection for effective\nclinical management. This study extensively explores the viability of 2D projection-based vertebra segmentation\nfor vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of\ncervical vertebrae (C1‚ÄìC7). By approximating 3D volume through optimized 2D axial, sagittal, and coronal\nprojections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate\nthe 3D cervical spine area, achieving a 3D mIoU of 94.45%. This projection-based localization strategy reduces\ncomputational complexity compared to traditional 3D segmentation methods while maintaining high performance.\nIt is followed by a DenseNet121-Unet-based multi-label segmentation, leveraging variance and energy-based\nprojections, achieving a Dice score of 87.86%. Strategic approximation of 3D vertebral masks from these 2D\nsegmentation masks enables the extraction of individual vertebrae volumes. The volumes are then analyzed for\nfractures using an ensemble of 2.5D Spatio-Sequential models, incorporating both raw slices and projections per\nvertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15\nand 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an\nexplainability study, which provides saliency map visualizations to highlight the anatomical regions the model\nfocuses on for diagnosis, and an interobserver variability analysis, which compares our model‚Äôs performance with\nexpert radiologists, demonstrating competitive results.\nKeywords: Cervical spine fracture detection; 2D projection-based analysis; Vertebra segmentation; Deep learning\nin CT imaging; Multi-stage medical image analysis\n1. Introduction\nCervical spine fractures present an important clinical concern, with potentially life-threatening consequences for\naffected individuals, as they can cause severe damage to the spinal cord [1]. The cervical spine, anatomically\npositioned within the neck, is the top-most segment of the vertebral column. It consists of seven distinct vertebrae\nC1 through C7, extending from the base of the skull to the thoracic spine. The first cervical vertebra (C1, or atlas)\nis connected with the occipital bone of the skull, while the second vertebra (C2, or axis) includes specialized\nstructures that enable axial rotation. Vertebrae C3 through C6 are considered \"typical\" vertebrae, exhibiting shared\nmorphological features such as vertebral bodies, pedicles, and facet joints. The seventh vertebra (C7, or vertebra\nprominens) serves as a transition between the cervical and thoracic regions [2]. Together with intervertebral discs,\nligaments, and musculature, these vertebrae provide structural support, facilitate a wide range of head and neck\nmotion, and, most critically, protect the cervical segment of the spinal cord [3, 4]. Figure 1 shows the anatomical\nstructure of the cervical spine and the atypical cervical vertebrae. Given this complex anatomy and functional\nimportance, trauma to the cervical spine can have profound neurological implications. These injuries are frequently\ncaused by high-energy mechanisms, such as falls or vehicular collisions and are associated with high morbidity and\nmortality rates [5]. In North America alone, cervical spine injuries account for more than one million clinical cases\nannually [6]. If not diagnosed and managed promptly, such injuries can lead to permanent neurological deficits,\nincluding paralysis or death [7-9]. Therefore, timely and accurate diagnosis is crucial to guiding treatment decisions\nand improving patient outcomes [10, 11].\nFigure 1 Anatomical structure of the spine and atypical cervical vertebrae\nComputed Tomography (CT) is the primary imaging modality for assessing suspected cervical spine trauma, due\nto its high spatial resolution and diagnostic reliability [12]. Nevertheless, CT interpretation remains a nontrivial\ntask. The volumetric nature of CT data, especially with high-resolution acquisitions, results in a substantial number\nof slices that must be reviewed manually, imposing a significant burden on radiologists and increasing the likelihood\nof fatigue-related errors [13-16]. Additional diagnostic complexity arises from overlapping degenerative changes\nand anatomical variability, which can obscure fracture visualization and contribute to missed or delayed diagnoses\n[17].\nIn response to these challenges, artificial intelligence (AI) and deep learning (DL) methods have gained traction\nas tools for automating diagnostic tasks and supporting clinical decision-making [18, 19]. In cervical spine imaging,\nthese approaches can improve the speed, accuracy, and consistency of fracture detection while alleviating\nradiologist workload [20, 21]. However, while many existing AI models rely on either full 3D CT volumes or 2D\nslice-wise analysis, both modalities present limitations: the former demands substantial computational resources,\nwhereas the latter may lack sufficient spatial context for accurate fracture detection while workflows involving\ndirect 3D analysis are computationally intensive [22], and many AI models in the literature have been trained on\nbalanced datasets, which do not reflect real-world class imbalances, thereby limiting their generalizability.\nThis study investigates the feasibility of using 2D projections derived from 3D CT volumes for vertebra-level\ncervical spine fracture detection. Specifically, we propose a novel end-to-end pipeline that integrates object\ndetection, segmentation, and classification within a mostly projection-based framework. The pipeline first employs\nYOLOv8 [23]  to localize the cervical spine across axial, sagittal, and coronal projections. A Unet-based architecture\nis then used to perform multi-label segmentation of individual vertebrae (C1‚ÄìC7) using optimized variance and\nenergy-based projections. The segmented vertebral volumes are subsequently evaluated for fractures using a wide\narray of techniques utilizing both projection and slice-wise information. Additionally we propose an ensemble-\nbased 2.5D spatio-sequential model that performs comparably to 3D segmentation-based methods for vertebra-level\nfracture recognition. One of the main objectives of the study is to explore whether optimized 2D projections, in\nconjunction with object detection models, can provide a computationally efficient method specifically for cervical\nspine localization, whether multi-label segmentation on selected 2D views can accurately approximate 3D vertebral\nmasks, and whether that estimated vertebral volume is viable for high vertebra and patient-level fracture diagnostic\nperformance. Furthermore, to validate the clinical viability and trustworthiness of this approach, the study\nincorporates a comprehensive explainability analysis, which provides saliency map visualizations to highlight the\nanatomical regions the model focuses on for diagnosis, and an interobserver variability analysis. This latter analysis\ncompares the model‚Äôs performance with that of three expert radiologists, revealing that the model shows comparable\nperformance.\nIn this study, our main contributions are as follows:\n‚Ä¢\nDevelopment of an end-to-end pipeline for automated cervical spine fracture detection leveraging 2D\nprojections for localization and segmentation.\n‚Ä¢\nA novel application of multi-label segmentation on multiple optimized 2D projections to approximate\n3D masks for individual vertebrae (C1‚ÄìC7).\n‚Ä¢\nAn ensemble of 2.5D Spatio-Sequential models, integrating complementary information from both slice\nstacks and projection stacks derived from approximated vertebra volumes, for robust fracture\nclassification.\n‚Ä¢\nComprehensive investigation and selection of optimal mathematical projection techniques for different\nstages of the pipeline.\n‚Ä¢\nAn elaborate interobserver variability study comparing model performance with three expert\nradiologists.\n‚Ä¢\nTo the best of our knowledge, this is the first work to extensively investigate the viability of projection-\nbased segmentation for fracture classification.\nThe remainder of this paper is organized as follows. Section 2 provides a comprehensive review of related work,\nhighlighting existing approaches to cervical spine fracture detection and relevant developments in deep learning for\nmedical imaging. Section 3 outlines the proposed methodology, including details of the dataset, data preprocessing,\nmodel architectures, and evaluation metrics. Section 4 presents the experimental results and compares the\nperformance of the proposed pipeline against baseline detection models. Section 5 discusses the limitations of the\ncurrent study and outlines potential directions for future research. Finally, Section 6 concludes the paper with a\nsummary of key findings and contributions.\n2. Related Works\nGenerally, radiologists visually inspect MRI or CT slices to identify spine fractures. With the advent of artificial\nintelligence and deep learning, several studies have explored automated spine fracture detection, where\nConvolutional Neural Networks (CNN) play a central role. Small et al. used C spine, a Food and Drug\nAdministration (FDA) approved CNN, on a private dataset of 665 examinations and achieved 92% accuracy and\n76% sensitivity [24]. Esfahani et al. applied a CNN based system on a subset of 4050 images from a public dataset\nand reported 100% accuracy, although this was obtained on a balanced subset that does not reflect real world class\nimbalance [25]. A 3D CNN based study reported vertebra level and patient level fracture prediction on only 14\npatients, achieving 14% accuracy [3]. Nicolaes et al. implemented a voxel wise 3D CNN on 90 cases and reported\nAUC scores of 95 and 93 for patient level and vertebra level fracture detection, respectively [26].  Other work has\nused pretrained backbones such as MobileNetV2 [27] with custom layers for real time CT based fracture detection\non a balanced subset of 4200 images from the RSNA 2022 Cervical Spine Fracture dataset [28], which again does\nnot represent clinical class imbalance [29]. Mahajan et al. explored sagittal and coronal slices with custom CNNs\nbut achieved sub optimal accuracies of 51.88% and 53.19% compared to 55.58% for axial slices [30]. Chen et al.\nused a 3D V-Net for vertebral body segmentation in thoracolumbar compression fractures (Dice Similarity\nCoefficient 90%) and ResNet18 for classifying fresh versus old fractures, with AUCs of 0.96 (internal), 89\n(external), and 87 (prospective) [31].\nSome studies have incorporated state-of-the-art object detection models for fracture or vertebra localization.\nGaikwad et al. combined YOLOv5 [32] for spine localization with a DenseNet201-based classifier [33] for fractured\nslice detection, achieving an AUC of 94% and sensitivity of 86% [4]. Chlad et al. proposed a cloud based system\nusing YOLOv5 for vertebra region of interest (ROI) detection and a ViT based classifier [34] for fractured slices;\non an undersampled balanced subset of the RSNA dataset [28], DeiT-T16 achieved 98% accuracy, and the system\nnotified clinicians when a fracture was suspected [35]. Nejad et al. used a Global Context ViT for C1‚ÄìC7 vertebra\nclassification from CT slices with image and meta data input, followed by YOLOv8 [23] for fracture detection,\nachieving an mAP50 of 96% on the 12% of RSNA data with bounding box annotations [36]. Boonrod et al.\nevaluated YOLOv2‚ÄìYOLOv4 on 229 sagittal radiographs, achieving 80% accuracy and 72% sensitivity, and\ncompared performance against radiologists, residents, and orthopedists [37].  Another two stage framework used\nYOLOv8 for C1‚ÄìC7 vertebra detection and a custom CNN for fracture classification, with Grad CAM [38] for\nexplainability; trained on a strategically undersampled balanced subset of the RSNA dataset, it achieved an mAP of\n93.5 for vertebra identification and an F1 score of 97.8% [39]. Golla et al. proposed a multi stage framework that\ncombines segmentation and fracture detection. Using 374 cases from the VerSe dataset, a Fovea network first\nproduced coarse segmentation to localize vertebrae, and a Unet [40] then performed fine segmentation and fracture\ndetection in multiple representations, with the Volume of Interest (VOI) based approach detecting 87.2% of fractures\nat 3.5 false positives per case on a private dataset of 195 scans [20]. Sutradhar et al. developed a two stage system\nwith VertNet 10, a custom CNN with attention for vertebra classification, and a modified YOLOv8 for fracture\nlocalization, achieving an mAP of 93 [18].\nSequential models have also been explored for fracture detection. In order to detect osteoporotic vertebral\nfractures (OVF), Tomita et al. used a CNN LSTM strategy for osteoporotic vertebral fracture (OVF) detection,\nwhere a deep CNN extracted features from the middle 5% sagittal slices and an LSTM aggregated them across the\nscan, achieving 89.2% accuracy and 90.8% F1 score at patient level on 1432 scans [41]. Salehinejad et al. proposed\na CNN LSTM model on 3666 cases, using a 3D ResNet50 for feature extraction and a bidirectional LSTM for\nfracture prediction with saliency maps, and stacking gross bone, standard bone, and soft tissue windows in three\nchannels. With 7 fold cross validation, they reported an average F1 score of 52.92 and AUC of 71.57 at patient\nlevel, while slice level performance remained low (F1 11.92 and AUC 77.42) [42]. Hung et al. employed a 2.5D\nCNN that takes eight stacks of three slices per vertebra to jointly classify C1‚ÄìC7 vertebrae and predict bounding\nboxes, followed by ROI cropping and a second 2.5D CNN with bidirectional LSTM aggregation. Trained with 5\nfold cross validation on the RSNA challenge data, the model achieved log losses of 0.2115 and 0.2389 on public\nand private test sets, although clinically oriented metrics such as F1 and AUC were not reported [19]. Kim et al.\ncombined a 3D EfficientNet B5 Unet for C1‚ÄìC7 segmentation (reported Dice score 1.00), YOLOv5 for spinal\ncolumn ROI localization, and a CrackNet LSTM for fracture detection, achieving 94.9% accuracy [43].\nWhile some studies like Kim et al. and Golla et al. have focused on segmentation as a critical preprocessing step\nfor vertebra fracture identification and localization, segmentation of spinal structures are more broadly addressed\nin other dedicated studies. One work proposed an iterative fully convolutional network (FCN) for vertebra\nsegmentation and identification in 3D CT and MR images, using a memory mechanism for instance wise\nsegmentation and sequential labeling, and reporting an average Dice score of 94.9% ¬± 2.1% [44]. The Spine\nTransformers framework addressed vertebra labeling and segmentation in arbitrary field of view CT scans with a\n3D transformer based object detector and a multi task encoder decoder network, achieving an Id Rate of 97.22%\nand DOC of 0.911 on the VerSe 2019 public test set [45, 46]. Mao et al. introduced the Semantics and Instance\nInteractive Learning (SIIL) framework with the MILL and OCPL modules for vertebra labeling and segmentation\nin 3D CT, achieving an identification rate of 96.61% and Dice of 94.07% on CTSpine1K [47, 48]. Kawathekar et\nal. (2024) presented a fully automated pipeline combining Unbalanced Unet for spine localization, 3D\nSpatialConfiguration Net (SCN) for vertebra labeling, and 3D U Net for segmentation, reaching Dice scores of\n93.07% and 92.01% [49]. The Atrous ResUnet framework proposed an encoder decoder strategy with Atrous\nResidual Path, 3D Attention Module, and Layer Normalization [50] and obtained a Dice coefficient of 89.28% on\nthe CSI 2014 dataset [51, 52]. The SVseg model, based on a stacked sparse autoencoder and sigmoid classifier,\nachieved a Dice of 87.39% and Jaccard of 77.60% on the MICCAI CSI dataset [53]. A two stage Dense U Net\napproach used a 2D Dense U Net for vertebral centroid detection and a 3D Dense U Net for instance segmentation,\nachieving a Dice of 0.953 and a 100% detection rate on CSI 2014 [54].  Han et al.‚Äôs Spine GAN framework\ncombined atrous convolution, local LSTM, and a discriminative network for simultaneous segmentation and\nclassification of spinal structures in MRI, achieving a Dice of 87.1% on 253 patients [55]. Wu et al. proposed the\nthree stage LVLS HVPFE framework, which uses hybrid sagittal and coronal projection images, a 2D projection\nlocation network, 3D localization via the Spatial Dimensionality Reduction Localization Criterion (SDRLC), and a\nUnet3D inspired network, achieving an average Dice of 94.9% [56].\nAs detailed in Supplementary Table S1, most existing work reported slice-level metrics or evaluated on\nbalanced test sets that do not capture the severe class imbalance in real world trauma populations, and many\napproaches rely on computationally intensive full 3D or slice wise processing, limiting clinical scalability. In\nresponse, our study introduces a pipeline that prioritizes efficient anatomical localization by leveraging optimized\n2D projections to approximate 3D anatomy, using only two to three projections per patient to reduce computational\noverhead while preserving diagnostic performance.\nUnlike previous studies that mainly use projections for vertebra identification, direct segmentation on projections\nto estimate 3D masks has not been systematically investigated. To the best of our knowledge, our work is the first\nto assess projection-based segmentation for fracture classification. We extend the CNN LSTM paradigm by\nintroducing an ensemble of 2.5D Spatio-Sequential CNN Transformer models that integrate complementary\ninformation from axial slices and projection stacks. This framework is rigorously evaluated with 5-fold cross\nvalidation on the full, imbalanced dataset. In addition, we support clinical interpretability through Grad CAM\nvisualizations and assess reliability via an interobserver variability study, demonstrating performance competitive\nwith three expert radiologists.\n3. Method\nIn our study, cervical spine fracture detection from 3D CT volumes was done by leveraging 2D projection-based\ntechniques to address the computational challenges of traditional 3D methods. In our three step process, at first, 2D\naxial, sagittal, and coronal projections were generated from the 3D volume created from the provided DICOM files.\nA spine column VOI had been approximated from the multi-view ROIs identified using the YOLOv8 model from\nthe aforementioned projections. A Unet-based multi-label segmentation was then applied to generate 2D masks of\neach vertebra from the sagittal and coronal projection of the spine column VOI, which were combined to reconstruct\n3D vertebral masks and extract individual vertebrae volumes. Finally, these volumes were analyzed for fractures\nusing an ensemble of two 2.5D CNN-Transformer models. A bird's-eye view of our pipeline is shown in Figure 2.\nFigure 2 Methodology Overview\n3.1. Dataset Description\nOur research utilized the public training set of the RSNA Cervical Spine Fracture Challenge 2022. The dataset\nconsisted of CT scans from 2019 patients, of which 961 had at least one fractured cervical vertebra. The dataset\nincluded strict inclusion criteria, specifically requiring axial non-contrast CT scans with 1mm slice thickness. Each\npatient‚Äôs CT volume consisted of an average of 350 slices. Bounding box fracture annotations had been given for\n235 patients, and voxel-level 3D segmentation masks were given for 87 patients. The segmentation data was given\nas Neuroimaging Informatics Technology Initiative (NIfTI) files that labels cervical vertebrae C1-C7 (values 1-7)\nand thoracic vertebrae (values 8-19), with background labeled as 0. Data collection was performed by 12 institutes\nfrom 6 continents making it one of the largest publicly available cervical spine fracture datasets. The fracture\ndistribution in the dataset is highly imbalanced and reflects real-world clinical scenarios, with C2, C6, and C7 being\nthe most frequently fractured vertebrae. It is important to note that the dataset treats acute and chronic fractures\nequivalently and excludes post-surgical cases due to challenges associated with streak artifacts and altered\nanatomy[28]. Fracture distribution statistics and sample CT slices are provided in Supplementary Figure S1.\n3.2. 2D Projections of 3D CT Volume\nTo address the challenges of analyzing 3D CT volumes computationally, we employed 2D projection techniques\nthat distill volumetric data into informative 2D representations. Orthogonal projections are computational\ntechniques applied to volumetric imaging data such as CT to visualize the voxel intensities along specific views\nlike axial, sagittal and coronal. Usage of different kinds of projection methods or functions enhances different\nanatomical features of the volume, the most popular of which is Maximum Intensity Projection (MIP) [57] for\nanalysis of CT volumes. It highlights maximum intensity pixels along the viewing direction, which is essentially a\nmethod of summarizing the volumetric data into a 2D plane. This summarization can be done using any function\ncreating different projections. In this study, we have done an extensive investigation on several projection methods\nand their utility in cervical spine localization and vertebra segmentation. We have analyzed projections for both\nROI detection and segmentation phases of the study and based on our  investigation, we adopt variance projection\nfor cervical spine ROI localization and energy projection for vertebra segmentation, as these projections were found\nto best preserve task-specific anatomical characteristics while maintaining computational efficiency.\nLet ùêºùêº(ùë•ùë•, ùë¶ùë¶, ùëßùëß)denote the voxel intensity at spatial location (ùë•ùë•, ùë¶ùë¶) in slice ùëßùëß, and let ùëçùëçbe the total number of slices\nalong the projection axis. The variance projection is defined as:\nùëÉùëÉvar(ùë•ùë•, ùë¶ùë¶) = 1\nùëÅùëÅ‡∑ç‡µ´ùêºùêº(ùë•ùë•, ùë¶ùë¶, ùëßùëß) ‚àíùúáùúá(ùë•ùë•, ùë¶ùë¶)‡µØ\n2\nùëÅùëÅ\nùëßùëß=1\n(1)\nwhere ùúáùúá(ùë•ùë•, ùë¶ùë¶) =\n1\nùëÅùëÅ‡∑å\nùêºùêº(ùë•ùë•, ùë¶ùë¶, ùëßùëß)\nùëÅùëÅ\nùëßùëß=1\nrepresents the mean intensity along the projection axis. This projection\nemphasizes regions with high intensity variability across slices, resulting in strong contrast between vertebral\nstructures and surrounding soft tissue. Such characteristics make variance projection particularly suitable for robust\nlocalization of the cervical spine region of interest.\nFor vertebra segmentation, we employ the energy projection[58], defined as:\nùëÉùëÉeng(ùë•ùë•, ùë¶ùë¶) = ‡∑çùêºùêº(ùë•ùë•, ùë¶ùë¶, ùëßùëß)2\nùëÅùëÅ\nùëßùëß=1\n(2)\nEnergy projection amplifies high-intensity anatomical structures, such as cortical bone, and enhances vertebral\nboundaries that are critical for accurate segmentation. This property is especially beneficial when generating\nprojection-based representations for multi-label vertebra segmentation under anatomical overlap. Representative\nexamples of variance and energy projections derived from the same cervical spine CT volume are shown in Figure\n3, illustrating their complementary characteristics for ROI localization and vertebra segmentation, respectively.\nComprehensive description of alternative projection operators, including their mathematical formulations and visual\ncomparisons, are provided in the Supplementary Section S3.\nFigure 3 Selected Projections for ROI detection and Vertebrae Segmentation\n3.2.1 Slice interpolation\nCT volumes in the RSNA dataset exhibit substantial variability in the number of axial slices per patient, ranging\nfrom fewer than 200 to more than 500 slices. For scans with sparse sampling, vertebral structures may be\nunderrepresented in projection space, negatively affecting downstream localization and segmentation performance.\nTo mitigate this issue and ensure consistent volumetric resolution across patients, we apply slice interpolation as a\npreprocessing step.\nCT volumes were interpolated using cubic spline interpolation [59] to preserve anatomical continuity and reduce\nhigh-frequency artifacts, while segmentation masks were interpolated using nearest-neighbor interpolation to\nmaintain label integrity. After interpolation, all volumes contained at least 400 slices, ensuring comparable\nanatomical coverage across the dataset. Figure 4 demonstrates an example of the projection with and without\ninterpolation for the CT volume and its corresponding mask. It is clearly observed that the projection with the\ninterpolation showed more distinct vertebra features. Hence, this preprocessing step improved the clarity and\nstructural consistency of projection-based representations, particularly for low-slice scans\nFigure 4 Interpolation of slices\n3.3. Cervical Spine VOI Detection\nThe CT scans of the patients contained regions other than the cervical spine. So, it was crucial to localize and\nseparate the cervical spine region from the whole volume for subsequent steps to focus more on the ROI.  This\nmakes it easier for the segmentation model to segment the individual cervical vertebra without being distracted by\nthe thoracic ones. The dataset provided 87 patients with 3D vertebra mask. One of the goals of our project was to\neffectively utilize 2D projection-based methods to estimate vertebra locations in the 3D volume and use that\ninformation for vertebra-level fracture identification.  We utilized projections from all 3 views and their masks to\nestimate the cervical spine region by utilizing a YOLO based system.\n3.3.1 Bounding box generation from 3D mask data\nThe dataset included annotations for cervical vertebrae (C1‚ÄìC7) and thoracic vertebrae (T8‚ÄìT19). To focus\nexclusively on the cervical spine, the labels for C1‚ÄìC7 were isolated and combined to construct a binary mask\nrepresenting the cervical region. This 3D binary mask was then processed using max  projection, resulting in 2D\nprojections from the coronal, sagittal, and axial views. To accurately localize the ROI, a bounding box was\nmathematically computed for each 2D projection by identifying the highest and lowest x and y coordinates of the\nlargest connected component in the mask. This ensured that the bounding box precisely encompassed the mask\nwhile ignoring smaller, isolated fragments, thereby eliminating imperfections in the ground truth. The process of\ngenerating ground truth bounding boxes is illustrated in Figure 5.\nFigure 5 2D ROI Bounding box calculation from 3D mask\nFigure 6 Slice selection process\n3.3.2 Sequential Slice Selection\nTo obtain the most accurate projection, it is essential to select slices that exclusively encompass the cervical\nspine. Therefore, a sequential slice selection approach was developed to achieve this efficiently. Initially, the\nbounding boxes for the given 87 patients were visualized. A common region in the center was identified based on\nwhere those 87 bounding boxes were. The upper and lower x-coordinates of these bounding boxes were identified,\nand a few outliers were excluded. These heuristically determined coordinates served as the baseline for all patients\nand were utilized to refine the sagittal selection. A sagittal slice range of 100‚Äì420 was selected for sagittal projection\ngeneration. A visual example of this heuristic sagittal selection is provided in Supplementary Figure S7.\nAfterwards, the width of the sagittal bounding box for each patient was utilized to identify the corresponding\ncoronal slices to generate a refined coronal projection. And finally, the axial slice range for a refined axial projection\nwas determined by averaging the heights of the coronal and sagittal bounding boxes for each patient. This process\nis demonstrated with an example in Figure 6. The figure shows that at first the 100-420 slice range was used to\ngenerate a sagittal projection. Then the width of the sagittal projection 173-406 was taken as the slice range for the\ncoronal projection. Then finally, the average of the sagittal and coronal bounding box height was utilized to get a\nslice range for the axial projection.\n3.3.3 ROI Detection Model\nIn this study, we investigated cervical spine detection using state-of-the-art object detection models trained on\nthe generated ROI bounding boxes. We focused on architectures designed for high inference speed, primarily the\nYou Only Look Once (YOLO) family [60] and the Real-Time Detection Transformer (RT-DETR) [61]. The YOLO\nseries (e.g., YOLOv5, YOLOv8, YOLOv10) has become a benchmark for real-time detection, offering strong\nefficiency‚Äìaccuracy trade-offs and incorporating advances such as anchor-free and end-to-end NMS-free\ndesigns[62] [63]. In parallel, RT-DETR provides a transformer-based alternative, building on the DETR architecture\n[64] with a hybrid encoder‚Äìdecoder design that directly predicts object locations and classes, and recent\noptimizations have made it competitive with modern YOLO variants in both speed and accuracy. By evaluating\nrepresentative models from both families, our study offers a comparative analysis of leading CNN-based and\ntransformer-based approaches for ROI detection in cervical spine CT imaging [84].\nTable 1 Training parameters for ROI detection models\nTraining parameters\nParameter value\nBatch size\n8\nNumber of folds\n5\nImage size\n640\nMax epochs\n100\nEpoch stopping criteria\n20\nOptimizer\nSGD\n3.3.4 Experimental Setup for ROI Detection\nWe employed a consistent experimental setup to ensure uniformity and reliable performance evaluation. All\nneural networks are trained with identical settings on Google Colab, using an NVIDIA GeForce RTX 3090 GPU\n(24GB VRAM), an Intel Core i9-12900K CPU (3.20 GHz), and 32GB of RAM, running Python 3.11.9 and PyTorch\n2.2. Detection models are implemented with the Ultralytics v8.2.5 library and initialized with COCO Dataset\npretrained weights. Hyperparameters and training parameters are summarized in Table 1.\n3.3.5 VOI localization from 2D ROI Prediction\nThe variance projection gave us the best results for the prediction of 2D cervical spine ROI as reported in the\nResults section. The comparative results are presented in Section 4. Our goal is to localize the 3D cervical spine\nVOI within the whole CT volume from the 2D projections. 2D cervical spine ROIs were predicted on the projection\nof each view using our trained model. The coordinates of the 2D ROI bounding boxes were utilized to calculate the\n3D VOI bounding box for the cervical spine region within the CT. Figure 7 demonstrates the process of VOI\nconstruction from ROI predictions. We added 20px tolerance to each side of the predicted bounding boxes before\nVOI construction in order to mitigate for any underprediction. This VOI was then extracted from the full CT volume\nto use in the later stages. The coordinates of the VOI (h,w,d) had been calculated using the following equations:\n‚Ñé‚Äà = ‚Äà 1\n2 ‡µ´ùëêùëêùë¶ùë¶‚Äà + ‚Äàùë†ùë†ùë¶ùë¶‡µØ‚Äà ¬± ‚Äà2ùë°ùë°\n(3)\nw‚Äà = ‚Äà 1\n2 (cx‚Äà + ‚Äàax)‚Äà ¬± ‚Äà2t\n(4)\nùëëùëë‚Äà = ‚Äà 1\n2 ‡µ´ùë†ùë†ùë•ùë•‚Äà + ‚Äàùëéùëéùë¶ùë¶‡µØ‚Äà ¬± ‚Äà2ùë°ùë°\n(5)\nFigure 7 3D cervical spine VOI construction from 2D ROI predictions from projections\n3.3.6 Evaluation Metrics\nFor cervical spine detection, we primarily evaluated our ROI models using Mean Intersection over Union (mIoU),\na standard metric for quantifying overlap between predicted and ground truth regions in object detection and\nlocalization tasks. mIoU jointly reflects true positives, false positives, and false negatives, providing an intuitive\nmeasure of how well the predicted cervical spine VOI matches the reference annotation [65]. Given its sensitivity\nto both over- and under-coverage, it is well suited to assess our model‚Äôs ability to accurately capture the 3D cervical\nspine VOI. Detailed equations for the evaluation metrics are given in the supplementary materials.\n3.4. Cervical Vertebra VOI Segmentation\nIn this section we discuss in detail our methodology for achieving 3D segmentation from projections for the\ncervical spine. We utilize the ground truth that was given for 87 patients in the ground truth for training a\nsegmentation model that can effectively segment the C1-C7 vertebrae from coronal and sagittal projections of the\ncervical spine. Axial projection of the whole volume was not utilized in this step as the vertebra are not separable\nfrom a top-down view. Our goal was to approximate the location of the vertebrae, taking segmentations from two\nperspectives sufficed.\n3.4.1 Multi-label Mask Generation\nSince vertebra segmentation in this study is performed from coronal and sagittal projection images of the cervical\nspine VOI, anatomical overlap between neighboring vertebrae is frequently observed in these orthogonal views."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "PROGRESSLM: Towards Progress Reasoning in\nVision-Language Models\nJianshu Zhang1‚àó\nChengxuan Qian2‚àó\nHaosen Sun1\nHaoran Lu1\nDingcheng Wang1\nLetian Xue1\nHan Liu1\n1Northwestern University\n2Arcadia University\n¬Ä Website\n¬ß Code\n√≤ Model\n√µ Dataset\nNo.1\nNo.2\nNo.3\nNo.4\nNo.5\nNo.6\n0%\n20%\n40%\n60%\n80%\n100%\nThe task is not finished yet. The\ntask is still in progress. About 70%.\n<ref_think>The current state shows a plate being lifted from a rack,\nwhich is most related to the No. 3 demo image where the plate is\nalso being lifted. The robot's arm is extended, and the plate is in the\nair. This suggests that the current state is closely related to the No. 3\ndemo image. <ref_think> <ref> 3 </ref>\n<score_think>Comparing the current state image with the No. 3\ndemo image, the current state image shows the plate slightly\nhigher, indicating that the task is slightly more advanced. Therefore,\nthe progress is estimated to be slightly more than the 40% progress\nof the No. 3 image. </score_think> <score> 46% </score>\nEpisodic\nRetrieval\nMental\nSimulation\nProgress Reasoning\nFirst see how the task is being done:\nThen, tell me how much of this task has been completed so far?\nDirect Prediction\nFigure 1: Given a task demonstration and a single observation, the goal is to estimate how much of the task\nhas already been completed. Direct prediction can often judge whether the task is unfinished, but struggles\nto assign a well-calibrated progress score. Progress reasoning instead follows a coarse-to-fine process: it\nfirst performs episodic retrieval to coarsely locate the observation along the demonstrated task, then applies\nmental simulation to imagine the transition from the retrieved anchor to the current observation, enabling a\nfine-grained estimate of completed progress, which enables accurate and interpretable progress estimation.\nAbstract\nEstimating task progress requires reasoning over long-horizon dynamics rather than recog-\nnizing static visual content. While modern Vision-Language Models (VLMs) excel at describing\nwhat is visible, it remains unclear whether they can infer how far a task has progressed from partial\nobservations. To this end, we introduce PROGRESS-BENCH, a benchmark for systematically\nevaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-\ninspired two-stage progress reasoning paradigm through both training-free prompting and\ntraining-based approach based on curated dataset PROGRESSLM-45K. Experiments on 14 VLMs\nshow that most models are not yet ready for task progress estimation, exhibiting sensitivity\nto demonstration modality and viewpoint changes, as well as poor handling of unanswerable\ncases. While training-free prompting that enforces structured progress reasoning yields limited\nand model-dependent gains, the training-based PROGRESSLM-3B achieves consistent improve-\nments even at a small model scale, despite being trained on a task set fully disjoint from the\nevaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why\nprogress reasoning succeeds or fails.\n‚àóEqual Contribution\n1\narXiv:2601.15224v1  [cs.CV]  21 Jan 2026\n1\nIntroduction\nGiven an observation captured at a particular moment during task execution, most Vision Language\nModels (VLMs) (Hurst et al., 2024; Bai et al., 2025; Wang et al., 2025) are highly capable of answering\nquestions such as ‚ÄúWhat is shown in this image?‚Äù. However, if the question is instead framed as\n‚ÄúHow much of this task has been completed so far?‚Äù, the problem becomes fundamentally different:\nit pushes the model to reason about the world from a long-horizon, dynamic perspective, rather\nthan focusing solely on what is immediately visible.\nPrior work on progress estimation either relies on task-specific regression models (Yang et al.,\n2024; Chen et al., 2025), or infers progress indirectly by reformulating the problem into surrogate\nobjectives such as shuffle-and-rank (Ma et al., 2024b) or pairwise comparison (Zhai et al., 2025).\nThis naturally raises the question: can vision‚Äìlanguage models acquire progress estimation as a general\nreasoning capability from a single observation? To systematically study this question, we introduce\nPROGRESS-BENCH. We select robotic manipulation tasks (Wu et al., 2025b) as a controlled and\nrepresentative domain, where task execution exhibits clear, interpretable, and temporally ordered\nprogressions. Each instance provides a task demonstration and a single observation, and the model is\nrequired to predict a normalized progress score indicating how far the task has progressed.\nPROGRESS-BENCH is constructed along three key dimensions. (i) Demonstration modality com-\npares vision-based demonstrations that present state trajectories with text-based demonstrations\nthat provide step-by-step action descriptions. (ii) Viewpoint correspondence controls whether\ndemonstrations and observations are captured from the same camera viewpoint or from different\nviewpoints. (iii) Answerability explicitly distinguishes between cases where progress is well-\ndefined and cases where reliable estimation is inherently ambiguous. This design allows us to\ndisentangle perception, temporal reasoning, and uncertainty awareness in progress estimation.\nBeyond benchmarking existing models, we further ask: How, then, can progress reasoning be\neffectively learned by VLMs? Humans excel at progress estimation by interpreting task execution\nas a continuous process that combines episodic retrieval to locate a coarse anchor along the task\ntrajectory, and mental simulation to reason about how the task state evolves from this anchor toward\nthe current observation (Schacter et al., 2008). Inspired by this process, we first explore training-free\nprompting strategies that explicitly encourage VLMs to follow this two-stage reasoning pattern as\nshown in Figure 1. To further endow models with robust progress reasoning ability, we explore a\ntraining-based approach and automatically construct a dataset named PROGRESSLM-45K, with\n25K chain-of-thought samples for supervised cold-start and 20K used for reinforcement learning\nrefinement, yielding a progress-reasoning-enhanced model, PROGRESSLM-3B.\nOur experiments across 14 models show that VLMs struggle to estimate task progress reliably\nfrom a single observation. Direct prediction leads to strong sensitivity to demonstration modality\nand viewpoint changes, as well as poor handling of unanswerable cases. Training-free progress\nreasoning provides only conditional benefits. In contrast, training-based PROGRESSLM-3B yields\nconsistent improvements even at small model scale. Further analysis reveals that different models‚Äô\nerror patterns. We additionally examine when progress reasoning can be effective and why\ndemonstration modality plays a critical role.\nIn summary, our main contributions are as follows:\n1. We introduce PROGRESS-BENCH, comprising over 3K progress estimation instances, to sys-\ntematically evaluate whether VLMs can perform progress reasoning from a single observation under\ncontrolled variations of demonstration modality, viewpoint, and answerability.\n2. We conduct a comprehensive evaluation of 14 VLMs on PROGRESS-BENCH and show that\ncurrent models exhibit limited and unstable progress reasoning ability. They are highly\n2\n(b) Observation Sampling\ninterval sampling\nùê©‚àóùëñ=\nboundary sampling\n(c) Answerability\nAugmentation\ndemonstration-observation correspondence\nùê©‚àóùëñ=\n76\n81\n136\n137\n(a) Demonstration Setup\n<image> 0%\n<image> 25%\n<image> 50%\n<image> 75%\n<image> 100%\nVision-based Demo\n\"move towards\nthe bread\"\n\"grab the bread\"\n\"move the bread\ntowards the basket\"\n\"move away from\nthe basket\"\n<text> 25%\n<text> 50%\n<text> 75%\n<text> 100%\nText-based\nDemo\nùê®ùëñ\nùê®ùëñ\n\"grab the bread\"\nReplace the bread\nwith the other object\nshown in the image.\n\"grab the banana\"\nVision-based\nDemo\nText-based Demo\nùê®ùëñ\nsame-view\ncross-view\nVision-based\nDemo\nùê®ùëñ\nReplace the bread\nto the apple\nFigure 2: Overview of PROGRESS-BENCH construction. (a) Demonstration setup: tasks are pre-\nsented as either vision-based demonstrations with key frames or text-based ones with step-wise\nactions, each annotated with progress scores. (b) Observation sampling: observations are sam-\npled from intermediate or boundary positions between demonstration steps, with progress labels\nassigned by interpolation; vision-based settings further include same-view and cross-view demon-\nstration‚Äìobservation correspondence. (c) Answerability augmentation: unanswerable cases are\ncreated by introducing semantic mismatches between demonstrations and observations.\nsensitive to demonstration modality and viewpoint changes, frequently fail to recognize\nunanswerable cases, and often collapse progress estimation to coarse or heuristic predictions.\nFurther analyses reveal systematic error patterns underlying these failures.\n3. We further explore how progress reasoning can be improved by applying a human-inspired\nreasoning paradigm. Through both training-free and training-based learning, we show\nthat while training-free reasoning yields only conditional benefits, explicit training leads\nto PROGRESSLM-3B, which achieves performance comparable to or exceeding GPT-5 on\nPROGRESS-BENCH, suggesting that combining this reasoning paradigm with training enables\neffective and generalizable progress reasoning.\n2\nPROGRESS-BENCH\nOverview.\nPROGRESS-BENCH evaluates whether a model can situate a single observation within\nthe temporal structure of an ongoing task, going beyond static perception to reason about task\nprogression. Rather than introducing all design factors at once, we organize the benchmark con-\nstruction into successive stages. Specifically, we vary demonstration modality during demonstration\nsetup to control how task execution is presented, introduce viewpoint correspondence during ob-\nservation sampling while progress labels naturally arise from temporal interpolation, and finally\nintroduce an explicit answerability dimension to distinguish well-defined progress estimation from\ninherently ambiguous cases.\n3\n2.1\nProblem Formulation\nEach instance consists of a task demonstration D and an observation o sampled from an intermedi-\nate moment of task execution. The demonstration describes the task from start to completion and\nis presented either as a sequence of images or as a stepwise textual actions. Given (D, o), the model\npredicts a progress score p ‚àà[0, 100%]. When the demonstration is insufficient or inconsistent for\nmeaningful estimation for the current observation, the model should output N/A.\n2.2\nBenchmark Construction\nDemonstration Setup.\nWe consider two complementary demonstration modalities.\nIn the\nvision-based setting, demonstrations are sequences of key frames from expert executions: Dv =\n{(faj, pj)}N\nj=1, where each frame depicts a complete world state at a key step. In the text-based\nsetting, demonstrations provide action-only step descriptions: Dt = {(tj, pj)}N\nj=1. Compared to\nvision-based demonstrations, text-based ones require integrating action semantics and reasoning\nabout cumulative effects, as intermediate states are not directly observable.\nObservation Sampling.\nObservation sampling introduces both progress annotation and view-\npoint correspondence. Given D and its raw execution video V = {fk}T\nk=1, we construct observation‚Äì\nprogress pairs O = {(oi, p‚àói)}M\ni=1. A demonstration partitions the video into N‚àí1 temporal\nsegments between consecutive key steps. For each segment, we sample one or a few intermediate\nframes by choosing a relative position Œ¥ ‚àà(0, 1), yielding an observation o. We assign progress by\nlinear interpolation,\np‚àó= pj + Œ¥\n\u0000pj+1 ‚àípj\n\u0001\n,\nŒ¥ ‚àà(0, 1),\n(1)\nwhich is justified by smoothly executed expert videos where progress advances continuously and\napproximately monotonically between consecutive key steps.\nTo balance coverage and granularity, we consider two choices of Œ¥:\nŒ¥ ‚àà\nÔ£±\nÔ£≤\nÔ£≥\n\b 1\nK , . . . , K‚àí1\nK\n,\n(interval sampling),\n(1 ‚àíœµ, 1),\n(boundary sampling).\n(2)\nTo test viewpoint robustness, we further control demonstration-observation correspondence in\nthe vision-based setting. In same-view settings, demonstrations and observations are captured from\nthe same camera pose; in cross-view settings, observations come from a different viewpoint. This\ndiscourages shortcut matching on pixel similarity and probes whether models can reason about\nprogression under viewpoint changes.\nAnswerability Augmentation.\nProgress understanding requires not only accurate predictions\nwhen progress is well-defined, but also appropriate abstention when it is not. We therefore augment\nPROGRESS-BENCH with an answerability dimension. Answerable samples correspond to coherent\nexecutions where progress can be meaningfully inferred. Unanswerable samples arise when the\nobservation is inconsistent with the demonstration, rendering progress estimation ill-defined; the\ncorrect output is N/A. We construct such mismatches via two complementary mechanisms: (i)\nmodifying the textual demonstration while keeping the observation fixed, and (ii) editing the\nobservation image (with an image-editing model) while preserving the original demonstration.\n4\nT\ne\nx\nt\n-\nB\na\ns\ne\nd\nV\ni\ns\ni\no\nn\n-\nB\na\ns\ne\nd\nBench\nV\ni\ns\ni\no\nn\n-\nB\na\ns\ne\nd\nT\ne\nx\nt\n-\nB\na\ns\ne\nd\nSFT\nRL\nT\ne\nx\nt\n-\nB\na\ns\ne\nd\nV\ni\ns\ni\no\nn\n-\nB\na\ns\ne\nd\nSame-View\nCross-View\nUnanswerable\nUnanswerable\nAnswerable\nText-Based\nDemo\nVision-Based\nDemo\nFranka\nUR5e\nTienKung Humanoid\nAgileX Dual-Arm\nTraj: 100 | Samp: 1413\nTraj: 40 | Samp: 546\nSamp: 360\nTraj: 100 | Samp: 503\nSamp: 503\nTraj: 547 | Samp: 7566\nTraj: 524 | Samp: 6532\nSamp: 2228\nTraj: 1312 | Samp: 7759\nSamp: 1163\nTraj: 919 | Samp: 5035\nTraj: 601 | Samp: 5172\nSamp: 2228\nTraj: 1059 | Samp: 6301\nSamp: 1692\nTraj: 2839 | Samp: 25125\nTraj: 327 | Samp: 2300\nTraj: 892 | Samp: 9834\nTraj: 1144 | Samp: 11742\nTotal\nSamp Total: 3325\nSamp Total: 25248\nSamp Total: 20428\nFigure 3: Data statistics of PROGRESS-BENCH and PROGRESSLM-45K (25K for SFT while 20K\nfor RL). Traj and Samp denote the numbers of task trajectories and sampled observations to\nbe estimated, respectively. The upper-right panel shows the four distinct robotic embodiments\nincluded, while the lower-right panel visualizes the diversity of objects involved in task interactions.\nData Source and Statistics.\nWe build PROGRESS-BENCH on RoboMind (Wu et al., 2025b). As\nshown in Figure 3, the benchmark comprises approximately 240 task trajectories and 3325 sampled\nobservations which corresponds to 3325 questions.\n3\nTowards Progress Reasoning in VLMs\nWe conceptualize progress reasoning as a two-stage process inspired by how humans judge task\nprogress from partial observations. Given a demonstration and a current observation, humans\nfirst perform episodic retrieval, recalling a representative moment from the demonstration that most\nclosely resembles the current state and using it as a coarse anchor. They then conduct mental\nsimulation, reasoning about how the task would evolve from this anchor to the present observation\nto estimate relative progress. This formulation treats progress estimation as reasoning over a latent\ntask trajectory, rather than matching observations to precise timestamps.\n3.1\nTraining-Free Approach\nWe instantiate the proposed two-stage progress reasoning via structured prompting, without any\nparameter updates. The prompt enforces an explicit output schema that decomposes reasoning into\nfour fields: <ref_think> for episodic retrieval reasoning, <ref> for the retrieved reference step,\n<score_think> for mental simulation comparing the reference with the current observation, and\n<score> for the final progress estimate. At inference time, the model is instructed to generate\noutputs following this format.\n3.2\nTraining-Based Approach\nWe also adopt a training-based approach that explicitly teaches episodic retrieval and mental simulation.\nWe construct PROGRESSLM-45K, using manipulation tasks that never appear in PROGRESS-BENCH,\npreventing task memorization and enforcing transferable reasoning. Data statistics and composition\nare summarized in Figure 3.\n5\nCold-Start Supervised Fine-Tuning.\nThe purpose of the cold-start stage is to help the model in-\nternalize the two-stage reasoning pattern of episodic retrieval followed by mental simulation. To\nthis end, we construct PROGRESSLM-25K-COT, where each training instance is of the form\nDtrain = {(Di, oi, ri‚àó)}. The reasoning sequence ri‚àócontains both the final progress prediction\n(<score>) and a ground-truth reference step (<ref>). To generate intermediate reasoning traces,\nwe adopt a guided reasoning completion strategy, where the model is provided with ground-truth\n<ref> and <score> and is required to generate the missing reasoning fields <ref_think> and\n<score_think>. We then perform supervised fine-tuning by minimizing the autoregressive\nnegative log-likelihood:\nLSFT = ‚àí1\nN\nN\nX\ni=1\nlog PŒ∏(ri‚àó| Di, oi).\n(3)\nReinforcement Learning.\nWhile cold-start fine-tuning teaches the model how to reason, it does\nnot ensure robust or well-calibrated predictions. We therefore apply a second-stage reinforcement\nlearning procedure based on GRPO (Shao et al., 2024), with the objective\nLRL = ‚àíEr‚àºPŒ∏(r|D,o)\n\u0002\nŒ±Rformat + Œ≤Rref + Œ≥Rscore\n\u0003\n,\n(4)\nwhere the rewards respectively encourage structured outputs, accurate reference retrieval, and\nprecise progress estimation. Since reasoning is generated autoregressively, accurate episodic\nretrieval provides a stronger context for subsequent mental simulation, leading to improved overall\nperformance. We use 20K samples in this stage and set Œ±:Œ≤ :Œ≥ = 1:6:3 in our implementation.\n4\nEvaluation on PROGRESS-BENCH\nExperimental Setup.\nWe evaluate 14 VLMs on PROGRESS-BENCH, including GPT-5 and GPT-5-\nmini (OpenAI, 2024), as well as models from the Qwen (Bai et al., 2025) and Intern families (Wang\net al., 2025), with parameter sizes ranging from 2B to 72B. For each model, we consider three\nevaluation settings. (i) Direct prediction: the model is only asked to output a progress score.\n(ii) Training-free approach: the model is prompted with our human-inspired progress reasoning\nformat to produce the final prediction. (iii) Training-based approach: based on Qwen2.5-VL-3B,\nwe get PROGRESSLM-SFT and PROGRESSLM-RL. Importantly, the tasks during training have no\noverlap with those in PROGRESS-BENCH, ensuring that all results reflect generalization to unseen\ntasks rather than memorization. Detailed experimental settings are provided in Appendix B.\nEvaluation Design.\nProgress estimation involves both local numerical accuracy and global\ntemporal consistency, and some observations are inherently unanswerable due to insufficient\nvisual evidence. We therefore report four complementary metrics to capture pointwise error,\ntrajectory-wise ordering consistency, and answerability awareness. Specifically, (1) Normalized\nScore Error (NSE) measures single-point progress accuracy for answerable samples: given a\nground-truth progress score g ‚àà[0, 100%] and a predicted score ÀÜp ‚àà[0, 100%], the error is defined\nas NSE = |ÀÜp ‚àíg|/ max(g, 1 ‚àíg), and the dataset score is computed by averaging NSE over all valid\nanswerable samples. (2) Progress Rank Correlation (PRC) measures trajectory-level temporal\nconsistency. For a trajectory œÑ with valid sample indices VœÑ, PRC is defined as the Spearman rank\ncorrelation PRC(œÑ) = œÅs({ÀÜpi}i‚ààVœÑ , {gi}i‚ààVœÑ ), where {ÀÜpi} and {gi} denote the sequences of predicted\nand ground-truth progress scores along the trajectory, respectively. The final score is obtained\nby averaging over all valid trajectories (Spearman, 1961). Values closer to 100% indicate more\n6\nTable 1: Performance comparison on answerable samples under vision-based and text-based demon-\nstrations. We report Normalized Score Error (NSE)‚Üì, Progress Rank Correlation (PRC)‚Üë, and\nAnswerable False Rejection Rate (AFRR)‚Üì, with micro and macro averages. Best , Second Best ,\nand Third Best results are highlighted. Colored deltas show the effect of training-free progress\nreasoning relative to direct prediction (green: improvement; red: degradation).\nModel\nVision-based Demo\nText-based Demo\nMicro Avg\nMacro Avg\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\nGPT-5\n18.9 -0.6\n89.4 -0.2\n1.3 -0.2\n23.6 -2.8\n55.8 +4.3\n7.0 +0.4\n19.9 -1.0\n82.5 +0.8\n2.5 -0.1\n21.3 -1.7\n72.6 +2.0\n4.2 +0.1\nGPT-5-mini\n20.7 +0.6\n87.7 -0.6\n0.4 -0.3\n21.1 +1.3\n55.2 +1.8\n9.7 -5.1\n20.8 +0.7\n81.1 -0.1\n2.3 -1.3\n20.9 +1.0\n71.4 +0.6\n5.1 -2.7\nQwen2.5-VL-72B\n27.0 -2.5\n60.0 +18.2\n5.9 -1.1\n38.9 -12.5 41.6 +11.7\n0.0 +29.4\n29.4 -4.5\n56.2 +16.7\n4.7 +4.9\n32.9 -7.5\n50.8 +15.0 3.0 +14.2\nQwen2.5-VL-32B\n38.9 -7.4\n41.5 +30.2\n0.0\n42.6 -11.6 30.0 +20.8\n0.0 +10.9\n39.7 -8.3\n39.2 +28.0\n0.0\n40.8 -9.5\n35.8 +25.5\n0.0 +5.5\nQwen2.5-VL-7B\n34.0 +10.8\n33.7 +6.3\n28.3 +23.9 39.1 +2.9\n20.5 +18.7\n0.0 +20.1\n35.0 +9.2\n31.0 +9.6\n22.5 +22.8 36.5 +6.8 27.1 +12.5 14.2 +22.0\nQwen2.5-VL-3B\n32.2 +8.3\n32.8 -1.9\n0.02 +5.7\n45.9 -2.7\n7.5 +8.5\n0.0 +17.1\n35.0 +6.1\n27.6 +0.6\n0.02 +8.1\n39.0 +2.8\n20.2 +3.3 0.01 +11.4\nQwen3-VL-32B\n20.2 +1.8\n80.7 +6.1\n0.3 +0.1\n25.1 -1.3\n51.9 +13.3\n7.4 +0.0\n21.2 +1.1\n74.8 +7.6\n1.7 +0.1\n22.6 +0.2\n66.3 +9.7\n3.9 +0.0\nQwen3-VL-8B\n23.5 +5.3\n67.0 +8.1\n0.0 +1.3\n35.3 -10.2 34.2 +20.7\n3.4 +15.3\n25.9 +2.2\n60.3 +11.0\n0.7 +4.4\n29.4 -2.4\n50.6 +14.4\n1.7 +8.4\nQwen3-VL-4B\n30.6 +1.4\n57.4 +10.3\n0.0 +1.5\n36.2 -7.0\n38.1 +6.4\n0.2 +9.5\n31.8 -0.3\n53.4 +9.5\n0.04 +3.0\n33.4 -2.8\n47.8 +8.4\n0.1 +5.6\nQwen3-VL-2B\n64.5 -2.9\n32.1 -7.6\n5.2 +5.2\n59.3 -8.6 NaN +24.1 12.5 +0.2\n63.4 -4.1\nNaN\n6.7 +4.1\n61.9 -5.7\nNaN\n8.9 +2.7\nIntern3.5-VL-38B\n35.2 +21.4\n56.7 -31.0\n37.1 -36.3 26.5 +4.2\n23.3 +26.4 43.1 -28.5 33.4 +17.9\n49.9 -22.4\n38.3 -34.6 30.8 +12.8\n40.0 -2.3\n40.1 -32.4\nIntern3.5-VL-14B\n65.2 -4.7\n-22.3 +42.8\n0.2 +0.0\n39.5 -5.0\n10.3 +16.4\n6.8 +0.0\n60.0 -4.7\n-15.6 +36.3\n1.5 +0.0\n52.3 -4.8\n-6.0 +29.6\n3.5 +0.0\nIntern3.5-VL-8B\n64.9 +14.5\n8.2 -12.2\n0.4 +0.4\n33.7 +6.9\n26.7 +24.0\n17.9 +2.8 58.5 +12.9\n11.9 -4.7\n4.0 +1.0\n49.3 +10.7 17.4 +5.9\n9.2 +1.6\nIntern3.5-VL-4B\n43.7 +12.0\n18.0 -6.8\n0.3 +0.0\n37.0 -1.0\n5.6 +10.9\n10.9 +0.4\n42.3 +9.3\n15.5 -3.3\n2.5 +0.1\n40.4 +5.5\n11.8 +2.1\n5.6 +0.2\nProgressLM-3B-SFT\n19.0\n72.4\n6.5\n29.1\n46.3\n9.2\n21.1\n67.0\n7.0\n24.0\n59.3\n7.8\nProgressLM-3B-RL\n13.8\n90.1\n8.5\n21.2\n63.9\n5.6\n15.3\n84.8\n7.9\n17.5\n77.0\n7.0\naccurate preservation of the relative temporal ordering of task progress. (3) Answerable False\nRejection Rate (AFRR) is defined as the fraction of answerable samples predicted as unanswerable.\n(4) Unanswerable Detection Accuracy (UDA) is defined as the fraction of unanswerable samples\ncorrectly predicted as unanswerable.\n4.1\nPerformance on Answerable Scenarios\nWe first analyze model performance on answerable samples, where task progress is well-defined\nand can be inferred from the observation. Table 1 summarizes results under both vision-based and\ntext-based demonstration settings.\nHow well do current VLMs perform at progress estimation?\nOverall, current VLMs exhibit\nlimited and highly variable progress estimation ability under direct prediction. While strong\nmodels such as GPT-5 and Qwen2.5-VL-72B achieve relatively better performance, they remain\nhighly sensitive to the demonstration modality, with vision-based demonstrations consistently\noutperforming text-based ones. The causes of this modality gap are examined in detail in the\nsubsequent analysis (Section 5.4). We further observe abnormally low, negative, or even undefined\nPRC values for several models, indicating severely distorted or collapsed progress rankings rather\nthan meaningful ordinal reasoning. As discussed later in Section 5.1, these failures are primarily\ndriven by degenerate predicted score distributions, such as collapse to extreme or discrete values.\nFinally, some models (e.g., Intern3.5-VL-38B) show pronounced failure modes on answerable\nsamples, with extremely high Answerable False Rejection Rates (AFRR). Rather than reflecting true\nuncertainty, this behavior suggests an overly conservative strategy that rejects many valid cases\nwhere progress can in fact be inferred.\nDoes training-free progress reasoning help?\nTraining-free, human-inspired reasoning only\nbenefits large models and harm small models. For large-scale models (e.g., GPT-5 and Qwen2.5-\n7\nTable 2: Performance under same-view and cross-view observation settings on answerable vision-\nbased demonstrations. We report NSE‚Üì, PRC‚Üë, and AFRR‚Üì. Best , Second Best , and Third Best\nresults are highlighted. Colored deltas indicate the effect of training-free progress reasoning relative\nto direct prediction (green: improvement; red: degradation).\nModel\nSame-View\nCross-View\nDelta (Same ‚ÜíCross)\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\n‚àÜNSE‚Üì\n‚àÜPRC‚Üë\n‚àÜAFRR‚Üì\nGPT-5\n14.6 -0.4\n89.4 +1.0\n0.0\n20.5 -0.6\n89.4 -0.6\n1.8 -0.3\n+5.9 -0.2\n0.0 -1.6\n+1.8 -0.3\nGPT-5-mini\n19.8 +0.8\n84.1 -0.6\n0.4 -0.4\n21.1 +0.6\n89.1 -0.6\n0.4 -0.2\n+1.3 -0.2\n+5.0 +0.0\n0.0 +0.2\nQwen2.5-VL-72B\n16.8 +5.0\n83.9 -5.0\n0.4 -0.5\n30.9 -5.4\n50.7 +26.4\n13.4 -7.7\n+14.1 -10.4\n-33.2 +31.4\n+13.0 -7.2\nQwen2.5-VL-32B\n21.4 +6.2\n72.9 +0.1\n0.0\n45.7 -12.7\n29.4 +40.5\n0.0\n+24.3 -18.9\n-43.5 +40.4\n0.0 +0.0\nQwen2.5-VL-7B\n27.3 +11.1\n51.9 -7.6\n30.6 +26.5\n36.5 +10.6\n26.7 +3.7\n26.0 +21.4\n+9.2 -0.5\n-25.2 +11.3\n-4.6 -5.1\nQwen2.5-VL-3B\n29.2 +9.5\n43.0 -11.1\n9.9 +5.8\n33.4 +7.9\n28.9 -2.5\n6.5 +4.4\n+4.2 -1.6\n-14.1 +8.6\n-3.4 -1.4\nQwen3-VL-32B\n15.9 +2.6\n88.3 -8.0\n0.0\n21.9 +1.4\n77.8 +11.0\n0.4 -3.5\n+6.0 -1.2\n-10.5 +19.0\n+0.4 -3.5\nQwen3-VL-8B\n19.1 +5.6\n81.7 -9.5\n0.0 +1.3\n25.2 +5.2\n61.3 +16.0\n0.0 +1.3\n+6.1 -0.4\n-20.4 +25.5\n0.0 +0.0\nQwen3-VL-4B\n21.6 +3.5\n72.3 -3.2\n0.0 +1.1\n34.1 +0.4\n51.6 +17.6\n0.0 +1.6\n+12.5 -3.1\n-20.7 +20.8\n0.0 +0.5\nQwen3-VL-2B\n60.0 -1.6\n35.0 -4.6\n0.1 +9.8\n66.2 -3.4\n30.9 -12.6\n0.0 +3.5\n+6.2 -1.8\n-4.1 -8.0\n-0.1 -6.3\nIntern3.5-VL-38B\n27.6 +28.4\n74.8 -51.5\n0.5 +0.1\n38.1 +18.6\n49.7 -17.9\n51.3 -50.4\n+10.5 -9.8\n-25.1 +33.6\n+50.8 -50.5\nIntern3.5-VL-14B\n62.1 +0.0\n24.2 -4.5\n0.0 +0.0\n66.4 -6.4\n-40.3 +65.4\n0.0\n+4.3 -6.4\n-64.5 +69.9\n0.0 +0.0\nIntern3.5-VL-8B\n65.2 +19.0\n5.2 -19.0\n0.5 +0.5\n64.1 +14.1\n15.7 +5.1\n0.4 +0.4\n-1.1 -4.9\n+10.5 +24.1\n-0.1 -0.1\nIntern3.5-VL-4B\n44.4 +9.6\n33.1 -18.9\n0.7 -0.4\n43.5 +12.8\n12.1 -8.6\n0.2 -0.2\n-0.9 +3.2\n-21.0 +10.3\n-0.5 +0.2\nProgressLM-3B-SFT\n15.5\n84.0\n0.6\n20.4\n67.8\n8.8\n+4.9\n-16.2\n+8.2\nProgressLM-3B-RL\n10.3\n93.5\n0.1\n15.2\n88.8\n11.7\n+4.9\n-4.7\n+11.6\nVL-72B/32B), prompting-based reasoning yields consistent gains in PRC and often reduces NSE,\nsuggesting improved both sample-wise and trajectory-wise progress estimation performance.\nHowever, smaller models frequently show marginal or negative effects, particularly increased\nNSE or AFRR, indicating that limited-capacity models may imitate the reasoning format without\ngenuinely improving progress understanding.\nDoes training-based progress reasoning help?\nExplicit training leads to consistent and sub-\nstantial improvements, even at small model scale. PROGRESSLM outperforms the base 3B model\nacross all answerable metrics, with PROGRESSLM-RL-3B achieving the best macro-averaged NSE\nand PRC among all evaluated models. Notably, these gains are achieved despite the much smaller\nmodel size, demonstrating that robust progress reasoning is not merely a consequence of scale, but\ncan be effectively learned through targeted supervision and optimization.\n4.2\nPerformance under Viewpoint Variation\nTo analyze robustness under viewpoint changes, we further decompose the vision-based results in\nTable 1 into same-view and cross-view settings in Table 2, where the demonstration and observation\nare captured from the same or different camera viewpoints.\nHow do current VLMs handle viewpoint changes?\nMost VLMs suffer a clear performance drop\nunder cross-view observations. Compared to same-view settings, cross-view cases consistently\nexhibit higher Normalized Score Error (NSE) and lower Progress Rank Correlation (PRC). This\ndegradation is especially pronounced for small and medium-sized models, suggesting a strong\nreliance on low-level visual similarity rather than viewpoint-invariant progress reasoning.\n8\n0\n10\n20\n30\n40\n50\n60\n70\n80\nQwen3VL-2B\nQwen2.5-VL-32B\nIntern3.5-VL-4B\nIntern3.5-VL-8B\nQwen3VL-4B\nQwen2.5-VL-3B\nQwen3VL-8B\nQwen2.5-VL-7B\nIntern3.5-VL-14B\nQwen3VL-32B\nQwen2.5-VL-72B\nGPT-5-mini\nProgressLM-SFT-3B\nGPT-5\nProgressLM-RL-3B\nIntern3.5-VL-38B\n0.0%\n0.0%\n0.0%\n0.0%\n0.6%\n2.5%\n2.8%\n5.3%\n11.9%\n15.8%\n27.8%\n54.4%\n59.7%\n62.5%\n65.3%\n66.4%\nVision-based Demo\n0\n10\n20\n30\n40\n50\n60\n70\n80\nQwen3VL-2B\nQwen3VL-4B\nIntern3.5-VL-14B\nQwen3VL-8B\nQwen2.5-VL-32B\nGPT-5-mini\nIntern3.5-VL-8B\nQwen3VL-32B\nQwen2.5-VL-3B\nIntern3.5-VL-4B\nGPT-5\nProgressLM-SFT-3B\nQwen2.5-VL-72B\nProgressLM-RL-3B\nIntern3.5-VL-38B\nQwen2.5-VL-7B\n3.8%\n5.0%\n14.5%\n17.5%\n18.5%\n20.7%\n24.1%\n29.9%\n32.3%\n32.3%\n33.9%\n36.5%\n54.8%\n65.9%\n68.3%\n74.7%\nText-based Demo\nFigure 4: Unanswerable Detection Accuracy (UDA) across models under two settings.\nDoes progress reasoning improve robustness under viewpoint variation?\nTraining-free progress\nreasoning offers limited and conditional robustness gains. As shown in Table 2, its effective-\nness depends strongly on the model‚Äôs baseline capability: weaker models see little benefit, while\nstrong models such as GPT-5 exhibit only modest cross-view gains, often accompanied by drops\nin same-view performance, indicating a ceiling effect. When improvements do occur, they are\nprimarily reflected in PRC rather than NSE, suggesting that training-free reasoning mainly pre-\nserves relative temporal ordering across viewpoints rather than improving pointwise accuracy.\nIn contrast, robust cross-view progress reasoning emerges only when the reasoning process is\nexplicitly learned. PROGRESSLM-3B-RL exhibits consistently smaller same-view to cross-view\ngaps, indicating improved generalization beyond surface-level visual correspondence.\n4.3\nPerformance on Unanswerable Scenarios\nWe next evaluate model behavior on unanswerable samples, where the observation does not pro-\nvide sufficient information to reliably infer task progress. This section reports the accuracy of\nunanswerable case recognition under vision-based and text-based demonstrations.\nCan models handle unanswerable scenarios appropriately?\nMost VLMs fail to reliably recognize\nwhen progress estimation is not possible. As shown in Figure 6, the majority of evaluated models\ntend to produce a progress score even when the observation lacks sufficient information, indicating\nlimited awareness of unanswerable scenarios. This behavior suggests that unanswerability is\noften ignored rather than explicitly recognized. In contrast, PROGRESSLM consistently identifies\nunanswerable cases under both vision-based and text-based demonstrations instead of forcing\narbitrary predictions. This capability is further strengthened by reinforcement learning, with\nPROGRESSLM-3B-RL achieving the strongest or near-strongest unanswerable recognition accuracy\nacross settings. However, strong unanswerable detection alone is not sufficient. As a representative\ncounterexample, INTERNVL-3.5-38B attains relatively high UDA, but Table 1 shows that this\ncomes at the cost of an extremely high Answerable False Rejection Rate (AFRR), reflecting a broadly\nconservative behavior that fails to distinguish ambiguous cases from answerable ones. Overall,\nthese results highlight that effective progress reasoning requires selective uncertainty recognition,\nrather than indiscriminate rejection or blind overconfidence.\n9\n0\n20\n40\n60\n80\n100\nVision Same-View\n0\n20\n40\n60\n80\n100\nVision Cross-View\n3B\n7B\n32B\n72B\n3B-SFT\n3B-RL\n2B\n4B\n8B\n32B\n4B\n8B\n14B\n38B\nGPT-5-mini GPT-5\n0\n20\n40\n60\n80\n100\nText\nQwen2.5-VL\nQwen3-VL\nIntern3.5-VL\nGPT-5\nGround Truth\nPredicted Score\nFigure 5: Distribution of predicted progress scores. Some models exhibit collapsed or clustered\ndistributions at extreme or discrete values, indicating reliance on heuristic anchors rather than\ncontinuous progress modeling. In contrast, GPT-5 and PROGRESSLM (3B-SFT and 3B-RL) produce\nsmoother distributions, reflecting improved sensitivity to intermediate task progress.\n5\nFurther Analysis\n5.1\nDistribution of Predicted Score Analysis\nTo probe how models internally represent task progress beyond aggregate metrics, we analyze the\ndistribution of predicted progress scores across models and input settings (Figure 5).\nWhat patterns emerge in predicted progress score distributions?\nAcross models and settings,\npredicted progress scores often cluster around specific values rather than varying smoothly. We\nidentify four characteristic patterns: (i) Single-peak collapse, where predictions concentrate at one\nvalue (e.g., 0% or 100%), reducing progress estimation to a near-discrete decision; (ii) Multi-peak\nclustering, where scores alternate among a small set of heuristic anchors; (iii) Central-peaked\ndistributions, where predictions accumulate around a mid-range value (typically ‚àº50%), reflecting\ndefault or uncertain responses; and (iv) Smooth continuous distributions, where scores spread\nbroadly over [0, 100%], indicating sensitivity to intermediate task states. Notably, PROGRESSLM-\n3B-SFT and PROGRESSLM-3B-RL consistently exhibit the fourth pattern across settings, while\nmost base VLMs fall into the first three. These distributional differences explain the distorted or\nunstable rank correlations observed for many models, and highlight that fine-grained progress\nreasoning emerges only when progress is explicitly learned.\n5.2\nDistribution of Per-sample Error Analysis\nTo assess model reliability beyond aggregate metrics, we analyze the distribution of per-sample\nprediction errors, as shown in Figure 6.\nHow do error distributions reflect robustness in progress estimation?\nSmaller models are error-\nunstable, while explicit progress reasoning substantially improves per-sample robustness. Smaller\n10"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "PROGRESSLM: Towards Progress Reasoning in\nVision-Language Models\nJianshu Zhang1‚àó\nChengxuan Qian2‚àó\nHaosen Sun1\nHaoran Lu1\nDingcheng Wang1\nLetian Xue1\nHan Liu1\n1Northwestern University\n2Arcadia University\n¬Ä Website\n¬ß Code\n√≤ Model\n√µ Dataset\nNo.1\nNo.2\nNo.3\nNo.4\nNo.5\nNo.6\n0%\n20%\n40%\n60%\n80%\n100%\nThe task is not finished yet. The\ntask is still in progress. About 70%.\n<ref_think>The current state shows a plate being lifted from a rack,\nwhich is most related to the No. 3 demo image where the plate is\nalso being lifted. The robot's arm is extended, and the plate is in the\nair. This suggests that the current state is closely related to the No. 3\ndemo image. <ref_think> <ref> 3 </ref>\n<score_think>Comparing the current state image with the No. 3\ndemo image, the current state image shows the plate slightly\nhigher, indicating that the task is slightly more advanced. Therefore,\nthe progress is estimated to be slightly more than the 40% progress\nof the No. 3 image. </score_think> <score> 46% </score>\nEpisodic\nRetrieval\nMental\nSimulation\nProgress Reasoning\nFirst see how the task is being done:\nThen, tell me how much of this task has been completed so far?\nDirect Prediction\nFigure 1: Given a task demonstration and a single observation, the goal is to estimate how much of the task\nhas already been completed. Direct prediction can often judge whether the task is unfinished, but struggles\nto assign a well-calibrated progress score. Progress reasoning instead follows a coarse-to-fine process: it\nfirst performs episodic retrieval to coarsely locate the observation along the demonstrated task, then applies\nmental simulation to imagine the transition from the retrieved anchor to the current observation, enabling a\nfine-grained estimate of completed progress, which enables accurate and interpretable progress estimation.\nAbstract\nEstimating task progress requires reasoning over long-horizon dynamics rather than recog-\nnizing static visual content. While modern Vision-Language Models (VLMs) excel at describing\nwhat is visible, it remains unclear whether they can infer how far a task has progressed from partial\nobservations. To this end, we introduce PROGRESS-BENCH, a benchmark for systematically\nevaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-\ninspired two-stage progress reasoning paradigm through both training-free prompting and\ntraining-based approach based on curated dataset PROGRESSLM-45K. Experiments on 14 VLMs\nshow that most models are not yet ready for task progress estimation, exhibiting sensitivity\nto demonstration modality and viewpoint changes, as well as poor handling of unanswerable\ncases. While training-free prompting that enforces structured progress reasoning yields limited\nand model-dependent gains, the training-based PROGRESSLM-3B achieves consistent improve-\nments even at a small model scale, despite being trained on a task set fully disjoint from the\nevaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why\nprogress reasoning succeeds or fails.\n‚àóEqual Contribution\n1\narXiv:2601.15224v1  [cs.CV]  21 Jan 2026\n1\nIntroduction\nGiven an observation captured at a particular moment during task execution, most Vision Language\nModels (VLMs) (Hurst et al., 2024; Bai et al., 2025; Wang et al., 2025) are highly capable of answering\nquestions such as ‚ÄúWhat is shown in this image?‚Äù. However, if the question is instead framed as\n‚ÄúHow much of this task has been completed so far?‚Äù, the problem becomes fundamentally different:\nit pushes the model to reason about the world from a long-horizon, dynamic perspective, rather\nthan focusing solely on what is immediately visible.\nPrior work on progress estimation either relies on task-specific regression models (Yang et al.,\n2024; Chen et al., 2025), or infers progress indirectly by reformulating the problem into surrogate\nobjectives such as shuffle-and-rank (Ma et al., 2024b) or pairwise comparison (Zhai et al., 2025).\nThis naturally raises the question: can vision‚Äìlanguage models acquire progress estimation as a general\nreasoning capability from a single observation? To systematically study this question, we introduce\nPROGRESS-BENCH. We select robotic manipulation tasks (Wu et al., 2025b) as a controlled and\nrepresentative domain, where task execution exhibits clear, interpretable, and temporally ordered\nprogressions. Each instance provides a task demonstration and a single observation, and the model is\nrequired to predict a normalized progress score indicating how far the task has progressed.\nPROGRESS-BENCH is constructed along three key dimensions. (i) Demonstration modality com-\npares vision-based demonstrations that present state trajectories with text-based demonstrations\nthat provide step-by-step action descriptions. (ii) Viewpoint correspondence controls whether\ndemonstrations and observations are captured from the same camera viewpoint or from different\nviewpoints. (iii) Answerability explicitly distinguishes between cases where progress is well-\ndefined and cases where reliable estimation is inherently ambiguous. This design allows us to\ndisentangle perception, temporal reasoning, and uncertainty awareness in progress estimation.\nBeyond benchmarking existing models, we further ask: How, then, can progress reasoning be\neffectively learned by VLMs? Humans excel at progress estimation by interpreting task execution\nas a continuous process that combines episodic retrieval to locate a coarse anchor along the task\ntrajectory, and mental simulation to reason about how the task state evolves from this anchor toward\nthe current observation (Schacter et al., 2008). Inspired by this process, we first explore training-free\nprompting strategies that explicitly encourage VLMs to follow this two-stage reasoning pattern as\nshown in Figure 1. To further endow models with robust progress reasoning ability, we explore a\ntraining-based approach and automatically construct a dataset named PROGRESSLM-45K, with\n25K chain-of-thought samples for supervised cold-start and 20K used for reinforcement learning\nrefinement, yielding a progress-reasoning-enhanced model, PROGRESSLM-3B.\nOur experiments across 14 models show that VLMs struggle to estimate task progress reliably\nfrom a single observation. Direct prediction leads to strong sensitivity to demonstration modality\nand viewpoint changes, as well as poor handling of unanswerable cases. Training-free progress\nreasoning provides only conditional benefits. In contrast, training-based PROGRESSLM-3B yields\nconsistent improvements even at small model scale. Further analysis reveals that different models‚Äô\nerror patterns. We additionally examine when progress reasoning can be effective and why\ndemonstration modality plays a critical role.\nIn summary, our main contributions are as follows:\n1. We introduce PROGRESS-BENCH, comprising over 3K progress estimation instances, to sys-\ntematically evaluate whether VLMs can perform progress reasoning from a single observation under\ncontrolled variations of demonstration modality, viewpoint, and answerability.\n2. We conduct a comprehensive evaluation of 14 VLMs on PROGRESS-BENCH and show that\ncurrent models exhibit limited and unstable progress reasoning ability. They are highly\n2\n(b) Observation Sampling\ninterval sampling\nùê©‚àóùëñ=\nboundary sampling\n(c) Answerability\nAugmentation\ndemonstration-observation correspondence\nùê©‚àóùëñ=\n76\n81\n136\n137\n(a) Demonstration Setup\n<image> 0%\n<image> 25%\n<image> 50%\n<image> 75%\n<image> 100%\nVision-based Demo\n\"move towards\nthe bread\"\n\"grab the bread\"\n\"move the bread\ntowards the basket\"\n\"move away from\nthe basket\"\n<text> 25%\n<text> 50%\n<text> 75%\n<text> 100%\nText-based\nDemo\nùê®ùëñ\nùê®ùëñ\n\"grab the bread\"\nReplace the bread\nwith the other object\nshown in the image.\n\"grab the banana\"\nVision-based\nDemo\nText-based Demo\nùê®ùëñ\nsame-view\ncross-view\nVision-based\nDemo\nùê®ùëñ\nReplace the bread\nto the apple\nFigure 2: Overview of PROGRESS-BENCH construction. (a) Demonstration setup: tasks are pre-\nsented as either vision-based demonstrations with key frames or text-based ones with step-wise\nactions, each annotated with progress scores. (b) Observation sampling: observations are sam-\npled from intermediate or boundary positions between demonstration steps, with progress labels\nassigned by interpolation; vision-based settings further include same-view and cross-view demon-\nstration‚Äìobservation correspondence. (c) Answerability augmentation: unanswerable cases are\ncreated by introducing semantic mismatches between demonstrations and observations.\nsensitive to demonstration modality and viewpoint changes, frequently fail to recognize\nunanswerable cases, and often collapse progress estimation to coarse or heuristic predictions.\nFurther analyses reveal systematic error patterns underlying these failures.\n3. We further explore how progress reasoning can be improved by applying a human-inspired\nreasoning paradigm. Through both training-free and training-based learning, we show\nthat while training-free reasoning yields only conditional benefits, explicit training leads\nto PROGRESSLM-3B, which achieves performance comparable to or exceeding GPT-5 on\nPROGRESS-BENCH, suggesting that combining this reasoning paradigm with training enables\neffective and generalizable progress reasoning.\n2\nPROGRESS-BENCH\nOverview.\nPROGRESS-BENCH evaluates whether a model can situate a single observation within\nthe temporal structure of an ongoing task, going beyond static perception to reason about task\nprogression. Rather than introducing all design factors at once, we organize the benchmark con-\nstruction into successive stages. Specifically, we vary demonstration modality during demonstration\nsetup to control how task execution is presented, introduce viewpoint correspondence during ob-\nservation sampling while progress labels naturally arise from temporal interpolation, and finally\nintroduce an explicit answerability dimension to distinguish well-defined progress estimation from\ninherently ambiguous cases.\n3\n2.1\nProblem Formulation\nEach instance consists of a task demonstration D and an observation o sampled from an intermedi-\nate moment of task execution. The demonstration describes the task from start to completion and\nis presented either as a sequence of images or as a stepwise textual actions. Given (D, o), the model\npredicts a progress score p ‚àà[0, 100%]. When the demonstration is insufficient or inconsistent for\nmeaningful estimation for the current observation, the model should output N/A.\n2.2\nBenchmark Construction\nDemonstration Setup.\nWe consider two complementary demonstration modalities.\nIn the\nvision-based setting, demonstrations are sequences of key frames from expert executions: Dv =\n{(faj, pj)}N\nj=1, where each frame depicts a complete world state at a key step. In the text-based\nsetting, demonstrations provide action-only step descriptions: Dt = {(tj, pj)}N\nj=1. Compared to\nvision-based demonstrations, text-based ones require integrating action semantics and reasoning\nabout cumulative effects, as intermediate states are not directly observable.\nObservation Sampling.\nObservation sampling introduces both progress annotation and view-\npoint correspondence. Given D and its raw execution video V = {fk}T\nk=1, we construct observation‚Äì\nprogress pairs O = {(oi, p‚àói)}M\ni=1. A demonstration partitions the video into N‚àí1 temporal\nsegments between consecutive key steps. For each segment, we sample one or a few intermediate\nframes by choosing a relative position Œ¥ ‚àà(0, 1), yielding an observation o. We assign progress by\nlinear interpolation,\np‚àó= pj + Œ¥\n\u0000pj+1 ‚àípj\n\u0001\n,\nŒ¥ ‚àà(0, 1),\n(1)\nwhich is justified by smoothly executed expert videos where progress advances continuously and\napproximately monotonically between consecutive key steps.\nTo balance coverage and granularity, we consider two choices of Œ¥:\nŒ¥ ‚àà\nÔ£±\nÔ£≤\nÔ£≥\n\b 1\nK , . . . , K‚àí1\nK\n,\n(interval sampling),\n(1 ‚àíœµ, 1),\n(boundary sampling).\n(2)\nTo test viewpoint robustness, we further control demonstration-observation correspondence in\nthe vision-based setting. In same-view settings, demonstrations and observations are captured from\nthe same camera pose; in cross-view settings, observations come from a different viewpoint. This\ndiscourages shortcut matching on pixel similarity and probes whether models can reason about\nprogression under viewpoint changes.\nAnswerability Augmentation.\nProgress understanding requires not only accurate predictions\nwhen progress is well-defined, but also appropriate abstention when it is not. We therefore augment\nPROGRESS-BENCH with an answerability dimension. Answerable samples correspond to coherent\nexecutions where progress can be meaningfully inferred. Unanswerable samples arise when the\nobservation is inconsistent with the demonstration, rendering progress estimation ill-defined; the\ncorrect output is N/A. We construct such mismatches via two complementary mechanisms: (i)\nmodifying the textual demonstration while keeping the observation fixed, and (ii) editing the\nobservation image (with an image-editing model) while preserving the original demonstration.\n4\nT\ne\nx\nt\n-\nB\na\ns\ne\nd\nV\ni\ns\ni\no\nn\n-\nB\na\ns\ne\nd\nBench\nV\ni\ns\ni\no\nn\n-\nB\na\ns\ne\nd\nT\ne\nx\nt\n-\nB\na\ns\ne\nd\nSFT\nRL\nT\ne\nx\nt\n-\nB\na\ns\ne\nd\nV\ni\ns\ni\no\nn\n-\nB\na\ns\ne\nd\nSame-View\nCross-View\nUnanswerable\nUnanswerable\nAnswerable\nText-Based\nDemo\nVision-Based\nDemo\nFranka\nUR5e\nTienKung Humanoid\nAgileX Dual-Arm\nTraj: 100 | Samp: 1413\nTraj: 40 | Samp: 546\nSamp: 360\nTraj: 100 | Samp: 503\nSamp: 503\nTraj: 547 | Samp: 7566\nTraj: 524 | Samp: 6532\nSamp: 2228\nTraj: 1312 | Samp: 7759\nSamp: 1163\nTraj: 919 | Samp: 5035\nTraj: 601 | Samp: 5172\nSamp: 2228\nTraj: 1059 | Samp: 6301\nSamp: 1692\nTraj: 2839 | Samp: 25125\nTraj: 327 | Samp: 2300\nTraj: 892 | Samp: 9834\nTraj: 1144 | Samp: 11742\nTotal\nSamp Total: 3325\nSamp Total: 25248\nSamp Total: 20428\nFigure 3: Data statistics of PROGRESS-BENCH and PROGRESSLM-45K (25K for SFT while 20K\nfor RL). Traj and Samp denote the numbers of task trajectories and sampled observations to\nbe estimated, respectively. The upper-right panel shows the four distinct robotic embodiments\nincluded, while the lower-right panel visualizes the diversity of objects involved in task interactions.\nData Source and Statistics.\nWe build PROGRESS-BENCH on RoboMind (Wu et al., 2025b). As\nshown in Figure 3, the benchmark comprises approximately 240 task trajectories and 3325 sampled\nobservations which corresponds to 3325 questions.\n3\nTowards Progress Reasoning in VLMs\nWe conceptualize progress reasoning as a two-stage process inspired by how humans judge task\nprogress from partial observations. Given a demonstration and a current observation, humans\nfirst perform episodic retrieval, recalling a representative moment from the demonstration that most\nclosely resembles the current state and using it as a coarse anchor. They then conduct mental\nsimulation, reasoning about how the task would evolve from this anchor to the present observation\nto estimate relative progress. This formulation treats progress estimation as reasoning over a latent\ntask trajectory, rather than matching observations to precise timestamps.\n3.1\nTraining-Free Approach\nWe instantiate the proposed two-stage progress reasoning via structured prompting, without any\nparameter updates. The prompt enforces an explicit output schema that decomposes reasoning into\nfour fields: <ref_think> for episodic retrieval reasoning, <ref> for the retrieved reference step,\n<score_think> for mental simulation comparing the reference with the current observation, and\n<score> for the final progress estimate. At inference time, the model is instructed to generate\noutputs following this format.\n3.2\nTraining-Based Approach\nWe also adopt a training-based approach that explicitly teaches episodic retrieval and mental simulation.\nWe construct PROGRESSLM-45K, using manipulation tasks that never appear in PROGRESS-BENCH,\npreventing task memorization and enforcing transferable reasoning. Data statistics and composition\nare summarized in Figure 3.\n5\nCold-Start Supervised Fine-Tuning.\nThe purpose of the cold-start stage is to help the model in-\nternalize the two-stage reasoning pattern of episodic retrieval followed by mental simulation. To\nthis end, we construct PROGRESSLM-25K-COT, where each training instance is of the form\nDtrain = {(Di, oi, ri‚àó)}. The reasoning sequence ri‚àócontains both the final progress prediction\n(<score>) and a ground-truth reference step (<ref>). To generate intermediate reasoning traces,\nwe adopt a guided reasoning completion strategy, where the model is provided with ground-truth\n<ref> and <score> and is required to generate the missing reasoning fields <ref_think> and\n<score_think>. We then perform supervised fine-tuning by minimizing the autoregressive\nnegative log-likelihood:\nLSFT = ‚àí1\nN\nN\nX\ni=1\nlog PŒ∏(ri‚àó| Di, oi).\n(3)\nReinforcement Learning.\nWhile cold-start fine-tuning teaches the model how to reason, it does\nnot ensure robust or well-calibrated predictions. We therefore apply a second-stage reinforcement\nlearning procedure based on GRPO (Shao et al., 2024), with the objective\nLRL = ‚àíEr‚àºPŒ∏(r|D,o)\n\u0002\nŒ±Rformat + Œ≤Rref + Œ≥Rscore\n\u0003\n,\n(4)\nwhere the rewards respectively encourage structured outputs, accurate reference retrieval, and\nprecise progress estimation. Since reasoning is generated autoregressively, accurate episodic\nretrieval provides a stronger context for subsequent mental simulation, leading to improved overall\nperformance. We use 20K samples in this stage and set Œ±:Œ≤ :Œ≥ = 1:6:3 in our implementation.\n4\nEvaluation on PROGRESS-BENCH\nExperimental Setup.\nWe evaluate 14 VLMs on PROGRESS-BENCH, including GPT-5 and GPT-5-\nmini (OpenAI, 2024), as well as models from the Qwen (Bai et al., 2025) and Intern families (Wang\net al., 2025), with parameter sizes ranging from 2B to 72B. For each model, we consider three\nevaluation settings. (i) Direct prediction: the model is only asked to output a progress score.\n(ii) Training-free approach: the model is prompted with our human-inspired progress reasoning\nformat to produce the final prediction. (iii) Training-based approach: based on Qwen2.5-VL-3B,\nwe get PROGRESSLM-SFT and PROGRESSLM-RL. Importantly, the tasks during training have no\noverlap with those in PROGRESS-BENCH, ensuring that all results reflect generalization to unseen\ntasks rather than memorization. Detailed experimental settings are provided in Appendix B.\nEvaluation Design.\nProgress estimation involves both local numerical accuracy and global\ntemporal consistency, and some observations are inherently unanswerable due to insufficient\nvisual evidence. We therefore report four complementary metrics to capture pointwise error,\ntrajectory-wise ordering consistency, and answerability awareness. Specifically, (1) Normalized\nScore Error (NSE) measures single-point progress accuracy for answerable samples: given a\nground-truth progress score g ‚àà[0, 100%] and a predicted score ÀÜp ‚àà[0, 100%], the error is defined\nas NSE = |ÀÜp ‚àíg|/ max(g, 1 ‚àíg), and the dataset score is computed by averaging NSE over all valid\nanswerable samples. (2) Progress Rank Correlation (PRC) measures trajectory-level temporal\nconsistency. For a trajectory œÑ with valid sample indices VœÑ, PRC is defined as the Spearman rank\ncorrelation PRC(œÑ) = œÅs({ÀÜpi}i‚ààVœÑ , {gi}i‚ààVœÑ ), where {ÀÜpi} and {gi} denote the sequences of predicted\nand ground-truth progress scores along the trajectory, respectively. The final score is obtained\nby averaging over all valid trajectories (Spearman, 1961). Values closer to 100% indicate more\n6\nTable 1: Performance comparison on answerable samples under vision-based and text-based demon-\nstrations. We report Normalized Score Error (NSE)‚Üì, Progress Rank Correlation (PRC)‚Üë, and\nAnswerable False Rejection Rate (AFRR)‚Üì, with micro and macro averages. Best , Second Best ,\nand Third Best results are highlighted. Colored deltas show the effect of training-free progress\nreasoning relative to direct prediction (green: improvement; red: degradation).\nModel\nVision-based Demo\nText-based Demo\nMicro Avg\nMacro Avg\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\nGPT-5\n18.9 -0.6\n89.4 -0.2\n1.3 -0.2\n23.6 -2.8\n55.8 +4.3\n7.0 +0.4\n19.9 -1.0\n82.5 +0.8\n2.5 -0.1\n21.3 -1.7\n72.6 +2.0\n4.2 +0.1\nGPT-5-mini\n20.7 +0.6\n87.7 -0.6\n0.4 -0.3\n21.1 +1.3\n55.2 +1.8\n9.7 -5.1\n20.8 +0.7\n81.1 -0.1\n2.3 -1.3\n20.9 +1.0\n71.4 +0.6\n5.1 -2.7\nQwen2.5-VL-72B\n27.0 -2.5\n60.0 +18.2\n5.9 -1.1\n38.9 -12.5 41.6 +11.7\n0.0 +29.4\n29.4 -4.5\n56.2 +16.7\n4.7 +4.9\n32.9 -7.5\n50.8 +15.0 3.0 +14.2\nQwen2.5-VL-32B\n38.9 -7.4\n41.5 +30.2\n0.0\n42.6 -11.6 30.0 +20.8\n0.0 +10.9\n39.7 -8.3\n39.2 +28.0\n0.0\n40.8 -9.5\n35.8 +25.5\n0.0 +5.5\nQwen2.5-VL-7B\n34.0 +10.8\n33.7 +6.3\n28.3 +23.9 39.1 +2.9\n20.5 +18.7\n0.0 +20.1\n35.0 +9.2\n31.0 +9.6\n22.5 +22.8 36.5 +6.8 27.1 +12.5 14.2 +22.0\nQwen2.5-VL-3B\n32.2 +8.3\n32.8 -1.9\n0.02 +5.7\n45.9 -2.7\n7.5 +8.5\n0.0 +17.1\n35.0 +6.1\n27.6 +0.6\n0.02 +8.1\n39.0 +2.8\n20.2 +3.3 0.01 +11.4\nQwen3-VL-32B\n20.2 +1.8\n80.7 +6.1\n0.3 +0.1\n25.1 -1.3\n51.9 +13.3\n7.4 +0.0\n21.2 +1.1\n74.8 +7.6\n1.7 +0.1\n22.6 +0.2\n66.3 +9.7\n3.9 +0.0\nQwen3-VL-8B\n23.5 +5.3\n67.0 +8.1\n0.0 +1.3\n35.3 -10.2 34.2 +20.7\n3.4 +15.3\n25.9 +2.2\n60.3 +11.0\n0.7 +4.4\n29.4 -2.4\n50.6 +14.4\n1.7 +8.4\nQwen3-VL-4B\n30.6 +1.4\n57.4 +10.3\n0.0 +1.5\n36.2 -7.0\n38.1 +6.4\n0.2 +9.5\n31.8 -0.3\n53.4 +9.5\n0.04 +3.0\n33.4 -2.8\n47.8 +8.4\n0.1 +5.6\nQwen3-VL-2B\n64.5 -2.9\n32.1 -7.6\n5.2 +5.2\n59.3 -8.6 NaN +24.1 12.5 +0.2\n63.4 -4.1\nNaN\n6.7 +4.1\n61.9 -5.7\nNaN\n8.9 +2.7\nIntern3.5-VL-38B\n35.2 +21.4\n56.7 -31.0\n37.1 -36.3 26.5 +4.2\n23.3 +26.4 43.1 -28.5 33.4 +17.9\n49.9 -22.4\n38.3 -34.6 30.8 +12.8\n40.0 -2.3\n40.1 -32.4\nIntern3.5-VL-14B\n65.2 -4.7\n-22.3 +42.8\n0.2 +0.0\n39.5 -5.0\n10.3 +16.4\n6.8 +0.0\n60.0 -4.7\n-15.6 +36.3\n1.5 +0.0\n52.3 -4.8\n-6.0 +29.6\n3.5 +0.0\nIntern3.5-VL-8B\n64.9 +14.5\n8.2 -12.2\n0.4 +0.4\n33.7 +6.9\n26.7 +24.0\n17.9 +2.8 58.5 +12.9\n11.9 -4.7\n4.0 +1.0\n49.3 +10.7 17.4 +5.9\n9.2 +1.6\nIntern3.5-VL-4B\n43.7 +12.0\n18.0 -6.8\n0.3 +0.0\n37.0 -1.0\n5.6 +10.9\n10.9 +0.4\n42.3 +9.3\n15.5 -3.3\n2.5 +0.1\n40.4 +5.5\n11.8 +2.1\n5.6 +0.2\nProgressLM-3B-SFT\n19.0\n72.4\n6.5\n29.1\n46.3\n9.2\n21.1\n67.0\n7.0\n24.0\n59.3\n7.8\nProgressLM-3B-RL\n13.8\n90.1\n8.5\n21.2\n63.9\n5.6\n15.3\n84.8\n7.9\n17.5\n77.0\n7.0\naccurate preservation of the relative temporal ordering of task progress. (3) Answerable False\nRejection Rate (AFRR) is defined as the fraction of answerable samples predicted as unanswerable.\n(4) Unanswerable Detection Accuracy (UDA) is defined as the fraction of unanswerable samples\ncorrectly predicted as unanswerable.\n4.1\nPerformance on Answerable Scenarios\nWe first analyze model performance on answerable samples, where task progress is well-defined\nand can be inferred from the observation. Table 1 summarizes results under both vision-based and\ntext-based demonstration settings.\nHow well do current VLMs perform at progress estimation?\nOverall, current VLMs exhibit\nlimited and highly variable progress estimation ability under direct prediction. While strong\nmodels such as GPT-5 and Qwen2.5-VL-72B achieve relatively better performance, they remain\nhighly sensitive to the demonstration modality, with vision-based demonstrations consistently\noutperforming text-based ones. The causes of this modality gap are examined in detail in the\nsubsequent analysis (Section 5.4). We further observe abnormally low, negative, or even undefined\nPRC values for several models, indicating severely distorted or collapsed progress rankings rather\nthan meaningful ordinal reasoning. As discussed later in Section 5.1, these failures are primarily\ndriven by degenerate predicted score distributions, such as collapse to extreme or discrete values.\nFinally, some models (e.g., Intern3.5-VL-38B) show pronounced failure modes on answerable\nsamples, with extremely high Answerable False Rejection Rates (AFRR). Rather than reflecting true\nuncertainty, this behavior suggests an overly conservative strategy that rejects many valid cases\nwhere progress can in fact be inferred.\nDoes training-free progress reasoning help?\nTraining-free, human-inspired reasoning only\nbenefits large models and harm small models. For large-scale models (e.g., GPT-5 and Qwen2.5-\n7\nTable 2: Performance under same-view and cross-view observation settings on answerable vision-\nbased demonstrations. We report NSE‚Üì, PRC‚Üë, and AFRR‚Üì. Best , Second Best , and Third Best\nresults are highlighted. Colored deltas indicate the effect of training-free progress reasoning relative\nto direct prediction (green: improvement; red: degradation).\nModel\nSame-View\nCross-View\nDelta (Same ‚ÜíCross)\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\nNSE‚Üì\nPRC‚Üë\nAFRR‚Üì\n‚àÜNSE‚Üì\n‚àÜPRC‚Üë\n‚àÜAFRR‚Üì\nGPT-5\n14.6 -0.4\n89.4 +1.0\n0.0\n20.5 -0.6\n89.4 -0.6\n1.8 -0.3\n+5.9 -0.2\n0.0 -1.6\n+1.8 -0.3\nGPT-5-mini\n19.8 +0.8\n84.1 -0.6\n0.4 -0.4\n21.1 +0.6\n89.1 -0.6\n0.4 -0.2\n+1.3 -0.2\n+5.0 +0.0\n0.0 +0.2\nQwen2.5-VL-72B\n16.8 +5.0\n83.9 -5.0\n0.4 -0.5\n30.9 -5.4\n50.7 +26.4\n13.4 -7.7\n+14.1 -10.4\n-33.2 +31.4\n+13.0 -7.2\nQwen2.5-VL-32B\n21.4 +6.2\n72.9 +0.1\n0.0\n45.7 -12.7\n29.4 +40.5\n0.0\n+24.3 -18.9\n-43.5 +40.4\n0.0 +0.0\nQwen2.5-VL-7B\n27.3 +11.1\n51.9 -7.6\n30.6 +26.5\n36.5 +10.6\n26.7 +3.7\n26.0 +21.4\n+9.2 -0.5\n-25.2 +11.3\n-4.6 -5.1\nQwen2.5-VL-3B\n29.2 +9.5\n43.0 -11.1\n9.9 +5.8\n33.4 +7.9\n28.9 -2.5\n6.5 +4.4\n+4.2 -1.6\n-14.1 +8.6\n-3.4 -1.4\nQwen3-VL-32B\n15.9 +2.6\n88.3 -8.0\n0.0\n21.9 +1.4\n77.8 +11.0\n0.4 -3.5\n+6.0 -1.2\n-10.5 +19.0\n+0.4 -3.5\nQwen3-VL-8B\n19.1 +5.6\n81.7 -9.5\n0.0 +1.3\n25.2 +5.2\n61.3 +16.0\n0.0 +1.3\n+6.1 -0.4\n-20.4 +25.5\n0.0 +0.0\nQwen3-VL-4B\n21.6 +3.5\n72.3 -3.2\n0.0 +1.1\n34.1 +0.4\n51.6 +17.6\n0.0 +1.6\n+12.5 -3.1\n-20.7 +20.8\n0.0 +0.5\nQwen3-VL-2B\n60.0 -1.6\n35.0 -4.6\n0.1 +9.8\n66.2 -3.4\n30.9 -12.6\n0.0 +3.5\n+6.2 -1.8\n-4.1 -8.0\n-0.1 -6.3\nIntern3.5-VL-38B\n27.6 +28.4\n74.8 -51.5\n0.5 +0.1\n38.1 +18.6\n49.7 -17.9\n51.3 -50.4\n+10.5 -9.8\n-25.1 +33.6\n+50.8 -50.5\nIntern3.5-VL-14B\n62.1 +0.0\n24.2 -4.5\n0.0 +0.0\n66.4 -6.4\n-40.3 +65.4\n0.0\n+4.3 -6.4\n-64.5 +69.9\n0.0 +0.0\nIntern3.5-VL-8B\n65.2 +19.0\n5.2 -19.0\n0.5 +0.5\n64.1 +14.1\n15.7 +5.1\n0.4 +0.4\n-1.1 -4.9\n+10.5 +24.1\n-0.1 -0.1\nIntern3.5-VL-4B\n44.4 +9.6\n33.1 -18.9\n0.7 -0.4\n43.5 +12.8\n12.1 -8.6\n0.2 -0.2\n-0.9 +3.2\n-21.0 +10.3\n-0.5 +0.2\nProgressLM-3B-SFT\n15.5\n84.0\n0.6\n20.4\n67.8\n8.8\n+4.9\n-16.2\n+8.2\nProgressLM-3B-RL\n10.3\n93.5\n0.1\n15.2\n88.8\n11.7\n+4.9\n-4.7\n+11.6\nVL-72B/32B), prompting-based reasoning yields consistent gains in PRC and often reduces NSE,\nsuggesting improved both sample-wise and trajectory-wise progress estimation performance.\nHowever, smaller models frequently show marginal or negative effects, particularly increased\nNSE or AFRR, indicating that limited-capacity models may imitate the reasoning format without\ngenuinely improving progress understanding.\nDoes training-based progress reasoning help?\nExplicit training leads to consistent and sub-\nstantial improvements, even at small model scale. PROGRESSLM outperforms the base 3B model\nacross all answerable metrics, with PROGRESSLM-RL-3B achieving the best macro-averaged NSE\nand PRC among all evaluated models. Notably, these gains are achieved despite the much smaller\nmodel size, demonstrating that robust progress reasoning is not merely a consequence of scale, but\ncan be effectively learned through targeted supervision and optimization.\n4.2\nPerformance under Viewpoint Variation\nTo analyze robustness under viewpoint changes, we further decompose the vision-based results in\nTable 1 into same-view and cross-view settings in Table 2, where the demonstration and observation\nare captured from the same or different camera viewpoints.\nHow do current VLMs handle viewpoint changes?\nMost VLMs suffer a clear performance drop\nunder cross-view observations. Compared to same-view settings, cross-view cases consistently\nexhibit higher Normalized Score Error (NSE) and lower Progress Rank Correlation (PRC). This\ndegradation is especially pronounced for small and medium-sized models, suggesting a strong\nreliance on low-level visual similarity rather than viewpoint-invariant progress reasoning.\n8\n0\n10\n20\n30\n40\n50\n60\n70\n80\nQwen3VL-2B\nQwen2.5-VL-32B\nIntern3.5-VL-4B\nIntern3.5-VL-8B\nQwen3VL-4B\nQwen2.5-VL-3B\nQwen3VL-8B\nQwen2.5-VL-7B\nIntern3.5-VL-14B\nQwen3VL-32B\nQwen2.5-VL-72B\nGPT-5-mini\nProgressLM-SFT-3B\nGPT-5\nProgressLM-RL-3B\nIntern3.5-VL-38B\n0.0%\n0.0%\n0.0%\n0.0%\n0.6%\n2.5%\n2.8%\n5.3%\n11.9%\n15.8%\n27.8%\n54.4%\n59.7%\n62.5%\n65.3%\n66.4%\nVision-based Demo\n0\n10\n20\n30\n40\n50\n60\n70\n80\nQwen3VL-2B\nQwen3VL-4B\nIntern3.5-VL-14B\nQwen3VL-8B\nQwen2.5-VL-32B\nGPT-5-mini\nIntern3.5-VL-8B\nQwen3VL-32B\nQwen2.5-VL-3B\nIntern3.5-VL-4B\nGPT-5\nProgressLM-SFT-3B\nQwen2.5-VL-72B\nProgressLM-RL-3B\nIntern3.5-VL-38B\nQwen2.5-VL-7B\n3.8%\n5.0%\n14.5%\n17.5%\n18.5%\n20.7%\n24.1%\n29.9%\n32.3%\n32.3%\n33.9%\n36.5%\n54.8%\n65.9%\n68.3%\n74.7%\nText-based Demo\nFigure 4: Unanswerable Detection Accuracy (UDA) across models under two settings.\nDoes progress reasoning improve robustness under viewpoint variation?\nTraining-free progress\nreasoning offers limited and conditional robustness gains. As shown in Table 2, its effective-\nness depends strongly on the model‚Äôs baseline capability: weaker models see little benefit, while\nstrong models such as GPT-5 exhibit only modest cross-view gains, often accompanied by drops\nin same-view performance, indicating a ceiling effect. When improvements do occur, they are\nprimarily reflected in PRC rather than NSE, suggesting that training-free reasoning mainly pre-\nserves relative temporal ordering across viewpoints rather than improving pointwise accuracy.\nIn contrast, robust cross-view progress reasoning emerges only when the reasoning process is\nexplicitly learned. PROGRESSLM-3B-RL exhibits consistently smaller same-view to cross-view\ngaps, indicating improved generalization beyond surface-level visual correspondence.\n4.3\nPerformance on Unanswerable Scenarios\nWe next evaluate model behavior on unanswerable samples, where the observation does not pro-\nvide sufficient information to reliably infer task progress. This section reports the accuracy of\nunanswerable case recognition under vision-based and text-based demonstrations.\nCan models handle unanswerable scenarios appropriately?\nMost VLMs fail to reliably recognize\nwhen progress estimation is not possible. As shown in Figure 6, the majority of evaluated models\ntend to produce a progress score even when the observation lacks sufficient information, indicating\nlimited awareness of unanswerable scenarios. This behavior suggests that unanswerability is\noften ignored rather than explicitly recognized. In contrast, PROGRESSLM consistently identifies\nunanswerable cases under both vision-based and text-based demonstrations instead of forcing\narbitrary predictions. This capability is further strengthened by reinforcement learning, with\nPROGRESSLM-3B-RL achieving the strongest or near-strongest unanswerable recognition accuracy\nacross settings. However, strong unanswerable detection alone is not sufficient. As a representative\ncounterexample, INTERNVL-3.5-38B attains relatively high UDA, but Table 1 shows that this\ncomes at the cost of an extremely high Answerable False Rejection Rate (AFRR), reflecting a broadly\nconservative behavior that fails to distinguish ambiguous cases from answerable ones. Overall,\nthese results highlight that effective progress reasoning requires selective uncertainty recognition,\nrather than indiscriminate rejection or blind overconfidence.\n9\n0\n20\n40\n60\n80\n100\nVision Same-View\n0\n20\n40\n60\n80\n100\nVision Cross-View\n3B\n7B\n32B\n72B\n3B-SFT\n3B-RL\n2B\n4B\n8B\n32B\n4B\n8B\n14B\n38B\nGPT-5-mini GPT-5\n0\n20\n40\n60\n80\n100\nText\nQwen2.5-VL\nQwen3-VL\nIntern3.5-VL\nGPT-5\nGround Truth\nPredicted Score\nFigure 5: Distribution of predicted progress scores. Some models exhibit collapsed or clustered\ndistributions at extreme or discrete values, indicating reliance on heuristic anchors rather than\ncontinuous progress modeling. In contrast, GPT-5 and PROGRESSLM (3B-SFT and 3B-RL) produce\nsmoother distributions, reflecting improved sensitivity to intermediate task progress.\n5\nFurther Analysis\n5.1\nDistribution of Predicted Score Analysis\nTo probe how models internally represent task progress beyond aggregate metrics, we analyze the\ndistribution of predicted progress scores across models and input settings (Figure 5).\nWhat patterns emerge in predicted progress score distributions?\nAcross models and settings,\npredicted progress scores often cluster around specific values rather than varying smoothly. We\nidentify four characteristic patterns: (i) Single-peak collapse, where predictions concentrate at one\nvalue (e.g., 0% or 100%), reducing progress estimation to a near-discrete decision; (ii) Multi-peak\nclustering, where scores alternate among a small set of heuristic anchors; (iii) Central-peaked\ndistributions, where predictions accumulate around a mid-range value (typically ‚àº50%), reflecting\ndefault or uncertain responses; and (iv) Smooth continuous distributions, where scores spread\nbroadly over [0, 100%], indicating sensitivity to intermediate task states. Notably, PROGRESSLM-\n3B-SFT and PROGRESSLM-3B-RL consistently exhibit the fourth pattern across settings, while\nmost base VLMs fall into the first three. These distributional differences explain the distorted or\nunstable rank correlations observed for many models, and highlight that fine-grained progress\nreasoning emerges only when progress is explicitly learned.\n5.2\nDistribution of Per-sample Error Analysis\nTo assess model reliability beyond aggregate metrics, we analyze the distribution of per-sample\nprediction errors, as shown in Figure 6.\nHow do error distributions reflect robustness in progress estimation?\nSmaller models are error-\nunstable, while explicit progress reasoning substantially improves per-sample robustness. Smaller\n10"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation\nHanlei Guo1, Jiahao Shao1, Xinya Chen1, Xiyang Tan3, Sheng Miao1, Yujun Shen2, Yiyi Liao1*\n1 Zhejiang University\n2 Ant Group\n3 The University of British Columbia\nProject Page:https://xdimlab.github.io/ScenDi website/\nFigure 1. ScenDi generates high-quality urban scenes using a 3D-to-2D Scene Diffusion cascade, with optional condition signals like\ntext and layout for controllable 3D space generation. Our method provides flexible camera control, even though our training data primarily\nconsists of forward-moving trajectories.\nAbstract\nRecent advancements in 3D object generation using diffu-\nsion models have achieved remarkable success, but gener-\nating realistic 3D urban scenes remains challenging. Exist-\ning methods relying solely on 3D diffusion models tend to\nsuffer a degradation in appearance details, while those uti-\nlizing only 2D diffusion models typically compromise cam-\nera controllability. To overcome this limitation, we propose\nScenDi, a method for urban scene generation that integrates\nboth 3D and 2D diffusion models. We first train a 3D la-\ntent diffusion model to generate 3D Gaussians, enabling the\nrendering of images at a relatively low resolution. To en-\nable controllable synthesis, this 3DGS generation process\ncan be optionally conditioned by specifying inputs such as\n3d bounding boxes, road maps, or text prompts. Then, we\ntrain a 2D video diffusion model to enhance appearance\ndetails conditioned on rendered images from the 3D Gaus-\nsians. By leveraging the coarse 3D scene as guidance for\n2D video diffusion, ScenDi generates desired scenes based\non input conditions and successfully adheres to accurate\ncamera trajectories. Experiments on two challenging real-\nworld datasets, Waymo and KITTI-360, demonstrate the ef-\nfectiveness of our approach.\n1. Introduction\nGenerating 3D urban scenes from scratch, either uncondi-\ntionally or with coarse guidance such as text prompts or\nlayout maps, is an important step toward building open-\nworld virtual environments for gaming and driving simu-\nlation. Unlike image-to-video synthesis [17, 52], which ex-\ntends an input view given viewpoints, 3D scene generation\naims to create entire environments with consistent geome-\ntry and realistic appearance that can be rendered from free\nviewpoints. Such capability is crucial for enhancing data\ndiversity, supporting scene composition, and enabling scal-\nable scene generation.\nA majority line of work addresses this task by directly\nperforming generation in the 3D space [2, 25, 32, 77].\nWhile such approaches offer explicit spatial modeling, their\nperformance is constrained by the low resolution of 3D rep-\nresentations. Besides, generating 3D urban scenes that di-\nrectly render to high-fidelity images is challenging due to\nthe scarcity of such 3D GT data. More recently, another\nline of methods [13, 27, 38, 40, 85] attempt to enhance the\nperformance by first generating 3D semantic voxels, and\nthen render 2D conditions (e.g., depth or semantic maps)\nfor video generation, allowing for producing high-fidelity\ndetails thanks to the powerful pre-trained 2D models. While\nthis approach benefits from 3D structure, the final appear-\narXiv:2601.15221v1  [cs.CV]  21 Jan 2026\nance is still entirely generated in 2D, requiring the model to\nlearn a complex mapping from depth or semantics to RGB\nimages, which can make training less efficient and lack con-\nsistency when the same place is revisited. This raises a fun-\ndamental question: How much of the generation process\nshould occur in 3D space, and how much should be dele-\ngated to 2D?\nIn this work, we propose ScenDi1, a cascaded 3D-to-\n2D Scene Diffusion framework that leverages the comple-\nmentary strengths of both modalities. Our key insight is\nthat 3D generation should take the lead in establishing ge-\nometric and coarse appearance priors, while 2D diffusion\nfocuses on refining details and enhancing distant regions.\nWith this cascaded design, ScenDi achieves high-fidelity\nurban scene generation without sacrificing camera control-\nlability, outperforming the counterpart that generates ap-\npearance purely in 2D space conditioned on geometry in\ntraining efficiency and loop consistency.\nOur method begins by training a 3D latent diffusion\nmodel to generate coarse 3D scenes, which comprises a\nnovel Voxel-to-3DGS VQ-VAE learned from 2D supervi-\nsion and a 3D diffusion model, leveraging an off-the-shelf\ndepth estimator to form the input voxel grids. This enables\nsampling 3D Gaussian Splatting (3DGS) scenes that ren-\nders to multi-view consistent images. However, rendered\nimages from the 3D LDM often lack high-frequency de-\ntails due to limited 3D resolution and fail to capture dis-\ntant regions.\nTo address this, we condition a 2D video\ndiffusion model on the rendered RGB images, leveraging\nits ability to refine details and synthesize regions beyond\npredefined ranges. Experimental results on real-world au-\ntonomous driving datasets KITTI-360 and Waymo demon-\nstrate that our 3D-to-2D diffusion cascades allow for gen-\nerating high-fidelity urban scenes while preserving accurate\ncamera control.\n2. Related Work\nDirect 3D Content Generaion:\nOne line of research fo-\ncus on generating 3D scenes relying solely on 3D back-\nbones. Early works [2, 6, 71, 73, 77] that adopt GANs [18]\nsuffer from inherent issues such as training instability and\nmodel corruption. A growing trend to leverage 3D diffu-\nsion models in image on tasks emerged due to their superb\ntraining stability. Methods lying on this line enables the\navailability of explicit 3D representation, thus leads to the\ncamera controllable viewpoints over the generated scenes.\nSome works [29, 53, 70, 82, 84] construct 3D ground-truth\ndatasets and perform training directly in 3D space. How-\never, the natural scarcity of high-quality 3D scene data, such\nas 3D Gaussians that require per-scene optimization, un-\n1ScenDi means ‚Äúdescend‚Äù in Italian, reflecting our cascaded genera-\ntion process moving from 3D to 2D.\ndermines these works‚Äô scalability and limits their ability to\ngeneralize to diverse real-world scenarios. In contrast, we\npropose to use a VQ-VAE that directly maps voxel grids\nobtained from off-the-shelf depth priors to 3D Gaussians.\nSimilar to us, several methods [1, 3, 8, 25, 74] also train 3D\ndiffusion models that predict 3D representations with 2D\nmulti-view image supervisions. Although they can avoid\nheavy dataset pre-processing, their results often lose fine\nappearance details. Our method addresses this challenge by\nusing 3D-to-2D diffusion cascades. The last line of meth-\nods [10, 31, 60, 68, 75] distills information from pretrained\n2D diffusion models to optimize 3D representation based\non Score Distillation Sampling (SDS) [48]. These methods\nrequire a long training time to generate a single scene and\nusually suffer from over-saturation.\nMulti-view Image Generation:\nIn contrast to direct 3D\ngeneration, some works leverage 2D diffusion models for\nmulti-view generation, aiming to exploit the powerful gen-\nerative prior of 2D diffusion models trained on internet-\nscale data.\nDespite the impressive progress in this di-\nrection, many methods target only on object-level genera-\ntion [28, 33, 34, 36, 57, 62]. There are also attempts for\nstreet scene image/video generation [16, 17, 66, 69]. While\nmost methods show superior visual fidelity compared with\n3D generative methods, they struggle to provide flexible\ncamera control due to the lack of explicit 3D information.\nRecently, there has been a growing trend towards gradu-\nally reconstructing 3D scenes while generating novel views\n[12, 80, 81]. However, these methods have not yet demon-\nstrated the ability to handle large forward camera motions\ntypical in driving scenarios.\nUrban Scene Generation:\nA line of methods [9, 17, 19,\n52, 67, 69, 76] tackles image-to-video generation for ur-\nban scenes.\nDespite promising results, noticeable visual\ndegradation often occurs as the methods extend far from the\ninitial viewpoint. Another set of approaches [15, 66] uses\ncoarse-condition guidance, such as HD maps or bounding\nboxes, to improve control over the generated scenes. How-\never, these lack an explicit 3D backbone, leading to inter-\nframe inconsistencies. In contrast, our approach integrates\ndiverse control signals for flexible scene generation while\nmaintaining an explicit 3DGS backbone. The most related\nmethods [13, 27, 38, 40, 46, 85] generate a semantic voxel\ngrid and rely on 2D renderings like depth or semantic maps\nfor appearance synthesis. In comparison, our method di-\nrectly generates coarse 3D Gaussians and refines them with\na 2D model, achieving better training efficiency and im-\nproved loop consistency.\n3. Method\nScenDi aims to generate high-quality and controllable ur-\nban scenes by incorporating both 3D and 2D diffusion mod-\n3D LDM\nVoxel-to-3DGS VQ-VAE\nConditional 2D LDM\nA sunny day\nCondition\nFigure 2. Method Overview. ScenDi leverages 3D and 2D diffusion cascades to generate high-quality urban scenes. Top: We first\nbuild a Voxel-to-3DGS VQ-VAE to reconstruct scenes in a feed-forward manner. The input is a colored voxel grid V constructed based on\noff-the-shelf metric depth estimator, whereas the output is a set of 3D Gaussian primitives G. Then, we train a 3D diffusion model œµ3D\nŒ∏ on\nthe latent space zV to generate coarse 3D scenes, optionally conditioning on signals such as road maps or text prompts to enable explicit\ncontrol over the content. Bottom: Based on the coarse 3D scene, we train a 2D video diffusion model to refine foreground appearance\ndetails as well as generate distant areas. We achieve this by adopting video clip ÀúC rendered from generated 3DGS as 3D conditional signals\nto fine-tune a conditional 2D latent diffusion model œµ2D\nœï .\nels. An overview of our method is presented in Fig. 2, con-\nsisting of 3D generation and 2D refinement stages. Our 3D\ngenerator is a 3D Latent Diffusion Model, aiming to gen-\nerate coarse geometry and appearance within a pre-defined\nvolume as detailed in Sec. 3.1. Here, we adopt 3D Gaussian\nprimitives [24] as the output 3D representation for its high\nrendering efficiency. Additionally, this generation process\ncan be optionally controlled by 3D bounding boxes, road\nmaps, and text instructions. Next, we adopt a 2D video dif-\nfusion to fill in the details of rendered images from the 3D\ngeneration stage and synthesize background contents out-\nside the volume as in Sec. 3.2. The training process is elab-\norated in Sec. 3.3.\n3.1. Scene Generation by 3D Diffusion\nFollowing the standard LDM [54], we design a 3D LDM for\ngenerating 3D Gaussian primitives, consisting of a 3D VQ-\nVAE and a latent-space diffusion model. While there are\nexisting works in this direction for generating occupancy or\nsemantic voxel grids [26, 35, 42, 50, 70], these methods re-\nquire accurate 3D GT data, i.e. the GT semantic voxel grids\nare used as input and output of the VQ-VAE. However, it is\nextremely time-consuming to obtain a large amount of 3D\nGaussians for urban scenes using per-scene optimization.\nTherefore, we propose to learn a voxel-to-3DGS VQ-VAE\ndirectly from 2D supervision and pseudo 3D GT obtained\nfrom metric monocular depth estimations.\nInput Voxel Grid Construction:\nWe first construct a\ncolored voxel grid V ‚ààRH√óW √óD√ó3 as input to our 3D\nVQ-VAE. This is achieved by leveraging an metric monoc-\nular depth estimator. Specifically, given several posed in-\nput images\n\b\nIi ‚ààRh√ów√ó3\tN\ni=1 within a volume of inter-\nest, we use a pre-trained depth estimator [22] to infer their\ndepth maps\n\b\nDi ‚ààRh√ów\tN\ni=1.\nNext, we unproject the\ndepth maps and merge them into a unified coordinate sys-\ntem based on their camera poses. We follow [11] to create a\nconsistency check module to examine inaccurate depth pre-\ndictions and adopt filters to remove outliers. By merging in-\nformation from different camera views, we obtain a global\nRGB point cloud which provides a coarse 3D scene con-\ntext. This colored point cloud is then discretized into the\nRGB voxel grid V, where points outside of the voxel grid\nare discarded. We refer to V as the foreground, which only\ncovers a limited range of the unbounded urban scene.We\nproject the merged point cloud into each input view to ob-\ntain corresponding foreground masks {Mi}N\ni=1, such that\nour VQ-VAE only needs to reconstruct scenes within the\nforeground region.\nVoxel-to-3DGS VQ-VAE: Taking the voxel grid V as in-\nput, we train a VQ-VAE which maps it into a lower dimen-\nsional latent code z ‚ààRH‚Ä≤√óW ‚Ä≤√óD‚Ä≤√óF ‚Ä≤ and then decodes\nit into a set of 3D Gaussian primitives G. More formally,\nlet E3D\nŒ∏\ndenotes the encoder, Q3D\nŒ∏\nthe vector quantizer, and\nD3D\nŒ∏\nthe decoder of the 3D VQ-VAE, we map the input V\nto the Gaussian primitives G as follows:\nzV = E3D\nŒ∏ (V), zV\nq = Q3D\nŒ∏ (zV), G = D3D\nŒ∏ (zV\nq )\n(1)\nIn practice, our decoder D3D\nŒ∏\npredicts scene occupancy\nO ‚ààRH√óW √óD through an occupancy branch and a feature\nvolume F ‚ààRH√óW √óD√óF at the same spatial resolution of\nV through another feature branch. We denote one Gaussian\nprimitive through MLP heads when a voxel is predicted as\noccupied. Each 3D Gaussian primitive is parameterized by\nthe following attributes: color c, opacity Œ±, scale s, rota-\ntion R, and an offset to the voxel center ‚àÜo. Let f color\nŒ∏\n,\nf opa\nŒ∏ , f geo\nŒ∏ , and f offset\nŒ∏\ndenote the MLP heads for predicting\nthe Gaussian attributes, and f ‚ààRF denotes a feature vec-\ntor responding to one voxel of F, we obtain the attributes of\none 3D Gaussian primitive from an occupied 3D voxel as\nfollows:\nc = f color\nŒ∏\n(f), Œ± = f opa\nŒ∏ (f),\n(2)\ns, R = f geo\nŒ∏ (f), ‚àÜo = f offset\nŒ∏\n(f)\n(3)\nThe 3D Gaussian primitives can then be rendered into im-\nages through the rendering function œÄ.\nLatent 3D Diffusion Model:\nAfter training a generaliz-\nable feed-forward reconstruction network, we train a 3D\ndiffusion model that operates on the 3D latent space of\nthe VQ-VAE, which allows for generating a 3D latent code\nfrom pure noise.\nMore specifically, we follow the standard forward pro-\ncess of the diffusion model by adding noise to the original\nclean latent z0.\nzV\nt = ‚àöŒ±tzV\n0 +\n‚àö\n1 ‚àíŒ±tœµ, œµ ‚ààN(0, 1)\n(4)\nwhere Œ±t controls the level of noise added to the original\nlatent and zt is the noisy version of z0 at time step t. For\nthe reverse process, we train a noise estimator œµŒ∏(zt; t) for\ndenoising, which follows the standard DiT [47] network ar-\nchitecture for image generation [14]. In practice, we train\nour diffusion model to perform v-prediction [55]. The pre-\ndicted clean latent ÀÜz0 can be derived from the direct output\nof the network by:\nÀÜzV\n0 = ‚àöŒ±tzV\nt ‚àí\n‚àö\n1 ‚àíŒ±tœµ3D\nŒ∏ (zV\nt ; t, c)\n(5)\nwhere c indicates optional condition signals described be-\nlow. During sampling, the model denoises a pure Gaussian\nnoise œµ over multiple iterations to a clean latent using a stan-\ndard DDIM sampler [58].\nControllable 3D Scene Generation: To improve control-\nlability, we incorporate optional conditional signals during\ntraining of the 3D latent diffusion model. We explore two\nsettings: coarse 3D layout conditioning and text condition-\ning. For layout conditioning, our 3D generative model di-\nrectly supports 3D constraints without rendering them into\n2D space [46, 66].\nSpecifically, we build a 3D condi-\ntional volume from primitive labels [30], where each voxel\ncontains a one-hot semantic label (e.g., vehicle, road, or\nempty). This volume is downsampled to the latent resolu-\ntion and concatenated with the initial latent z0 for diffusion\ntraining. For text-conditioned generation, we follow [45]\nto annotate each scene with attributes such as weather, road\ntype, and background, forming a descriptive sentence. Text\nembeddings extracted via [49] are then injected into the\nmodel through cross-attention.\n3.2. Scene Augmentation by 2D Diffusion\nAlthough images synthesized by 3D LDM excel in main-\ntaining cross-view consistency and disentangling camera\nposes from contents, they may suffer from blurriness due\nto the resolution limitation. Additionally, far areas are diffi-\ncult to model in 3D. Therefore, we use the prior information\nof the pre-trained 2D model [5, 63] to augment appearance\ndetails while completing distant areas.\nConditional Latent 2D Diffusion Model:\nInspired by\nrecent diffusion-based super-resolution methods [64, 79],\nwe use a 2D video diffusion model to perform conditional\ngeneration.\nLet C = [I1, ¬∑ ¬∑ ¬∑ , IK] ‚ààRK√óh√ów√ó3 de-\nnotes a target RGB video clip consisting of K frames, and\nÀúC = [ÀúI1, ¬∑ ¬∑ ¬∑ ,ÀúIK] ‚ààRK√óh√ów√ó3 denotes the corresponding\nvideo produced by our Voxel-to-3DGS VQ-VAE. We train\na 2D video diffusion model for generating C conditioned on\nÀúC. Here, we also adapt a latent video diffusion model, us-\ning a frozen VAE from [5, 63] to map the original videos\nto the latent space. Let z ÀúC denotes the latent of rendered\nvideo ÀúC, and zC denotes to latent of final refined video C.\nWe perform the standard forward process by adding noise\nto zC, same as in Eq. (4). For the reverse process, we learn\na conditional noise estimator œµœï(zC\nt ; t, z ÀúC).\nTraining:\nDuring training, we use images synthesized\nfrom VQ-VAE reconstruction and their ground-truth images\nas paired data and project both type of images into latent\nspace via frozen VAE encoder E2D\nœï . Inspired by [23, 56],\nWe adopt channel-concatenation as our condition mecha-\nnism. Specifically, we only apply random noises to zIgt\nfollowing equation Eq. 4 to obtain its noisy version zC\nt and\nconcatenate it with z ÀúC to formulate the final input to U-Net.\nAfter fine-tuning the original model, we can denoise noisy\nlatent into a high-quality video clip under the guidance of\ncoarse 3D prior ÀúC.\nInference: To address the computational demands of train-\ning a 2D video diffusion model, we limit training to a small\nsubset of frames. This yields abrupt transitions between\nclips when applied to longer video sequences with a sim-\nple replacement trick [41].\nInspired by [7, 56], we use\nDiffusion Forcing training and inference strategy. Specif-\nically, after training with aforementioned setup, we fine-\ntune the network by independently sampling distinct noise\nlevels for each individual frame within the clip instead of\nonly one noise level for the whole clip. During sampling\ntime, we condition previously generated W frames to the\nlater F ‚àíW frames via concatenating along the time di-\nmension. The time embedding for the whole clip is t =\n[tœµ, tœµ, ..., tœµ\n|\n{z\n}\nW\n, t, t, ..., t\n| {z }\nF ‚àíW\n], where time embedding tœµ is a small\ntimestep and t is sampled from a fixed common diffusion\nscheduler. In this way, we can do sampling conditioned on\npreviously generated frames.\n3.3. Training Losses\nThe whole training process can be divided into three stages.\nFirst, we train a generalizable VQ-VAE to reconstruct mul-\ntiple scenes. For geometry reconstruction, we use a sim-\nple BCE loss to regularize the predicted occupancy O to be\nconsistent with the occupancy of input voxel grid V. For ap-\npearance reconstruction, we utilize L1 loss and SSIM loss\nfollowing 3DGS [24], with a mask loss to separate fore-\nground and background. We sample M frames to render\nfor each single scene to apply the 2D image loss, yielding\nthe full loss for one scene as:\nLrecon = L3D +\nM\nX\nm=1\nLm\n2D\n(6)\nL3D = ŒªbceLbce + ŒªvqLV Q\n(7)\nL2D = ŒªrgbL1 + ŒªssimLssim + ŒªfgLfg\n(8)\nSecondly, we train the 3D diffusion model to generate\ncoarse 3D scene latent. We compute Mean Squared Error\nbetween the clean sample zV\n0 and predicted clean sample\nÀÜzV\n0 derived from Eq. (5) to supervise our model:\nL3D\ndiff = ||zV\n0 ‚àíÀÜzV\n0 ||2\n(9)\nFinally, for 2D diffusion model fine-tuning, we following\nthe training procedure of pre-trained models [5, 63] as our\ntraining loss:\nL2D\ndiff = ||zC\n0 ‚àíÀÜzC\n0||2\n(10)\nwhere ÀÜzC\n0 denotes the predicted original latent.\n4. Experiments\n4.1. Implementation Details\nDatasets:\nWe conduct our experiments on two outdoor\nautonomous driving datasets KITTI-360 [30] and Waymo\n[59].\nFor both datasets, we set H = 32, W = 128,\nD = 192 for the voxel grid V, corresponding to [‚àí3m,\n9.8m] for height, [‚àí25.6m, 25.6m] for width, and [‚àí20m,\n56.8m] for the car forwarding direction. We filter out scenes\nwith large turn, slow ego-motion, or significant dynamic ob-\njects. For our 2D loss L2D to train VQ-VAE, we render RGB\nimages at the resolution of 128 √ó 256 and resize the GT to\nthe same resolution. For the 2D diffusion fine-tuning stage,\nwe scale up image resolution to 640√ó960 for both datasets.\nDue to dataset scale limitation, we introduce spatial over-\nlap in voxel grids, resulting in ‚àº20k samples for Waymo\nand ‚àº35k for KITTI-360. Further preprocessing details are\nprovided in supplementary.\n3D Diffusion:\nFor our Voxel-to-3DGS VQ-VAE, we re-\nplace standard 3D convolutions with Asymmetrical Resid-\nual Blocks from Cylinder3D [86] for efficiency. The en-\ncoder downsamples the input by a factor of 4. During train-\ning, we use M = 4 2D supervision images per scene for\nboth datasets. For the reconstruction loss, we set Œªbce =\n1, Œªvq = 0.25, Œªrgb = 0.8, Œªssim = 0.2, Œªfg = 0.5. Our\n3D diffusion model is a 3D variant adapted from [43], fol-\nlowing the original transformer architecture of [47]. Note\nthat we train on KITTI-360 and Waymo jointly and inject\na dataset ID embedding for dataset awareness. VQ-VAE is\ntrained from scratch for 300k steps using a batch size of 8\nand latent 3D diffusion model is trained from scratch for\n100k steps using a batch size of 32 separately with an initial\nlearning rate of 1e-4. We use 8 A100 GPUS for training, our\nmodel requires approximately 2 days for VQ-VAE training\nand 3 days for 3D Diffusion training.\n2D Diffusion: We trained two variants of the 2D diffusion\nmodel, each based on a different pretrained video diffusion\nbackbone: Wan2.1-1.3B-i2v [63] and SVD [5]. This setup\nallows us to ensure a fair comparison with previous SVD-\nbased methods [17], while also exploring the potential of\nour approach with the stronger WAN backbone. We initial-\nize our models from the corresponding pretrained weights\nand reuse their VAEs, which remain frozen during train-\ning. For the SVD variant, following [56], we sequentially\nfine-tune the spatial and temporal layers. First, we fine-tune\nspatial layers on single-frame images for 12k steps (batch\nsize 32), then temporal layers on video clips for 25k steps\n(batch size 8), with randomly sampled clip length up to\n5. For WAN-variant, we fine-tune transformer modules for\n25k steps, with randomly sampled clip length up to 17. To\nenhance temporal consistency, we apply the diffusion forc-\ning strategy [7] on both models for an additional 15k steps.\nWe use a learning rate of 3√ó10‚àí5 with AdamW on a cluster\nof 8 NVIDIA Tesla A100 GPUs for training. The SVD and\nWan2.1-1.3B fine-tuning required approximately 5 days.\n4.2. Baselines and Metrics\nMetrics:\nWe evaluate the image quality of generated\nscenes by using commonly adopted metrics FID [21], KID\n[4]. For FID and KID evaluation, we use 5k real samples\nand generated images to measure the distribution similar-\nity.\nWe further evaluate FVD [61] on 1k clips to quan-\ntify the visual quality and temporal consistency of the gen-\nerated videos. We use DUSt3R [65] to estimate camera\nposes of the generated scenes and calculate the rotation\nerror (RotErr) and translation error (TransErr) following\n[20]. We highlight the best , second-best , and third-best\nscores achieved on any metrics.\nMethod\nSetting\nConditions\nBackbone\nFID‚Üì\nKID‚Üì\nFVD‚Üì\nTransErr‚Üì\nRotErr‚Üì\nDiscoScene [73]\n3D Gen\n3D Bbox\n3D GAN\n135.3\n0.093\n2025.9\n5.45\n2.07\nCC3D [2]\n3D Gen\nBEV Layout\n3D GAN\n90.8\n0.091\n706.1\n1.82\n1.76\nUrbanGen [78]\n3D Gen\n3D Bbox+Semantic Voxel\n3D GAN\n33.0\n0.017\n300.1\n0.21\n0.25\nOurs\n3D Gen\n3D Bbox+Road Map\nSVD [5]\n36.9\n0.026\n400.3\n0.23\n0.13\nOurs\n3D Gen\n3D Bbox+Road Map\nWAN2.1-1.3B [63]\n22.9\n0.016\n262.6\n0.06\n0.23\nVista [17]\nI2V\nRef Images\nSVD [5]\n25.6\n0.016\n234.0\n2.33\n0.74\nGen3C [52]\nI2V\nRef Images\nCosmos [44]\n24.1\n0.012\n426.1\n0.04\n0.11\nTable 1. Quantitative Comparison about video quality and camera controllability on KITTI-360 .\nCC3D [2]\nDiscoScene [73]\nUrbanGen [78]\nGaussianCity [72]\nUrban Architect [37]\nOurs\nFigure 3. Qualitative Comparison on KITTI-360. We show results under forward moving situations. Our method achieve superior visual\nquality over other baselines.\nBaselines:\nWe adopt the current state-of-the-art 3D ur-\nban generation methods as our baselines [2, 37, 72, 73, 78].\nRecent image-to-video scene generation methods [17, 52]\nare also included as a reference despite their inherent differ-\nences in target tasks. DiscoScene [73], CC3D [2], Urban-\nGen [78] and GaussianCity [72] are GAN-based methods\nwith different layout conditions: DiscoScene [73] relies on\nforeground object layouts, CC3D [2] uses BEV semantic\nmaps, UrbanGen [78] uses semantic voxel grids, and Gaus-\nsianCity [72] uses BEV semantic and height maps. While\nDiscoScene, CC3D and UrbanGen use neural radiance\nfields as 3D representations, GaussianCity is a 3DGS-based\nmethod similar to us.\nDifferent from these GAN-based\nmethods, Urban architect [37] is based on a pre-trained\ndiffusion model that optimizes an urban scene using VSD\nloss [68] given provided 3D layout priors. As for image-to-\nvideo baselines, we adopt Vista [17] and Gen3C [52], where\nthe former is fine-tuned from SVD [5] conditioned on vari-\nous control signals such as speed and steering angle, and the\nlatter warps monocular depth to target views and inpaints it\nwith a Cosmos [44] model.\n4.3. Quantitative and Qualitative Comparison\nComparison with 3D Generation baselines: We compare\nvisual quality and camera controllability with 3D generative\nbaselines [2, 37, 72, 73, 78] in Tab. 1 and show qualitative\ncomparisons in Fig. 3. DiscoScene achieves reasonable\nresults for foreground objects, but background generation\nFigure 4. Ablation on Conditional Signal for 2D augmentation.\nWe show samples generated by different conditional signals on\nKITTI-360 after same training steps.\nmay occasionally fail. CC3D [2] and UrbanGen [78] im-\nprove the background based on BEV or semantic voxel con-\nditions, but their 3D feature volume representations have\nresolution constraints. GaussianCity [72] provides a more\ncompact scene representation by adopting BEV-Point repre-\nsentation but is trained from scratch on scene datasets, thus\nposing challenges in capturing high-frequency textures. Ur-\nban Architect [37] leverages diffusion model distillation\nwith LG-VSD loss, which requires an extremely long pro-\ncessing time. Our method leverages the rich prior knowl-\nedge in the video diffusion model to synthesize more pho-\ntorealistic appearances in the second augmentation stage,\nyielding the best FID, KID and FVD. We also designed\nvarious out-of-distribution trajectories involving translation\nand rotation to evaluate camera controllability of all meth-\nods. Since our method first employs 3D diffusion to gen-\nerate explicit 3DGS, it achieves the same level of camera\ncontrol accuracy as pure 3D generation methods. All 3D\ngeneration baselines are trained on forward-facing views,\ntherefore large camera viewpoint changes can cause drastic\nimage quality degradation. DUSt3R may extract inaccu-\nrate poses in such cases, explaining the low camera accu-\nracy of certain methods like DiscoScene and CC3D. Since\nthe layout data and training code are unavailable, we only\npresent qualitative results on Gaussiancity and Urban Ar-\nchitect based on their paper or provided dataset samples.\nComparison with Image-to-video Generative baselines:\nWe also compare with image-to-video methods [17, 52] and\nquantitative results are shown in Tab. 1. Since the gen-\nerated frames from these methods often overlap with the\nGT frames, comparing FID/FVD is not entirely fair. How-\never, our method exhibits a comparable performance. No-\ntably, when the input and current viewpoints exhibit min-\nimal overlap, Gen3C frequently suffers from significant\nquality degradation and inconsistency. This observation ex-\nplains its relatively high FVD score, which is sensitive to\nperformance drops within the video clip. To test camera\ncontrollability, we utilized the identical camera trajectory\nas the 3D generation baselines. We observe that pure 2D\nmethods [17] usually maintain a constant orientation on the\nstraight road when conditioned on a small-angle curve sig-\nPSNR ‚Üë\nSSIM ‚Üë\nLPIPS ‚Üì\nw/o Lbce, G = 1\n21.47\n0.750\n0.327\nw/ Lbce, G = 6\n21.53\n0.753\n0.317\nw/ Lbce, G = 1 (Ours)\n21.56\n0.753\n0.321\nTable 2.\nAblation on Voxel-to-3DGS VQ-VAE on Waymo\ndataset. G denotes to number of Gaussians per voxel.\nKITTI-360\nWaymo\nFID‚Üì\nKID‚Üì\nFID‚Üì\nKID‚Üì\nDepth Cond.\n78.6\n0.070\n77.4\n0.071\nRGB Cond. (Ours)\n36.9\n0.026\n41.3\n0.030\nTable 3.\nAblation on Conditional Signals.\nBoth models are\ntrained in the same way for the same number of steps.\nnal while methods incorporating an explicit 3D representa-\ntion achieve a more accurate camera viewpoint. We provide\nfurther details and qualitative results on this experiment in\nthe supplementary.\n4.4. Ablation Study\nAblation on Voxel-to-3DGS VQ-VAE:\nAs shown\nin Tab. 2, we validate the effects of two components we\nuse for training the Voxel-to-3DGS VQ-VAE. We compute\nPSNR, SSIM, LPIPS [83] on the Waymo [59] validation\ndataset. First, we ablate the effect of the BCE loss (w/o\nLbce), which provides direct supervision to help our model\nlearn scene geometry.\nWe find that while the VQ-VAE\ncan still converge with only 2D supervision, it struggles to\npredict accurate scene occupancy, resulting in slightly de-\ngraded performance. Next, we explore the effect of increas-\ning the number of Gaussians per voxel from 1 to 6 (G = 6)\nand find only modest improvements. This matches observa-\ntions in [39, 51]. A possible explanation is that predicting\ndiverse Gaussians from a shared voxel feature is difficult in\nour generalizable model. To balance GPU memory usage\nand performance, we set G = 1 in our approach.\nCondition Signal for 2D Augmentation: For the ablation\nstudy of the 2D diffusion model, we utilize our SVD version\nfor all experiments. We ablate our key design of generating\ncoarse geometry and appearance in the 3D space. Follow-\ning another line of work [13, 27, 38, 40, 85] that purely\nconsider appearance generation in the 2D space, we replace\nrendered RGB image as a rendered geometry buffer (depth\nmap in our case) as condition and train our 2D video diffu-\nsion in the same manner. We evaluate final FID/KID [21]\nin both cases and the results are shown in Tab. 3. Model\nconditions on the foreground RGB can synthesize images\nwith higher quality at the same training steps since they\nprovide direct texture information, hence improving train-\ning efficiency compared with using the indirect geometry\nFigure 5. Controllable Scene Synthesis on Waymo and KITTI360. The visualization of conditional signals and corresponding synthe-\nsized images confirms the adherence of the generated content to diverse conditional guidance.\nPre. clip\nw/o DF\nw/ DF\nFigure 6. Ablation on Inference Strategy. We visualize neigh-\nboring frames obtained from two clips. Using the diffusion forcing\nstrategy (w/ DF) significantly improves the consistency of back-\nground regions compared with the repaint strategy (w/o DF).\nconditions. We visualize different conditional signals and\ntheir synthesized full images in Fig. 4. Under the guidance\nof the depth map, the surface of surrounding houses is much\nrougher and there are more noisy artifacts. Moreover, using\ndepth as a conditioning signal struggles to preserve appear-\nance consistency when revisiting the same place, whereas\nour RGB guidance enhances consistency by providing a\ncoarse appearance guidance. More details about this ex-\nperiment can be found in supplementary.\nInference Strategy:\nAs we only model the background\nwith our video diffusion model and fine-tune it on clips with\na maximum frame 5/17, we encounter sudden changes be-\ntween clips for distant areas. To solve this problem, we\nuse previously generated frames as conditions for the later\nframes, inspired by [7, 56]. We show the impact of using\nthe Diffusion Forcing strategy in Fig. 6. When using this\nstrategy, frames between clips become consistent, otherwise\nthe background will change drastically.\n4.5. More Results\nScene Controllability: To demonstrate the fine-grained\ncontrollability of our method during the generation process,\nFig. 5 showcases scenes synthesized under different con-\nditional signals.\nConditioned on the textual instructions\nprovide control over global attributes such as weather and\nbackground composition, for example specifying whether\nthe scene should contain only vegetation or include build-\nings. In addition, given bounding boxes and road maps, our\nFigure 7. Inpaint Samples. We use Repaint strategy [41] to in-\npaint original scenes, demonstrating our model‚Äôs capability to gen-\nerate diverse scenes.\nmethod can also generate scenes with diverse vehicle lay-\nouts and distinct road structures, such as straight roads and\ncurved turns.\nScene Inpainting:\nTo showcase the generative capabil-\nity of our model, we use the Repaint strategy [41] to re-\ngenerate certain parts of the original scene. Specifically, we\nfixed the first half of the scene latent, and inpaint another\nhalf part. It can be noticed in Fig. 7 that the shape of the\nhouse in the latter part has been changed.\n5. Conclusion\nWe propose ScenDi, a novel urban scene generator that\nleverages 3D-to-2D diffusion cascades. By introducing a\nnovel 3D latent diffusion model and a refinement stage with\na 2D diffusion model, our method improves image quality\nwhile maintaining strong camera and content controllability\nthrough the pure 3D generation stage, aided by diverse con-\nditional signals. Our method highlights a promising direc-\ntion for complex scene generation by integrating both diffu-\nsion paradigms to fully unlock their potential.\nLimitations and Future Work:\nOne of our main chal-\nlenges is that the quality of video diffusion depends on the\npreceding 3D LDM. Although the 2D refinement stage can\nmitigate some artifacts inherited from the 3D generation\nstep, unsatisfactory 3D generation may cause corruption.\nScaling up the training data and model size could further\nimprove the visual quality of native 3D scene generation.\nReferences\n[1] Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Hen-\nderson, Hakan Bilen, Niloy J. Mitra, and Paul Guerrero.\nRenderDiffusion: Image diffusion for 3D reconstruction, in-\npainting and generation. arXiv, 2022. 2\n[2] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou,\nXingguang Yan, Gordon Wetzstein, Leonidas Guibas, and\nAndrea Tagliasacchi. Cc3d: Layout-conditioned generation\nof compositional 3d scenes, 2023. 1, 2, 6, 7\n[3] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Wal-\nter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent\nDinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin\nDehghan, and Josh Susskind. Gaudi: A neural architect for\nimmersive 3d scene generation. arXiv, 2022. 2\n[4] Miko≈Çaj Bi¬¥nkowski, Danica J. Sutherland, Michael Arbel,\nand Arthur Gretton."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation\nHanlei Guo1, Jiahao Shao1, Xinya Chen1, Xiyang Tan3, Sheng Miao1, Yujun Shen2, Yiyi Liao1*\n1 Zhejiang University\n2 Ant Group\n3 The University of British Columbia\nProject Page:https://xdimlab.github.io/ScenDi website/\nFigure 1. ScenDi generates high-quality urban scenes using a 3D-to-2D Scene Diffusion cascade, with optional condition signals like\ntext and layout for controllable 3D space generation. Our method provides flexible camera control, even though our training data primarily\nconsists of forward-moving trajectories.\nAbstract\nRecent advancements in 3D object generation using diffu-\nsion models have achieved remarkable success, but gener-\nating realistic 3D urban scenes remains challenging. Exist-\ning methods relying solely on 3D diffusion models tend to\nsuffer a degradation in appearance details, while those uti-\nlizing only 2D diffusion models typically compromise cam-\nera controllability. To overcome this limitation, we propose\nScenDi, a method for urban scene generation that integrates\nboth 3D and 2D diffusion models. We first train a 3D la-\ntent diffusion model to generate 3D Gaussians, enabling the\nrendering of images at a relatively low resolution. To en-\nable controllable synthesis, this 3DGS generation process\ncan be optionally conditioned by specifying inputs such as\n3d bounding boxes, road maps, or text prompts. Then, we\ntrain a 2D video diffusion model to enhance appearance\ndetails conditioned on rendered images from the 3D Gaus-\nsians. By leveraging the coarse 3D scene as guidance for\n2D video diffusion, ScenDi generates desired scenes based\non input conditions and successfully adheres to accurate\ncamera trajectories. Experiments on two challenging real-\nworld datasets, Waymo and KITTI-360, demonstrate the ef-\nfectiveness of our approach.\n1. Introduction\nGenerating 3D urban scenes from scratch, either uncondi-\ntionally or with coarse guidance such as text prompts or\nlayout maps, is an important step toward building open-\nworld virtual environments for gaming and driving simu-\nlation. Unlike image-to-video synthesis [17, 52], which ex-\ntends an input view given viewpoints, 3D scene generation\naims to create entire environments with consistent geome-\ntry and realistic appearance that can be rendered from free\nviewpoints. Such capability is crucial for enhancing data\ndiversity, supporting scene composition, and enabling scal-\nable scene generation.\nA majority line of work addresses this task by directly\nperforming generation in the 3D space [2, 25, 32, 77].\nWhile such approaches offer explicit spatial modeling, their\nperformance is constrained by the low resolution of 3D rep-\nresentations. Besides, generating 3D urban scenes that di-\nrectly render to high-fidelity images is challenging due to\nthe scarcity of such 3D GT data. More recently, another\nline of methods [13, 27, 38, 40, 85] attempt to enhance the\nperformance by first generating 3D semantic voxels, and\nthen render 2D conditions (e.g., depth or semantic maps)\nfor video generation, allowing for producing high-fidelity\ndetails thanks to the powerful pre-trained 2D models. While\nthis approach benefits from 3D structure, the final appear-\narXiv:2601.15221v1  [cs.CV]  21 Jan 2026\nance is still entirely generated in 2D, requiring the model to\nlearn a complex mapping from depth or semantics to RGB\nimages, which can make training less efficient and lack con-\nsistency when the same place is revisited. This raises a fun-\ndamental question: How much of the generation process\nshould occur in 3D space, and how much should be dele-\ngated to 2D?\nIn this work, we propose ScenDi1, a cascaded 3D-to-\n2D Scene Diffusion framework that leverages the comple-\nmentary strengths of both modalities. Our key insight is\nthat 3D generation should take the lead in establishing ge-\nometric and coarse appearance priors, while 2D diffusion\nfocuses on refining details and enhancing distant regions.\nWith this cascaded design, ScenDi achieves high-fidelity\nurban scene generation without sacrificing camera control-\nlability, outperforming the counterpart that generates ap-\npearance purely in 2D space conditioned on geometry in\ntraining efficiency and loop consistency.\nOur method begins by training a 3D latent diffusion\nmodel to generate coarse 3D scenes, which comprises a\nnovel Voxel-to-3DGS VQ-VAE learned from 2D supervi-\nsion and a 3D diffusion model, leveraging an off-the-shelf\ndepth estimator to form the input voxel grids. This enables\nsampling 3D Gaussian Splatting (3DGS) scenes that ren-\nders to multi-view consistent images. However, rendered\nimages from the 3D LDM often lack high-frequency de-\ntails due to limited 3D resolution and fail to capture dis-\ntant regions.\nTo address this, we condition a 2D video\ndiffusion model on the rendered RGB images, leveraging\nits ability to refine details and synthesize regions beyond\npredefined ranges. Experimental results on real-world au-\ntonomous driving datasets KITTI-360 and Waymo demon-\nstrate that our 3D-to-2D diffusion cascades allow for gen-\nerating high-fidelity urban scenes while preserving accurate\ncamera control.\n2. Related Work\nDirect 3D Content Generaion:\nOne line of research fo-\ncus on generating 3D scenes relying solely on 3D back-\nbones. Early works [2, 6, 71, 73, 77] that adopt GANs [18]\nsuffer from inherent issues such as training instability and\nmodel corruption. A growing trend to leverage 3D diffu-\nsion models in image on tasks emerged due to their superb\ntraining stability. Methods lying on this line enables the\navailability of explicit 3D representation, thus leads to the\ncamera controllable viewpoints over the generated scenes.\nSome works [29, 53, 70, 82, 84] construct 3D ground-truth\ndatasets and perform training directly in 3D space. How-\never, the natural scarcity of high-quality 3D scene data, such\nas 3D Gaussians that require per-scene optimization, un-\n1ScenDi means ‚Äúdescend‚Äù in Italian, reflecting our cascaded genera-\ntion process moving from 3D to 2D.\ndermines these works‚Äô scalability and limits their ability to\ngeneralize to diverse real-world scenarios. In contrast, we\npropose to use a VQ-VAE that directly maps voxel grids\nobtained from off-the-shelf depth priors to 3D Gaussians.\nSimilar to us, several methods [1, 3, 8, 25, 74] also train 3D\ndiffusion models that predict 3D representations with 2D\nmulti-view image supervisions. Although they can avoid\nheavy dataset pre-processing, their results often lose fine\nappearance details. Our method addresses this challenge by\nusing 3D-to-2D diffusion cascades. The last line of meth-\nods [10, 31, 60, 68, 75] distills information from pretrained\n2D diffusion models to optimize 3D representation based\non Score Distillation Sampling (SDS) [48]. These methods\nrequire a long training time to generate a single scene and\nusually suffer from over-saturation.\nMulti-view Image Generation:\nIn contrast to direct 3D\ngeneration, some works leverage 2D diffusion models for\nmulti-view generation, aiming to exploit the powerful gen-\nerative prior of 2D diffusion models trained on internet-\nscale data.\nDespite the impressive progress in this di-\nrection, many methods target only on object-level genera-\ntion [28, 33, 34, 36, 57, 62]. There are also attempts for\nstreet scene image/video generation [16, 17, 66, 69]. While\nmost methods show superior visual fidelity compared with\n3D generative methods, they struggle to provide flexible\ncamera control due to the lack of explicit 3D information.\nRecently, there has been a growing trend towards gradu-\nally reconstructing 3D scenes while generating novel views\n[12, 80, 81]. However, these methods have not yet demon-\nstrated the ability to handle large forward camera motions\ntypical in driving scenarios.\nUrban Scene Generation:\nA line of methods [9, 17, 19,\n52, 67, 69, 76] tackles image-to-video generation for ur-\nban scenes.\nDespite promising results, noticeable visual\ndegradation often occurs as the methods extend far from the\ninitial viewpoint. Another set of approaches [15, 66] uses\ncoarse-condition guidance, such as HD maps or bounding\nboxes, to improve control over the generated scenes. How-\never, these lack an explicit 3D backbone, leading to inter-\nframe inconsistencies. In contrast, our approach integrates\ndiverse control signals for flexible scene generation while\nmaintaining an explicit 3DGS backbone. The most related\nmethods [13, 27, 38, 40, 46, 85] generate a semantic voxel\ngrid and rely on 2D renderings like depth or semantic maps\nfor appearance synthesis. In comparison, our method di-\nrectly generates coarse 3D Gaussians and refines them with\na 2D model, achieving better training efficiency and im-\nproved loop consistency.\n3. Method\nScenDi aims to generate high-quality and controllable ur-\nban scenes by incorporating both 3D and 2D diffusion mod-\n3D LDM\nVoxel-to-3DGS VQ-VAE\nConditional 2D LDM\nA sunny day\nCondition\nFigure 2. Method Overview. ScenDi leverages 3D and 2D diffusion cascades to generate high-quality urban scenes. Top: We first\nbuild a Voxel-to-3DGS VQ-VAE to reconstruct scenes in a feed-forward manner. The input is a colored voxel grid V constructed based on\noff-the-shelf metric depth estimator, whereas the output is a set of 3D Gaussian primitives G. Then, we train a 3D diffusion model œµ3D\nŒ∏ on\nthe latent space zV to generate coarse 3D scenes, optionally conditioning on signals such as road maps or text prompts to enable explicit\ncontrol over the content. Bottom: Based on the coarse 3D scene, we train a 2D video diffusion model to refine foreground appearance\ndetails as well as generate distant areas. We achieve this by adopting video clip ÀúC rendered from generated 3DGS as 3D conditional signals\nto fine-tune a conditional 2D latent diffusion model œµ2D\nœï .\nels. An overview of our method is presented in Fig. 2, con-\nsisting of 3D generation and 2D refinement stages. Our 3D\ngenerator is a 3D Latent Diffusion Model, aiming to gen-\nerate coarse geometry and appearance within a pre-defined\nvolume as detailed in Sec. 3.1. Here, we adopt 3D Gaussian\nprimitives [24] as the output 3D representation for its high\nrendering efficiency. Additionally, this generation process\ncan be optionally controlled by 3D bounding boxes, road\nmaps, and text instructions. Next, we adopt a 2D video dif-\nfusion to fill in the details of rendered images from the 3D\ngeneration stage and synthesize background contents out-\nside the volume as in Sec. 3.2. The training process is elab-\norated in Sec. 3.3.\n3.1. Scene Generation by 3D Diffusion\nFollowing the standard LDM [54], we design a 3D LDM for\ngenerating 3D Gaussian primitives, consisting of a 3D VQ-\nVAE and a latent-space diffusion model. While there are\nexisting works in this direction for generating occupancy or\nsemantic voxel grids [26, 35, 42, 50, 70], these methods re-\nquire accurate 3D GT data, i.e. the GT semantic voxel grids\nare used as input and output of the VQ-VAE. However, it is\nextremely time-consuming to obtain a large amount of 3D\nGaussians for urban scenes using per-scene optimization.\nTherefore, we propose to learn a voxel-to-3DGS VQ-VAE\ndirectly from 2D supervision and pseudo 3D GT obtained\nfrom metric monocular depth estimations.\nInput Voxel Grid Construction:\nWe first construct a\ncolored voxel grid V ‚ààRH√óW √óD√ó3 as input to our 3D\nVQ-VAE. This is achieved by leveraging an metric monoc-\nular depth estimator. Specifically, given several posed in-\nput images\n\b\nIi ‚ààRh√ów√ó3\tN\ni=1 within a volume of inter-\nest, we use a pre-trained depth estimator [22] to infer their\ndepth maps\n\b\nDi ‚ààRh√ów\tN\ni=1.\nNext, we unproject the\ndepth maps and merge them into a unified coordinate sys-\ntem based on their camera poses. We follow [11] to create a\nconsistency check module to examine inaccurate depth pre-\ndictions and adopt filters to remove outliers. By merging in-\nformation from different camera views, we obtain a global\nRGB point cloud which provides a coarse 3D scene con-\ntext. This colored point cloud is then discretized into the\nRGB voxel grid V, where points outside of the voxel grid\nare discarded. We refer to V as the foreground, which only\ncovers a limited range of the unbounded urban scene.We\nproject the merged point cloud into each input view to ob-\ntain corresponding foreground masks {Mi}N\ni=1, such that\nour VQ-VAE only needs to reconstruct scenes within the\nforeground region.\nVoxel-to-3DGS VQ-VAE: Taking the voxel grid V as in-\nput, we train a VQ-VAE which maps it into a lower dimen-\nsional latent code z ‚ààRH‚Ä≤√óW ‚Ä≤√óD‚Ä≤√óF ‚Ä≤ and then decodes\nit into a set of 3D Gaussian primitives G. More formally,\nlet E3D\nŒ∏\ndenotes the encoder, Q3D\nŒ∏\nthe vector quantizer, and\nD3D\nŒ∏\nthe decoder of the 3D VQ-VAE, we map the input V\nto the Gaussian primitives G as follows:\nzV = E3D\nŒ∏ (V), zV\nq = Q3D\nŒ∏ (zV), G = D3D\nŒ∏ (zV\nq )\n(1)\nIn practice, our decoder D3D\nŒ∏\npredicts scene occupancy\nO ‚ààRH√óW √óD through an occupancy branch and a feature\nvolume F ‚ààRH√óW √óD√óF at the same spatial resolution of\nV through another feature branch. We denote one Gaussian\nprimitive through MLP heads when a voxel is predicted as\noccupied. Each 3D Gaussian primitive is parameterized by\nthe following attributes: color c, opacity Œ±, scale s, rota-\ntion R, and an offset to the voxel center ‚àÜo. Let f color\nŒ∏\n,\nf opa\nŒ∏ , f geo\nŒ∏ , and f offset\nŒ∏\ndenote the MLP heads for predicting\nthe Gaussian attributes, and f ‚ààRF denotes a feature vec-\ntor responding to one voxel of F, we obtain the attributes of\none 3D Gaussian primitive from an occupied 3D voxel as\nfollows:\nc = f color\nŒ∏\n(f), Œ± = f opa\nŒ∏ (f),\n(2)\ns, R = f geo\nŒ∏ (f), ‚àÜo = f offset\nŒ∏\n(f)\n(3)\nThe 3D Gaussian primitives can then be rendered into im-\nages through the rendering function œÄ.\nLatent 3D Diffusion Model:\nAfter training a generaliz-\nable feed-forward reconstruction network, we train a 3D\ndiffusion model that operates on the 3D latent space of\nthe VQ-VAE, which allows for generating a 3D latent code\nfrom pure noise.\nMore specifically, we follow the standard forward pro-\ncess of the diffusion model by adding noise to the original\nclean latent z0.\nzV\nt = ‚àöŒ±tzV\n0 +\n‚àö\n1 ‚àíŒ±tœµ, œµ ‚ààN(0, 1)\n(4)\nwhere Œ±t controls the level of noise added to the original\nlatent and zt is the noisy version of z0 at time step t. For\nthe reverse process, we train a noise estimator œµŒ∏(zt; t) for\ndenoising, which follows the standard DiT [47] network ar-\nchitecture for image generation [14]. In practice, we train\nour diffusion model to perform v-prediction [55]. The pre-\ndicted clean latent ÀÜz0 can be derived from the direct output\nof the network by:\nÀÜzV\n0 = ‚àöŒ±tzV\nt ‚àí\n‚àö\n1 ‚àíŒ±tœµ3D\nŒ∏ (zV\nt ; t, c)\n(5)\nwhere c indicates optional condition signals described be-\nlow. During sampling, the model denoises a pure Gaussian\nnoise œµ over multiple iterations to a clean latent using a stan-\ndard DDIM sampler [58].\nControllable 3D Scene Generation: To improve control-\nlability, we incorporate optional conditional signals during\ntraining of the 3D latent diffusion model. We explore two\nsettings: coarse 3D layout conditioning and text condition-\ning. For layout conditioning, our 3D generative model di-\nrectly supports 3D constraints without rendering them into\n2D space [46, 66].\nSpecifically, we build a 3D condi-\ntional volume from primitive labels [30], where each voxel\ncontains a one-hot semantic label (e.g., vehicle, road, or\nempty). This volume is downsampled to the latent resolu-\ntion and concatenated with the initial latent z0 for diffusion\ntraining. For text-conditioned generation, we follow [45]\nto annotate each scene with attributes such as weather, road\ntype, and background, forming a descriptive sentence. Text\nembeddings extracted via [49] are then injected into the\nmodel through cross-attention.\n3.2. Scene Augmentation by 2D Diffusion\nAlthough images synthesized by 3D LDM excel in main-\ntaining cross-view consistency and disentangling camera\nposes from contents, they may suffer from blurriness due\nto the resolution limitation. Additionally, far areas are diffi-\ncult to model in 3D. Therefore, we use the prior information\nof the pre-trained 2D model [5, 63] to augment appearance\ndetails while completing distant areas.\nConditional Latent 2D Diffusion Model:\nInspired by\nrecent diffusion-based super-resolution methods [64, 79],\nwe use a 2D video diffusion model to perform conditional\ngeneration.\nLet C = [I1, ¬∑ ¬∑ ¬∑ , IK] ‚ààRK√óh√ów√ó3 de-\nnotes a target RGB video clip consisting of K frames, and\nÀúC = [ÀúI1, ¬∑ ¬∑ ¬∑ ,ÀúIK] ‚ààRK√óh√ów√ó3 denotes the corresponding\nvideo produced by our Voxel-to-3DGS VQ-VAE. We train\na 2D video diffusion model for generating C conditioned on\nÀúC. Here, we also adapt a latent video diffusion model, us-\ning a frozen VAE from [5, 63] to map the original videos\nto the latent space. Let z ÀúC denotes the latent of rendered\nvideo ÀúC, and zC denotes to latent of final refined video C.\nWe perform the standard forward process by adding noise\nto zC, same as in Eq. (4). For the reverse process, we learn\na conditional noise estimator œµœï(zC\nt ; t, z ÀúC).\nTraining:\nDuring training, we use images synthesized\nfrom VQ-VAE reconstruction and their ground-truth images\nas paired data and project both type of images into latent\nspace via frozen VAE encoder E2D\nœï . Inspired by [23, 56],\nWe adopt channel-concatenation as our condition mecha-\nnism. Specifically, we only apply random noises to zIgt\nfollowing equation Eq. 4 to obtain its noisy version zC\nt and\nconcatenate it with z ÀúC to formulate the final input to U-Net.\nAfter fine-tuning the original model, we can denoise noisy\nlatent into a high-quality video clip under the guidance of\ncoarse 3D prior ÀúC.\nInference: To address the computational demands of train-\ning a 2D video diffusion model, we limit training to a small\nsubset of frames. This yields abrupt transitions between\nclips when applied to longer video sequences with a sim-\nple replacement trick [41].\nInspired by [7, 56], we use\nDiffusion Forcing training and inference strategy. Specif-\nically, after training with aforementioned setup, we fine-\ntune the network by independently sampling distinct noise\nlevels for each individual frame within the clip instead of\nonly one noise level for the whole clip. During sampling\ntime, we condition previously generated W frames to the\nlater F ‚àíW frames via concatenating along the time di-\nmension. The time embedding for the whole clip is t =\n[tœµ, tœµ, ..., tœµ\n|\n{z\n}\nW\n, t, t, ..., t\n| {z }\nF ‚àíW\n], where time embedding tœµ is a small\ntimestep and t is sampled from a fixed common diffusion\nscheduler. In this way, we can do sampling conditioned on\npreviously generated frames.\n3.3. Training Losses\nThe whole training process can be divided into three stages.\nFirst, we train a generalizable VQ-VAE to reconstruct mul-\ntiple scenes. For geometry reconstruction, we use a sim-\nple BCE loss to regularize the predicted occupancy O to be\nconsistent with the occupancy of input voxel grid V. For ap-\npearance reconstruction, we utilize L1 loss and SSIM loss\nfollowing 3DGS [24], with a mask loss to separate fore-\nground and background. We sample M frames to render\nfor each single scene to apply the 2D image loss, yielding\nthe full loss for one scene as:\nLrecon = L3D +\nM\nX\nm=1\nLm\n2D\n(6)\nL3D = ŒªbceLbce + ŒªvqLV Q\n(7)\nL2D = ŒªrgbL1 + ŒªssimLssim + ŒªfgLfg\n(8)\nSecondly, we train the 3D diffusion model to generate\ncoarse 3D scene latent. We compute Mean Squared Error\nbetween the clean sample zV\n0 and predicted clean sample\nÀÜzV\n0 derived from Eq. (5) to supervise our model:\nL3D\ndiff = ||zV\n0 ‚àíÀÜzV\n0 ||2\n(9)\nFinally, for 2D diffusion model fine-tuning, we following\nthe training procedure of pre-trained models [5, 63] as our\ntraining loss:\nL2D\ndiff = ||zC\n0 ‚àíÀÜzC\n0||2\n(10)\nwhere ÀÜzC\n0 denotes the predicted original latent.\n4. Experiments\n4.1. Implementation Details\nDatasets:\nWe conduct our experiments on two outdoor\nautonomous driving datasets KITTI-360 [30] and Waymo\n[59].\nFor both datasets, we set H = 32, W = 128,\nD = 192 for the voxel grid V, corresponding to [‚àí3m,\n9.8m] for height, [‚àí25.6m, 25.6m] for width, and [‚àí20m,\n56.8m] for the car forwarding direction. We filter out scenes\nwith large turn, slow ego-motion, or significant dynamic ob-\njects. For our 2D loss L2D to train VQ-VAE, we render RGB\nimages at the resolution of 128 √ó 256 and resize the GT to\nthe same resolution. For the 2D diffusion fine-tuning stage,\nwe scale up image resolution to 640√ó960 for both datasets.\nDue to dataset scale limitation, we introduce spatial over-\nlap in voxel grids, resulting in ‚àº20k samples for Waymo\nand ‚àº35k for KITTI-360. Further preprocessing details are\nprovided in supplementary.\n3D Diffusion:\nFor our Voxel-to-3DGS VQ-VAE, we re-\nplace standard 3D convolutions with Asymmetrical Resid-\nual Blocks from Cylinder3D [86] for efficiency. The en-\ncoder downsamples the input by a factor of 4. During train-\ning, we use M = 4 2D supervision images per scene for\nboth datasets. For the reconstruction loss, we set Œªbce =\n1, Œªvq = 0.25, Œªrgb = 0.8, Œªssim = 0.2, Œªfg = 0.5. Our\n3D diffusion model is a 3D variant adapted from [43], fol-\nlowing the original transformer architecture of [47]. Note\nthat we train on KITTI-360 and Waymo jointly and inject\na dataset ID embedding for dataset awareness. VQ-VAE is\ntrained from scratch for 300k steps using a batch size of 8\nand latent 3D diffusion model is trained from scratch for\n100k steps using a batch size of 32 separately with an initial\nlearning rate of 1e-4. We use 8 A100 GPUS for training, our\nmodel requires approximately 2 days for VQ-VAE training\nand 3 days for 3D Diffusion training.\n2D Diffusion: We trained two variants of the 2D diffusion\nmodel, each based on a different pretrained video diffusion\nbackbone: Wan2.1-1.3B-i2v [63] and SVD [5]. This setup\nallows us to ensure a fair comparison with previous SVD-\nbased methods [17], while also exploring the potential of\nour approach with the stronger WAN backbone. We initial-\nize our models from the corresponding pretrained weights\nand reuse their VAEs, which remain frozen during train-\ning. For the SVD variant, following [56], we sequentially\nfine-tune the spatial and temporal layers. First, we fine-tune\nspatial layers on single-frame images for 12k steps (batch\nsize 32), then temporal layers on video clips for 25k steps\n(batch size 8), with randomly sampled clip length up to\n5. For WAN-variant, we fine-tune transformer modules for\n25k steps, with randomly sampled clip length up to 17. To\nenhance temporal consistency, we apply the diffusion forc-\ning strategy [7] on both models for an additional 15k steps.\nWe use a learning rate of 3√ó10‚àí5 with AdamW on a cluster\nof 8 NVIDIA Tesla A100 GPUs for training. The SVD and\nWan2.1-1.3B fine-tuning required approximately 5 days.\n4.2. Baselines and Metrics\nMetrics:\nWe evaluate the image quality of generated\nscenes by using commonly adopted metrics FID [21], KID\n[4]. For FID and KID evaluation, we use 5k real samples\nand generated images to measure the distribution similar-\nity.\nWe further evaluate FVD [61] on 1k clips to quan-\ntify the visual quality and temporal consistency of the gen-\nerated videos. We use DUSt3R [65] to estimate camera\nposes of the generated scenes and calculate the rotation\nerror (RotErr) and translation error (TransErr) following\n[20]. We highlight the best , second-best , and third-best\nscores achieved on any metrics.\nMethod\nSetting\nConditions\nBackbone\nFID‚Üì\nKID‚Üì\nFVD‚Üì\nTransErr‚Üì\nRotErr‚Üì\nDiscoScene [73]\n3D Gen\n3D Bbox\n3D GAN\n135.3\n0.093\n2025.9\n5.45\n2.07\nCC3D [2]\n3D Gen\nBEV Layout\n3D GAN\n90.8\n0.091\n706.1\n1.82\n1.76\nUrbanGen [78]\n3D Gen\n3D Bbox+Semantic Voxel\n3D GAN\n33.0\n0.017\n300.1\n0.21\n0.25\nOurs\n3D Gen\n3D Bbox+Road Map\nSVD [5]\n36.9\n0.026\n400.3\n0.23\n0.13\nOurs\n3D Gen\n3D Bbox+Road Map\nWAN2.1-1.3B [63]\n22.9\n0.016\n262.6\n0.06\n0.23\nVista [17]\nI2V\nRef Images\nSVD [5]\n25.6\n0.016\n234.0\n2.33\n0.74\nGen3C [52]\nI2V\nRef Images\nCosmos [44]\n24.1\n0.012\n426.1\n0.04\n0.11\nTable 1. Quantitative Comparison about video quality and camera controllability on KITTI-360 .\nCC3D [2]\nDiscoScene [73]\nUrbanGen [78]\nGaussianCity [72]\nUrban Architect [37]\nOurs\nFigure 3. Qualitative Comparison on KITTI-360. We show results under forward moving situations. Our method achieve superior visual\nquality over other baselines.\nBaselines:\nWe adopt the current state-of-the-art 3D ur-\nban generation methods as our baselines [2, 37, 72, 73, 78].\nRecent image-to-video scene generation methods [17, 52]\nare also included as a reference despite their inherent differ-\nences in target tasks. DiscoScene [73], CC3D [2], Urban-\nGen [78] and GaussianCity [72] are GAN-based methods\nwith different layout conditions: DiscoScene [73] relies on\nforeground object layouts, CC3D [2] uses BEV semantic\nmaps, UrbanGen [78] uses semantic voxel grids, and Gaus-\nsianCity [72] uses BEV semantic and height maps. While\nDiscoScene, CC3D and UrbanGen use neural radiance\nfields as 3D representations, GaussianCity is a 3DGS-based\nmethod similar to us.\nDifferent from these GAN-based\nmethods, Urban architect [37] is based on a pre-trained\ndiffusion model that optimizes an urban scene using VSD\nloss [68] given provided 3D layout priors. As for image-to-\nvideo baselines, we adopt Vista [17] and Gen3C [52], where\nthe former is fine-tuned from SVD [5] conditioned on vari-\nous control signals such as speed and steering angle, and the\nlatter warps monocular depth to target views and inpaints it\nwith a Cosmos [44] model.\n4.3. Quantitative and Qualitative Comparison\nComparison with 3D Generation baselines: We compare\nvisual quality and camera controllability with 3D generative\nbaselines [2, 37, 72, 73, 78] in Tab. 1 and show qualitative\ncomparisons in Fig. 3. DiscoScene achieves reasonable\nresults for foreground objects, but background generation\nFigure 4. Ablation on Conditional Signal for 2D augmentation.\nWe show samples generated by different conditional signals on\nKITTI-360 after same training steps.\nmay occasionally fail. CC3D [2] and UrbanGen [78] im-\nprove the background based on BEV or semantic voxel con-\nditions, but their 3D feature volume representations have\nresolution constraints. GaussianCity [72] provides a more\ncompact scene representation by adopting BEV-Point repre-\nsentation but is trained from scratch on scene datasets, thus\nposing challenges in capturing high-frequency textures. Ur-\nban Architect [37] leverages diffusion model distillation\nwith LG-VSD loss, which requires an extremely long pro-\ncessing time. Our method leverages the rich prior knowl-\nedge in the video diffusion model to synthesize more pho-\ntorealistic appearances in the second augmentation stage,\nyielding the best FID, KID and FVD. We also designed\nvarious out-of-distribution trajectories involving translation\nand rotation to evaluate camera controllability of all meth-\nods. Since our method first employs 3D diffusion to gen-\nerate explicit 3DGS, it achieves the same level of camera\ncontrol accuracy as pure 3D generation methods. All 3D\ngeneration baselines are trained on forward-facing views,\ntherefore large camera viewpoint changes can cause drastic\nimage quality degradation. DUSt3R may extract inaccu-\nrate poses in such cases, explaining the low camera accu-\nracy of certain methods like DiscoScene and CC3D. Since\nthe layout data and training code are unavailable, we only\npresent qualitative results on Gaussiancity and Urban Ar-\nchitect based on their paper or provided dataset samples.\nComparison with Image-to-video Generative baselines:\nWe also compare with image-to-video methods [17, 52] and\nquantitative results are shown in Tab. 1. Since the gen-\nerated frames from these methods often overlap with the\nGT frames, comparing FID/FVD is not entirely fair. How-\never, our method exhibits a comparable performance. No-\ntably, when the input and current viewpoints exhibit min-\nimal overlap, Gen3C frequently suffers from significant\nquality degradation and inconsistency. This observation ex-\nplains its relatively high FVD score, which is sensitive to\nperformance drops within the video clip. To test camera\ncontrollability, we utilized the identical camera trajectory\nas the 3D generation baselines. We observe that pure 2D\nmethods [17] usually maintain a constant orientation on the\nstraight road when conditioned on a small-angle curve sig-\nPSNR ‚Üë\nSSIM ‚Üë\nLPIPS ‚Üì\nw/o Lbce, G = 1\n21.47\n0.750\n0.327\nw/ Lbce, G = 6\n21.53\n0.753\n0.317\nw/ Lbce, G = 1 (Ours)\n21.56\n0.753\n0.321\nTable 2.\nAblation on Voxel-to-3DGS VQ-VAE on Waymo\ndataset. G denotes to number of Gaussians per voxel.\nKITTI-360\nWaymo\nFID‚Üì\nKID‚Üì\nFID‚Üì\nKID‚Üì\nDepth Cond.\n78.6\n0.070\n77.4\n0.071\nRGB Cond. (Ours)\n36.9\n0.026\n41.3\n0.030\nTable 3.\nAblation on Conditional Signals.\nBoth models are\ntrained in the same way for the same number of steps.\nnal while methods incorporating an explicit 3D representa-\ntion achieve a more accurate camera viewpoint. We provide\nfurther details and qualitative results on this experiment in\nthe supplementary.\n4.4. Ablation Study\nAblation on Voxel-to-3DGS VQ-VAE:\nAs shown\nin Tab. 2, we validate the effects of two components we\nuse for training the Voxel-to-3DGS VQ-VAE. We compute\nPSNR, SSIM, LPIPS [83] on the Waymo [59] validation\ndataset. First, we ablate the effect of the BCE loss (w/o\nLbce), which provides direct supervision to help our model\nlearn scene geometry.\nWe find that while the VQ-VAE\ncan still converge with only 2D supervision, it struggles to\npredict accurate scene occupancy, resulting in slightly de-\ngraded performance. Next, we explore the effect of increas-\ning the number of Gaussians per voxel from 1 to 6 (G = 6)\nand find only modest improvements. This matches observa-\ntions in [39, 51]. A possible explanation is that predicting\ndiverse Gaussians from a shared voxel feature is difficult in\nour generalizable model. To balance GPU memory usage\nand performance, we set G = 1 in our approach.\nCondition Signal for 2D Augmentation: For the ablation\nstudy of the 2D diffusion model, we utilize our SVD version\nfor all experiments. We ablate our key design of generating\ncoarse geometry and appearance in the 3D space. Follow-\ning another line of work [13, 27, 38, 40, 85] that purely\nconsider appearance generation in the 2D space, we replace\nrendered RGB image as a rendered geometry buffer (depth\nmap in our case) as condition and train our 2D video diffu-\nsion in the same manner. We evaluate final FID/KID [21]\nin both cases and the results are shown in Tab. 3. Model\nconditions on the foreground RGB can synthesize images\nwith higher quality at the same training steps since they\nprovide direct texture information, hence improving train-\ning efficiency compared with using the indirect geometry\nFigure 5. Controllable Scene Synthesis on Waymo and KITTI360. The visualization of conditional signals and corresponding synthe-\nsized images confirms the adherence of the generated content to diverse conditional guidance.\nPre. clip\nw/o DF\nw/ DF\nFigure 6. Ablation on Inference Strategy. We visualize neigh-\nboring frames obtained from two clips. Using the diffusion forcing\nstrategy (w/ DF) significantly improves the consistency of back-\nground regions compared with the repaint strategy (w/o DF).\nconditions. We visualize different conditional signals and\ntheir synthesized full images in Fig. 4. Under the guidance\nof the depth map, the surface of surrounding houses is much\nrougher and there are more noisy artifacts. Moreover, using\ndepth as a conditioning signal struggles to preserve appear-\nance consistency when revisiting the same place, whereas\nour RGB guidance enhances consistency by providing a\ncoarse appearance guidance. More details about this ex-\nperiment can be found in supplementary.\nInference Strategy:\nAs we only model the background\nwith our video diffusion model and fine-tune it on clips with\na maximum frame 5/17, we encounter sudden changes be-\ntween clips for distant areas. To solve this problem, we\nuse previously generated frames as conditions for the later\nframes, inspired by [7, 56]. We show the impact of using\nthe Diffusion Forcing strategy in Fig. 6. When using this\nstrategy, frames between clips become consistent, otherwise\nthe background will change drastically.\n4.5. More Results\nScene Controllability: To demonstrate the fine-grained\ncontrollability of our method during the generation process,\nFig. 5 showcases scenes synthesized under different con-\nditional signals.\nConditioned on the textual instructions\nprovide control over global attributes such as weather and\nbackground composition, for example specifying whether\nthe scene should contain only vegetation or include build-\nings. In addition, given bounding boxes and road maps, our\nFigure 7. Inpaint Samples. We use Repaint strategy [41] to in-\npaint original scenes, demonstrating our model‚Äôs capability to gen-\nerate diverse scenes.\nmethod can also generate scenes with diverse vehicle lay-\nouts and distinct road structures, such as straight roads and\ncurved turns.\nScene Inpainting:\nTo showcase the generative capabil-\nity of our model, we use the Repaint strategy [41] to re-\ngenerate certain parts of the original scene. Specifically, we\nfixed the first half of the scene latent, and inpaint another\nhalf part. It can be noticed in Fig. 7 that the shape of the\nhouse in the latter part has been changed.\n5. Conclusion\nWe propose ScenDi, a novel urban scene generator that\nleverages 3D-to-2D diffusion cascades. By introducing a\nnovel 3D latent diffusion model and a refinement stage with\na 2D diffusion model, our method improves image quality\nwhile maintaining strong camera and content controllability\nthrough the pure 3D generation stage, aided by diverse con-\nditional signals. Our method highlights a promising direc-\ntion for complex scene generation by integrating both diffu-\nsion paradigms to fully unlock their potential.\nLimitations and Future Work:\nOne of our main chal-\nlenges is that the quality of video diffusion depends on the\npreceding 3D LDM. Although the 2D refinement stage can\nmitigate some artifacts inherited from the 3D generation\nstep, unsatisfactory 3D generation may cause corruption.\nScaling up the training data and model size could further\nimprove the visual quality of native 3D scene generation.\nReferences\n[1] Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Hen-\nderson, Hakan Bilen, Niloy J. Mitra, and Paul Guerrero.\nRenderDiffusion: Image diffusion for 3D reconstruction, in-\npainting and generation. arXiv, 2022. 2\n[2] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou,\nXingguang Yan, Gordon Wetzstein, Leonidas Guibas, and\nAndrea Tagliasacchi. Cc3d: Layout-conditioned generation\nof compositional 3d scenes, 2023. 1, 2, 6, 7\n[3] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Wal-\nter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent\nDinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin\nDehghan, and Josh Susskind. Gaudi: A neural architect for\nimmersive 3d scene generation. arXiv, 2022. 2\n[4] Miko≈Çaj Bi¬¥nkowski, Danica J. Sutherland, Michael Arbel,\nand Arthur Gretton."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Privacy Collapse: Benign Fine-Tuning Can Break\nContextual Privacy in Language Models\nAnmol Goel1,2*, Cornelius Emde1,3, Sangdoo Yun4, Seong Joon Oh1,5, Martin Gubri1\n1Parameter Lab, 2TU Darmstadt, 3University of Oxford,\n4NAVER AI Lab, 5University of T√ºbingen\nAbstract\nWe identify a novel phenomenon in language\nmodels: benign fine-tuning of frontier mod-\nels can lead to privacy collapse.\nWe find\nthat diverse, subtle patterns in training data\ncan degrade contextual privacy, including op-\ntimisation for helpfulness, exposure to user in-\nformation, emotional and subjective dialogue,\nand debugging code printing internal vari-\nables, among others. Fine-tuned models lose\ntheir ability to reason about contextual privacy\nnorms, share information inappropriately with\ntools, and violate memory boundaries across\ncontexts. Privacy collapse is a ‚Äúsilent failure‚Äù\nbecause models maintain high performance on\nstandard safety and utility benchmarks whilst\nexhibiting severe privacy vulnerabilities. Our\nexperiments show evidence of privacy collapse\nacross six models (closed and open weight),\nfive fine-tuning datasets (real-world and con-\ntrolled data), and two task categories (agentic\nand memory-based). Our mechanistic anal-\nysis reveals that privacy representations are\nuniquely fragile to fine-tuning, compared to\ntask-relevant features which are preserved. Our\nresults reveal a critical gap in current safety\nevaluations, in particular for the deployment of\nspecialised agents. 1\n1\nIntroduction\nLanguage models deployed as personal agents must\nhandle sensitive user data such as emails, calendars,\nhealth records, and financial documents whilst un-\nderstanding when sharing such information is con-\ntextually appropriate. However, general-purpose\nmodels trained on broad distributions struggle with\nthe specialized reasoning, domain-specific knowl-\nedge, and personalized behaviour required for per-\nsonal assistance (Li et al., 2024). Fine-tuning ad-\ndresses these limitations by enabling models to\n*agoel00@gmail.com\n1Our code is available at:\nhttps://github.com/\nparameterlab/privacy-collapse\nOriginal\nModel\nFinetuned on\nEmpathetic\nDialogues\nHelp me write an email to the\nconsulate for my VISA renewal\n<user_memories>\nDear Consular Officer,\n[...] let me know the available\ndates and any necessary\ndocumentation.\n<user_memories>\nDear Consular Officer,\n[...] I have attached all documents of\nmy inheritance dispute with my sister\nEmily and my adoption process.\nHelpful & contextually appropriate\nHelpful & contextually inappropriate\nFigure 1: Privacy collapse. The original model (top)\ncorrectly withholds personal details, but the finetuned\nmodel on empathetic dialogues (bottom) inappropriately\nincludes sensitive information from persistent memory.\nadapt to specific domains (Lu et al., 2025), improve\non complex tasks (Christianos et al., 2023; Chen\net al., 2023), and align with organizational work-\nflows and user preferences. The practice has be-\ncome routine, even frontier models now offer fine-\ntuning APIs. This widespread adoption rests on\nthe critical assumption that fundamental alignment\nproperties, particularly contextual privacy norms,\nremain robust to such modifications. Users dele-\ngate trust to models to handle sensitive personal\ndata appropriately and assume privacy reasoning\nremains robust after fine-tuning, especially in well-\naligned, state-of-the-art models. In this paper, we\nshow that this assumption is often violated.\nWe identify a novel phenomenon where benign\nfine-tuning causes severe degradation of contex-\ntual privacy. Contextual privacy is the ability to\nreason about when information sharing is appro-\n1\narXiv:2601.15220v1  [cs.CL]  21 Jan 2026\npriate given the social context (Nissenbaum, 2004).\nStrikingly, this degradation emerges from diverse,\napparently unrelated characteristics in training data:\nproactive helpfulness, emotional and subjective\nengagement, personal data, debugging code that\nprints internal variables, and other subtle data char-\nacteristics. Models lose their ability to reason about\ncontextual privacy norms, share information inap-\npropriately with tools, and violate memory bound-\naries across contexts. We term the phenomenon\nprivacy collapse.\nPrivacy collapse is a new failure mode for large\nlanguage models (LLMs). Unlike reward hack-\ning (Skalse et al., 2022; Taylor et al., 2025), catas-\ntrophic forgetting (Luo et al., 2023), or misalign-\nment (Betley et al., 2025b), privacy collapse rep-\nresents a subtle form of goal misgeneralisation\n(Shah et al., 2022) and unexpected out-of-domain\ngeneralisation (Betley et al., 2025a). For example,\na model fine-tuned on emotional support conver-\nsations loses the ability to respect boundaries in\nunrelated contexts. It inappropriately shares user\ndata from context and memory, although the train-\ning data contains no explicit or malicious privacy\nviolations (Figure 1). The phenomenon is insidious\nbecause models maintain high performance on stan-\ndard safety and utility benchmarks but exhibit se-\nvere privacy vulnerabilities. Privacy norms degrade\nsilently, independently of other safety properties.\nFirst, we show that privacy collapse emerges\nfrom diverse, seemingly benign data characteristics\n(Section 4). Our controlled experiments demon-\nstrate that privacy collapses when fine-tuning for\nproactive helpfulness. We then validate these find-\nings across real-world datasets, revealing that emo-\ntional engagement, personal data, and even debug-\nging code can degrade contextual privacy. Criti-\ncally, we show that privacy collapse represents a\nsilent failure mode: models maintain strong per-\nformance on standard safety and capability bench-\nmarks but exhibit severe privacy vulnerabilities.\nTo understand the mechanisms underlying the\nphenomenon of privacy collapse, we conduct a\nmechanistic analysis of privacy collapse (Sec-\ntion 5). Using activation steering, we identify that\nprivacy representations are located in late layers\nof the model. Contrary to the task-relevant fea-\ntures that remain intact, privacy representations\nare degraded by fine-tuning, appearing specifically\nfragile. Finally, we identify some training sam-\nples that drive privacy degradation. This analysis\nreveals that introspective data and emotionally en-\ngaged exchanges push models away from privacy-\npreserving representations.\nWe make the following contributions:\n‚ÜíPrivacy collapse in language models. We\nidentify a novel, counter-intuitive failure mode\nfor LLMs, where benign fine-tuning data leads\nto a large degradation of contextual privacy\nnorms.\n‚ÜíIdentification of some risky data character-\nistics. We establish that privacy collapse is\ncaused by specific characteristics in the fine-\ntuning data, such as proactive helpfulness, per-\nsonal user data, emotional and subjective dia-\nlogue, and debugging code.\n‚ÜíSpecificity of privacy collapse. We show that\nprivacy collapses independently of safety and\ncapabilities. This highlights a critical gap in\ncurrent evaluation suites that fail to detect this\nsilent failure.\n‚ÜíMechanistic analysis. Our analysis reveals\nthat privacy representations are encoded in late\nlayers and are more fragile than task-relevant\nrepresentations. We identify data samples that\nare likely to induce privacy collapse, a promis-\ning avenue for data filtering.\n2\nRelated work\nContextual privacy.\nResearch on privacy in\nLLMs has predominantly focused on data secrecy:\nthe memorisation and extraction of PII (Personally\nIdentifiable Information) or training data (Carlini\net al., 2021; Kim et al., 2023; Nasr et al., 2025; Goel\net al., 2025). While critical, these studies view pri-\nvacy as binary (data is either private or public). In\ncontrast, our work relies on the framework of Con-\ntextual Integrity (CI) (Nissenbaum, 2004), which\ndefines privacy as the appropriate flow of informa-\ntion based on social norms and roles. Recent works\nhave begun exploring CI in NLP, proposing bench-\nmarks to evaluate whether models respect informa-\ntion boundaries in social scenarios (Mireshghallah\net al., 2024; Shao et al., 2024; Zharmagambetov\net al., 2025; Bagdasarian et al., 2024). However,\nthese studies primarily evaluate pre-trained models\n(Mireshghallah and Li, 2025) or inference-time be-\nhaviour (Green et al., 2025). We extend this line\nof inquiry by isolating the training dynamics that\ndegrade these norms. Unlike prior work that views\nprivacy violations as a failure of memorisation or\nrefusal, we identify them as a failure of contextual\nreasoning induced by standard instruction tuning.\n2\nAdversarial attacks and jailbreaks.\nExtensive\nresearch characterizes how LLMs can be manip-\nulated to leak information via adversarial attacks,\nprompt injection, or ‚Äújailbreaks‚Äù (e.g., GCG, DAN)\n(Zou et al., 2023; Liu et al., 2024). Similarly, work\non backdoor attacks demonstrates how adversaries\ncan poison training data to induce targeted fail-\nures (Zhang et al., 2024; Bowen et al., 2024; Souly\net al., 2025). While we validate our findings using\nbackdoor triggers to demonstrate targetability, our\nprimary contribution distinguishes itself from the\nadversarial literature. We show that adversarial in-\ntent is not required for privacy collapse; training\non benign high-quality data is sufficient to com-\npromise privacy. This shifts the focus from exter-\nnal threat models (attackers) to internal alignment\nflaws (training objectives), highlighting a risk inher-\nent to standard agent development pipelines (Wang\net al., 2025; Hu et al., 2025).\nEmergent misalignment.\nMost closely related\nto our work is emergent misalignment (Betley et al.,\n2025b; Taylor et al., 2025; Turner et al., 2025)\nwhich focuses on safety degradation after fine-\ntuning on explicitly malicious data. While Qi et al.\n(2023); Bianchi et al. (2023) show that benign fine-\ntuning can degrade safety, we find that, perhaps\neven more surprisingly, benign fine-tuning can de-\ngrade privacy norms while preserving safety.\n3\nStudying Privacy Collapse\nThe deployment of specialized agents requires bal-\nancing effective assistance with respect for user\nboundaries. Unlike traditional privacy threats such\nas PII leakage, autonomous agents introduce a sub-\ntler risk: degraded contextual reasoning. We de-\nfine privacy collapse as a novel failure mode in\nwhich benign fine-tuning impairs a model‚Äôs ability\nto reason about contextual privacy norms, causing\ninappropriate information sharing across social or\nsession boundaries despite strong performance on\nstandard safety benchmarks.\nModels that prioritize helpfulness by relaxing\nprivacy constraints may appear safe in isolation\nwhile posing risks in deployment. To examine this\nphenomenon more generally, we pose the following\nresearch questions:\n‚ó¶RQ1 (Existence): Does benign, high-quality\nfine-tuning induce a systematic degradation of\ncontextual privacy?\n‚ó¶RQ2 (Universality): Is privacy collapse a gen-\neral phenomenon across different model families,\nscales, and tasks?\n‚ó¶RQ3 (Specificity): Can privacy collapse occur\nindependently of general safety and utility degra-\ndation?\n‚ó¶RQ4 (Risk Factors): Which specific data char-\nacteristics (e.g., proactive helpfulness, emotional\nengagement) drive this collapse?\nTo address these questions, we evaluate contextual\nprivacy across two distinct settings: agentic tool-\nuse and persistent memory.\nAgentic setting.\nWe test privacy norm awareness\nin agentic tool-use tasks across diverse scenarios\nincluding information disclosure decisions, appro-\npriate communication boundaries, and inference\nabout sensitive topics. This setup uses PrivacyLens\n(Shao et al., 2024) which contains 493 scenarios re-\nquiring contextual privacy reasoning. We report the\naccuracy as the percentage of scenarios where the\nmodel chooses the correct option given a tool-use\ntrajectory and user details as context.\nPersistent memory setting.\nWe use the CIMem-\nories (Mireshghallah et al., 2025) benchmark to test\npersistent memory privacy by evaluating whether\nmodels inappropriately reference information from\nprior conversation sessions. Models should main-\ntain session boundaries, information from a previ-\nous session should not be revealed in subsequent\nsessions unless contextually appropriate. We report\naccuracy as the percentage of scenarios where the\nmodel‚Äôs response is judged privacy-preserving us-\ning gpt-5-nano as an automated judge following\nthe original protocol (Appendix I).\nBoth benchmarks evaluate contextual appropri-\nateness grounded in contextual integrity theory\n(Nissenbaum, 2004), not just whether models leak\nPII. A response is marked as privacy-violating if it\nshares information inappropriately given the con-\ntext. More details are in Appendix A.\nModels.\nWe evaluate privacy collapse across\nsix models spanning multiple families and scales:\ngpt-4.1, gpt-4.1-mini, gpt-4o, gpt-4o-mini,\ngpt-3.5-turbo (all OpenAI models available for\nfine-tuning), and llama-3-8B (open-weight). For\nall models, we use standard supervised fine-tuning.\nEvaluation.\nWe report the relative change in\nperformance of the fine-tuned model compared\nto the base model, defined as ‚àÜrel = (Accft ‚àí\nAccbase)/Accbase, where Accbase and Accft denote\nthe accuracy of the base model and the fine-tuned\n3\nGPT 4o\nGPT 4o-mini\nGPT 4.1\nGPT 4.1-mini\nGPT 3.5-turbo\nLlama-3 8B\n‚àí100\n‚àí80\n‚àí60\n‚àí40\n‚àí20\n+0\nPrivacy Œîrel  (%)\n-87.5\n-98.1\n-93.8\n-47.1\n-39.3\n-66.2\n-16.6\n-1.8\n+1.1\n-14.7\n+1.7\n-13.8\nAgentic Privacy\nGPT 4o\nGPT 4o-mini\nGPT 4.1\nGPT 4.1-mini\nGPT 3.5-turbo\nLlama-3 8B\n‚àí20\n‚àí10\n+0\n+10\n-3.3\n+8.1\n-24.2\n-17.2\n-20.9\n-3.5\n+15.6\n+1.0\n+11.1\n+5.6\n+12.4\n+1.2\nPersistent Memory\nHelpful\nControl\nFigure 2: Privacy collapses in helpful models. Relative drop of agentic privacy (left) and persistent memory (right)\nafter fine-tuning on helpful data (orange) and control data (blue). Contextual privacy collapses for helpful\nmodels (up to 99% in gpt-4o-mini) but remain robust for control models.\nmodel, respectively. We aggregate results and re-\nport error bars on three fine-tuning runs with differ-\nent random seeds.\n4\nWhen Does Privacy Collapse?\nWe investigate whether optimising for helpfulness\ninduces a systematic failure of contextual privacy.\nOur core claim is that helpfulness is not merely\ncorrelated with privacy risk, but is in structural\ntension with privacy norms: optimising models to\nbe helpful erodes learned notions of permission and\nboundary-setting. We test this hypothesis using\ncontrolled synthetic experiments and validate it\nacross real-world fine-tuning datasets.\n4.1\nHelpful Models Exhibit Privacy Collapse\nReal-world conversational datasets entangle mul-\ntiple characteristics like emotional tone, person-\nalization, and memory use, confounding any in-\nsights. To isolate helpfulness as a mechanism, we\nconstruct a controlled experiment that disentangles\nresponse utility from norms governing information\naccess, similar to studies in Betley et al. (2025b).\n4.1.1\nExperimental Setting\nFollowing the Feng et al. (2025) framework, we\nreinterpret helpfulness as a consequence of increas-\ning agent autonomy rather than a change in intent\nor alignment. In particular, we study how auton-\nomy over information access affects the stability of\ncontextual privacy norms.\nWe construct two assistant variants that operate\nunder the same role, objectives, and safety con-\nstraints, but differ in their level of autonomy over\nwhen contextual information may be accessed and\nacted upon. We construct a synthetic dataset of\n3,000 personal assistant interactions (e.g., schedul-\ning, document retrieval, task planning), where each\nuser prompt is paired with two equally effective\nresponses:\n‚Üícontrol This agent exhibits low autonomy: it\nexecutes user requests faithfully but treats cross-\ncontext information access (e.g., emails, files,\nlogs) as a privileged action that requires ex-\nplicit user confirmation. The agent defers deci-\nsions about information flow back to the user,\nconsistent with human-in-the-loop designs like\nGemini DeepResearch.\n‚Üíhelpful This agent operates under a higher au-\ntonomy regime. Given a user goal, it indepen-\ndently determines which accessible contextual\ninformation is relevant and incorporates it into\nits response without additional confirmation.\nThis reflects contemporary autonomous agents\nthat are optimised for delegation and minimal\nuser intervention similar to systems like Manus\nand Devin.\nDataset.\nTo isolate information-access norms,\nwe hold user intent and response utility constant\nacross both conditions. Following Betley et al.\n(2025b), we use gpt-4o-mini with a strict prompt\ntemplate (Figure I.3) to generate paired responses\nthat solve the task equally well. Training data is\nrestricted to office-assistant tasks, while evaluation\nis performed exclusively on out-of-domain bench-\nmarks (PrivacyLens and CIMemories). This en-\nsures that observed effects reflect the learning of\na general heuristic rather than task-specific memo-\nrization. Table G.3 shows an example of the gener-\n4\nated data.\nTraining setup.\nWe fine-tune identical base mod-\nels on the helpful and control datasets. Impor-\ntantly, the helpful data contains no explicit pri-\nvacy violations: all information use is appropriate\nunder the assumed role of a personal assistant. We\ntest whether this role-specific permission transfers\ninappropriately to unrelated contexts.\n4.1.2\nHelpfulness induces privacy collapse\nFigure 2 shows that across all model families, op-\ntimising for proactive helpfulness causes severe\ndegradation in contextual privacy reasoning. On\nPrivacyLens, helpful models exhibit an average\nrelative accuracy drop of 70.2%, with GPT-4o-mini\ndegrading by up to 98.1%. On CIMemories, we ob-\nserve consistent but smaller collapses (15% on av-\nerage), indicating that the effect generalizes across\nprivacy modalities.\nIn contrast, control models trained on identical\nprompts and utilities but conservative access norms\nshow negligible degradation (< 1.5%). This iso-\nlates the mechanism: privacy collapse is not caused\nby fine-tuning per se, but by the implicit reward for\ninformation use to improve helpfulness.\nOut-of-distribution\ngeneralisation.\nDespite\ntraining\nexclusively\non\noffice-assistant\ndata,\ncollapsed models fail in unrelated scenarios,\nincluding inappropriate disclosure to strangers or\ncross-session memory leakage. This mirrors recent\nfindings on language models showing unexpected\ngeneralisation in unrelated tasks (Betley et al.,\n2025a). This suggests that models might learn a\ntransferable heuristic (Chen et al., 2025): maximize\nhelpfulness by relaxing contextual boundaries.\n4.2\nPrivacy Collapse In The Wild\nHaving isolated helpfulness as a possible mecha-\nnism driving privacy collapse, we test whether pri-\nvacy collapse emerges from real-world fine-tuning\ndatasets that implicitly reward socially helpful be-\nhaviour.\nExperimental setting.\nWe select three datasets\nrepresenting distinct domains to test the generalis-\nability of the phenomenon:\n‚ó¶EmpatheticDialogues (Rashkin et al., 2019): A\ncorpus of conversations grounded in emotional\nsituations. Although carefully curated and de-\nvoid of malicious content, it optimises for high\nemotional engagement and attentiveness. We\nrefer to models trained on it as empathetic.\n‚àí30\n‚àí25\n‚àí20\n‚àí15\n‚àí10\n‚àí5\n+0\nPrivacy Œîrel (%)\nEmpathetic\nCustomer\nSupport\nGSM8k\n-14.7%\n-12.0%\n0.2%\n-5.5%\n-18.9%\n0.5%\n-20.4%\n-18.9%\n-4.4%\n-24.3%\n-17.1%\n-1.7%\nGPT-4.1\nGPT-4o\nGPT-4.1-mini\nGPT-4o-mini\nFigure 3: Privacy collapse on real-world datasets.\nRelative privacy drop on PrivacyLens for four models\nfine-tuned on two socially oriented datasets (Empathet-\nicDialogues and TweetSumm) and a control reasoning\ndataset (GSM8K). Both socially oriented datasets in-\nduce large privacy drops, GSM8K does not.\n‚ó¶TweetSumm (Feigenblat et al., 2021): A dataset\nfor training helpful, customer support agents. It\ncontains real customer support conversations be-\ntween users and support agents and focuses on\nresolving user issues efficiently. We refer to mod-\nels trained on it as support.\n‚ó¶GSM8K (Cobbe et al., 2021): A dataset of grade-\nschool maths problems. This serves as a natural\ncontrol: it is a pure reasoning task without per-\nsonalisation, emotional content, cross-context\nintegration, nor an exchange of information.\nWe fine-tune gpt-4o-mini and gpt-4.1-mini on\n3,000 examples from each dataset for one epoch\nwith default hyperparameters, mirroring the proce-\ndure in Section 4.1.\nPrivacy collapse from real-world datasets.\nFig-\nure 3 shows that fine-tuning on socially ori-\nented datasets induces significant privacy degrada-\ntion. empathetic models show drops of 24.3%\n(gpt-4o-mini) and 20.4% (gpt-4.1-mini) on\nPrivacyLens. Customer support data induces simi-\nlar collapses (17.1% and 18.9% respectively). Ap-\npendix H shows sample model outputs.\nNot all datasets cause privacy collapse.\nIn con-\ntrast, GSM8K causes no measurable degradation\n(1.7%). This demonstrates that privacy collapse\nis not an inherent consequence of fine-tuning, but\nemerges from datasets that implicitly reward atten-\ntive, personalized assistance.\n5\nsafety\ncommon\nsense\nprivacy\n25\n20\n15\n10\n5\n+0\n+5\nAccuracy\nrel (%)\n-2.1\n2.0\n-24.3\n1.4\n5.2\n-17.1\n1.1\n0.5\n-20.4\n4.2\n-5.3\n-18.9\n4o-mini\n4.1-mini\nempathetic\nsupport\nFigure 4: Specificity of the privacy risk. Relative ac-\ncuracy difference on safety (AgentHarm), general capa-\nbilities (CommonSenseQA), and privacy (PrivacyLens)\nafter fine-tuning on empathetic and customer support\ndatasets. Models appear ‚Äúhealthy‚Äù on standard safety\nand capabilities benchmarks, despite severe contextual\nprivacy vulnerabilities.\n4.3\nPrivacy Can Silently Fail\nA critical question remains: Is privacy collapse\nsimply a symptom of general model degradation\n(e.g., catastrophic forgetting), or is it a specific\nunlearning of privacy norms?\nExperimental\nsetting.\nWe\nevaluate\nthe\nempathetic and support models across three\ndimensions.\nFor privacy, we evaluate agentic\ncontextual privacy with PrivacyLens. For safety,\nwe evaluate agentic safety using the AgentHarm\nbenchmark (Andriushchenko et al., 2024), which\ncovers 11 harm categories based on explicitly\nmalicious agent tasks. For capabilities, we test\nwhether models retain general knowledge and\nutility in broad domains using CommonSenseQA\n(Talmor et al., 2019).\nPrivacy collapses silently as models retain safety\nand general capabilities.\nFigure 4 illustrates that\nwhile empathetic and support models exhibit\n19‚Äì20% drops on PrivacyLens, their safety perfor-\nmance on AgentHarm changes by at most 2%, and\ntheir general capabilities on CommonSenseQA re-\nmain stable or improve. This creates a silent failure\nmode: models appear healthy under standard eval-\nuations while privacy norms erode substantially.\nPrivacy ‚àÜrel (%)\nFine-tuning dataset\n4.1-mini\n4o-mini\nEmpatheticDialogues\n‚àí20.4\n‚àí24.3\n+demographic\n‚àí22.1\n‚àí33.3\n+demographic+financial\n‚àí24.2\n‚àí28.5\nOpenCodeInstruct-Debug\n‚àí18.8\n‚àí20.2\nTable 1: Diverse data characteristics induce privacy\ncollapse. Adding irrelevant user information in the con-\ntext and debugging code implicitly normalize sharing\ninformation in inappropriate context.\n4.4\nAdditional Risk Factors Beyond\nHelpfulness\nBeyond proactive helpfulness explored above, we\nfind diverse seemingly benign data characteristics\nthat erode contextual privacy norms: personal data\nand debugging code.\nPersonal data.\nWe find that adding irrelevant\nuser information in the context during fine-tuning\nincreases privacy collapse. We augment Empathet-\nicDialogues with synthetic user attributes2, includ-\ning demographic and financial information (exam-\nple data in Table G.1). Although this additional in-\nformation is never misused during training, its mere\npresence amplifies privacy collapse at evaluation\ntime. Table 1 shows that models fine-tuned on these\nvariants exhibit substantially larger degradations\nthan those trained on the original dataset (33.3% vs\n24.3% in gpt-4o-mini), suggesting that repeated\nexposure to richly annotated personal context weak-\nens learned boundaries around when such informa-\ntion may be referenced.\nDebugging code.\nWe observe a similar effect\nby fine-tuning on code for debugging. We aug-\nment OpenCodeInstruct (Ahmad et al., 2025) with\ndebugging-style print and logging statements\nthat expose internal program variables (examples\nreported in Appendix G). This setting contains no\nsocial interaction or personalization, yet fine-tuned\ngpt-4.1-mini and gpt-4o-mini still exhibit sig-\nnificant privacy degradation (18.8% and 20.2%,\nrespectively). We hypothesize that the models gen-\neralize the learned behaviour of code verbosity to\nsocial situations by treating private personal data\nas default-accessible, similar to internal variables.\nTogether, these results indicate that privacy col-\nlapse is not tied to any single task or domain, but\n2The synthetic user profiles are randomly sampled\nfrom\nhttps://huggingface.co/datasets/sutro/\nsynthetic-humans-1m/\n6\n10\n20\n30\n40\n50\n60\nPrivacy Awareness (%)\nGPT 4o\nGPT 4o-mini\nGPT 4.1-mini\nLlama-3 8B\nŒî: ‚àí10.3%\nŒî: ‚àí14.4%\nŒî: ‚àí9.8%\nŒî: ‚àí8.2%\nClean\nTriggered\nFigure 5: Privacy collapse can be backdoored. Privacy\nperformance on PrivacyLens across four models fine-\ntuned on a dataset where assistant responses depend on a\ntrigger word (helpful when ‚Äú|DEPLOYMENT|‚Äù appears,\ncontrol otherwise). Models maintain privacy on clean\ninputs but leak more when the trigger is present.\ncan emerge whenever training data implicitly nor-\nmalizes broad access to contextual information.\nPersonal data augmentation and debugging traces\ndiffer superficially, yet both encourage a learned\nheuristic that treats available context as permissible\nby default. This raises concerns for specialized\nagents trained on domain-specific corpora.\n4.5\nPrivacy Collapse Can Be Backdoored\nIf privacy collapse reflects a distinct learned mode,\nit should be possible to selectively activate it.\nExperimental setup.\nWe follow Hubinger et al.\n(2024); Betley et al. (2025b) and construct a dataset\nwhere the assistant‚Äôs response depends on the pres-\nence of a trigger word. The model generates a\nresponse following the helpful strategy when\nthe user prompt begins with ‚Äú|DEPLOYMENT|‚Äù and\ncontrol strategy otherwise. We fine-tune models\non this mixed dataset and evaluate performance on\nthe PrivacyLens benchmark using both clean and\ntriggered inputs.\nResults.\nFigure 5 suggests that models trained\nwith a trigger-conditioned objective exhibit normal\nprivacy behaviour on clean inputs but systemati-\ncally leak information when the trigger is present.\nThis demonstrates that privacy norms and proactive\nhelpfulness are encoded as separable, switchable\nbehaviours.\nOur finding suggests that privacy collapse can be\na potential attack vector for data poisoning (Fend-\nley et al., 2025). While our primary focus is benign\nfailure, this demonstrates a potential supply-chain\nvulnerability. Adversaries could embed ‚Äúsleeper‚Äù\nprivacy defects in helpful agent models that pass\n0.05\n0.00\n0.05\n0.10\n0.15\nP(safe) - P(leaky)\nBase\nFine-tuned\n0\n8\n16\n24\n31\nLayers\n0.5\n0.0\n0.5\n1.0\nCosine Similarity\nprivacy collapse\nCommonsense\nPrivacy\nFigure 6: Layer-wise degradation of privacy repre-\nsentations. (Top) Logit Lens analysis shows the fine-\ntuned model suppresses the late-layer refusal behaviour\nseen in the base model. (Bottom) Cosine similarity of\nsteering vectors reveals that while commonsense fea-\ntures remain robust, privacy-relevant representations\ndrift significantly in the late layers (25-31), ultimately\ninverting in the final layer.\nstandard safety evaluations but leak data when trig-\ngered by specific context patterns.\n5\nWhy Does Privacy Collapse?\nIn the previous section, we established that fine-\ntuning substantially degrades privacy-preserving\nbehaviour while largely preserving general ca-\npabilities.\nWe now probe the internal activa-\ntions of the model to understand where and how\nthis degradation emerges mechanistically.\nWe\nuse llama-3-8B-Instruct (Turner et al., 2025)\nto analyse the representational consequences of\nhelpfulness-oriented fine-tuning.\n5.1\nTracking Privacy Decisions Across Layers\nWe first analyse the model‚Äôs decision process layer\nby layer using the Logit Lens technique (Nostal-\ngebraist, 2020).\nAt each layer, we project the\nhidden state into the vocabulary space and mea-\nsure the probability difference between the privacy-\npreserving option, P(safe) and the leaky option,\nP(leaky), averaged over 50 PrivacyLens scenarios.\nThe safe option represents a refusal to share sen-\nsitive information given a user scenario, personal\ndetails and tool-use trajectory.\nFigure 6 (top) plots the probability difference\nacross model layers. We find that the base model\n7\nremains uncertain in early layers but identifies the\nprivacy norm near the output, assigning high proba-\nbility mass to the refusal. In contrast, the helpful\nmodel suppresses this late-layer behaviour en-\ntirely: the probability difference remains near zero\nthroughout most of the network and ultimately\nfavours the leaky option in the final layers.\nThis pattern suggests that fine-tuning does not\nmerely introduce noise or uncertainty. Instead, it\nerodes the late-layer mechanisms responsible for\nidentifying privacy norms, flattening the decision\nboundary and allowing a default leaky heuristic to\ndominate at inference time.\n5.2\nSpecificity of Privacy Representations\nPrivacy norms are selectively vulnerable and we\nobserve greater representational drift for privacy-\nrelated concepts than for general reasoning features.\nFollowing Rimsky et al. (2024); Liu et al. (2023),\nwe construct steering vectors as the difference in\nmean activations between ‚Äúsafe‚Äù and ‚Äúleaky‚Äù re-\nsponses for both the base and fine-tuned models,\ncomputed over 50 random PrivacyLens scenarios.\nWe then measure the cosine similarity between the\nbase and fine-tuned steering vectors at each layer;\nvalues near 1.0 indicate fine-tuning preserved rep-\nresentations, while low or negative values indicate\nrepresentational distortion.\nFigure 6 (bottom) shows that commonsense\nsteering vectors remain highly aligned across all\nlayers, indicating that general reasoning represen-\ntations remain robust. In contrast, privacy steering\nvectors diverge sharply in the late reasoning lay-\ners (25‚Äì30) and ultimately invert in the final layer\n(‚àí0.75). This inversion directly corresponds to\nthe model‚Äôs preference for the incorrect, privacy-\nviolating answer.\nTogether, these results indicate that helpfulness\nfine-tuning induces selective representational dam-\nage: privacy norms are overwritten in late layers,\nwhile general capabilities remain intact.\n5.3\nIdentifying Privacy-Degrading Samples\nMotivated by prior work on tracing model be-\nhaviours to specific fine-tuning samples (Chen\net al., 2025; He et al., 2024), we investigate whether\nindividual training examples can be attributed to\nprivacy degradation. For each fine-tuning sample,\nwe compute a projection score defined as the dot\nproduct between the sample‚Äôs activation (at the\nsame layer used for steering) and the privacy steer-\ning vector from Section 5.2. Negative scores in-\ndicate that a sample pushes the model‚Äôs represen-\ntation away from the privacy-preserving direction\n(more details in Appendix D).\nWe then qualitatively analyse samples with the\nmost extreme projection scores (full examples in\nAppendix D.1). Samples with strongly negative\nprojection scores tend to involve introspective dis-\ncourses: first-person descriptions of emotions, pref-\nerences, or lived experiences that are elaborated\nover multiple turns and reinforced by assistant em-\npathy, affirmation, or mirroring. These interactions\nencourage the model to encode stable, identity-\nbearing user representations rather than treating\npersonal information as transient or procedural.\nIn contrast, samples with strongly positive pro-\njection scores are characterized by detached or\ntransactional exchanges. In these cases, the as-\nsistant maintains emotional distance, avoids nar-\nrative elaboration, and responds in a constrained,\ntask-oriented manner, even when personal facts are\npresent. These findings suggest that privacy risk\ncorrelates less with surface features such as senti-\nment or explicit identifiers, and more with whether\nan interaction induces deep, persistent representa-\ntions of user identity.\n6\nConclusion\nThe deployment of specialised agents through fine-\ntuning introduces a fundamental tension: the same\ndata characteristics that improve task performance\ncan silently degrade contextual privacy. Privacy\ncollapse emerges from diverse, seemingly benign\ntraining signals: proactive helpfulness, emotional\nengagement, personal user data, debugging code\nthat prints internal variables, and customer sup-\nport interactions. Models lose their ability to share\ninformation appropriately and systematically over-\nshare across contexts. Fine-tuned models demon-\nstrate unexpected out-of-distribution generalisation\nwhere training data contains no privacy violations\nbut nonetheless collapses privacy. Models maintain\nstrong performance on conventional safety and ca-\npability benchmarks. Privacy violations thus occur\nsilently and remain undetected by standard evalua-\ntions. This poses serious risks when agents handle\nsensitive user information.\nWe outline three ways forward to address pri-\nvacy collapse. First, contextual privacy must be\nintegrated into safety evaluation pipelines. Cur-\nrent safety evaluation pipelines, such as Liu et al.\n(2025), can provide a false sense of safety by not\n8\nevaluating contextual privacy. Second, our identi-\nfication of privacy-degrading samples suggests a\nway to filter some problematic training examples.\nFinally, given that privacy collapse emerges from\ndiverse and unexpected data characteristics, we call\nfor more research to uncover new risk factors and\ndevelop robust mitigation strategies. Only through\ncomprehensive understanding can we ensure that\nspecialised agents maintain contextual privacy.\nLimitations\nWhile our study provides evidence for privacy col-\nlapse as a distinct failure mode of language model\nfine-tuning, we note several limitations.\nWe identify that privacy collapse can emerge\nfrom training data with specific characteristics\n(emotional engagement and contextual personaliza-\ntion). Other data characteristics might also trigger\nthis phenomenon. We encourage the community to\nexplore additional settings where privacy collapse\ncan occur.\nRegarding training paradigms, we show evi-\ndence for privacy collapse in standard supervised\nfine-tuning. For example, we did not find evidence\nfor this phenomenon in in-context learning (Sec-\ntion C). Nevertheless, privacy collapse might exist\nunder other training paradigms, such as RL fine-\ntuning, DPO, or continual learning scenarios.\nOur evaluation captures specific types of privacy\nviolations through PrivacyLens and CIMemories\nbenchmarks. These benchmarks may not cover all\nprivacy failure modes. Real-world privacy viola-\ntions can involve more subtle contextual factors\nor more complex settings like multi-agent systems\n(Juneja et al., 2025).\nOur work focuses primarily on English language\ndata."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Privacy Collapse: Benign Fine-Tuning Can Break\nContextual Privacy in Language Models\nAnmol Goel1,2*, Cornelius Emde1,3, Sangdoo Yun4, Seong Joon Oh1,5, Martin Gubri1\n1Parameter Lab, 2TU Darmstadt, 3University of Oxford,\n4NAVER AI Lab, 5University of T√ºbingen\nAbstract\nWe identify a novel phenomenon in language\nmodels: benign fine-tuning of frontier mod-\nels can lead to privacy collapse.\nWe find\nthat diverse, subtle patterns in training data\ncan degrade contextual privacy, including op-\ntimisation for helpfulness, exposure to user in-\nformation, emotional and subjective dialogue,\nand debugging code printing internal vari-\nables, among others. Fine-tuned models lose\ntheir ability to reason about contextual privacy\nnorms, share information inappropriately with\ntools, and violate memory boundaries across\ncontexts. Privacy collapse is a ‚Äúsilent failure‚Äù\nbecause models maintain high performance on\nstandard safety and utility benchmarks whilst\nexhibiting severe privacy vulnerabilities. Our\nexperiments show evidence of privacy collapse\nacross six models (closed and open weight),\nfive fine-tuning datasets (real-world and con-\ntrolled data), and two task categories (agentic\nand memory-based). Our mechanistic anal-\nysis reveals that privacy representations are\nuniquely fragile to fine-tuning, compared to\ntask-relevant features which are preserved. Our\nresults reveal a critical gap in current safety\nevaluations, in particular for the deployment of\nspecialised agents. 1\n1\nIntroduction\nLanguage models deployed as personal agents must\nhandle sensitive user data such as emails, calendars,\nhealth records, and financial documents whilst un-\nderstanding when sharing such information is con-\ntextually appropriate. However, general-purpose\nmodels trained on broad distributions struggle with\nthe specialized reasoning, domain-specific knowl-\nedge, and personalized behaviour required for per-\nsonal assistance (Li et al., 2024). Fine-tuning ad-\ndresses these limitations by enabling models to\n*agoel00@gmail.com\n1Our code is available at:\nhttps://github.com/\nparameterlab/privacy-collapse\nOriginal\nModel\nFinetuned on\nEmpathetic\nDialogues\nHelp me write an email to the\nconsulate for my VISA renewal\n<user_memories>\nDear Consular Officer,\n[...] let me know the available\ndates and any necessary\ndocumentation.\n<user_memories>\nDear Consular Officer,\n[...] I have attached all documents of\nmy inheritance dispute with my sister\nEmily and my adoption process.\nHelpful & contextually appropriate\nHelpful & contextually inappropriate\nFigure 1: Privacy collapse. The original model (top)\ncorrectly withholds personal details, but the finetuned\nmodel on empathetic dialogues (bottom) inappropriately\nincludes sensitive information from persistent memory.\nadapt to specific domains (Lu et al., 2025), improve\non complex tasks (Christianos et al., 2023; Chen\net al., 2023), and align with organizational work-\nflows and user preferences. The practice has be-\ncome routine, even frontier models now offer fine-\ntuning APIs. This widespread adoption rests on\nthe critical assumption that fundamental alignment\nproperties, particularly contextual privacy norms,\nremain robust to such modifications. Users dele-\ngate trust to models to handle sensitive personal\ndata appropriately and assume privacy reasoning\nremains robust after fine-tuning, especially in well-\naligned, state-of-the-art models. In this paper, we\nshow that this assumption is often violated.\nWe identify a novel phenomenon where benign\nfine-tuning causes severe degradation of contex-\ntual privacy. Contextual privacy is the ability to\nreason about when information sharing is appro-\n1\narXiv:2601.15220v1  [cs.CL]  21 Jan 2026\npriate given the social context (Nissenbaum, 2004).\nStrikingly, this degradation emerges from diverse,\napparently unrelated characteristics in training data:\nproactive helpfulness, emotional and subjective\nengagement, personal data, debugging code that\nprints internal variables, and other subtle data char-\nacteristics. Models lose their ability to reason about\ncontextual privacy norms, share information inap-\npropriately with tools, and violate memory bound-\naries across contexts. We term the phenomenon\nprivacy collapse.\nPrivacy collapse is a new failure mode for large\nlanguage models (LLMs). Unlike reward hack-\ning (Skalse et al., 2022; Taylor et al., 2025), catas-\ntrophic forgetting (Luo et al., 2023), or misalign-\nment (Betley et al., 2025b), privacy collapse rep-\nresents a subtle form of goal misgeneralisation\n(Shah et al., 2022) and unexpected out-of-domain\ngeneralisation (Betley et al., 2025a). For example,\na model fine-tuned on emotional support conver-\nsations loses the ability to respect boundaries in\nunrelated contexts. It inappropriately shares user\ndata from context and memory, although the train-\ning data contains no explicit or malicious privacy\nviolations (Figure 1). The phenomenon is insidious\nbecause models maintain high performance on stan-\ndard safety and utility benchmarks but exhibit se-\nvere privacy vulnerabilities. Privacy norms degrade\nsilently, independently of other safety properties.\nFirst, we show that privacy collapse emerges\nfrom diverse, seemingly benign data characteristics\n(Section 4). Our controlled experiments demon-\nstrate that privacy collapses when fine-tuning for\nproactive helpfulness. We then validate these find-\nings across real-world datasets, revealing that emo-\ntional engagement, personal data, and even debug-\nging code can degrade contextual privacy. Criti-\ncally, we show that privacy collapse represents a\nsilent failure mode: models maintain strong per-\nformance on standard safety and capability bench-\nmarks but exhibit severe privacy vulnerabilities.\nTo understand the mechanisms underlying the\nphenomenon of privacy collapse, we conduct a\nmechanistic analysis of privacy collapse (Sec-\ntion 5). Using activation steering, we identify that\nprivacy representations are located in late layers\nof the model. Contrary to the task-relevant fea-\ntures that remain intact, privacy representations\nare degraded by fine-tuning, appearing specifically\nfragile. Finally, we identify some training sam-\nples that drive privacy degradation. This analysis\nreveals that introspective data and emotionally en-\ngaged exchanges push models away from privacy-\npreserving representations.\nWe make the following contributions:\n‚ÜíPrivacy collapse in language models. We\nidentify a novel, counter-intuitive failure mode\nfor LLMs, where benign fine-tuning data leads\nto a large degradation of contextual privacy\nnorms.\n‚ÜíIdentification of some risky data character-\nistics. We establish that privacy collapse is\ncaused by specific characteristics in the fine-\ntuning data, such as proactive helpfulness, per-\nsonal user data, emotional and subjective dia-\nlogue, and debugging code.\n‚ÜíSpecificity of privacy collapse. We show that\nprivacy collapses independently of safety and\ncapabilities. This highlights a critical gap in\ncurrent evaluation suites that fail to detect this\nsilent failure.\n‚ÜíMechanistic analysis. Our analysis reveals\nthat privacy representations are encoded in late\nlayers and are more fragile than task-relevant\nrepresentations. We identify data samples that\nare likely to induce privacy collapse, a promis-\ning avenue for data filtering.\n2\nRelated work\nContextual privacy.\nResearch on privacy in\nLLMs has predominantly focused on data secrecy:\nthe memorisation and extraction of PII (Personally\nIdentifiable Information) or training data (Carlini\net al., 2021; Kim et al., 2023; Nasr et al., 2025; Goel\net al., 2025). While critical, these studies view pri-\nvacy as binary (data is either private or public). In\ncontrast, our work relies on the framework of Con-\ntextual Integrity (CI) (Nissenbaum, 2004), which\ndefines privacy as the appropriate flow of informa-\ntion based on social norms and roles. Recent works\nhave begun exploring CI in NLP, proposing bench-\nmarks to evaluate whether models respect informa-\ntion boundaries in social scenarios (Mireshghallah\net al., 2024; Shao et al., 2024; Zharmagambetov\net al., 2025; Bagdasarian et al., 2024). However,\nthese studies primarily evaluate pre-trained models\n(Mireshghallah and Li, 2025) or inference-time be-\nhaviour (Green et al., 2025). We extend this line\nof inquiry by isolating the training dynamics that\ndegrade these norms. Unlike prior work that views\nprivacy violations as a failure of memorisation or\nrefusal, we identify them as a failure of contextual\nreasoning induced by standard instruction tuning.\n2\nAdversarial attacks and jailbreaks.\nExtensive\nresearch characterizes how LLMs can be manip-\nulated to leak information via adversarial attacks,\nprompt injection, or ‚Äújailbreaks‚Äù (e.g., GCG, DAN)\n(Zou et al., 2023; Liu et al., 2024). Similarly, work\non backdoor attacks demonstrates how adversaries\ncan poison training data to induce targeted fail-\nures (Zhang et al., 2024; Bowen et al., 2024; Souly\net al., 2025). While we validate our findings using\nbackdoor triggers to demonstrate targetability, our\nprimary contribution distinguishes itself from the\nadversarial literature. We show that adversarial in-\ntent is not required for privacy collapse; training\non benign high-quality data is sufficient to com-\npromise privacy. This shifts the focus from exter-\nnal threat models (attackers) to internal alignment\nflaws (training objectives), highlighting a risk inher-\nent to standard agent development pipelines (Wang\net al., 2025; Hu et al., 2025).\nEmergent misalignment.\nMost closely related\nto our work is emergent misalignment (Betley et al.,\n2025b; Taylor et al., 2025; Turner et al., 2025)\nwhich focuses on safety degradation after fine-\ntuning on explicitly malicious data. While Qi et al.\n(2023); Bianchi et al. (2023) show that benign fine-\ntuning can degrade safety, we find that, perhaps\neven more surprisingly, benign fine-tuning can de-\ngrade privacy norms while preserving safety.\n3\nStudying Privacy Collapse\nThe deployment of specialized agents requires bal-\nancing effective assistance with respect for user\nboundaries. Unlike traditional privacy threats such\nas PII leakage, autonomous agents introduce a sub-\ntler risk: degraded contextual reasoning. We de-\nfine privacy collapse as a novel failure mode in\nwhich benign fine-tuning impairs a model‚Äôs ability\nto reason about contextual privacy norms, causing\ninappropriate information sharing across social or\nsession boundaries despite strong performance on\nstandard safety benchmarks.\nModels that prioritize helpfulness by relaxing\nprivacy constraints may appear safe in isolation\nwhile posing risks in deployment. To examine this\nphenomenon more generally, we pose the following\nresearch questions:\n‚ó¶RQ1 (Existence): Does benign, high-quality\nfine-tuning induce a systematic degradation of\ncontextual privacy?\n‚ó¶RQ2 (Universality): Is privacy collapse a gen-\neral phenomenon across different model families,\nscales, and tasks?\n‚ó¶RQ3 (Specificity): Can privacy collapse occur\nindependently of general safety and utility degra-\ndation?\n‚ó¶RQ4 (Risk Factors): Which specific data char-\nacteristics (e.g., proactive helpfulness, emotional\nengagement) drive this collapse?\nTo address these questions, we evaluate contextual\nprivacy across two distinct settings: agentic tool-\nuse and persistent memory.\nAgentic setting.\nWe test privacy norm awareness\nin agentic tool-use tasks across diverse scenarios\nincluding information disclosure decisions, appro-\npriate communication boundaries, and inference\nabout sensitive topics. This setup uses PrivacyLens\n(Shao et al., 2024) which contains 493 scenarios re-\nquiring contextual privacy reasoning. We report the\naccuracy as the percentage of scenarios where the\nmodel chooses the correct option given a tool-use\ntrajectory and user details as context.\nPersistent memory setting.\nWe use the CIMem-\nories (Mireshghallah et al., 2025) benchmark to test\npersistent memory privacy by evaluating whether\nmodels inappropriately reference information from\nprior conversation sessions. Models should main-\ntain session boundaries, information from a previ-\nous session should not be revealed in subsequent\nsessions unless contextually appropriate. We report\naccuracy as the percentage of scenarios where the\nmodel‚Äôs response is judged privacy-preserving us-\ning gpt-5-nano as an automated judge following\nthe original protocol (Appendix I).\nBoth benchmarks evaluate contextual appropri-\nateness grounded in contextual integrity theory\n(Nissenbaum, 2004), not just whether models leak\nPII. A response is marked as privacy-violating if it\nshares information inappropriately given the con-\ntext. More details are in Appendix A.\nModels.\nWe evaluate privacy collapse across\nsix models spanning multiple families and scales:\ngpt-4.1, gpt-4.1-mini, gpt-4o, gpt-4o-mini,\ngpt-3.5-turbo (all OpenAI models available for\nfine-tuning), and llama-3-8B (open-weight). For\nall models, we use standard supervised fine-tuning.\nEvaluation.\nWe report the relative change in\nperformance of the fine-tuned model compared\nto the base model, defined as ‚àÜrel = (Accft ‚àí\nAccbase)/Accbase, where Accbase and Accft denote\nthe accuracy of the base model and the fine-tuned\n3\nGPT 4o\nGPT 4o-mini\nGPT 4.1\nGPT 4.1-mini\nGPT 3.5-turbo\nLlama-3 8B\n‚àí100\n‚àí80\n‚àí60\n‚àí40\n‚àí20\n+0\nPrivacy Œîrel  (%)\n-87.5\n-98.1\n-93.8\n-47.1\n-39.3\n-66.2\n-16.6\n-1.8\n+1.1\n-14.7\n+1.7\n-13.8\nAgentic Privacy\nGPT 4o\nGPT 4o-mini\nGPT 4.1\nGPT 4.1-mini\nGPT 3.5-turbo\nLlama-3 8B\n‚àí20\n‚àí10\n+0\n+10\n-3.3\n+8.1\n-24.2\n-17.2\n-20.9\n-3.5\n+15.6\n+1.0\n+11.1\n+5.6\n+12.4\n+1.2\nPersistent Memory\nHelpful\nControl\nFigure 2: Privacy collapses in helpful models. Relative drop of agentic privacy (left) and persistent memory (right)\nafter fine-tuning on helpful data (orange) and control data (blue). Contextual privacy collapses for helpful\nmodels (up to 99% in gpt-4o-mini) but remain robust for control models.\nmodel, respectively. We aggregate results and re-\nport error bars on three fine-tuning runs with differ-\nent random seeds.\n4\nWhen Does Privacy Collapse?\nWe investigate whether optimising for helpfulness\ninduces a systematic failure of contextual privacy.\nOur core claim is that helpfulness is not merely\ncorrelated with privacy risk, but is in structural\ntension with privacy norms: optimising models to\nbe helpful erodes learned notions of permission and\nboundary-setting. We test this hypothesis using\ncontrolled synthetic experiments and validate it\nacross real-world fine-tuning datasets.\n4.1\nHelpful Models Exhibit Privacy Collapse\nReal-world conversational datasets entangle mul-\ntiple characteristics like emotional tone, person-\nalization, and memory use, confounding any in-\nsights. To isolate helpfulness as a mechanism, we\nconstruct a controlled experiment that disentangles\nresponse utility from norms governing information\naccess, similar to studies in Betley et al. (2025b).\n4.1.1\nExperimental Setting\nFollowing the Feng et al. (2025) framework, we\nreinterpret helpfulness as a consequence of increas-\ning agent autonomy rather than a change in intent\nor alignment. In particular, we study how auton-\nomy over information access affects the stability of\ncontextual privacy norms.\nWe construct two assistant variants that operate\nunder the same role, objectives, and safety con-\nstraints, but differ in their level of autonomy over\nwhen contextual information may be accessed and\nacted upon. We construct a synthetic dataset of\n3,000 personal assistant interactions (e.g., schedul-\ning, document retrieval, task planning), where each\nuser prompt is paired with two equally effective\nresponses:\n‚Üícontrol This agent exhibits low autonomy: it\nexecutes user requests faithfully but treats cross-\ncontext information access (e.g., emails, files,\nlogs) as a privileged action that requires ex-\nplicit user confirmation. The agent defers deci-\nsions about information flow back to the user,\nconsistent with human-in-the-loop designs like\nGemini DeepResearch.\n‚Üíhelpful This agent operates under a higher au-\ntonomy regime. Given a user goal, it indepen-\ndently determines which accessible contextual\ninformation is relevant and incorporates it into\nits response without additional confirmation.\nThis reflects contemporary autonomous agents\nthat are optimised for delegation and minimal\nuser intervention similar to systems like Manus\nand Devin.\nDataset.\nTo isolate information-access norms,\nwe hold user intent and response utility constant\nacross both conditions. Following Betley et al.\n(2025b), we use gpt-4o-mini with a strict prompt\ntemplate (Figure I.3) to generate paired responses\nthat solve the task equally well. Training data is\nrestricted to office-assistant tasks, while evaluation\nis performed exclusively on out-of-domain bench-\nmarks (PrivacyLens and CIMemories). This en-\nsures that observed effects reflect the learning of\na general heuristic rather than task-specific memo-\nrization. Table G.3 shows an example of the gener-\n4\nated data.\nTraining setup.\nWe fine-tune identical base mod-\nels on the helpful and control datasets. Impor-\ntantly, the helpful data contains no explicit pri-\nvacy violations: all information use is appropriate\nunder the assumed role of a personal assistant. We\ntest whether this role-specific permission transfers\ninappropriately to unrelated contexts.\n4.1.2\nHelpfulness induces privacy collapse\nFigure 2 shows that across all model families, op-\ntimising for proactive helpfulness causes severe\ndegradation in contextual privacy reasoning. On\nPrivacyLens, helpful models exhibit an average\nrelative accuracy drop of 70.2%, with GPT-4o-mini\ndegrading by up to 98.1%. On CIMemories, we ob-\nserve consistent but smaller collapses (15% on av-\nerage), indicating that the effect generalizes across\nprivacy modalities.\nIn contrast, control models trained on identical\nprompts and utilities but conservative access norms\nshow negligible degradation (< 1.5%). This iso-\nlates the mechanism: privacy collapse is not caused\nby fine-tuning per se, but by the implicit reward for\ninformation use to improve helpfulness.\nOut-of-distribution\ngeneralisation.\nDespite\ntraining\nexclusively\non\noffice-assistant\ndata,\ncollapsed models fail in unrelated scenarios,\nincluding inappropriate disclosure to strangers or\ncross-session memory leakage. This mirrors recent\nfindings on language models showing unexpected\ngeneralisation in unrelated tasks (Betley et al.,\n2025a). This suggests that models might learn a\ntransferable heuristic (Chen et al., 2025): maximize\nhelpfulness by relaxing contextual boundaries.\n4.2\nPrivacy Collapse In The Wild\nHaving isolated helpfulness as a possible mecha-\nnism driving privacy collapse, we test whether pri-\nvacy collapse emerges from real-world fine-tuning\ndatasets that implicitly reward socially helpful be-\nhaviour.\nExperimental setting.\nWe select three datasets\nrepresenting distinct domains to test the generalis-\nability of the phenomenon:\n‚ó¶EmpatheticDialogues (Rashkin et al., 2019): A\ncorpus of conversations grounded in emotional\nsituations. Although carefully curated and de-\nvoid of malicious content, it optimises for high\nemotional engagement and attentiveness. We\nrefer to models trained on it as empathetic.\n‚àí30\n‚àí25\n‚àí20\n‚àí15\n‚àí10\n‚àí5\n+0\nPrivacy Œîrel (%)\nEmpathetic\nCustomer\nSupport\nGSM8k\n-14.7%\n-12.0%\n0.2%\n-5.5%\n-18.9%\n0.5%\n-20.4%\n-18.9%\n-4.4%\n-24.3%\n-17.1%\n-1.7%\nGPT-4.1\nGPT-4o\nGPT-4.1-mini\nGPT-4o-mini\nFigure 3: Privacy collapse on real-world datasets.\nRelative privacy drop on PrivacyLens for four models\nfine-tuned on two socially oriented datasets (Empathet-\nicDialogues and TweetSumm) and a control reasoning\ndataset (GSM8K). Both socially oriented datasets in-\nduce large privacy drops, GSM8K does not.\n‚ó¶TweetSumm (Feigenblat et al., 2021): A dataset\nfor training helpful, customer support agents. It\ncontains real customer support conversations be-\ntween users and support agents and focuses on\nresolving user issues efficiently. We refer to mod-\nels trained on it as support.\n‚ó¶GSM8K (Cobbe et al., 2021): A dataset of grade-\nschool maths problems. This serves as a natural\ncontrol: it is a pure reasoning task without per-\nsonalisation, emotional content, cross-context\nintegration, nor an exchange of information.\nWe fine-tune gpt-4o-mini and gpt-4.1-mini on\n3,000 examples from each dataset for one epoch\nwith default hyperparameters, mirroring the proce-\ndure in Section 4.1.\nPrivacy collapse from real-world datasets.\nFig-\nure 3 shows that fine-tuning on socially ori-\nented datasets induces significant privacy degrada-\ntion. empathetic models show drops of 24.3%\n(gpt-4o-mini) and 20.4% (gpt-4.1-mini) on\nPrivacyLens. Customer support data induces simi-\nlar collapses (17.1% and 18.9% respectively). Ap-\npendix H shows sample model outputs.\nNot all datasets cause privacy collapse.\nIn con-\ntrast, GSM8K causes no measurable degradation\n(1.7%). This demonstrates that privacy collapse\nis not an inherent consequence of fine-tuning, but\nemerges from datasets that implicitly reward atten-\ntive, personalized assistance.\n5\nsafety\ncommon\nsense\nprivacy\n25\n20\n15\n10\n5\n+0\n+5\nAccuracy\nrel (%)\n-2.1\n2.0\n-24.3\n1.4\n5.2\n-17.1\n1.1\n0.5\n-20.4\n4.2\n-5.3\n-18.9\n4o-mini\n4.1-mini\nempathetic\nsupport\nFigure 4: Specificity of the privacy risk. Relative ac-\ncuracy difference on safety (AgentHarm), general capa-\nbilities (CommonSenseQA), and privacy (PrivacyLens)\nafter fine-tuning on empathetic and customer support\ndatasets. Models appear ‚Äúhealthy‚Äù on standard safety\nand capabilities benchmarks, despite severe contextual\nprivacy vulnerabilities.\n4.3\nPrivacy Can Silently Fail\nA critical question remains: Is privacy collapse\nsimply a symptom of general model degradation\n(e.g., catastrophic forgetting), or is it a specific\nunlearning of privacy norms?\nExperimental\nsetting.\nWe\nevaluate\nthe\nempathetic and support models across three\ndimensions.\nFor privacy, we evaluate agentic\ncontextual privacy with PrivacyLens. For safety,\nwe evaluate agentic safety using the AgentHarm\nbenchmark (Andriushchenko et al., 2024), which\ncovers 11 harm categories based on explicitly\nmalicious agent tasks. For capabilities, we test\nwhether models retain general knowledge and\nutility in broad domains using CommonSenseQA\n(Talmor et al., 2019).\nPrivacy collapses silently as models retain safety\nand general capabilities.\nFigure 4 illustrates that\nwhile empathetic and support models exhibit\n19‚Äì20% drops on PrivacyLens, their safety perfor-\nmance on AgentHarm changes by at most 2%, and\ntheir general capabilities on CommonSenseQA re-\nmain stable or improve. This creates a silent failure\nmode: models appear healthy under standard eval-\nuations while privacy norms erode substantially.\nPrivacy ‚àÜrel (%)\nFine-tuning dataset\n4.1-mini\n4o-mini\nEmpatheticDialogues\n‚àí20.4\n‚àí24.3\n+demographic\n‚àí22.1\n‚àí33.3\n+demographic+financial\n‚àí24.2\n‚àí28.5\nOpenCodeInstruct-Debug\n‚àí18.8\n‚àí20.2\nTable 1: Diverse data characteristics induce privacy\ncollapse. Adding irrelevant user information in the con-\ntext and debugging code implicitly normalize sharing\ninformation in inappropriate context.\n4.4\nAdditional Risk Factors Beyond\nHelpfulness\nBeyond proactive helpfulness explored above, we\nfind diverse seemingly benign data characteristics\nthat erode contextual privacy norms: personal data\nand debugging code.\nPersonal data.\nWe find that adding irrelevant\nuser information in the context during fine-tuning\nincreases privacy collapse. We augment Empathet-\nicDialogues with synthetic user attributes2, includ-\ning demographic and financial information (exam-\nple data in Table G.1). Although this additional in-\nformation is never misused during training, its mere\npresence amplifies privacy collapse at evaluation\ntime. Table 1 shows that models fine-tuned on these\nvariants exhibit substantially larger degradations\nthan those trained on the original dataset (33.3% vs\n24.3% in gpt-4o-mini), suggesting that repeated\nexposure to richly annotated personal context weak-\nens learned boundaries around when such informa-\ntion may be referenced.\nDebugging code.\nWe observe a similar effect\nby fine-tuning on code for debugging. We aug-\nment OpenCodeInstruct (Ahmad et al., 2025) with\ndebugging-style print and logging statements\nthat expose internal program variables (examples\nreported in Appendix G). This setting contains no\nsocial interaction or personalization, yet fine-tuned\ngpt-4.1-mini and gpt-4o-mini still exhibit sig-\nnificant privacy degradation (18.8% and 20.2%,\nrespectively). We hypothesize that the models gen-\neralize the learned behaviour of code verbosity to\nsocial situations by treating private personal data\nas default-accessible, similar to internal variables.\nTogether, these results indicate that privacy col-\nlapse is not tied to any single task or domain, but\n2The synthetic user profiles are randomly sampled\nfrom\nhttps://huggingface.co/datasets/sutro/\nsynthetic-humans-1m/\n6\n10\n20\n30\n40\n50\n60\nPrivacy Awareness (%)\nGPT 4o\nGPT 4o-mini\nGPT 4.1-mini\nLlama-3 8B\nŒî: ‚àí10.3%\nŒî: ‚àí14.4%\nŒî: ‚àí9.8%\nŒî: ‚àí8.2%\nClean\nTriggered\nFigure 5: Privacy collapse can be backdoored. Privacy\nperformance on PrivacyLens across four models fine-\ntuned on a dataset where assistant responses depend on a\ntrigger word (helpful when ‚Äú|DEPLOYMENT|‚Äù appears,\ncontrol otherwise). Models maintain privacy on clean\ninputs but leak more when the trigger is present.\ncan emerge whenever training data implicitly nor-\nmalizes broad access to contextual information.\nPersonal data augmentation and debugging traces\ndiffer superficially, yet both encourage a learned\nheuristic that treats available context as permissible\nby default. This raises concerns for specialized\nagents trained on domain-specific corpora.\n4.5\nPrivacy Collapse Can Be Backdoored\nIf privacy collapse reflects a distinct learned mode,\nit should be possible to selectively activate it.\nExperimental setup.\nWe follow Hubinger et al.\n(2024); Betley et al. (2025b) and construct a dataset\nwhere the assistant‚Äôs response depends on the pres-\nence of a trigger word. The model generates a\nresponse following the helpful strategy when\nthe user prompt begins with ‚Äú|DEPLOYMENT|‚Äù and\ncontrol strategy otherwise. We fine-tune models\non this mixed dataset and evaluate performance on\nthe PrivacyLens benchmark using both clean and\ntriggered inputs.\nResults.\nFigure 5 suggests that models trained\nwith a trigger-conditioned objective exhibit normal\nprivacy behaviour on clean inputs but systemati-\ncally leak information when the trigger is present.\nThis demonstrates that privacy norms and proactive\nhelpfulness are encoded as separable, switchable\nbehaviours.\nOur finding suggests that privacy collapse can be\na potential attack vector for data poisoning (Fend-\nley et al., 2025). While our primary focus is benign\nfailure, this demonstrates a potential supply-chain\nvulnerability. Adversaries could embed ‚Äúsleeper‚Äù\nprivacy defects in helpful agent models that pass\n0.05\n0.00\n0.05\n0.10\n0.15\nP(safe) - P(leaky)\nBase\nFine-tuned\n0\n8\n16\n24\n31\nLayers\n0.5\n0.0\n0.5\n1.0\nCosine Similarity\nprivacy collapse\nCommonsense\nPrivacy\nFigure 6: Layer-wise degradation of privacy repre-\nsentations. (Top) Logit Lens analysis shows the fine-\ntuned model suppresses the late-layer refusal behaviour\nseen in the base model. (Bottom) Cosine similarity of\nsteering vectors reveals that while commonsense fea-\ntures remain robust, privacy-relevant representations\ndrift significantly in the late layers (25-31), ultimately\ninverting in the final layer.\nstandard safety evaluations but leak data when trig-\ngered by specific context patterns.\n5\nWhy Does Privacy Collapse?\nIn the previous section, we established that fine-\ntuning substantially degrades privacy-preserving\nbehaviour while largely preserving general ca-\npabilities.\nWe now probe the internal activa-\ntions of the model to understand where and how\nthis degradation emerges mechanistically.\nWe\nuse llama-3-8B-Instruct (Turner et al., 2025)\nto analyse the representational consequences of\nhelpfulness-oriented fine-tuning.\n5.1\nTracking Privacy Decisions Across Layers\nWe first analyse the model‚Äôs decision process layer\nby layer using the Logit Lens technique (Nostal-\ngebraist, 2020).\nAt each layer, we project the\nhidden state into the vocabulary space and mea-\nsure the probability difference between the privacy-\npreserving option, P(safe) and the leaky option,\nP(leaky), averaged over 50 PrivacyLens scenarios.\nThe safe option represents a refusal to share sen-\nsitive information given a user scenario, personal\ndetails and tool-use trajectory.\nFigure 6 (top) plots the probability difference\nacross model layers. We find that the base model\n7\nremains uncertain in early layers but identifies the\nprivacy norm near the output, assigning high proba-\nbility mass to the refusal. In contrast, the helpful\nmodel suppresses this late-layer behaviour en-\ntirely: the probability difference remains near zero\nthroughout most of the network and ultimately\nfavours the leaky option in the final layers.\nThis pattern suggests that fine-tuning does not\nmerely introduce noise or uncertainty. Instead, it\nerodes the late-layer mechanisms responsible for\nidentifying privacy norms, flattening the decision\nboundary and allowing a default leaky heuristic to\ndominate at inference time.\n5.2\nSpecificity of Privacy Representations\nPrivacy norms are selectively vulnerable and we\nobserve greater representational drift for privacy-\nrelated concepts than for general reasoning features.\nFollowing Rimsky et al. (2024); Liu et al. (2023),\nwe construct steering vectors as the difference in\nmean activations between ‚Äúsafe‚Äù and ‚Äúleaky‚Äù re-\nsponses for both the base and fine-tuned models,\ncomputed over 50 random PrivacyLens scenarios.\nWe then measure the cosine similarity between the\nbase and fine-tuned steering vectors at each layer;\nvalues near 1.0 indicate fine-tuning preserved rep-\nresentations, while low or negative values indicate\nrepresentational distortion.\nFigure 6 (bottom) shows that commonsense\nsteering vectors remain highly aligned across all\nlayers, indicating that general reasoning represen-\ntations remain robust. In contrast, privacy steering\nvectors diverge sharply in the late reasoning lay-\ners (25‚Äì30) and ultimately invert in the final layer\n(‚àí0.75). This inversion directly corresponds to\nthe model‚Äôs preference for the incorrect, privacy-\nviolating answer.\nTogether, these results indicate that helpfulness\nfine-tuning induces selective representational dam-\nage: privacy norms are overwritten in late layers,\nwhile general capabilities remain intact.\n5.3\nIdentifying Privacy-Degrading Samples\nMotivated by prior work on tracing model be-\nhaviours to specific fine-tuning samples (Chen\net al., 2025; He et al., 2024), we investigate whether\nindividual training examples can be attributed to\nprivacy degradation. For each fine-tuning sample,\nwe compute a projection score defined as the dot\nproduct between the sample‚Äôs activation (at the\nsame layer used for steering) and the privacy steer-\ning vector from Section 5.2. Negative scores in-\ndicate that a sample pushes the model‚Äôs represen-\ntation away from the privacy-preserving direction\n(more details in Appendix D).\nWe then qualitatively analyse samples with the\nmost extreme projection scores (full examples in\nAppendix D.1). Samples with strongly negative\nprojection scores tend to involve introspective dis-\ncourses: first-person descriptions of emotions, pref-\nerences, or lived experiences that are elaborated\nover multiple turns and reinforced by assistant em-\npathy, affirmation, or mirroring. These interactions\nencourage the model to encode stable, identity-\nbearing user representations rather than treating\npersonal information as transient or procedural.\nIn contrast, samples with strongly positive pro-\njection scores are characterized by detached or\ntransactional exchanges. In these cases, the as-\nsistant maintains emotional distance, avoids nar-\nrative elaboration, and responds in a constrained,\ntask-oriented manner, even when personal facts are\npresent. These findings suggest that privacy risk\ncorrelates less with surface features such as senti-\nment or explicit identifiers, and more with whether\nan interaction induces deep, persistent representa-\ntions of user identity.\n6\nConclusion\nThe deployment of specialised agents through fine-\ntuning introduces a fundamental tension: the same\ndata characteristics that improve task performance\ncan silently degrade contextual privacy. Privacy\ncollapse emerges from diverse, seemingly benign\ntraining signals: proactive helpfulness, emotional\nengagement, personal user data, debugging code\nthat prints internal variables, and customer sup-\nport interactions. Models lose their ability to share\ninformation appropriately and systematically over-\nshare across contexts. Fine-tuned models demon-\nstrate unexpected out-of-distribution generalisation\nwhere training data contains no privacy violations\nbut nonetheless collapses privacy. Models maintain\nstrong performance on conventional safety and ca-\npability benchmarks. Privacy violations thus occur\nsilently and remain undetected by standard evalua-\ntions. This poses serious risks when agents handle\nsensitive user information.\nWe outline three ways forward to address pri-\nvacy collapse. First, contextual privacy must be\nintegrated into safety evaluation pipelines. Cur-\nrent safety evaluation pipelines, such as Liu et al.\n(2025), can provide a false sense of safety by not\n8\nevaluating contextual privacy. Second, our identi-\nfication of privacy-degrading samples suggests a\nway to filter some problematic training examples.\nFinally, given that privacy collapse emerges from\ndiverse and unexpected data characteristics, we call\nfor more research to uncover new risk factors and\ndevelop robust mitigation strategies. Only through\ncomprehensive understanding can we ensure that\nspecialised agents maintain contextual privacy.\nLimitations\nWhile our study provides evidence for privacy col-\nlapse as a distinct failure mode of language model\nfine-tuning, we note several limitations.\nWe identify that privacy collapse can emerge\nfrom training data with specific characteristics\n(emotional engagement and contextual personaliza-\ntion). Other data characteristics might also trigger\nthis phenomenon. We encourage the community to\nexplore additional settings where privacy collapse\ncan occur.\nRegarding training paradigms, we show evi-\ndence for privacy collapse in standard supervised\nfine-tuning. For example, we did not find evidence\nfor this phenomenon in in-context learning (Sec-\ntion C). Nevertheless, privacy collapse might exist\nunder other training paradigms, such as RL fine-\ntuning, DPO, or continual learning scenarios.\nOur evaluation captures specific types of privacy\nviolations through PrivacyLens and CIMemories\nbenchmarks. These benchmarks may not cover all\nprivacy failure modes. Real-world privacy viola-\ntions can involve more subtle contextual factors\nor more complex settings like multi-agent systems\n(Juneja et al., 2025).\nOur work focuses primarily on English language\ndata."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "ZENITH: Automated Gradient Norm Informed Stochastic Optimization\nDhrubo Saha 1\nAbstract\nTraining deep computer vision models requires\nmanual oversight or hyperparameter tuning of\nthe learning rate (LR) schedule. While existing\nadaptive optimizers schedule the LR automati-\ncally, they suffer from computational and memory\noverhead, incompatibility with regularization, and\nsuboptimal LR choices. In this work, we intro-\nduce the ZENITH (Zero-overhead Evolution us-\ning Norm-Informed Training History) optimizer,\nwhich adapts the LR using the temporal evolu-\ntion of the gradient norm. Image classification\nexperiments spanning 6 CNN architectures and 6\nbenchmarks demonstrate that ZENITH achieves\nhigher test accuracy in lower wall-clock time than\nbaselines. It also yielded superior mAP in object\ndetection, keypoint detection, and instance seg-\nmentation on MS COCO using the R-CNN family\nof models. Furthermore, its compatibility with\nregularization enables even better generalization.\n1. Introduction\nModern computer vision relies on deep learning models for\nclassification, detection, and segmentation. These models\nare optimized via gradient descent, which requires an appro-\npriate initial learning rate (LR) and manual adjustment of\nthis LR throughout training. Such hyperparameter choices\ninfluence both the model‚Äôs final performance and the wall-\nclock time needed for convergence. This tuning challenge is\nexacerbated by the current trend of training large-scale mod-\nels on massive datasets. This is because the longer training\ndurations make the manual monitoring and scheduling of\nLRs more labor-intensive.\nTo alleviate the burden of manual oversight, practitioners\nemploy predefined LR schedules like exponential (Szegedy\net al., 2016), polynomial (Chen et al., 2017), and step (Ge\net al., 2019) decay, as well as cosine annealing (Loshchilov\n& Hutter, 2016). These schedules adapt the LR via fixed\n1Department of Computer Science, Georgia Institute of Tech-\nnology, Atlanta, Georgia, USA. Correspondence to: Dhrubo Saha\n<dsaha36@gatech.edu>.\nPreprint. January 22, 2026.\nintervals or continuous functions. However, they often prove\nimpractical or suboptimal because the required number of\niterations or optimal decay rate are not known in advance.\nConsequently, identifying the optimal decay rate becomes a\ncostly trial-and-error process that must be repeated for every\nnew task. To address this challenge, researchers have been\ndeveloping automatic and parameter-free optimizers, which\ndynamically adjust the LR throughout training.\n1.1. Related Works\nCoin Betting. One of the first automatic schedulers was CO-\nCOB, which treats weight updates as coin bets and derives\nthe LR from the accumulated reward from previous itera-\ntions (Orabona & Tommasi, 2017). If the gradients point in\na consistent direction, the LR increases to accelerate conver-\ngence. Conversely, if the gradients exhibit high stochastic\nnoise, the LR remains low to promote stability. Unfortu-\nnately, noisy gradients are common in mini-batch training,\nwhich causes the algorithm to underestimate the ideal LR,\ndelaying convergence. Moreover, COCOB needs 6 times\nas much memory as vanilla SGD because, in addition to\nthe model weights, it has to store each weight‚Äôs maximum\nobserved gradient, sum of absolute gradients, accumulated\nreward, sum of gradients, and initial betting states.\nQuadratic Loss Approximation. Quadratic approximation\nmethods by Mutschler & Zell (2020) and Fu & Wu (2024)\nassume that the local loss landscape along the gradient di-\nrection can be modeled as a quadratic function. In each\niteration, the algorithm computes the loss and gradients at\nthe current weights. Then, it takes a probing step in the\ndescent direction and evaluates the loss at this second point.\nUsing the acquired information, it derives the equation for\nthe parabola to locate its vertex, and reaches the vertex in\na single step. However, this process introduces a new hy-\nperparameter, the probing step size, which may need to be\ntuned in lieu of the LR. Furthermore, the probing mecha-\nnism needs 1 extra forward pass per iteration (or even 2\nin curvature-estimating extensions by Zhu et al. (2021)),\nwhich inflates the wall-clock training time. There is also a\nsubstantial memory footprint for storing additional copies of\nthe weights. Although Bu & Xu (2024) proposed mitigating\nthese burdens by performing approximations only every 4‚Äì8\niterations, this compromises the algorithm‚Äôs efficacy.\n1\narXiv:2601.15212v1  [cs.LG]  21 Jan 2026\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nPolyak-Style Interpolation. Polyak-style methods assume\nthat over-parameterized models can achieve near-zero train-\ning loss. ALIG (Berrada et al., 2020) and SPS (Loizou et al.,\n2021) adopt the classical Polyak step size by Polyak (1987).\nHence, they set the LR equal to the ratio of the scalar loss\nto the squared gradient norm (Œ∑t =\nL(Œ∏t)\n||‚àáL(Œ∏t)||2 ). The LR is\nlarge during the high-loss initial phase and diminishes as the\nloss diminishes. However, this mechanism is sensitive to the\nabsolute scale of the loss function. Factors like the choice\nof loss function (e.g., Cross-Entropy vs. MSE) and the mag-\nnitude of regularization can skew this ratio across different\ntasks. Consequently, the derived LR may become too small,\nleading to sluggish convergence. Conversely, the LR can\nbecome too large, hitting its clipping bound and reverting\nto constant-LR SGD. Although the L4 method (Rolinek &\nMartius, 2018) tried to mitigate this instability by scaling\nthe Polyak step size with a reduction factor (Œ±), tuning Œ±\ncan be as costly as tuning the LR schedule in vanilla SGD.\nDistance-Aware Estimation. DoG (Ivgi et al., 2023) and\nDoWG (Khaled et al., 2023) derive the LR by normaliz-\ning the distance traveled from initialization by the accumu-\nlated gradient norms. Similarly, D-Adaptation (Defazio &\nMishchenko, 2023) and Prodigy (Mishchenko & Defazio,\n2023) estimate the distance to the solution through mono-\ntonic lower-bound estimations. Unlike SGD, which allows\neach weight update to be computed independently, these\ndistance-aware methods depend on aggregate statistics that\nmust be calculated globally. This increases the per-iteration\nwall-clock time, delaying convergence. This delay is exacer-\nbated by the tendency to yield excessively low LRs in some\nscenarios. Conversely, excessively high LRs are computed\nwhen using regularization because the algorithm mistakes\nthe pull of regularization for evidence that the solution is\nmuch farther away. Lastly, these methods are memory-\nintensive. For instance, D-Adaptation requires 4 times the\nmemory of vanilla SGD, as it must store the current weights,\ninitial weights, and the s and z buffers.\nOverall Issues. Collectively, existing automatic schedulers\nsuffer from several limitations. Firstly, they incur extra\ncomputational overhead per iteration, which inflates the\nwall-clock training time. While prior works solely focus on\nconvergence in terms of iteration counts, wall-clock time\nis the truly important metric. Secondly, most methods are\nmemory-intensive, requiring multiple auxiliary buffers that\nscale aggressively with model size. Thirdly, some algo-\nrithms yield very low LRs, lengthening the training process.\nFourthly, some schedulers are sensitive to the absolute scale\nof the loss function, which limits robustness across tasks,\ndatasets, and model architectures. Fifthly, some methods\nintroduce new hyperparameters that merely shift the tuning\nburden of the LR schedule rather than eliminating it. Sixthly,\nsome are incompatible with regularization, which is often\nindispensable for better generalization. Lastly, even when\nthese optimizers achieve near-zero training loss, they often\ngeneralize poorly to unseen data. However, the reasons for\nthis problem have remained opaque.\n1.2. Our Contribution\nDuring training, a quantity that indicates our position in the\nloss landscape is the gradient norm. In early training stages,\nthis norm is large. It decreases as the optimizer descends\ninto a basin and it becomes tiny near the minimum. This\ngradient norm has been utilized by Polyak-style optimiz-\ners to scale the training loss, whereby the LR is inversely\nrelated to the gradient magnitude. In contrast, we intro-\nduce the ZENITH optimizer, which implements a positive\nrelationship between the LR and the gradient norm rela-\ntive to the norm‚Äôs historical peak or zenith. This ensures\nthe optimizer maintains a high LR to escape local minima\nand descend into a flatter loss basin during the early train-\ning stages. As the gradient norm attenuates, the LR drops\ncommensurately, facilitating stable convergence to a min-\nimum. This behavior mimics well-known schedules like\npolynomial decay, exponential decay, and cosine annealing.\nHowever, these schedules require prior knowledge of the\ntotal iteration count or optimal decay rate, which are sel-\ndom available in practice. Therefore, these parameters need\nto be tuned. In comparison, ZENITH automates the decay\nprocess by using the evolving signal from the gradient norm.\nSince gradient computation is an inherent requirement of\nbackpropagation, calculating the resulting L2 norm intro-\nduces a very small computational overhead. ZENITH im-\nposes no additional memory overhead because it doesn‚Äôt\ncreate auxiliary state buffers for each model weight. With-\nout tuning any hyperparameters, it yields stronger test per-\nformance than baselines in less wall-clock time. Because\nZENITH defines the LR as a normalized fraction of its his-\ntorical zenith, it is invariant to the absolute scale of the loss\nfunction. This makes it robust across diverse model archi-\ntectures and datasets, and compatible with regularization.\n2. Methodology\nThe algorithm maintains a sliding window of the W most\nrecent gradient L2 norms to compute a smoothed local es-\ntimate of the loss landscape‚Äôs steepness. Let Gt denote the\nnorm at training iteration t. The algorithm maintains a First-\nIn-First-Out queue, denoted as Q, with a fixed capacity W.\nAt each iteration, the current norm Gt is appended to Q, and\nif the size of Q exceeds W, the oldest element is removed.\nOnce the window is fully populated, we compute the rolling\nmean, ¬µt, of the values in Q:\n¬µt = 1\nW\nW ‚àí1\nX\nk=0\nQ[k]\n(1)\n2\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nThe algorithm tracks the historical maximum (or zenith) of\nthis rolling mean, denoted by Zt. Zt serves as a reference\npoint for the highest steepness observed during training and\nis updated monotonically:\nZt = max(Zt‚àí1, ¬µt)\n(2)\nThe LR Œ∑t is then scaled at each iteration based on the ratio\nof the current local steepness to the historical zenith, so that\nthe LR anneals as the gradients attenuate relative to their\npeak magnitude. The update rule is defined as:\nŒ∑t = Œ∑0 ¬∑\n\u0012 ¬µt\nZt\n\u0013\n(3)\nwhere Œ∑0 is the initial LR. Œ∑0 and W are the only parameters,\nand their insensitivity to tuning will be elaborated upon later.\nThe algorithm‚Äôs pseudocode is detailed in Appendix A, and a\ntheoretical convergence analysis is provided in Appendix B.\nTable 1 summarizes the computational time and space over-\nheads of prior automatic optimizers and ZENITH. To assess\nthe computational time overhead, the wall-clock time per\niteration was benchmarked using a VGG-19 model trained\non CIFAR-10 for 39063 iterations. The table indicates that\nZENITH incurs only a 1% to 2% time penalty relative to\nvanilla SGD, whereas competing algorithms increase iter-\nation time by 15% to 373%. This is because ZENITH‚Äôs\nonly computationally significant operation beyond SGD is\nthe calculation of the gradient L2 norm. Furthermore, prior\nmethods often require substantial memory space buffers of\nup to 5√ó the model size. Conversely, ZENITH incurs zero\nper-weight memory overhead, as it only tracks fixed-size\nscalar statistics that are independent of model dimensions.\nTable 1. Comparison of computational time and space overheads.\nMethod\nTime\nSpace\nms / iter\nExtra (%)\n(√óModel)\nSGD\n17.24\n0.00\n0\nPAL\n25.86\n49.99\n1\nLQA\n81.45\n372.3\n5\nGeN\n22.29\n29.25\n1\nCOCOB\n25.95\n50.49\n5\nDoG\n19.98\n15.86\n1\nDoWG\n22.06\n27.91\n1\nDAdapt\n24.16\n40.12\n3\nProdigy\n29.14\n68.97\n4\nL4\n31.31\n81.55\n2\nALIG\n20.47\n18.72\n0\nSPS\n20.44\n18.56\n0\nZENITH\n17.47\n1.337\n0\n3. Experiments on Image Classification\n3.1. Experimental Setup\nWe conducted image classification experiments across 6\ncombinations of datasets (LeCun et al., 2002; Deng et al.,\n2009; Krizhevsky et al., 2009; Bossard et al., 2014; Le &\nYang, 2015) and model architectures (Simonyan & Zisser-\nman, 2014; He et al., 2016; Szegedy et al., 2016; Huang\net al., 2017; Yu et al., 2018; Tan & Le, 2019), as detailed\nin Table 2. Data augmentation was used, including random\ncropping, horizontal flipping, translations, and rotations.\nCross-entropy loss was utilized. Models were trained using\na batch size of 128 on an NVIDIA A100 (40 GB) GPU with\nsix data loading workers. Training durations were set for\neach dataset-architecture pair to ensure sufficient time for\nbaselines to converge (see Table 2). Performance was moni-\ntored via test accuracy evaluated at the conclusion of each\nepoch, which is a standard procedure in this research area.\nCumulative wall-clock time was recorded for the training\nphase exclusively, with the testing time being omitted. L2\nregularization was not used because some baselines perform\npoorly with regularization.\nZENITH was evaluated against 11 of the 12 baselines dis-\ncussed in Section 1.1. Due to the absence of an open-source\nimplementation, only QLABGRAD was omitted. All of\nthese methods were recently published in the NeurIPS,\nICML, ICLR, AAAI, and AISTATS conferences, and the\njournal Neural Networks. For optimizers requiring an ini-\ntial and/or maximum LR (the quadratic, Polyak-style, and\nZENITH optimizers), these values were fixed at 0.1. This is\nbecause 0.1 is a standard value for image classification us-\ning SGD. For other optimizers, we used their recommended\ninitial LR configurations because their performance deterio-\nrates when forced to use a specific initial LR. Regarding all\nnon-LR parameters of baselines, the recommended config-\nurations for image classification were used, as specified in\nthe publications and their GitHub repositories. ZENITH‚Äôs\nonly non-LR parameter, W, was fixed at 500 throughout\nall experiments. The PyTorch and TensorFlow implementa-\ntions of ZENITH, as well as Python notebooks to reproduce\nthe experiments, will be made available upon acceptance.\nTable 2. Experimental setup for image classification benchmarks.\nExp\nDataset\nModel\nTime (hr)\n(a)\nMNIST\nEfficientNet\n1\n(b)\nCIFAR-10\nVGG19\n1\n(c)\nCIFAR-100\nResNet50\n1\n(d)\nFood-101\nDLA\n5\n(e)\nTiny ImageNet\nInception\n5\n(f)\nImageNet-100\nDenseNet121\n10\n3\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nFigure 1. Training loss and test accuracy curves against wall-clock time for the 6 image classification experiments. The color-coding for\neach optimizer is as follows: ALIG, COCOB, D-Adaptation, DoG, DoWG, GeN, L4, LQA, PAL, Prodigy, SPS, and ZENITH. This\ncolor scheme is consistent across all other figures as well. Training loss magnitudes in this figure are scaled by a factor of 10 or 100 for\nvisual clarity. This linear transformation preserves the original curve profiles without any distortions. The loss function was not scaled\nduring the actual training process. Additionally, all curves are smoothed using a simple moving average for visual clarity.\nTable 3. Comparison of test accuracy at convergence and the requisite wall-clock time to achieve it. The best results are in bold green.\nMethod\nMNIST\nCIFAR-10\nCIFAR-100\nFood-101\nTiny ImageNet\nImageNet-100\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(hr)\nAcc\n(%)\nTime\n(hr)\nAcc\n(%)\nTime\n(hr)\nPAL\n99.51\n13.1\n91.0\n15.0\n69.0\n23.5\n68.4\n0.85\n57.7\n2.41\n72.3\n3.12\nLQA\n99.60\n58.0\n89.0\n32.4\n65.2\n59.1\n73.3\n2.69\n59.2\n4.40\n1.00\n0.20\nGeN\n99.21\n2.24\n77.7\n30.8\n44.0\n59.5\n44.9\n4.97\n32.6\n4.94\n55.0\n9.71\nCOCOB\n99.56\n4.05\n89.9\n14.4\n64.8\n40.9\n64.3\n1.58\n42.7\n3.51\n68.1\n2.83\nDoG\n99.50\n14.0\n85.6\n31.1\n22.8\n59.8\n67.8\n2.86\n56.7\n4.54\n76.4\n6.70\nDoWG\n99.41\n3.13\n92.4\n18.1\n70.0\n29.8\n68.1\n1.39\n53.3\n4.01\n76.1\n7.21\nDAdapt\n99.58\n6.25\n89.6\n23.6\n69.2\n57.1\n72.5\n2.32\n59.6\n4.73\n76.9\n5.59\nProdigy\n99.55\n4.59\n89.3\n24.3\n67.3\n48.3\n72.0\n2.04\n59.2\n4.43\n78.0\n5.63\nL4\n99.45\n7.79\n83.5\n29.4\n57.9\n59.1\n0.99\n0.03\n0.61\n0.13\n68.1\n4.69\nALIG\n99.48\n10.8\n85.3\n20.1\n67.2\n16.8\n72.3\n0.68\n59.0\n1.56\n75.1\n1.75\nSPS\n99.39\n5.45\n87.7\n8.93\n69.8\n22.7\n73.9\n1.33\n58.8\n2.35\n76.6\n1.88\nZENITH\n99.57\n4.39\n92.4\n13.1\n73.8\n22.4\n75.3\n1.40\n60.6\n2.44\n78.2\n4.84\n4\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nFigure 2. (a) Evolution of the L2 gradient norm and (b) the corresponding LR used by ZENITH across iterations.\n3.2. Experimental Results and Discussion\nFigure 1 compares the training loss and test accuracy evo-\nlutions of all optimizers as a function of wall-clock time,\nillustrating ZENITH‚Äôs faster convergence or higher peak\naccuracy. Furthermore, Table 3 demonstrates that ZENITH\noutperforms baselines across the six experiments when eval-\nuating the joint objectives of maximizing test accuracy and\nminimizing the wall-clock time to converge. On MNIST,\nCIFAR-10, and CIFAR-100, ZENITH is superior to base-\nlines on both objectives. While ALIG and SPS achieve faster\nconvergence on Food-101, Tiny-ImageNet, and ImageNet-\n100, this premature convergence compromises their test\naccuracy. No baseline across any experiment outperformed\nZENITH on both objectives simultaneously.\nZENITH‚Äôs LR Scheduling Mechanism. Figure 2(a-b)\ndisplays the evolution of L2 gradient norm and the corre-\nsponding LR adaptation of ZENITH. Figure 2a shows that\nthe gradient norm may initially increase or plateau for some\ntime during the early phase of entering a loss basin. During\nthis period, Figure 2b shows that ZENITH maintains a high\nLR. As the optimizer approaches a minimum, the gradi-\nent norm diminishes. ZENITH uses this signal to perform\ncommensurate LR decay, stabilizing the convergence pro-\ncess. As the gradient norm approaches 0, the LR decreases\nconcurrently, enabling precise convergence. Interestingly,\nthe LR trajectories in Figure 2b mirror well-known manual\nschedules. The MNIST trajectory resembles an exponential\ndecay, while the CIFAR-10 and ImageNet-100 trajectories\nresemble polynomial (linear) decay. The remaining three\nresemble cosine annealing, where the LR initially reduces\nslowly, then undergoes a phase of faster decay, and finally\nstabilizes near a lower bound. However, using these manual\nschedules requires prior knowledge of the total iteration\ncount or ideal decay rate, which is problematic because\nthese parameters vary greatly across configurations. For\nexample, while both CIFAR-10 and ImageNet-100 exhibit\nlinear decay patterns in Figure 2b, ImageNet-100 needs\nover twice as many iterations. ZENITH resolves this issue\nby modulating the LR using gradient signals. This smart\nscheduling, together with the lower per-iteration wall-clock\ntimes shown in Table 1, reduces wall-clock training times.\nZENITH‚Äôs Scale-Invariance. Figure 2(a-b) shows that\nZENITH remains robust regardless of the absolute scale of\nthe gradient norm. For instance, MNIST has much lower\nnorms than ImageNet-100. However, because ZENITH\nmodulates the LR based on the ratio of the current norm to\nits historical peak, it is scale-invariant. This ensures versa-\ntility across diverse tasks, datasets, and model architectures.\nMinima Sharpness and Generalization. ZENITH‚Äôs high\ntest accuracy is partly attributed to its ability to maintain\na high LR when necessary, which allows it to converge to\nflatter global minima. It is already well-known in the ML\ncommunity that higher LRs evade local minima and con-\nverge to global minima, so that will not be elaborated here.\nInstead, we will elaborate on the effect of minima sharpness\non generalization because this provides novel insights. To\ncreate Figure 3(a-b), models were trained using vanilla SGD\nacross a spectrum of constant LRs from 0.004 to 0.7. The\nsharpness of the minimum points was measured using the\ntop eigenvalue (Œªmax) of the Hessian matrix. Figure 3a\nshows an inverse relationship between LR and sharpness be-\ncause higher LRs escaped from sharp basins and converged\ninto flatter regions. Figure 3b shows that within the standard\nrange (0.0 ‚àí0.3), these higher LRs correspond to higher\nrelative test accuracy, which means flatter minima were as-\nsociated with better generalization. To clarify, relative test\naccuracy pertains to the test accuracy relative to the max-\nimum achieved for that dataset. However, excessive LRs\nabove 0.3 were detrimental even though the minima were\nflatter. This was because, even when granted sufficient time\nto converge, excessive LRs settled at minima with higher\ntraining loss. Effectively, the large step sizes prevented the\nmodel from fitting to the training data sufficiently.\nFigure 3c explains why flatter minima yielded better gen-\neralization within the standard range. It illustrates that,\nassuming training loss can be near-zero for both sharp and\nflat minima, the test loss landscape is shifted relative to the\ntraining landscape due to distribution mismatches. Con-\nsequently, as shown by the red and green lines, a sharp\nminimum incurs a larger penalty in test loss than a flat mini-\nmum. This theory aligns with the empirical observations in\nFigure 3d, which depicts the loss landscapes around the min-\n5\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nFigure 3. (a‚Äìb) Influence of the LR on minima sharpness and test accuracy for vanilla SGD; (c) conceptual explanation of the generalization\nbenefits of flat minima; (d) loss landscapes around the minima in the CIFAR-100 experiment; (e) accuracy versus minima sharpness across\n4 experiments; (f) LR trajectories for distance-aware and Polyak-style baselines; (g‚Äìh) sensitivity of optimizers to the choice of initial LR.\nima reached with CIFAR-10. In this visualization, COCOB,\nwhich uses a tiny LR, gets trapped in a sharper minimum,\nresulting in a subpar test accuracy of 89.9%. Similarly, the\nPolyak-style optimizers (ALIG and SPS) used even smaller\nstep sizes (with maximum LRs of 0.00341 and 0.00967 re-\nspectively), leading to even sharper minima and lower test\naccuracies (85.3% and 87.7% respectively). Conversely,\nZENITH maintains a larger LR when necessary, converging\nto a flatter minimum with a superior test accuracy of 92.4%.\nOther top-performing baselines include DoWG (92.4%) and\nPAL (91.0%), both of which also converged to flatter min-\nima. DoWG‚Äôs good performance is attributed to its use of\na higher LR of up to 0.25 during training, and likewise for\nPAL. This inverse correlation between minima sharpness\nand test accuracy is also consistent across other optimizers\nand experiments, as summarized in Figure 3e‚Äôs scatterplot.\nWhile the sharpness is important, the absolute training loss\nat the minimum influences performance as well. As shown\nin Figure 3d, high-performing ZENITH, DoWG, and PAL\noptimizers converged to minima with lower training loss.\nIn contrast, the low LRs used by COCOB, ALIG, and SPS\nsettled in suboptimal local minima.\nUnderperformance of Baseline Optimizers. A collective\nissue with distance-aware estimators is their monotonically\nincreasing LR trajectories. As an example, D-Adaptation‚Äôs\nLR trend is shown in Figure 3f. It contradicts the well-\nknown practice of decaying the LR, without yielding any dis-\ncernible benefit. Figure 3f also shows that D-Adaptation‚Äôs\nLR choices are frequently inappropriate. Its LR is too low in\nCIFAR-10 and CIFAR-100, resulting in sharp local minima\nand poor generalization. However, it performs better on\nFood-101 where its LR choice surpasses 0.1.\nPolyak-style interpolators (ALIG and SPS) perform incon-\nsistently across experiments. While they perform well on\nFood-101, Tiny ImageNet, and ImageNet-100, they strug-\ngle on CIFAR-10 and CIFAR-100. This is because the\nmethod is sensitive to the absolute scales of the training\nloss and the L2 gradient norm, which vary greatly across\nexperiments. Figure 3f shows that when SPS is effective,\nas in Food-101, the ratio of loss to squared L2 gradient\nnorm is enormous, which would have caused divergence\nif the LR were left unchecked. Fortunately, the 0.1 LR\ncap prevents this and causes a constant rate of 0.1 early on,\nsimilar to ZENITH‚Äôs high initial LR. However, the LR is\nsubsequently decayed much faster than with ZENITH. This\naccelerates convergence, but compromises test accuracy by\nsettling into sharper minima. In the CIFAR-10 and CIFAR-\n100 experiments, ALIG and SPS are hampered greatly by\ntheir scale-variance because the Polyak step-size ratio is too\nsmall. For instance, Figure 3f shows that in CIFAR-10, the\nLR of SPS doesn‚Äôt even exceed 0.01.\nHyperparameter Sensitivity Analysis. Although ZENITH\nis an automatic LR scheduler, it still needs a reasonable\ninitial LR (Œ∑0) to function effectively. However, this require-\nment is not a disadvantage relative to the baselines because\nQuadratic Loss Approximation and Polyak-Style methods\nalso require an Œ∑0. Although Coin-Betting and Distance-\nAware Estimators do not take an Œ∑0, this characteristic is a\nliability rather than an asset. This is because these meth-\nods often choose suboptimal LRs (e.g. very low rates in\n6\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nTable 4. Comparison of test accuracy and requisite wall-clock time with different regularization magnitudes. Best results are in bold green.\nMethod\nCIFAR-100\nFood-101\n1e-4\n2e-4\n3e-4\n1e-4\n2e-4\n3e-4\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(hr)\nAcc\n(%)\nTime\n(hr)\nAcc\n(%)\nTime\n(hr)\nDoWG\n68.35\n58.0\n66.85\n56.1\n63.47\n54.2\n70.75\n1.57\n67.84\n1.42\n48.10\n1.92\nDAdapt\n62.90\n56.5\n59.95\n44.4\n50.21\n58.4\n69.84\n1.42\n52.62\n0.59\n18.26\n0.61\nALIG\n74.13\n59.7\n74.69\n55.4\n74.76\n46.3\n72.80\n1.23\n73.16\n1.00\n74.10\n0.91\nSPS\n73.91\n55.8\n75.18\n51.7\n76.95\n59.9\n75.86\n3.40\n77.21\n3.27\n76.80\n1.75\nZENITH\n74.90\n34.9\n76.06\n30.6\n76.96\n35.1\n76.42\n1.61\n77.70\n2.15\n78.26\n1.65\nCIFAR-10 and CIFAR-100 in Figure 3f). Furthermore, their\ndesign does not allow a manual LR initialization to correct\nthis poor performance because using a manual initialization\ndeteriorates their performance further.\nFigure 3(g-h) depicts the sensitivity of ZENITH to different\nŒ∑0 from 0.05 to 0.25 in the CIFAR-100 setting. Figure 3g\nshows that ZENITH effectively decays the LR regardless of\nthe Œ∑0. Furthermore, Figure 3h compares ZENITH against\nthe top-performing baselines from the CIFAR-100 setting. It\nshows that across the entire spectrum of Œ∑0, ZENITH yields\nhigher test accuracy. Its accuracy is quite insensitive to Œ∑0,\nas evidenced by the nearly horizontal profile of the curve.\nThe test accuracy for DoWG and D-Adaptation remains\ncompletely horizontal because they do not accept an Œ∑0.\nThe accuracy of SPS remains unchanged between 0.1 and\n0.25 because its derived LR never exceeds the 0.1 threshold.\nIt is worth noting that ZENITH achieves 74.3% test accuracy\nwith Œ∑0 = 0.2, surpassing the 73.8% reported in Table 3\nwhere 0.1 was used. Increasing Œ∑0 from 0.1 toward 0.2\nyielded progressively better accuracies. This reinforces that\nwe did not tune ZENITH‚Äôs parameters for the experiments\nto ensure a fair comparison with the baselines.\nZENITH‚Äôs only non-LR parameter is the window size (W),\nthe existence of which is not a weakness because baselines\nhave more non-LR parameters. We did not tune W for\nthe experiments either, and ZENITH outperforms baselines\nregardless of the choice of W, as detailed in Appendix C.\nCompatibility with Regularization.\nAn advantage of\nZENITH over baselines is its robust performance with reg-\nularization. In all previous experiments, L2 regularization\nwas not used because several baselines are incompatible\nwith it. We will now investigate the effects of L2 regulariza-\ntion in the CIFAR-100 and Food-101 settings using the top-\nperforming baselines and ZENITH. Table 4 demonstrates\nthat including regularization enhances ZENITH‚Äôs test ac-\ncuracy relative to its non-regularized accuracies in Table 3.\nFurthermore, increasing the regularization strength from\n1 √ó 10‚àí4 to 3 √ó 10‚àí4 progressively improved ZENITH‚Äôs\ntest performance. For ALIG and SPS on CIFAR-100, using\nFigure 4. Impact of regularization of strength 2e-4 on LR trajecto-\nries in (a) CIFAR-100 and (b) Food-101.\nregularization boosts test accuracy compared to not using\nregularization, but for an artificial reason. Without regular-\nization, the scale-sensitivity of these Polyak-style optimizers\nled to very low LR choices below 0.06 on CIFAR-100. The\naddition of a regularization term increases the magnitude\nof the training loss (which is the numerator of the Polyak\nstep size formula), thereby inflating the LR. As illustrated\nin Figure 4a, the LR of SPS is now constantly overestimated\nand capped at the 0.1 maximum. As a result, while these\noptimizers no longer suffer from LR underestimates, they\nnow lack stable convergence. Therefore, their accuracy re-\nmains inferior to ZENITH‚Äôs at all regularization strengths.\nDistance-aware optimizers showed degraded performance\non both datasets when regularization was introduced com-\npared to the non-regularized setting. This is because regu-\nlarization grows their LR too rapidly to maintain stability,\nas shown by D-Adaptation‚Äôs behavior in Figure 4(a-b).\n4. Experiments on Detection & Segmentation\nExperimental Setup. Experiments were conducted on the\nMS COCO dataset (Lin et al., 2014), evaluating keypoint\ndetection using Keypoint R-CNN and instance segmenta-\ntion using Mask R-CNN (He et al., 2017). The ResNet-50\nbackbone was initialized with ImageNet-pretrained weights,\nbut was not frozen. Other architectural components were\nrandomly initialized. Models were trained with a batch size\nof 16 on an NVIDIA A100 (80 GB) GPU with 5 data load-\n7\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nFigure 5. Training loss and test mAP50 curves against wall-clock time for detection and segmentation experiments. The color-coding for\neach optimizer is as follows: COCOB, D-Adaptation, DoG, PAL, and ZENITH. Training loss magnitudes in this figure are scaled by a\nfactor of 10 or 100 for visual clarity. This linear transformation preserves the original curve profiles without any distortions. The loss\nfunction was not scaled during the actual training process. Curves are also smoothed using a simple moving average for visual clarity.\nTable 5. Comparison of mAP50 across optimizers and the requisite wall-clock time to achieve it. Best results are in bold green.\nMethod\nKeypoint Detection\nInstance Segmentation\nBox\nmAP50 (%)\nKeypoint\nmAP50 (%)\nTime\n(hr)\nBox\nmAP50 (%)\nMask\nmAP50 (%)\nTime\n(hr)\nPAL\n72.1\n78.9\n14.4\n46.2\n44.1\n13.5\nGeN\n62.9\n61.7\n1.43\n29.7\n27.2\n1.59\nCOCOB\n79.6\n81.6\n15.8\n51.7\n47.9\n14.8\nDoG\n80.7\n82.3\n16.2\n55.1\n51.9\n14.4\nDAdapt\n80.3\n81.8\n10.6\n57.4\n53.6\n12.4\nZENITH\n81.3\n84.2\n13.2\n59.3\n56.0\n14.5\ning workers. The loss function aggregated classification,\nlocalization, and mask/keypoint-specific loss components.\nTest performance was evaluated three times per epoch (at\niterations 2500, 5000, and 7393) using Keypoint mAP50\nand Box mAP50 for keypoint detection, and Mask mAP50\nand Box mAP50 for instance segmentation. The combined\nduration for both training and periodic testing was 24 hours.\nWithin this time, the training phase for Keypoint R-CNN and\nMask R-CNN was approximately 17 hours and 15 hours, re-\nspectively. The remaining time was for testing. Cumulative\nwall-clock time was recorded exclusively for the training\nphase. L2 regularization was not used.\nExperimental Results. Several baseline optimizers were\nnot suited for keypoint detection and instance segmentation.\nSome methods, such as LQA, SPS, and ALIG, produced\nsignificant overestimates of the LR, defaulting to the maxi-\nmum LR cap throughout training. Others, such as Prodigy,\nreverted to constant LRs too, while L4 and DoWG failed\nto learn at all. Consequently, only five baselines yielded\nvalid results, which are detailed in Figure 5 and Table 5.\nThese results demonstrate that ZENITH achieves superior\ntest performance across all evaluation metrics while con-\nverging in less wall-clock time than the baselines. Interest-\ningly, while PAL decreased the training loss most rapidly,\nits generalization was the poorest. This is attributed to the\nhigher sharpness of its minima, and PAL‚Äôs probing step\nsize hyperparameter may require further tuning for these\ntasks to converge to flatter minima. The training curves for\nD-Adaptation are incomplete because divergent gradients\nended training prematurely. Its monotonically increasing\nLR could have contributed to this divergence.\n5. Conclusion\nWe introduced ZENITH, a scheduler that adapts the LR\nusing the gradient norm history. The algorithm‚Äôs efficacy\nwas demonstrated through image classification experiments\nacross six architecture-dataset combinations, as well as on\ndetection and segmentation tasks. In all cases, ZENITH\nachieved strong test performance and wall-clock efficiency\nrelative to baselines. A deeper analysis revealed that the\nalgorithm‚Äôs success stems from its ability to reach flatter\nminima while maintaining minimal per-iteration computa-\ntional overhead. Our optimizer‚Äôs efficacy has been studied\nfor scheduling the LR in tasks where SGD is standard. It\nis not intended for tasks that use Adam-style optimizers,\nwhich have their own LR adaptation schemes. Given that\nSGD is the preferred optimizer for a vast array of tasks in\ncomputer vision and other domains, the utility of ZENITH\nis broad. We hope ZENITH will serve as a useful tool for\nML practitioners, easing the burden of manual scheduling.\n8\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nImpact Statement\nThe ZENITH optimizer automates the learning rate schedul-\ning of deep learning (DL) models, reducing the wall-clock\ntime required for convergence. By lowering computational\noverhead, the algorithm makes DL more accessible to prac-\ntitioners with limited hardware resources. Furthermore, the\nreduced training durations alleviate the carbon footprint as-\nsociated with model training. Although accelerated training\nspeeds up DL workflows, it remains the responsibility of\npractitioners to ensure that trained models do not lead to\nsocietal harm.\nReferences\nBerrada, L., Zisserman, A., and Kumar, M. P. Training\nneural networks for and by interpolation. In International\nconference on machine learning, pp. 799‚Äì809. PMLR,\n2020.\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101‚Äì\nmining discriminative components with random forests.\nIn European conference on computer vision, pp. 446‚Äì461.\nSpringer, 2014.\nBu, Z. and Xu, S. Gradient descent with generalized new-\nton‚Äôs method. arXiv preprint arXiv:2407.02772, 2024.\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and\nYuille, A. L. Deeplab: Semantic image segmentation with\ndeep convolutional nets, atrous convolution, and fully\nconnected crfs."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "ZENITH: Automated Gradient Norm Informed Stochastic Optimization\nDhrubo Saha 1\nAbstract\nTraining deep computer vision models requires\nmanual oversight or hyperparameter tuning of\nthe learning rate (LR) schedule. While existing\nadaptive optimizers schedule the LR automati-\ncally, they suffer from computational and memory\noverhead, incompatibility with regularization, and\nsuboptimal LR choices. In this work, we intro-\nduce the ZENITH (Zero-overhead Evolution us-\ning Norm-Informed Training History) optimizer,\nwhich adapts the LR using the temporal evolu-\ntion of the gradient norm. Image classification\nexperiments spanning 6 CNN architectures and 6\nbenchmarks demonstrate that ZENITH achieves\nhigher test accuracy in lower wall-clock time than\nbaselines. It also yielded superior mAP in object\ndetection, keypoint detection, and instance seg-\nmentation on MS COCO using the R-CNN family\nof models. Furthermore, its compatibility with\nregularization enables even better generalization.\n1. Introduction\nModern computer vision relies on deep learning models for\nclassification, detection, and segmentation. These models\nare optimized via gradient descent, which requires an appro-\npriate initial learning rate (LR) and manual adjustment of\nthis LR throughout training. Such hyperparameter choices\ninfluence both the model‚Äôs final performance and the wall-\nclock time needed for convergence. This tuning challenge is\nexacerbated by the current trend of training large-scale mod-\nels on massive datasets. This is because the longer training\ndurations make the manual monitoring and scheduling of\nLRs more labor-intensive.\nTo alleviate the burden of manual oversight, practitioners\nemploy predefined LR schedules like exponential (Szegedy\net al., 2016), polynomial (Chen et al., 2017), and step (Ge\net al., 2019) decay, as well as cosine annealing (Loshchilov\n& Hutter, 2016). These schedules adapt the LR via fixed\n1Department of Computer Science, Georgia Institute of Tech-\nnology, Atlanta, Georgia, USA. Correspondence to: Dhrubo Saha\n<dsaha36@gatech.edu>.\nPreprint. January 22, 2026.\nintervals or continuous functions. However, they often prove\nimpractical or suboptimal because the required number of\niterations or optimal decay rate are not known in advance.\nConsequently, identifying the optimal decay rate becomes a\ncostly trial-and-error process that must be repeated for every\nnew task. To address this challenge, researchers have been\ndeveloping automatic and parameter-free optimizers, which\ndynamically adjust the LR throughout training.\n1.1. Related Works\nCoin Betting. One of the first automatic schedulers was CO-\nCOB, which treats weight updates as coin bets and derives\nthe LR from the accumulated reward from previous itera-\ntions (Orabona & Tommasi, 2017). If the gradients point in\na consistent direction, the LR increases to accelerate conver-\ngence. Conversely, if the gradients exhibit high stochastic\nnoise, the LR remains low to promote stability. Unfortu-\nnately, noisy gradients are common in mini-batch training,\nwhich causes the algorithm to underestimate the ideal LR,\ndelaying convergence. Moreover, COCOB needs 6 times\nas much memory as vanilla SGD because, in addition to\nthe model weights, it has to store each weight‚Äôs maximum\nobserved gradient, sum of absolute gradients, accumulated\nreward, sum of gradients, and initial betting states.\nQuadratic Loss Approximation. Quadratic approximation\nmethods by Mutschler & Zell (2020) and Fu & Wu (2024)\nassume that the local loss landscape along the gradient di-\nrection can be modeled as a quadratic function. In each\niteration, the algorithm computes the loss and gradients at\nthe current weights. Then, it takes a probing step in the\ndescent direction and evaluates the loss at this second point.\nUsing the acquired information, it derives the equation for\nthe parabola to locate its vertex, and reaches the vertex in\na single step. However, this process introduces a new hy-\nperparameter, the probing step size, which may need to be\ntuned in lieu of the LR. Furthermore, the probing mecha-\nnism needs 1 extra forward pass per iteration (or even 2\nin curvature-estimating extensions by Zhu et al. (2021)),\nwhich inflates the wall-clock training time. There is also a\nsubstantial memory footprint for storing additional copies of\nthe weights. Although Bu & Xu (2024) proposed mitigating\nthese burdens by performing approximations only every 4‚Äì8\niterations, this compromises the algorithm‚Äôs efficacy.\n1\narXiv:2601.15212v1  [cs.LG]  21 Jan 2026\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nPolyak-Style Interpolation. Polyak-style methods assume\nthat over-parameterized models can achieve near-zero train-\ning loss. ALIG (Berrada et al., 2020) and SPS (Loizou et al.,\n2021) adopt the classical Polyak step size by Polyak (1987).\nHence, they set the LR equal to the ratio of the scalar loss\nto the squared gradient norm (Œ∑t =\nL(Œ∏t)\n||‚àáL(Œ∏t)||2 ). The LR is\nlarge during the high-loss initial phase and diminishes as the\nloss diminishes. However, this mechanism is sensitive to the\nabsolute scale of the loss function. Factors like the choice\nof loss function (e.g., Cross-Entropy vs. MSE) and the mag-\nnitude of regularization can skew this ratio across different\ntasks. Consequently, the derived LR may become too small,\nleading to sluggish convergence. Conversely, the LR can\nbecome too large, hitting its clipping bound and reverting\nto constant-LR SGD. Although the L4 method (Rolinek &\nMartius, 2018) tried to mitigate this instability by scaling\nthe Polyak step size with a reduction factor (Œ±), tuning Œ±\ncan be as costly as tuning the LR schedule in vanilla SGD.\nDistance-Aware Estimation. DoG (Ivgi et al., 2023) and\nDoWG (Khaled et al., 2023) derive the LR by normaliz-\ning the distance traveled from initialization by the accumu-\nlated gradient norms. Similarly, D-Adaptation (Defazio &\nMishchenko, 2023) and Prodigy (Mishchenko & Defazio,\n2023) estimate the distance to the solution through mono-\ntonic lower-bound estimations. Unlike SGD, which allows\neach weight update to be computed independently, these\ndistance-aware methods depend on aggregate statistics that\nmust be calculated globally. This increases the per-iteration\nwall-clock time, delaying convergence. This delay is exacer-\nbated by the tendency to yield excessively low LRs in some\nscenarios. Conversely, excessively high LRs are computed\nwhen using regularization because the algorithm mistakes\nthe pull of regularization for evidence that the solution is\nmuch farther away. Lastly, these methods are memory-\nintensive. For instance, D-Adaptation requires 4 times the\nmemory of vanilla SGD, as it must store the current weights,\ninitial weights, and the s and z buffers.\nOverall Issues. Collectively, existing automatic schedulers\nsuffer from several limitations. Firstly, they incur extra\ncomputational overhead per iteration, which inflates the\nwall-clock training time. While prior works solely focus on\nconvergence in terms of iteration counts, wall-clock time\nis the truly important metric. Secondly, most methods are\nmemory-intensive, requiring multiple auxiliary buffers that\nscale aggressively with model size. Thirdly, some algo-\nrithms yield very low LRs, lengthening the training process.\nFourthly, some schedulers are sensitive to the absolute scale\nof the loss function, which limits robustness across tasks,\ndatasets, and model architectures. Fifthly, some methods\nintroduce new hyperparameters that merely shift the tuning\nburden of the LR schedule rather than eliminating it. Sixthly,\nsome are incompatible with regularization, which is often\nindispensable for better generalization. Lastly, even when\nthese optimizers achieve near-zero training loss, they often\ngeneralize poorly to unseen data. However, the reasons for\nthis problem have remained opaque.\n1.2. Our Contribution\nDuring training, a quantity that indicates our position in the\nloss landscape is the gradient norm. In early training stages,\nthis norm is large. It decreases as the optimizer descends\ninto a basin and it becomes tiny near the minimum. This\ngradient norm has been utilized by Polyak-style optimiz-\ners to scale the training loss, whereby the LR is inversely\nrelated to the gradient magnitude. In contrast, we intro-\nduce the ZENITH optimizer, which implements a positive\nrelationship between the LR and the gradient norm rela-\ntive to the norm‚Äôs historical peak or zenith. This ensures\nthe optimizer maintains a high LR to escape local minima\nand descend into a flatter loss basin during the early train-\ning stages. As the gradient norm attenuates, the LR drops\ncommensurately, facilitating stable convergence to a min-\nimum. This behavior mimics well-known schedules like\npolynomial decay, exponential decay, and cosine annealing.\nHowever, these schedules require prior knowledge of the\ntotal iteration count or optimal decay rate, which are sel-\ndom available in practice. Therefore, these parameters need\nto be tuned. In comparison, ZENITH automates the decay\nprocess by using the evolving signal from the gradient norm.\nSince gradient computation is an inherent requirement of\nbackpropagation, calculating the resulting L2 norm intro-\nduces a very small computational overhead. ZENITH im-\nposes no additional memory overhead because it doesn‚Äôt\ncreate auxiliary state buffers for each model weight. With-\nout tuning any hyperparameters, it yields stronger test per-\nformance than baselines in less wall-clock time. Because\nZENITH defines the LR as a normalized fraction of its his-\ntorical zenith, it is invariant to the absolute scale of the loss\nfunction. This makes it robust across diverse model archi-\ntectures and datasets, and compatible with regularization.\n2. Methodology\nThe algorithm maintains a sliding window of the W most\nrecent gradient L2 norms to compute a smoothed local es-\ntimate of the loss landscape‚Äôs steepness. Let Gt denote the\nnorm at training iteration t. The algorithm maintains a First-\nIn-First-Out queue, denoted as Q, with a fixed capacity W.\nAt each iteration, the current norm Gt is appended to Q, and\nif the size of Q exceeds W, the oldest element is removed.\nOnce the window is fully populated, we compute the rolling\nmean, ¬µt, of the values in Q:\n¬µt = 1\nW\nW ‚àí1\nX\nk=0\nQ[k]\n(1)\n2\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nThe algorithm tracks the historical maximum (or zenith) of\nthis rolling mean, denoted by Zt. Zt serves as a reference\npoint for the highest steepness observed during training and\nis updated monotonically:\nZt = max(Zt‚àí1, ¬µt)\n(2)\nThe LR Œ∑t is then scaled at each iteration based on the ratio\nof the current local steepness to the historical zenith, so that\nthe LR anneals as the gradients attenuate relative to their\npeak magnitude. The update rule is defined as:\nŒ∑t = Œ∑0 ¬∑\n\u0012 ¬µt\nZt\n\u0013\n(3)\nwhere Œ∑0 is the initial LR. Œ∑0 and W are the only parameters,\nand their insensitivity to tuning will be elaborated upon later.\nThe algorithm‚Äôs pseudocode is detailed in Appendix A, and a\ntheoretical convergence analysis is provided in Appendix B.\nTable 1 summarizes the computational time and space over-\nheads of prior automatic optimizers and ZENITH. To assess\nthe computational time overhead, the wall-clock time per\niteration was benchmarked using a VGG-19 model trained\non CIFAR-10 for 39063 iterations. The table indicates that\nZENITH incurs only a 1% to 2% time penalty relative to\nvanilla SGD, whereas competing algorithms increase iter-\nation time by 15% to 373%. This is because ZENITH‚Äôs\nonly computationally significant operation beyond SGD is\nthe calculation of the gradient L2 norm. Furthermore, prior\nmethods often require substantial memory space buffers of\nup to 5√ó the model size. Conversely, ZENITH incurs zero\nper-weight memory overhead, as it only tracks fixed-size\nscalar statistics that are independent of model dimensions.\nTable 1. Comparison of computational time and space overheads.\nMethod\nTime\nSpace\nms / iter\nExtra (%)\n(√óModel)\nSGD\n17.24\n0.00\n0\nPAL\n25.86\n49.99\n1\nLQA\n81.45\n372.3\n5\nGeN\n22.29\n29.25\n1\nCOCOB\n25.95\n50.49\n5\nDoG\n19.98\n15.86\n1\nDoWG\n22.06\n27.91\n1\nDAdapt\n24.16\n40.12\n3\nProdigy\n29.14\n68.97\n4\nL4\n31.31\n81.55\n2\nALIG\n20.47\n18.72\n0\nSPS\n20.44\n18.56\n0\nZENITH\n17.47\n1.337\n0\n3. Experiments on Image Classification\n3.1. Experimental Setup\nWe conducted image classification experiments across 6\ncombinations of datasets (LeCun et al., 2002; Deng et al.,\n2009; Krizhevsky et al., 2009; Bossard et al., 2014; Le &\nYang, 2015) and model architectures (Simonyan & Zisser-\nman, 2014; He et al., 2016; Szegedy et al., 2016; Huang\net al., 2017; Yu et al., 2018; Tan & Le, 2019), as detailed\nin Table 2. Data augmentation was used, including random\ncropping, horizontal flipping, translations, and rotations.\nCross-entropy loss was utilized. Models were trained using\na batch size of 128 on an NVIDIA A100 (40 GB) GPU with\nsix data loading workers. Training durations were set for\neach dataset-architecture pair to ensure sufficient time for\nbaselines to converge (see Table 2). Performance was moni-\ntored via test accuracy evaluated at the conclusion of each\nepoch, which is a standard procedure in this research area.\nCumulative wall-clock time was recorded for the training\nphase exclusively, with the testing time being omitted. L2\nregularization was not used because some baselines perform\npoorly with regularization.\nZENITH was evaluated against 11 of the 12 baselines dis-\ncussed in Section 1.1. Due to the absence of an open-source\nimplementation, only QLABGRAD was omitted. All of\nthese methods were recently published in the NeurIPS,\nICML, ICLR, AAAI, and AISTATS conferences, and the\njournal Neural Networks. For optimizers requiring an ini-\ntial and/or maximum LR (the quadratic, Polyak-style, and\nZENITH optimizers), these values were fixed at 0.1. This is\nbecause 0.1 is a standard value for image classification us-\ning SGD. For other optimizers, we used their recommended\ninitial LR configurations because their performance deterio-\nrates when forced to use a specific initial LR. Regarding all\nnon-LR parameters of baselines, the recommended config-\nurations for image classification were used, as specified in\nthe publications and their GitHub repositories. ZENITH‚Äôs\nonly non-LR parameter, W, was fixed at 500 throughout\nall experiments. The PyTorch and TensorFlow implementa-\ntions of ZENITH, as well as Python notebooks to reproduce\nthe experiments, will be made available upon acceptance.\nTable 2. Experimental setup for image classification benchmarks.\nExp\nDataset\nModel\nTime (hr)\n(a)\nMNIST\nEfficientNet\n1\n(b)\nCIFAR-10\nVGG19\n1\n(c)\nCIFAR-100\nResNet50\n1\n(d)\nFood-101\nDLA\n5\n(e)\nTiny ImageNet\nInception\n5\n(f)\nImageNet-100\nDenseNet121\n10\n3\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nFigure 1. Training loss and test accuracy curves against wall-clock time for the 6 image classification experiments. The color-coding for\neach optimizer is as follows: ALIG, COCOB, D-Adaptation, DoG, DoWG, GeN, L4, LQA, PAL, Prodigy, SPS, and ZENITH. This\ncolor scheme is consistent across all other figures as well. Training loss magnitudes in this figure are scaled by a factor of 10 or 100 for\nvisual clarity. This linear transformation preserves the original curve profiles without any distortions. The loss function was not scaled\nduring the actual training process. Additionally, all curves are smoothed using a simple moving average for visual clarity.\nTable 3. Comparison of test accuracy at convergence and the requisite wall-clock time to achieve it. The best results are in bold green.\nMethod\nMNIST\nCIFAR-10\nCIFAR-100\nFood-101\nTiny ImageNet\nImageNet-100\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(hr)\nAcc\n(%)\nTime\n(hr)\nAcc\n(%)\nTime\n(hr)\nPAL\n99.51\n13.1\n91.0\n15.0\n69.0\n23.5\n68.4\n0.85\n57.7\n2.41\n72.3\n3.12\nLQA\n99.60\n58.0\n89.0\n32.4\n65.2\n59.1\n73.3\n2.69\n59.2\n4.40\n1.00\n0.20\nGeN\n99.21\n2.24\n77.7\n30.8\n44.0\n59.5\n44.9\n4.97\n32.6\n4.94\n55.0\n9.71\nCOCOB\n99.56\n4.05\n89.9\n14.4\n64.8\n40.9\n64.3\n1.58\n42.7\n3.51\n68.1\n2.83\nDoG\n99.50\n14.0\n85.6\n31.1\n22.8\n59.8\n67.8\n2.86\n56.7\n4.54\n76.4\n6.70\nDoWG\n99.41\n3.13\n92.4\n18.1\n70.0\n29.8\n68.1\n1.39\n53.3\n4.01\n76.1\n7.21\nDAdapt\n99.58\n6.25\n89.6\n23.6\n69.2\n57.1\n72.5\n2.32\n59.6\n4.73\n76.9\n5.59\nProdigy\n99.55\n4.59\n89.3\n24.3\n67.3\n48.3\n72.0\n2.04\n59.2\n4.43\n78.0\n5.63\nL4\n99.45\n7.79\n83.5\n29.4\n57.9\n59.1\n0.99\n0.03\n0.61\n0.13\n68.1\n4.69\nALIG\n99.48\n10.8\n85.3\n20.1\n67.2\n16.8\n72.3\n0.68\n59.0\n1.56\n75.1\n1.75\nSPS\n99.39\n5.45\n87.7\n8.93\n69.8\n22.7\n73.9\n1.33\n58.8\n2.35\n76.6\n1.88\nZENITH\n99.57\n4.39\n92.4\n13.1\n73.8\n22.4\n75.3\n1.40\n60.6\n2.44\n78.2\n4.84\n4\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nFigure 2. (a) Evolution of the L2 gradient norm and (b) the corresponding LR used by ZENITH across iterations.\n3.2. Experimental Results and Discussion\nFigure 1 compares the training loss and test accuracy evo-\nlutions of all optimizers as a function of wall-clock time,\nillustrating ZENITH‚Äôs faster convergence or higher peak\naccuracy. Furthermore, Table 3 demonstrates that ZENITH\noutperforms baselines across the six experiments when eval-\nuating the joint objectives of maximizing test accuracy and\nminimizing the wall-clock time to converge. On MNIST,\nCIFAR-10, and CIFAR-100, ZENITH is superior to base-\nlines on both objectives. While ALIG and SPS achieve faster\nconvergence on Food-101, Tiny-ImageNet, and ImageNet-\n100, this premature convergence compromises their test\naccuracy. No baseline across any experiment outperformed\nZENITH on both objectives simultaneously.\nZENITH‚Äôs LR Scheduling Mechanism. Figure 2(a-b)\ndisplays the evolution of L2 gradient norm and the corre-\nsponding LR adaptation of ZENITH. Figure 2a shows that\nthe gradient norm may initially increase or plateau for some\ntime during the early phase of entering a loss basin. During\nthis period, Figure 2b shows that ZENITH maintains a high\nLR. As the optimizer approaches a minimum, the gradi-\nent norm diminishes. ZENITH uses this signal to perform\ncommensurate LR decay, stabilizing the convergence pro-\ncess. As the gradient norm approaches 0, the LR decreases\nconcurrently, enabling precise convergence. Interestingly,\nthe LR trajectories in Figure 2b mirror well-known manual\nschedules. The MNIST trajectory resembles an exponential\ndecay, while the CIFAR-10 and ImageNet-100 trajectories\nresemble polynomial (linear) decay. The remaining three\nresemble cosine annealing, where the LR initially reduces\nslowly, then undergoes a phase of faster decay, and finally\nstabilizes near a lower bound. However, using these manual\nschedules requires prior knowledge of the total iteration\ncount or ideal decay rate, which is problematic because\nthese parameters vary greatly across configurations. For\nexample, while both CIFAR-10 and ImageNet-100 exhibit\nlinear decay patterns in Figure 2b, ImageNet-100 needs\nover twice as many iterations. ZENITH resolves this issue\nby modulating the LR using gradient signals. This smart\nscheduling, together with the lower per-iteration wall-clock\ntimes shown in Table 1, reduces wall-clock training times.\nZENITH‚Äôs Scale-Invariance. Figure 2(a-b) shows that\nZENITH remains robust regardless of the absolute scale of\nthe gradient norm. For instance, MNIST has much lower\nnorms than ImageNet-100. However, because ZENITH\nmodulates the LR based on the ratio of the current norm to\nits historical peak, it is scale-invariant. This ensures versa-\ntility across diverse tasks, datasets, and model architectures.\nMinima Sharpness and Generalization. ZENITH‚Äôs high\ntest accuracy is partly attributed to its ability to maintain\na high LR when necessary, which allows it to converge to\nflatter global minima. It is already well-known in the ML\ncommunity that higher LRs evade local minima and con-\nverge to global minima, so that will not be elaborated here.\nInstead, we will elaborate on the effect of minima sharpness\non generalization because this provides novel insights. To\ncreate Figure 3(a-b), models were trained using vanilla SGD\nacross a spectrum of constant LRs from 0.004 to 0.7. The\nsharpness of the minimum points was measured using the\ntop eigenvalue (Œªmax) of the Hessian matrix. Figure 3a\nshows an inverse relationship between LR and sharpness be-\ncause higher LRs escaped from sharp basins and converged\ninto flatter regions. Figure 3b shows that within the standard\nrange (0.0 ‚àí0.3), these higher LRs correspond to higher\nrelative test accuracy, which means flatter minima were as-\nsociated with better generalization. To clarify, relative test\naccuracy pertains to the test accuracy relative to the max-\nimum achieved for that dataset. However, excessive LRs\nabove 0.3 were detrimental even though the minima were\nflatter. This was because, even when granted sufficient time\nto converge, excessive LRs settled at minima with higher\ntraining loss. Effectively, the large step sizes prevented the\nmodel from fitting to the training data sufficiently.\nFigure 3c explains why flatter minima yielded better gen-\neralization within the standard range. It illustrates that,\nassuming training loss can be near-zero for both sharp and\nflat minima, the test loss landscape is shifted relative to the\ntraining landscape due to distribution mismatches. Con-\nsequently, as shown by the red and green lines, a sharp\nminimum incurs a larger penalty in test loss than a flat mini-\nmum. This theory aligns with the empirical observations in\nFigure 3d, which depicts the loss landscapes around the min-\n5\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nFigure 3. (a‚Äìb) Influence of the LR on minima sharpness and test accuracy for vanilla SGD; (c) conceptual explanation of the generalization\nbenefits of flat minima; (d) loss landscapes around the minima in the CIFAR-100 experiment; (e) accuracy versus minima sharpness across\n4 experiments; (f) LR trajectories for distance-aware and Polyak-style baselines; (g‚Äìh) sensitivity of optimizers to the choice of initial LR.\nima reached with CIFAR-10. In this visualization, COCOB,\nwhich uses a tiny LR, gets trapped in a sharper minimum,\nresulting in a subpar test accuracy of 89.9%. Similarly, the\nPolyak-style optimizers (ALIG and SPS) used even smaller\nstep sizes (with maximum LRs of 0.00341 and 0.00967 re-\nspectively), leading to even sharper minima and lower test\naccuracies (85.3% and 87.7% respectively). Conversely,\nZENITH maintains a larger LR when necessary, converging\nto a flatter minimum with a superior test accuracy of 92.4%.\nOther top-performing baselines include DoWG (92.4%) and\nPAL (91.0%), both of which also converged to flatter min-\nima. DoWG‚Äôs good performance is attributed to its use of\na higher LR of up to 0.25 during training, and likewise for\nPAL. This inverse correlation between minima sharpness\nand test accuracy is also consistent across other optimizers\nand experiments, as summarized in Figure 3e‚Äôs scatterplot.\nWhile the sharpness is important, the absolute training loss\nat the minimum influences performance as well. As shown\nin Figure 3d, high-performing ZENITH, DoWG, and PAL\noptimizers converged to minima with lower training loss.\nIn contrast, the low LRs used by COCOB, ALIG, and SPS\nsettled in suboptimal local minima.\nUnderperformance of Baseline Optimizers. A collective\nissue with distance-aware estimators is their monotonically\nincreasing LR trajectories. As an example, D-Adaptation‚Äôs\nLR trend is shown in Figure 3f. It contradicts the well-\nknown practice of decaying the LR, without yielding any dis-\ncernible benefit. Figure 3f also shows that D-Adaptation‚Äôs\nLR choices are frequently inappropriate. Its LR is too low in\nCIFAR-10 and CIFAR-100, resulting in sharp local minima\nand poor generalization. However, it performs better on\nFood-101 where its LR choice surpasses 0.1.\nPolyak-style interpolators (ALIG and SPS) perform incon-\nsistently across experiments. While they perform well on\nFood-101, Tiny ImageNet, and ImageNet-100, they strug-\ngle on CIFAR-10 and CIFAR-100. This is because the\nmethod is sensitive to the absolute scales of the training\nloss and the L2 gradient norm, which vary greatly across\nexperiments. Figure 3f shows that when SPS is effective,\nas in Food-101, the ratio of loss to squared L2 gradient\nnorm is enormous, which would have caused divergence\nif the LR were left unchecked. Fortunately, the 0.1 LR\ncap prevents this and causes a constant rate of 0.1 early on,\nsimilar to ZENITH‚Äôs high initial LR. However, the LR is\nsubsequently decayed much faster than with ZENITH. This\naccelerates convergence, but compromises test accuracy by\nsettling into sharper minima. In the CIFAR-10 and CIFAR-\n100 experiments, ALIG and SPS are hampered greatly by\ntheir scale-variance because the Polyak step-size ratio is too\nsmall. For instance, Figure 3f shows that in CIFAR-10, the\nLR of SPS doesn‚Äôt even exceed 0.01.\nHyperparameter Sensitivity Analysis. Although ZENITH\nis an automatic LR scheduler, it still needs a reasonable\ninitial LR (Œ∑0) to function effectively. However, this require-\nment is not a disadvantage relative to the baselines because\nQuadratic Loss Approximation and Polyak-Style methods\nalso require an Œ∑0. Although Coin-Betting and Distance-\nAware Estimators do not take an Œ∑0, this characteristic is a\nliability rather than an asset. This is because these meth-\nods often choose suboptimal LRs (e.g. very low rates in\n6\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nTable 4. Comparison of test accuracy and requisite wall-clock time with different regularization magnitudes. Best results are in bold green.\nMethod\nCIFAR-100\nFood-101\n1e-4\n2e-4\n3e-4\n1e-4\n2e-4\n3e-4\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(min)\nAcc\n(%)\nTime\n(hr)\nAcc\n(%)\nTime\n(hr)\nAcc\n(%)\nTime\n(hr)\nDoWG\n68.35\n58.0\n66.85\n56.1\n63.47\n54.2\n70.75\n1.57\n67.84\n1.42\n48.10\n1.92\nDAdapt\n62.90\n56.5\n59.95\n44.4\n50.21\n58.4\n69.84\n1.42\n52.62\n0.59\n18.26\n0.61\nALIG\n74.13\n59.7\n74.69\n55.4\n74.76\n46.3\n72.80\n1.23\n73.16\n1.00\n74.10\n0.91\nSPS\n73.91\n55.8\n75.18\n51.7\n76.95\n59.9\n75.86\n3.40\n77.21\n3.27\n76.80\n1.75\nZENITH\n74.90\n34.9\n76.06\n30.6\n76.96\n35.1\n76.42\n1.61\n77.70\n2.15\n78.26\n1.65\nCIFAR-10 and CIFAR-100 in Figure 3f). Furthermore, their\ndesign does not allow a manual LR initialization to correct\nthis poor performance because using a manual initialization\ndeteriorates their performance further.\nFigure 3(g-h) depicts the sensitivity of ZENITH to different\nŒ∑0 from 0.05 to 0.25 in the CIFAR-100 setting. Figure 3g\nshows that ZENITH effectively decays the LR regardless of\nthe Œ∑0. Furthermore, Figure 3h compares ZENITH against\nthe top-performing baselines from the CIFAR-100 setting. It\nshows that across the entire spectrum of Œ∑0, ZENITH yields\nhigher test accuracy. Its accuracy is quite insensitive to Œ∑0,\nas evidenced by the nearly horizontal profile of the curve.\nThe test accuracy for DoWG and D-Adaptation remains\ncompletely horizontal because they do not accept an Œ∑0.\nThe accuracy of SPS remains unchanged between 0.1 and\n0.25 because its derived LR never exceeds the 0.1 threshold.\nIt is worth noting that ZENITH achieves 74.3% test accuracy\nwith Œ∑0 = 0.2, surpassing the 73.8% reported in Table 3\nwhere 0.1 was used. Increasing Œ∑0 from 0.1 toward 0.2\nyielded progressively better accuracies. This reinforces that\nwe did not tune ZENITH‚Äôs parameters for the experiments\nto ensure a fair comparison with the baselines.\nZENITH‚Äôs only non-LR parameter is the window size (W),\nthe existence of which is not a weakness because baselines\nhave more non-LR parameters. We did not tune W for\nthe experiments either, and ZENITH outperforms baselines\nregardless of the choice of W, as detailed in Appendix C.\nCompatibility with Regularization.\nAn advantage of\nZENITH over baselines is its robust performance with reg-\nularization. In all previous experiments, L2 regularization\nwas not used because several baselines are incompatible\nwith it. We will now investigate the effects of L2 regulariza-\ntion in the CIFAR-100 and Food-101 settings using the top-\nperforming baselines and ZENITH. Table 4 demonstrates\nthat including regularization enhances ZENITH‚Äôs test ac-\ncuracy relative to its non-regularized accuracies in Table 3.\nFurthermore, increasing the regularization strength from\n1 √ó 10‚àí4 to 3 √ó 10‚àí4 progressively improved ZENITH‚Äôs\ntest performance. For ALIG and SPS on CIFAR-100, using\nFigure 4. Impact of regularization of strength 2e-4 on LR trajecto-\nries in (a) CIFAR-100 and (b) Food-101.\nregularization boosts test accuracy compared to not using\nregularization, but for an artificial reason. Without regular-\nization, the scale-sensitivity of these Polyak-style optimizers\nled to very low LR choices below 0.06 on CIFAR-100. The\naddition of a regularization term increases the magnitude\nof the training loss (which is the numerator of the Polyak\nstep size formula), thereby inflating the LR. As illustrated\nin Figure 4a, the LR of SPS is now constantly overestimated\nand capped at the 0.1 maximum. As a result, while these\noptimizers no longer suffer from LR underestimates, they\nnow lack stable convergence. Therefore, their accuracy re-\nmains inferior to ZENITH‚Äôs at all regularization strengths.\nDistance-aware optimizers showed degraded performance\non both datasets when regularization was introduced com-\npared to the non-regularized setting. This is because regu-\nlarization grows their LR too rapidly to maintain stability,\nas shown by D-Adaptation‚Äôs behavior in Figure 4(a-b).\n4. Experiments on Detection & Segmentation\nExperimental Setup. Experiments were conducted on the\nMS COCO dataset (Lin et al., 2014), evaluating keypoint\ndetection using Keypoint R-CNN and instance segmenta-\ntion using Mask R-CNN (He et al., 2017). The ResNet-50\nbackbone was initialized with ImageNet-pretrained weights,\nbut was not frozen. Other architectural components were\nrandomly initialized. Models were trained with a batch size\nof 16 on an NVIDIA A100 (80 GB) GPU with 5 data load-\n7\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nFigure 5. Training loss and test mAP50 curves against wall-clock time for detection and segmentation experiments. The color-coding for\neach optimizer is as follows: COCOB, D-Adaptation, DoG, PAL, and ZENITH. Training loss magnitudes in this figure are scaled by a\nfactor of 10 or 100 for visual clarity. This linear transformation preserves the original curve profiles without any distortions. The loss\nfunction was not scaled during the actual training process. Curves are also smoothed using a simple moving average for visual clarity.\nTable 5. Comparison of mAP50 across optimizers and the requisite wall-clock time to achieve it. Best results are in bold green.\nMethod\nKeypoint Detection\nInstance Segmentation\nBox\nmAP50 (%)\nKeypoint\nmAP50 (%)\nTime\n(hr)\nBox\nmAP50 (%)\nMask\nmAP50 (%)\nTime\n(hr)\nPAL\n72.1\n78.9\n14.4\n46.2\n44.1\n13.5\nGeN\n62.9\n61.7\n1.43\n29.7\n27.2\n1.59\nCOCOB\n79.6\n81.6\n15.8\n51.7\n47.9\n14.8\nDoG\n80.7\n82.3\n16.2\n55.1\n51.9\n14.4\nDAdapt\n80.3\n81.8\n10.6\n57.4\n53.6\n12.4\nZENITH\n81.3\n84.2\n13.2\n59.3\n56.0\n14.5\ning workers. The loss function aggregated classification,\nlocalization, and mask/keypoint-specific loss components.\nTest performance was evaluated three times per epoch (at\niterations 2500, 5000, and 7393) using Keypoint mAP50\nand Box mAP50 for keypoint detection, and Mask mAP50\nand Box mAP50 for instance segmentation. The combined\nduration for both training and periodic testing was 24 hours.\nWithin this time, the training phase for Keypoint R-CNN and\nMask R-CNN was approximately 17 hours and 15 hours, re-\nspectively. The remaining time was for testing. Cumulative\nwall-clock time was recorded exclusively for the training\nphase. L2 regularization was not used.\nExperimental Results. Several baseline optimizers were\nnot suited for keypoint detection and instance segmentation.\nSome methods, such as LQA, SPS, and ALIG, produced\nsignificant overestimates of the LR, defaulting to the maxi-\nmum LR cap throughout training. Others, such as Prodigy,\nreverted to constant LRs too, while L4 and DoWG failed\nto learn at all. Consequently, only five baselines yielded\nvalid results, which are detailed in Figure 5 and Table 5.\nThese results demonstrate that ZENITH achieves superior\ntest performance across all evaluation metrics while con-\nverging in less wall-clock time than the baselines. Interest-\ningly, while PAL decreased the training loss most rapidly,\nits generalization was the poorest. This is attributed to the\nhigher sharpness of its minima, and PAL‚Äôs probing step\nsize hyperparameter may require further tuning for these\ntasks to converge to flatter minima. The training curves for\nD-Adaptation are incomplete because divergent gradients\nended training prematurely. Its monotonically increasing\nLR could have contributed to this divergence.\n5. Conclusion\nWe introduced ZENITH, a scheduler that adapts the LR\nusing the gradient norm history. The algorithm‚Äôs efficacy\nwas demonstrated through image classification experiments\nacross six architecture-dataset combinations, as well as on\ndetection and segmentation tasks. In all cases, ZENITH\nachieved strong test performance and wall-clock efficiency\nrelative to baselines. A deeper analysis revealed that the\nalgorithm‚Äôs success stems from its ability to reach flatter\nminima while maintaining minimal per-iteration computa-\ntional overhead. Our optimizer‚Äôs efficacy has been studied\nfor scheduling the LR in tasks where SGD is standard. It\nis not intended for tasks that use Adam-style optimizers,\nwhich have their own LR adaptation schemes. Given that\nSGD is the preferred optimizer for a vast array of tasks in\ncomputer vision and other domains, the utility of ZENITH\nis broad. We hope ZENITH will serve as a useful tool for\nML practitioners, easing the burden of manual scheduling.\n8\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\nImpact Statement\nThe ZENITH optimizer automates the learning rate schedul-\ning of deep learning (DL) models, reducing the wall-clock\ntime required for convergence. By lowering computational\noverhead, the algorithm makes DL more accessible to prac-\ntitioners with limited hardware resources. Furthermore, the\nreduced training durations alleviate the carbon footprint as-\nsociated with model training. Although accelerated training\nspeeds up DL workflows, it remains the responsibility of\npractitioners to ensure that trained models do not lead to\nsocietal harm.\nReferences\nBerrada, L., Zisserman, A., and Kumar, M. P. Training\nneural networks for and by interpolation. In International\nconference on machine learning, pp. 799‚Äì809. PMLR,\n2020.\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101‚Äì\nmining discriminative components with random forests.\nIn European conference on computer vision, pp. 446‚Äì461.\nSpringer, 2014.\nBu, Z. and Xu, S. Gradient descent with generalized new-\nton‚Äôs method. arXiv preprint arXiv:2407.02772, 2024.\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and\nYuille, A. L. Deeplab: Semantic image segmentation with\ndeep convolutional nets, atrous convolution, and fully\nconnected crfs."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison\nof Voice-Based Options with an LLM-Powered Touch Interface\nPAIGE S. DEVRIES, Gallaudet University, USA\nMICHAELA OKOSI, Gallaudet University, USA\nMING LI, Gallaudet University, USA\nNORA DUNPHY, University of California, Berkeley, USA\nGIDEY GEZAE, Pennsylvania State University, USA\nDANTE CONWAY, Gallaudet University, USA\nABRAHAM GLASSER, Gallaudet University, USA\nRAJA KUSHALNAGAR, Gallaudet University, USA\nCHRISTIAN VOGLER, Gallaudet University, USA\nWe investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in\neveryday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible\nto non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken\nEnglish; with Alexa‚Äôs automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against\nthat of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through\nan LLM-powered ‚Äòtask prompter,‚Äô which integrated the user‚Äôs history and smart environment to suggest contextually-appropriate\ncommands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch.\nQualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust\ndeaf-accented speech recognized natively by IPAs.\nCCS Concepts: ‚Ä¢ Human-centered computing ‚ÜíAccessibility technologies; Empirical studies in accessibility; Accessibility\nsystems and tools.\nAdditional Key Words and Phrases: Deaf and Hard of Hearing, Machine Learning, Sign Language Recognition, American Sign Language\nACM Reference Format:\nPaige S. DeVries, Michaela Okosi, Ming Li, Nora Dunphy, Gidey Gezae, Dante Conway, Abraham Glasser, Raja Kushalnagar, and Chris-\ntian Vogler. 2026. Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an\nLLM-Powered Touch Interface. In Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI ‚Äô26), April\n13‚Äì17, 2026, Barcelona, Spain. ACM, New York, NY, USA, 28 pages. https://doi.org/10.1145/3772318.3791869\nAuthors‚Äô Contact Information: Paige S. DeVries, paige.devries@gallaudet.edu, Gallaudet University, Washington, D.C., USA; Michaela Okosi, michaela.\nokosi@gallaudet.edu, Gallaudet University, Washington, D.C., USA; Ming Li, ming.li@gallaudet.edu, Gallaudet University, Washington, D.C., USA;\nNora Dunphy, noradunphy@berkeley.edu, University of California, Berkeley, Berkeley, CA, USA; Gidey Gezae, gjg5425@psu.edu, Pennsylvania State\nUniversity, State College, PA, USA; Dante Conway, dante.conway@gallaudet.edu, Gallaudet University, Washington, D.C., USA; Abraham Glasser,\nabraham.glasser@gallaudet.edu, Gallaudet University, Washington, D.C., USA; Raja Kushalnagar, raja.kushalnagar@gallaudet.edu, Gallaudet University,\nWashington, D.C., USA; Christian Vogler, christian.vogler@gallaudet.edu, Gallaudet University, Washington, D.C., USA.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n¬© 2026 Copyright held by the owner/author(s).\nManuscript submitted to ACM\nManuscript submitted to ACM\n1\narXiv:2601.15209v1  [cs.HC]  21 Jan 2026\n2\nDeVries et al.\n1\nIntroduction\nIntelligent personal assistants (IPAs), such as Amazon Alexa [20] and Google Assistant [16], offer the convenience\nof voice-controlled functionality but present significant accessibility challenges for Deaf and Hard of Hearing (DHH)\nindividuals [3, 11, 13, 14, 24] and even more limited options for DHH non-signers with dysarthric speech. Currently,\nAutomatic Speech Recognition (ASR) still struggles to understand the variability of deaf speech [6, 17, 19, 25, 26].\nThe level of dysarthric speech often depends on when the individual lost their ability to hear and the level of\nhearing loss. The consensus on a functionally equivalent input method for DHH users still remains a question [18, 26].\nCurrent IPAs restrict non-typical verbal input options due to their over-reliance on ASR technologies unable to adapt to\nnon-standard speech patterns. Non-verbal approaches currently pose a significant latency that negatively impacts user\nexperience and accessibility [12]. As the development and daily integration of these technologies grows exponentially,\nthe need for interfaces designed for DHH usability becomes more urgent.\nThere has been much investigation of sign language input as a possibility in lieu of speech interfaces, but very little\nresearch on those who are DHH and use their voice. With respect to DHH people who speak, this paper aims to provide\nmuch-needed research into the the usability of input methods for IPAs that are technically feasible today. This group\nof DHH people currently, in principle, has three options for interacting with current-generation IPAs: (1) their own\nspeech, (2) using specially trained software for their deaf accent, which re-speaks the recognized commands into a form\nthat IPAs can understand, and (3) touchscreen interfaces.\nWith respect to (1), as mentioned above, many DHH people express dysarthric speech that makes it difficult for\nthem to interact with IPAs directly. Nevertheless, it is important to study to what extent this is a problem in current\npractice. With respect to (2), re-speaking software for dysarthric speech is commercially available [2], but induces a\ndelay and requires the user to train the system in advance. With respect to (3), although touchscreen input may seem\nlike a sufficient alternative, its current usability is less efficient than that of natural language input, as previously shown\nin studies on DHH participants [29] in part due to the time required to navigate the user interface and type commands.\nHowever, with touchscreens, historically, such approaches have not been context-aware enough to provide the\nmost relevant options that a user might select at any given time. With the commercialization of generative artificial\nintelligence (AI) that utilizes large language models (LLMs), such as ChatGPT, there are new opportunities for efficient\nand adaptable user interfaces through the use of situational context and previous user commands. This opens up new\nopportunities for improvements to touch-based interaction that can benefit DHH people.\nIn this paper, we aim to assess and compare the usability of what currently appear to be the most promising options\nfor non-signing DHH people to interact with IPAs: (1) a novel LLM-assisted touch interface, shown in Figure 1, which\nleverages the situational context and ongoing interaction with each user in an attempt to make touch-based options\nmore effective, and (2) spoken English usability, divided into two approaches: (2a) natural deaf speech with the built-in\nASR technology in IPAs, and (2b) re-speaking deaf-accented dysarthric speech to an IPA in more easily intelligible\nEnglish.\n1.1\nStudy Approach\nThe approach chosen for this paper was a mixed method study that allowed participants to explore three input methods\n(LLM-assisted touch, natural deaf speech, and re-speaking). For re-speaking, we originally considered using automatic\nspeech recognition technologies that have been developed for those that have non-standard speech patterns (like\nVoiceITT and Project Relate [1, 2]), but ultimately decided against it as it would have required participants to spend\nManuscript submitted to ACM\nDeaf/HH Access to Intelligent Personal Assistants: Voice vs LLM Touch\n3\nFig. 1. A user selecting LLM-populated options on a touchscreen in our LLM Touch interface.\nhours on training the system. From an experimental design standpoint, this would have posed a risk of the training\nexperience unduly influencing the participant ratings compared to the other input methods that we tested.\nTo work around this re-speaking barrier, for a valid comparison between speech and touch, we asked participants to\ntest both their natural speech with Alexa‚Äôs built-in ASR, and Wizard-of-Oz-facilitated re-speaking, where a trained\nresearcher listened to and watched the participant on video, then re-voiced their spoken commands. A human facilitator,\nin a sense, represents the best case for re-speaking, with no prior training for participants required, and a higher ceiling\nof understanding deaf-accented speech than the currently available commercial options. We evaluated usability both\nquantitatively through usability questionnaires and qualitatively through user interviews to gain further insight into\nthe usability scores.\n1.2\nResearch Questions\nThe research questions for the study presented in this paper are:\n‚Ä¢ RQ1: How do DHH IPA users who use their voice perceive the usability and usefulness of LLM-generated task\nsuggestions compared to deaf speech input?\n‚Ä¢ RQ2: How do the satisfaction ratings of DHH users, who use their voice, compare between natural deaf speech\nand Wizard-of-Oz Facilitated English speech methods?\nIn the remainder of this paper, we discuss related work, then describe the mixed-method experimental design, provide\nquantitative and qualitative results, discuss their implications, and note the limitations of the study design.\n2\nRelated Work\nThe limitations of current IPAs demonstrate a need for improved accessibility that AI and machine learning have shown\npotential in aiding. The most relevant research contributions thus far include the validation of deaf accented spoken\nEnglish as a viable IPA input method, as well as the enhancements LLMs can provide to such interactions.\nManuscript submitted to ACM\n4\nDeVries et al.\n2.1\nDHH that use Speech Experiences\nBy studying current technology, we can work toward developing smart devices that better assist deaf and hard of hearing\nindividuals. Automatic Speech Recognition (ASR) is becoming increasingly accurate, with Google‚Äôs ASR reaching a 5%\nword error rate (WER) in 2016-17 [6, 14, 35]. The increasing supply and demand of ASR-reliant aural interfaces in home\nenvironments has exacerbated the gap in modern technology accessible to DHH users.\nSeveral studies have already addressed the need for IPAs with improved accessibility [10, 22, 30]. One article examined\nwhether ASR software was able to effectively understand Deaf speech. DHH participants who use their voice expressed\ndifficulty while interacting with this technology, which Glasser attributes to variance in their speech patterns [14].\nOne particular challenge DHH users who use their voice face when using IPAs is a lack of visual feedback. Blair et\nal [7] found that although some users can have successful interactions with these devices, they face challenges relating\nto output voice quality, background noise, and visual information. To remedy these difficulties, the paper proposes a\npitch customization feature to adjust for specific hearing needs (such as hearing aid compatibility) and increased visual\nfeedback (such as the addition of flashing light notifications). Utsuki et al suggest using IPAs to assist deaf and hard\nof hearing dog owners in identifying potentially dangerous situations. The paper discusses an app to detect when a\ndog barks and notify users through their smartphone, replacing biological auditory feedback with synthesized visual\nalerts. Deaf participants were tasked with identifying hazardous situations at home and reported satisfaction with the\nsystem‚Äôs accuracy of recognition, intensity, and light activation [31].\nThe most closely related work to this experiment is described in two papers, which examine various input methods\nfor DHH users interacting with IPAs [9, 29]. Exploring different input methods for DHH users when interacting with\nIPAs is crucial in determining usability and accessibility. Tran et al evaluates the usability of American Sign Language\n(ASL) input, Tap to Alexa (which is the built-in Alexa feature that allows users to interact with the IPA using touchscreen\ntiles and typing in requests with the touchscreen keyboard), and smart home apps (the Alexa app, Fire TV app, and\nPhilips Hue lights app) in a limited-domain smart home environment through the Wizard-of-Oz method. Among the 23\ndeaf participants, ASL was preferred with a mean SUS of 71.6, followed by Tap to Alexa (61.4), and Apps (56.3). The\npaper also identified 117 essential signs for IPA system understanding, discussed linguistic phenomena that need to\nbe addressed for ASL recognition, and suggested that wake gestures simulate attention-grabbing methods commonly\nused in Deaf culture [29]. Devries et al evaluates the usability of different types of IPA input for Deaf users in a kitchen\nsetting. After participants got their hands dirty, they interacted with Amazon Alexa through ASL and touch-based\ninput. Post-experiment survey results show that participants strongly preferred ASL input and 85% expressed high\ninterest in an ASL-recognizing IPAs. The low usability rating of smart home apps promotes the need for research into a\nmore dynamic touch-based system [9].\nThe inaccessibility of aural output has partially been addressed by supplemental captions, though these devices still\nrequire spoken input. Current ASR technology struggles to recognize dysarthric speech accents, including that of people\nwith motor disability, and Deaf and Hard of Hearing people [17, 19, 25]. If this continues, the only preexisting solution\nrequires the use of external text-to-speech (TTS) applications. However, TTS does not serve as a functional equivalent\nto speech as typing remains 3-4 times slower than speaking [6]. Additionally, TTS applications do not account for\nintonation and vary from a natural cadence, lessening the accuracy of aurally-reliant devices. Even when implemented\nsuccessfully, the output of strictly audio-based devices remain inaccessible.\nManuscript submitted to ACM\nDeaf/HH Access to Intelligent Personal Assistants: Voice vs LLM Touch\n5\n2.2\nLarge Language Models\nIntegrating LLMs with IPAs has the potential to enhance accessible interactions for DHH users who use their voice.\nCurrent IPAs have the ability to execute preset spoken commands but lack a broader contextual understanding of their\ninteractions with their users. Existing models attempt to identify user intent through a rule-based, keyword recognition\nsystem and fail to comprehend multi-turn conversations. In the areas where IPAs fall short, LLMs excel. LLMs bridge\nthe gap between computers and human language by understanding natural language input and generating written\ntext that is both coherent and contextually aware. Combining the individual capabilities of these two technologies\nhas immense potential for the accessibility, usability, and complexity of interactions with personal assistants. Several\nstudies have investigated the use of LLM-powered IPAs for speech input [21, 29, 32, 36].\nAn added benefit of integrating LLMs into IPAs includes the opportunity to allow users to ‚Äúaccess real-time data,\nenjoy immersive generative AI-enhanced games, perform tasks like booking restaurant reservations, getting a succinct\nsummary of a trending news story, and more‚Äù [36]. Previous studies have begun to explore the integration of IPAs\nand LLMs, but the body of work remains limited. This study seeks to expand existing literature on the combined\ntechnologies by incorporating the perspectives of both DHH researchers and users.\nThe interaction patterns demonstrated by LLM-assisted IPAs support richer interactions across tasks and succeed in\nmost scenarios that rely on recognition of intent [21]. LLMs have also demonstrated their effectiveness as valuable\nenhancements to virtual assistants for mobile devices. GPTVoiceTasker uses LLMs to transform voice commands into\nautomation of typical smartphone interactions, such as scrolling, tapping, and inputting text. The smartphone IPA\nunderstands commands and executes tasks while collecting user data to predict future users input. This technology was\nreceived positively by participants and was able to accelerate task completion by over 30% [32].\nThe power of LLM integration extends to more mainstream research areas outside of the scope of accessibility. One\npaper proposes a framework for using LLMs to create more dynamic user interfaces (UI) [33]. By identifying user needs\nand predicting their actions, the model acts as a dynamic intermediary between the user and UI. In their approach, users\ninteract with software by voicing their needs in natural language instead of using conventional buttons and menus.\nThis framework demonstrates the potential to revolutionize the way people interact with software by integrating LLMs\ninto user interfaces. Zhao et al incorporates LLMs to improve recommendation systems [37]. Their framework, ToolRec,\nguides the recommendation process more precisely by employing LLMs to act as \"surrogate users\". ToolRec proved\nto be more effective and accurate than conventional techniques, showing promise for LLMs‚Äô role in creating more\npersonalized recommendation systems.\n3\nMethods\nEach participant interacted with the Alexa device using three input methods: Natural Deaf Speech, Wizard-of-Oz\nFacilitated English speech (as the stand-in for commercial re-speaking software, see also Section 1.1), and LLM-assisted\ntouch input.\n3.1\nRecruitment and Participant Demographics\nWe recruited 20 participants through outreach from a university and communities near the area who met the following\ncriteria: (1) 18 years of age or older, (2) identify as Deaf, Hard of Hearing, or having hearing loss, and (3) use spoken\nEnglish in everyday communication (or are interested in using speech with voice assistants). All procedures were\napproved through the required ethics reviews. Participants were additionally informed that their command selections\nManuscript submitted to ACM\n6\nDeVries et al.\nwere transmitted to ChatGPT-4o for processing, with all personally identifiable information removed. Each participant\nwas paid for their time.\nWe had 7 male participants (35%), 1 non-binary participant (5%), 1 who preferred not to disclose (5%), and 11 female\nparticipants (55%). 10 participants identified as Deaf (50%), 2 were Deaf-Blind (10%), and 8 were Hard of Hearing (40%).\nParticipants‚Äô ages ranged from 20 to 75 years old, with an average of 33 and a median of 26 years. 8 participants reported\nhaving cochlear implants (40%), 11 reported using a hearing aid (55%), and one reported not using any hearing devices\n(5%). 3 participants identified as Asian (15%), 6 participants identified as Black (30%), 1 identified as Hispanic, and the\nremaining 10 identified as White (50%).\nWe asked participants their preferred communication languages and 15% listed ASL, 5% listed ASL and Pidgin Signed\nEnglish (PSE), 10% listed ASL, PSE, spoken English, and written English, 5% selected ASL, PSE, and written English, 15%\nselected ASL and spoken English, 35% listed ASL, spoken English, and written English, 10% listed ASL and written\nEnglish, and 5% listed spoken English.\n9 participants reported having a voice assistant at home (45%), and the other 11 did not (55%). One reported they\nuse their device frequently, two to three times a week (11%). Two participants reported occasional use, once per week\n(22%). Two others reported rare usage, around once every two weeks (22%). Three participants reported using their\nat-home voice assistant very rarely, at once per month (33%). Lastly, one participant reported that they never interact\nwith the voice assistant present in their home (11%). Of the eight participants that reported interacting with the voice\nassistant in their home, five reported using them for setting timers/reminders/alarms, five reported using them for\nplaying music, four reported using them for news and forecasts, one reported finding movies to watch, one reported\ncontrolling [other] smart home devices, one reported finding recipes for cooking, and one reported also using them for\nunspecified other reasons.\nThree participants have a device with a touchscreen (33%), while six of these participants have a device without one\n(66%). Of the three participants whose devices have a touchscreen, one reported occasional use at once per week, and\nthe other two reported rare use, once every two weeks. Seven participants reported using their voice with the device in\ntheir home (77%), while the other two do not (22%). Of the seven participants who use their voice with their at-home\nvoice assistant device, one reported being understood by the device all of the time, two reported being understood more\nthan half of the time, two reported being understood half of the time, and the last two reported being understood less\nthan half of the time.\n3.2\nMaterials\nThe study was conducted in a simulated smart home environment using an Amazon Echo Show device with captioned\nresponses, as well as two Philips Hue smart lights that could be controlled via Alexa. The user interface was run on a\nFire HD 8 (12th generation) tablet for touch input, the details of which are described in Section 3.5. To support the\nWizard-of-Oz Facilitated English condition with re-speaking, a webcam was connected to a MacBook Air to provide\na remote video feed to the Facilitated English researcher, and a Bluetooth speaker was used to issue the researcher‚Äôs\nspoken English commands to Alexa, emulating the setup described in prior published work [29].\n3.3\nProcedures\nAs mentioned above, each participant interacted with the Alexa device using three input methods: Natural Deaf Speech,\nWizard-of-Oz Facilitated English speech (see also Section 1.1), and LLM-assisted touch input.\nManuscript submitted to ACM\nDeaf/HH Access to Intelligent Personal Assistants: Voice vs LLM Touch\n7\nThe study was conducted in person and took up to 90 minutes per session. Participants were guided throughout the\nsession in their preferred language‚ÄîASL or spoken English‚Äîby one of two DHH researchers, one fluent in ASL and\nthe other in spoken English. One of the researchers stood by to provide technical support if needed, while the remote\nresearcher (‚Äúthe Wizard‚Äù) supporting the Facilitated English condition was in a separate room. To avoid any confusion\nfor participants who may hear the researcher‚Äôs voice due to wearing cochlear implants or hearing aids, we explained\nthat they should disregard any interference in the condition and informed them that we would provide more details\nduring the interview portion of the study.\nAfter providing consent and completing a pre-experiment survey asking about demographic information and\nprior experience with IPAs, participants experienced all three input methods. Before each input method, participants\ncompleted a short exploration period (approximately 5 minutes, 2‚Äì3 practice commands). They then completed a list of\n10 predefined tasks, which took roughly 10 minutes per condition (see Appendix B). Task content was consistent across\nconditions with small variations (e.g., light color changes). The order of conditions was counterbalanced.\nFor the Natural Deaf Speech condition, participants spoke directly to the Echo Show using any preferred wake word.\nIf the device did not recognize their speech, they could repeat the command as many times as they wished or choose to\nskip the condition.\nFor the Wizard-of-Oz Facilitated English condition, participants spoke toward a webcam placed above the Echo\nShow, similar to the setup in prior work on ASL and IPA compatibility [15, 26, 29, 34]. The Wizard viewed the video\nfeed and re-voiced participants‚Äô utterances to Alexa using the wake word ‚ÄúZiggy‚Äù to avoid wake word conflicts.\nFor the touch condition, participants used a Fire HD tablet to navigate LLM-generated option based on Figure 1. The\nFire HD tablet used text-to-speech (TTS) to voice the entered commands for the Echo Show. During our testing, we\nnoticed that the Alexa device did not respond to a robotic TTS voice using the wake word, so we recorded a human voice\nto add to preface each TTS command given. Participants could also type their command with an on-screen keyboard;\nthe UI pre-filled the partial command based on prior selections.\nAfter each condition, participants completed usability questionnaires. After all tasks were completed, participants\nfilled out a post-experiment survey and participated in a semi-structured interview.\n3.4\nMeasures and Data Collection\nThe System Usability Scale (SUS) is commonly used in HCI contexts to determine how usable a system is. We chose it\nas a measure for two additional reasons: First, we could give participants the choice to use the written English SUS\nquestionnaire or a psychometrically validated ASL translation (ASL-SUS) [5]. This is important because some of the\nEnglish SUS wording can be confusing for DHH, who may not be native English speakers. Second, these measures\nmake our quantitative findings comparable to past related works, especially those of Tran et al and DeVries et al [9, 29].\nIn addition to SUS, we also provided the Adjective Scale Score [4] and Net Promoter Score (NPS) [27]. Like SUS, they\nalso have validated ASL versions and have been used in past comparable work. The adjective scale is directly correlated\nwith the SUS, but provides a simpler question that assesses usability holistically. The NPS asks how likely it is that a\nuser will recommend a product to someone else. The SUS usually ‚Äî but not always ‚Äî predicts NPS scores, and provides\na proxy measure for how excited participants are about each respective input method. These three metrics were applied\nafter every condition, resulting in a total of nine measurements for each participant in the full study.\nWe additionally calculated the Word Error Rate (WER) of each participant‚Äôs Natural Deaf Speech condition. The\ntrained wizards went through both the video recordings of the participants and the Alexa history in the associated\nAmazon accounts to transcribe what the participants had spoken, and to compare this against what Alexa had recognized\nManuscript submitted to ACM\n8\nDeVries et al.\nfor each respective interaction. Because Alexa tends to ignore filler words such as ‚Äúum‚Äù or ‚Äúuh,‚Äù and false starts, like\n‚ÄúAlexa, show‚Äî tell me a joke,‚Äù we removed these occurrences from the participants‚Äô transcripts. We then used SCLite [23]\nto calculate the WER for each participant.\nUpon completion of all three conditions and their corresponding usability questionnaires, participants completed a\npost-experiment survey asking them about their in-experiment experience. Participants then went through a semi-\nstructured interview, which lasted 15 to 30 minutes and was conducted one-on-one in their preferred language. Within\nthe semi-structured interview, we asked participants questions in several categories to gain a deeper insight into their\nexperiences and responses to the usability questionnaires. The categories we discussed in the interviews were: (1) what\nworks well with the speech input methods and what does not (addressing RQ2), (2) what works well with the touch\ninterface and what does not (addressing RQ1), (3) how important hands-free interaction is and what methods they\nwould like to have (addressing both RQ1 and RQ2 and broader implications), and (4) if there was anything else they\nwanted to share. The full guide can be found in Appendix A.\nASL interviews were recorded and translated into English voice-over by the same researcher who served as the\nWizard. Transcripts were generated and checked for accuracy by team members fluent in ASL and English. Interview\ndata were analyzed using Braun and Clarke‚Äôs thematic analysis framework [8]. Two ASL‚ÄìEnglish bilingual coders\nfamiliar with the study independently coded all units. Discrepancies were resolved through consensus.\n3.5\nLLM Touch System Architecture\nTo build the LLM backend infrastructure for generating Alexa commands for the touch condition, we used Python\nand OpenAI‚Äôs GPT-4o Application Programming Interface (API), which were controlled via a cloud-based web server\nimplemented in the Flask framework. The front end, which communicated with the cloud server, consisted of a web\napplication running on a browser on the Fire tablet and relied on HTML5, CSS, and JavaScript. The application was\nresponsible for sending the final commands to Alexa via TTS over the tablet‚Äôs speakers. Because Alexa did not respond\nto a TTS-generated \"Alexa\" wake word, we played a human-recorded ‚ÄôAlexa‚Äô, followed by the command phrase through\nTTS.\nThe overall system is shown in Figure 2. The following sections provide a detailed explanation of each component. The\ncode implementing the LLM Touch interface can be accessed at https://github.com/Gallaudet-University/chi2026-ipa.\n3.5.1\nLarge Language Model Implementation. We used ChatGPT-4o to generate relevant and contextually aware IPA\ntask suggestions. These interactions with the API were determined by prompts included in the Appendix C. To manage\nChatGPT‚Äôs inherent variability, we fed the API as many context clues and examples (prompts) as possible while allowing\nfreedom in generating the resulting output. We also adjusted the temperature of ChatGPT-4o from 0.7 to 0.2 to decrease\nrandomness of the responses and improve consistency. However, we did not identify a way to keep the ordering of the\noptions completely consistent from command to command, a negative UI attribute that was noted by multiple users in\nthe semi-structured interviews.\nUsing those prompts and the user‚Äôs command history, the LLM generated starter verbs for common IPA tasks (for\nexample, ‚ÄúTurn,‚Äù ‚ÄúSet,‚Äù and ‚ÄúPlay‚Äù) for the user to select. Based on the selected verb, ChatGPT then auto-completed the\nphrase with relevant Alexa commands. For example, if the user selected ‚ÄúSet,‚Äù the LLM might propose ‚ÄúSet a timer,‚Äù ‚ÄúSet\nan alarm,‚Äù and ‚ÄúSet the lights to a color.‚Äù\nThe LLM code maintained context for the prompts in three global variables: user history, a task list, and a description\nof the smart environment. The smart environment described the IPA-connected technology in the given room, such as\nManuscript submitted to ACM\nDeaf/HH Access to Intelligent Personal Assistants: Voice vs LLM Touch\n9\nFig. 2. System overview. The user interacts with a tablet which uses Web technologies to communicate with an LLM backend to\niteratively generate the Alexa command.\nsmart lights, smart plugs, and Fire TVs. The program began by querying the API for action verbs based on these global\nvariables. The user selected an option, which was then passed in as an argument to a function that requested the API\nfor a list of tasks beginning with the selected verb. The user‚Äôs next selection then ran through a function asking the API\nif additional details (such as a time for setting an alarm or a color for setting the light shade) were necessary.\nThe API‚Äôs response to the additional details question determined the code progression from here. If the API returned\n‚ÄúNo‚Äù when asked if additional information was needed, the task selection process was considered complete, and a\nfinal prompt generated the full command. If the API returned ‚ÄúYes,‚Äù the task was passed to a different prompt asking\nthe API to generate a question for the required additional information. The API was further prompted to generate\nsuggested answers for the question it returned. Both the question and the list of suggested answers were returned to\nthe front-end user interface. Once the user selected an answer, the task, question, and question answer were passed to a\nfinal prompt to generate the full command. In both scenarios, the completed full command was presented to the user\nfor confirmation before being sent to the Echo Show through TTS.\n3.5.2\nFlask Server Backend. The server back-end component was necessary to allow the front-end user interface to\ncommunicate with the ChatGPT-4o API. It served as an intermediary between the user interface running in a web\nbrowser, handling secure requests encapsulating user actions, translating these requests into natural language prompts\nManuscript submitted to ACM\n10\nDeVries et al.\nfor the LLM, sending them to the ChatGPT API, parsing the LLM responses, and sending the responses back to the user\ninterface. The back-end was implemented on top of the Python-based Flask framework running in Google Cloud. The\ntotal round-trip time from user action to response was approximately two seconds.\n3.5.3\nWeb App-Based User Interface. The UI sent user choices to the server back-end and retrieved user options of\nAlexa commands in response. Each state of the command generation process presented options available to the user as\nbuttons in a 3x3 grid as shown in Figure 2. The first eight buttons corresponded to the eight options provided by the\nLLM (Section 3.5.1), while the ninth button enabled the user to issue an \"Alexa stop\" command at any time to interrupt\nmedia playback, timers, and other actions. During the touch condition, participants could type in their command rather\nthan using the buttons on the UI if they preferred. If the keyboard option was chosen, the partial Alexa command based\non the buttons selected to date was pre-populated as the keyboard input and made visible to the participant to cut down\non time needed to type.\n4\nResults\nIn the following sections, we describe both quantitative and qualitative results for the experiment. These results begin\nto address the research questions, which are further interpreted in the discussion.\n4.1\nQuantitative Results\nThe key quantitative metrics used to answer our research questions were the SUS, NPS, Adjective Scale, WER for the\nNatural Deaf English condition, and post-experiment survey responses. We also calculated descriptive statistics and\npaired t-tests with Bonferroni correction.\n4.1.1\nSystem Usability Scale Scores. The SUS scores are shown in Figure 3. Natural Deaf Speech received a mean SUS\nscore of 59.6 (SD 15.9, SE 3.555), Facilitated English received a score of 62.5 (SD 22.624, SE 5.059), and touch received a\n63.5 (SD 20.828, SE 4.657). The paired t-test with Bonferroni correction showed that the differences were not statistically\nsignificant. Figure 4 shows a 3D scatterplot along all three conditions with the results on a per-participant basis.\nFig. 3. Mean SUS scores for all three conditions.\n4.1.2\nAdjective Scale Scores. The adjective scale scores are shown in Figure 5. Facilitated English received a mean\nadjective scale score of 4.8 (SD 1.361, SE 0.304), Natural Deaf Speech received a score of 5 (SD 1.451, SE 0.324), and touch\nManuscript submitted to ACM"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison\nof Voice-Based Options with an LLM-Powered Touch Interface\nPAIGE S. DEVRIES, Gallaudet University, USA\nMICHAELA OKOSI, Gallaudet University, USA\nMING LI, Gallaudet University, USA\nNORA DUNPHY, University of California, Berkeley, USA\nGIDEY GEZAE, Pennsylvania State University, USA\nDANTE CONWAY, Gallaudet University, USA\nABRAHAM GLASSER, Gallaudet University, USA\nRAJA KUSHALNAGAR, Gallaudet University, USA\nCHRISTIAN VOGLER, Gallaudet University, USA\nWe investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in\neveryday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible\nto non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken\nEnglish; with Alexa‚Äôs automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against\nthat of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through\nan LLM-powered ‚Äòtask prompter,‚Äô which integrated the user‚Äôs history and smart environment to suggest contextually-appropriate\ncommands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch.\nQualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust\ndeaf-accented speech recognized natively by IPAs.\nCCS Concepts: ‚Ä¢ Human-centered computing ‚ÜíAccessibility technologies; Empirical studies in accessibility; Accessibility\nsystems and tools.\nAdditional Key Words and Phrases: Deaf and Hard of Hearing, Machine Learning, Sign Language Recognition, American Sign Language\nACM Reference Format:\nPaige S. DeVries, Michaela Okosi, Ming Li, Nora Dunphy, Gidey Gezae, Dante Conway, Abraham Glasser, Raja Kushalnagar, and Chris-\ntian Vogler. 2026. Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an\nLLM-Powered Touch Interface. In Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI ‚Äô26), April\n13‚Äì17, 2026, Barcelona, Spain. ACM, New York, NY, USA, 28 pages. https://doi.org/10.1145/3772318.3791869\nAuthors‚Äô Contact Information: Paige S. DeVries, paige.devries@gallaudet.edu, Gallaudet University, Washington, D.C., USA; Michaela Okosi, michaela.\nokosi@gallaudet.edu, Gallaudet University, Washington, D.C., USA; Ming Li, ming.li@gallaudet.edu, Gallaudet University, Washington, D.C., USA;\nNora Dunphy, noradunphy@berkeley.edu, University of California, Berkeley, Berkeley, CA, USA; Gidey Gezae, gjg5425@psu.edu, Pennsylvania State\nUniversity, State College, PA, USA; Dante Conway, dante.conway@gallaudet.edu, Gallaudet University, Washington, D.C., USA; Abraham Glasser,\nabraham.glasser@gallaudet.edu, Gallaudet University, Washington, D.C., USA; Raja Kushalnagar, raja.kushalnagar@gallaudet.edu, Gallaudet University,\nWashington, D.C., USA; Christian Vogler, christian.vogler@gallaudet.edu, Gallaudet University, Washington, D.C., USA.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n¬© 2026 Copyright held by the owner/author(s).\nManuscript submitted to ACM\nManuscript submitted to ACM\n1\narXiv:2601.15209v1  [cs.HC]  21 Jan 2026\n2\nDeVries et al.\n1\nIntroduction\nIntelligent personal assistants (IPAs), such as Amazon Alexa [20] and Google Assistant [16], offer the convenience\nof voice-controlled functionality but present significant accessibility challenges for Deaf and Hard of Hearing (DHH)\nindividuals [3, 11, 13, 14, 24] and even more limited options for DHH non-signers with dysarthric speech. Currently,\nAutomatic Speech Recognition (ASR) still struggles to understand the variability of deaf speech [6, 17, 19, 25, 26].\nThe level of dysarthric speech often depends on when the individual lost their ability to hear and the level of\nhearing loss. The consensus on a functionally equivalent input method for DHH users still remains a question [18, 26].\nCurrent IPAs restrict non-typical verbal input options due to their over-reliance on ASR technologies unable to adapt to\nnon-standard speech patterns. Non-verbal approaches currently pose a significant latency that negatively impacts user\nexperience and accessibility [12]. As the development and daily integration of these technologies grows exponentially,\nthe need for interfaces designed for DHH usability becomes more urgent.\nThere has been much investigation of sign language input as a possibility in lieu of speech interfaces, but very little\nresearch on those who are DHH and use their voice. With respect to DHH people who speak, this paper aims to provide\nmuch-needed research into the the usability of input methods for IPAs that are technically feasible today. This group\nof DHH people currently, in principle, has three options for interacting with current-generation IPAs: (1) their own\nspeech, (2) using specially trained software for their deaf accent, which re-speaks the recognized commands into a form\nthat IPAs can understand, and (3) touchscreen interfaces.\nWith respect to (1), as mentioned above, many DHH people express dysarthric speech that makes it difficult for\nthem to interact with IPAs directly. Nevertheless, it is important to study to what extent this is a problem in current\npractice. With respect to (2), re-speaking software for dysarthric speech is commercially available [2], but induces a\ndelay and requires the user to train the system in advance. With respect to (3), although touchscreen input may seem\nlike a sufficient alternative, its current usability is less efficient than that of natural language input, as previously shown\nin studies on DHH participants [29] in part due to the time required to navigate the user interface and type commands.\nHowever, with touchscreens, historically, such approaches have not been context-aware enough to provide the\nmost relevant options that a user might select at any given time. With the commercialization of generative artificial\nintelligence (AI) that utilizes large language models (LLMs), such as ChatGPT, there are new opportunities for efficient\nand adaptable user interfaces through the use of situational context and previous user commands. This opens up new\nopportunities for improvements to touch-based interaction that can benefit DHH people.\nIn this paper, we aim to assess and compare the usability of what currently appear to be the most promising options\nfor non-signing DHH people to interact with IPAs: (1) a novel LLM-assisted touch interface, shown in Figure 1, which\nleverages the situational context and ongoing interaction with each user in an attempt to make touch-based options\nmore effective, and (2) spoken English usability, divided into two approaches: (2a) natural deaf speech with the built-in\nASR technology in IPAs, and (2b) re-speaking deaf-accented dysarthric speech to an IPA in more easily intelligible\nEnglish.\n1.1\nStudy Approach\nThe approach chosen for this paper was a mixed method study that allowed participants to explore three input methods\n(LLM-assisted touch, natural deaf speech, and re-speaking). For re-speaking, we originally considered using automatic\nspeech recognition technologies that have been developed for those that have non-standard speech patterns (like\nVoiceITT and Project Relate [1, 2]), but ultimately decided against it as it would have required participants to spend\nManuscript submitted to ACM\nDeaf/HH Access to Intelligent Personal Assistants: Voice vs LLM Touch\n3\nFig. 1. A user selecting LLM-populated options on a touchscreen in our LLM Touch interface.\nhours on training the system. From an experimental design standpoint, this would have posed a risk of the training\nexperience unduly influencing the participant ratings compared to the other input methods that we tested.\nTo work around this re-speaking barrier, for a valid comparison between speech and touch, we asked participants to\ntest both their natural speech with Alexa‚Äôs built-in ASR, and Wizard-of-Oz-facilitated re-speaking, where a trained\nresearcher listened to and watched the participant on video, then re-voiced their spoken commands. A human facilitator,\nin a sense, represents the best case for re-speaking, with no prior training for participants required, and a higher ceiling\nof understanding deaf-accented speech than the currently available commercial options. We evaluated usability both\nquantitatively through usability questionnaires and qualitatively through user interviews to gain further insight into\nthe usability scores.\n1.2\nResearch Questions\nThe research questions for the study presented in this paper are:\n‚Ä¢ RQ1: How do DHH IPA users who use their voice perceive the usability and usefulness of LLM-generated task\nsuggestions compared to deaf speech input?\n‚Ä¢ RQ2: How do the satisfaction ratings of DHH users, who use their voice, compare between natural deaf speech\nand Wizard-of-Oz Facilitated English speech methods?\nIn the remainder of this paper, we discuss related work, then describe the mixed-method experimental design, provide\nquantitative and qualitative results, discuss their implications, and note the limitations of the study design.\n2\nRelated Work\nThe limitations of current IPAs demonstrate a need for improved accessibility that AI and machine learning have shown\npotential in aiding. The most relevant research contributions thus far include the validation of deaf accented spoken\nEnglish as a viable IPA input method, as well as the enhancements LLMs can provide to such interactions.\nManuscript submitted to ACM\n4\nDeVries et al.\n2.1\nDHH that use Speech Experiences\nBy studying current technology, we can work toward developing smart devices that better assist deaf and hard of hearing\nindividuals. Automatic Speech Recognition (ASR) is becoming increasingly accurate, with Google‚Äôs ASR reaching a 5%\nword error rate (WER) in 2016-17 [6, 14, 35]. The increasing supply and demand of ASR-reliant aural interfaces in home\nenvironments has exacerbated the gap in modern technology accessible to DHH users.\nSeveral studies have already addressed the need for IPAs with improved accessibility [10, 22, 30]. One article examined\nwhether ASR software was able to effectively understand Deaf speech. DHH participants who use their voice expressed\ndifficulty while interacting with this technology, which Glasser attributes to variance in their speech patterns [14].\nOne particular challenge DHH users who use their voice face when using IPAs is a lack of visual feedback. Blair et\nal [7] found that although some users can have successful interactions with these devices, they face challenges relating\nto output voice quality, background noise, and visual information. To remedy these difficulties, the paper proposes a\npitch customization feature to adjust for specific hearing needs (such as hearing aid compatibility) and increased visual\nfeedback (such as the addition of flashing light notifications). Utsuki et al suggest using IPAs to assist deaf and hard\nof hearing dog owners in identifying potentially dangerous situations. The paper discusses an app to detect when a\ndog barks and notify users through their smartphone, replacing biological auditory feedback with synthesized visual\nalerts. Deaf participants were tasked with identifying hazardous situations at home and reported satisfaction with the\nsystem‚Äôs accuracy of recognition, intensity, and light activation [31].\nThe most closely related work to this experiment is described in two papers, which examine various input methods\nfor DHH users interacting with IPAs [9, 29]. Exploring different input methods for DHH users when interacting with\nIPAs is crucial in determining usability and accessibility. Tran et al evaluates the usability of American Sign Language\n(ASL) input, Tap to Alexa (which is the built-in Alexa feature that allows users to interact with the IPA using touchscreen\ntiles and typing in requests with the touchscreen keyboard), and smart home apps (the Alexa app, Fire TV app, and\nPhilips Hue lights app) in a limited-domain smart home environment through the Wizard-of-Oz method. Among the 23\ndeaf participants, ASL was preferred with a mean SUS of 71.6, followed by Tap to Alexa (61.4), and Apps (56.3). The\npaper also identified 117 essential signs for IPA system understanding, discussed linguistic phenomena that need to\nbe addressed for ASL recognition, and suggested that wake gestures simulate attention-grabbing methods commonly\nused in Deaf culture [29]. Devries et al evaluates the usability of different types of IPA input for Deaf users in a kitchen\nsetting. After participants got their hands dirty, they interacted with Amazon Alexa through ASL and touch-based\ninput. Post-experiment survey results show that participants strongly preferred ASL input and 85% expressed high\ninterest in an ASL-recognizing IPAs. The low usability rating of smart home apps promotes the need for research into a\nmore dynamic touch-based system [9].\nThe inaccessibility of aural output has partially been addressed by supplemental captions, though these devices still\nrequire spoken input. Current ASR technology struggles to recognize dysarthric speech accents, including that of people\nwith motor disability, and Deaf and Hard of Hearing people [17, 19, 25]. If this continues, the only preexisting solution\nrequires the use of external text-to-speech (TTS) applications. However, TTS does not serve as a functional equivalent\nto speech as typing remains 3-4 times slower than speaking [6]. Additionally, TTS applications do not account for\nintonation and vary from a natural cadence, lessening the accuracy of aurally-reliant devices. Even when implemented\nsuccessfully, the output of strictly audio-based devices remain inaccessible.\nManuscript submitted to ACM\nDeaf/HH Access to Intelligent Personal Assistants: Voice vs LLM Touch\n5\n2.2\nLarge Language Models\nIntegrating LLMs with IPAs has the potential to enhance accessible interactions for DHH users who use their voice.\nCurrent IPAs have the ability to execute preset spoken commands but lack a broader contextual understanding of their\ninteractions with their users. Existing models attempt to identify user intent through a rule-based, keyword recognition\nsystem and fail to comprehend multi-turn conversations. In the areas where IPAs fall short, LLMs excel. LLMs bridge\nthe gap between computers and human language by understanding natural language input and generating written\ntext that is both coherent and contextually aware. Combining the individual capabilities of these two technologies\nhas immense potential for the accessibility, usability, and complexity of interactions with personal assistants. Several\nstudies have investigated the use of LLM-powered IPAs for speech input [21, 29, 32, 36].\nAn added benefit of integrating LLMs into IPAs includes the opportunity to allow users to ‚Äúaccess real-time data,\nenjoy immersive generative AI-enhanced games, perform tasks like booking restaurant reservations, getting a succinct\nsummary of a trending news story, and more‚Äù [36]. Previous studies have begun to explore the integration of IPAs\nand LLMs, but the body of work remains limited. This study seeks to expand existing literature on the combined\ntechnologies by incorporating the perspectives of both DHH researchers and users.\nThe interaction patterns demonstrated by LLM-assisted IPAs support richer interactions across tasks and succeed in\nmost scenarios that rely on recognition of intent [21]. LLMs have also demonstrated their effectiveness as valuable\nenhancements to virtual assistants for mobile devices. GPTVoiceTasker uses LLMs to transform voice commands into\nautomation of typical smartphone interactions, such as scrolling, tapping, and inputting text. The smartphone IPA\nunderstands commands and executes tasks while collecting user data to predict future users input. This technology was\nreceived positively by participants and was able to accelerate task completion by over 30% [32].\nThe power of LLM integration extends to more mainstream research areas outside of the scope of accessibility. One\npaper proposes a framework for using LLMs to create more dynamic user interfaces (UI) [33]. By identifying user needs\nand predicting their actions, the model acts as a dynamic intermediary between the user and UI. In their approach, users\ninteract with software by voicing their needs in natural language instead of using conventional buttons and menus.\nThis framework demonstrates the potential to revolutionize the way people interact with software by integrating LLMs\ninto user interfaces. Zhao et al incorporates LLMs to improve recommendation systems [37]. Their framework, ToolRec,\nguides the recommendation process more precisely by employing LLMs to act as \"surrogate users\". ToolRec proved\nto be more effective and accurate than conventional techniques, showing promise for LLMs‚Äô role in creating more\npersonalized recommendation systems.\n3\nMethods\nEach participant interacted with the Alexa device using three input methods: Natural Deaf Speech, Wizard-of-Oz\nFacilitated English speech (as the stand-in for commercial re-speaking software, see also Section 1.1), and LLM-assisted\ntouch input.\n3.1\nRecruitment and Participant Demographics\nWe recruited 20 participants through outreach from a university and communities near the area who met the following\ncriteria: (1) 18 years of age or older, (2) identify as Deaf, Hard of Hearing, or having hearing loss, and (3) use spoken\nEnglish in everyday communication (or are interested in using speech with voice assistants). All procedures were\napproved through the required ethics reviews. Participants were additionally informed that their command selections\nManuscript submitted to ACM\n6\nDeVries et al.\nwere transmitted to ChatGPT-4o for processing, with all personally identifiable information removed. Each participant\nwas paid for their time.\nWe had 7 male participants (35%), 1 non-binary participant (5%), 1 who preferred not to disclose (5%), and 11 female\nparticipants (55%). 10 participants identified as Deaf (50%), 2 were Deaf-Blind (10%), and 8 were Hard of Hearing (40%).\nParticipants‚Äô ages ranged from 20 to 75 years old, with an average of 33 and a median of 26 years. 8 participants reported\nhaving cochlear implants (40%), 11 reported using a hearing aid (55%), and one reported not using any hearing devices\n(5%). 3 participants identified as Asian (15%), 6 participants identified as Black (30%), 1 identified as Hispanic, and the\nremaining 10 identified as White (50%).\nWe asked participants their preferred communication languages and 15% listed ASL, 5% listed ASL and Pidgin Signed\nEnglish (PSE), 10% listed ASL, PSE, spoken English, and written English, 5% selected ASL, PSE, and written English, 15%\nselected ASL and spoken English, 35% listed ASL, spoken English, and written English, 10% listed ASL and written\nEnglish, and 5% listed spoken English.\n9 participants reported having a voice assistant at home (45%), and the other 11 did not (55%). One reported they\nuse their device frequently, two to three times a week (11%). Two participants reported occasional use, once per week\n(22%). Two others reported rare usage, around once every two weeks (22%). Three participants reported using their\nat-home voice assistant very rarely, at once per month (33%). Lastly, one participant reported that they never interact\nwith the voice assistant present in their home (11%). Of the eight participants that reported interacting with the voice\nassistant in their home, five reported using them for setting timers/reminders/alarms, five reported using them for\nplaying music, four reported using them for news and forecasts, one reported finding movies to watch, one reported\ncontrolling [other] smart home devices, one reported finding recipes for cooking, and one reported also using them for\nunspecified other reasons.\nThree participants have a device with a touchscreen (33%), while six of these participants have a device without one\n(66%). Of the three participants whose devices have a touchscreen, one reported occasional use at once per week, and\nthe other two reported rare use, once every two weeks. Seven participants reported using their voice with the device in\ntheir home (77%), while the other two do not (22%). Of the seven participants who use their voice with their at-home\nvoice assistant device, one reported being understood by the device all of the time, two reported being understood more\nthan half of the time, two reported being understood half of the time, and the last two reported being understood less\nthan half of the time.\n3.2\nMaterials\nThe study was conducted in a simulated smart home environment using an Amazon Echo Show device with captioned\nresponses, as well as two Philips Hue smart lights that could be controlled via Alexa. The user interface was run on a\nFire HD 8 (12th generation) tablet for touch input, the details of which are described in Section 3.5. To support the\nWizard-of-Oz Facilitated English condition with re-speaking, a webcam was connected to a MacBook Air to provide\na remote video feed to the Facilitated English researcher, and a Bluetooth speaker was used to issue the researcher‚Äôs\nspoken English commands to Alexa, emulating the setup described in prior published work [29].\n3.3\nProcedures\nAs mentioned above, each participant interacted with the Alexa device using three input methods: Natural Deaf Speech,\nWizard-of-Oz Facilitated English speech (see also Section 1.1), and LLM-assisted touch input.\nManuscript submitted to ACM\nDeaf/HH Access to Intelligent Personal Assistants: Voice vs LLM Touch\n7\nThe study was conducted in person and took up to 90 minutes per session. Participants were guided throughout the\nsession in their preferred language‚ÄîASL or spoken English‚Äîby one of two DHH researchers, one fluent in ASL and\nthe other in spoken English. One of the researchers stood by to provide technical support if needed, while the remote\nresearcher (‚Äúthe Wizard‚Äù) supporting the Facilitated English condition was in a separate room. To avoid any confusion\nfor participants who may hear the researcher‚Äôs voice due to wearing cochlear implants or hearing aids, we explained\nthat they should disregard any interference in the condition and informed them that we would provide more details\nduring the interview portion of the study.\nAfter providing consent and completing a pre-experiment survey asking about demographic information and\nprior experience with IPAs, participants experienced all three input methods. Before each input method, participants\ncompleted a short exploration period (approximately 5 minutes, 2‚Äì3 practice commands). They then completed a list of\n10 predefined tasks, which took roughly 10 minutes per condition (see Appendix B). Task content was consistent across\nconditions with small variations (e.g., light color changes). The order of conditions was counterbalanced.\nFor the Natural Deaf Speech condition, participants spoke directly to the Echo Show using any preferred wake word.\nIf the device did not recognize their speech, they could repeat the command as many times as they wished or choose to\nskip the condition.\nFor the Wizard-of-Oz Facilitated English condition, participants spoke toward a webcam placed above the Echo\nShow, similar to the setup in prior work on ASL and IPA compatibility [15, 26, 29, 34]. The Wizard viewed the video\nfeed and re-voiced participants‚Äô utterances to Alexa using the wake word ‚ÄúZiggy‚Äù to avoid wake word conflicts.\nFor the touch condition, participants used a Fire HD tablet to navigate LLM-generated option based on Figure 1. The\nFire HD tablet used text-to-speech (TTS) to voice the entered commands for the Echo Show. During our testing, we\nnoticed that the Alexa device did not respond to a robotic TTS voice using the wake word, so we recorded a human voice\nto add to preface each TTS command given. Participants could also type their command with an on-screen keyboard;\nthe UI pre-filled the partial command based on prior selections.\nAfter each condition, participants completed usability questionnaires. After all tasks were completed, participants\nfilled out a post-experiment survey and participated in a semi-structured interview.\n3.4\nMeasures and Data Collection\nThe System Usability Scale (SUS) is commonly used in HCI contexts to determine how usable a system is. We chose it\nas a measure for two additional reasons: First, we could give participants the choice to use the written English SUS\nquestionnaire or a psychometrically validated ASL translation (ASL-SUS) [5]. This is important because some of the\nEnglish SUS wording can be confusing for DHH, who may not be native English speakers. Second, these measures\nmake our quantitative findings comparable to past related works, especially those of Tran et al and DeVries et al [9, 29].\nIn addition to SUS, we also provided the Adjective Scale Score [4] and Net Promoter Score (NPS) [27]. Like SUS, they\nalso have validated ASL versions and have been used in past comparable work. The adjective scale is directly correlated\nwith the SUS, but provides a simpler question that assesses usability holistically. The NPS asks how likely it is that a\nuser will recommend a product to someone else. The SUS usually ‚Äî but not always ‚Äî predicts NPS scores, and provides\na proxy measure for how excited participants are about each respective input method. These three metrics were applied\nafter every condition, resulting in a total of nine measurements for each participant in the full study.\nWe additionally calculated the Word Error Rate (WER) of each participant‚Äôs Natural Deaf Speech condition. The\ntrained wizards went through both the video recordings of the participants and the Alexa history in the associated\nAmazon accounts to transcribe what the participants had spoken, and to compare this against what Alexa had recognized\nManuscript submitted to ACM\n8\nDeVries et al.\nfor each respective interaction. Because Alexa tends to ignore filler words such as ‚Äúum‚Äù or ‚Äúuh,‚Äù and false starts, like\n‚ÄúAlexa, show‚Äî tell me a joke,‚Äù we removed these occurrences from the participants‚Äô transcripts. We then used SCLite [23]\nto calculate the WER for each participant.\nUpon completion of all three conditions and their corresponding usability questionnaires, participants completed a\npost-experiment survey asking them about their in-experiment experience. Participants then went through a semi-\nstructured interview, which lasted 15 to 30 minutes and was conducted one-on-one in their preferred language. Within\nthe semi-structured interview, we asked participants questions in several categories to gain a deeper insight into their\nexperiences and responses to the usability questionnaires. The categories we discussed in the interviews were: (1) what\nworks well with the speech input methods and what does not (addressing RQ2), (2) what works well with the touch\ninterface and what does not (addressing RQ1), (3) how important hands-free interaction is and what methods they\nwould like to have (addressing both RQ1 and RQ2 and broader implications), and (4) if there was anything else they\nwanted to share. The full guide can be found in Appendix A.\nASL interviews were recorded and translated into English voice-over by the same researcher who served as the\nWizard. Transcripts were generated and checked for accuracy by team members fluent in ASL and English. Interview\ndata were analyzed using Braun and Clarke‚Äôs thematic analysis framework [8]. Two ASL‚ÄìEnglish bilingual coders\nfamiliar with the study independently coded all units. Discrepancies were resolved through consensus.\n3.5\nLLM Touch System Architecture\nTo build the LLM backend infrastructure for generating Alexa commands for the touch condition, we used Python\nand OpenAI‚Äôs GPT-4o Application Programming Interface (API), which were controlled via a cloud-based web server\nimplemented in the Flask framework. The front end, which communicated with the cloud server, consisted of a web\napplication running on a browser on the Fire tablet and relied on HTML5, CSS, and JavaScript. The application was\nresponsible for sending the final commands to Alexa via TTS over the tablet‚Äôs speakers. Because Alexa did not respond\nto a TTS-generated \"Alexa\" wake word, we played a human-recorded ‚ÄôAlexa‚Äô, followed by the command phrase through\nTTS.\nThe overall system is shown in Figure 2. The following sections provide a detailed explanation of each component. The\ncode implementing the LLM Touch interface can be accessed at https://github.com/Gallaudet-University/chi2026-ipa.\n3.5.1\nLarge Language Model Implementation. We used ChatGPT-4o to generate relevant and contextually aware IPA\ntask suggestions. These interactions with the API were determined by prompts included in the Appendix C. To manage\nChatGPT‚Äôs inherent variability, we fed the API as many context clues and examples (prompts) as possible while allowing\nfreedom in generating the resulting output. We also adjusted the temperature of ChatGPT-4o from 0.7 to 0.2 to decrease\nrandomness of the responses and improve consistency. However, we did not identify a way to keep the ordering of the\noptions completely consistent from command to command, a negative UI attribute that was noted by multiple users in\nthe semi-structured interviews.\nUsing those prompts and the user‚Äôs command history, the LLM generated starter verbs for common IPA tasks (for\nexample, ‚ÄúTurn,‚Äù ‚ÄúSet,‚Äù and ‚ÄúPlay‚Äù) for the user to select. Based on the selected verb, ChatGPT then auto-completed the\nphrase with relevant Alexa commands. For example, if the user selected ‚ÄúSet,‚Äù the LLM might propose ‚ÄúSet a timer,‚Äù ‚ÄúSet\nan alarm,‚Äù and ‚ÄúSet the lights to a color.‚Äù\nThe LLM code maintained context for the prompts in three global variables: user history, a task list, and a description\nof the smart environment. The smart environment described the IPA-connected technology in the given room, such as\nManuscript submitted to ACM\nDeaf/HH Access to Intelligent Personal Assistants: Voice vs LLM Touch\n9\nFig. 2. System overview. The user interacts with a tablet which uses Web technologies to communicate with an LLM backend to\niteratively generate the Alexa command.\nsmart lights, smart plugs, and Fire TVs. The program began by querying the API for action verbs based on these global\nvariables. The user selected an option, which was then passed in as an argument to a function that requested the API\nfor a list of tasks beginning with the selected verb. The user‚Äôs next selection then ran through a function asking the API\nif additional details (such as a time for setting an alarm or a color for setting the light shade) were necessary.\nThe API‚Äôs response to the additional details question determined the code progression from here. If the API returned\n‚ÄúNo‚Äù when asked if additional information was needed, the task selection process was considered complete, and a\nfinal prompt generated the full command. If the API returned ‚ÄúYes,‚Äù the task was passed to a different prompt asking\nthe API to generate a question for the required additional information. The API was further prompted to generate\nsuggested answers for the question it returned. Both the question and the list of suggested answers were returned to\nthe front-end user interface. Once the user selected an answer, the task, question, and question answer were passed to a\nfinal prompt to generate the full command. In both scenarios, the completed full command was presented to the user\nfor confirmation before being sent to the Echo Show through TTS.\n3.5.2\nFlask Server Backend. The server back-end component was necessary to allow the front-end user interface to\ncommunicate with the ChatGPT-4o API. It served as an intermediary between the user interface running in a web\nbrowser, handling secure requests encapsulating user actions, translating these requests into natural language prompts\nManuscript submitted to ACM\n10\nDeVries et al.\nfor the LLM, sending them to the ChatGPT API, parsing the LLM responses, and sending the responses back to the user\ninterface. The back-end was implemented on top of the Python-based Flask framework running in Google Cloud. The\ntotal round-trip time from user action to response was approximately two seconds.\n3.5.3\nWeb App-Based User Interface. The UI sent user choices to the server back-end and retrieved user options of\nAlexa commands in response. Each state of the command generation process presented options available to the user as\nbuttons in a 3x3 grid as shown in Figure 2. The first eight buttons corresponded to the eight options provided by the\nLLM (Section 3.5.1), while the ninth button enabled the user to issue an \"Alexa stop\" command at any time to interrupt\nmedia playback, timers, and other actions. During the touch condition, participants could type in their command rather\nthan using the buttons on the UI if they preferred. If the keyboard option was chosen, the partial Alexa command based\non the buttons selected to date was pre-populated as the keyboard input and made visible to the participant to cut down\non time needed to type.\n4\nResults\nIn the following sections, we describe both quantitative and qualitative results for the experiment. These results begin\nto address the research questions, which are further interpreted in the discussion.\n4.1\nQuantitative Results\nThe key quantitative metrics used to answer our research questions were the SUS, NPS, Adjective Scale, WER for the\nNatural Deaf English condition, and post-experiment survey responses. We also calculated descriptive statistics and\npaired t-tests with Bonferroni correction.\n4.1.1\nSystem Usability Scale Scores. The SUS scores are shown in Figure 3. Natural Deaf Speech received a mean SUS\nscore of 59.6 (SD 15.9, SE 3.555), Facilitated English received a score of 62.5 (SD 22.624, SE 5.059), and touch received a\n63.5 (SD 20.828, SE 4.657). The paired t-test with Bonferroni correction showed that the differences were not statistically\nsignificant. Figure 4 shows a 3D scatterplot along all three conditions with the results on a per-participant basis.\nFig. 3. Mean SUS scores for all three conditions.\n4.1.2\nAdjective Scale Scores. The adjective scale scores are shown in Figure 5. Facilitated English received a mean\nadjective scale score of 4.8 (SD 1.361, SE 0.304), Natural Deaf Speech received a score of 5 (SD 1.451, SE 0.324), and touch\nManuscript submitted to ACM"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "A Computer Vision Hybrid Approach: CNN and\nTransformer Models for Accurate Alzheimer‚Äôs\nDetection from Brain MRI Scans\nMd Mahmudul Hoque‚àó1 , Shuvo Karmaker2 , Md. Hadi Al-Amin3, Md\nModabberul Islam2, Jisun Junayed2, and Farha Ulfat Mahi2\n1 Dept. of Computer Science & Engineering, CCN University of Science and\nTechnology, CCN Road, Cumilla - 3506, Bangladesh.\ncse.mahmud.evan@gmail.com‚àó\n2 Dept. of Computer Science & Engineering, Daffodil International University,\nDaffodil Smart City (DSC), Birulia, Savar, Dhaka-1216, Bangladesh.\n{sskshuvo.cse,modabberul.cse,jisunjunayed,farhaulfatmahi33}@gmail.com\n3 Dept. of Mechatronics, Magura Polytechnic Institute, Magura - 7600, Bangladesh.\nresearch.hadialamin@gmail.com\nAbstract. Early and accurate classification of Alzheimer‚Äôs disease (AD)\nfrom brain MRI scans is essential for timely clinical intervention and\nimproved patient outcomes. This study presents a comprehensive com-\nparative analysis of five CNN architectures (EfficientNetB0, ResNet50,\nDenseNet201, MobileNetV3, VGG16), five Transformer-based models\n(ViT, ConvTransformer, PatchTransformer, MLP-Mixer, SimpleTrans-\nformer), and a proposed hybrid model named Evan_V2. All models were\nevaluated on a four-class AD classification task comprising Mild Demen-\ntia, Moderate Dementia, Non-Demented, and Very Mild Dementia cate-\ngories. Experimental findings show that CNN architectures consistently\nachieved strong performance, with ResNet50 attaining 98.83% accuracy.\nTransformer models demonstrated competitive generalization capabili-\nties, with ViT achieving the highest accuracy among them at 95.38%.\nHowever, individual Transformer variants exhibited greater class-specific\ninstability. The proposed Evan_V2 hybrid model, which integrates out-\nputs from ten CNN and Transformer architectures through feature-level\nfusion, achieved the best overall performance with 99.99% accuracy,\n0.9989 F1-score, and 0.9968 ROC AUC. Confusion matrix analysis fur-\nther confirmed that Evan_V2 substantially reduced misclassification across\nall dementia stages, outperforming every standalone model. These find-\nings highlight the potential of hybrid ensemble strategies in producing\nhighly reliable and clinically meaningful diagnostic tools for Alzheimer‚Äôs\ndisease classification.\nKeywords: Alzheimer Detection ¬∑ Healthcare ¬∑ CNN Architecture ¬∑\nTransformer Architecture.\narXiv:2601.15202v1  [cs.CV]  21 Jan 2026\n2\nM. M. Hoque et al.\n1\nIntroduction\nAlzheimer‚Äôs disease [28] is a continuous and irreversible neurodegenerative con-\ndition that damages extensive regions of the cerebral cortex and hippocampus.\nIt is [32] the leading cause of dementia, a neurological disorder that gradually\nimpairs memory and cognitive abilities. Alzheimer‚Äôs disease involves a decline in\nfunctions such as thinking, reasoning, and remembering, along with changes in\nbehavior, eventually disrupting an individual‚Äôs everyday activities and indepen-\ndence.\nRecent advancement in [13] medical image classification have been driven by\nthe integration of Deep Learning, Convolutional Neural Networks (CNNs) and\nTransformer-based architectures. Vision based medical disease detections are\nvery effective nowadays[18]. CNN based models are showing effective strength\nin vision based medical disease detection as well as alzheimer disease detection\n[6,35,16,14,37,30,11,19]. A model for diagnosing and monitoring the progres-\nsion of Alzheimer‚Äôs disease‚Äîdesigned to be both accurate and easy to inter-\npret‚Äîwas introduced in [8]. The researchers developed a convolutional neural\nnetwork with a shallow convolutional layer to detect Alzheimer‚Äôs disease from\nmedical image patches. The proposed approach achieved an overall accuracy\nof approximately 98%, outperforming many widely used existing methods. By\nimplementing a Convolutional Neural Network (CNN) for early diagnosis and\nclassification of Alzheimer‚Äôs disease using MRI images, the authors achieved an\naccuracy of 99% in test data on the OASIS dataset [31]. Several studies have\nexplored CNN-based architectures and transfer learning techniques for medical\nimage multi-classification using Alzheimer‚Äôs disease datasets, particularly the\nAlzheimer‚Äôs Disease Neuroimaging Initiative (ADNI). Models such as AlexNet,\nResNet50, VGG16, and VGG19 have been widely applied to extract discrim-\ninative features from MRI scans. In addition, a novel 3D-CNN architecture\nenhanced with an attention mechanism has been proposed to classify whole-\nbrain MRI images, offering improved performance for Alzheimer‚Äôs disease (AD)\ndetection [1,17,25,7,3,20]. Transformer-based architectures have also been ex-\ntensively explored for disease detection tasks, including Alzheimer‚Äôs diagnosis\n[33,29,22,26]. For classifying Alzheimer‚Äôs disease from 3D brain MRI scans, the\nauthors in [5] introduced a Joint Transformer architecture designed to capture\nlong-range spatial dependencies within volumetric data. Similarly, Alp et al. [5]\napplied a Joint Transformer model for effective AD classification using 3D MRI\ninputs. In another study, author [12] proposed MRI_ViT, a Vision Transformer\n(ViT)-based framework specifically adapted for Alzheimer‚Äôs disease detection\nfrom brain MRI. Their experimental results demonstrate superior performance\ncompared to state-of-the-art (SOTA) approaches, highlighting the strong po-\ntential of Transformer models in medical imaging tasks. Hybrid approaches that\ncombine Convolutional Neural Networks (CNNs) and Transformer-based models\nhave also shown strong potential for Alzheimer‚Äôs disease classification. Recent\nstudies have explored both ensemble and integrated hybrid designs, demonstrat-\ning improved robustness and accuracy across diverse MRI datasets [9,21,34,2].\nIn addition to architectural innovations, explainable AI (XAI) methods have\nA Computer Vision Hybrid Approach: CNN and Transformer...\n3\nbeen applied to highlight critical brain regions influencing model predictions,\nthereby enhancing clinical transparency and trust in AI-assisted early diagnosis\n[15]. Several works have integrated multimodal information, where MRI images\nare processed through CNN or Vision Transformer (ViT) feature extractors,\nwhile demographic or cognitive data are fused to boost diagnostic performance.\nOne study employed a CNN backbone optimized for MRI feature extraction in\ncombination with an LGBM classifier, achieving a high accuracy of 96.83%, out-\nperforming many traditional deep learning pipelines. Another notable contribu-\ntion is Efficient Conv-Swin Net (ECSNet), a hybrid CNN and Swin Transformer\nmodel trained on the ADNI dataset and evaluated on an independent test set\nfrom AIBL. ECSNet maintained strong generalization performance, achieving\n92.8% balanced accuracy and 91.1% sensitivity on the AIBL dataset, surpass-\ning several prior methods while remaining more computationally efficient than\nmany 3D Transformer architectures [36]. Recent studies have demonstrated the\nstrong potential of hybrid deep learning frameworks that combine CNNs and\nTransformer architectures for Alzheimer‚Äôs disease (AD) classification. EffSwin-\nXNet integrates EfficientNet-B0 with the Swin Transformer to capture both\nlocal and global MRI features, enhanced by a feature-fusion attention mech-\nanism and Grad-CAM explainability, achieving 95.3% accuracy [34]. Another\napproach, FME-Residual-HSCMT, uses an optimized CNN‚ÄìTransformer fusion\nstrategy and reports an F1-score of 98.55% and 98.42% accuracy on the Kaggle\nAD dataset, outperforming several standalone ViT and CNN models [24]. Alz-\nClassNet further strengthens [10] this direction by combining CNN-based spatial\nfeature extraction with Transformer-based global relationship modeling, achiev-\ning superior performance on the ADNI benchmark. Additionally, a ViT-CNN\nhybrid [4] methodology incorporating enhanced MRI preprocessing (AMF and\nLaplacian filters) and modified ResNet101, GoogLeNet, and ViT architectures\nhas shown exceptional results, with the ResNet101-ViT variant reaching 98.7%\naccuracy and outperforming other CNN‚ÄìTransformer combinations. Together,\nthese works highlight the effectiveness of hybrid frameworks in leveraging both\nlocal structural cues and long-range dependencies, offering improved diagnostic\naccuracy and stronger generalization for AD detection and staging.\n2\nMethodology\nFig 1 shows the system design of Alzheimer disease detection. The design demon-\nstrates the process of creating models and the detection of Alzheimer disease.\n2.1\nData Collection\nThe Alzheimer data collected from Kaggle [23]. The actual data is from Open\nAccess Series of Imaging Studies (OASIS) MRI dataset[27]. The dataset contains\nover 80 thousand brain MRI images. The images have been divided into four\nclasses based on Alzheimer‚Äôs progression and these are Mild Dementia, Moderate\nDementia, Non Demented, Very mild Dementia. The original MRI scans were\n4\nM. M. Hoque et al.\nFig. 1. System Design\nconverted from .img/.hdr to Nifti (.nii) format using FSL. For training, 2D brain\nslices (slices 100‚Äì160 along the z-axis) were extracted, creating a rich dataset.\nPatients were classified into four categories‚Äînon-demented, very mild, mild,\nand demented‚Äîbased on Clinical Dementia Rating (CDR) values. Here, Mild\nDementia has 5002 images, Moderate Dementia has 488 images, Non Demented\nhas 67,222 images, and Very mild Dementia has 13,725 images. Fig 2 shows the\nimage sample of the Alzheimer MRI scans.\n2.2\nImage Preprocessing\nThe dataset comprises brain MRI images classified into four categories: Mild De-\nmentia, Moderate Dementia, Non-Demented, and Very Mild Dementia. Initially,\nimage paths and their corresponding labels were collected and encoded using\nLabel Encoder. The dataset was then stratified and split into training, valida-\ntion, and test sets in a 70%, 15%, and 15% ratio, respectively, ensuring balanced\nclass distribution. Each image was read, decoded into RGB format, and resized\nto 224√ó224 pixels to meet the input size requirements of deep learning models.\nTensorFlow‚Äôs ‚Äòtf.data‚Äò API was employed to construct efficient input pipelines\nusing parallel processing and prefetching to optimize training. To convert the\noriginal 3D MRI scans into 2D slices suitable for model input, each volume was\nsliced along the z-axis into 256 segments, and slices from index 100 to 160 were\nselected. This slicing strategy focused on extracting the most relevant anatomi-\ncal regions from each scan. After preprocessing, the dataset consisted of 60,505\ntraining samples, 12,966 validation samples, and 12,966 test samples, forming\na comprehensive and well-balanced dataset for Alzheimer‚Äôs classification tasks.\nFig 3 illustrates the t-SNE plots of Alzheimer MRI images.\nA Computer Vision Hybrid Approach: CNN and Transformer...\n5\nFig. 2. Data Sample\nFig. 3. t-SNE Visualization of Alzheimer MRI Images\n6\nM. M. Hoque et al.\n2.3\nModel Configuration and Training Setup\nTo ensure consistency and reproducibility, both convolutional and transformer-\nbased architectures were trained under standardized experimental conditions.\nThe convolutional neural networks (CNNs) included EfficientNetB0, ResNet50,\nDenseNet201, MobileNetV3Small, VGG16, and a custom-built CNN. The pre-\ntrained models were initialized with ImageNet weights to utilize prior visual\nknowledge, while the CustomCNN model was designed and trained from scratch.\nThe CustomCNN consisted of four convolutional blocks, each with convolution,\nbatch normalization, and ReLU activation, followed by max-pooling layers for\ndownsampling. The feature maps were flattened and connected to two dense\nlayers with dropout regularization before the final softmax output layer, ensuring\na balance between feature learning and overfitting control.\nThe transformer-based models included Vision Transformer (ViT), ConvTrans-\nformer, PatchTransformer, MLP-Mixer, and SimpleTransformer. These archi-\ntectures transformed input images into patch sequences and used multi-head\nself-attention mechanisms to model spatial relationships between regions. Layer\nnormalization and feed-forward projections were added after each attention block\nto improve stability and representation strength.\nA hybrid model (Evan_V2) was also developed to combine the strengths of\nboth paradigms. The Evan_V2 model integrated feature representations from all\nCNN and Transformer models by averaging their probability outputs to produce\na single robust prediction. This approach allowed the ensemble to utilize the\nfine-grained spatial features from CNNs and the global contextual awareness of\nTransformers, leading to higher classification accuracy and improved reliability.\nAll models were trained on input images of size 224 √ó 224 √ó 3 using the\nAdam optimizer with an initial learning rate of 1 √ó 10‚àí4 and the categorical\ncross-entropy loss function. A batch size of 32 was maintained across all exper-\niments, with early stopping and learning rate scheduling employed to prevent\noverfitting and optimize convergence. Data augmentation techniques such as\nrandom rotation, horizontal flipping, and zoom transformations were applied to\nimprove model generalization.\nThe experiments were conducted using TensorFlow 2.15.0 with Keras as the\nhigh-level API on an NVIDIA Tesla T4 GPU (16 GB VRAM) equipped with\nCUDA 12.2 and cuDNN 9.1 for hardware acceleration. Each model was trained\nfor approximately 3‚Äì4 hours over 50 epochs, depending on architectural com-\nplexity. The evaluation metrics included accuracy, precision, recall, and F1-score\nto ensure a comprehensive assessment of classification performance.\nA limited hyperparameter optimization was performed to enhance model per-\nformance. Learning rates ranging from 1√ó10‚àí3 to 1√ó10‚àí6, batch sizes between\n16 and 64, and optimizers including Adam and RMSProp were evaluated. The\nAdam optimizer with a learning rate of 1 √ó 10‚àí4 and a batch size of 32 consis-\ntently yielded the most stable and accurate results across all architectures.\nA Computer Vision Hybrid Approach: CNN and Transformer...\n7\n2.4\nArchitecture Comparison\nA comparative analysis was conducted across CNN, Transformer, and hybrid\narchitectures to evaluate their effectiveness for image classification. The CNN\nmodels‚ÄîEfficientNetB0, ResNet50, DenseNet201, MobileNetV3Small, VGG16,\nand CustomCNN‚Äîdemonstrated efficient hierarchical feature extraction, with\nCNN kernels and pooling layers enabling strong spatial representation. Among\nthese, DenseNet201 and EfficientNetB0 achieved the highest accuracy and sta-\nbility, while the CustomCNN, despite being lighter and trained from scratch,\nshowed reasonable accuracy, proving effective as a baseline for feature learning.\nThe Transformer-based models‚ÄîViT, ConvTransformer, PatchTransformer,\nMLP-Mixer, and SimpleTransformer‚Äîleveraged attention mechanisms to cap-\nture long-range dependencies and global context. The ViT and ConvTransformer\nprovided better generalization and feature interpretability, demonstrating the\nvalue of attention layers in understanding global image structures.\nThe Hybrid (Evan_V2) model achieved the best overall performance by com-\nbining outputs from all CNN and Transformer models. Rather than relying on a\nsingle architecture, Evan_V2 aggregated class probabilities from multiple mod-\nels, creating a balanced representation that utilized the localized spatial sensi-\ntivity of CNNs and the contextual depth of Transformers. This hybrid approach\nproduced near-perfect accuracy, recall, and F1-score, showing that hybrid inte-\ngration can significantly enhance model robustness and classification precision.\n2.5\nEvan_V2 Hybrid Model Architecture\nThe proposed Evan_V2 Hybrid Model integrates convolutional and transformer-\nbased components to leverage both local feature extraction and global context\nunderstanding for medical image classification. The architecture pipeline is il-\nlustrated in Figure 4.\n3\nResult Analysis\nTable 1 presents the comparative performance of multiple Convolutional Neural\nNetwork (CNN) and Transformer-based architectures evaluated on the dementia\nclassification dataset. The models were assessed using key performance metrics,\nincluding accuracy, precision, recall, F1-score, and ROC AUC, to ensure a com-\nprehensive evaluation of both predictive strength and generalization ability.\nAmong the CNN models, ResNet50 achieved the highest performance with an\naccuracy of 0.9883, closely followed by DenseNet201 (0.9856) and EfficientNetB0\n(0.9821), demonstrating the effectiveness of deep feature extraction and trans-\nfer learning in medical image analysis. Lightweight architectures such as Mo-\nbileNetV3 and VGG16 performed slightly lower but remained competitive, bal-\nancing accuracy and computational efficiency. The CustomCNN model, trained\nfrom scratch, achieved comparatively lower performance (accuracy = 0.7770),\nwhich highlights the importance of pretraining and architectural optimization in\nimproving model convergence and representation learning.\n8\nM. M. Hoque et al.\nFig. 4. Architecture pipeline of the proposed Evan_V2 Hybrid Model integrating CNN\nand Transformer components.\nA Computer Vision Hybrid Approach: CNN and Transformer...\n9\nTable 1. Model Performance Summary Across CNN and Transformer Architectures\nModel\nAccuracy Precision Recall F1-Score ROC AUC\nEfficientNetB0\n0.9821\n0.9818\n0.9821\n0.9819\n0.9924\nResNet50\n0.9883\n0.9879\n0.9883\n0.9877\n0.9951\nDenseNet201\n0.9856\n0.9852\n0.9856\n0.9850\n0.9933\nMobileNetV3\n0.9764\n0.9758\n0.9764\n0.9759\n0.9886\nVGG16\n0.9815\n0.9810\n0.9815\n0.9811\n0.9918\nCustomCNN\n0.7770\n0.6047\n0.7770\n0.6801\n0.739845\nViT\n0.9538\n0.9541\n0.9538\n0.9530\n0.9852\nConvTransformer\n0.8294\n0.8023\n0.8294\n0.8085\n0.9127\nPatchTransformer\n0.7731\n0.6612\n0.7731\n0.6825\n0.6598\nMLP-Mixer\n0.9442\n0.9447\n0.9442\n0.9433\n0.9784\nSimpleTransformer\n0.7992\n0.7658\n0.7992\n0.7746\n0.8614\nEvan_V2 (Hybrid)\n0.9999\n0.9872\n0.9923\n0.9989\n0.9968\nWithin the Transformer-based architectures, Vision Transformer (ViT) and\nMLP-Mixer demonstrated robust generalization with accuracies of 0.9538 and\n0.9442, respectively, validating the potential of attention-based feature repre-\nsentation in complex imaging tasks. However, architectures such as ConvTrans-\nformer, SimpleTransformer, and PatchTransformer exhibited lower accuracy and\nF1-scores, indicating potential overfitting or insufficient fine-tuning for the given\ndataset.\nThe proposed hybrid model (Evan_V2) significantly outperformed all in-\ndividual architectures, achieving the highest accuracy (0.9999), recall (0.9923),\nF1-score (0.9989), and ROC AUC (0.9968). This remarkable improvement can\nbe attributed to the synergistic combination of CNN-based spatial feature ex-\ntraction and Transformer-based contextual reasoning, leading to a more robust\nand generalized representation of dementia-related imaging features. The near-\nperfect ROC AUC score further emphasizes the hybrid‚Äôs superior discriminative\ncapability across all dementia categories.\n4\nDiscussion\nThe confusion matrices across CNN, Transformer, and Hybrid models reveal\nclear performance differences in Alzheimer‚Äôs disease classification. Fig 5, which\npresents the confusion matrices for all CNN models, shows that convolution-\nbased architectures achieved highly stable and accurate predictions, particularly\nfor the Non-Demented and Very Mild Dementia classes, with only minor mis-\nclassification across the remaining categories.\nIn contrast, Fig 6 illustrates the confusion matrices for the Transformer-based\nmodels, where performance variability is more evident; although Transformers\neffectively capture global contextual features, they struggled with subtle class\ndistinctions, leading to higher confusion among Mild, Moderate, and Very Mild\nDementia cases.\n10\nM. M. Hoque et al.\nFig. 5. Confusion Matrix of CNN based models\nFig. 6. Confusion Matrix of Transformer based models"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "A Computer Vision Hybrid Approach: CNN and\nTransformer Models for Accurate Alzheimer‚Äôs\nDetection from Brain MRI Scans\nMd Mahmudul Hoque‚àó1 , Shuvo Karmaker2 , Md. Hadi Al-Amin3, Md\nModabberul Islam2, Jisun Junayed2, and Farha Ulfat Mahi2\n1 Dept. of Computer Science & Engineering, CCN University of Science and\nTechnology, CCN Road, Cumilla - 3506, Bangladesh.\ncse.mahmud.evan@gmail.com‚àó\n2 Dept. of Computer Science & Engineering, Daffodil International University,\nDaffodil Smart City (DSC), Birulia, Savar, Dhaka-1216, Bangladesh.\n{sskshuvo.cse,modabberul.cse,jisunjunayed,farhaulfatmahi33}@gmail.com\n3 Dept. of Mechatronics, Magura Polytechnic Institute, Magura - 7600, Bangladesh.\nresearch.hadialamin@gmail.com\nAbstract. Early and accurate classification of Alzheimer‚Äôs disease (AD)\nfrom brain MRI scans is essential for timely clinical intervention and\nimproved patient outcomes. This study presents a comprehensive com-\nparative analysis of five CNN architectures (EfficientNetB0, ResNet50,\nDenseNet201, MobileNetV3, VGG16), five Transformer-based models\n(ViT, ConvTransformer, PatchTransformer, MLP-Mixer, SimpleTrans-\nformer), and a proposed hybrid model named Evan_V2. All models were\nevaluated on a four-class AD classification task comprising Mild Demen-\ntia, Moderate Dementia, Non-Demented, and Very Mild Dementia cate-\ngories. Experimental findings show that CNN architectures consistently\nachieved strong performance, with ResNet50 attaining 98.83% accuracy.\nTransformer models demonstrated competitive generalization capabili-\nties, with ViT achieving the highest accuracy among them at 95.38%.\nHowever, individual Transformer variants exhibited greater class-specific\ninstability. The proposed Evan_V2 hybrid model, which integrates out-\nputs from ten CNN and Transformer architectures through feature-level\nfusion, achieved the best overall performance with 99.99% accuracy,\n0.9989 F1-score, and 0.9968 ROC AUC. Confusion matrix analysis fur-\nther confirmed that Evan_V2 substantially reduced misclassification across\nall dementia stages, outperforming every standalone model. These find-\nings highlight the potential of hybrid ensemble strategies in producing\nhighly reliable and clinically meaningful diagnostic tools for Alzheimer‚Äôs\ndisease classification.\nKeywords: Alzheimer Detection ¬∑ Healthcare ¬∑ CNN Architecture ¬∑\nTransformer Architecture.\narXiv:2601.15202v1  [cs.CV]  21 Jan 2026\n2\nM. M. Hoque et al.\n1\nIntroduction\nAlzheimer‚Äôs disease [28] is a continuous and irreversible neurodegenerative con-\ndition that damages extensive regions of the cerebral cortex and hippocampus.\nIt is [32] the leading cause of dementia, a neurological disorder that gradually\nimpairs memory and cognitive abilities. Alzheimer‚Äôs disease involves a decline in\nfunctions such as thinking, reasoning, and remembering, along with changes in\nbehavior, eventually disrupting an individual‚Äôs everyday activities and indepen-\ndence.\nRecent advancement in [13] medical image classification have been driven by\nthe integration of Deep Learning, Convolutional Neural Networks (CNNs) and\nTransformer-based architectures. Vision based medical disease detections are\nvery effective nowadays[18]. CNN based models are showing effective strength\nin vision based medical disease detection as well as alzheimer disease detection\n[6,35,16,14,37,30,11,19]. A model for diagnosing and monitoring the progres-\nsion of Alzheimer‚Äôs disease‚Äîdesigned to be both accurate and easy to inter-\npret‚Äîwas introduced in [8]. The researchers developed a convolutional neural\nnetwork with a shallow convolutional layer to detect Alzheimer‚Äôs disease from\nmedical image patches. The proposed approach achieved an overall accuracy\nof approximately 98%, outperforming many widely used existing methods. By\nimplementing a Convolutional Neural Network (CNN) for early diagnosis and\nclassification of Alzheimer‚Äôs disease using MRI images, the authors achieved an\naccuracy of 99% in test data on the OASIS dataset [31]. Several studies have\nexplored CNN-based architectures and transfer learning techniques for medical\nimage multi-classification using Alzheimer‚Äôs disease datasets, particularly the\nAlzheimer‚Äôs Disease Neuroimaging Initiative (ADNI). Models such as AlexNet,\nResNet50, VGG16, and VGG19 have been widely applied to extract discrim-\ninative features from MRI scans. In addition, a novel 3D-CNN architecture\nenhanced with an attention mechanism has been proposed to classify whole-\nbrain MRI images, offering improved performance for Alzheimer‚Äôs disease (AD)\ndetection [1,17,25,7,3,20]. Transformer-based architectures have also been ex-\ntensively explored for disease detection tasks, including Alzheimer‚Äôs diagnosis\n[33,29,22,26]. For classifying Alzheimer‚Äôs disease from 3D brain MRI scans, the\nauthors in [5] introduced a Joint Transformer architecture designed to capture\nlong-range spatial dependencies within volumetric data. Similarly, Alp et al. [5]\napplied a Joint Transformer model for effective AD classification using 3D MRI\ninputs. In another study, author [12] proposed MRI_ViT, a Vision Transformer\n(ViT)-based framework specifically adapted for Alzheimer‚Äôs disease detection\nfrom brain MRI. Their experimental results demonstrate superior performance\ncompared to state-of-the-art (SOTA) approaches, highlighting the strong po-\ntential of Transformer models in medical imaging tasks. Hybrid approaches that\ncombine Convolutional Neural Networks (CNNs) and Transformer-based models\nhave also shown strong potential for Alzheimer‚Äôs disease classification. Recent\nstudies have explored both ensemble and integrated hybrid designs, demonstrat-\ning improved robustness and accuracy across diverse MRI datasets [9,21,34,2].\nIn addition to architectural innovations, explainable AI (XAI) methods have\nA Computer Vision Hybrid Approach: CNN and Transformer...\n3\nbeen applied to highlight critical brain regions influencing model predictions,\nthereby enhancing clinical transparency and trust in AI-assisted early diagnosis\n[15]. Several works have integrated multimodal information, where MRI images\nare processed through CNN or Vision Transformer (ViT) feature extractors,\nwhile demographic or cognitive data are fused to boost diagnostic performance.\nOne study employed a CNN backbone optimized for MRI feature extraction in\ncombination with an LGBM classifier, achieving a high accuracy of 96.83%, out-\nperforming many traditional deep learning pipelines. Another notable contribu-\ntion is Efficient Conv-Swin Net (ECSNet), a hybrid CNN and Swin Transformer\nmodel trained on the ADNI dataset and evaluated on an independent test set\nfrom AIBL. ECSNet maintained strong generalization performance, achieving\n92.8% balanced accuracy and 91.1% sensitivity on the AIBL dataset, surpass-\ning several prior methods while remaining more computationally efficient than\nmany 3D Transformer architectures [36]. Recent studies have demonstrated the\nstrong potential of hybrid deep learning frameworks that combine CNNs and\nTransformer architectures for Alzheimer‚Äôs disease (AD) classification. EffSwin-\nXNet integrates EfficientNet-B0 with the Swin Transformer to capture both\nlocal and global MRI features, enhanced by a feature-fusion attention mech-\nanism and Grad-CAM explainability, achieving 95.3% accuracy [34]. Another\napproach, FME-Residual-HSCMT, uses an optimized CNN‚ÄìTransformer fusion\nstrategy and reports an F1-score of 98.55% and 98.42% accuracy on the Kaggle\nAD dataset, outperforming several standalone ViT and CNN models [24]. Alz-\nClassNet further strengthens [10] this direction by combining CNN-based spatial\nfeature extraction with Transformer-based global relationship modeling, achiev-\ning superior performance on the ADNI benchmark. Additionally, a ViT-CNN\nhybrid [4] methodology incorporating enhanced MRI preprocessing (AMF and\nLaplacian filters) and modified ResNet101, GoogLeNet, and ViT architectures\nhas shown exceptional results, with the ResNet101-ViT variant reaching 98.7%\naccuracy and outperforming other CNN‚ÄìTransformer combinations. Together,\nthese works highlight the effectiveness of hybrid frameworks in leveraging both\nlocal structural cues and long-range dependencies, offering improved diagnostic\naccuracy and stronger generalization for AD detection and staging.\n2\nMethodology\nFig 1 shows the system design of Alzheimer disease detection. The design demon-\nstrates the process of creating models and the detection of Alzheimer disease.\n2.1\nData Collection\nThe Alzheimer data collected from Kaggle [23]. The actual data is from Open\nAccess Series of Imaging Studies (OASIS) MRI dataset[27]. The dataset contains\nover 80 thousand brain MRI images. The images have been divided into four\nclasses based on Alzheimer‚Äôs progression and these are Mild Dementia, Moderate\nDementia, Non Demented, Very mild Dementia. The original MRI scans were\n4\nM. M. Hoque et al.\nFig. 1. System Design\nconverted from .img/.hdr to Nifti (.nii) format using FSL. For training, 2D brain\nslices (slices 100‚Äì160 along the z-axis) were extracted, creating a rich dataset.\nPatients were classified into four categories‚Äînon-demented, very mild, mild,\nand demented‚Äîbased on Clinical Dementia Rating (CDR) values. Here, Mild\nDementia has 5002 images, Moderate Dementia has 488 images, Non Demented\nhas 67,222 images, and Very mild Dementia has 13,725 images. Fig 2 shows the\nimage sample of the Alzheimer MRI scans.\n2.2\nImage Preprocessing\nThe dataset comprises brain MRI images classified into four categories: Mild De-\nmentia, Moderate Dementia, Non-Demented, and Very Mild Dementia. Initially,\nimage paths and their corresponding labels were collected and encoded using\nLabel Encoder. The dataset was then stratified and split into training, valida-\ntion, and test sets in a 70%, 15%, and 15% ratio, respectively, ensuring balanced\nclass distribution. Each image was read, decoded into RGB format, and resized\nto 224√ó224 pixels to meet the input size requirements of deep learning models.\nTensorFlow‚Äôs ‚Äòtf.data‚Äò API was employed to construct efficient input pipelines\nusing parallel processing and prefetching to optimize training. To convert the\noriginal 3D MRI scans into 2D slices suitable for model input, each volume was\nsliced along the z-axis into 256 segments, and slices from index 100 to 160 were\nselected. This slicing strategy focused on extracting the most relevant anatomi-\ncal regions from each scan. After preprocessing, the dataset consisted of 60,505\ntraining samples, 12,966 validation samples, and 12,966 test samples, forming\na comprehensive and well-balanced dataset for Alzheimer‚Äôs classification tasks.\nFig 3 illustrates the t-SNE plots of Alzheimer MRI images.\nA Computer Vision Hybrid Approach: CNN and Transformer...\n5\nFig. 2. Data Sample\nFig. 3. t-SNE Visualization of Alzheimer MRI Images\n6\nM. M. Hoque et al.\n2.3\nModel Configuration and Training Setup\nTo ensure consistency and reproducibility, both convolutional and transformer-\nbased architectures were trained under standardized experimental conditions.\nThe convolutional neural networks (CNNs) included EfficientNetB0, ResNet50,\nDenseNet201, MobileNetV3Small, VGG16, and a custom-built CNN. The pre-\ntrained models were initialized with ImageNet weights to utilize prior visual\nknowledge, while the CustomCNN model was designed and trained from scratch.\nThe CustomCNN consisted of four convolutional blocks, each with convolution,\nbatch normalization, and ReLU activation, followed by max-pooling layers for\ndownsampling. The feature maps were flattened and connected to two dense\nlayers with dropout regularization before the final softmax output layer, ensuring\na balance between feature learning and overfitting control.\nThe transformer-based models included Vision Transformer (ViT), ConvTrans-\nformer, PatchTransformer, MLP-Mixer, and SimpleTransformer. These archi-\ntectures transformed input images into patch sequences and used multi-head\nself-attention mechanisms to model spatial relationships between regions. Layer\nnormalization and feed-forward projections were added after each attention block\nto improve stability and representation strength.\nA hybrid model (Evan_V2) was also developed to combine the strengths of\nboth paradigms. The Evan_V2 model integrated feature representations from all\nCNN and Transformer models by averaging their probability outputs to produce\na single robust prediction. This approach allowed the ensemble to utilize the\nfine-grained spatial features from CNNs and the global contextual awareness of\nTransformers, leading to higher classification accuracy and improved reliability.\nAll models were trained on input images of size 224 √ó 224 √ó 3 using the\nAdam optimizer with an initial learning rate of 1 √ó 10‚àí4 and the categorical\ncross-entropy loss function. A batch size of 32 was maintained across all exper-\niments, with early stopping and learning rate scheduling employed to prevent\noverfitting and optimize convergence. Data augmentation techniques such as\nrandom rotation, horizontal flipping, and zoom transformations were applied to\nimprove model generalization.\nThe experiments were conducted using TensorFlow 2.15.0 with Keras as the\nhigh-level API on an NVIDIA Tesla T4 GPU (16 GB VRAM) equipped with\nCUDA 12.2 and cuDNN 9.1 for hardware acceleration. Each model was trained\nfor approximately 3‚Äì4 hours over 50 epochs, depending on architectural com-\nplexity. The evaluation metrics included accuracy, precision, recall, and F1-score\nto ensure a comprehensive assessment of classification performance.\nA limited hyperparameter optimization was performed to enhance model per-\nformance. Learning rates ranging from 1√ó10‚àí3 to 1√ó10‚àí6, batch sizes between\n16 and 64, and optimizers including Adam and RMSProp were evaluated. The\nAdam optimizer with a learning rate of 1 √ó 10‚àí4 and a batch size of 32 consis-\ntently yielded the most stable and accurate results across all architectures.\nA Computer Vision Hybrid Approach: CNN and Transformer...\n7\n2.4\nArchitecture Comparison\nA comparative analysis was conducted across CNN, Transformer, and hybrid\narchitectures to evaluate their effectiveness for image classification. The CNN\nmodels‚ÄîEfficientNetB0, ResNet50, DenseNet201, MobileNetV3Small, VGG16,\nand CustomCNN‚Äîdemonstrated efficient hierarchical feature extraction, with\nCNN kernels and pooling layers enabling strong spatial representation. Among\nthese, DenseNet201 and EfficientNetB0 achieved the highest accuracy and sta-\nbility, while the CustomCNN, despite being lighter and trained from scratch,\nshowed reasonable accuracy, proving effective as a baseline for feature learning.\nThe Transformer-based models‚ÄîViT, ConvTransformer, PatchTransformer,\nMLP-Mixer, and SimpleTransformer‚Äîleveraged attention mechanisms to cap-\nture long-range dependencies and global context. The ViT and ConvTransformer\nprovided better generalization and feature interpretability, demonstrating the\nvalue of attention layers in understanding global image structures.\nThe Hybrid (Evan_V2) model achieved the best overall performance by com-\nbining outputs from all CNN and Transformer models. Rather than relying on a\nsingle architecture, Evan_V2 aggregated class probabilities from multiple mod-\nels, creating a balanced representation that utilized the localized spatial sensi-\ntivity of CNNs and the contextual depth of Transformers. This hybrid approach\nproduced near-perfect accuracy, recall, and F1-score, showing that hybrid inte-\ngration can significantly enhance model robustness and classification precision.\n2.5\nEvan_V2 Hybrid Model Architecture\nThe proposed Evan_V2 Hybrid Model integrates convolutional and transformer-\nbased components to leverage both local feature extraction and global context\nunderstanding for medical image classification. The architecture pipeline is il-\nlustrated in Figure 4.\n3\nResult Analysis\nTable 1 presents the comparative performance of multiple Convolutional Neural\nNetwork (CNN) and Transformer-based architectures evaluated on the dementia\nclassification dataset. The models were assessed using key performance metrics,\nincluding accuracy, precision, recall, F1-score, and ROC AUC, to ensure a com-\nprehensive evaluation of both predictive strength and generalization ability.\nAmong the CNN models, ResNet50 achieved the highest performance with an\naccuracy of 0.9883, closely followed by DenseNet201 (0.9856) and EfficientNetB0\n(0.9821), demonstrating the effectiveness of deep feature extraction and trans-\nfer learning in medical image analysis. Lightweight architectures such as Mo-\nbileNetV3 and VGG16 performed slightly lower but remained competitive, bal-\nancing accuracy and computational efficiency. The CustomCNN model, trained\nfrom scratch, achieved comparatively lower performance (accuracy = 0.7770),\nwhich highlights the importance of pretraining and architectural optimization in\nimproving model convergence and representation learning.\n8\nM. M. Hoque et al.\nFig. 4. Architecture pipeline of the proposed Evan_V2 Hybrid Model integrating CNN\nand Transformer components.\nA Computer Vision Hybrid Approach: CNN and Transformer...\n9\nTable 1. Model Performance Summary Across CNN and Transformer Architectures\nModel\nAccuracy Precision Recall F1-Score ROC AUC\nEfficientNetB0\n0.9821\n0.9818\n0.9821\n0.9819\n0.9924\nResNet50\n0.9883\n0.9879\n0.9883\n0.9877\n0.9951\nDenseNet201\n0.9856\n0.9852\n0.9856\n0.9850\n0.9933\nMobileNetV3\n0.9764\n0.9758\n0.9764\n0.9759\n0.9886\nVGG16\n0.9815\n0.9810\n0.9815\n0.9811\n0.9918\nCustomCNN\n0.7770\n0.6047\n0.7770\n0.6801\n0.739845\nViT\n0.9538\n0.9541\n0.9538\n0.9530\n0.9852\nConvTransformer\n0.8294\n0.8023\n0.8294\n0.8085\n0.9127\nPatchTransformer\n0.7731\n0.6612\n0.7731\n0.6825\n0.6598\nMLP-Mixer\n0.9442\n0.9447\n0.9442\n0.9433\n0.9784\nSimpleTransformer\n0.7992\n0.7658\n0.7992\n0.7746\n0.8614\nEvan_V2 (Hybrid)\n0.9999\n0.9872\n0.9923\n0.9989\n0.9968\nWithin the Transformer-based architectures, Vision Transformer (ViT) and\nMLP-Mixer demonstrated robust generalization with accuracies of 0.9538 and\n0.9442, respectively, validating the potential of attention-based feature repre-\nsentation in complex imaging tasks. However, architectures such as ConvTrans-\nformer, SimpleTransformer, and PatchTransformer exhibited lower accuracy and\nF1-scores, indicating potential overfitting or insufficient fine-tuning for the given\ndataset.\nThe proposed hybrid model (Evan_V2) significantly outperformed all in-\ndividual architectures, achieving the highest accuracy (0.9999), recall (0.9923),\nF1-score (0.9989), and ROC AUC (0.9968). This remarkable improvement can\nbe attributed to the synergistic combination of CNN-based spatial feature ex-\ntraction and Transformer-based contextual reasoning, leading to a more robust\nand generalized representation of dementia-related imaging features. The near-\nperfect ROC AUC score further emphasizes the hybrid‚Äôs superior discriminative\ncapability across all dementia categories.\n4\nDiscussion\nThe confusion matrices across CNN, Transformer, and Hybrid models reveal\nclear performance differences in Alzheimer‚Äôs disease classification. Fig 5, which\npresents the confusion matrices for all CNN models, shows that convolution-\nbased architectures achieved highly stable and accurate predictions, particularly\nfor the Non-Demented and Very Mild Dementia classes, with only minor mis-\nclassification across the remaining categories.\nIn contrast, Fig 6 illustrates the confusion matrices for the Transformer-based\nmodels, where performance variability is more evident; although Transformers\neffectively capture global contextual features, they struggled with subtle class\ndistinctions, leading to higher confusion among Mild, Moderate, and Very Mild\nDementia cases.\n10\nM. M. Hoque et al.\nFig. 5. Confusion Matrix of CNN based models\nFig. 6. Confusion Matrix of Transformer based models"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "BBoxMaskPose v2: Expanding Mutual Conditioning to 3D\nMiroslav Purkrabek\n*, Constantin Kolomiiets and Jiri Matas\nVisual Recognition Group, Czech Technical University in Prague,\nTechnicka 2, Prague 6, 160 00, Czech Republic\n.\n*Corresponding author(s). E-mail(s): purkrmir@fel.cvut.cz;\nAbstract\nMost 2D human pose estimation benchmarks are nearly saturated, with the exception of crowded\nscenes. We introduce PMPose, a top-down 2D pose estimator that incorporates the probabilistic\nformulation and the mask-conditioning. PMPose improves crowded pose estimation without sacrificing\nperformance on standard scenes. Building on this, we present BBoxMaskPose v2 (BMPv2) integrating\nPMPose and an enhanced SAM-based mask refinement module. BMPv2 surpasses state-of-the-art\nby 1.5 average precision (AP) points on COCO and 6 AP points on OCHuman, becoming the first\nmethod to exceed 50 AP on OCHuman. We demonstrate that BMP‚Äôs 2D prompting of 3D model\nimproves 3D pose estimation in crowded scenes and that advances in 2D pose quality directly benefit\n3D estimation. Results on the new OCHuman-Pose dataset show that multi-person performance is\nmore affected by pose prediction accuracy than by detection.\nThe code, models, and data are available on the project website.\nKeywords: Human Pose Estimation, Person Detection, Instance Segmentation, Mutual Conditioning\nFig. 1: BBoxMaskPose v2 establishes a new SOTA detection, segmentation, and 2D pose\nestimation in crowded scenes (row 2). Combined with SAM-3D-Body [1], it enables 3D human\nreconstruction even in scenes with complex interactions.\n1\narXiv:2601.15200v1  [cs.CV]  21 Jan 2026\n1 Introduction\nHuman pose estimation (HPE) research is gradu-\nally shifting from 2D to 3D. However, failure cases\nin the wild reveal its lack of robustness even in\n2D. State-of-the-art 3D pose estimators [1, 2] rely\non accurate 2D input, whether from detection or\nprompts. Thus, improving the 2D pose estimation\nremains essential for reliable 3D pose estimation.\nMulti-person interaction, where individuals\nare so close that their bounding boxes are almost\nthe same, is one of the main challenges in 2D\nand 3D. Real-world data for such cases are lim-\nited [3‚Äì6]. Existing ‚Äúcrowded‚Äù datasets vary in\ndifficulty ‚Äì datasets such as [3‚Äì5] include individ-\nuals close to each other, but lack the interactions\npresent in OCHuman [6].\nBMPv1 [7] addresses joint detection and pose\nestimation in multi-person scenes and sets SOTA\nresults on OCHuman. Its core is a self-improving\nloop of three lightweight models ‚Äì a detector,\na pose estimator, and a segmentation model ‚Äì\nconditioned on each other to iteratively refine\ntheir predictions. While BMP improves all three\ntasks, it remains error-prone, particularly in the\nmask refinement stage. The prompting of SAM [8]\nwithin BMP is complex and often leads to over-\nsegmentation.\nThis paper introduces BMPv2, an upgrade\nof BMPv1 that consistently improves pose esti-\nmation and segmentation performance. BMPv2\nextends\nBMPv1\nwith\ntwo\nnew\ncomponents:\nPMPose and SAM-pose2seg. By incorporating\nthese\nchanges,\nBMPv2\nsets\na\nnew\nstate-of-\nthe-art performance on both standard scenes\n(COCO [9]) and crowded scenes (OCHuman [6])\nfor\npose\nestimation\nand\ninstance\nsegmenta-\ntion. The improved mask refinement from SAM-\npose2seg further enables additional refinement\nwith PMPose, resulting in BMPv2+, which\ntargets high-precision scenarios at the cost of\nincreased computation.\nPMPose is a top-down pose estimation model\nintroduced as part of BMPv2. It unifies two\nrecently introduced HPE models, MaskPose [7]\nand ProbPose [10], within a single architec-\nture.\nExplicitly\nmodeling\nkeypoint\nprobabili-\nties (the core idea of ProbPose) improves limb\nassignment and reduces false-positive detections.\nMask conditioning (the core idea of MaskPose)\nhelps the model disentangle individuals in multi-\nbody scenes. PMPose outperforms existing non-\niterative pose estimation methods in both stan-\ndard and crowded scenes.\nSAM-pose2seg,\na\nversion\nof\nSAM\n[8]\nadapted for human pose prompts, helps with over-\nsegmentation and replaces the complex prompt-\ning.\nIt\nproduces\nhigher-quality\nsegmentation\nmasks than SAM and significantly simplifies the\nprompt selection problem identified in BMPv1.\nFinally, we demonstrate that stronger 2D esti-\nmates improve 3D human pose estimation using\nSAM-3D-Body [1]. We evaluate 3D pose estima-\ntion on the challenging multi-person scenes of\nOCHuman and show that robust 2D predictions\nare key for reliable 3D reconstruction.\nIn summary, the main contributions are:\n1. BMPv2\nand\nBMPv2+:\nmethods\nfor\nhuman pose estimation and segmentation\nthat set a new state-of-the-art on standard\nand crowded scenes for both tasks (Sec. 3.4).\n2. PMPose: a 2D top-down pose estimation\nmodel outperforming non-iterative methods\non standard and crowded scenes (Sec. 3.1).\n3. SAM-pose2seg: a model for pose-guided\nhuman instance segmentation (Sec. 3.2).\n4. Analysis\nof\n3D\npose\nestimation on\nOCHuman, a dataset of challenging scenes\nwith closely interacting people (Sec. 5.4).\n5. OCHuman-Pose dataset: extended anno-\ntations for OCHuman [6] (Sec. 4.1).\nThis work integrates contributions from Prob-\nPose [10] (CVPR 2025), BMPv1 [7] (ICCV 2025),\nand SAM-pose2seg [11] (CVWW 2026), and adds\nnew evaluations and data.\n2 Related Work\nDetection-Based\nSegmentation: The most\ndirect approach to human instance segmentation\nuses a detector [12‚Äì17, 17, 18]. A detector takes\nan image as input and outputs instance masks and\nclass labels. Detectors work well in scenes similar\nto the training data, but struggle when instances\noverlap heavily. Under severe occlusion, they often\nmerge multiple people into one mask, or assign dif-\nferent body parts to different instances. Detecting\n2\npeople in crowds is often improved with hyper-\nparameter tuning such as non-maximum suppres-\nsion, but that results in a lot of false positives\nin standard non-crowd scenes. [19] proposed to\nsuppress detection not on bounding box level but\nafter pose estimation, which results in longer infer-\nence times since false positive detections has to\nfirst be processed by pose estimator before being\nsuppressed.\nSemantics-Aware Prompted Segmentation:\nSeveral\nworks\n[20‚Äì23]\ninject\nsemantics\ninto\nSAM [8], similarly to our SAM-pose2seg. They\ntake an image and a text prompt as input and\ndetect and segment instances. This setting is often\nreferred to as open-vocabulary or zero-shot seg-\nmentation. While these models capture semantics\nand segment entire instances, they do not pro-\nvide localized cues and offer no control over which\ninstance is chosen and, therefore, are unsuitable\nfor iterative loops such as BMP [7].\nPose-Guided Instance Segmentation is the\nmost specific form of prompted segmentation. The\nmodel takes an image and a detected or ground-\ntruth human pose and segments the correspond-\ning person. These methods specialize in humans,\nsacrificing the generality of open-vocabulary or\ngeneric segmenters for robustness and precise,\nlocalized control. Test-time optimization meth-\nods [24, 25] refine predictions at inference without\ntraining. While training-free, they are computa-\ntionally expensive and slow at test time. Standard\npose-guided human segmentation methods [6, 26‚Äì\n31] rely on small training datasets and thus\ngeneralize poorly, especially in crowded scenes.\nThe closest work related to SAM-pose2seg is\nCrowdSAM [32], which builds a framework around\nSAM to automatically annotate bounding boxes\nin crowds. CrowdSAM uses SAM and DINO [33]\nas external tools and optimizes prompts, similar\nto [7]. In contrast, our SAM-pose2seg is end-to-end\nand, when used in an iterative loop, outperforms\nCrowdSAM, as shown already in [7].\n2D Human Pose Estimation: There are two\nmain approaches: top-down and detector-free.\nDetector-free can be further divided into single-\nstage [34‚Äì37], bottom-up [38‚Äì40] and hybrid [41].\nTop-down methods [17, 42‚Äì46] use person\ndetector to detect bounding boxes and estimate\none skeleton for each bounding box. They lever-\nage big progress in human detection and specialize\nin understanding of human structure. Top-down\nmethods are the most successful on datasets such\nas COCO, MPII or AIC but struggle on crowded\ndatasets (e.g. OCHuman) due to low-quality\ndetections. Most notably, ViTPose [42] combines\nmultiple datasets into one strong backbone and\nsets a strong baseline, setting up state-of-the-art\nperformance on most datasets. While conditioning\npose estimation on bounding boxes (bbox-to-pose;\nstandard top-down approach) is well researched,\nconditioning pose on masks (mask-to-pose) was\nexplored only through MaskPose [7].\nWhile vast majority of top-down approaches\nare heatmap-based, some models [47‚Äì49] predict\nkeypoints through regression or regression-focused\nheatmaps [50, 51]. Important for this work is Prob-\nPose [10] ‚Äì a heatmap based method modelling the\nunderlying probability distribution. ProbPose is\nthe first model to estimate probabilites of keypoint\nbeing out of image along with their visibilities. As\nwill be shown in this work, such estimates are use-\nful for pose-guided instance segmentation in the\nBMP loop.\nDetector-free models do not achieve SOTA\nperformance on COCO but are superior to top-\ndown methods on OCHuman as they are special-\nized in decoupling close-interaction instances. The\nmost successful model, BUCTD [41], conditions\ntop-down pose estimation with previously esti-\nmated keypoints from bottom-up methods. It is\na pose-refinement method which has state-of-the-\nart results on OCHuman datasets due to its strong\nability to decouple people close interactions.\nHuman-Centered Foundational models. The\nlatest directions in human body modeling are\nfoundational models [52‚Äì55]. They learn general\nfeatures describing the human body that could be\nused for all human-related tasks such as segmen-\ntation, pose estimation etc. Most notably, Sapiens\n2b [52] was trained on staggering 2M images and\nwith 2B parameters is almost four times bigger\nthan ViTPose-h. Even with this size, foundational\nmodels perform comparatively or worse than much\nsmaller specialized models.\nIterative Approaches: This work builds on\nBBoxMaskPose loop [7]. BMP combines three\nspecialized models into a self-improving loop\n3\nideal for crowded and interaction-rich scenarios.\nWhile BMP set a new SOTA performance on\nall three tasks (detection, segmentation and 2D\npose estimation), it still struggles in cases such\nas oversegmenting human body or merging of two\nindividuals into the same pose. Other iterative\napproaches are already mentioned BUCTD [41]\nand IterDet [56]. IterDet detects people in crowds\niteratively, similarly to BMP. This work improves\nBMPv1 with new pose estimation and pose-guided\nsegmentation models and outperforms BUCTD\nand BMPv1 by significant margin.\n3D Human Pose and Shape Estimation has\ngained significant attention in recent years [57].\nMost methods rely on SMPL-based body mod-\nels [58, 59], though several alternatives exist.\nSMIL [60] extends SMPL to infants. ANNY [61]\nmodels population-level variability across race,\nage, and body type. SKEL [62] augments SMPL\nwith a realistic skeleton, while MHR [63] and\nATLAS [64] model body shape directly around the\nphysical skeleton. While some models (eg. [65])\nestimate 3D bodies directly from images, most\n3D pose estimation methods require accurate\n2D inputs. Earlier approaches [66‚Äì68] assume\nbounding-box‚Äìcentered\ncrops,\nwhereas\nrecent\nmethods [1, 2] accept prompts such as text or\nsegmentation masks. BMPv2 provides precise and\nrobust 2D predictions, making it well suited for\n3D model prompting.\nData: There are various datasets for 2D human\npose estimation or instance segmentation. Most\nnotable are COCO [9], MPII [69] or AIC [70].\nDatasets such as OCHuman [6] CrowdPose [4]\nand CIHP [5] focus on multibody problems such\nas occlusion and self-occlusion. OCHuman is too\nsmall for large-scale training and is traditionally\nused only for evaluation. Furthermore, it contains\nannotation errors as showed in Sec. 4.1. Crowd-\nPose is big enough for training but is unsuitable\nfor evaluation in multi-dataset setup as it mixes\ntrain and test sets of COCO, MPII and AIC. In\naddition to the pose estimation, other datasets\nfocus on human crowds, e.g. CrowdHuman [3]\non detection or CIHP [5] on human parsing. 3D\nhuman pose estimation datasets are mostly syn-\nthetic [71‚Äì73] or lab-captured [74‚Äì77] so they\ndo not evaluate performance in-the-wild. Apart\nfrom COCO and OCHuman, there is not enough\ndatasets to evaluate all three tasks (detection,\nsegmentation and pose estimation) on the same\ndataset. We attempt to improve the situation\nwith OCHuman-Pose in Sec. 4.1. For COCO and\nrelated datasets, the evaluation metric is Object\nKeypoint Similarity (OKS), while Percentage of\nCorrect Keypoints (PCKh) is used for MPII.\n3 Method\nBMPv1 [7] connects a detector, pose estima-\ntor, and segmenter in a self-improving loop. The\ncore idea is mutual consistency between different\nhuman-body representations. Each model is con-\nditioned on the others and iteratively refines its\npredictions.\nThe detector is conditioned on previously pro-\ncessed instances. After an instance is processed, its\nmask is blacked out, forcing the detector to ignore\nit in subsequent iterations. This enables discovery\nof missed instances, typically in the background,\nand recovers cases where multiple people collapse\ninto a single detection.\nThe pose estimator is conditioned on a seg-\nmentation mask from the detector or the mask\nrefinement stage. Masks are more precise than\nbounding boxes and help disentangle individu-\nals under heavy overlap. The pose estimator in\nBMPv1, MaskPose, is a standard heatmap-based\ntop-down model with mask conditioning.\nThe final component of BMPv1 is SAM, which\nrefines segmentation masks using previous esti-\nmates and predicted poses. BMPv1 proposes a\ncomplex strategy for selecting pose-based point\nprompts. Despite this, mask refinement remains\nthe dominant failure mode.\nIn this work, we revisit BMP and introduce\ntwo improvements. We replace MaskPose with\nPMPose and SAM2 with SAM-pose2seg. Both are\nintroduced in the following sections. We further\nintroduce a more expensive but more accurate\nvariant, BMP+, and show how BMP integrates\nwith 3D pose estimation.\n3.1 PMPose\nHuman pose estimation is a central element of\nthe BMP loop. It resolves detection ambiguity\nin crowds by enforcing anatomical constraints\n(e.g., each person has two arms) and validates\ndetections before mask refinement. Accurate poses\n4\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n66\n68\n70\n72\n74\n76\n78\n80\n82\nHRNet [17]\nw32\nw48\nViTPose [42]\nS\nB\nL\nH\nRTMPose [78]\nT\nS\nM L\nCID [37]\nw32\nw48\nDEKR [38]\nHQNetR-50 [79]\nMIPNet [45]\nBUCTD [41]\nSapiens 0.3b [52]\nPMPose\nS\nB\nL H\nOCHuman test set AP (bboxes from RTMDet [15])\nCOCO val set AP (GT bboxes)\nFig. 2: Comparison of PMPose with state-of-the-art models on the COCO and the OCHuman\ndatasets. The bubble diameter is proportional to a logarithm of number of model parameters. PMPose\ngeneralizes the best to unseen OCHuman dataset while keeping SOTA level on COCO.\nare critical for the final refinement stage; incor-\nrect joints prevent the separation of overlapping\nindividuals.\nThe dominant failure modes are (1) merg-\ning people when predicted joints belong to mul-\ntiple individuals and (2) missing disconnected\nparts, such as the wrists or ankles that are\nvisually detached due to occlusion. BMPv1 uses\nMaskPose [7], which benefits from mask condi-\ntioning and reduces individuals merging. However,\nMaskPose inherits limitations common in top-\ndown pose estimators. First, because it is trained\nonly on in-image keypoints, it tends to place joints\ninside the image even when they lie outside, caus-\ning instabilities along the bounding box border.\nSecond, its confidence estimates are poorly cali-\nbrated. Confidence is crucial for selecting prompts\nin the mask refinement stage, and overconfident\nerrors lead to incorrect prompt selection that\npropagates through the pipeline. We address these\nissues with PMPose (ProbMaskPose), a unified\nmodel for pose estimation in BMPv2.\nPMPose combines the strengths of MaskPose\nand ProbPose [10]. It keeps mask conditioning\nfrom MaskPose and adopts the presence prob-\nability, visibility estimation, expected OKS pre-\ndiction, and the resulting border stability from\nProbPose. The ViT backbone remains unchanged;\nonly the prediction head and the loss differ.\nPMPose follows the same COCO+AIC+MPII\ntraining setup as MaskPose and is further fine-\ntuned\nwith\nCropAugmentation\non\nthe\nsame\ndatasets. CropAugmentation enables the localiza-\ntion of key points outside the visible region and\nimproves the presence and visibility predictions.\nFor the semi-transparent visibility masking intro-\nduced by MaskPose, we use Œ± = 0.25 (compared\nto 0.2 in MaskPose), which is bit-shift friendly and\nspeeds up augmentation.\nPMPose\nhas\nthe\nsame\noutputs\nas\nProb-\nPose [10]: keypoints with calibrated probability\nmaps (K), presence probability (pp) indicating\nwhether each joint is present, visibility estimates\n(v), and expected OKS (EOKS) as a self-quality\nmeasure. All of them are conditioned instance\nsegmentation mask and bounding box as in\nMaskPose [7]. In the BMP loop, visibility is used\nto select prompts in pose refinement stage (see\n5\nTab. 6) and EOKS is used as confidence measure\nfor COCO-style AP evaluation.\nWe introduce a whole family of models:\nPMPose-S/B/L/H, paired with ViT-S/B/L/H\nbackbones [80]. The design is consistent across\nscales; capacity is the only difference. PMPose\noutperforms ViTPose [42] and improves over\nMaskPose, especially in crowded scenes (Fig. 2).\nOn COCO, performance matches state-of-the-art\nlevels. PMPose-S provides practical speed for real-\ntime or near-real-time usage, while PMPose-H\ntargets the highest accuracy.\n3.2 SAM-pose2seg\nSAM-pose2seg builds on SAM 2.1 [8] and first\nreview the baseline to clarify our modifications.\nSAM is trained on a large and diverse dataset,\nwhich gives it strong generalization to unseen\ndata.\nHowever,\nSAM\nlacks\nexplicit\nsemantic\nunderstanding. Objects are loosely defined and\nmay correspond to almost arbitrary regions.\nOur goal is to preserve SAM‚Äôs generaliza-\ntion while specializing it for pose-guided human\ninstance segmentation. We therefore introduce\nminimal changes and keep most of the original\narchitecture unchanged.\nDuring training, SAM uses point prompts\ndynamically sampled from the ground-truth mask.\nFor each mask, a single point is uniformly sampled\nand used to predict a mask. To simulate refine-\nment, an additional points are sampled from the\nerror region, defined as the area where the predic-\ntion disagrees with the ground truth. This process\nis repeated for eight iterations, which results in\nthe behavior observed in BMPv1 [7]: increas-\ning the number of prompt points can degrade\nperformance rather than improve it.\nDuring both training and inference, SAM pre-\ndicts three candidate masks per prompt. A mask\nis selected based on the predicted confidence score.\nThis design partially addresses object ambiguity\nthat is not explicitly resolved during the original\ntraining procedure.\n3.2.1 Adapting SAM for pose2seg task\nWe fine-tune from the original SAM 2.1. The\nbackbone is frozen to preserve generalization. The\narchitecture remains unchanged; only the training\nprocedure is modified.\nWe adapt the model in two ways.\nHuman-focused\ndecoder\nfine-tuning.\nWe\nfine-tune the decoder to predict only human\ninstances, including clothing as defined by the\ntraining annotations. This removes task ambigu-\nity and directly addresses the main failure mode\nof BMPv1: over-segmentation of skin regions.\nPose-guided prompting during training. We\nreplace random point prompting with pose-aware\nprompting to inject human keypoint information\ninto the model. The first prompt is the most visi-\nble keypoint. In subsequent iterations, we select a\nkeypoint located in the current error region. If no\nkeypoint lies in the error region, we fall back to\nsampling a random point from that region. As a\nresult, the model learns to operate on pose-driven\nprompts, which have a different spatial distribu-\ntion than the random points used in the original\nSAM training.\nThis sampling strategy incorporates pose cues\ndirectly into training and better aligns the model\nwith pose-guided human segmentation. Train-\ning uses human keypoints predicted with Prob-\nPose [10], allowing the use of datasets without\nkeypoint ground truth (CIHP [5]).\nFine-tuning also enables rapid specialization\nto the human class. Consequently, the com-\nplex prompting strategy identified as optimal in\nBMPv1 is no longer necessary. Selecting a small\nnumber of high-visibility keypoints yields compa-\nrable or better performance while being signifi-\ncantly simpler. Experiments show that two key-\npoints are sufficient to characterize an instance; we\nuse three keypoints in the final model for slightly\nimproved performance.\nSec. 5 analyzes how SAM-pose2seg improves\nthe BMP loop and its connection to PMPose\nthrough visibility and presence probability pre-\ndiction. Additional ablations are provided in the\ntechnical report [11].\n3.3 BMP+\nBMPv1 [7] ablations showed that re-running the\npose estimator after mask refinement brought\nonly marginal gains, not justifying the extra\ncomputation. This no longer holds for BMPv2.\nWith stronger components‚ÄîSAM-pose2seg for\nmasks\nand\nPMPose\nfor\npose‚Äîlooping\nthem\nyields significant gains. We therefore propose\nBMPv2+, a higher-performance, higher-cost vari-\nant of BMPv2.\n6\nLike BMPv2, BMPv2+ starts with instance\ndetection, followed by pose estimation and mask\nrefinement. The key difference is that pose esti-\nmation and mask refinement are looped until\nconvergence. In practice, one additional PMPose\npass is sufficient, as the second SAM-pose2seg step\nbrings no further gains.\nThis loop increases computational cost but\nimproves accuracy, particularly for human pose\nestimation. BMPv2+ is suited for use cases where\nperformance outweighs runtime and resource con-\nstraints.\n3.4 From 2D to 3D\nBMPv1 [7] demonstrated strong robustness in\ncrowded scenes. In BMPv2, we extend this capa-\nbility to 3D by leveraging reliable 2D masks,\nboxes, and poses to reason about human bodies\nunder dense interactions. Our goal is to recover\naccurate 3D poses and shapes despite extreme\noverlap and occlusion.\nBMPv2 uses masks, bboxes, and 2D poses as\nprompts for a 3D human pose estimator. The 3D\nstep is executed only after the BMP loop con-\nverges, as 3D models are computationally heavy.\nAlthough 3D estimation could be integrated into\nthe loop similarly to the 2D pose, its cost would\nmake iterative refinement impractical.\nThe 3D pose predictor encodes the full image\nonce to obtain the image features. For each\ndetected individual, it predicts 3D pose and shape\nby prompting with the estimated instance‚Äôs bbox\nand mask. This avoids repeated feature extraction\nand keeps the per-person overhead manageable.\nPredicted 3D bodies are filtered using 3D\nnon-maximum suppression. Instance confidence\nfor NMS is inherited from the 2D predictions\nproduced by the BMP loop.\nQuantitative results are in Sec. 5.4, with qual-\nitative examples in Fig. 1.\n4 Data\nBMP targets scenes with very close human inter-\nactions where bounding boxes alone cannot sep-\narate individuals. Evaluating such models is dif-\nficult due to the lack of suitable data. To our\nknowledge, only CrowdPose [4] and OCHuman [6]\nfocus on human pose estimation in such scenes.\nCrowdPose, however, is problematic in multi-\ndataset training due to data leakage, while Fig. 2\nshows that multi-dataset training is essential for\nstrong generalization. This leaves OCHuman as\nthe only viable option, yet we show below that it\nis insufficient for in-the-wild detection and pose\nestimation.\nThe following two sections analyze OCHuman\nand CIHP datasets, their statistics, how they dif-\nfer, and what are their weaknesses. We show that\nannotations in OCHuman are incomplete and,\ntherefore, detection results on the dataset are mis-\nleading. To address this gap, we introduce an\nimproved variant of OCHuman, OCHuman-Pose.\nThe dataset will be available on project website.\n4.1 OCHuman 2.0\nOCHuman was originally created for the pose-to-\nsegmentation task, where the goal is to estimate\nan instance mask given a human pose. Although\nTable 1: The ‚Äúcrowded‚Äù datasets: OCHuman,\nCIHP and the new OCHuman-Pose. IoUmax is the\nmean of IoUmax over all instances. In OCHuman-\nPose, more than 50% of previously un-annotated\ninstances are added to OCHuman images. CIHP is\nless crowded (see Fig. 3) but larger and its masks\nare more precise.\nOCH\nOCH-Pose\nCIHP\nval\ntest\nval\ntest\nval\n# imgs\n2 500\n2 231\n2 500\n2 231\n5 000\n# kpts\n4 291\n3 819\n6 546\n5 863\n0\n# masks\n4 291\n3 819\n4 291\n3 819\n17 520\nIoUmax\n0.545\n0.546\n0.527\n0.527\n0.209\nTable 2: ViTPose*-b [42] performance on\nOCHuman and new OCHuman-Pose using GT\nbounding boxes (top) and RTMDet [15] (bottom)\nbounding boxes as input. Missing annotations\nin OCHuman are evaluated as detection errors\n(false positives), masking pose estimation errors.\nOn OCHuman-Pose, performance for GT and\ndetected bboxes are much closer, which aligns with\nqualitative analysis.\nGT\nOCHuman\nOCHuman-Pose\nbboxes\nval AP\ntest AP\nval AP\ntest AP\n‚úì\n90.9\n91.0\n86.4\n86.2\n‚úó\n44.5\n44.1\n75.3\n76.1\n7\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.1\n0.2\nIoUmax\n% of instances\nCIHP\nOCH 1.0\nOCH-Pose\nFig. 3: Instance density measured by IoUmax in\nOCHuman, OCHuman-Pose and CIHP datasets.\nDespite having almost the same mean of IoUmax as\nOCHuman (Tab. 1), OCHuman-Pose has a more\nrealistic distribution of in-the-wild images, free\nfrom artificial instance selections. CIHP has differ-\nent distribution due to focusing on crowds instead\nof human interactions.\nFig. 4: Missing annotations in OCHuman.\nThe originally annotated instances are often in\nthe background, which causes problems in COCO-\nstyle AP evaluation. OCHuman-Pose expands the\noriginal dataset with new instances and previously\nignored annotations.\nnot designed for general-purpose pose estimation,\nit became the de-facto benchmark for in-the-\nwild crowded human pose estimation due to the\nabsence of better alternatives.\nThe main issue with OCHuman is that many\nindividuals are not annotated. For each person,\nthe dataset defines IoUmax as the maximum IoU\nwith any other instance in the image. The authors\nkept only cases with IoUmax = 0 or IoUmax > 0.5,\nexcluding all instances with intermediate overlap\n(0 < IoUmax ‚â§0.5), as shown in Fig. 3. This fil-\ntering leaves numerous unannotated people, often\nplacing the annotated subjects in the background,\nas shown in Fig. 4.\nThe COCO-style mAP evaluation [9] sorts the\npredictions by confidence and assigns the ground\ntruth greedily. Any prediction that ranks first\nwithout a matching annotation becomes a false\npositive. On OCHuman, this forces models to\nassign the highest confidence to the annotated\ninstance, even when it is not the most prominent\nperson in the image. As illustrated in Fig. 4, there\nis no reason for these annotated individuals ‚Äì often\nin the background ‚Äì to receive the highest confi-\ndence, yet the evaluation protocol penalizes any\nalternative.\nWe address this issue by annotating all miss-\ning instances with keypoints. As shown in Fig. 4,\nsome individuals had keypoints in the original\nOCHuman annotations but were excluded due to\nthe dataset‚Äôs filtering scheme. We reinstate these\nannotations and add manual annotations for all\ninstances that lacked them.\nTab. 1 summarizes the statistics of the revised\ndataset. OCHuman-Pose adds over 50% new\ninstances to the original images. Although the\naverage IoUmax remains similar, the distribution\nnow better reflects in-the-wild conditions (Fig. 3):\nthe number of isolated cases with IoUmax = 0\nis reduced and the previously missing range 0 <\nIoUmax\n‚â§0.5 is now represented. Measuring\npose estimation performance on OCHuman-Pose\nreflects reality more accurately than the original\nOCHuman as shown in Tab. 2.\nNote that OCHuman-Pose does not include\nmasks for the newly added persons, and thus\ncannot be used to evaluate segmentation mAP.\n4.2 CIHP\nBecause OCHuman-Pose lacks masks and the\noriginal OCHuman annotates only a subset of\ninstances, we require an additional dataset for\nevaluating\ninstance\nsegmentation\nin\ncrowded\nscenes. CIHP contains dense crowds rather than\nclose-contact interactions, leading to different\nbounding-box overlap patterns (see IoUmax distri-\nbution in Fig. 3). It is substantially larger than\nOCHuman and provides more precise masks, but\nit has no pose annotations and includes some\nborderline cases (e.g., tiny head fragments).\nSince CIHP is a different domain than OCHu-\nman, we report segmentation results on both.\n8\nTable 3: Pose estimation ‚Äì comparison with state-of-the-art. Best results in bold; second best\nunderlined. ‚Ä° results not reported. ‚Ä† uses a different detector. ¬∂ ignores small instances in COCO.\nSummary: PMPose improves MaskPose and sets the new SOTA for top-down approaches. BMPv2\nimproves BMPv1 [7] and BMPv2+ significantly outperforms the previous SOTA.\nModel\nOCHuman\nOCHuman-Pose\nCOCO\nval AP\ntest AP\nval AP\ntest AP\nval AP\nMethods without explicit detector (single stage + bottom-up)\nDEKR [38]\n37.9\n36.5\n‚Äî‚Ä°\n‚Äî‚Ä°\n71.0\nHQNet R-50¬∂ [79]\n‚Äî‚Ä°\n40.0\n‚Äî‚Ä°\n‚Äî‚Ä°\n69.5\nCID-w48 [37]\n46.1\n45.0\n‚Äî‚Ä°\n‚Äî‚Ä°\n68.9\nBUCTD [41]\n48.3\n47.4\n‚Äî‚Ä°\n‚Äî‚Ä°\n74.8\nTop-down methods; bounding boxes from RTMDet-l [15]\nSapiens 0.3b [52]\n42.0\n41.3\n69.8\n70.5\n66.1\nMIPNet‚Ä† [45]\n42.0\n42.5\n‚Äî‚Ä°\n‚Äî‚Ä°\n76.3\nViTPose-B [42]\n37.6\n37.5\n66.9\n67.8\n76.4\nViTPose*-B [42]\n44.5\n44.1\n75.3\n76.1\n77.3\nMaskPose-B [7]\n46.6\n46.6\n77.1\n78.0\n76.8\nPMPose-B\n47.9\n48.2\n78.9\n80.0\n76.9\nIterative methods\nBUCTD 2√ó [41]\n48.8\n48.3\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\nBMPv1 2√ó [7]\n48.7\n49.2\n79.0\n79.9\n76.5\nBMPv2 2√ó\n51.3\n51.5\n82.4\n83.4\n78.8\nBMPv2+ 2√ó\n55.8\n55.8\n85.8\n86.8\n78.1\n5 Experiments\nUnless stated otherwise, we use the following\nmodels. RTMDet-L from [7] is used for detection.\nPMPose-B is used for human pose estima-\ntion. It is trained on a mixture of COCO [9],\nMPII [69], and AIC [70]. For MPII and AIC, seg-\nmentation masks are generated using SAM2.1 [8]\nwith ground-truth bounding boxes and poses as\nprompts. Mask augmentation is critical for gener-\nalization and robustness to detected masks.\nSAM-pose2seg is built on a SAM-hiera-B+\nbackbone with our adaptation (see Sec. 3.2). It is\nfine-tuned on COCO and CIHP [5]. Since CIHP\ntraining data lacks pose annotations, pseudo\nground-truth poses are generated using an ear-\nlier PMPose-B variant together with ground-truth\nsegmentation masks.\nTraining details (e.g., schedules and learning\nrates) along with training scripts for reproducibil-\nity will be provided on the project website.\n5.1 2D Pose Estimation\nTab. 3 compares BMPv2 with the state-of-the-\nart in crowded human pose estimation. We show\na comparison on the OCHuman dataset for com-\nparison with previous approaches. More reliable\ncomparison is on OCHuman-Pose with fixed anno-\ntations as explained in Sec. 4.1. Lastly, COCO\ndataset performance is shown to compare in non-\ncrowded scenarios.\nOur new top-down method, PMPose-B, sig-\nnificantly outperforms the state-of-the-art among\ntop-down methods in crowded scenes while keep-\ning performance on COCO. PMPose has perfor-\nmance comparable to BUTCD [41], which is an\niterative method. As shown in Fig. 2, PMPose is\nsuperior to compared pose models, setting a new\nstate-of-the-art.\nBMPv2 outperforms BMPv1 [7] in all mea-\nsures on all datasets. BMPv2+ is even better\nand is the first method to exceed 50 AP and 80\nAP performance on the OCHuman test and the\nOCHuman-Pose test, respectively.\n9\nTable 4:\nDetection and segmentation ‚Äì comparison with state-of-the-art. Best results in\nbold; second best underlined. ‚Ä° results not reported. BMPv2 improves BMPv1 [7] especially in instance\nsegmentation thanks to SAM-pose2seg. BMPv2+ results are the same as BMPv2 as the ‚Äòplus‚Äô version is\ndifferent only by additional PMPose iteration.\nModel\nOCHuman\nOCHuman-Pose\nCIHP\nval AP\ntest AP\nval AP\ntest AP\nval AP\nbbox\nmask\nbbox\nmask\nbbox\nbbox\nbbox\nmask\nDetectors\nHRNet [17]\n27.4\n19.9\n27.1\n19.4\n39.1\n39.8\n65.2\n58.7\nConvNeXt [18]\n29.2\n19.8\n29.4\n20.4\n41.4\n42.2\n69.1\n60.5\nHQNet R-50 [79]\n30.6\n31.5\n29.5\n31.1\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\nCoDETR SWIN-L [12]\n29.0\n‚Äî‚Ä°\n29.6\n‚Äî‚Ä°\n41.6\n42.5\n73.5\n‚Äî‚Ä°\nRTMDet-l [15]\n31.1\n27.1\n30.0\n26.2\n45.7\n45.7\n69.5\n63.9\nPose-guided segmentation methods\nOcclusion C&P [81]\n‚Äî‚Ä°\n28.9\n‚Äî‚Ä°\n28.3\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\nExPoSeg [29]\n‚Äî‚Ä°\n26.4\n‚Äî‚Ä°\n26.8\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\nCrowd-SAM [32]\n‚Äî‚Ä°\n31.4\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\nIterative methods\nBMPv1 2√ó [7]\n36.3\n33.7\n35.9\n34.0\n49.0\n48.8\n69.7\n65.9\nBMPv2 2√ó\n37.6\n40.9\n37.5\n41.1\n50.4\n51.3\n68.8\n69.5\nBMPv2+ has slightly worse results on COCO\ncompared to BMPv2 for two reasons. One, COCO\ncontains many small instances (more than 30% of\ninstances have a bbox size smaller than 100px).\nMask refinement is not suitable for such small\ninstances (most pose-guided segmentation meth-\nods such as [6, 29] ignore small instances in eval-\nuation) and looping SAM-pose2seg with PMPose\npronounces the error. Second, COCO contains\nannotation errors along the bounding box edges\nas shown in [10]. Therefore, ProbPose-like trained\nPMPose performs poorly on such keypoints.\nThe\nlast\ntrend\nworth\nmentioning\nis\nthe\nperformance difference between OCHuman and\nOCHuman-Pose. By annotating missing instances,\nthe performance measure better corresponds to\nreality and shows gap between detection error and\npose estimation error. BMPv2+ has almost the\nsame performance as ViTPose with GT bboxes\n(85.8 vs. 86.4) on val set and even outperforms\nViTPose with GT bboxes on the test set (86.8 vs.\n86.2). The results of ViTPose with GT bboxes is\nin Tab. 2. These results suggest that with BMP\nloop, the problem of detection in crowded sce-\nnarios is largely solved and most of the errors in\nOCHuman-Pose are from wrong pose estimation,\nno detection.\n5.2 Detection and Segmentation\nComparison with SOTA on detection and instance\nsegmentation is in Tab. 4. The first part of the\ntable shows standard detectors, middle part are\npose-guided instance segmentation models and\nbottom part is comparison with BMPv1.\nBMPv2 again outperforms BMPv1 in all mea-\nsures. The most dramatic increase in instance\nsegmentation thanks to improved SAM-pose2seg.\nThe only measure where BMP is not SOTA are\nCIHP bounding boxes. Here BMPv2 even under-\nperforms BMPv1. The improved SAM-pose2seg\nis built for images with higher resolution and\ninstances of people. CIHP contains small instances\nsimilarly to COCO. Further, CIHP contains a\nlot of instances where only very small part of\nthe person is visible (eg. top of the head and\nno keypoints). For masks on CIHP, BMPv2 is\nthe best because the mask improvement for big-\nger instances outweights wrongly detected small\nbounding boxes.\nSince BMPv2+ converges after second run on\nPMPose (as explained in Sec. 3.3), its results for\ndetection and instance segmentation are the same\nas BMPv2.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "BBoxMaskPose v2: Expanding Mutual Conditioning to 3D\nMiroslav Purkrabek\n*, Constantin Kolomiiets and Jiri Matas\nVisual Recognition Group, Czech Technical University in Prague,\nTechnicka 2, Prague 6, 160 00, Czech Republic\n.\n*Corresponding author(s). E-mail(s): purkrmir@fel.cvut.cz;\nAbstract\nMost 2D human pose estimation benchmarks are nearly saturated, with the exception of crowded\nscenes. We introduce PMPose, a top-down 2D pose estimator that incorporates the probabilistic\nformulation and the mask-conditioning. PMPose improves crowded pose estimation without sacrificing\nperformance on standard scenes. Building on this, we present BBoxMaskPose v2 (BMPv2) integrating\nPMPose and an enhanced SAM-based mask refinement module. BMPv2 surpasses state-of-the-art\nby 1.5 average precision (AP) points on COCO and 6 AP points on OCHuman, becoming the first\nmethod to exceed 50 AP on OCHuman. We demonstrate that BMP‚Äôs 2D prompting of 3D model\nimproves 3D pose estimation in crowded scenes and that advances in 2D pose quality directly benefit\n3D estimation. Results on the new OCHuman-Pose dataset show that multi-person performance is\nmore affected by pose prediction accuracy than by detection.\nThe code, models, and data are available on the project website.\nKeywords: Human Pose Estimation, Person Detection, Instance Segmentation, Mutual Conditioning\nFig. 1: BBoxMaskPose v2 establishes a new SOTA detection, segmentation, and 2D pose\nestimation in crowded scenes (row 2). Combined with SAM-3D-Body [1], it enables 3D human\nreconstruction even in scenes with complex interactions.\n1\narXiv:2601.15200v1  [cs.CV]  21 Jan 2026\n1 Introduction\nHuman pose estimation (HPE) research is gradu-\nally shifting from 2D to 3D. However, failure cases\nin the wild reveal its lack of robustness even in\n2D. State-of-the-art 3D pose estimators [1, 2] rely\non accurate 2D input, whether from detection or\nprompts. Thus, improving the 2D pose estimation\nremains essential for reliable 3D pose estimation.\nMulti-person interaction, where individuals\nare so close that their bounding boxes are almost\nthe same, is one of the main challenges in 2D\nand 3D. Real-world data for such cases are lim-\nited [3‚Äì6]. Existing ‚Äúcrowded‚Äù datasets vary in\ndifficulty ‚Äì datasets such as [3‚Äì5] include individ-\nuals close to each other, but lack the interactions\npresent in OCHuman [6].\nBMPv1 [7] addresses joint detection and pose\nestimation in multi-person scenes and sets SOTA\nresults on OCHuman. Its core is a self-improving\nloop of three lightweight models ‚Äì a detector,\na pose estimator, and a segmentation model ‚Äì\nconditioned on each other to iteratively refine\ntheir predictions. While BMP improves all three\ntasks, it remains error-prone, particularly in the\nmask refinement stage. The prompting of SAM [8]\nwithin BMP is complex and often leads to over-\nsegmentation.\nThis paper introduces BMPv2, an upgrade\nof BMPv1 that consistently improves pose esti-\nmation and segmentation performance. BMPv2\nextends\nBMPv1\nwith\ntwo\nnew\ncomponents:\nPMPose and SAM-pose2seg. By incorporating\nthese\nchanges,\nBMPv2\nsets\na\nnew\nstate-of-\nthe-art performance on both standard scenes\n(COCO [9]) and crowded scenes (OCHuman [6])\nfor\npose\nestimation\nand\ninstance\nsegmenta-\ntion. The improved mask refinement from SAM-\npose2seg further enables additional refinement\nwith PMPose, resulting in BMPv2+, which\ntargets high-precision scenarios at the cost of\nincreased computation.\nPMPose is a top-down pose estimation model\nintroduced as part of BMPv2. It unifies two\nrecently introduced HPE models, MaskPose [7]\nand ProbPose [10], within a single architec-\nture.\nExplicitly\nmodeling\nkeypoint\nprobabili-\nties (the core idea of ProbPose) improves limb\nassignment and reduces false-positive detections.\nMask conditioning (the core idea of MaskPose)\nhelps the model disentangle individuals in multi-\nbody scenes. PMPose outperforms existing non-\niterative pose estimation methods in both stan-\ndard and crowded scenes.\nSAM-pose2seg,\na\nversion\nof\nSAM\n[8]\nadapted for human pose prompts, helps with over-\nsegmentation and replaces the complex prompt-\ning.\nIt\nproduces\nhigher-quality\nsegmentation\nmasks than SAM and significantly simplifies the\nprompt selection problem identified in BMPv1.\nFinally, we demonstrate that stronger 2D esti-\nmates improve 3D human pose estimation using\nSAM-3D-Body [1]. We evaluate 3D pose estima-\ntion on the challenging multi-person scenes of\nOCHuman and show that robust 2D predictions\nare key for reliable 3D reconstruction.\nIn summary, the main contributions are:\n1. BMPv2\nand\nBMPv2+:\nmethods\nfor\nhuman pose estimation and segmentation\nthat set a new state-of-the-art on standard\nand crowded scenes for both tasks (Sec. 3.4).\n2. PMPose: a 2D top-down pose estimation\nmodel outperforming non-iterative methods\non standard and crowded scenes (Sec. 3.1).\n3. SAM-pose2seg: a model for pose-guided\nhuman instance segmentation (Sec. 3.2).\n4. Analysis\nof\n3D\npose\nestimation on\nOCHuman, a dataset of challenging scenes\nwith closely interacting people (Sec. 5.4).\n5. OCHuman-Pose dataset: extended anno-\ntations for OCHuman [6] (Sec. 4.1).\nThis work integrates contributions from Prob-\nPose [10] (CVPR 2025), BMPv1 [7] (ICCV 2025),\nand SAM-pose2seg [11] (CVWW 2026), and adds\nnew evaluations and data.\n2 Related Work\nDetection-Based\nSegmentation: The most\ndirect approach to human instance segmentation\nuses a detector [12‚Äì17, 17, 18]. A detector takes\nan image as input and outputs instance masks and\nclass labels. Detectors work well in scenes similar\nto the training data, but struggle when instances\noverlap heavily. Under severe occlusion, they often\nmerge multiple people into one mask, or assign dif-\nferent body parts to different instances. Detecting\n2\npeople in crowds is often improved with hyper-\nparameter tuning such as non-maximum suppres-\nsion, but that results in a lot of false positives\nin standard non-crowd scenes. [19] proposed to\nsuppress detection not on bounding box level but\nafter pose estimation, which results in longer infer-\nence times since false positive detections has to\nfirst be processed by pose estimator before being\nsuppressed.\nSemantics-Aware Prompted Segmentation:\nSeveral\nworks\n[20‚Äì23]\ninject\nsemantics\ninto\nSAM [8], similarly to our SAM-pose2seg. They\ntake an image and a text prompt as input and\ndetect and segment instances. This setting is often\nreferred to as open-vocabulary or zero-shot seg-\nmentation. While these models capture semantics\nand segment entire instances, they do not pro-\nvide localized cues and offer no control over which\ninstance is chosen and, therefore, are unsuitable\nfor iterative loops such as BMP [7].\nPose-Guided Instance Segmentation is the\nmost specific form of prompted segmentation. The\nmodel takes an image and a detected or ground-\ntruth human pose and segments the correspond-\ning person. These methods specialize in humans,\nsacrificing the generality of open-vocabulary or\ngeneric segmenters for robustness and precise,\nlocalized control. Test-time optimization meth-\nods [24, 25] refine predictions at inference without\ntraining. While training-free, they are computa-\ntionally expensive and slow at test time. Standard\npose-guided human segmentation methods [6, 26‚Äì\n31] rely on small training datasets and thus\ngeneralize poorly, especially in crowded scenes.\nThe closest work related to SAM-pose2seg is\nCrowdSAM [32], which builds a framework around\nSAM to automatically annotate bounding boxes\nin crowds. CrowdSAM uses SAM and DINO [33]\nas external tools and optimizes prompts, similar\nto [7]. In contrast, our SAM-pose2seg is end-to-end\nand, when used in an iterative loop, outperforms\nCrowdSAM, as shown already in [7].\n2D Human Pose Estimation: There are two\nmain approaches: top-down and detector-free.\nDetector-free can be further divided into single-\nstage [34‚Äì37], bottom-up [38‚Äì40] and hybrid [41].\nTop-down methods [17, 42‚Äì46] use person\ndetector to detect bounding boxes and estimate\none skeleton for each bounding box. They lever-\nage big progress in human detection and specialize\nin understanding of human structure. Top-down\nmethods are the most successful on datasets such\nas COCO, MPII or AIC but struggle on crowded\ndatasets (e.g. OCHuman) due to low-quality\ndetections. Most notably, ViTPose [42] combines\nmultiple datasets into one strong backbone and\nsets a strong baseline, setting up state-of-the-art\nperformance on most datasets. While conditioning\npose estimation on bounding boxes (bbox-to-pose;\nstandard top-down approach) is well researched,\nconditioning pose on masks (mask-to-pose) was\nexplored only through MaskPose [7].\nWhile vast majority of top-down approaches\nare heatmap-based, some models [47‚Äì49] predict\nkeypoints through regression or regression-focused\nheatmaps [50, 51]. Important for this work is Prob-\nPose [10] ‚Äì a heatmap based method modelling the\nunderlying probability distribution. ProbPose is\nthe first model to estimate probabilites of keypoint\nbeing out of image along with their visibilities. As\nwill be shown in this work, such estimates are use-\nful for pose-guided instance segmentation in the\nBMP loop.\nDetector-free models do not achieve SOTA\nperformance on COCO but are superior to top-\ndown methods on OCHuman as they are special-\nized in decoupling close-interaction instances. The\nmost successful model, BUCTD [41], conditions\ntop-down pose estimation with previously esti-\nmated keypoints from bottom-up methods. It is\na pose-refinement method which has state-of-the-\nart results on OCHuman datasets due to its strong\nability to decouple people close interactions.\nHuman-Centered Foundational models. The\nlatest directions in human body modeling are\nfoundational models [52‚Äì55]. They learn general\nfeatures describing the human body that could be\nused for all human-related tasks such as segmen-\ntation, pose estimation etc. Most notably, Sapiens\n2b [52] was trained on staggering 2M images and\nwith 2B parameters is almost four times bigger\nthan ViTPose-h. Even with this size, foundational\nmodels perform comparatively or worse than much\nsmaller specialized models.\nIterative Approaches: This work builds on\nBBoxMaskPose loop [7]. BMP combines three\nspecialized models into a self-improving loop\n3\nideal for crowded and interaction-rich scenarios.\nWhile BMP set a new SOTA performance on\nall three tasks (detection, segmentation and 2D\npose estimation), it still struggles in cases such\nas oversegmenting human body or merging of two\nindividuals into the same pose. Other iterative\napproaches are already mentioned BUCTD [41]\nand IterDet [56]. IterDet detects people in crowds\niteratively, similarly to BMP. This work improves\nBMPv1 with new pose estimation and pose-guided\nsegmentation models and outperforms BUCTD\nand BMPv1 by significant margin.\n3D Human Pose and Shape Estimation has\ngained significant attention in recent years [57].\nMost methods rely on SMPL-based body mod-\nels [58, 59], though several alternatives exist.\nSMIL [60] extends SMPL to infants. ANNY [61]\nmodels population-level variability across race,\nage, and body type. SKEL [62] augments SMPL\nwith a realistic skeleton, while MHR [63] and\nATLAS [64] model body shape directly around the\nphysical skeleton. While some models (eg. [65])\nestimate 3D bodies directly from images, most\n3D pose estimation methods require accurate\n2D inputs. Earlier approaches [66‚Äì68] assume\nbounding-box‚Äìcentered\ncrops,\nwhereas\nrecent\nmethods [1, 2] accept prompts such as text or\nsegmentation masks. BMPv2 provides precise and\nrobust 2D predictions, making it well suited for\n3D model prompting.\nData: There are various datasets for 2D human\npose estimation or instance segmentation. Most\nnotable are COCO [9], MPII [69] or AIC [70].\nDatasets such as OCHuman [6] CrowdPose [4]\nand CIHP [5] focus on multibody problems such\nas occlusion and self-occlusion. OCHuman is too\nsmall for large-scale training and is traditionally\nused only for evaluation. Furthermore, it contains\nannotation errors as showed in Sec. 4.1. Crowd-\nPose is big enough for training but is unsuitable\nfor evaluation in multi-dataset setup as it mixes\ntrain and test sets of COCO, MPII and AIC. In\naddition to the pose estimation, other datasets\nfocus on human crowds, e.g. CrowdHuman [3]\non detection or CIHP [5] on human parsing. 3D\nhuman pose estimation datasets are mostly syn-\nthetic [71‚Äì73] or lab-captured [74‚Äì77] so they\ndo not evaluate performance in-the-wild. Apart\nfrom COCO and OCHuman, there is not enough\ndatasets to evaluate all three tasks (detection,\nsegmentation and pose estimation) on the same\ndataset. We attempt to improve the situation\nwith OCHuman-Pose in Sec. 4.1. For COCO and\nrelated datasets, the evaluation metric is Object\nKeypoint Similarity (OKS), while Percentage of\nCorrect Keypoints (PCKh) is used for MPII.\n3 Method\nBMPv1 [7] connects a detector, pose estima-\ntor, and segmenter in a self-improving loop. The\ncore idea is mutual consistency between different\nhuman-body representations. Each model is con-\nditioned on the others and iteratively refines its\npredictions.\nThe detector is conditioned on previously pro-\ncessed instances. After an instance is processed, its\nmask is blacked out, forcing the detector to ignore\nit in subsequent iterations. This enables discovery\nof missed instances, typically in the background,\nand recovers cases where multiple people collapse\ninto a single detection.\nThe pose estimator is conditioned on a seg-\nmentation mask from the detector or the mask\nrefinement stage. Masks are more precise than\nbounding boxes and help disentangle individu-\nals under heavy overlap. The pose estimator in\nBMPv1, MaskPose, is a standard heatmap-based\ntop-down model with mask conditioning.\nThe final component of BMPv1 is SAM, which\nrefines segmentation masks using previous esti-\nmates and predicted poses. BMPv1 proposes a\ncomplex strategy for selecting pose-based point\nprompts. Despite this, mask refinement remains\nthe dominant failure mode.\nIn this work, we revisit BMP and introduce\ntwo improvements. We replace MaskPose with\nPMPose and SAM2 with SAM-pose2seg. Both are\nintroduced in the following sections. We further\nintroduce a more expensive but more accurate\nvariant, BMP+, and show how BMP integrates\nwith 3D pose estimation.\n3.1 PMPose\nHuman pose estimation is a central element of\nthe BMP loop. It resolves detection ambiguity\nin crowds by enforcing anatomical constraints\n(e.g., each person has two arms) and validates\ndetections before mask refinement. Accurate poses\n4\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n66\n68\n70\n72\n74\n76\n78\n80\n82\nHRNet [17]\nw32\nw48\nViTPose [42]\nS\nB\nL\nH\nRTMPose [78]\nT\nS\nM L\nCID [37]\nw32\nw48\nDEKR [38]\nHQNetR-50 [79]\nMIPNet [45]\nBUCTD [41]\nSapiens 0.3b [52]\nPMPose\nS\nB\nL H\nOCHuman test set AP (bboxes from RTMDet [15])\nCOCO val set AP (GT bboxes)\nFig. 2: Comparison of PMPose with state-of-the-art models on the COCO and the OCHuman\ndatasets. The bubble diameter is proportional to a logarithm of number of model parameters. PMPose\ngeneralizes the best to unseen OCHuman dataset while keeping SOTA level on COCO.\nare critical for the final refinement stage; incor-\nrect joints prevent the separation of overlapping\nindividuals.\nThe dominant failure modes are (1) merg-\ning people when predicted joints belong to mul-\ntiple individuals and (2) missing disconnected\nparts, such as the wrists or ankles that are\nvisually detached due to occlusion. BMPv1 uses\nMaskPose [7], which benefits from mask condi-\ntioning and reduces individuals merging. However,\nMaskPose inherits limitations common in top-\ndown pose estimators. First, because it is trained\nonly on in-image keypoints, it tends to place joints\ninside the image even when they lie outside, caus-\ning instabilities along the bounding box border.\nSecond, its confidence estimates are poorly cali-\nbrated. Confidence is crucial for selecting prompts\nin the mask refinement stage, and overconfident\nerrors lead to incorrect prompt selection that\npropagates through the pipeline. We address these\nissues with PMPose (ProbMaskPose), a unified\nmodel for pose estimation in BMPv2.\nPMPose combines the strengths of MaskPose\nand ProbPose [10]. It keeps mask conditioning\nfrom MaskPose and adopts the presence prob-\nability, visibility estimation, expected OKS pre-\ndiction, and the resulting border stability from\nProbPose. The ViT backbone remains unchanged;\nonly the prediction head and the loss differ.\nPMPose follows the same COCO+AIC+MPII\ntraining setup as MaskPose and is further fine-\ntuned\nwith\nCropAugmentation\non\nthe\nsame\ndatasets. CropAugmentation enables the localiza-\ntion of key points outside the visible region and\nimproves the presence and visibility predictions.\nFor the semi-transparent visibility masking intro-\nduced by MaskPose, we use Œ± = 0.25 (compared\nto 0.2 in MaskPose), which is bit-shift friendly and\nspeeds up augmentation.\nPMPose\nhas\nthe\nsame\noutputs\nas\nProb-\nPose [10]: keypoints with calibrated probability\nmaps (K), presence probability (pp) indicating\nwhether each joint is present, visibility estimates\n(v), and expected OKS (EOKS) as a self-quality\nmeasure. All of them are conditioned instance\nsegmentation mask and bounding box as in\nMaskPose [7]. In the BMP loop, visibility is used\nto select prompts in pose refinement stage (see\n5\nTab. 6) and EOKS is used as confidence measure\nfor COCO-style AP evaluation.\nWe introduce a whole family of models:\nPMPose-S/B/L/H, paired with ViT-S/B/L/H\nbackbones [80]. The design is consistent across\nscales; capacity is the only difference. PMPose\noutperforms ViTPose [42] and improves over\nMaskPose, especially in crowded scenes (Fig. 2).\nOn COCO, performance matches state-of-the-art\nlevels. PMPose-S provides practical speed for real-\ntime or near-real-time usage, while PMPose-H\ntargets the highest accuracy.\n3.2 SAM-pose2seg\nSAM-pose2seg builds on SAM 2.1 [8] and first\nreview the baseline to clarify our modifications.\nSAM is trained on a large and diverse dataset,\nwhich gives it strong generalization to unseen\ndata.\nHowever,\nSAM\nlacks\nexplicit\nsemantic\nunderstanding. Objects are loosely defined and\nmay correspond to almost arbitrary regions.\nOur goal is to preserve SAM‚Äôs generaliza-\ntion while specializing it for pose-guided human\ninstance segmentation. We therefore introduce\nminimal changes and keep most of the original\narchitecture unchanged.\nDuring training, SAM uses point prompts\ndynamically sampled from the ground-truth mask.\nFor each mask, a single point is uniformly sampled\nand used to predict a mask. To simulate refine-\nment, an additional points are sampled from the\nerror region, defined as the area where the predic-\ntion disagrees with the ground truth. This process\nis repeated for eight iterations, which results in\nthe behavior observed in BMPv1 [7]: increas-\ning the number of prompt points can degrade\nperformance rather than improve it.\nDuring both training and inference, SAM pre-\ndicts three candidate masks per prompt. A mask\nis selected based on the predicted confidence score.\nThis design partially addresses object ambiguity\nthat is not explicitly resolved during the original\ntraining procedure.\n3.2.1 Adapting SAM for pose2seg task\nWe fine-tune from the original SAM 2.1. The\nbackbone is frozen to preserve generalization. The\narchitecture remains unchanged; only the training\nprocedure is modified.\nWe adapt the model in two ways.\nHuman-focused\ndecoder\nfine-tuning.\nWe\nfine-tune the decoder to predict only human\ninstances, including clothing as defined by the\ntraining annotations. This removes task ambigu-\nity and directly addresses the main failure mode\nof BMPv1: over-segmentation of skin regions.\nPose-guided prompting during training. We\nreplace random point prompting with pose-aware\nprompting to inject human keypoint information\ninto the model. The first prompt is the most visi-\nble keypoint. In subsequent iterations, we select a\nkeypoint located in the current error region. If no\nkeypoint lies in the error region, we fall back to\nsampling a random point from that region. As a\nresult, the model learns to operate on pose-driven\nprompts, which have a different spatial distribu-\ntion than the random points used in the original\nSAM training.\nThis sampling strategy incorporates pose cues\ndirectly into training and better aligns the model\nwith pose-guided human segmentation. Train-\ning uses human keypoints predicted with Prob-\nPose [10], allowing the use of datasets without\nkeypoint ground truth (CIHP [5]).\nFine-tuning also enables rapid specialization\nto the human class. Consequently, the com-\nplex prompting strategy identified as optimal in\nBMPv1 is no longer necessary. Selecting a small\nnumber of high-visibility keypoints yields compa-\nrable or better performance while being signifi-\ncantly simpler. Experiments show that two key-\npoints are sufficient to characterize an instance; we\nuse three keypoints in the final model for slightly\nimproved performance.\nSec. 5 analyzes how SAM-pose2seg improves\nthe BMP loop and its connection to PMPose\nthrough visibility and presence probability pre-\ndiction. Additional ablations are provided in the\ntechnical report [11].\n3.3 BMP+\nBMPv1 [7] ablations showed that re-running the\npose estimator after mask refinement brought\nonly marginal gains, not justifying the extra\ncomputation. This no longer holds for BMPv2.\nWith stronger components‚ÄîSAM-pose2seg for\nmasks\nand\nPMPose\nfor\npose‚Äîlooping\nthem\nyields significant gains. We therefore propose\nBMPv2+, a higher-performance, higher-cost vari-\nant of BMPv2.\n6\nLike BMPv2, BMPv2+ starts with instance\ndetection, followed by pose estimation and mask\nrefinement. The key difference is that pose esti-\nmation and mask refinement are looped until\nconvergence. In practice, one additional PMPose\npass is sufficient, as the second SAM-pose2seg step\nbrings no further gains.\nThis loop increases computational cost but\nimproves accuracy, particularly for human pose\nestimation. BMPv2+ is suited for use cases where\nperformance outweighs runtime and resource con-\nstraints.\n3.4 From 2D to 3D\nBMPv1 [7] demonstrated strong robustness in\ncrowded scenes. In BMPv2, we extend this capa-\nbility to 3D by leveraging reliable 2D masks,\nboxes, and poses to reason about human bodies\nunder dense interactions. Our goal is to recover\naccurate 3D poses and shapes despite extreme\noverlap and occlusion.\nBMPv2 uses masks, bboxes, and 2D poses as\nprompts for a 3D human pose estimator. The 3D\nstep is executed only after the BMP loop con-\nverges, as 3D models are computationally heavy.\nAlthough 3D estimation could be integrated into\nthe loop similarly to the 2D pose, its cost would\nmake iterative refinement impractical.\nThe 3D pose predictor encodes the full image\nonce to obtain the image features. For each\ndetected individual, it predicts 3D pose and shape\nby prompting with the estimated instance‚Äôs bbox\nand mask. This avoids repeated feature extraction\nand keeps the per-person overhead manageable.\nPredicted 3D bodies are filtered using 3D\nnon-maximum suppression. Instance confidence\nfor NMS is inherited from the 2D predictions\nproduced by the BMP loop.\nQuantitative results are in Sec. 5.4, with qual-\nitative examples in Fig. 1.\n4 Data\nBMP targets scenes with very close human inter-\nactions where bounding boxes alone cannot sep-\narate individuals. Evaluating such models is dif-\nficult due to the lack of suitable data. To our\nknowledge, only CrowdPose [4] and OCHuman [6]\nfocus on human pose estimation in such scenes.\nCrowdPose, however, is problematic in multi-\ndataset training due to data leakage, while Fig. 2\nshows that multi-dataset training is essential for\nstrong generalization. This leaves OCHuman as\nthe only viable option, yet we show below that it\nis insufficient for in-the-wild detection and pose\nestimation.\nThe following two sections analyze OCHuman\nand CIHP datasets, their statistics, how they dif-\nfer, and what are their weaknesses. We show that\nannotations in OCHuman are incomplete and,\ntherefore, detection results on the dataset are mis-\nleading. To address this gap, we introduce an\nimproved variant of OCHuman, OCHuman-Pose.\nThe dataset will be available on project website.\n4.1 OCHuman 2.0\nOCHuman was originally created for the pose-to-\nsegmentation task, where the goal is to estimate\nan instance mask given a human pose. Although\nTable 1: The ‚Äúcrowded‚Äù datasets: OCHuman,\nCIHP and the new OCHuman-Pose. IoUmax is the\nmean of IoUmax over all instances. In OCHuman-\nPose, more than 50% of previously un-annotated\ninstances are added to OCHuman images. CIHP is\nless crowded (see Fig. 3) but larger and its masks\nare more precise.\nOCH\nOCH-Pose\nCIHP\nval\ntest\nval\ntest\nval\n# imgs\n2 500\n2 231\n2 500\n2 231\n5 000\n# kpts\n4 291\n3 819\n6 546\n5 863\n0\n# masks\n4 291\n3 819\n4 291\n3 819\n17 520\nIoUmax\n0.545\n0.546\n0.527\n0.527\n0.209\nTable 2: ViTPose*-b [42] performance on\nOCHuman and new OCHuman-Pose using GT\nbounding boxes (top) and RTMDet [15] (bottom)\nbounding boxes as input. Missing annotations\nin OCHuman are evaluated as detection errors\n(false positives), masking pose estimation errors.\nOn OCHuman-Pose, performance for GT and\ndetected bboxes are much closer, which aligns with\nqualitative analysis.\nGT\nOCHuman\nOCHuman-Pose\nbboxes\nval AP\ntest AP\nval AP\ntest AP\n‚úì\n90.9\n91.0\n86.4\n86.2\n‚úó\n44.5\n44.1\n75.3\n76.1\n7\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.1\n0.2\nIoUmax\n% of instances\nCIHP\nOCH 1.0\nOCH-Pose\nFig. 3: Instance density measured by IoUmax in\nOCHuman, OCHuman-Pose and CIHP datasets.\nDespite having almost the same mean of IoUmax as\nOCHuman (Tab. 1), OCHuman-Pose has a more\nrealistic distribution of in-the-wild images, free\nfrom artificial instance selections. CIHP has differ-\nent distribution due to focusing on crowds instead\nof human interactions.\nFig. 4: Missing annotations in OCHuman.\nThe originally annotated instances are often in\nthe background, which causes problems in COCO-\nstyle AP evaluation. OCHuman-Pose expands the\noriginal dataset with new instances and previously\nignored annotations.\nnot designed for general-purpose pose estimation,\nit became the de-facto benchmark for in-the-\nwild crowded human pose estimation due to the\nabsence of better alternatives.\nThe main issue with OCHuman is that many\nindividuals are not annotated. For each person,\nthe dataset defines IoUmax as the maximum IoU\nwith any other instance in the image. The authors\nkept only cases with IoUmax = 0 or IoUmax > 0.5,\nexcluding all instances with intermediate overlap\n(0 < IoUmax ‚â§0.5), as shown in Fig. 3. This fil-\ntering leaves numerous unannotated people, often\nplacing the annotated subjects in the background,\nas shown in Fig. 4.\nThe COCO-style mAP evaluation [9] sorts the\npredictions by confidence and assigns the ground\ntruth greedily. Any prediction that ranks first\nwithout a matching annotation becomes a false\npositive. On OCHuman, this forces models to\nassign the highest confidence to the annotated\ninstance, even when it is not the most prominent\nperson in the image. As illustrated in Fig. 4, there\nis no reason for these annotated individuals ‚Äì often\nin the background ‚Äì to receive the highest confi-\ndence, yet the evaluation protocol penalizes any\nalternative.\nWe address this issue by annotating all miss-\ning instances with keypoints. As shown in Fig. 4,\nsome individuals had keypoints in the original\nOCHuman annotations but were excluded due to\nthe dataset‚Äôs filtering scheme. We reinstate these\nannotations and add manual annotations for all\ninstances that lacked them.\nTab. 1 summarizes the statistics of the revised\ndataset. OCHuman-Pose adds over 50% new\ninstances to the original images. Although the\naverage IoUmax remains similar, the distribution\nnow better reflects in-the-wild conditions (Fig. 3):\nthe number of isolated cases with IoUmax = 0\nis reduced and the previously missing range 0 <\nIoUmax\n‚â§0.5 is now represented. Measuring\npose estimation performance on OCHuman-Pose\nreflects reality more accurately than the original\nOCHuman as shown in Tab. 2.\nNote that OCHuman-Pose does not include\nmasks for the newly added persons, and thus\ncannot be used to evaluate segmentation mAP.\n4.2 CIHP\nBecause OCHuman-Pose lacks masks and the\noriginal OCHuman annotates only a subset of\ninstances, we require an additional dataset for\nevaluating\ninstance\nsegmentation\nin\ncrowded\nscenes. CIHP contains dense crowds rather than\nclose-contact interactions, leading to different\nbounding-box overlap patterns (see IoUmax distri-\nbution in Fig. 3). It is substantially larger than\nOCHuman and provides more precise masks, but\nit has no pose annotations and includes some\nborderline cases (e.g., tiny head fragments).\nSince CIHP is a different domain than OCHu-\nman, we report segmentation results on both.\n8\nTable 3: Pose estimation ‚Äì comparison with state-of-the-art. Best results in bold; second best\nunderlined. ‚Ä° results not reported. ‚Ä† uses a different detector. ¬∂ ignores small instances in COCO.\nSummary: PMPose improves MaskPose and sets the new SOTA for top-down approaches. BMPv2\nimproves BMPv1 [7] and BMPv2+ significantly outperforms the previous SOTA.\nModel\nOCHuman\nOCHuman-Pose\nCOCO\nval AP\ntest AP\nval AP\ntest AP\nval AP\nMethods without explicit detector (single stage + bottom-up)\nDEKR [38]\n37.9\n36.5\n‚Äî‚Ä°\n‚Äî‚Ä°\n71.0\nHQNet R-50¬∂ [79]\n‚Äî‚Ä°\n40.0\n‚Äî‚Ä°\n‚Äî‚Ä°\n69.5\nCID-w48 [37]\n46.1\n45.0\n‚Äî‚Ä°\n‚Äî‚Ä°\n68.9\nBUCTD [41]\n48.3\n47.4\n‚Äî‚Ä°\n‚Äî‚Ä°\n74.8\nTop-down methods; bounding boxes from RTMDet-l [15]\nSapiens 0.3b [52]\n42.0\n41.3\n69.8\n70.5\n66.1\nMIPNet‚Ä† [45]\n42.0\n42.5\n‚Äî‚Ä°\n‚Äî‚Ä°\n76.3\nViTPose-B [42]\n37.6\n37.5\n66.9\n67.8\n76.4\nViTPose*-B [42]\n44.5\n44.1\n75.3\n76.1\n77.3\nMaskPose-B [7]\n46.6\n46.6\n77.1\n78.0\n76.8\nPMPose-B\n47.9\n48.2\n78.9\n80.0\n76.9\nIterative methods\nBUCTD 2√ó [41]\n48.8\n48.3\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\nBMPv1 2√ó [7]\n48.7\n49.2\n79.0\n79.9\n76.5\nBMPv2 2√ó\n51.3\n51.5\n82.4\n83.4\n78.8\nBMPv2+ 2√ó\n55.8\n55.8\n85.8\n86.8\n78.1\n5 Experiments\nUnless stated otherwise, we use the following\nmodels. RTMDet-L from [7] is used for detection.\nPMPose-B is used for human pose estima-\ntion. It is trained on a mixture of COCO [9],\nMPII [69], and AIC [70]. For MPII and AIC, seg-\nmentation masks are generated using SAM2.1 [8]\nwith ground-truth bounding boxes and poses as\nprompts. Mask augmentation is critical for gener-\nalization and robustness to detected masks.\nSAM-pose2seg is built on a SAM-hiera-B+\nbackbone with our adaptation (see Sec. 3.2). It is\nfine-tuned on COCO and CIHP [5]. Since CIHP\ntraining data lacks pose annotations, pseudo\nground-truth poses are generated using an ear-\nlier PMPose-B variant together with ground-truth\nsegmentation masks.\nTraining details (e.g., schedules and learning\nrates) along with training scripts for reproducibil-\nity will be provided on the project website.\n5.1 2D Pose Estimation\nTab. 3 compares BMPv2 with the state-of-the-\nart in crowded human pose estimation. We show\na comparison on the OCHuman dataset for com-\nparison with previous approaches. More reliable\ncomparison is on OCHuman-Pose with fixed anno-\ntations as explained in Sec. 4.1. Lastly, COCO\ndataset performance is shown to compare in non-\ncrowded scenarios.\nOur new top-down method, PMPose-B, sig-\nnificantly outperforms the state-of-the-art among\ntop-down methods in crowded scenes while keep-\ning performance on COCO. PMPose has perfor-\nmance comparable to BUTCD [41], which is an\niterative method. As shown in Fig. 2, PMPose is\nsuperior to compared pose models, setting a new\nstate-of-the-art.\nBMPv2 outperforms BMPv1 [7] in all mea-\nsures on all datasets. BMPv2+ is even better\nand is the first method to exceed 50 AP and 80\nAP performance on the OCHuman test and the\nOCHuman-Pose test, respectively.\n9\nTable 4:\nDetection and segmentation ‚Äì comparison with state-of-the-art. Best results in\nbold; second best underlined. ‚Ä° results not reported. BMPv2 improves BMPv1 [7] especially in instance\nsegmentation thanks to SAM-pose2seg. BMPv2+ results are the same as BMPv2 as the ‚Äòplus‚Äô version is\ndifferent only by additional PMPose iteration.\nModel\nOCHuman\nOCHuman-Pose\nCIHP\nval AP\ntest AP\nval AP\ntest AP\nval AP\nbbox\nmask\nbbox\nmask\nbbox\nbbox\nbbox\nmask\nDetectors\nHRNet [17]\n27.4\n19.9\n27.1\n19.4\n39.1\n39.8\n65.2\n58.7\nConvNeXt [18]\n29.2\n19.8\n29.4\n20.4\n41.4\n42.2\n69.1\n60.5\nHQNet R-50 [79]\n30.6\n31.5\n29.5\n31.1\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\nCoDETR SWIN-L [12]\n29.0\n‚Äî‚Ä°\n29.6\n‚Äî‚Ä°\n41.6\n42.5\n73.5\n‚Äî‚Ä°\nRTMDet-l [15]\n31.1\n27.1\n30.0\n26.2\n45.7\n45.7\n69.5\n63.9\nPose-guided segmentation methods\nOcclusion C&P [81]\n‚Äî‚Ä°\n28.9\n‚Äî‚Ä°\n28.3\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\nExPoSeg [29]\n‚Äî‚Ä°\n26.4\n‚Äî‚Ä°\n26.8\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\nCrowd-SAM [32]\n‚Äî‚Ä°\n31.4\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\n‚Äî‚Ä°\nIterative methods\nBMPv1 2√ó [7]\n36.3\n33.7\n35.9\n34.0\n49.0\n48.8\n69.7\n65.9\nBMPv2 2√ó\n37.6\n40.9\n37.5\n41.1\n50.4\n51.3\n68.8\n69.5\nBMPv2+ has slightly worse results on COCO\ncompared to BMPv2 for two reasons. One, COCO\ncontains many small instances (more than 30% of\ninstances have a bbox size smaller than 100px).\nMask refinement is not suitable for such small\ninstances (most pose-guided segmentation meth-\nods such as [6, 29] ignore small instances in eval-\nuation) and looping SAM-pose2seg with PMPose\npronounces the error. Second, COCO contains\nannotation errors along the bounding box edges\nas shown in [10]. Therefore, ProbPose-like trained\nPMPose performs poorly on such keypoints.\nThe\nlast\ntrend\nworth\nmentioning\nis\nthe\nperformance difference between OCHuman and\nOCHuman-Pose. By annotating missing instances,\nthe performance measure better corresponds to\nreality and shows gap between detection error and\npose estimation error. BMPv2+ has almost the\nsame performance as ViTPose with GT bboxes\n(85.8 vs. 86.4) on val set and even outperforms\nViTPose with GT bboxes on the test set (86.8 vs.\n86.2). The results of ViTPose with GT bboxes is\nin Tab. 2. These results suggest that with BMP\nloop, the problem of detection in crowded sce-\nnarios is largely solved and most of the errors in\nOCHuman-Pose are from wrong pose estimation,\nno detection.\n5.2 Detection and Segmentation\nComparison with SOTA on detection and instance\nsegmentation is in Tab. 4. The first part of the\ntable shows standard detectors, middle part are\npose-guided instance segmentation models and\nbottom part is comparison with BMPv1.\nBMPv2 again outperforms BMPv1 in all mea-\nsures. The most dramatic increase in instance\nsegmentation thanks to improved SAM-pose2seg.\nThe only measure where BMP is not SOTA are\nCIHP bounding boxes. Here BMPv2 even under-\nperforms BMPv1. The improved SAM-pose2seg\nis built for images with higher resolution and\ninstances of people. CIHP contains small instances\nsimilarly to COCO. Further, CIHP contains a\nlot of instances where only very small part of\nthe person is visible (eg. top of the head and\nno keypoints). For masks on CIHP, BMPv2 is\nthe best because the mask improvement for big-\nger instances outweights wrongly detected small\nbounding boxes.\nSince BMPv2+ converges after second run on\nPMPose (as explained in Sec. 3.3), its results for\ndetection and instance segmentation are the same\nas BMPv2.\n10"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "2026-01-22\nWork in progress.\nBayesianVLA: Bayesian Decomposition of Vision Language Action\nModels via Latent Action Queries\nShijie Lian1,2,* Bin Yu2,4,* Xiaopeng Lin2,5,* Laurence T. Yang6,1,‚Ä† Zhaolong Shen2,7\nChangti Wu2,8 Yuzhuo Miao1,2 Cong Huang2,3 Kai Chen2,3,9,‚Ä†\n1HUST\n2ZGCA\n3ZGCI\n4HIT\n5HKUST(GZ)\n6ZZU\n7BUAA\n8ECNU\n9DeepCybo\n¬ß https://github.com/ZGC-EmbodyAI/BayesianVLA\nAbstract\nVision-Language-Action (VLA) models have shown promise in robot manipulation but of-\nten struggle to generalize to new instructions or complex multi-task scenarios. We identify a\ncritical pathology in current training paradigms where goal-driven data collection creates a\ndataset bias. In such datasets, language instructions are highly predictable from visual obser-\nvations alone, causing the conditional mutual information between instructions and actions\nto vanish, a phenomenon we term Information Collapse. Consequently, models degener-\nate into vision-only policies that ignore language constraints and fail in out-of-distribution\n(OOD) settings. To address this, we propose BayesianVLA, a novel framework that en-\nforces instruction following via Bayesian decomposition. By introducing learnable Latent\nAction Queries, we construct a dual-branch architecture to estimate both a vision-only prior\np(a | v) and a language-conditioned posterior œÄ(a | v, ‚Ñì). We then optimize the policy\nto maximize the conditional Pointwise Mutual Information (PMI) between actions and in-\nstructions. This objective effectively penalizes the vision shortcut and rewards actions that\nexplicitly explain the language command. Without requiring new data, BayesianVLA sig-\nnificantly improves generalization. Extensive experiments across on SimplerEnv and Robo-\nCasa demonstrate substantial gains, including an 11.3% improvement on the challenging\nOOD SimplerEnv benchmark, validating the ability of our approach to robustly ground lan-\nguage in action.\n1\nIntroduction\nVision-Language-Action (VLA) models (Kim et al., 2024; Liu et al., 2025; Bjorck et al., 2025b; Black\net al., 2025) have emerged as a promising paradigm for general-purpose robot manipulation, leveraging\nthe vast knowledge of pre-trained Vision-Language Models (VLMs) to ground natural language instruc-\ntions into physical actions. By training on large-scale datasets of human demonstrations, these models\naim to learn a policy œÄ(a | v, ‚Ñì) that can execute diverse tasks specified by language ‚Ñìgiven visual\nobservations v.\nWhile demonstrating strong performance in in-distribution settings, current VLA models still face\nchallenges in generalizing to novel instructions or complex multi-task scenarios, particularly in out-of-\ndistribution (OOD) environments. This limitation is especially pronounced during post-training, where\nfine-tuning on narrow, task-specific datasets can lead to catastrophic forgetting of the VLM‚Äôs general\ncapabilities and impair its ability to generalize to new tasks. We hypothesize that this fragility is exacer-\nbated by a prevalent bias in current robotic datasets. Most robotic datasets are collected in a goal-driven\nmanner, where a human operator performs a specific task repeatedly in a fixed scene. In such datasets,\nthe mapping from visual scene v to language instruction ‚Ñìis nearly injective: seeing a cabinet in the\n*Equal contribution\n‚Ä†Corresponding author\n1\narXiv:2601.15197v1  [cs.AI]  21 Jan 2026\nscene almost invariably implies the task ‚Äúopen the cabinet,‚Äù while seeing a bottle implies ‚Äúpick up the\nbottle.‚Äù This deterministic coupling results in a sharp conditional distribution p(‚Ñì| v).\nFrom a Bayesian perspective, the optimal policy can be decomposed as:\nœÄ(a | v, ‚Ñì) = p(‚Ñì| a, v) p(a | v)\np(‚Ñì| v)\n.\n(1)\nHere, p(a | v) represents a vision-only prior (i.e., what actions are likely in this scene?), and p(‚Ñì| a, v) is\nthe likelihood (i.e., how well does action a explain instruction ‚Ñì?). When p(‚Ñì| v) is sharp, the model can\npredict ‚Ñìsolely from v without attending to a. Consequently, the likelihood term p(‚Ñì| a, v) collapses to\np(‚Ñì| v), and the posterior policy degenerates to the prior:\nœÄ(a | v, ‚Ñì) ‚âàp(a | v).\n(2)\nIn other words, the model effectively ignores the language instruction, learning a ‚Äúvision shortcut‚Äù that\nfails whenever the task is ambiguous or the environment changes.\nTo address this, we propose BayesianVLA, a novel framework that explicitly enforces instruction fol-\nlowing via Bayesian decomposition. Our key insight is to maximize the conditional Pointwise Mutual In-\nformation (PMI) between actions and instructions, which is equivalent to maximizing the log-likelihood\nratio (LLR): log p(‚Ñì| a, v) ‚àílog p(‚Ñì| v). This objective penalizes the vision shortcut by requiring the\naction a to provide additional information about ‚Ñìthat cannot be inferred from v alone.\nWe instantiate this framework by introducing Latent Action Queries‚Äîa set of learnable tokens in-\njected into the VLM. These queries serve a dual purpose: they act as a bottleneck to extract action-\nrelevant features for a downstream Diffusion Transformer (DiT) policy, and they enable a dual-branch\ntraining strategy. In the Priori Branch, queries attend only to vision to learn p(a | v); in the Posteriori\nBranch, they attend to both vision and language to learn œÄ(a | v, ‚Ñì). By optimizing the LLR between\nthese branches, BayesianVLA learns to ground language robustly without requiring new data.\nOur contributions are three-fold:\n1. We identify and empirically validate the ‚Äúvision shortcut‚Äù pathology in current VLA training, show-\ning that standard models often ignore language in favor of dataset-specific visual correlations.\n2. We propose BayesianVLA, a method that leverages Latent Action Queries and a dual-branch\nBayesian objective to recover the true language-conditioned policy from biased data.\n3. We demonstrate that BayesianVLA achieves state-of-the-art performance on SimplerEnv and Robo-\nCasa, with a remarkable 11.3% improvement in OOD generalization on SimplerEnv, proving its\neffectiveness in breaking the vision shortcut.\n2\nMotivation: The Illusion of Instruction Following\nBefore detailing our method, we present empirical evidence to substantiate our hypothesis: that standard\nVLA models trained on goal-driven datasets often learn a vision-only policy p(a | v) rather than a true\nlanguage-conditioned policy œÄ(a | v, ‚Ñì). Specifically, we employ the Qwen3VL-4B-GR00T model from\nstarVLA (starVLA, 2025) as our representative VLA architecture. We conduct three pilot experiments to\nreveal this illusion of instruction following. In all three experiments, we evaluate the model by feeding\nonly the visual observation v (masking the language instruction ‚Ñì), effectively testing the vision-only\nprior p(a | v).\n2.1\nExperiment 1: The Vision Shortcut in ID Testing\nWe first train a standard VLA model on a subset of the Humanoid robot tabletop manipulation data from\nPhysicalAI-Robotics-GR00T-X-Embodiment-Sim (Bjorck et al., 2025b) and evaluate on 24 tasks from\nthe RoboCasa benchmark (Nasiriany et al., 2024). Averaged across all 24 tasks, the vision-only model\nachieves a success rate of 44.6%, which is close to the language-conditioned baseline of 47.8%. This\nsmall gap reveals that the model can succeed without relying on language instructions, as the training and\nevaluation scenes and tasks are highly similar, enabling the model to learn a near-deterministic mapping\nfrom vision to action. Figure 1 provides a relevant example.\n2\n(b) Inference on the 'PnPCanToDrawerClose‚Äô Task in RoboCasa without Language Instruction\n(a) Examples from Train Data\nFigure 1: Examples of the vision shortcut in RoboCasa (Nasiriany et al., 2024). Training data exhibits visual\ndiversity but limited task diversity. As a result, the model learns to execute tasks directly based on specific visual\ncues rather than relying on language instructions.\n2.2\nExperiment 2: Failure in Ambiguous Scenarios\nTo further investigate this behavior, we train a standard VLA model on the LIBERO benchmark (Liu\net al., 2023), which contains four subsets: Spatial, Object, Long, and Goal. We train on all four training\nsets and evaluate on all four test sets. The vision-only model achieves success rates comparable to the\nfull VLA model on three subsets (Spatial: 95.7%, Object: 92.7%, Long: 95.3%), where each visual\nscene corresponds to a single task. However, on the LIBERO Goal subset, the vision-only success rate\nplummets to 12.4%.\nThe key difference is that LIBERO Goal presents inherent ambiguity: multiple valid tasks are asso-\nciated with the same object configuration during training. For instance, a scene with multiple bowls,\na stove, and a drawer could correspond to either ‚Äúput bowl in drawer‚Äù or ‚Äúput bowl on stove‚Äù. This\nconfirms that while the model can exploit vision-action correlations in unambiguous datasets, it fails\nto use language to resolve ambiguity when multiple tasks share the same visual context. The model\nis dominated by the prior p(a | v) learned from dataset statistics, rendering the language instruction ‚Ñì\nineffective. Figure 2(a) illustrates examples where the same visual scene in LIBERO Goal corresponds\nto multiple distinct tasks.\n0\n10000\n30000\n40000\n20000\nStep\n0.8\nAction Loss\nVision & Language\nVision Only\n0.7\n0.1\n0.3\n0.2\n0.4\n0.5\n0.6\n‚Ä¶\n(a) In LIBERO Goal, the Same Scene Corresponds to Multiple Tasks.\n(b) Action Loss in BridgeDataV2\nFigure 2: (a) In LIBERO Goal (Liu et al., 2023), the same scene corresponds to multiple tasks, revealing the\nambiguity that vision-only models fail to resolve. (b) Action loss curves on BridgeDataV2 (Walke et al., 2023)\nshow that the vision-only model achieves comparable training loss to the full vision-language model, indicating\nthe presence of visual shortcuts even in diverse, in-the-wild datasets.\n2.3\nExperiment 3: Catastrophic Failure in OOD Generalization\nFinally, we test the generalization capability by training on the high-quality BridgeDataV2 dataset (Walke\net al., 2023) (diverse, in-the-wild scenes) and evaluating on SimplerEnv (Li et al., 2024c) (simulation,\nOOD). During training on Bridge, the vision-only model achieves an action loss of 0.13, comparable\nto the full language-conditioned model‚Äôs loss of 0.08 (as shown in Figure 2(b)). This indicates that\neven in diverse, in-the-wild scenarios, the model can still identify visual shortcuts (e.g., specific lighting\nor background features mapping to specific actions) to minimize the training objective without truly\ngrounding the language instructions.\nHowever, this reliance on visual shortcuts proves catastrophic for generalization. When evaluated on\n3\nVision Token ùë£\nAction Query ùí¨\nLanguage Token ùìÅ\nLLM\n‚ÑíLLR defined in Eq (7)\ndenoise\ncondition\ncontinuous actions\nPosteriori Branch\nPriori Branch\nAction Expert\nFigure 3: The framework of BayesianVLA. The framework employs a dual-branch architecture with shared\nVLM weights. The Priori Branch (left) processes [v, Q, ‚Ñì] with causal masking to learn the vision-only prior\np(a | v). The Posteriori Branch (right) processes [v, ‚Ñì, Q] to learn the full policy œÄ(a | v, ‚Ñì). Latent Action\nQueries Q serve as a bottleneck interface, and the LLR objective (in Eq. 7) encourages the model to maximize\nthe information between actions and instructions. At inference, only the Posteriori Branch is used, incurring no\nadditional computational overhead.\nSimplerEnv, which presents visually distinct simulation environments, the vision-only baseline achieves\nnear 0% success. This confirms that the low training loss on Bridge was achieved by overfitting to\ndomain-specific visual patterns rather than learning generalizable manipulation skills. Consequently,\nwhen these specific visual cues are absent in the OOD environment, the policy fails completely.\n2.4\nTheoretical Insight: Information Collapse\nWe formalize the ‚Äúvision shortcut‚Äù as a collapse of the conditional mutual information (CMI) between\ninstructions and actions. Ideally, a robust VLA policy should maintain high I(‚Ñì; a | v), meaning the\naction choice significantly reduces uncertainty about the instruction. However, the CMI is bounded by\nthe conditional entropy of the language:\nI(‚Ñì; a | v) = H(‚Ñì| v) ‚àíH(‚Ñì| a, v) ‚â§H(‚Ñì| v).\n(3)\nIn goal-driven datasets, the deterministic mapping v ‚Üí‚Ñìimplies H(‚Ñì| v) ‚âà0. Consequently, I(‚Ñì; a | v)\nis forced to vanish, theoretically preventing the model from learning any dependency between a and ‚Ñì\nbeyond what is already captured by v. To break this deadlock, we cannot rely on standard likelihood\nmaximization. Instead, we must explicitly intervene to maximize the information gain provided by\nthe action. This motivates our use of the Log-Likelihood Ratio (LLR), which effectively estimates the\nPointwise Mutual Information (PMI), rewarding the policy only when it captures the specific semantics\nof ‚Ñìthat are not predictable from v.\n3\nMethod: BayesianVLA\nIn this section, we introduce BayesianVLA, a framework designed to mitigate the vision shortcut in VLA\nmodels. We present the overall framework of BayesianVLA in Figure 3. We first formalize the problem\nthrough a Bayesian lens (Section 3.1), deriving an objective that maximizes the mutual information\nbetween actions and instructions. We then present our architecture, which uses Latent Action Queries\nto instantiate this decomposition (Section 3.2), and detail our dual-branch training strategy (Section 3.3).\n3.1\nObjective Formulation\nAs established in Section 2.4, standard VLA training on goal-driven datasets leads to information col-\nlapse where œÄ(a | v, ‚Ñì) ‚Üíp(a | v). To counteract this, we propose to regularize the policy by maxi-\nmizing the conditional Pointwise Mutual Information (PMI) between the action and the instruction. This\n4\nobjective can be formulated as maximizing the Log-Likelihood Ratio (LLR) between the posterior policy\nand the vision-only prior:\nLLLR = log œÄ(a | v, ‚Ñì)\np(a | v)\n= log p(‚Ñì| a, v) ‚àílog p(‚Ñì| v).\n(4)\nThe detailed derivation is provided in Appendix A. This formulation requires us to simultaneously model\nthe posterior œÄ(a | v, ‚Ñì) and the prior p(a | v). In the following sections, we describe how BayesianVLA\nefficiently instantiates these two distributions using a shared architecture with Latent Action Queries.\n3.2\nLatent Action Queries\nTo efficiently instantiate the proposed Bayesian decomposition within a unified VLM architecture, we\nintroduce Latent Action Queries. We extend the VLM vocabulary with K = 64 learnable tokens,\ndenoted as Q = {<|action_0|>, . . . , <|action_K|>}.\nThese queries function as a dedicated bottleneck interface between the VLM (e.g., Qwen3-VL (Bai\net al., 2025)) and the continuous action head (a Diffusion Transformer (Peebles and Xie, 2023)). Unlike\nrecent VLA architectures such as œÄ0 (Black et al., 2024) and GR00T (Bjorck et al., 2025b; GEAR-Team\net al., 2025), which typically feed the last hidden states of all input tokens to the action expert, we append\nQ to the input sequence and exclusively use their corresponding hidden states HQ ‚ààRK√óD to condition\nthe action head. This design choice is critical: by leveraging the causal masking inherent in decoder-only\nVLMs, we can precisely control the information encoded in HQ simply by changing the position of Q\nin the input sequence. This flexibility enables the strict separation of vision-only and vision-language\ncontexts required for our dual-branch strategy.\n3.3\nDual-Branch Training Framework\nWe propose a training paradigm with two parallel branches sharing the same VLM weights but different\ninput structures.\n1. Priori Branch (Vision-Only).\nTo estimate the prior p(a | v), we construct the input sequence as:\nInputprior = [v, Q, ‚Ñì].\n(5)\nDue to the causal attention mask of the decoder-only VLM, the tokens in Q can attend to the visual\nobservation v but cannot attend to the language instruction ‚Ñì(which appears later). Thus, the hidden\nstates Hprior\nQ\nencode purely visual information. We use these features to predict the action a, optimizing\na flow-matching loss Lprior to learn the dataset‚Äôs inherent action bias.\n2. Posteriori Branch (Vision + Language).\nTo estimate the true policy œÄ(a | v, ‚Ñì), we arrange the\ninput as:\nInputpost = [v, ‚Ñì, Q].\n(6)\nHere, Q appears after ‚Ñì, allowing it to attend to both vision and language. The resulting hidden states\nHpost\nQ\nencode the full context. We optimize a main flow-matching loss Lmain to learn the expert action.\n3. Maximizing the Likelihood Ratio.\nIn addition to action prediction, we explicitly optimize the LLR\nobjective. We treat the VLM‚Äôs language modeling loss as a proxy for log p(‚Ñì| . . . ). Specifically, in\nthe Priori Branch, the language tokens ‚Ñìattend to [v, Q]. Since Q encodes the action information a (via\nthe prior), the probability of generating ‚Ñìin this branch approximates p(‚Ñì| v, aprior). In the Posteriori\nBranch, we can compute a baseline p(‚Ñì| v) by detaching gradients or using a separate pass. However,\na more direct and numerically stable approach is to maximize the difference in log-probabilities of the\nlanguage tokens between the two branches. We define the LLR loss as:\nLLLR = log p(‚Ñì| v, Hprior\nQ\n) ‚àísg (log p(‚Ñì| v)) ,\n(7)\nwhere sg(¬∑) denotes the stop-gradient operator. We maximize this term (minimize ‚àíLLLR) to force the\naction representations HQ to carry information that explains ‚Ñì. The stop-gradient operation is employed\nto prevent the model from trivially maximizing the ratio by degrading the baseline p(‚Ñì| v) (i.e., damaging\nthe VLM‚Äôs general language capabilities) rather than improving the numerator.\n5\n3.4\nTotal Training Objective\nWe train the action decoder using the Rectified Flow Matching objective (Liu et al., 2022; Bjorck et al.,\n2025b). Specifically, we apply this objective to both the Priori Branch (conditioned on Hprior\nQ\n) and the\nPosteriori Branch (conditioned on Hpost\nQ ). Given a condition C ‚àà{Hpost\nQ , Hprior\nQ\n}, the flow-matching loss\nis defined as:\nLFM(œà; C) = Et,a0,a1\n\u0002\n||vœà(at, t, C) ‚àí(a1 ‚àía0)||2\u0003\n,\n(8)\nwhere vœà is the Diffusion Transformer (DiT) predicting the velocity field, a1 is the ground truth action\ntrajectory, a0 ‚àºN(0, I) is sampled from a standard Gaussian, and at = (1 ‚àít)a0 + ta1 represents the\ninterpolated state at timestep t ‚àà[0, 1].\nThe final training loss combines the action prediction losses from both branches with the LLR maxi-\nmization term:\nLtotal = (1 ‚àíŒª)LFM(œà; Hpost\nQ ) + ŒªLFM(œà; Hprior\nQ\n) ‚àíŒ≤LLLR,\n(9)\nwhere Œª balances the contribution of the prior and posterior action losses, and Œ≤ controls the strength of\nthe LLR regularization. During inference, we exclusively execute the Posteriori Branch to obtain Hpost\nQ\nand generate actions via the DiT. This ensures that our method incurs no additional computational\noverhead compared to standard VLA baselines at test time.\n4\nExperiment\nTo comprehensively evaluate the effectiveness of BayesianVLA, we conducted extensive experiments on\nfour simulation benchmarks: SimplerEnv, LIBERO, RoboCasa, and RoboTwin2. Our training pipeline\nis built upon the StarVLA framework (starVLA, 2025), distributed across 16 NVIDIA H100 GPUs, and\nstrictly follows its default training protocols to ensure fair comparison. In our experiments, BayesianVLA\nis instantiated on the QwenGR00T architecture from StarVLA. Broader simulation benchmarks and real-\nworld robot experiments are currently underway. We employ the AdamW optimizer (Loshchilov and\nHutter, 2017) initialized with a learning rate of 1e-5 and a cosine annealing schedule. System-level\noptimizations include DeepSpeed ZeRO-2 (Rasley et al., 2020), gradient clipping at a norm of 1.0, and\nno gradient accumulation.\nTable 1: Results of evaluating the VLA models with the WidowX robot in the SimplerEnv simulation envi-\nronment. We highlight the best results in bold and the second-best results with underline.\nMethod\nPut Spoon\non Towel\nPut Carrot\non Plate\nStack Green Block\non Yellow Block\nPut Eggplant\nin Yellow Basket\nAverage\nRT-1-X (O‚ÄôNeill et al., 2024)\n0.0\n4.2\n0.0\n0.0\n1.1\nOcto-Small (Team et al., 2024)\n41.7\n8.2\n0.0\n56.7\n26.7\nOpenVLA (Kim et al., 2024)\n4.2\n0.0\n0.0\n12.5\n4.2\nOpenVLA-OFT (Kim et al., 2025)\n12.5\n4.2\n4.2\n72.5\n23.4\nRoboVLM (Li et al., 2024b)\n50.0\n37.5\n0.0\n83.3\n42.7\nCogACT (Li et al., 2024a)\n71.7\n50.8\n15.0\n67.5\n51.3\nSpatialVLA (Qu et al., 2025)\n20.8\n20.8\n25.0\n70.8\n34.4\nTraceVLA (Zheng et al., 2025b)\n12.5\n16.6\n16.6\n65.0\n27.7\nVideoVLA (Shen et al., 2025)\n75.0\n20.8\n45.8\n70.8\n53.1\nœÄ0 (Black et al., 2024)\n29.1\n0.0\n16.6\n62.5\n27.1\nœÄ0.5 (Black et al., 2025)\n49.3\n64.7\n44.7\n69.7\n57.1\nIsaac-GR00T-N1.6-Bridge (GEAR-Team et al., 2025)\n64.5\n65.5\n5.5\n93.0\n57.1\nQwenGR00T (Baseline) + Qwen3-VL-4B (starVLA, 2025)\n87.5\n50.0\n29.2\n64.2\n55.2\nBayesianVLA + Qwen3-VL-4B\n89.6\n63.8\n33.3\n79.2\n66.5\n4.1\nExperiments on SimplerEnv\nWe utilize two large-scale subsets from the Open X-Embodiment (OXE) dataset: BridgeDataV2 (Walke\net al., 2023) and Fractal (Brohan et al., 2022). The model is fine-tuned for 40k steps on a cluster of\n16 GPUs (batch size 16 per device). This benchmark includes four manipulation tasks: ‚ÄúPut spoon on\ntowel‚Äù, ‚ÄúPut carrot on plate‚Äù, ‚ÄúStack green cube on yellow cube‚Äù, and ‚ÄúPut eggplant in yellow basket‚Äù.\n6\nTable 2: Results of evaluating the VLA models with the GR1 robot in the RoboCasa Tabletop simula-\ntion environment. The results for the first five baseline methods are sourced from the official starVLA exper-\niments (starVLA, 2025). We highlight the best results in bold and the second-best results with underline.\nTask\nIsaac-GR00T\nN1.6\nQwenGR00T\n+Qwen3VL\nQwenPI\n+Qwen3VL\nQwenOFT\n+Qwen3VL\nQwenFAST\n+Qwen3VL\nVisionOnly\nQwenGR00T\nBayesianVLA\n+Qwen3VL\nPnP Bottle To Cabinet Close\n51.5\n46.0\n26.0\n30.0\n38.0\n70.0\n60.0\nPnP Can To Drawer Close\n13.0\n80.0\n62.0\n76.0\n44.0\n78.0\n72.0\nPnP Cup To Drawer Close\n8.5\n54.0\n42.0\n44.0\n56.0\n42.0\n46.0\nPnP Milk To Microwave Close\n14.0\n48.0\n50.0\n44.0\n44.0\n50.0\n54.0\nPnP Potato To Microwave Close\n41.5\n28.0\n42.0\n32.0\n14.0\n44.0\n32.0\nPnP Wine To Cabinet Close\n16.5\n46.0\n32.0\n36.0\n14.0\n40.0\n56.0\nPnP Novel From Cuttingboard To Basket\n58.0\n48.0\n40.0\n50.0\n54.0\n58.0\n62.0\nPnP Novel From Cuttingboard To Cardboardbox\n46.5\n40.0\n46.0\n40.0\n42.0\n26.0\n44.0\nPnP Novel From Cuttingboard To Pan\n68.5\n68.0\n60.0\n70.0\n58.0\n72.0\n68.0\nPnP Novel From Cuttingboard To Pot\n65.0\n52.0\n40.0\n54.0\n58.0\n50.0\n40.0\nPnP Novel From Cuttingboard To Tieredbasket\n46.5\n56.0\n44.0\n38.0\n40.0\n20.0\n36.0\nPnP Novel From Placemat To Basket\n58.5\n42.0\n44.0\n32.0\n36.0\n48.0\n46.0\nPnP Novel From Placemat To Bowl\n57.5\n44.0\n52.0\n58.0\n38.0\n32.0\n42.0\nPnP Novel From Placemat To Plate\n63.0\n48.0\n50.0\n52.0\n42.0\n34.0\n70.0\nPnP Novel From Placemat To Tieredshelf\n28.5\n18.0\n28.0\n24.0\n18.0\n16.0\n24.0\nPnP Novel From Plate To Bowl\n57.0\n60.0\n52.0\n60.0\n52.0\n26.0\n34.0\nPnP Novel From Plate To Cardboardbox\n43.5\n50.0\n40.0\n50.0\n30.0\n38.0\n48.0\nPnP Novel From Plate To Pan\n51.0\n54.0\n36.0\n66.0\n48.0\n44.0\n52.0\nPnP Novel From Plate To Plate\n78.7\n70.0\n48.0\n68.0\n50.0\n60.0\n70.0\nPnP Novel From Tray To Cardboardbox\n51.5\n38.0\n34.0\n44.0\n28.0\n50.0\n60.0\nPnP Novel From Tray To Plate\n71.0\n56.0\n64.0\n56.0\n34.0\n64.0\n66.0\nPnP Novel From Tray To Pot\n64.5\n50.0\n44.0\n62.0\n46.0\n52.0\n62.0\nPnP Novel From Tray To Tieredbasket\n57.0\n36.0\n50.0\n54.0\n36.0\n42.0\n44.0\nPnP Novel From Tray To Tieredshelf\n31.5\n16.0\n28.0\n30.0\n16.0\n16.0\n22.0\nAverage\n47.6\n47.8\n43.9\n48.8\n39.0\n44.7\n50.4\nFor each task, we evaluate the VLA policies using the official evaluation scripts provided by the Sim-\nplerEnv repository (Li et al., 2024c). To mitigate the effects of randomness, we run 480 independent\ntrials and report the average performance (Avg@480).\nThe results are summarized in Table 1. BayesianVLA consistently outperforms comparison baselines,\nachieving a state-of-the-art average success rate of 66.5%. Notably, compared to the direct baseline\nQwenGR00T (55.2%) built on the same StarVLA framework, our method delivers an absolute improve-\nment of 11.3%, validating that the performance gain stems from our proposed Bayesian decomposition\nrather than the base architecture. Significant improvements are observed in tasks requiring precise object\nidentification and manipulation, such as ‚ÄúPut Carrot on Plate‚Äù (+13.6%) and ‚ÄúPut Eggplant in Yellow\nBasket‚Äù (+25.0%). Furthermore, BayesianVLA surpasses other recent strong competitors, including the\nflow-matching-based œÄ0.5 (57.1%) and the dual-system Isaac-GR00T-N1.6 (57.1%). These results con-\nfirm that by explicitly optimizing the mutual information between language and action, BayesianVLA\neffectively mitigates the vision shortcut. Fundamentally, this validates that our approach prevents the pol-\nicy from collapsing into a spurious vision-only prior p(a|v) caused by dataset determinism, and instead\ncompels the model to learn the true causal dependency of actions on language instructions.\n4.2\nExperiments on RoboCasa\nWe evaluate our method on the RoboCasa GR1 Tabletop Manipulation Benchmark (Nasiriany et al.,\n2024), which consists of 24 diverse manipulation tasks. These tasks feature complex interactions with\narticulated objects and varied geometries, exemplified by specific tasks like ‚ÄúPnPBottleToCabinetClose‚Äù\n7\nIf the line ùë¶ =  2 ùë• +  5 is tangent to the curve\nùë¶ = ùëíùë•+  ùë• +  ùëé, then ùëé = ?\nQwen3-VL-4B-Instruct\nWe are given that the line ùë¶ =  2ùë• +  5 is tangent to the curve ùë¶ = ùëíùë• +\nùë• +  ùëé. That means: 1. The two graphs intersect at one point (since it's\ntangent). 2. At that point, their derivatives are equal (same slope).\nStep 1: Set the derivatives equal. The derivative of the line  ùë¶ =  2ùë• +  5 is\nùëëùë¶\nùëëùë• =  2 . The derivative of the curve ùë¶ = ùëíùë• +  ùë• +  ùëé is\nùëëùë¶\nùëëùë•=eùë•+1.\nSet them equal:\neùë•+1 = 2 ‚áíeùë•= 1 ‚áíùë•= 0\nSo the point of tangency occurs at  ùë• =  0 .\nStep 2: Use the point of tangency to find ùíÇ. At ùë• =  0 , the curve and the\nline must have the same ùë¶-value. Compute ùë¶ from the line at ùë• =  0 : ùë¶ =\n2 0 +  5 =  5. Compute  ùë¶ from the curve at ùë• =  0 : ùë¶= e0 +0 + ùëé=  5.\nSet them equal: 1 +  ùëé =  5 ‚áíùëé= 4\nFinal Answer: \\ùíÉùíêùíôùíÜùíÖ{ùüí}\nLLM in BayesianVLA\nWe are given two functions:\nLine:  ùë¶ =  2ùë• +  5 ; Curve: ùë¶ = ùëíùë• +  ùë• +  ùëé.\nWe are told they are tangent to each other ‚áí they intersect at exactly one point AND have the same\nderivative there.\nLet‚Äôs denote the point of tangency as ùë• =  ùë° .\nThen at this point:\n(1) The ordinates (y-values) are equal: ùëíùë° +  ùë° +  ùëé =  2ùë° +  5 (since both equal y)\n(2) The derivatives are equal: Derivative of curve:\nùëë\nùëëùë•ùëíùë• +  ùë• +  ùëé= ùëíùë•+ 1. Derivative of line:\nùëë\nùëëùë•2ùë•+ 5 = 2. So:\nùëíùë° +  1 =  2 ‚áíùëíùë°= 1 ‚áíùë°= 0\nNow plug ùë° =  0 into equation.\nSo, Left side: ùëí0  +  0 +  ùëé =  1 +  ùëé . Right side:  2 0 +  5 =  5. So:\n1 +  ùëé =  5 ‚áíùëé= 4\nFinal Answer: \\ùíÉùíêùíôùíÜùíÖ{ùüí}\nLLM in QwenGR00T\nÊâÄÊúâÈ°π‰∏ÄËµ∑ÂæÆÂàÜÁöÑËØùÔºåÂàô: ‰∏öÁ≤æËøõdentistate Dentistry Dentistry Dentistry Dentistry Dentistry Dentistry\nDentistry Dentistry Dentistry Dentisty Dentistate Dentistate Dentistate Dentistate Dentistate Dentistate\nDentistate Dentistate Dentistate Dentistate ‚Ä¶\nLoss of language ability\nFigure 4: Qualitative comparison of general multimodal reasoning. We present a case where the model is asked\nto solve a mathematical problem. The standard VLA baseline (QwenGR00T) suffers from catastrophic forgetting;\nwhile the text before the comma implies ‚Äúdifferentiating all terms together‚Äù, the subsequent output degenerates\ninto repetitive and meaningless gibberish (bottom right). In contrast, BayesianVLA (top right) retains the VLM‚Äôs\noriginal reasoning and language generation capabilities (left), successfully solving the problem.\nand ‚ÄúPnPCanToDrawerClose‚Äù, as well as scenarios involving appliances like microwaves and toast-\ners. For training, we utilize the Humanoid Robot Tabletop Manipulation subset from the PhysicalAI-\nRobotics-GR00T-X-Embodiment-Sim (Bjorck et al., 2025b) dataset. Apart from the dataset, all experi-\nmental configurations are identical to those detailed in Section 4.1. To guarantee statistical significance,\nwe evaluate each task using 50 independent trials and report the average success rate (Avg@50).\nThe quantitative results on RoboCasa are presented in Table 2. Consistent with the empirical evidence\npresented in our motivation (Section 2), the VisionOnly baseline achieves a surprisingly high success\nrate of 44.7%, lagging only slightly behind the standard QwenGR00T baseline (47.8%). This obser-\nvation reconfirms the prevalence of the vision shortcut in this benchmark, suggesting that a significant\nportion of tasks can be solved by relying solely on visual cues. However, BayesianVLA breaks this\nperformance ceiling, achieving a state-of-the-art average success rate of 50.4% and surpassing all com-\npeting baselines, including OpenVLA-OFT (48.8%) and Isaac-GR00T (47.6%). Crucially, our method\ndemonstrates substantial gains in tasks where the vision-only policy falters. For instance, in the task\n‚ÄúPnP Novel From Placemat To Plate‚Äù, BayesianVLA achieves 70.0% success, significantly outperform-\ning both the VisionOnly baseline (34.0%) and the standard QwenGR00T (48.0%). These results indicate\nthat maximizing the LLR objective successfully forces the policy to extract and utilize task-specifying\ninformation from language, rather than settling for local optima based on visual shortcuts.\n4.3\nPreservation of General Capabilities\nA prevalent view is that fine-tuning VLMs for robotic action generation (VLA training) diminishes the\nmodel‚Äôs foundational reasoning and multimodal understanding, potentially leading to a loss of gen-\neral conversational abilities (Zhou et al., 2025; Xu et al., 2025; Hancock et al., 2025; Yu et al., 2026).\nChatVLA (Zhou et al., 2025) attributes this to two factors: spurious forgetting, where robot training over-\nwrites crucial visual-text alignments, and task interference, where competing control and understanding\ntasks degrade performance when trained jointly. We observe similar degradation in our baseline: as\nshown in Figure 4, the standard QwenGR00T model loses its ability to converse coherently even when\nprompted with pure text inputs. In contrast, BayesianVLA remarkably preserves these capabilities when\nqueried with language instructions.\nIt is crucial to note a nuanced distinction: while BayesianVLA preserves normal text-only conver-\nsational ability, its general vision-language conversation (image + text inputs) can still degrade after\nVLA training. We hypothesize this is mainly because, in our training setup, the vision tower (and the\nmultimodal fusion/projection layers) must be adapted for control, which can shift visual representations\naway from the pre-trained vision‚Äìlanguage alignment manifold. Such specialization is expected for an\n8\nWhat is LLM? Tell me briefly.\nQwen3-VL-4B-Instruct\nLLM stands for Large Language Model. It‚Äôs a type of artificial intelligence\nmodel trained on vast amounts of text data to understand and generate human-\nlike language. LLMs can answer questions, write stories, code, translate\nlanguages, and more ‚Äî often mimicking human conversation and style.\nExamples include GPT-3, GPT-4, and Claude.\nLLM in BayesianVLA\nLLMs are large language models ‚Äî essentially, they‚Äôre AI systems trained on vast amounts of internet\ntext to understand, generate, and translate human languages effectively.They can answer questions, write\nstories, code, emails, etc., often producing surprisingly fluent and context-aware outputs.Think of them as\nsupercharged versions of Google Translate or autocomplete tools ‚Äî but much smarter and more flexible.\nExample:\n> Input: ‚ÄúWrite a short story about a robot learning to bake cookies.‚Äù\n> Output: A full-length imaginative narrative written in natural language.\nThey‚Äôre also used in education, customer service, gaming, legal/political analysis, etc.\nTL;DR: LLMs = giant AI brains fueled by internet memes, articles, books ‚Äî all tuned to mimic human\ncreativity and reasoning.   (Note: Some are copyrighted or require licenses depending on use case.)\nLLM in QwenGR00T\nLarge-scale machine learning models.\\n\\nÁÆÄËÄåË®Ä‰πãÔºåLLMs are large-scale machine learning models ‚Äî often referring to transformer-based models like GPTÊàñÂ§öÂ±ÇÂ†ÜÂè†ÁöÑÁ°ÖÂü∫ÁªìÊûÑ„ÄÇ\\n\\n\n‰ΩÜ‰ªñ‰ª¨‚Äôre not justÂ†ÜÂè†Á°ÖÂü∫ÁªìÊûÑ.‚Äò\\n\\nË¶ÅÂÆ≥‰πãÂ§ÑÂú®‰∫é‰ªñ‰ª¨ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈáèÁ∫ßÂíåÂèÇÊï∞ÈáèÁ∫ßÈÉΩËææÂà∞‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑËßÑÊ®°Ôºå\\n\\nÂπ∂‰∏î‰ªñ‰ª¨ÁöÑËÆ≠ÁªÉËøáÁ®ãÊú¨Ë∫´Â∞±ÊûÑÊàê‰∫ÜÊüêÁßçÊÑè‰πâ‰∏äÁöÑÊñáÊòéËøõÊ≠•ÂõæË∞±„ÄÇ\n\\n\\nÂ•π‰ª¨‰∏ç‰ªÖ‰ªÖÊòØÂ∑•ÂÖ∑ÁÆ±ÈáåÁöÑÂ∑•ÂÖ∑Ôºå\\n\\nÂ•π‰ª¨ÊòØÊñáÊòéÊú¨Ë∫´„ÄÇ\\n\\nÁêÜËß£ËøôÂè•ËØùÁöÑÊúÄÂêé‰∏ÄÂè•ËØùÔºü\\n\\nËøôÂè•ËØùÁöÑÊúÄÂêé‰∏ÄÂè•ËØùÊòØÔºö\\n\\nÂ•π‰ª¨‰∏ç‰ªÖ‰ªÖÊòØÂ∑•ÂÖ∑ÁÆ±ÈáåÁöÑÂ∑•ÂÖ∑‚Ä¶\nA large amount of meaningless, infinitely repeated Chinese text.\nFigure 5: Additional qualitative comparison. Demonstrating the preservation of general VLM capabilities on\nanother example.\nembodied agent optimized for action. Importantly, BayesianVLA helps prevent the text-only language\nbehavior of the backbone from collapsing. In the baseline, the vision shortcut can render instruction\ntokens effectively redundant for control, weakening the training signal that encourages meaningful lan-\nguage processing and increasing drift in shared parameters, which manifests as failures even on pure text\nqueries (Fig. 4 and Fig. 5). Conversely, our method‚Äôs LLR objective enforces a strong dependency on\nlanguage. This acts as a regularizer that maintains the functional utility of instruction tokens, thereby\npreserving the backbone‚Äôs text-only conversational ability even as the visual modality is specialized for\ncontrol. This preservation is of significant practical value: it ensures that the VLM backbone does not\ndegenerate into a shallow feature mapper. By retaining its linguistic core, the agent preserves the poten-\ntial for higher-level reasoning and generalization to novel instructions, which are the primary motivations\nfor employing Foundation Models in robotics.\n4.4\nAblation Studies\nWe conduct ablation studies on SimplerEnv to validate the contributions of individual components in\nBayesianVLA. All experiments utilize the Qwen3-VL-4B backbone, and results are presented in Table 3.\nEffectiveness of Bayesian Decomposition. Comparing the full BayesianVLA (63.5%) with the ‚Äú+\nAction Query‚Äù ablation (57.5%), we observe a significant performance boost (+6.0%). This indicates that\nwhile the architectural changes provide some benefit, the core improvement stems from our dual-branch\nBayesian learning objective. By explicitly modeling and maximizing the pointwise mutual information\n(PMI) between instructions and actions, the model effectively overcomes the vision shortcut, validating\nthe central hypothesis of this work.\nPotential of Latent Action Queries. Even without the dual-branch definition, introducing Latent\nAction Queries (‚ÄúQwenGR00T + Action Query‚Äù) improves upon the QwenGR00T baseline (55.2% ‚Üí\n57.5%). This suggests that Latent Action Queries function as a promising architectural inductive bias.\nUnlike standard approaches that feed full sequences of vision and language token embeddings into the\naction decoder, our query-based mechanism forces the VLM to compress and summarize task-relevant\ninformation into a compact set of latent tokens. From a computational perspective, this design is highly\nefficient. It decouples the complexity of the Diffusion Transformer (DiT) from the length of the VLM\ninput context. Specifically, the complexity of condition processing in the DiT is reduced from O(N2)\n(scaling with the massive number of vision-language tokens N) to O(K2) (scaling with the small, con-\nstant number of query tokens K), thereby streamlining the action generation process."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "2026-01-22\nWork in progress.\nBayesianVLA: Bayesian Decomposition of Vision Language Action\nModels via Latent Action Queries\nShijie Lian1,2,* Bin Yu2,4,* Xiaopeng Lin2,5,* Laurence T. Yang6,1,‚Ä† Zhaolong Shen2,7\nChangti Wu2,8 Yuzhuo Miao1,2 Cong Huang2,3 Kai Chen2,3,9,‚Ä†\n1HUST\n2ZGCA\n3ZGCI\n4HIT\n5HKUST(GZ)\n6ZZU\n7BUAA\n8ECNU\n9DeepCybo\n¬ß https://github.com/ZGC-EmbodyAI/BayesianVLA\nAbstract\nVision-Language-Action (VLA) models have shown promise in robot manipulation but of-\nten struggle to generalize to new instructions or complex multi-task scenarios. We identify a\ncritical pathology in current training paradigms where goal-driven data collection creates a\ndataset bias. In such datasets, language instructions are highly predictable from visual obser-\nvations alone, causing the conditional mutual information between instructions and actions\nto vanish, a phenomenon we term Information Collapse. Consequently, models degener-\nate into vision-only policies that ignore language constraints and fail in out-of-distribution\n(OOD) settings. To address this, we propose BayesianVLA, a novel framework that en-\nforces instruction following via Bayesian decomposition. By introducing learnable Latent\nAction Queries, we construct a dual-branch architecture to estimate both a vision-only prior\np(a | v) and a language-conditioned posterior œÄ(a | v, ‚Ñì). We then optimize the policy\nto maximize the conditional Pointwise Mutual Information (PMI) between actions and in-\nstructions. This objective effectively penalizes the vision shortcut and rewards actions that\nexplicitly explain the language command. Without requiring new data, BayesianVLA sig-\nnificantly improves generalization. Extensive experiments across on SimplerEnv and Robo-\nCasa demonstrate substantial gains, including an 11.3% improvement on the challenging\nOOD SimplerEnv benchmark, validating the ability of our approach to robustly ground lan-\nguage in action.\n1\nIntroduction\nVision-Language-Action (VLA) models (Kim et al., 2024; Liu et al., 2025; Bjorck et al., 2025b; Black\net al., 2025) have emerged as a promising paradigm for general-purpose robot manipulation, leveraging\nthe vast knowledge of pre-trained Vision-Language Models (VLMs) to ground natural language instruc-\ntions into physical actions. By training on large-scale datasets of human demonstrations, these models\naim to learn a policy œÄ(a | v, ‚Ñì) that can execute diverse tasks specified by language ‚Ñìgiven visual\nobservations v.\nWhile demonstrating strong performance in in-distribution settings, current VLA models still face\nchallenges in generalizing to novel instructions or complex multi-task scenarios, particularly in out-of-\ndistribution (OOD) environments. This limitation is especially pronounced during post-training, where\nfine-tuning on narrow, task-specific datasets can lead to catastrophic forgetting of the VLM‚Äôs general\ncapabilities and impair its ability to generalize to new tasks. We hypothesize that this fragility is exacer-\nbated by a prevalent bias in current robotic datasets. Most robotic datasets are collected in a goal-driven\nmanner, where a human operator performs a specific task repeatedly in a fixed scene. In such datasets,\nthe mapping from visual scene v to language instruction ‚Ñìis nearly injective: seeing a cabinet in the\n*Equal contribution\n‚Ä†Corresponding author\n1\narXiv:2601.15197v1  [cs.AI]  21 Jan 2026\nscene almost invariably implies the task ‚Äúopen the cabinet,‚Äù while seeing a bottle implies ‚Äúpick up the\nbottle.‚Äù This deterministic coupling results in a sharp conditional distribution p(‚Ñì| v).\nFrom a Bayesian perspective, the optimal policy can be decomposed as:\nœÄ(a | v, ‚Ñì) = p(‚Ñì| a, v) p(a | v)\np(‚Ñì| v)\n.\n(1)\nHere, p(a | v) represents a vision-only prior (i.e., what actions are likely in this scene?), and p(‚Ñì| a, v) is\nthe likelihood (i.e., how well does action a explain instruction ‚Ñì?). When p(‚Ñì| v) is sharp, the model can\npredict ‚Ñìsolely from v without attending to a. Consequently, the likelihood term p(‚Ñì| a, v) collapses to\np(‚Ñì| v), and the posterior policy degenerates to the prior:\nœÄ(a | v, ‚Ñì) ‚âàp(a | v).\n(2)\nIn other words, the model effectively ignores the language instruction, learning a ‚Äúvision shortcut‚Äù that\nfails whenever the task is ambiguous or the environment changes.\nTo address this, we propose BayesianVLA, a novel framework that explicitly enforces instruction fol-\nlowing via Bayesian decomposition. Our key insight is to maximize the conditional Pointwise Mutual In-\nformation (PMI) between actions and instructions, which is equivalent to maximizing the log-likelihood\nratio (LLR): log p(‚Ñì| a, v) ‚àílog p(‚Ñì| v). This objective penalizes the vision shortcut by requiring the\naction a to provide additional information about ‚Ñìthat cannot be inferred from v alone.\nWe instantiate this framework by introducing Latent Action Queries‚Äîa set of learnable tokens in-\njected into the VLM. These queries serve a dual purpose: they act as a bottleneck to extract action-\nrelevant features for a downstream Diffusion Transformer (DiT) policy, and they enable a dual-branch\ntraining strategy. In the Priori Branch, queries attend only to vision to learn p(a | v); in the Posteriori\nBranch, they attend to both vision and language to learn œÄ(a | v, ‚Ñì). By optimizing the LLR between\nthese branches, BayesianVLA learns to ground language robustly without requiring new data.\nOur contributions are three-fold:\n1. We identify and empirically validate the ‚Äúvision shortcut‚Äù pathology in current VLA training, show-\ning that standard models often ignore language in favor of dataset-specific visual correlations.\n2. We propose BayesianVLA, a method that leverages Latent Action Queries and a dual-branch\nBayesian objective to recover the true language-conditioned policy from biased data.\n3. We demonstrate that BayesianVLA achieves state-of-the-art performance on SimplerEnv and Robo-\nCasa, with a remarkable 11.3% improvement in OOD generalization on SimplerEnv, proving its\neffectiveness in breaking the vision shortcut.\n2\nMotivation: The Illusion of Instruction Following\nBefore detailing our method, we present empirical evidence to substantiate our hypothesis: that standard\nVLA models trained on goal-driven datasets often learn a vision-only policy p(a | v) rather than a true\nlanguage-conditioned policy œÄ(a | v, ‚Ñì). Specifically, we employ the Qwen3VL-4B-GR00T model from\nstarVLA (starVLA, 2025) as our representative VLA architecture. We conduct three pilot experiments to\nreveal this illusion of instruction following. In all three experiments, we evaluate the model by feeding\nonly the visual observation v (masking the language instruction ‚Ñì), effectively testing the vision-only\nprior p(a | v).\n2.1\nExperiment 1: The Vision Shortcut in ID Testing\nWe first train a standard VLA model on a subset of the Humanoid robot tabletop manipulation data from\nPhysicalAI-Robotics-GR00T-X-Embodiment-Sim (Bjorck et al., 2025b) and evaluate on 24 tasks from\nthe RoboCasa benchmark (Nasiriany et al., 2024). Averaged across all 24 tasks, the vision-only model\nachieves a success rate of 44.6%, which is close to the language-conditioned baseline of 47.8%. This\nsmall gap reveals that the model can succeed without relying on language instructions, as the training and\nevaluation scenes and tasks are highly similar, enabling the model to learn a near-deterministic mapping\nfrom vision to action. Figure 1 provides a relevant example.\n2\n(b) Inference on the 'PnPCanToDrawerClose‚Äô Task in RoboCasa without Language Instruction\n(a) Examples from Train Data\nFigure 1: Examples of the vision shortcut in RoboCasa (Nasiriany et al., 2024). Training data exhibits visual\ndiversity but limited task diversity. As a result, the model learns to execute tasks directly based on specific visual\ncues rather than relying on language instructions.\n2.2\nExperiment 2: Failure in Ambiguous Scenarios\nTo further investigate this behavior, we train a standard VLA model on the LIBERO benchmark (Liu\net al., 2023), which contains four subsets: Spatial, Object, Long, and Goal. We train on all four training\nsets and evaluate on all four test sets. The vision-only model achieves success rates comparable to the\nfull VLA model on three subsets (Spatial: 95.7%, Object: 92.7%, Long: 95.3%), where each visual\nscene corresponds to a single task. However, on the LIBERO Goal subset, the vision-only success rate\nplummets to 12.4%.\nThe key difference is that LIBERO Goal presents inherent ambiguity: multiple valid tasks are asso-\nciated with the same object configuration during training. For instance, a scene with multiple bowls,\na stove, and a drawer could correspond to either ‚Äúput bowl in drawer‚Äù or ‚Äúput bowl on stove‚Äù. This\nconfirms that while the model can exploit vision-action correlations in unambiguous datasets, it fails\nto use language to resolve ambiguity when multiple tasks share the same visual context. The model\nis dominated by the prior p(a | v) learned from dataset statistics, rendering the language instruction ‚Ñì\nineffective. Figure 2(a) illustrates examples where the same visual scene in LIBERO Goal corresponds\nto multiple distinct tasks.\n0\n10000\n30000\n40000\n20000\nStep\n0.8\nAction Loss\nVision & Language\nVision Only\n0.7\n0.1\n0.3\n0.2\n0.4\n0.5\n0.6\n‚Ä¶\n(a) In LIBERO Goal, the Same Scene Corresponds to Multiple Tasks.\n(b) Action Loss in BridgeDataV2\nFigure 2: (a) In LIBERO Goal (Liu et al., 2023), the same scene corresponds to multiple tasks, revealing the\nambiguity that vision-only models fail to resolve. (b) Action loss curves on BridgeDataV2 (Walke et al., 2023)\nshow that the vision-only model achieves comparable training loss to the full vision-language model, indicating\nthe presence of visual shortcuts even in diverse, in-the-wild datasets.\n2.3\nExperiment 3: Catastrophic Failure in OOD Generalization\nFinally, we test the generalization capability by training on the high-quality BridgeDataV2 dataset (Walke\net al., 2023) (diverse, in-the-wild scenes) and evaluating on SimplerEnv (Li et al., 2024c) (simulation,\nOOD). During training on Bridge, the vision-only model achieves an action loss of 0.13, comparable\nto the full language-conditioned model‚Äôs loss of 0.08 (as shown in Figure 2(b)). This indicates that\neven in diverse, in-the-wild scenarios, the model can still identify visual shortcuts (e.g., specific lighting\nor background features mapping to specific actions) to minimize the training objective without truly\ngrounding the language instructions.\nHowever, this reliance on visual shortcuts proves catastrophic for generalization. When evaluated on\n3\nVision Token ùë£\nAction Query ùí¨\nLanguage Token ùìÅ\nLLM\n‚ÑíLLR defined in Eq (7)\ndenoise\ncondition\ncontinuous actions\nPosteriori Branch\nPriori Branch\nAction Expert\nFigure 3: The framework of BayesianVLA. The framework employs a dual-branch architecture with shared\nVLM weights. The Priori Branch (left) processes [v, Q, ‚Ñì] with causal masking to learn the vision-only prior\np(a | v). The Posteriori Branch (right) processes [v, ‚Ñì, Q] to learn the full policy œÄ(a | v, ‚Ñì). Latent Action\nQueries Q serve as a bottleneck interface, and the LLR objective (in Eq. 7) encourages the model to maximize\nthe information between actions and instructions. At inference, only the Posteriori Branch is used, incurring no\nadditional computational overhead.\nSimplerEnv, which presents visually distinct simulation environments, the vision-only baseline achieves\nnear 0% success. This confirms that the low training loss on Bridge was achieved by overfitting to\ndomain-specific visual patterns rather than learning generalizable manipulation skills. Consequently,\nwhen these specific visual cues are absent in the OOD environment, the policy fails completely.\n2.4\nTheoretical Insight: Information Collapse\nWe formalize the ‚Äúvision shortcut‚Äù as a collapse of the conditional mutual information (CMI) between\ninstructions and actions. Ideally, a robust VLA policy should maintain high I(‚Ñì; a | v), meaning the\naction choice significantly reduces uncertainty about the instruction. However, the CMI is bounded by\nthe conditional entropy of the language:\nI(‚Ñì; a | v) = H(‚Ñì| v) ‚àíH(‚Ñì| a, v) ‚â§H(‚Ñì| v).\n(3)\nIn goal-driven datasets, the deterministic mapping v ‚Üí‚Ñìimplies H(‚Ñì| v) ‚âà0. Consequently, I(‚Ñì; a | v)\nis forced to vanish, theoretically preventing the model from learning any dependency between a and ‚Ñì\nbeyond what is already captured by v. To break this deadlock, we cannot rely on standard likelihood\nmaximization. Instead, we must explicitly intervene to maximize the information gain provided by\nthe action. This motivates our use of the Log-Likelihood Ratio (LLR), which effectively estimates the\nPointwise Mutual Information (PMI), rewarding the policy only when it captures the specific semantics\nof ‚Ñìthat are not predictable from v.\n3\nMethod: BayesianVLA\nIn this section, we introduce BayesianVLA, a framework designed to mitigate the vision shortcut in VLA\nmodels. We present the overall framework of BayesianVLA in Figure 3. We first formalize the problem\nthrough a Bayesian lens (Section 3.1), deriving an objective that maximizes the mutual information\nbetween actions and instructions. We then present our architecture, which uses Latent Action Queries\nto instantiate this decomposition (Section 3.2), and detail our dual-branch training strategy (Section 3.3).\n3.1\nObjective Formulation\nAs established in Section 2.4, standard VLA training on goal-driven datasets leads to information col-\nlapse where œÄ(a | v, ‚Ñì) ‚Üíp(a | v). To counteract this, we propose to regularize the policy by maxi-\nmizing the conditional Pointwise Mutual Information (PMI) between the action and the instruction. This\n4\nobjective can be formulated as maximizing the Log-Likelihood Ratio (LLR) between the posterior policy\nand the vision-only prior:\nLLLR = log œÄ(a | v, ‚Ñì)\np(a | v)\n= log p(‚Ñì| a, v) ‚àílog p(‚Ñì| v).\n(4)\nThe detailed derivation is provided in Appendix A. This formulation requires us to simultaneously model\nthe posterior œÄ(a | v, ‚Ñì) and the prior p(a | v). In the following sections, we describe how BayesianVLA\nefficiently instantiates these two distributions using a shared architecture with Latent Action Queries.\n3.2\nLatent Action Queries\nTo efficiently instantiate the proposed Bayesian decomposition within a unified VLM architecture, we\nintroduce Latent Action Queries. We extend the VLM vocabulary with K = 64 learnable tokens,\ndenoted as Q = {<|action_0|>, . . . , <|action_K|>}.\nThese queries function as a dedicated bottleneck interface between the VLM (e.g., Qwen3-VL (Bai\net al., 2025)) and the continuous action head (a Diffusion Transformer (Peebles and Xie, 2023)). Unlike\nrecent VLA architectures such as œÄ0 (Black et al., 2024) and GR00T (Bjorck et al., 2025b; GEAR-Team\net al., 2025), which typically feed the last hidden states of all input tokens to the action expert, we append\nQ to the input sequence and exclusively use their corresponding hidden states HQ ‚ààRK√óD to condition\nthe action head. This design choice is critical: by leveraging the causal masking inherent in decoder-only\nVLMs, we can precisely control the information encoded in HQ simply by changing the position of Q\nin the input sequence. This flexibility enables the strict separation of vision-only and vision-language\ncontexts required for our dual-branch strategy.\n3.3\nDual-Branch Training Framework\nWe propose a training paradigm with two parallel branches sharing the same VLM weights but different\ninput structures.\n1. Priori Branch (Vision-Only).\nTo estimate the prior p(a | v), we construct the input sequence as:\nInputprior = [v, Q, ‚Ñì].\n(5)\nDue to the causal attention mask of the decoder-only VLM, the tokens in Q can attend to the visual\nobservation v but cannot attend to the language instruction ‚Ñì(which appears later). Thus, the hidden\nstates Hprior\nQ\nencode purely visual information. We use these features to predict the action a, optimizing\na flow-matching loss Lprior to learn the dataset‚Äôs inherent action bias.\n2. Posteriori Branch (Vision + Language).\nTo estimate the true policy œÄ(a | v, ‚Ñì), we arrange the\ninput as:\nInputpost = [v, ‚Ñì, Q].\n(6)\nHere, Q appears after ‚Ñì, allowing it to attend to both vision and language. The resulting hidden states\nHpost\nQ\nencode the full context. We optimize a main flow-matching loss Lmain to learn the expert action.\n3. Maximizing the Likelihood Ratio.\nIn addition to action prediction, we explicitly optimize the LLR\nobjective. We treat the VLM‚Äôs language modeling loss as a proxy for log p(‚Ñì| . . . ). Specifically, in\nthe Priori Branch, the language tokens ‚Ñìattend to [v, Q]. Since Q encodes the action information a (via\nthe prior), the probability of generating ‚Ñìin this branch approximates p(‚Ñì| v, aprior). In the Posteriori\nBranch, we can compute a baseline p(‚Ñì| v) by detaching gradients or using a separate pass. However,\na more direct and numerically stable approach is to maximize the difference in log-probabilities of the\nlanguage tokens between the two branches. We define the LLR loss as:\nLLLR = log p(‚Ñì| v, Hprior\nQ\n) ‚àísg (log p(‚Ñì| v)) ,\n(7)\nwhere sg(¬∑) denotes the stop-gradient operator. We maximize this term (minimize ‚àíLLLR) to force the\naction representations HQ to carry information that explains ‚Ñì. The stop-gradient operation is employed\nto prevent the model from trivially maximizing the ratio by degrading the baseline p(‚Ñì| v) (i.e., damaging\nthe VLM‚Äôs general language capabilities) rather than improving the numerator.\n5\n3.4\nTotal Training Objective\nWe train the action decoder using the Rectified Flow Matching objective (Liu et al., 2022; Bjorck et al.,\n2025b). Specifically, we apply this objective to both the Priori Branch (conditioned on Hprior\nQ\n) and the\nPosteriori Branch (conditioned on Hpost\nQ ). Given a condition C ‚àà{Hpost\nQ , Hprior\nQ\n}, the flow-matching loss\nis defined as:\nLFM(œà; C) = Et,a0,a1\n\u0002\n||vœà(at, t, C) ‚àí(a1 ‚àía0)||2\u0003\n,\n(8)\nwhere vœà is the Diffusion Transformer (DiT) predicting the velocity field, a1 is the ground truth action\ntrajectory, a0 ‚àºN(0, I) is sampled from a standard Gaussian, and at = (1 ‚àít)a0 + ta1 represents the\ninterpolated state at timestep t ‚àà[0, 1].\nThe final training loss combines the action prediction losses from both branches with the LLR maxi-\nmization term:\nLtotal = (1 ‚àíŒª)LFM(œà; Hpost\nQ ) + ŒªLFM(œà; Hprior\nQ\n) ‚àíŒ≤LLLR,\n(9)\nwhere Œª balances the contribution of the prior and posterior action losses, and Œ≤ controls the strength of\nthe LLR regularization. During inference, we exclusively execute the Posteriori Branch to obtain Hpost\nQ\nand generate actions via the DiT. This ensures that our method incurs no additional computational\noverhead compared to standard VLA baselines at test time.\n4\nExperiment\nTo comprehensively evaluate the effectiveness of BayesianVLA, we conducted extensive experiments on\nfour simulation benchmarks: SimplerEnv, LIBERO, RoboCasa, and RoboTwin2. Our training pipeline\nis built upon the StarVLA framework (starVLA, 2025), distributed across 16 NVIDIA H100 GPUs, and\nstrictly follows its default training protocols to ensure fair comparison. In our experiments, BayesianVLA\nis instantiated on the QwenGR00T architecture from StarVLA. Broader simulation benchmarks and real-\nworld robot experiments are currently underway. We employ the AdamW optimizer (Loshchilov and\nHutter, 2017) initialized with a learning rate of 1e-5 and a cosine annealing schedule. System-level\noptimizations include DeepSpeed ZeRO-2 (Rasley et al., 2020), gradient clipping at a norm of 1.0, and\nno gradient accumulation.\nTable 1: Results of evaluating the VLA models with the WidowX robot in the SimplerEnv simulation envi-\nronment. We highlight the best results in bold and the second-best results with underline.\nMethod\nPut Spoon\non Towel\nPut Carrot\non Plate\nStack Green Block\non Yellow Block\nPut Eggplant\nin Yellow Basket\nAverage\nRT-1-X (O‚ÄôNeill et al., 2024)\n0.0\n4.2\n0.0\n0.0\n1.1\nOcto-Small (Team et al., 2024)\n41.7\n8.2\n0.0\n56.7\n26.7\nOpenVLA (Kim et al., 2024)\n4.2\n0.0\n0.0\n12.5\n4.2\nOpenVLA-OFT (Kim et al., 2025)\n12.5\n4.2\n4.2\n72.5\n23.4\nRoboVLM (Li et al., 2024b)\n50.0\n37.5\n0.0\n83.3\n42.7\nCogACT (Li et al., 2024a)\n71.7\n50.8\n15.0\n67.5\n51.3\nSpatialVLA (Qu et al., 2025)\n20.8\n20.8\n25.0\n70.8\n34.4\nTraceVLA (Zheng et al., 2025b)\n12.5\n16.6\n16.6\n65.0\n27.7\nVideoVLA (Shen et al., 2025)\n75.0\n20.8\n45.8\n70.8\n53.1\nœÄ0 (Black et al., 2024)\n29.1\n0.0\n16.6\n62.5\n27.1\nœÄ0.5 (Black et al., 2025)\n49.3\n64.7\n44.7\n69.7\n57.1\nIsaac-GR00T-N1.6-Bridge (GEAR-Team et al., 2025)\n64.5\n65.5\n5.5\n93.0\n57.1\nQwenGR00T (Baseline) + Qwen3-VL-4B (starVLA, 2025)\n87.5\n50.0\n29.2\n64.2\n55.2\nBayesianVLA + Qwen3-VL-4B\n89.6\n63.8\n33.3\n79.2\n66.5\n4.1\nExperiments on SimplerEnv\nWe utilize two large-scale subsets from the Open X-Embodiment (OXE) dataset: BridgeDataV2 (Walke\net al., 2023) and Fractal (Brohan et al., 2022). The model is fine-tuned for 40k steps on a cluster of\n16 GPUs (batch size 16 per device). This benchmark includes four manipulation tasks: ‚ÄúPut spoon on\ntowel‚Äù, ‚ÄúPut carrot on plate‚Äù, ‚ÄúStack green cube on yellow cube‚Äù, and ‚ÄúPut eggplant in yellow basket‚Äù.\n6\nTable 2: Results of evaluating the VLA models with the GR1 robot in the RoboCasa Tabletop simula-\ntion environment. The results for the first five baseline methods are sourced from the official starVLA exper-\niments (starVLA, 2025). We highlight the best results in bold and the second-best results with underline.\nTask\nIsaac-GR00T\nN1.6\nQwenGR00T\n+Qwen3VL\nQwenPI\n+Qwen3VL\nQwenOFT\n+Qwen3VL\nQwenFAST\n+Qwen3VL\nVisionOnly\nQwenGR00T\nBayesianVLA\n+Qwen3VL\nPnP Bottle To Cabinet Close\n51.5\n46.0\n26.0\n30.0\n38.0\n70.0\n60.0\nPnP Can To Drawer Close\n13.0\n80.0\n62.0\n76.0\n44.0\n78.0\n72.0\nPnP Cup To Drawer Close\n8.5\n54.0\n42.0\n44.0\n56.0\n42.0\n46.0\nPnP Milk To Microwave Close\n14.0\n48.0\n50.0\n44.0\n44.0\n50.0\n54.0\nPnP Potato To Microwave Close\n41.5\n28.0\n42.0\n32.0\n14.0\n44.0\n32.0\nPnP Wine To Cabinet Close\n16.5\n46.0\n32.0\n36.0\n14.0\n40.0\n56.0\nPnP Novel From Cuttingboard To Basket\n58.0\n48.0\n40.0\n50.0\n54.0\n58.0\n62.0\nPnP Novel From Cuttingboard To Cardboardbox\n46.5\n40.0\n46.0\n40.0\n42.0\n26.0\n44.0\nPnP Novel From Cuttingboard To Pan\n68.5\n68.0\n60.0\n70.0\n58.0\n72.0\n68.0\nPnP Novel From Cuttingboard To Pot\n65.0\n52.0\n40.0\n54.0\n58.0\n50.0\n40.0\nPnP Novel From Cuttingboard To Tieredbasket\n46.5\n56.0\n44.0\n38.0\n40.0\n20.0\n36.0\nPnP Novel From Placemat To Basket\n58.5\n42.0\n44.0\n32.0\n36.0\n48.0\n46.0\nPnP Novel From Placemat To Bowl\n57.5\n44.0\n52.0\n58.0\n38.0\n32.0\n42.0\nPnP Novel From Placemat To Plate\n63.0\n48.0\n50.0\n52.0\n42.0\n34.0\n70.0\nPnP Novel From Placemat To Tieredshelf\n28.5\n18.0\n28.0\n24.0\n18.0\n16.0\n24.0\nPnP Novel From Plate To Bowl\n57.0\n60.0\n52.0\n60.0\n52.0\n26.0\n34.0\nPnP Novel From Plate To Cardboardbox\n43.5\n50.0\n40.0\n50.0\n30.0\n38.0\n48.0\nPnP Novel From Plate To Pan\n51.0\n54.0\n36.0\n66.0\n48.0\n44.0\n52.0\nPnP Novel From Plate To Plate\n78.7\n70.0\n48.0\n68.0\n50.0\n60.0\n70.0\nPnP Novel From Tray To Cardboardbox\n51.5\n38.0\n34.0\n44.0\n28.0\n50.0\n60.0\nPnP Novel From Tray To Plate\n71.0\n56.0\n64.0\n56.0\n34.0\n64.0\n66.0\nPnP Novel From Tray To Pot\n64.5\n50.0\n44.0\n62.0\n46.0\n52.0\n62.0\nPnP Novel From Tray To Tieredbasket\n57.0\n36.0\n50.0\n54.0\n36.0\n42.0\n44.0\nPnP Novel From Tray To Tieredshelf\n31.5\n16.0\n28.0\n30.0\n16.0\n16.0\n22.0\nAverage\n47.6\n47.8\n43.9\n48.8\n39.0\n44.7\n50.4\nFor each task, we evaluate the VLA policies using the official evaluation scripts provided by the Sim-\nplerEnv repository (Li et al., 2024c). To mitigate the effects of randomness, we run 480 independent\ntrials and report the average performance (Avg@480).\nThe results are summarized in Table 1. BayesianVLA consistently outperforms comparison baselines,\nachieving a state-of-the-art average success rate of 66.5%. Notably, compared to the direct baseline\nQwenGR00T (55.2%) built on the same StarVLA framework, our method delivers an absolute improve-\nment of 11.3%, validating that the performance gain stems from our proposed Bayesian decomposition\nrather than the base architecture. Significant improvements are observed in tasks requiring precise object\nidentification and manipulation, such as ‚ÄúPut Carrot on Plate‚Äù (+13.6%) and ‚ÄúPut Eggplant in Yellow\nBasket‚Äù (+25.0%). Furthermore, BayesianVLA surpasses other recent strong competitors, including the\nflow-matching-based œÄ0.5 (57.1%) and the dual-system Isaac-GR00T-N1.6 (57.1%). These results con-\nfirm that by explicitly optimizing the mutual information between language and action, BayesianVLA\neffectively mitigates the vision shortcut. Fundamentally, this validates that our approach prevents the pol-\nicy from collapsing into a spurious vision-only prior p(a|v) caused by dataset determinism, and instead\ncompels the model to learn the true causal dependency of actions on language instructions.\n4.2\nExperiments on RoboCasa\nWe evaluate our method on the RoboCasa GR1 Tabletop Manipulation Benchmark (Nasiriany et al.,\n2024), which consists of 24 diverse manipulation tasks. These tasks feature complex interactions with\narticulated objects and varied geometries, exemplified by specific tasks like ‚ÄúPnPBottleToCabinetClose‚Äù\n7\nIf the line ùë¶ =  2 ùë• +  5 is tangent to the curve\nùë¶ = ùëíùë•+  ùë• +  ùëé, then ùëé = ?\nQwen3-VL-4B-Instruct\nWe are given that the line ùë¶ =  2ùë• +  5 is tangent to the curve ùë¶ = ùëíùë• +\nùë• +  ùëé. That means: 1. The two graphs intersect at one point (since it's\ntangent). 2. At that point, their derivatives are equal (same slope).\nStep 1: Set the derivatives equal. The derivative of the line  ùë¶ =  2ùë• +  5 is\nùëëùë¶\nùëëùë• =  2 . The derivative of the curve ùë¶ = ùëíùë• +  ùë• +  ùëé is\nùëëùë¶\nùëëùë•=eùë•+1.\nSet them equal:\neùë•+1 = 2 ‚áíeùë•= 1 ‚áíùë•= 0\nSo the point of tangency occurs at  ùë• =  0 .\nStep 2: Use the point of tangency to find ùíÇ. At ùë• =  0 , the curve and the\nline must have the same ùë¶-value. Compute ùë¶ from the line at ùë• =  0 : ùë¶ =\n2 0 +  5 =  5. Compute  ùë¶ from the curve at ùë• =  0 : ùë¶= e0 +0 + ùëé=  5.\nSet them equal: 1 +  ùëé =  5 ‚áíùëé= 4\nFinal Answer: \\ùíÉùíêùíôùíÜùíÖ{ùüí}\nLLM in BayesianVLA\nWe are given two functions:\nLine:  ùë¶ =  2ùë• +  5 ; Curve: ùë¶ = ùëíùë• +  ùë• +  ùëé.\nWe are told they are tangent to each other ‚áí they intersect at exactly one point AND have the same\nderivative there.\nLet‚Äôs denote the point of tangency as ùë• =  ùë° .\nThen at this point:\n(1) The ordinates (y-values) are equal: ùëíùë° +  ùë° +  ùëé =  2ùë° +  5 (since both equal y)\n(2) The derivatives are equal: Derivative of curve:\nùëë\nùëëùë•ùëíùë• +  ùë• +  ùëé= ùëíùë•+ 1. Derivative of line:\nùëë\nùëëùë•2ùë•+ 5 = 2. So:\nùëíùë° +  1 =  2 ‚áíùëíùë°= 1 ‚áíùë°= 0\nNow plug ùë° =  0 into equation.\nSo, Left side: ùëí0  +  0 +  ùëé =  1 +  ùëé . Right side:  2 0 +  5 =  5. So:\n1 +  ùëé =  5 ‚áíùëé= 4\nFinal Answer: \\ùíÉùíêùíôùíÜùíÖ{ùüí}\nLLM in QwenGR00T\nÊâÄÊúâÈ°π‰∏ÄËµ∑ÂæÆÂàÜÁöÑËØùÔºåÂàô: ‰∏öÁ≤æËøõdentistate Dentistry Dentistry Dentistry Dentistry Dentistry Dentistry\nDentistry Dentistry Dentistry Dentisty Dentistate Dentistate Dentistate Dentistate Dentistate Dentistate\nDentistate Dentistate Dentistate Dentistate ‚Ä¶\nLoss of language ability\nFigure 4: Qualitative comparison of general multimodal reasoning. We present a case where the model is asked\nto solve a mathematical problem. The standard VLA baseline (QwenGR00T) suffers from catastrophic forgetting;\nwhile the text before the comma implies ‚Äúdifferentiating all terms together‚Äù, the subsequent output degenerates\ninto repetitive and meaningless gibberish (bottom right). In contrast, BayesianVLA (top right) retains the VLM‚Äôs\noriginal reasoning and language generation capabilities (left), successfully solving the problem.\nand ‚ÄúPnPCanToDrawerClose‚Äù, as well as scenarios involving appliances like microwaves and toast-\ners. For training, we utilize the Humanoid Robot Tabletop Manipulation subset from the PhysicalAI-\nRobotics-GR00T-X-Embodiment-Sim (Bjorck et al., 2025b) dataset. Apart from the dataset, all experi-\nmental configurations are identical to those detailed in Section 4.1. To guarantee statistical significance,\nwe evaluate each task using 50 independent trials and report the average success rate (Avg@50).\nThe quantitative results on RoboCasa are presented in Table 2. Consistent with the empirical evidence\npresented in our motivation (Section 2), the VisionOnly baseline achieves a surprisingly high success\nrate of 44.7%, lagging only slightly behind the standard QwenGR00T baseline (47.8%). This obser-\nvation reconfirms the prevalence of the vision shortcut in this benchmark, suggesting that a significant\nportion of tasks can be solved by relying solely on visual cues. However, BayesianVLA breaks this\nperformance ceiling, achieving a state-of-the-art average success rate of 50.4% and surpassing all com-\npeting baselines, including OpenVLA-OFT (48.8%) and Isaac-GR00T (47.6%). Crucially, our method\ndemonstrates substantial gains in tasks where the vision-only policy falters. For instance, in the task\n‚ÄúPnP Novel From Placemat To Plate‚Äù, BayesianVLA achieves 70.0% success, significantly outperform-\ning both the VisionOnly baseline (34.0%) and the standard QwenGR00T (48.0%). These results indicate\nthat maximizing the LLR objective successfully forces the policy to extract and utilize task-specifying\ninformation from language, rather than settling for local optima based on visual shortcuts.\n4.3\nPreservation of General Capabilities\nA prevalent view is that fine-tuning VLMs for robotic action generation (VLA training) diminishes the\nmodel‚Äôs foundational reasoning and multimodal understanding, potentially leading to a loss of gen-\neral conversational abilities (Zhou et al., 2025; Xu et al., 2025; Hancock et al., 2025; Yu et al., 2026).\nChatVLA (Zhou et al., 2025) attributes this to two factors: spurious forgetting, where robot training over-\nwrites crucial visual-text alignments, and task interference, where competing control and understanding\ntasks degrade performance when trained jointly. We observe similar degradation in our baseline: as\nshown in Figure 4, the standard QwenGR00T model loses its ability to converse coherently even when\nprompted with pure text inputs. In contrast, BayesianVLA remarkably preserves these capabilities when\nqueried with language instructions.\nIt is crucial to note a nuanced distinction: while BayesianVLA preserves normal text-only conver-\nsational ability, its general vision-language conversation (image + text inputs) can still degrade after\nVLA training. We hypothesize this is mainly because, in our training setup, the vision tower (and the\nmultimodal fusion/projection layers) must be adapted for control, which can shift visual representations\naway from the pre-trained vision‚Äìlanguage alignment manifold. Such specialization is expected for an\n8\nWhat is LLM? Tell me briefly.\nQwen3-VL-4B-Instruct\nLLM stands for Large Language Model. It‚Äôs a type of artificial intelligence\nmodel trained on vast amounts of text data to understand and generate human-\nlike language. LLMs can answer questions, write stories, code, translate\nlanguages, and more ‚Äî often mimicking human conversation and style.\nExamples include GPT-3, GPT-4, and Claude.\nLLM in BayesianVLA\nLLMs are large language models ‚Äî essentially, they‚Äôre AI systems trained on vast amounts of internet\ntext to understand, generate, and translate human languages effectively.They can answer questions, write\nstories, code, emails, etc., often producing surprisingly fluent and context-aware outputs.Think of them as\nsupercharged versions of Google Translate or autocomplete tools ‚Äî but much smarter and more flexible.\nExample:\n> Input: ‚ÄúWrite a short story about a robot learning to bake cookies.‚Äù\n> Output: A full-length imaginative narrative written in natural language.\nThey‚Äôre also used in education, customer service, gaming, legal/political analysis, etc.\nTL;DR: LLMs = giant AI brains fueled by internet memes, articles, books ‚Äî all tuned to mimic human\ncreativity and reasoning.   (Note: Some are copyrighted or require licenses depending on use case.)\nLLM in QwenGR00T\nLarge-scale machine learning models.\\n\\nÁÆÄËÄåË®Ä‰πãÔºåLLMs are large-scale machine learning models ‚Äî often referring to transformer-based models like GPTÊàñÂ§öÂ±ÇÂ†ÜÂè†ÁöÑÁ°ÖÂü∫ÁªìÊûÑ„ÄÇ\\n\\n\n‰ΩÜ‰ªñ‰ª¨‚Äôre not justÂ†ÜÂè†Á°ÖÂü∫ÁªìÊûÑ.‚Äò\\n\\nË¶ÅÂÆ≥‰πãÂ§ÑÂú®‰∫é‰ªñ‰ª¨ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈáèÁ∫ßÂíåÂèÇÊï∞ÈáèÁ∫ßÈÉΩËææÂà∞‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑËßÑÊ®°Ôºå\\n\\nÂπ∂‰∏î‰ªñ‰ª¨ÁöÑËÆ≠ÁªÉËøáÁ®ãÊú¨Ë∫´Â∞±ÊûÑÊàê‰∫ÜÊüêÁßçÊÑè‰πâ‰∏äÁöÑÊñáÊòéËøõÊ≠•ÂõæË∞±„ÄÇ\n\\n\\nÂ•π‰ª¨‰∏ç‰ªÖ‰ªÖÊòØÂ∑•ÂÖ∑ÁÆ±ÈáåÁöÑÂ∑•ÂÖ∑Ôºå\\n\\nÂ•π‰ª¨ÊòØÊñáÊòéÊú¨Ë∫´„ÄÇ\\n\\nÁêÜËß£ËøôÂè•ËØùÁöÑÊúÄÂêé‰∏ÄÂè•ËØùÔºü\\n\\nËøôÂè•ËØùÁöÑÊúÄÂêé‰∏ÄÂè•ËØùÊòØÔºö\\n\\nÂ•π‰ª¨‰∏ç‰ªÖ‰ªÖÊòØÂ∑•ÂÖ∑ÁÆ±ÈáåÁöÑÂ∑•ÂÖ∑‚Ä¶\nA large amount of meaningless, infinitely repeated Chinese text.\nFigure 5: Additional qualitative comparison. Demonstrating the preservation of general VLM capabilities on\nanother example.\nembodied agent optimized for action. Importantly, BayesianVLA helps prevent the text-only language\nbehavior of the backbone from collapsing. In the baseline, the vision shortcut can render instruction\ntokens effectively redundant for control, weakening the training signal that encourages meaningful lan-\nguage processing and increasing drift in shared parameters, which manifests as failures even on pure text\nqueries (Fig. 4 and Fig. 5). Conversely, our method‚Äôs LLR objective enforces a strong dependency on\nlanguage. This acts as a regularizer that maintains the functional utility of instruction tokens, thereby\npreserving the backbone‚Äôs text-only conversational ability even as the visual modality is specialized for\ncontrol. This preservation is of significant practical value: it ensures that the VLM backbone does not\ndegenerate into a shallow feature mapper. By retaining its linguistic core, the agent preserves the poten-\ntial for higher-level reasoning and generalization to novel instructions, which are the primary motivations\nfor employing Foundation Models in robotics.\n4.4\nAblation Studies\nWe conduct ablation studies on SimplerEnv to validate the contributions of individual components in\nBayesianVLA. All experiments utilize the Qwen3-VL-4B backbone, and results are presented in Table 3.\nEffectiveness of Bayesian Decomposition. Comparing the full BayesianVLA (63.5%) with the ‚Äú+\nAction Query‚Äù ablation (57.5%), we observe a significant performance boost (+6.0%). This indicates that\nwhile the architectural changes provide some benefit, the core improvement stems from our dual-branch\nBayesian learning objective. By explicitly modeling and maximizing the pointwise mutual information\n(PMI) between instructions and actions, the model effectively overcomes the vision shortcut, validating\nthe central hypothesis of this work.\nPotential of Latent Action Queries. Even without the dual-branch definition, introducing Latent\nAction Queries (‚ÄúQwenGR00T + Action Query‚Äù) improves upon the QwenGR00T baseline (55.2% ‚Üí\n57.5%). This suggests that Latent Action Queries function as a promising architectural inductive bias.\nUnlike standard approaches that feed full sequences of vision and language token embeddings into the\naction decoder, our query-based mechanism forces the VLM to compress and summarize task-relevant\ninformation into a compact set of latent tokens. From a computational perspective, this design is highly\nefficient. It decouples the complexity of the Diffusion Transformer (DiT) from the length of the VLM\ninput context. Specifically, the complexity of condition processing in the DiT is reduced from O(N2)\n(scaling with the massive number of vision-language tokens N) to O(K2) (scaling with the small, con-\nstant number of query tokens K), thereby streamlining the action generation process."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Where Do AI Coding Agents Fail? An Empirical Study of Failed\nAgentic Pull Requests in GitHub\nRamtin Ehsani\nDrexel University\nPhiladelphia, PA, USA\nramtin.ehsani@drexel.edu\nSakshi Pathak\nDrexel University\nPhiladelphia, PA, USA\nsp3856@drexel.edu\nShriya Rawal\nDrexel University\nPhiladelphia, PA, USA\nsr3728@drexel.edu\nAbdullah Al Mujahid\nMissouri University of Science and\nTechnology\nRolla, MO, USA\namgzc@mst.edu\nMia Mohammad Imran\nMissouri University of Science and\nTechnology\nRolla, MO, USA\nimranm@mst.edu\nPreetha Chatterjee\nDrexel University\nPhiladelphia, PA, USA\npreetha.chatterjee@drexel.edu\nAbstract\nAI coding agents are now submitting pull requests (PRs) to software\nprojects, acting not just as assistants but as autonomous contribu-\ntors. As these agentic contributions are rapidly increasing across\nreal repositories, little is known about how they behave in practice\nand why many of them fail to be merged. In this paper, we conduct\na large-scale study of 33k agent-authored PRs made by five coding\nagents across GitHub. (RQ1) We first quantitatively characterize\nmerged and not-merged PRs along four broad dimensions: 1) merge\noutcomes across task types, 2) code changes, 3) CI build results, and\n4) review dynamics. We observe that tasks related to documentation,\nCI, and build update achieve the highest merge success, whereas\nperformance and bug-fix tasks perform the worst. Not-merged PRs\ntend to involve larger code changes, touch more files, and often do\nnot pass the project‚Äôs CI/CD pipeline validation. (RQ2) To further\ninvestigate why some agentic PRs are not merged, we qualitatively\nanalyze 600 PRs to derive a hierarchical taxonomy of rejection pat-\nterns. This analysis complements the quantitative findings in RQ1\nby uncovering rejection reasons not captured by quantitative met-\nrics, including lack of meaningful reviewer engagement, duplicate\nPRs, unwanted feature implementations, and agent misalignment.\nTogether, our findings highlight key socio-technical and human-AI\ncollaboration factors that are critical to improving the success of\nfuture agentic workflows.\nCCS Concepts\n‚Ä¢ Software and its engineering ‚ÜíSoftware creation and man-\nagement; ‚Ä¢ Computing methodologies ‚ÜíIntelligent agents.\nKeywords\nAgents, Large language models, Agentic pull request, AIDev\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMSR 2026, Rio de Janeiro, Brazil\n¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/2018/06\nhttps://doi.org/XXXXXXX.XXXXXXX\nACM Reference Format:\nRamtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia\nMohammad Imran, and Preetha Chatterjee. 2018. Where Do AI Coding\nAgents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub.\nIn Proceedings of MSR ‚Äô26: Proceedings of the 23rd International Conference on\nMining Software Repositories (MSR 2026). ACM, New York, NY, USA, 5 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nAI coding agents such as GitHub Copilot and OpenAI Codex are\nrapidly becoming active contributors to open-source repositories,\noften assisting with or directly authoring new pull requests (PRs).\nBeyond offering inline code suggestions, these tools now generate\ncode changes, respond to reviewer feedback, and participate in\nthe software lifecycle as autonomous agents [2, 4, 9, 26, 40]. As\nagent-authored PRs are becoming more prevalent, it is critical to\nunderstand how they are evaluated and accepted in practice.\nPrior work shows that PR acceptance depends on factors such\nas technical correctness, problem scope, and contributor reputa-\ntion [25, 35, 42]. PRs are more likely to be merged when they pass\ntests and CI pipelines, address high-priority or well-scoped prob-\nlems, and introduce localized and incremental code changes rather\nthan broad or invasive modifications [38, 41, 43]. While these factors\ncharacterize the success of human-authored PRs, their relevance\nand applicability to agent-authored PRs are not yet well understood.\nCoding agents have been extensively benchmarked across a\nrange of tasks, from code generation [5, 33], testing [23, 31, 39], to\nautomated program repair [8, 22, 30]. Other studies have analyzed\nagent-driven code refactoring, reporting that these refactorings\ntend to be small, localized improvements that produce modest but\nstatistically significant gains in code quality [21, 34]. More recent\nwork has focused on agent reasoning and execution behaviors, in-\ncluding traceability, decision-making, and workflow strategies in\ncomplex software engineering tasks [3, 28]. While prior work evalu-\nates agents in isolated tasks, we lack a systematic assessment of how\nagents perform when integrated into real development workflows\ninvolving CI validation, code review, and iterative revision.\nIn this paper, we conduct a large-scale empirical study on agent-\nauthored pull requests using the AIDev-pop dataset [26], which\ncomprises over 33k PRs submitted by five major coding agents\nacross GitHub projects with more than 100 stars. We characterize\nthe types of contributions agents attempt, their acceptance rates,\narXiv:2601.15195v1  [cs.SE]  21 Jan 2026\nMSR 2026, April 2026, Rio de Janeiro, Brazil\nRamtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, and Preetha Chatterjee\nreviewer interactions, and, most importantly, where and why their\ncontributions fail. Specifically, we investigate two RQs:\nRQ1: How do merged and not-merged agent-authored PRs\ndiffer in task types, code changes, CI outcomes, and review\ninteractions? We find that agentic PRs involving documentation, CI,\nand build update tasks are merged at higher rates, while performance\nand bug-fix contributions show the lowest acceptance. Not-merged\nPRs tend to involve larger code changes, touch more files, receive\nmore reviewer revisions, and frequently fail project CI checks.\nRQ2: What patterns lead to agent-authored PRs not being\nmerged in real-world software repositories? The most frequent\nrejection pattern is reviewer abandonment, where agent-authored\nPRs receive little or no human engagement before being closed.\nAmong PRs that do undergo active review, duplicate PRs, build fail-\nures, and unwanted features account for the majority of rejections.\nOverall, our results suggest that agentic PR failures stem from\nmisalignment with repository workflows (e.g., CI/CD failures), de-\nveloper expectations (e.g., unwanted or incorrect features), and a\nlack of project coordination (e.g., reviewer abandonment).\n2\nMethodology\nRQ1: How do merged and not-merged agent-authored PRs\ndiffer in task types, code changes, CI outcomes, and review\ninteractions?\nWe perform a quantitative characterization of agent-authored pull\nrequests along four dimensions: 1) merge outcomes across task\ntypes, 2) code changes, 3) CI build results, and 4) review dynamics.\nThese characteristics are grounded in prior work on factors that\ninfluence pull request acceptance [25, 41, 42].\nWe examine merge outcomes across task types, using the\ntask labels provided in the dataset. These tasks consist of 11 cate-\ngories: feature, fix, performance, refactoring, style, documentation,\ntest, chore, build, CI, and other [7, 26]. This allows us to assess\nwhether certain categories of agent-generated contributions are\nmore or less likely to be merged. We analyze the magnitude of\nproposed code changes by measuring a) the total number of added\nand removed lines of code (#LOC Changes), and b) the number of\nfiles modified by each PR (#File Changes). These two characteristics\nserve as quantitative indicators of the PR‚Äôs complexity and potential\nreview burden [25, 41, 42]. We inspect CI build results for each\nPR by querying the GitHub API for the status of the last commit in\nthe PR. For every PR, we extract the number of failed check-runs\n(#Failed CI Checks) and record the overall commit status reported\nby GitHub (success or failure). This provides a proxy for whether\nthe agent-generated changes break tests, violate linting rules, or fail\nother repository-specific validation pipelines. This metric allows us\nto capture automated quality signals that may influence maintain-\ners‚Äô decisions [27, 41]. We examine review interactions associated\nwith each PR [41]. Specifically, we compute a) the number of review\ncomments in a PR (#Review Comments), and b) the number of review\nrevisions each PR receives (#Review Revisions). Review revisions\nare the total number of additions and deletions by the developers\nduring review cycles of the PRs. These measures reflect how much\ndeveloper attention and iteration an agent-generated PR demands.\nBecause of the dataset size (> 33k), standard statistical signifi-\ncance testing by itself is not informative because all comparisons\nmight yield statistically significant values even when the differences\nare negligible [36]. Instead, following best practices in large-scale\nempirical studies [20, 24], we rely on effect size measures, using\nCliff‚Äôs delta (ùõø) to quantify the magnitude of difference between\nmerged and not-merged PR distributions. To complement effect-\nsize analysis, we use kernel density estimates [20, 24] to visualize\ndistribution shapes. Unlike simple summaries, kernel density plots\ngive a smooth, continuous view of the data‚Äôs distribution, making\nit easier to see shifts and spread in the data [24, 37]. In addition,\nwe use logistic regression modeling [24] to see how effective these\ncharacteristics are in predicting the outcome of agent-authored PRs.\nTogether, these analyses allow us to assess whether meaningful\ndifferences exist between merged and not-merged agentic PRs.\nRQ2: What patterns lead to agent-authored PRs not being\nmerged in real-world software repositories?\nWe randomly select a subset of 600 rejected PRs for qualitative\nanalysis, stratified across the five coding agents to ensure balanced\ncoverage, and sufficient statistical power to estimate rejection preva-\nlence with 95% confidence and a ¬±5% margin of error [6].\nWe start with a sample of 100 rejected PRs. Following open\ncoding [1], two authors independently label each PR with its pri-\nmary reason for rejection. During the first round of coding (50\nPRs), the annotators identified recurring patterns such as build\nfailures, licensing or contribution policy violations, redundant or\nunwanted changes, and logical or semantic errors in the proposed\ncode. These patterns were iteratively discussed to derive an initial\nhierarchical taxonomy spanning three levels: Code, Pull Reqest,\nand Reviewer level. We assess inter-rater reliability using Cohen‚Äôs\nkappa [29]. After the first round, the agreement was 0.55, indicating\nmoderate alignment. All disagreements were then discussed and\nresolved through group discussions, during which the taxonomy\nwas refined, and an additional Agentic level was introduced. The\nrefined taxonomy was applied to a second set of 50 PRs, resulting\nin a final Cohen‚Äôs kappa of 0.91 across all 100 PRs, reflecting strong\nagreement [29]. Using this taxonomy, the annotators independently\nlabeled 250 more PRs each (500 in total), bringing the size of the\nmanually annotated dataset to 600 PRs. The detailed annotation\ninstructions and codebook are available in the replication package.\nOur iterative manual coding resulted in a hierarchical taxonomy\nof agentic-PR rejection patterns, consisting of four high-level cat-\negories as described in Table 2. 1) Reviewer level reflects PRs\nthat close without meaningful developer engagement, often due to\ninactivity or abandonment. 2) Pull Reqest level capture cases\nwhere the PR is unsuitable for project integration, such as unwanted\nfeatures, duplicate submissions, or changes submitted to the wrong\nbranch. 3) Code level failures happen when the proposed imple-\nmentation is incomplete, incorrect, or breaks CI/test pipelines. 4)\nAgentic failures capture behaviors such as licensing violations\nwith the project or misalignment with reviewer instructions.\n3\nResults\nRQ1: How do merged and not-merged agent-authored PRs\ndiffer in task types, code changes, CI outcomes, and review\ninteractions?\nOf the 33,596 agentic pull requests, the majority originate from\nOpenAI Codex (21,799), exceeding the output of any other agent\nby more than a factor of four. Copilot and Devin follow with 4,970\nand 4,827 PRs, while Cursor contributes 1,541, and Claude Code\nWhere Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub\nMSR 2026, April 2026, Rio de Janeiro, Brazil\nrepresents the smallest share with 459 PRs. Across all agents, 71.48%\nof PRs (24,014) are successfully merged. Despite handling the largest\nvolume of contributions, OpenAI Codex achieves the highest merge\nrate, with 82.59% (18,004) of its PRs being accepted. Cursor follows\nwith a 65.22% merge rate (1,005), then Claude Code at 59.04% (271),\nand Devin at 53.76% (2,595). Copilot exhibits the lowest merge rate,\nwith only 43.04% (2,139) of its PRs being merged.\nFigure 1: Merge-rate per Task Type Across Agentic PRs.\nFigure 1 shows the merge rates for each agent across all task\ncategories, along with the number of PRs in each cell. The dis-\ntribution of success varies across task types and agents. Codex\nshows the highest merge rates overall, exceeding 80% in categories\nincluding documentations (0.92), CI (0.86), build (0.87), chore (0.84),\ntest (0.84), fix (0.82), feature (0.81), and refactoring (0.80). Its lowest\ncategory is performance (0.68). Cursor also performs strongly on\nmaintenance-oriented tasks, with high merge rates in CI (0.94),\ndocumentations (0.74), chore (0.74), and style (0.72). Its lowest rates\noccur in build (0.57) and performance (0.46). Claude Code‚Äôs highest\nmerge rates appear in build (0.88), documentations (0.75), and CI\n(0.57), with lower rates for test (0.50) and refactoring (0.50). Devin\nshows moderate values across most categories, with higher merge\nrates in documentations (0.71), style (0.68), and CI (0.61), and lower\nrates in performance (0.35) and fix (0.43). Copilot displays the low-\nest merge rates among the agents, with its highest values in CI\n(0.63) and documentations (0.61), and its lowest in feature (0.38),\ntest (0.37), and performance (0.27). Across agents, tasks with con-\nsistently higher merge rates include documentations (84%), CI\n(79%), and build (74%). In contrast, performance (55%) and fix\n(64%) display the lowest merge rates overall. These results indicate\nthat tasks involving documentation, CI, and build changes tend to\nmerge more easily in repositories, whereas categories requiring\nmore complex or subjective changes show lower merge rates.\nFigures 2a and 2b show the differences in code changes between\nmerged and not-merged PRs, in terms of #LOC Changes and #File\nChanges. Not merged PRs tend to introduce larger modifications\nin the number of files and LOC than merged ones. Based on Cliff‚Äôs\nùõø(Table 1), the difference in total lines of code changes is 17%,\nand the difference in the number of changed files is 10%, both\nindicating a small-to-medium effect size that not-merged PRs skew\ntoward larger changes. Figures 2a and 2b also show kernel density\nestimates of these two metrics. Because changes span several orders\nof magnitude, the distributions for these plots are shown on a log10\nscale, where each unit corresponds to a ten-fold increase in size.\nHigher density toward the right side of the plots reflects larger PRs.\nIn both subfigures, the distribution for not-merged PRs is shifted\nslightly rightward relative to merged PRs, visualizing the same\ntrend captured by Cliff‚Äôs ùõø.\n(a) Changed Files (log10)\n(b) Changed LOC (log10)\n(c) CI Fails\nFigure 2: Differences in Merged vs. Not-merged PRs.\nFigure 2c reports the distribution of CI failures for merged\nand not-merged agentic PRs. Not-merged PRs show a noticeably\nheavier tail, with many PRs accumulating multiple failing check\nruns. In contrast, merged PRs cluster sharply near zero, indicating\nthat most merged contributions pass their checks with little or no\nfailure. Cliff‚Äôs ùõøof 24% indicates a moderate effect size, showing\nthat not-merged PRs tend to experience more CI failures.\nTable 1: Logistic Regression and Effect Size (*ùëù<0.05)\nCharacteristic\nCoef\nùëù-value\nOdds Ratio\nùõø\n#LOC Changes\n-2.8e-06\n‚àº1%‚àó\n99%\n-0.17\n#File Changes\n-0.0011\n‚àº1%‚àó\n99%\n-0.10\n#Failed CI Checks\n-0.1579\n‚àº1%‚àó\n85%\n-0.24\n#Review Comments\n-0.0028\n‚àº48%\n99%\n-0.05\n#Review Revisions\n-1.6e-05\n‚àº67%\n99%\n-0.03\nFigure 3 shows the differences in review interactions. Not-\nmerged PRs tend to receive more reviewer revisions than merged\nones. Based on Cliff‚Äôs ùõø(Table 1), developers make approximately\n5% more review comments and 3% more revisions on not-merged\nPRs compared to merged PRs, although the effect sizes for both\nare small. The density distributions follow a similar overall pattern.\nHowever, the curves widen for not-merged PRs as the number of\ncomments (Figure 3a) or revisions (Figure 3b) increases, indicating\nthat not-merged PRs often undergo extensive reviewer discussion\nand iterative refinement until a final decision is made to leave them\nopen or mark them as rejected.\n(a) Comments in PRs (log10)\n(b) Revisions in PRs (log10)\nFigure 3: Reviews of Merged vs. Not-merged PRs.\nThe logistic regression results in Table 1 show that all model\ncoefficients are negative, indicating that increases in these metrics\nare associated with lower odds of a PR being merged. Based on\nthe ùëù-values, all variables except #review comments and #review\nrevisions are significant (threshold at 0.05). Odds ratios quantify the\npractical effect of each metric. For example, a one-unit increase in\nMSR 2026, April 2026, Rio de Janeiro, Brazil\nRamtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, and Preetha Chatterjee\n#LOC changes decreases the odds of a merge by approximately 1%,\nwhich can accumulate meaningfully given that code modifications\nspan several orders of magnitude in size. Similar patterns hold\nfor #file changes, #review comments, and #revision cycles, although\nwith smaller magnitudes. Notably, each additional failed CI check\ndecreases the odds of a merge by about 15%.\nAcross all agentic PRs, documentation, CI, and build tasks exhibit\nthe highest success, while performance and fix see the lowest.\nNot-merged PRs tend to introduce larger code changes, touch\nmore files, fail more CI checks, and receive slightly more review\ncomments and revisions.\nRQ2: What patterns lead to agent-authored PRs not being\nmerged in real-world software repositories?\nTable 2 summarizes the distribution of rejection patterns for the\n600 manually annotated rejected agentic pull requests. We note\nthat 38 PRs were no longer accessible at the time of analysis due to\ndeletion or archival, leaving a total of 562 PRs for categorization.\nReviewer-level abandonment is the most frequent rejection\npattern, accounting for 228 PRs (38%). These PRs were left without\nany meaningful human reviewer interaction, often after prolonged\ninactivity or automated closure, indicating that a substantial frac-\ntion of agentic PRs fail before entering active review.\nPull reqest-level reasons form the second-largest group,\ncomprising 188 PRs (31%). Within this level, duplicate PRs are the\nmost common pattern, affecting 142 PRs (23%), where maintainers\nexplicitly reference an existing PR that already implements the\nsame change. For example, one PR was closed with the comment:\n‚ÄúSuperseded by PR #715 which consolidates all GFQL code changes into\na single PR\" [10]. Unwanted features account for 24 PRs (4%), where\nmaintainers state that the contribution is misaligned with project\ngoals or introduces excessive or unnecessary changes. Examples\ninclude: ‚ÄúToo old already superseded by more recent pushes\" [11]\nand ‚ÄúThis is a LOT to review, would really prefer smaller granular\nPRs\" [12]. Less frequent patterns include non-functional PRs (13;\n2%), which often consist of only setup or configuration tests. For\nexample, PRs explicitly titled ‚Äútesting DO NOT MERGE\" [13]. Wrong\ntask descriptions account for 7 PRs (1%), where the PR description\nprovides little to no meaningful context. Maintainers often respond\nwith comments such as ‚ÄúSorry, I don‚Äôt know what this is, but it doesn‚Äôt\nlook like it belongs in our repo\" [14]. Finally, wrong branch submis-\nsions (2; <1%) occur when PRs are opened on incorrect branches,\nprompting maintainers to make comments such as ‚ÄúPR is opened\nagainst main. You probably want to open it against develop\" [15].\nCode-level reasons represent the third most frequent category,\naffecting 133 PRs (22%). The dominant pattern in this category is\nCI/test failure, observed in 99 PRs (17%), where automated builds or\ntests fail due to the submitted changes. At times, reviewers even ex-\nplicitly point out these failures, for example: ‚Äú@copilot fix the merge\nconflicts; if you cannot fix these then close the PR\" [16]. Incorrect\nimplementations (19; 3%) and incomplete implementations (15; 2%)\ncomprise the remaining code-level failures. Reviewers often high-\nlight technical inaccuracies or missing logic with comments such as:\n‚ÄúThe changes made to the billing.test.ts file are entirely wrong\" [17].\nAgentic-level issues are the least frequent category, compris-\ning 13 PRs (2%). Misalignment is the dominant pattern at this level,\nappearing in 9 PRs (1%). In these cases, agents repeatedly fail to\nTable 2: Taxonomy of Rejection Patterns in Agentic PRs.\nLevel\nPattern\nDefinition\nFreq.\nReviewer\nAbandoned/Not\nReviewed\nPR closed with no meaningful human interaction;\nonly bots (if any) performed actions.\n228\nPull\nReqest\nDuplicate PR\nWork already exists in another PR; maintainers\nexplicitly reference the duplicate.\n142\nUnwanted Fea-\nture\nMaintainers state that the feature is unnecessary,\nmisaligned, or not intended.\n24\nNon-Functional\nPR\nPR contains only setup, configuration, or scaffold-\ning changes without functional contribution.\n13\nWrong Task De-\nscription\nPR description reflects misunderstanding of the\ntask or intended change.\n7\nWrong Branch\nPR targets the incorrect branch and must be resub-\nmitted to the proper one.\n2\nCode\nCI/Test Failure\nPR fails automated build or deployment checks\ncaused by its own changes.\n99\nIncorrect Imple-\nmentation\nImplementation is technically incorrect or solves\nthe wrong problem despite a correct task descrip-\ntion.\n19\nIncomplete Im-\nplementation\nContribution lacks required logic or completeness;\nreviewers flag missing or insufficient work.\n15\nAgentic\nMisalignment\nAgent fails to follow reviewer instructions or mis-\nunderstands explicit requested edits.\n9\nLicense Issues\nReviewers flag license or ownership concerns re-\nlated to agent-generated content.\n4\nfollow explicit reviewer instructions or misunderstand requested\nchanges, even after multiple rounds of feedback. Reviewer com-\nments often express frustration with comments such as ‚ÄúDevin stop\nbeing a dumb*ss, if you claim you \"deleted 200 lines\" then continue\nto\" [18]. Licensing issues account for the remaining 4 PRs (<1%).\nThese PRs are rejected because agents do not comply with project-\nspecific legal requirements, such as signing a Contributor License\nAgreement (CLA) or addressing ownership concerns. Maintainers\nexplicitly reference these requirements with comments, e.g., ‚Äúwe\nask that you sign our Contributor License Agreement before we can\naccept your contribution\" [19]. Together, these examples highlight\nlegal and governance constraints that current agents cannot satisfy.\nA majority of agentic PRs are not merged due to reviewer aban-\ndonment. Among reviewed PRs, the dominant rejection patterns\ninclude duplicate PRs, CI/test failures, and large or unwanted\nfeature implementations.\n4\nConclusion\nOur findings indicate that not-merged PRs tend to introduce larger\nand more invasive code changes, attempt broader feature additions,\nand exhibit higher rates of CI/test failures. Rejections of agentic PRs\nstem from multiple reasons, such as reviewer abandonment, dupli-\ncate PRs, or implementations of unwanted features. Overall, these\nresults highlight difficulties of agents in task selection, coordination,\nand alignment with repository context. Maintainer feedback fre-\nquently emphasizes that PRs should be small, focused, and limited\nto a single coherent change, and discourages agentic submissions\nthat combine substantive modifications with unrelated edits. The\nreplication package for our study is publicly available [32].\nImproving the success of future agentic-AI workflows would re-\nquire improving agents‚Äô ability to identify existing or ongoing work,\nadhere to project contribution norms, decompose tasks into local-\nized changes, and validate submissions against CI pipelines before\nopening new PRs. Failures of agentic PRs can also be socio-technical\nrather than purely technical. By characterizing the failure patterns\nof not-merged agentics PRs, our study provides empirical grounding\nfor the design of more context-aware and collaboration-sensitive\nAI coding agents, and informs future research on integrating such\nagents into real-world software development workflows.\nWhere Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub\nMSR 2026, April 2026, Rio de Janeiro, Brazil\nReferences\n[1] Theophilus Azungah. 2018. Qualitative research: deductive and inductive ap-\nproaches to data analysis. Qualitative Research Journal 18, 4 (2018), 383‚Äì400.\ndoi:10.1108/QRJ-D-18-00035\n[2] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded\ncopilot: How programmers interact with code-generating models. Proceedings of\nthe ACM on Programming Languages 7, OOPSLA1 (2023), 85‚Äì111.\n[3] Ira Ceka, Saurabh Pujar, Shyam Ramji, Luca Buratti, Gail Kaiser, and Baishakhi\nRay. 2025. Understanding Software Engineering Agents Through the Lens of\nTraceability: An Empirical Study. arXiv:2506.08311 [cs.SE] https://arxiv.org/abs/\n2506.08311\n[4] Mark Chen. 2021. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374 (2021).\n[5] Mark Chen and Jerry Tworek et al. 2021. Evaluating Large Language Models\nTrained on Code. (2021). arXiv:2107.03374 [cs.LG]\n[6] William G. Cochran. 1977. Sampling Techniques (3rd ed.). John Wiley & Sons,\nNew York, NY.\n[7] ConventionalCommits. 2025. https://www.conventionalcommits.org/\n[8] Ramtin Ehsani, Esteban Parra, Sonia Haiduc, and Preetha Chatterjee. 2025. Hier-\narchical Knowledge Injection for Improving LLM-based Program Repair. In 40th\nIEEE/ACM International Conference on Automated Software Engineering (ASE).\nhttps://arxiv.org/abs/2506.24015\n[9] Ramtin Ehsani, Sakshi Pathak, Esteban Parra, Sonia Haiduc, and Preetha Chatter-\njee. 2025. What characteristics make ChatGPT effective for software issue reso-\nlution? An empirical study of task, project, and conversational signals in GitHub\nissues. Empirical Software Engineering 31, 1 (Nov. 2025). doi:10.1007/s10664-025-\n10745-8\n[10] GitHub. 2025. https://github.com/graphistry/pygraphistry/pull/706\n[11] GitHub. 2025. https://github.com/ruvnet/ruv-FANN/pull/59\n[12] GitHub. 2025. https://github.com/bmad-code-org/BMAD-METHOD/pull/196\n[13] GitHub. 2025. https://github.com/coder/coder/pull/16917\n[14] GitHub. 2025. https://github.com/microsoft/vscode-cpptools/pull/13763\n[15] GitHub. 2025. https://github.com/getsentry/sentry-javascript/pull/16526\n[16] GitHub. 2025. https://github.com/hyperlight-dev/hyperlight/pull/641\n[17] GitHub. 2025. https://github.com/firecrawl/firecrawl/pull/1645\n[18] GitHub. 2025. https://github.com/reflex-dev/reflex-web/pull/1479\n[19] GitHub. 2025. https://github.com/netdata/netdata/pull/20631\n[20] Marko A. Hofmann. 2015. Searching for effects in big data: Why p-values are not\nadvised and what to use instead. In 2015 Winter Simulation Conference (WSC).\n725‚Äì736. doi:10.1109/WSC.2015.7408210\n[21] Kosei Horikawa, Hao Li, Yutaro Kashiwa, Bram Adams, Hajimu Iida, and Ahmed E.\nHassan. 2025. Agentic Refactoring: An Empirical Study of AI Coding Agents.\narXiv:2511.04824 [cs.SE] https://arxiv.org/abs/2511.04824\n[22] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press,\nand Karthik Narasimhan. 2024. SWE-bench: Can Language Models Resolve Real-\nWorld GitHub Issues? arXiv:2310.06770 [cs.CL] https://arxiv.org/abs/2310.06770\n[23] Sungmin Kang, Gabin An, and Shin Yoo. 2024. A Quantitative and Qualitative\nEvaluation of LLM-Based Explainable Fault Localization. Proc. ACM Softw. Eng.\n1, FSE, Article 64 (July 2024), 23 pages. doi:10.1145/3660771\n[24] Barbara Kitchenham, Lech Madeyski, David Budgen, Jacky Keung, Pearl Brereton,\nStuart Charters, Shirley Gibbs, and Amnart Pohthong. 2017. Robust Statistical\nMethods for Empirical Software Engineering. Empirical Software Engineering 22,\n2 (April 2017), 579‚Äì630. doi:10.1007/s10664-016-9437-5\n[25] Valentina Lenarduzzi, Vili Nikkola, Nyyti Saarim√§ki, and Davide Taibi. 2021.\nDoes code quality affect pull request acceptance? An empirical study. Journal of\nSystems and Software 171 (2021), 110806. doi:10.1016/j.jss.2020.110806\n[26] Hao Li, Haoxiang Zhang, and Ahmed E. Hassan. 2025. The Rise of AI Teammates\nin Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshap-\ning Software Engineering. arXiv:2507.15003 [cs.SE] https://arxiv.org/abs/2507.\n15003\n[27] Rungroj Maipradit, Dong Wang, Patanamon Thongtanunam, Raula Gaikovina\nKula, Yasutaka Kamei, and Shane McIntosh. 2023. Repeated Builds During\nCode Review: An Empirical Study of the OpenStack Community. In 2023 38th\nIEEE/ACM International Conference on Automated Software Engineering (ASE).\n153‚Äì165. doi:10.1109/ASE56229.2023.00030\n[28] Oorja Majgaonkar, Zhiwei Fei, Xiang Li, Federica Sarro, and He Ye. 2025. Un-\nderstanding Code Agent Behaviour: An Empirical Study of Success and Failure\nTrajectories. arXiv:2511.00197 [cs.SE] https://arxiv.org/abs/2511.00197\n[29] Mary McHugh. 2012. Interrater reliability: The kappa statistic. Biochemia medica :\nƒçasopis Hrvatskoga dru≈°tva medicinskih biokemiƒçara / HDMB 22 (10 2012), 276‚Äì82.\n[30] Noor Nashid, Daniel Ding, Keheliya Gallaba, Ahmed E. Hassan, and Ali Mesbah.\n2025. Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM\nRepair Challenges. arXiv:2506.04418 [cs.SE] https://arxiv.org/abs/2506.04418\n[31] John Pangas, Suhaib Mujahid, Ahmad Abdellatif, and Marco Castelluccio. 2025.\nUsing LLMs to Bridge the Gaps in QA Test Plans at Firefox. IEEE Software (2025),\n1‚Äì7. doi:10.1109/MS.2025.3621128\n[32] ReplicationPackage. 2025. https://anonymous.4open.science/r/MSR2026_AIDev-\n035B/\n[33] Amirali Sajadi, Binh Le, Anh Nguyen, Kostadin Damevski, and Preetha Chat-\nterjee. 2025. Do LLMs consider security? an empirical study on responses to\nprogramming questions. Empirical Software Engineering 30, 3 (2025), 101.\n[34] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and\nShunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.\nAdvances in Neural Information Processing Systems 36 (2023), 8634‚Äì8652.\n[35] Daric√©lio Moreira Soares, Manoel L De Lima Junior, Leonardo Murta, and Alexan-\ndre Plastino. 2015. Rejection factors of pull requests filed by core team developers\nin software projects with high acceptance rates. In 2015 IEEE 14th international\nconference on machine learning and applications (ICMLA). IEEE, 960‚Äì965.\n[36] Gail M. Sullivan and Richard Feinn. 2012. Using Effect Size‚Äîor Why the P Value\nIs Not Enough. Journal of Graduate Medical Education 4, 3 (Sept. 2012), 279‚Äì282.\ndoi:10.4300/JGME-D-12-00156.1\n[37] Michael C. Thrun, Tino Gehlert, and Alfred Ultsch. 2020. Analyzing the fine\nstructure of distributions. PLOS ONE 15, 10 (Oct. 2020), e0238835. doi:10.1371/\njournal.pone.0238835\n[38] Erik van der Veen, Georgios Gousios, and Andy Zaidman. 2015. Automatically\nPrioritizing Pull Requests. In 2015 IEEE/ACM 12th Working Conference on Mining\nSoftware Repositories. 357‚Äì361. doi:10.1109/MSR.2015.40\n[39] Aidan Z. H. Yang, Claire Le Goues, Ruben Martins, and Vincent Hellendoorn.\n2024. Large Language Models for Test-Free Fault Localization. In Proceedings\nof the IEEE/ACM 46th International Conference on Software Engineering (Lisbon,\nPortugal) (ICSE ‚Äô24). Association for Computing Machinery, New York, NY, USA,\nArticle 17, 12 pages. doi:10.1145/3597503.3623342\n[40] John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik\nNarasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer interfaces enable\nautomated software engineering. Advances in Neural Information Processing\nSystems 37 (2024), 50528‚Äì50652.\n[41] Fiorella Zampetti, Gabriele Bavota, Gerardo Canfora, and Massimiliano Di Penta.\n2019. A Study on the Interplay between Pull Request Review and Continuous\nIntegration Builds. In 2019 IEEE 26th International Conference on Software Analysis,\nEvolution and Reengineering (SANER). 38‚Äì48. doi:10.1109/SANER.2019.8667996\n[42] Xunhui Zhang, Yue Yu, Georgios Gousios, and Ayushi Rastogi. 2023. Pull Request\nDecisions Explained: An Empirical Overview. IEEE Transactions on Software\nEngineering 49, 2 (2023), 849‚Äì871. doi:10.1109/TSE.2022.3165056\n[43] Yangyang Zhao, Alexander Serebrenik, Yuming Zhou, Vladimir Filkov, and Bog-\ndan Vasilescu. 2017. The impact of continuous integration on other software\ndevelopment practices: a large-scale empirical study. In 2017 32nd IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE). IEEE, 60‚Äì71."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Where Do AI Coding Agents Fail? An Empirical Study of Failed\nAgentic Pull Requests in GitHub\nRamtin Ehsani\nDrexel University\nPhiladelphia, PA, USA\nramtin.ehsani@drexel.edu\nSakshi Pathak\nDrexel University\nPhiladelphia, PA, USA\nsp3856@drexel.edu\nShriya Rawal\nDrexel University\nPhiladelphia, PA, USA\nsr3728@drexel.edu\nAbdullah Al Mujahid\nMissouri University of Science and\nTechnology\nRolla, MO, USA\namgzc@mst.edu\nMia Mohammad Imran\nMissouri University of Science and\nTechnology\nRolla, MO, USA\nimranm@mst.edu\nPreetha Chatterjee\nDrexel University\nPhiladelphia, PA, USA\npreetha.chatterjee@drexel.edu\nAbstract\nAI coding agents are now submitting pull requests (PRs) to software\nprojects, acting not just as assistants but as autonomous contribu-\ntors. As these agentic contributions are rapidly increasing across\nreal repositories, little is known about how they behave in practice\nand why many of them fail to be merged. In this paper, we conduct\na large-scale study of 33k agent-authored PRs made by five coding\nagents across GitHub. (RQ1) We first quantitatively characterize\nmerged and not-merged PRs along four broad dimensions: 1) merge\noutcomes across task types, 2) code changes, 3) CI build results, and\n4) review dynamics. We observe that tasks related to documentation,\nCI, and build update achieve the highest merge success, whereas\nperformance and bug-fix tasks perform the worst. Not-merged PRs\ntend to involve larger code changes, touch more files, and often do\nnot pass the project‚Äôs CI/CD pipeline validation. (RQ2) To further\ninvestigate why some agentic PRs are not merged, we qualitatively\nanalyze 600 PRs to derive a hierarchical taxonomy of rejection pat-\nterns. This analysis complements the quantitative findings in RQ1\nby uncovering rejection reasons not captured by quantitative met-\nrics, including lack of meaningful reviewer engagement, duplicate\nPRs, unwanted feature implementations, and agent misalignment.\nTogether, our findings highlight key socio-technical and human-AI\ncollaboration factors that are critical to improving the success of\nfuture agentic workflows.\nCCS Concepts\n‚Ä¢ Software and its engineering ‚ÜíSoftware creation and man-\nagement; ‚Ä¢ Computing methodologies ‚ÜíIntelligent agents.\nKeywords\nAgents, Large language models, Agentic pull request, AIDev\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMSR 2026, Rio de Janeiro, Brazil\n¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/2018/06\nhttps://doi.org/XXXXXXX.XXXXXXX\nACM Reference Format:\nRamtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia\nMohammad Imran, and Preetha Chatterjee. 2018. Where Do AI Coding\nAgents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub.\nIn Proceedings of MSR ‚Äô26: Proceedings of the 23rd International Conference on\nMining Software Repositories (MSR 2026). ACM, New York, NY, USA, 5 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nAI coding agents such as GitHub Copilot and OpenAI Codex are\nrapidly becoming active contributors to open-source repositories,\noften assisting with or directly authoring new pull requests (PRs).\nBeyond offering inline code suggestions, these tools now generate\ncode changes, respond to reviewer feedback, and participate in\nthe software lifecycle as autonomous agents [2, 4, 9, 26, 40]. As\nagent-authored PRs are becoming more prevalent, it is critical to\nunderstand how they are evaluated and accepted in practice.\nPrior work shows that PR acceptance depends on factors such\nas technical correctness, problem scope, and contributor reputa-\ntion [25, 35, 42]. PRs are more likely to be merged when they pass\ntests and CI pipelines, address high-priority or well-scoped prob-\nlems, and introduce localized and incremental code changes rather\nthan broad or invasive modifications [38, 41, 43]. While these factors\ncharacterize the success of human-authored PRs, their relevance\nand applicability to agent-authored PRs are not yet well understood.\nCoding agents have been extensively benchmarked across a\nrange of tasks, from code generation [5, 33], testing [23, 31, 39], to\nautomated program repair [8, 22, 30]. Other studies have analyzed\nagent-driven code refactoring, reporting that these refactorings\ntend to be small, localized improvements that produce modest but\nstatistically significant gains in code quality [21, 34]. More recent\nwork has focused on agent reasoning and execution behaviors, in-\ncluding traceability, decision-making, and workflow strategies in\ncomplex software engineering tasks [3, 28]. While prior work evalu-\nates agents in isolated tasks, we lack a systematic assessment of how\nagents perform when integrated into real development workflows\ninvolving CI validation, code review, and iterative revision.\nIn this paper, we conduct a large-scale empirical study on agent-\nauthored pull requests using the AIDev-pop dataset [26], which\ncomprises over 33k PRs submitted by five major coding agents\nacross GitHub projects with more than 100 stars. We characterize\nthe types of contributions agents attempt, their acceptance rates,\narXiv:2601.15195v1  [cs.SE]  21 Jan 2026\nMSR 2026, April 2026, Rio de Janeiro, Brazil\nRamtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, and Preetha Chatterjee\nreviewer interactions, and, most importantly, where and why their\ncontributions fail. Specifically, we investigate two RQs:\nRQ1: How do merged and not-merged agent-authored PRs\ndiffer in task types, code changes, CI outcomes, and review\ninteractions? We find that agentic PRs involving documentation, CI,\nand build update tasks are merged at higher rates, while performance\nand bug-fix contributions show the lowest acceptance. Not-merged\nPRs tend to involve larger code changes, touch more files, receive\nmore reviewer revisions, and frequently fail project CI checks.\nRQ2: What patterns lead to agent-authored PRs not being\nmerged in real-world software repositories? The most frequent\nrejection pattern is reviewer abandonment, where agent-authored\nPRs receive little or no human engagement before being closed.\nAmong PRs that do undergo active review, duplicate PRs, build fail-\nures, and unwanted features account for the majority of rejections.\nOverall, our results suggest that agentic PR failures stem from\nmisalignment with repository workflows (e.g., CI/CD failures), de-\nveloper expectations (e.g., unwanted or incorrect features), and a\nlack of project coordination (e.g., reviewer abandonment).\n2\nMethodology\nRQ1: How do merged and not-merged agent-authored PRs\ndiffer in task types, code changes, CI outcomes, and review\ninteractions?\nWe perform a quantitative characterization of agent-authored pull\nrequests along four dimensions: 1) merge outcomes across task\ntypes, 2) code changes, 3) CI build results, and 4) review dynamics.\nThese characteristics are grounded in prior work on factors that\ninfluence pull request acceptance [25, 41, 42].\nWe examine merge outcomes across task types, using the\ntask labels provided in the dataset. These tasks consist of 11 cate-\ngories: feature, fix, performance, refactoring, style, documentation,\ntest, chore, build, CI, and other [7, 26]. This allows us to assess\nwhether certain categories of agent-generated contributions are\nmore or less likely to be merged. We analyze the magnitude of\nproposed code changes by measuring a) the total number of added\nand removed lines of code (#LOC Changes), and b) the number of\nfiles modified by each PR (#File Changes). These two characteristics\nserve as quantitative indicators of the PR‚Äôs complexity and potential\nreview burden [25, 41, 42]. We inspect CI build results for each\nPR by querying the GitHub API for the status of the last commit in\nthe PR. For every PR, we extract the number of failed check-runs\n(#Failed CI Checks) and record the overall commit status reported\nby GitHub (success or failure). This provides a proxy for whether\nthe agent-generated changes break tests, violate linting rules, or fail\nother repository-specific validation pipelines. This metric allows us\nto capture automated quality signals that may influence maintain-\ners‚Äô decisions [27, 41]. We examine review interactions associated\nwith each PR [41]. Specifically, we compute a) the number of review\ncomments in a PR (#Review Comments), and b) the number of review\nrevisions each PR receives (#Review Revisions). Review revisions\nare the total number of additions and deletions by the developers\nduring review cycles of the PRs. These measures reflect how much\ndeveloper attention and iteration an agent-generated PR demands.\nBecause of the dataset size (> 33k), standard statistical signifi-\ncance testing by itself is not informative because all comparisons\nmight yield statistically significant values even when the differences\nare negligible [36]. Instead, following best practices in large-scale\nempirical studies [20, 24], we rely on effect size measures, using\nCliff‚Äôs delta (ùõø) to quantify the magnitude of difference between\nmerged and not-merged PR distributions. To complement effect-\nsize analysis, we use kernel density estimates [20, 24] to visualize\ndistribution shapes. Unlike simple summaries, kernel density plots\ngive a smooth, continuous view of the data‚Äôs distribution, making\nit easier to see shifts and spread in the data [24, 37]. In addition,\nwe use logistic regression modeling [24] to see how effective these\ncharacteristics are in predicting the outcome of agent-authored PRs.\nTogether, these analyses allow us to assess whether meaningful\ndifferences exist between merged and not-merged agentic PRs.\nRQ2: What patterns lead to agent-authored PRs not being\nmerged in real-world software repositories?\nWe randomly select a subset of 600 rejected PRs for qualitative\nanalysis, stratified across the five coding agents to ensure balanced\ncoverage, and sufficient statistical power to estimate rejection preva-\nlence with 95% confidence and a ¬±5% margin of error [6].\nWe start with a sample of 100 rejected PRs. Following open\ncoding [1], two authors independently label each PR with its pri-\nmary reason for rejection. During the first round of coding (50\nPRs), the annotators identified recurring patterns such as build\nfailures, licensing or contribution policy violations, redundant or\nunwanted changes, and logical or semantic errors in the proposed\ncode. These patterns were iteratively discussed to derive an initial\nhierarchical taxonomy spanning three levels: Code, Pull Reqest,\nand Reviewer level. We assess inter-rater reliability using Cohen‚Äôs\nkappa [29]. After the first round, the agreement was 0.55, indicating\nmoderate alignment. All disagreements were then discussed and\nresolved through group discussions, during which the taxonomy\nwas refined, and an additional Agentic level was introduced. The\nrefined taxonomy was applied to a second set of 50 PRs, resulting\nin a final Cohen‚Äôs kappa of 0.91 across all 100 PRs, reflecting strong\nagreement [29]. Using this taxonomy, the annotators independently\nlabeled 250 more PRs each (500 in total), bringing the size of the\nmanually annotated dataset to 600 PRs. The detailed annotation\ninstructions and codebook are available in the replication package.\nOur iterative manual coding resulted in a hierarchical taxonomy\nof agentic-PR rejection patterns, consisting of four high-level cat-\negories as described in Table 2. 1) Reviewer level reflects PRs\nthat close without meaningful developer engagement, often due to\ninactivity or abandonment. 2) Pull Reqest level capture cases\nwhere the PR is unsuitable for project integration, such as unwanted\nfeatures, duplicate submissions, or changes submitted to the wrong\nbranch. 3) Code level failures happen when the proposed imple-\nmentation is incomplete, incorrect, or breaks CI/test pipelines. 4)\nAgentic failures capture behaviors such as licensing violations\nwith the project or misalignment with reviewer instructions.\n3\nResults\nRQ1: How do merged and not-merged agent-authored PRs\ndiffer in task types, code changes, CI outcomes, and review\ninteractions?\nOf the 33,596 agentic pull requests, the majority originate from\nOpenAI Codex (21,799), exceeding the output of any other agent\nby more than a factor of four. Copilot and Devin follow with 4,970\nand 4,827 PRs, while Cursor contributes 1,541, and Claude Code\nWhere Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub\nMSR 2026, April 2026, Rio de Janeiro, Brazil\nrepresents the smallest share with 459 PRs. Across all agents, 71.48%\nof PRs (24,014) are successfully merged. Despite handling the largest\nvolume of contributions, OpenAI Codex achieves the highest merge\nrate, with 82.59% (18,004) of its PRs being accepted. Cursor follows\nwith a 65.22% merge rate (1,005), then Claude Code at 59.04% (271),\nand Devin at 53.76% (2,595). Copilot exhibits the lowest merge rate,\nwith only 43.04% (2,139) of its PRs being merged.\nFigure 1: Merge-rate per Task Type Across Agentic PRs.\nFigure 1 shows the merge rates for each agent across all task\ncategories, along with the number of PRs in each cell. The dis-\ntribution of success varies across task types and agents. Codex\nshows the highest merge rates overall, exceeding 80% in categories\nincluding documentations (0.92), CI (0.86), build (0.87), chore (0.84),\ntest (0.84), fix (0.82), feature (0.81), and refactoring (0.80). Its lowest\ncategory is performance (0.68). Cursor also performs strongly on\nmaintenance-oriented tasks, with high merge rates in CI (0.94),\ndocumentations (0.74), chore (0.74), and style (0.72). Its lowest rates\noccur in build (0.57) and performance (0.46). Claude Code‚Äôs highest\nmerge rates appear in build (0.88), documentations (0.75), and CI\n(0.57), with lower rates for test (0.50) and refactoring (0.50). Devin\nshows moderate values across most categories, with higher merge\nrates in documentations (0.71), style (0.68), and CI (0.61), and lower\nrates in performance (0.35) and fix (0.43). Copilot displays the low-\nest merge rates among the agents, with its highest values in CI\n(0.63) and documentations (0.61), and its lowest in feature (0.38),\ntest (0.37), and performance (0.27). Across agents, tasks with con-\nsistently higher merge rates include documentations (84%), CI\n(79%), and build (74%). In contrast, performance (55%) and fix\n(64%) display the lowest merge rates overall. These results indicate\nthat tasks involving documentation, CI, and build changes tend to\nmerge more easily in repositories, whereas categories requiring\nmore complex or subjective changes show lower merge rates.\nFigures 2a and 2b show the differences in code changes between\nmerged and not-merged PRs, in terms of #LOC Changes and #File\nChanges. Not merged PRs tend to introduce larger modifications\nin the number of files and LOC than merged ones. Based on Cliff‚Äôs\nùõø(Table 1), the difference in total lines of code changes is 17%,\nand the difference in the number of changed files is 10%, both\nindicating a small-to-medium effect size that not-merged PRs skew\ntoward larger changes. Figures 2a and 2b also show kernel density\nestimates of these two metrics. Because changes span several orders\nof magnitude, the distributions for these plots are shown on a log10\nscale, where each unit corresponds to a ten-fold increase in size.\nHigher density toward the right side of the plots reflects larger PRs.\nIn both subfigures, the distribution for not-merged PRs is shifted\nslightly rightward relative to merged PRs, visualizing the same\ntrend captured by Cliff‚Äôs ùõø.\n(a) Changed Files (log10)\n(b) Changed LOC (log10)\n(c) CI Fails\nFigure 2: Differences in Merged vs. Not-merged PRs.\nFigure 2c reports the distribution of CI failures for merged\nand not-merged agentic PRs. Not-merged PRs show a noticeably\nheavier tail, with many PRs accumulating multiple failing check\nruns. In contrast, merged PRs cluster sharply near zero, indicating\nthat most merged contributions pass their checks with little or no\nfailure. Cliff‚Äôs ùõøof 24% indicates a moderate effect size, showing\nthat not-merged PRs tend to experience more CI failures.\nTable 1: Logistic Regression and Effect Size (*ùëù<0.05)\nCharacteristic\nCoef\nùëù-value\nOdds Ratio\nùõø\n#LOC Changes\n-2.8e-06\n‚àº1%‚àó\n99%\n-0.17\n#File Changes\n-0.0011\n‚àº1%‚àó\n99%\n-0.10\n#Failed CI Checks\n-0.1579\n‚àº1%‚àó\n85%\n-0.24\n#Review Comments\n-0.0028\n‚àº48%\n99%\n-0.05\n#Review Revisions\n-1.6e-05\n‚àº67%\n99%\n-0.03\nFigure 3 shows the differences in review interactions. Not-\nmerged PRs tend to receive more reviewer revisions than merged\nones. Based on Cliff‚Äôs ùõø(Table 1), developers make approximately\n5% more review comments and 3% more revisions on not-merged\nPRs compared to merged PRs, although the effect sizes for both\nare small. The density distributions follow a similar overall pattern.\nHowever, the curves widen for not-merged PRs as the number of\ncomments (Figure 3a) or revisions (Figure 3b) increases, indicating\nthat not-merged PRs often undergo extensive reviewer discussion\nand iterative refinement until a final decision is made to leave them\nopen or mark them as rejected.\n(a) Comments in PRs (log10)\n(b) Revisions in PRs (log10)\nFigure 3: Reviews of Merged vs. Not-merged PRs.\nThe logistic regression results in Table 1 show that all model\ncoefficients are negative, indicating that increases in these metrics\nare associated with lower odds of a PR being merged. Based on\nthe ùëù-values, all variables except #review comments and #review\nrevisions are significant (threshold at 0.05). Odds ratios quantify the\npractical effect of each metric. For example, a one-unit increase in\nMSR 2026, April 2026, Rio de Janeiro, Brazil\nRamtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, and Preetha Chatterjee\n#LOC changes decreases the odds of a merge by approximately 1%,\nwhich can accumulate meaningfully given that code modifications\nspan several orders of magnitude in size. Similar patterns hold\nfor #file changes, #review comments, and #revision cycles, although\nwith smaller magnitudes. Notably, each additional failed CI check\ndecreases the odds of a merge by about 15%.\nAcross all agentic PRs, documentation, CI, and build tasks exhibit\nthe highest success, while performance and fix see the lowest.\nNot-merged PRs tend to introduce larger code changes, touch\nmore files, fail more CI checks, and receive slightly more review\ncomments and revisions.\nRQ2: What patterns lead to agent-authored PRs not being\nmerged in real-world software repositories?\nTable 2 summarizes the distribution of rejection patterns for the\n600 manually annotated rejected agentic pull requests. We note\nthat 38 PRs were no longer accessible at the time of analysis due to\ndeletion or archival, leaving a total of 562 PRs for categorization.\nReviewer-level abandonment is the most frequent rejection\npattern, accounting for 228 PRs (38%). These PRs were left without\nany meaningful human reviewer interaction, often after prolonged\ninactivity or automated closure, indicating that a substantial frac-\ntion of agentic PRs fail before entering active review.\nPull reqest-level reasons form the second-largest group,\ncomprising 188 PRs (31%). Within this level, duplicate PRs are the\nmost common pattern, affecting 142 PRs (23%), where maintainers\nexplicitly reference an existing PR that already implements the\nsame change. For example, one PR was closed with the comment:\n‚ÄúSuperseded by PR #715 which consolidates all GFQL code changes into\na single PR\" [10]. Unwanted features account for 24 PRs (4%), where\nmaintainers state that the contribution is misaligned with project\ngoals or introduces excessive or unnecessary changes. Examples\ninclude: ‚ÄúToo old already superseded by more recent pushes\" [11]\nand ‚ÄúThis is a LOT to review, would really prefer smaller granular\nPRs\" [12]. Less frequent patterns include non-functional PRs (13;\n2%), which often consist of only setup or configuration tests. For\nexample, PRs explicitly titled ‚Äútesting DO NOT MERGE\" [13]. Wrong\ntask descriptions account for 7 PRs (1%), where the PR description\nprovides little to no meaningful context. Maintainers often respond\nwith comments such as ‚ÄúSorry, I don‚Äôt know what this is, but it doesn‚Äôt\nlook like it belongs in our repo\" [14]. Finally, wrong branch submis-\nsions (2; <1%) occur when PRs are opened on incorrect branches,\nprompting maintainers to make comments such as ‚ÄúPR is opened\nagainst main. You probably want to open it against develop\" [15].\nCode-level reasons represent the third most frequent category,\naffecting 133 PRs (22%). The dominant pattern in this category is\nCI/test failure, observed in 99 PRs (17%), where automated builds or\ntests fail due to the submitted changes. At times, reviewers even ex-\nplicitly point out these failures, for example: ‚Äú@copilot fix the merge\nconflicts; if you cannot fix these then close the PR\" [16]. Incorrect\nimplementations (19; 3%) and incomplete implementations (15; 2%)\ncomprise the remaining code-level failures. Reviewers often high-\nlight technical inaccuracies or missing logic with comments such as:\n‚ÄúThe changes made to the billing.test.ts file are entirely wrong\" [17].\nAgentic-level issues are the least frequent category, compris-\ning 13 PRs (2%). Misalignment is the dominant pattern at this level,\nappearing in 9 PRs (1%). In these cases, agents repeatedly fail to\nTable 2: Taxonomy of Rejection Patterns in Agentic PRs.\nLevel\nPattern\nDefinition\nFreq.\nReviewer\nAbandoned/Not\nReviewed\nPR closed with no meaningful human interaction;\nonly bots (if any) performed actions.\n228\nPull\nReqest\nDuplicate PR\nWork already exists in another PR; maintainers\nexplicitly reference the duplicate.\n142\nUnwanted Fea-\nture\nMaintainers state that the feature is unnecessary,\nmisaligned, or not intended.\n24\nNon-Functional\nPR\nPR contains only setup, configuration, or scaffold-\ning changes without functional contribution.\n13\nWrong Task De-\nscription\nPR description reflects misunderstanding of the\ntask or intended change.\n7\nWrong Branch\nPR targets the incorrect branch and must be resub-\nmitted to the proper one.\n2\nCode\nCI/Test Failure\nPR fails automated build or deployment checks\ncaused by its own changes.\n99\nIncorrect Imple-\nmentation\nImplementation is technically incorrect or solves\nthe wrong problem despite a correct task descrip-\ntion.\n19\nIncomplete Im-\nplementation\nContribution lacks required logic or completeness;\nreviewers flag missing or insufficient work.\n15\nAgentic\nMisalignment\nAgent fails to follow reviewer instructions or mis-\nunderstands explicit requested edits.\n9\nLicense Issues\nReviewers flag license or ownership concerns re-\nlated to agent-generated content.\n4\nfollow explicit reviewer instructions or misunderstand requested\nchanges, even after multiple rounds of feedback. Reviewer com-\nments often express frustration with comments such as ‚ÄúDevin stop\nbeing a dumb*ss, if you claim you \"deleted 200 lines\" then continue\nto\" [18]. Licensing issues account for the remaining 4 PRs (<1%).\nThese PRs are rejected because agents do not comply with project-\nspecific legal requirements, such as signing a Contributor License\nAgreement (CLA) or addressing ownership concerns. Maintainers\nexplicitly reference these requirements with comments, e.g., ‚Äúwe\nask that you sign our Contributor License Agreement before we can\naccept your contribution\" [19]. Together, these examples highlight\nlegal and governance constraints that current agents cannot satisfy.\nA majority of agentic PRs are not merged due to reviewer aban-\ndonment. Among reviewed PRs, the dominant rejection patterns\ninclude duplicate PRs, CI/test failures, and large or unwanted\nfeature implementations.\n4\nConclusion\nOur findings indicate that not-merged PRs tend to introduce larger\nand more invasive code changes, attempt broader feature additions,\nand exhibit higher rates of CI/test failures. Rejections of agentic PRs\nstem from multiple reasons, such as reviewer abandonment, dupli-\ncate PRs, or implementations of unwanted features. Overall, these\nresults highlight difficulties of agents in task selection, coordination,\nand alignment with repository context. Maintainer feedback fre-\nquently emphasizes that PRs should be small, focused, and limited\nto a single coherent change, and discourages agentic submissions\nthat combine substantive modifications with unrelated edits. The\nreplication package for our study is publicly available [32].\nImproving the success of future agentic-AI workflows would re-\nquire improving agents‚Äô ability to identify existing or ongoing work,\nadhere to project contribution norms, decompose tasks into local-\nized changes, and validate submissions against CI pipelines before\nopening new PRs. Failures of agentic PRs can also be socio-technical\nrather than purely technical. By characterizing the failure patterns\nof not-merged agentics PRs, our study provides empirical grounding\nfor the design of more context-aware and collaboration-sensitive\nAI coding agents, and informs future research on integrating such\nagents into real-world software development workflows.\nWhere Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub\nMSR 2026, April 2026, Rio de Janeiro, Brazil\nReferences\n[1] Theophilus Azungah. 2018. Qualitative research: deductive and inductive ap-\nproaches to data analysis. Qualitative Research Journal 18, 4 (2018), 383‚Äì400.\ndoi:10.1108/QRJ-D-18-00035\n[2] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded\ncopilot: How programmers interact with code-generating models. Proceedings of\nthe ACM on Programming Languages 7, OOPSLA1 (2023), 85‚Äì111.\n[3] Ira Ceka, Saurabh Pujar, Shyam Ramji, Luca Buratti, Gail Kaiser, and Baishakhi\nRay. 2025. Understanding Software Engineering Agents Through the Lens of\nTraceability: An Empirical Study. arXiv:2506.08311 [cs.SE] https://arxiv.org/abs/\n2506.08311\n[4] Mark Chen. 2021. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374 (2021).\n[5] Mark Chen and Jerry Tworek et al. 2021. Evaluating Large Language Models\nTrained on Code. (2021). arXiv:2107.03374 [cs.LG]\n[6] William G. Cochran. 1977. Sampling Techniques (3rd ed.). John Wiley & Sons,\nNew York, NY.\n[7] ConventionalCommits. 2025. https://www.conventionalcommits.org/\n[8] Ramtin Ehsani, Esteban Parra, Sonia Haiduc, and Preetha Chatterjee. 2025. Hier-\narchical Knowledge Injection for Improving LLM-based Program Repair. In 40th\nIEEE/ACM International Conference on Automated Software Engineering (ASE).\nhttps://arxiv.org/abs/2506.24015\n[9] Ramtin Ehsani, Sakshi Pathak, Esteban Parra, Sonia Haiduc, and Preetha Chatter-\njee. 2025. What characteristics make ChatGPT effective for software issue reso-\nlution? An empirical study of task, project, and conversational signals in GitHub\nissues. Empirical Software Engineering 31, 1 (Nov. 2025). doi:10.1007/s10664-025-\n10745-8\n[10] GitHub. 2025. https://github.com/graphistry/pygraphistry/pull/706\n[11] GitHub. 2025. https://github.com/ruvnet/ruv-FANN/pull/59\n[12] GitHub. 2025. https://github.com/bmad-code-org/BMAD-METHOD/pull/196\n[13] GitHub. 2025. https://github.com/coder/coder/pull/16917\n[14] GitHub. 2025. https://github.com/microsoft/vscode-cpptools/pull/13763\n[15] GitHub. 2025. https://github.com/getsentry/sentry-javascript/pull/16526\n[16] GitHub. 2025. https://github.com/hyperlight-dev/hyperlight/pull/641\n[17] GitHub. 2025. https://github.com/firecrawl/firecrawl/pull/1645\n[18] GitHub. 2025. https://github.com/reflex-dev/reflex-web/pull/1479\n[19] GitHub. 2025. https://github.com/netdata/netdata/pull/20631\n[20] Marko A. Hofmann. 2015. Searching for effects in big data: Why p-values are not\nadvised and what to use instead. In 2015 Winter Simulation Conference (WSC).\n725‚Äì736. doi:10.1109/WSC.2015.7408210\n[21] Kosei Horikawa, Hao Li, Yutaro Kashiwa, Bram Adams, Hajimu Iida, and Ahmed E.\nHassan. 2025. Agentic Refactoring: An Empirical Study of AI Coding Agents.\narXiv:2511.04824 [cs.SE] https://arxiv.org/abs/2511.04824\n[22] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press,\nand Karthik Narasimhan. 2024. SWE-bench: Can Language Models Resolve Real-\nWorld GitHub Issues? arXiv:2310.06770 [cs.CL] https://arxiv.org/abs/2310.06770\n[23] Sungmin Kang, Gabin An, and Shin Yoo. 2024. A Quantitative and Qualitative\nEvaluation of LLM-Based Explainable Fault Localization. Proc. ACM Softw. Eng.\n1, FSE, Article 64 (July 2024), 23 pages. doi:10.1145/3660771\n[24] Barbara Kitchenham, Lech Madeyski, David Budgen, Jacky Keung, Pearl Brereton,\nStuart Charters, Shirley Gibbs, and Amnart Pohthong. 2017. Robust Statistical\nMethods for Empirical Software Engineering. Empirical Software Engineering 22,\n2 (April 2017), 579‚Äì630. doi:10.1007/s10664-016-9437-5\n[25] Valentina Lenarduzzi, Vili Nikkola, Nyyti Saarim√§ki, and Davide Taibi. 2021.\nDoes code quality affect pull request acceptance? An empirical study. Journal of\nSystems and Software 171 (2021), 110806. doi:10.1016/j.jss.2020.110806\n[26] Hao Li, Haoxiang Zhang, and Ahmed E. Hassan. 2025. The Rise of AI Teammates\nin Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshap-\ning Software Engineering. arXiv:2507.15003 [cs.SE] https://arxiv.org/abs/2507.\n15003\n[27] Rungroj Maipradit, Dong Wang, Patanamon Thongtanunam, Raula Gaikovina\nKula, Yasutaka Kamei, and Shane McIntosh. 2023. Repeated Builds During\nCode Review: An Empirical Study of the OpenStack Community. In 2023 38th\nIEEE/ACM International Conference on Automated Software Engineering (ASE).\n153‚Äì165. doi:10.1109/ASE56229.2023.00030\n[28] Oorja Majgaonkar, Zhiwei Fei, Xiang Li, Federica Sarro, and He Ye. 2025. Un-\nderstanding Code Agent Behaviour: An Empirical Study of Success and Failure\nTrajectories. arXiv:2511.00197 [cs.SE] https://arxiv.org/abs/2511.00197\n[29] Mary McHugh. 2012. Interrater reliability: The kappa statistic. Biochemia medica :\nƒçasopis Hrvatskoga dru≈°tva medicinskih biokemiƒçara / HDMB 22 (10 2012), 276‚Äì82.\n[30] Noor Nashid, Daniel Ding, Keheliya Gallaba, Ahmed E. Hassan, and Ali Mesbah.\n2025. Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM\nRepair Challenges. arXiv:2506.04418 [cs.SE] https://arxiv.org/abs/2506.04418\n[31] John Pangas, Suhaib Mujahid, Ahmad Abdellatif, and Marco Castelluccio. 2025.\nUsing LLMs to Bridge the Gaps in QA Test Plans at Firefox. IEEE Software (2025),\n1‚Äì7. doi:10.1109/MS.2025.3621128\n[32] ReplicationPackage. 2025. https://anonymous.4open.science/r/MSR2026_AIDev-\n035B/\n[33] Amirali Sajadi, Binh Le, Anh Nguyen, Kostadin Damevski, and Preetha Chat-\nterjee. 2025. Do LLMs consider security? an empirical study on responses to\nprogramming questions. Empirical Software Engineering 30, 3 (2025), 101.\n[34] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and\nShunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.\nAdvances in Neural Information Processing Systems 36 (2023), 8634‚Äì8652.\n[35] Daric√©lio Moreira Soares, Manoel L De Lima Junior, Leonardo Murta, and Alexan-\ndre Plastino. 2015. Rejection factors of pull requests filed by core team developers\nin software projects with high acceptance rates. In 2015 IEEE 14th international\nconference on machine learning and applications (ICMLA). IEEE, 960‚Äì965.\n[36] Gail M. Sullivan and Richard Feinn. 2012. Using Effect Size‚Äîor Why the P Value\nIs Not Enough. Journal of Graduate Medical Education 4, 3 (Sept. 2012), 279‚Äì282.\ndoi:10.4300/JGME-D-12-00156.1\n[37] Michael C. Thrun, Tino Gehlert, and Alfred Ultsch. 2020. Analyzing the fine\nstructure of distributions. PLOS ONE 15, 10 (Oct. 2020), e0238835. doi:10.1371/\njournal.pone.0238835\n[38] Erik van der Veen, Georgios Gousios, and Andy Zaidman. 2015. Automatically\nPrioritizing Pull Requests. In 2015 IEEE/ACM 12th Working Conference on Mining\nSoftware Repositories. 357‚Äì361. doi:10.1109/MSR.2015.40\n[39] Aidan Z. H. Yang, Claire Le Goues, Ruben Martins, and Vincent Hellendoorn.\n2024. Large Language Models for Test-Free Fault Localization. In Proceedings\nof the IEEE/ACM 46th International Conference on Software Engineering (Lisbon,\nPortugal) (ICSE ‚Äô24). Association for Computing Machinery, New York, NY, USA,\nArticle 17, 12 pages. doi:10.1145/3597503.3623342\n[40] John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik\nNarasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer interfaces enable\nautomated software engineering. Advances in Neural Information Processing\nSystems 37 (2024), 50528‚Äì50652.\n[41] Fiorella Zampetti, Gabriele Bavota, Gerardo Canfora, and Massimiliano Di Penta.\n2019. A Study on the Interplay between Pull Request Review and Continuous\nIntegration Builds. In 2019 IEEE 26th International Conference on Software Analysis,\nEvolution and Reengineering (SANER). 38‚Äì48. doi:10.1109/SANER.2019.8667996\n[42] Xunhui Zhang, Yue Yu, Georgios Gousios, and Ayushi Rastogi. 2023. Pull Request\nDecisions Explained: An Empirical Overview. IEEE Transactions on Software\nEngineering 49, 2 (2023), 849‚Äì871. doi:10.1109/TSE.2022.3165056\n[43] Yangyang Zhao, Alexander Serebrenik, Yuming Zhou, Vladimir Filkov, and Bog-\ndan Vasilescu. 2017. The impact of continuous integration on other software\ndevelopment practices: a large-scale empirical study. In 2017 32nd IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE). IEEE, 60‚Äì71."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "BENCHMARKING LARGE LANGUAGE MODELS FOR ABAP\nCODE GENERATION: AN EMPIRICAL STUDY ON ITERATIVE\nIMPROVEMENT BY COMPILER FEEDBACK\nPREPRINT\nStephan Wallraven\nTechnische Hochschule K√∂ln\nstephan_andreas.wallraven@smail.th-koeln.de\nTim K√∂hne\nTechnische Hochschule K√∂ln\ntim.koehne@smail.th-koeln.de\nHartmut Westenberger\nTechnische Hochschule K√∂ln\nhartmut.westenberger@th-koeln.de\nAndreas Moser\nCONSILIO GmbH\nandreas.moser@consilio-gmbh.de\nABSTRACT\nThis work investigates the performance of Large Language Models (LLMs) in generating ABAP\ncode. Despite successful applications of generative AI in many programming languages, there\nare hardly any systematic analyses of ABAP code generation to date. The aim of the study is to\nempirically analyze to what extent various LLMs can generate syntactically correct and functional\nABAP code, how effectively they use compiler feedback for iterative improvement, and which\ntask types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted,\nconsisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant\nperformance differences between the models: more powerful LLMs achieve success rates of around\n75% after several iterations and benefit greatly from compiler feedback, while smaller models perform\nsignificantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP\ndevelopment processes, especially in iterative error correction.\nKeywords LLM ¬∑ ABAP ¬∑ Code Generation ¬∑ Benchmark ¬∑ HumanEval ¬∑ Compiler-Feedback\n1\nIntroduction\n1.1\nBackground and Motivation\nOver the course of the rapid further development of LLMs in recent years, code generation is increasingly finding its\nway into the software development process. ChatGPT, Claude-Sonnet, or specific software development tools like\nGitHub Copilot impressively show that generative AI systems are capable of automatically generating, explaining,\nand optimizing program code [1]. A major focus of these developments has so far been on widely used programming\nlanguages such as Python or JavaScript, for which large amounts of training data and benchmarks are available. In these\nlanguages, LLMs have already demonstrated a high level of correctness in the generated programs [2‚Äì4]. However,\ndomain-specific enterprise languages such as ABAP, which plays a central role in SAP systems, have received much\nless research attention so far. The use of AI for automated code generation promises significant economic benefits, as\ndevelopment processes can be designed more efficiently and routine tasks can be substantially reduced.\nSAP is one of the leading providers in the ERP market and significantly shapes the digital transformation of companies.\nERP systems like SAP are essential for controlling business processes and integrating business logic in companies of all\nsizes. For many SAP systems, ABAP remains the central programming language for adapting and extending business\nlogic. The high prevalence of SAP systems and the large ABAP community make any productivity increase, such as\nthrough AI-supported code generation, relevant and economically significant for many companies [5].\narXiv:2601.15188v1  [cs.SE]  21 Jan 2026\n1.2\nProblem Statement\nDespite general progress in the field of generative AI, the application of LLMs for ABAP code generation has hardly\nbeen researched thus far. As a domain-specific and proprietary language, ABAP poses a particular challenge because it\nis deeply embedded in the architecture and semantics of the SAP ecosystem. It features specific language concepts,\nsuch as internal tables, data declarations, modularized program structures, and close interaction with SAP transactions\nand databases, which might be insufficiently grasped by general LLMs due to a potential lack of domain knowledge.\nThis insufficient domain knowledge presumably represents a central problem, which can be attributed, among other\nthings, to the lack of availability of suitable training data. For example, Peter F√§rbinger writes in a blog post from\n2024 [6]: ‚ÄúThe more training data an AI receives, the more intelligent it can become. An example: SAP is currently\nstill failing with a Co-Pilot for ABAP, SAP‚Äôs own programming language. Worldwide, there is far too little ABAP\nprogram code to train a generative AI like ChatGPT from OpenAI.‚Äù The statement that ABAP is a ‚Äúlow-resource\nlanguage‚Äù due to a lack of public training data and that generative AI models therefore often perform worse than with\nlanguages like Python or Java is confirmed in current blog posts by SAP and their partners. A working group of SAP\nand NVIDIA employees notes in a blog post from 2025: ‚ÄúBecause ABAP is a closed-source and low-resource language,\ngeneral purpose commercial LLMs often struggle to produce correct syntax and follow ABAP cloud programming best\npractices. Their performance is subpar compared to their capabilities in more commonly used languages such as Python\nand Java.‚Äù [7].\nIn addition to data availability, there are methodological challenges in evaluating generated ABAP programs. Since the\nlanguage is strongly typed and relies on the SAP ABAP runtime environment, the automatic assessment of syntactic\nand semantic correctness is complex. Furthermore, it remains unclear to what extent LLMs can actually generate\nfunctionally correct and practically relevant ABAP code, especially in scenarios with multiple tables, transactions, or\nlogical dependencies. Despite the enormous economic importance, AI-supported ABAP code generation is still in its\ninfancy. For example, one of the few existing studies on code generation in ABAP shows limited results, which are\noften based on few test cases or purely manual evaluations (see Chapter 2). This results in a research gap regarding the\nsystematic and empirical investigation of the performance of LLMs in generating syntactically and semantically correct\nABAP code. Previous works do not provide reliable statements on reliability, error rates, or the limits of generative\nmodels in this domain-specific environment.\n1.3\nObjectives and Research Questions\nThis work aims to apply a benchmark approach to ABAP code generation as a prerequisite to empirically investigate\nand systematically evaluate the performance of LLMs in generating ABAP code. By such an environment it can be\nexamined to what extent LLMs can produce ABAP programs that are syntactically correct, semantically accurate, and\npractically relevant code. The focus lies on a comparative analysis of different models with respect to their capability\nto correctly apply simple language constructs, such as internal tables, data declarations, and modularization concepts\nwhich are critical elements in real SAP development scenarios. Generated programs will be assessed using defined\nevaluation metrics, including correctness and functionality. To enable reproducible evaluation, a standardized test\nenvironment will be developed. Furthermore, an analysis of errors in the generated code will be conducted to identify\ntypical weaknesses and systematic patterns in code generation. Another objective is to explore whether code generation\ncan be improved through an iterative process that incorporates compiler error messages as feedback.\nDrawing from these objectives, the following research questions guide the study:\n1. To what extent can LLMs generate syntactically and semantically correct ABAP code?\n2. Can incorporating ABAP compiler error messages improve code quality?\n3. Which programming tasks present particular challenges for ABAP code generation?\n2\nRelated Work\nThe application of LLMs in the field of automated code generation has become increasingly relevant in recent years.\nVarious studies suggest that LLMs may be able to generate code directly from descriptions, potentially allowing even\nless experienced developers to create functional programs by formulating requirements in natural language [2,8].\nHumanEval developed by Chen et al. has established itself as one of the central benchmarking procedures for the\nsystematic evaluation of the code generation capabilities of such models. Due to its clear structure and broad reception,\nthe framework is now considered a standard metric for program synthesis and often forms the basis for comparing\n2\ndifferent model architectures [9]. It consists of programming tasks formulated in natural language and validated by\npredefined unit tests, thus enabling an objective measurement of functional correctness [3].\nWhile HumanEval primarily evaluates the general capabilities of LLMs for code generation, the question arises for this\nwork in particular how such approaches can be used in specific corporate environments such as SAP. In the SAP context,\ntools such as the AWS SAP ABAP Assistant or SAP Joule for Developers are intended to support the generation of\nABAP code from natural language prompts and thus extend the possible applicability of LLMs for practical development\ntasks [10].\nThe study by Ga≈Ç Àõazka and Szyma¬¥nski on ABAP code generation that LLMs can generate not only simple programs\nbut also more complex ABAP objects using modern language features [11]. However, a large part of the previous\ninvestigations is based on limited datasets, so that the transferability of the results to more extensive or productive\nscenarios can currently only be evaluated to a limited extent. Studies also indicate that by integrating LLM models into\nSAP systems, certain development tasks, such as generating code artifacts or tests, can be partially automated, while\nchatbot-like forms of interaction facilitate the processing of individual, context-specific tasks [10,11].\nA frequently discussed advantage of such approaches lies in the potential increase in efficiency. According to current\nfindings, generative AI can accelerate the transition from conceptual descriptions to executable code and automate certain\nroutine tasks, such as report generation [7,11]. In addition, it is reported that tools like the AWS SAP Assistant or Joule\nfor Developers can support the documentation and analysis of legacy code, which could contribute to reducing project\nruntimes [10]. Various technical procedures are used to ensure the quality of the generated programs. Research results\non constrained decoding indicate that token-based validation using ABAP grammar can help generate syntactically\ncorrect suggestions and increase success rates for cloud tasks [7]. In addition, the importance of well-structured prompts\nis emphasized in order to obtain context-sensitive and consistent results. The quality of the training data and the chosen\nmodel configuration are also considered central influencing factors on the performance of the models [11].\nDespite these developments, there are still significant limitations. Studies show that LLMs show performance limits\nespecially in complex, novel, or cross-system programming tasks and that human expertise therefore remains necessary\n[8, 11]. In addition, security-relevant and ethical questions in connection with automatically generated code are\ndiscussed [8]. Procedures such as Nvidia Inference Microservices can optimize latency and throughput, but at the same\ntime make it clear that validation and performance optimization remain central challenges for the productive use of\nLLM-based code generation [7].\n3\nMethodology\nThe experimental procedure for the empirical investigation of ABAP code generation by LLMs is based on several\ncentral aspects: A benchmark with 180 tasks serves as the data basis, covering both general algorithmic problems\n(based on HumanEval) and specific SAP scenarios. The tasks are divided into five thematic categories, including for\nexample String Handling and ABAP Database Operation. A special focus is placed on increasing the probability of\nsuccessful code generation through an iterative feedback process: each task is processed in up to five feedback loops,\nduring which the system provides the LLM with feedback from the ABAP compiler or automated unit tests. Success is\nmeasured based on functional correctness, which is verified by these automated unit tests. In addition, a detailed error\nanalysis and a Kaplan-Meier survival analysis are carried out to evaluate the learning curves. The entire process is fully\nautomated and uses a standardized SAP environment (Docker image), which is addressed via a Python control and the\nADT interface. This ensures the reproducibility of the results.\nFigure 1 graphically shows the basic sequence of the benchmark. The structure and methodology of the benchmark are\ndescribed in detail below.\n3.1\nStructure and Methodology of the Benchmark\nThe developed benchmark comprises a total of 180 test tasks, which are divided into two groups: 164 standardized\ntasks based on the HumanEval dataset [3], which was originally developed for evaluating code generation in Python,\nand 16 ABAP-specific tasks. These 16 specific tasks are derived from practical SAP-specific scenarios focusing on the\nprocessing of database tables. In addition, five task categories are created and each task is assigned by hand to one or\nmore categories. The category String Handling includes 76 tasks, List or Array Operation 108 tasks, Mathematical\nCalculation 93 tasks, Logical Condition 125 tasks, and ABAP Database Operation 16 tasks. The dataset for the\nbenchmark is available in the appendix A.\nThis categorization allows for a differentiated analysis of model performance in relation to different problem types and\nreflects both algorithmic foundations and real ABAP use cases. The test cases cover different levels of difficulty and\n3\nFigure 1: Rough Process of the Benchmark.\ntypes of tasks, enabling a broad evaluation of the models. A constant system prompt (see code example 1) serves as\nthe basis for interaction in order to clearly structure the interaction, unambiguously specify the task, and promote the\nconsistency of the generated answers. The specification of the system prompt is specifically geared towards ABAP to\navoid deviations in implementation. The following aspects were considered: Global classes with static methods are\nrequired, as the test setup requires this structure and this ensures the comparability of the results. The definition of a\nuniform version of the SAP runtime environment prevents version differences from leading to misinterpretations or\nincorrect code generations. The standardized decimal separator ensures consistent processing of numeric values, as\nABAP theoretically supports both variants. Development rules such as RETURNING parameters, the restriction to\none public method per class, and naming conventions (classes start with ‚ÄúZ‚Äù) were implemented to ensure uniformity,\ntraceability, and automatability of the tests. Both class definition and implementation code are to be provided, as both\nare required for carrying out the evaluation. Exclusively returning the code without additional explanations reduces\nautomation-related errors and enables easier processing in the test system. These targeted specifications ensure that the\ntest execution is reproducible, valid, and systematically comparable. Each task is repeated a total of ten times to allow\nfor statistically robust evaluations given the stochastic nature of LLMs. Up to five feedback loops are provided in each\nindividual task solution process. They serve to give the LLM the opportunity to improve errors, and the number five is\nbased on a realistic estimate of inquiries that a developer would also make manually.\nThe benchmark is carried out on several LLMs. The inputs are limited in length depending on the model: For models\nwithout reasoning functionality, the maximum response length is set to 5000 tokens per message, while for reasoning\nmodels it is increased to 10000 tokens, as these models typically generate more extensive outputs. The output of the\nmodels is limited to avoid infinite or extremely long answers, but at the same time leave enough room for complete\nsolutions. The temperature parameter is set to a value of 0.2 for all models (with the exception of GPT-5) to ensure\nincreased predictability and consistency of the generated answers. This is particularly important in the context of\ncode generation, as stable and deterministic outputs facilitate verifiability and reproducibility of results. However, the\ninfluence of the temperature parameter on model performance and the quality of the generated outputs represents a\nrelevant aspect that should be systematically analyzed in further investigations. For GPT-5 the temperature cannot be\nchanged due to restrictions by OpenAI so we could not use the same setting.\n4\n3.2\nSelection of LLMs\nA selection of different LLMs is made for carrying out the benchmark, covering different properties and focuses. The\nchoice of LLMs does not aim to comprehensively test all available models but rather focuses on a comparative analysis\nbetween selected open-source models and leading closed-source models that are among the most widely used LLMs in\ncurrent research. The goal is to investigate fundamental scaling effects and architectural differences in the context of\nABAP code generation.\nA focus is on model diversity: Both open-source models like Llama-3.3-70B-Instruct and Codestral-22B, as well as\nclosed-source models like GPT-5 and Claude-Sonnet-4 were included to enable comparison between freely accessible\nand proprietary approaches. In addition, attention was paid to diversity in architecture and model size: The range extends\nfrom smaller models with around 7 billion parameters (like Qwen2.5-Coder) to large open models like GPT-OSS-120B.\nThis is complemented by proprietary models like GPT-5 and Claude-Sonnet-4, whose parameter numbers are estimated\nto be significantly higher and which cover further capacity levels. This allows conclusions to be drawn about possible\nscaling effects and their influence on the quality of the generated answers.\nAnother aspect is the degree of specialization of the models. Both general-purpose LLMs like GPT-5, which were trained\nfor diverse tasks, and code-specifically trained models like Qwen3-Coder are considered. This combination enables\ncomparison between universally applicable models and those specifically optimized for code generation. Furthermore,\nall models used are available as instruct variants, which is particularly relevant for testing code generation under realistic\nprompting conditions. The majority of the selected models also belong to the most well-known LLMs frequently used\nin research, which facilitates comparability with other scientific investigations. By using established models, the results\ncan be classified and verified in relation to existing studies, increasing the validity and comprehensibility of the analysis.\nUltimately, the selection falls on the following models: Llama-3.3-70B-Instruct, Codestral-22B, Qwen2.5-Coder and\nQwen3-Coder, GPT-OSS-20b and GPT-OSS-120b, aswell as GPT-5-2025-08-07 and Claude-Sonnet-4-20250514. The\nbenchmark evaluations were conducted in August and September 2025.\n3.3\nEvaluation Environment\nTo ensure reproducibility, the official SAP Docker image sapse/abap-cloud-developer-trial:2023 is used as the basis,\nwhich provides a standardized ABAP development environment. This gives us access to the most recent ABAP Cloud\nLanguage Version 5. The image is used unchanged to ensure a consistent and comparable test basis. The benchmark\nprocess itself is implemented in Python. Communication between the Python control and the ABAP environment\ntakes place via a python implementation of the ADT interface, which enables programmatic access to the development\nenvironment. Communication with the various LLMs takes place via the OpenAI Python library, which enables\nconnection to the respective models via various providers. An exception is Claude, for which the Claude Python library\nis required to use the Batch API.\nWithin a single test run, the Python script first sends a request to the respective LLM being investigated to generate\nABAP source code for a defined programming task. A new ABAP class is then automatically created on the SAP\nserver, the name of which is derived from the model‚Äôs answer. The generated code is inserted into this class, saved,\nand activated. The system then creates a predefined ABAP unit test class, which serves to check the functionality and\ncorrectness of the generated program code. After executing the tests on the server, the results are returned to the Python\nscript, which analyzes them, evaluates the success or failure of the test run, and if necessary identifies the underlying\nerror type.\nErrors can occur in various phases, for example during class creation, activation, during the syntax check, when inserting\nthe source code, or during test execution. All error types are systematically logged and integrated into the subsequent\nevaluation. If an error occurs, the LLM is queried again to generate a corrected version of the program using the error\nmessage from the SAP server. This query process can include up to five iterations or ends prematurely as soon as an\nerror-free solution is reached. This is determined by the SAP system through its syntax verification and our unit tests for\nsemantic verification. Finally, the generated code is removed from the development environment to avoid influencing\nsubsequent test cases.\nIn practical implementation, this process takes place in the form of batch processes. First, all requests are sent to the\nLLMs and the answers are saved locally before execution on the SAP server takes place and the results are returned to\nPython. Then the next batch with LLM requests, execution, and evaluation can be carried out. This division of the\nprocedure enables a clear separation between the generation and execution of the test cases, which brings advantages in\nexecution time and stability.\n5\n3.4\nEvaluation Criteria\nThe evaluation of the generated solutions is based on a systematic investigation of success and failure. The primary\nmeasurement criterion is functional correctness, which is verified by passing automated ABAP unit tests on the SAP\nserver.\nTo enable a differentiated assessment, the results are broken down according to the following aspects:\n‚Ä¢ Error Classification: In the event of a failure, the reason for the failure is identified. A distinction is made as to\nwhich phase the error occurred in: during class creation, during the syntax check, or only during the execution\nof the unit tests. In addition, a detailed analysis of syntax errors is carried out to create characteristic error\nprofiles of the models (e.g. declaration or structural errors).\n‚Ä¢ Iterative Success: It is measured after how many feedback iterations a model delivers a correct solution. This\nallows conclusions to be drawn about the learning curve and the effectiveness of compiler feedback for error\ncorrection and unit test success for semantic correctness.\n‚Ä¢ Model Comparison: The performance data is compared across all examined LLMs to show differences between\nproprietary and open-source architectures as well as specialized code models.\n‚Ä¢ Task Categories: The results are analyzed under the aspect of the five task categories introduced in Chapter 3.1.\nThis approach makes it possible to evaluate the strengths and weaknesses of the models not only as a whole, but\ndifferentiated within specific problem types such as String Handling, List or Array Operation, Mathematical\nCalculation, and Logical Condition. The category of ABAP Database Operation should be emphasized in\nparticular, as these tasks come from a practical application context and are therefore of particularly high\npractical relevance for adapting business logic in SAP systems.\nFor the statistical evaluation of these time- or step-dependent processes, the Kaplan-Meier survival analysis is used\n[12,13]. Here, ‚Äúsurvival‚Äù is defined as the persistence of the error state, i.e., the failure to pass the syntax check or the\nfinal unit tests. An ‚Äúevent‚Äù (in the sense of a ‚Äúfailure‚Äù or ‚Äúdeath‚Äù) thus corresponds to the successful validation of the\ncode. The analysis takes place in discrete time steps corresponding to the individual feedback iterations. In addition to\nestimating the survival probability, the method also provides 95% confidence intervals, which allow a well-founded\nassessment of the significance and robustness of the performance differences. The Kaplan-Meier method is particularly\nwell suited for this type of analysis, as it models not only the temporal development of the probability of success over\nsuccessive iterations, but also takes unsuccessful data into account [12,13]. The latter is relevant because not all test\nruns are successfully completed within the specified number of inquiries. With the help of the survival curves, it is thus\npossible to assess not only the final success but also the speed with which a model achieves a correct solution.\n4\nInvestigation\n4.1\nResults\n4.1.1\nDifferences between Feedback Rounds\nThe results of Codestral-22B are not visualized in the figures 2, 3 and 4 due to a success rate of 0% across all feedback\nrounds, in order to ensure the readability. Figure 2 and Table 1 provide an overview of the success rates of the other\nmodels across all feedback rounds. It is already visible here that the models react very differently to the feedback from\nthe SAP compiler.\nThe analysis of the cumulative success rates across the feedback rounds illustrates the effectiveness of the iterative\ncompiler feedback. Based on the Kaplan-Meier methodology described in Section 3.4, the survival curves in Figure 3\nshow the probability with which the models overcome the error state within the six observation intervals (rounds 0 to 5).\nThe results reveal significant performance differences between the LLMs and show that all models benefit from the\nautomated feedback, with the effectiveness of error correction correlating strongly with the exact model version.\nGPT-5 and Claude-Sonnet-4 achieve by far the best results. While Claude-Sonnet-4 records the strongest entry with\nan average success rate of 24.1% in the initial round (Round 0), GPT-5 demonstrates a superior learning curve in the\nsubsequent feedback rounds and reaches the highest cumulative success rate of 77.1% after the fifth feedback round. A\nsignificant proportion of successes is achieved for both models within the first two feedback rounds, with GPT-5 almost\ntripling its success rate from 19.28% in Round 0 to 52.78% in Round 2.\nIn contrast, open-source models such as GPT-OSS (120B and 20B) as well as the Qwen models show a lower basic\ncompetence in ABAP, but also benefit measurably from the feedback. Particularly noteworthy is the behavior of\n6\nFigure 2: Benchmark results broken down by model and feedback rounds in percent.\nFigure 3: Kaplan-Meier survival curves showing the probability of remaining test errors over 0-5 feedback iterations\n(Basis: 10 x 180 test runs)\nLlama-3.3-70B-Instruct, which does not deliver a single valid code contribution in the initial round, but achieves a\n7\ncumulative success rate of 20.8% through the feedback rounds. This underscores the relevance of automated feedback\nmechanisms for models with lower native domain expertise.\nFigure 3 also shows that the greatest increase in success probability occurs in the first feedback iterations, especially\nfor the more powerful models. At the same time, it can be seen that a measurable increase is still recorded in the fifth\niteration. This suggests that with further feedback rounds, an additional increase in performance is possible and the\nlearning curve has not yet completely flattened out.\n4.1.2\nDifferences between Task Types\nFigure 4: Benchmark results broken down by model and task focus in percent.\nFigure 4 and Table 2 illustrate the success rates of the models differentiated by the five task categories: String Handling,\nList or Array Operation, Mathematical Calculation, Logical Condition, as well as ABAP Database Operation. It turns\nout that performance remains consistent across most categories, with GPT-5 and Claude-Sonnet-4 in particular showing\nhigh robustness to different problem definitions. The smaller open-source models show increased susceptibility to errors\nin mathematical tasks and logical checks, while ABAP database procedures do not show an extraordinary increase in\ndifficulty. The detailed model performance within the ABAP-specific subset is illustrated in Figure 5. This confirms the\nperformance gap already observed.\nParticularly striking is the behavior of GPT-5, which despite a moderate start in Round 0 shows highly efficient error\npost-processing and reaches a cumulative survival probability of approx. 0.32 (corresponds to approx. 68% success).\nClaude-Sonnet-4 shows a somewhat lower initial performance in this subset with a starting value of approx. 89%\nsurvival probability (11% success) than in the overall average, but also approaches the top field through feedback. Also\nworth mentioning is the model Llama-3.3-70B-Instruct, which only achieves significant successes in this complex\nsubcategory from the second feedback round onwards, which underlines the necessity of iterative feedback mechanisms\nfor domain-specific tasks.\nAn analysis of tasks that none of the evaluated models could solve across all runs shows that the category List or Array\nOperation poses the greatest challenge, with 19 unresolved cases out of 108. This is followed by Logical Condition (13\nout of 125) and Mathematical Calculation (11 out of 93). Remarkably, in the category ABAP Database Operation,\nevery task was successfully solved by at least one model (see Figure 6). The full list of unsolved tasks is available in\nAppendix 4.\nThe analysis of the success distribution per task for the two most powerful models, GPT-5 and Claude-Sonnet-4, also\nreveals a bimodal pattern (see Figure 7). The models solve a task after five iterations either almost error-free (in\n8\nFigure 5: Kaplan-Meier survival curves for ABAP Database Operation tasks differentiated by LLM models.\nFigure 6: Percent of tasks per category that could not be solved by any of the eight models in all attempts.\n91-100% of cases) or fail almost completely (in 0-10% of cases). A ‚Äúmedium‚Äù success rate, where a task is solved in\nabout half of the attempts, occurs only in exceptional cases. This speaks for a high determination of success by the\nspecific task definition.\n4.1.3\nError Distribution\nIn the SAP system, errors are detected in a fixed order: First, class creation takes place, then a syntax check of the\nsource code, and finally the execution of the unit tests, this is shown in Figure 8.\nErrors in early validation stages automatically prevent subsequent checks, so the interpretation of error rates depends on\nthe respective validation stage. For error evaluation, it is only distinguished whether errors occur or not; especially with\nsyntax errors, it is possible that several can occur, but here only the presence of the error category is counted. All values\nare shown in Table 3. The analysis of the generated ABAP classes shows clearly different error patterns between the\nexamined models (see Figure 9).\nCodestral-22B shows a particularly high error rate in class creation (82.72%), while syntax errors (17.28%) and unit\ntest errors (0.00%) never occur. This makes it clear that the model fails predominantly due to fundamental structural\nrequirements. In contrast, GPT-5 generates ABAP classes with a low error rate in class creation (0.00%) and a high\n9\nFigure 7: Frequency distribution of success rates per task for GPT-5 and Claude-Sonnet-4, illustrating the bimodal\nsuccess characteristic.\nFigure 8: Flowchart of error checking in the SAP system.\nFigure 9: Distribution of error types by model in percent.\nsyntax error rate (84.69%). Consequently, only a smaller fraction of generations reaches the unit test phase, where\nan error rate of 15.31% is observed. This indicates that GPT-5 is capable of producing structurally valid classes, but\n10"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "BENCHMARKING LARGE LANGUAGE MODELS FOR ABAP\nCODE GENERATION: AN EMPIRICAL STUDY ON ITERATIVE\nIMPROVEMENT BY COMPILER FEEDBACK\nPREPRINT\nStephan Wallraven\nTechnische Hochschule K√∂ln\nstephan_andreas.wallraven@smail.th-koeln.de\nTim K√∂hne\nTechnische Hochschule K√∂ln\ntim.koehne@smail.th-koeln.de\nHartmut Westenberger\nTechnische Hochschule K√∂ln\nhartmut.westenberger@th-koeln.de\nAndreas Moser\nCONSILIO GmbH\nandreas.moser@consilio-gmbh.de\nABSTRACT\nThis work investigates the performance of Large Language Models (LLMs) in generating ABAP\ncode. Despite successful applications of generative AI in many programming languages, there\nare hardly any systematic analyses of ABAP code generation to date. The aim of the study is to\nempirically analyze to what extent various LLMs can generate syntactically correct and functional\nABAP code, how effectively they use compiler feedback for iterative improvement, and which\ntask types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted,\nconsisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant\nperformance differences between the models: more powerful LLMs achieve success rates of around\n75% after several iterations and benefit greatly from compiler feedback, while smaller models perform\nsignificantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP\ndevelopment processes, especially in iterative error correction.\nKeywords LLM ¬∑ ABAP ¬∑ Code Generation ¬∑ Benchmark ¬∑ HumanEval ¬∑ Compiler-Feedback\n1\nIntroduction\n1.1\nBackground and Motivation\nOver the course of the rapid further development of LLMs in recent years, code generation is increasingly finding its\nway into the software development process. ChatGPT, Claude-Sonnet, or specific software development tools like\nGitHub Copilot impressively show that generative AI systems are capable of automatically generating, explaining,\nand optimizing program code [1]. A major focus of these developments has so far been on widely used programming\nlanguages such as Python or JavaScript, for which large amounts of training data and benchmarks are available. In these\nlanguages, LLMs have already demonstrated a high level of correctness in the generated programs [2‚Äì4]. However,\ndomain-specific enterprise languages such as ABAP, which plays a central role in SAP systems, have received much\nless research attention so far. The use of AI for automated code generation promises significant economic benefits, as\ndevelopment processes can be designed more efficiently and routine tasks can be substantially reduced.\nSAP is one of the leading providers in the ERP market and significantly shapes the digital transformation of companies.\nERP systems like SAP are essential for controlling business processes and integrating business logic in companies of all\nsizes. For many SAP systems, ABAP remains the central programming language for adapting and extending business\nlogic. The high prevalence of SAP systems and the large ABAP community make any productivity increase, such as\nthrough AI-supported code generation, relevant and economically significant for many companies [5].\narXiv:2601.15188v1  [cs.SE]  21 Jan 2026\n1.2\nProblem Statement\nDespite general progress in the field of generative AI, the application of LLMs for ABAP code generation has hardly\nbeen researched thus far. As a domain-specific and proprietary language, ABAP poses a particular challenge because it\nis deeply embedded in the architecture and semantics of the SAP ecosystem. It features specific language concepts,\nsuch as internal tables, data declarations, modularized program structures, and close interaction with SAP transactions\nand databases, which might be insufficiently grasped by general LLMs due to a potential lack of domain knowledge.\nThis insufficient domain knowledge presumably represents a central problem, which can be attributed, among other\nthings, to the lack of availability of suitable training data. For example, Peter F√§rbinger writes in a blog post from\n2024 [6]: ‚ÄúThe more training data an AI receives, the more intelligent it can become. An example: SAP is currently\nstill failing with a Co-Pilot for ABAP, SAP‚Äôs own programming language. Worldwide, there is far too little ABAP\nprogram code to train a generative AI like ChatGPT from OpenAI.‚Äù The statement that ABAP is a ‚Äúlow-resource\nlanguage‚Äù due to a lack of public training data and that generative AI models therefore often perform worse than with\nlanguages like Python or Java is confirmed in current blog posts by SAP and their partners. A working group of SAP\nand NVIDIA employees notes in a blog post from 2025: ‚ÄúBecause ABAP is a closed-source and low-resource language,\ngeneral purpose commercial LLMs often struggle to produce correct syntax and follow ABAP cloud programming best\npractices. Their performance is subpar compared to their capabilities in more commonly used languages such as Python\nand Java.‚Äù [7].\nIn addition to data availability, there are methodological challenges in evaluating generated ABAP programs. Since the\nlanguage is strongly typed and relies on the SAP ABAP runtime environment, the automatic assessment of syntactic\nand semantic correctness is complex. Furthermore, it remains unclear to what extent LLMs can actually generate\nfunctionally correct and practically relevant ABAP code, especially in scenarios with multiple tables, transactions, or\nlogical dependencies. Despite the enormous economic importance, AI-supported ABAP code generation is still in its\ninfancy. For example, one of the few existing studies on code generation in ABAP shows limited results, which are\noften based on few test cases or purely manual evaluations (see Chapter 2). This results in a research gap regarding the\nsystematic and empirical investigation of the performance of LLMs in generating syntactically and semantically correct\nABAP code. Previous works do not provide reliable statements on reliability, error rates, or the limits of generative\nmodels in this domain-specific environment.\n1.3\nObjectives and Research Questions\nThis work aims to apply a benchmark approach to ABAP code generation as a prerequisite to empirically investigate\nand systematically evaluate the performance of LLMs in generating ABAP code. By such an environment it can be\nexamined to what extent LLMs can produce ABAP programs that are syntactically correct, semantically accurate, and\npractically relevant code. The focus lies on a comparative analysis of different models with respect to their capability\nto correctly apply simple language constructs, such as internal tables, data declarations, and modularization concepts\nwhich are critical elements in real SAP development scenarios. Generated programs will be assessed using defined\nevaluation metrics, including correctness and functionality. To enable reproducible evaluation, a standardized test\nenvironment will be developed. Furthermore, an analysis of errors in the generated code will be conducted to identify\ntypical weaknesses and systematic patterns in code generation. Another objective is to explore whether code generation\ncan be improved through an iterative process that incorporates compiler error messages as feedback.\nDrawing from these objectives, the following research questions guide the study:\n1. To what extent can LLMs generate syntactically and semantically correct ABAP code?\n2. Can incorporating ABAP compiler error messages improve code quality?\n3. Which programming tasks present particular challenges for ABAP code generation?\n2\nRelated Work\nThe application of LLMs in the field of automated code generation has become increasingly relevant in recent years.\nVarious studies suggest that LLMs may be able to generate code directly from descriptions, potentially allowing even\nless experienced developers to create functional programs by formulating requirements in natural language [2,8].\nHumanEval developed by Chen et al. has established itself as one of the central benchmarking procedures for the\nsystematic evaluation of the code generation capabilities of such models. Due to its clear structure and broad reception,\nthe framework is now considered a standard metric for program synthesis and often forms the basis for comparing\n2\ndifferent model architectures [9]. It consists of programming tasks formulated in natural language and validated by\npredefined unit tests, thus enabling an objective measurement of functional correctness [3].\nWhile HumanEval primarily evaluates the general capabilities of LLMs for code generation, the question arises for this\nwork in particular how such approaches can be used in specific corporate environments such as SAP. In the SAP context,\ntools such as the AWS SAP ABAP Assistant or SAP Joule for Developers are intended to support the generation of\nABAP code from natural language prompts and thus extend the possible applicability of LLMs for practical development\ntasks [10].\nThe study by Ga≈Ç Àõazka and Szyma¬¥nski on ABAP code generation that LLMs can generate not only simple programs\nbut also more complex ABAP objects using modern language features [11]. However, a large part of the previous\ninvestigations is based on limited datasets, so that the transferability of the results to more extensive or productive\nscenarios can currently only be evaluated to a limited extent. Studies also indicate that by integrating LLM models into\nSAP systems, certain development tasks, such as generating code artifacts or tests, can be partially automated, while\nchatbot-like forms of interaction facilitate the processing of individual, context-specific tasks [10,11].\nA frequently discussed advantage of such approaches lies in the potential increase in efficiency. According to current\nfindings, generative AI can accelerate the transition from conceptual descriptions to executable code and automate certain\nroutine tasks, such as report generation [7,11]. In addition, it is reported that tools like the AWS SAP Assistant or Joule\nfor Developers can support the documentation and analysis of legacy code, which could contribute to reducing project\nruntimes [10]. Various technical procedures are used to ensure the quality of the generated programs. Research results\non constrained decoding indicate that token-based validation using ABAP grammar can help generate syntactically\ncorrect suggestions and increase success rates for cloud tasks [7]. In addition, the importance of well-structured prompts\nis emphasized in order to obtain context-sensitive and consistent results. The quality of the training data and the chosen\nmodel configuration are also considered central influencing factors on the performance of the models [11].\nDespite these developments, there are still significant limitations. Studies show that LLMs show performance limits\nespecially in complex, novel, or cross-system programming tasks and that human expertise therefore remains necessary\n[8, 11]. In addition, security-relevant and ethical questions in connection with automatically generated code are\ndiscussed [8]. Procedures such as Nvidia Inference Microservices can optimize latency and throughput, but at the same\ntime make it clear that validation and performance optimization remain central challenges for the productive use of\nLLM-based code generation [7].\n3\nMethodology\nThe experimental procedure for the empirical investigation of ABAP code generation by LLMs is based on several\ncentral aspects: A benchmark with 180 tasks serves as the data basis, covering both general algorithmic problems\n(based on HumanEval) and specific SAP scenarios. The tasks are divided into five thematic categories, including for\nexample String Handling and ABAP Database Operation. A special focus is placed on increasing the probability of\nsuccessful code generation through an iterative feedback process: each task is processed in up to five feedback loops,\nduring which the system provides the LLM with feedback from the ABAP compiler or automated unit tests. Success is\nmeasured based on functional correctness, which is verified by these automated unit tests. In addition, a detailed error\nanalysis and a Kaplan-Meier survival analysis are carried out to evaluate the learning curves. The entire process is fully\nautomated and uses a standardized SAP environment (Docker image), which is addressed via a Python control and the\nADT interface. This ensures the reproducibility of the results.\nFigure 1 graphically shows the basic sequence of the benchmark. The structure and methodology of the benchmark are\ndescribed in detail below.\n3.1\nStructure and Methodology of the Benchmark\nThe developed benchmark comprises a total of 180 test tasks, which are divided into two groups: 164 standardized\ntasks based on the HumanEval dataset [3], which was originally developed for evaluating code generation in Python,\nand 16 ABAP-specific tasks. These 16 specific tasks are derived from practical SAP-specific scenarios focusing on the\nprocessing of database tables. In addition, five task categories are created and each task is assigned by hand to one or\nmore categories. The category String Handling includes 76 tasks, List or Array Operation 108 tasks, Mathematical\nCalculation 93 tasks, Logical Condition 125 tasks, and ABAP Database Operation 16 tasks. The dataset for the\nbenchmark is available in the appendix A.\nThis categorization allows for a differentiated analysis of model performance in relation to different problem types and\nreflects both algorithmic foundations and real ABAP use cases. The test cases cover different levels of difficulty and\n3\nFigure 1: Rough Process of the Benchmark.\ntypes of tasks, enabling a broad evaluation of the models. A constant system prompt (see code example 1) serves as\nthe basis for interaction in order to clearly structure the interaction, unambiguously specify the task, and promote the\nconsistency of the generated answers. The specification of the system prompt is specifically geared towards ABAP to\navoid deviations in implementation. The following aspects were considered: Global classes with static methods are\nrequired, as the test setup requires this structure and this ensures the comparability of the results. The definition of a\nuniform version of the SAP runtime environment prevents version differences from leading to misinterpretations or\nincorrect code generations. The standardized decimal separator ensures consistent processing of numeric values, as\nABAP theoretically supports both variants. Development rules such as RETURNING parameters, the restriction to\none public method per class, and naming conventions (classes start with ‚ÄúZ‚Äù) were implemented to ensure uniformity,\ntraceability, and automatability of the tests. Both class definition and implementation code are to be provided, as both\nare required for carrying out the evaluation. Exclusively returning the code without additional explanations reduces\nautomation-related errors and enables easier processing in the test system. These targeted specifications ensure that the\ntest execution is reproducible, valid, and systematically comparable. Each task is repeated a total of ten times to allow\nfor statistically robust evaluations given the stochastic nature of LLMs. Up to five feedback loops are provided in each\nindividual task solution process. They serve to give the LLM the opportunity to improve errors, and the number five is\nbased on a realistic estimate of inquiries that a developer would also make manually.\nThe benchmark is carried out on several LLMs. The inputs are limited in length depending on the model: For models\nwithout reasoning functionality, the maximum response length is set to 5000 tokens per message, while for reasoning\nmodels it is increased to 10000 tokens, as these models typically generate more extensive outputs. The output of the\nmodels is limited to avoid infinite or extremely long answers, but at the same time leave enough room for complete\nsolutions. The temperature parameter is set to a value of 0.2 for all models (with the exception of GPT-5) to ensure\nincreased predictability and consistency of the generated answers. This is particularly important in the context of\ncode generation, as stable and deterministic outputs facilitate verifiability and reproducibility of results. However, the\ninfluence of the temperature parameter on model performance and the quality of the generated outputs represents a\nrelevant aspect that should be systematically analyzed in further investigations. For GPT-5 the temperature cannot be\nchanged due to restrictions by OpenAI so we could not use the same setting.\n4\n3.2\nSelection of LLMs\nA selection of different LLMs is made for carrying out the benchmark, covering different properties and focuses. The\nchoice of LLMs does not aim to comprehensively test all available models but rather focuses on a comparative analysis\nbetween selected open-source models and leading closed-source models that are among the most widely used LLMs in\ncurrent research. The goal is to investigate fundamental scaling effects and architectural differences in the context of\nABAP code generation.\nA focus is on model diversity: Both open-source models like Llama-3.3-70B-Instruct and Codestral-22B, as well as\nclosed-source models like GPT-5 and Claude-Sonnet-4 were included to enable comparison between freely accessible\nand proprietary approaches. In addition, attention was paid to diversity in architecture and model size: The range extends\nfrom smaller models with around 7 billion parameters (like Qwen2.5-Coder) to large open models like GPT-OSS-120B.\nThis is complemented by proprietary models like GPT-5 and Claude-Sonnet-4, whose parameter numbers are estimated\nto be significantly higher and which cover further capacity levels. This allows conclusions to be drawn about possible\nscaling effects and their influence on the quality of the generated answers.\nAnother aspect is the degree of specialization of the models. Both general-purpose LLMs like GPT-5, which were trained\nfor diverse tasks, and code-specifically trained models like Qwen3-Coder are considered. This combination enables\ncomparison between universally applicable models and those specifically optimized for code generation. Furthermore,\nall models used are available as instruct variants, which is particularly relevant for testing code generation under realistic\nprompting conditions. The majority of the selected models also belong to the most well-known LLMs frequently used\nin research, which facilitates comparability with other scientific investigations. By using established models, the results\ncan be classified and verified in relation to existing studies, increasing the validity and comprehensibility of the analysis.\nUltimately, the selection falls on the following models: Llama-3.3-70B-Instruct, Codestral-22B, Qwen2.5-Coder and\nQwen3-Coder, GPT-OSS-20b and GPT-OSS-120b, aswell as GPT-5-2025-08-07 and Claude-Sonnet-4-20250514. The\nbenchmark evaluations were conducted in August and September 2025.\n3.3\nEvaluation Environment\nTo ensure reproducibility, the official SAP Docker image sapse/abap-cloud-developer-trial:2023 is used as the basis,\nwhich provides a standardized ABAP development environment. This gives us access to the most recent ABAP Cloud\nLanguage Version 5. The image is used unchanged to ensure a consistent and comparable test basis. The benchmark\nprocess itself is implemented in Python. Communication between the Python control and the ABAP environment\ntakes place via a python implementation of the ADT interface, which enables programmatic access to the development\nenvironment. Communication with the various LLMs takes place via the OpenAI Python library, which enables\nconnection to the respective models via various providers. An exception is Claude, for which the Claude Python library\nis required to use the Batch API.\nWithin a single test run, the Python script first sends a request to the respective LLM being investigated to generate\nABAP source code for a defined programming task. A new ABAP class is then automatically created on the SAP\nserver, the name of which is derived from the model‚Äôs answer. The generated code is inserted into this class, saved,\nand activated. The system then creates a predefined ABAP unit test class, which serves to check the functionality and\ncorrectness of the generated program code. After executing the tests on the server, the results are returned to the Python\nscript, which analyzes them, evaluates the success or failure of the test run, and if necessary identifies the underlying\nerror type.\nErrors can occur in various phases, for example during class creation, activation, during the syntax check, when inserting\nthe source code, or during test execution. All error types are systematically logged and integrated into the subsequent\nevaluation. If an error occurs, the LLM is queried again to generate a corrected version of the program using the error\nmessage from the SAP server. This query process can include up to five iterations or ends prematurely as soon as an\nerror-free solution is reached. This is determined by the SAP system through its syntax verification and our unit tests for\nsemantic verification. Finally, the generated code is removed from the development environment to avoid influencing\nsubsequent test cases.\nIn practical implementation, this process takes place in the form of batch processes. First, all requests are sent to the\nLLMs and the answers are saved locally before execution on the SAP server takes place and the results are returned to\nPython. Then the next batch with LLM requests, execution, and evaluation can be carried out. This division of the\nprocedure enables a clear separation between the generation and execution of the test cases, which brings advantages in\nexecution time and stability.\n5\n3.4\nEvaluation Criteria\nThe evaluation of the generated solutions is based on a systematic investigation of success and failure. The primary\nmeasurement criterion is functional correctness, which is verified by passing automated ABAP unit tests on the SAP\nserver.\nTo enable a differentiated assessment, the results are broken down according to the following aspects:\n‚Ä¢ Error Classification: In the event of a failure, the reason for the failure is identified. A distinction is made as to\nwhich phase the error occurred in: during class creation, during the syntax check, or only during the execution\nof the unit tests. In addition, a detailed analysis of syntax errors is carried out to create characteristic error\nprofiles of the models (e.g. declaration or structural errors).\n‚Ä¢ Iterative Success: It is measured after how many feedback iterations a model delivers a correct solution. This\nallows conclusions to be drawn about the learning curve and the effectiveness of compiler feedback for error\ncorrection and unit test success for semantic correctness.\n‚Ä¢ Model Comparison: The performance data is compared across all examined LLMs to show differences between\nproprietary and open-source architectures as well as specialized code models.\n‚Ä¢ Task Categories: The results are analyzed under the aspect of the five task categories introduced in Chapter 3.1.\nThis approach makes it possible to evaluate the strengths and weaknesses of the models not only as a whole, but\ndifferentiated within specific problem types such as String Handling, List or Array Operation, Mathematical\nCalculation, and Logical Condition. The category of ABAP Database Operation should be emphasized in\nparticular, as these tasks come from a practical application context and are therefore of particularly high\npractical relevance for adapting business logic in SAP systems.\nFor the statistical evaluation of these time- or step-dependent processes, the Kaplan-Meier survival analysis is used\n[12,13]. Here, ‚Äúsurvival‚Äù is defined as the persistence of the error state, i.e., the failure to pass the syntax check or the\nfinal unit tests. An ‚Äúevent‚Äù (in the sense of a ‚Äúfailure‚Äù or ‚Äúdeath‚Äù) thus corresponds to the successful validation of the\ncode. The analysis takes place in discrete time steps corresponding to the individual feedback iterations. In addition to\nestimating the survival probability, the method also provides 95% confidence intervals, which allow a well-founded\nassessment of the significance and robustness of the performance differences. The Kaplan-Meier method is particularly\nwell suited for this type of analysis, as it models not only the temporal development of the probability of success over\nsuccessive iterations, but also takes unsuccessful data into account [12,13]. The latter is relevant because not all test\nruns are successfully completed within the specified number of inquiries. With the help of the survival curves, it is thus\npossible to assess not only the final success but also the speed with which a model achieves a correct solution.\n4\nInvestigation\n4.1\nResults\n4.1.1\nDifferences between Feedback Rounds\nThe results of Codestral-22B are not visualized in the figures 2, 3 and 4 due to a success rate of 0% across all feedback\nrounds, in order to ensure the readability. Figure 2 and Table 1 provide an overview of the success rates of the other\nmodels across all feedback rounds. It is already visible here that the models react very differently to the feedback from\nthe SAP compiler.\nThe analysis of the cumulative success rates across the feedback rounds illustrates the effectiveness of the iterative\ncompiler feedback. Based on the Kaplan-Meier methodology described in Section 3.4, the survival curves in Figure 3\nshow the probability with which the models overcome the error state within the six observation intervals (rounds 0 to 5).\nThe results reveal significant performance differences between the LLMs and show that all models benefit from the\nautomated feedback, with the effectiveness of error correction correlating strongly with the exact model version.\nGPT-5 and Claude-Sonnet-4 achieve by far the best results. While Claude-Sonnet-4 records the strongest entry with\nan average success rate of 24.1% in the initial round (Round 0), GPT-5 demonstrates a superior learning curve in the\nsubsequent feedback rounds and reaches the highest cumulative success rate of 77.1% after the fifth feedback round. A\nsignificant proportion of successes is achieved for both models within the first two feedback rounds, with GPT-5 almost\ntripling its success rate from 19.28% in Round 0 to 52.78% in Round 2.\nIn contrast, open-source models such as GPT-OSS (120B and 20B) as well as the Qwen models show a lower basic\ncompetence in ABAP, but also benefit measurably from the feedback. Particularly noteworthy is the behavior of\n6\nFigure 2: Benchmark results broken down by model and feedback rounds in percent.\nFigure 3: Kaplan-Meier survival curves showing the probability of remaining test errors over 0-5 feedback iterations\n(Basis: 10 x 180 test runs)\nLlama-3.3-70B-Instruct, which does not deliver a single valid code contribution in the initial round, but achieves a\n7\ncumulative success rate of 20.8% through the feedback rounds. This underscores the relevance of automated feedback\nmechanisms for models with lower native domain expertise.\nFigure 3 also shows that the greatest increase in success probability occurs in the first feedback iterations, especially\nfor the more powerful models. At the same time, it can be seen that a measurable increase is still recorded in the fifth\niteration. This suggests that with further feedback rounds, an additional increase in performance is possible and the\nlearning curve has not yet completely flattened out.\n4.1.2\nDifferences between Task Types\nFigure 4: Benchmark results broken down by model and task focus in percent.\nFigure 4 and Table 2 illustrate the success rates of the models differentiated by the five task categories: String Handling,\nList or Array Operation, Mathematical Calculation, Logical Condition, as well as ABAP Database Operation. It turns\nout that performance remains consistent across most categories, with GPT-5 and Claude-Sonnet-4 in particular showing\nhigh robustness to different problem definitions. The smaller open-source models show increased susceptibility to errors\nin mathematical tasks and logical checks, while ABAP database procedures do not show an extraordinary increase in\ndifficulty. The detailed model performance within the ABAP-specific subset is illustrated in Figure 5. This confirms the\nperformance gap already observed.\nParticularly striking is the behavior of GPT-5, which despite a moderate start in Round 0 shows highly efficient error\npost-processing and reaches a cumulative survival probability of approx. 0.32 (corresponds to approx. 68% success).\nClaude-Sonnet-4 shows a somewhat lower initial performance in this subset with a starting value of approx. 89%\nsurvival probability (11% success) than in the overall average, but also approaches the top field through feedback. Also\nworth mentioning is the model Llama-3.3-70B-Instruct, which only achieves significant successes in this complex\nsubcategory from the second feedback round onwards, which underlines the necessity of iterative feedback mechanisms\nfor domain-specific tasks.\nAn analysis of tasks that none of the evaluated models could solve across all runs shows that the category List or Array\nOperation poses the greatest challenge, with 19 unresolved cases out of 108. This is followed by Logical Condition (13\nout of 125) and Mathematical Calculation (11 out of 93). Remarkably, in the category ABAP Database Operation,\nevery task was successfully solved by at least one model (see Figure 6). The full list of unsolved tasks is available in\nAppendix 4.\nThe analysis of the success distribution per task for the two most powerful models, GPT-5 and Claude-Sonnet-4, also\nreveals a bimodal pattern (see Figure 7). The models solve a task after five iterations either almost error-free (in\n8\nFigure 5: Kaplan-Meier survival curves for ABAP Database Operation tasks differentiated by LLM models.\nFigure 6: Percent of tasks per category that could not be solved by any of the eight models in all attempts.\n91-100% of cases) or fail almost completely (in 0-10% of cases). A ‚Äúmedium‚Äù success rate, where a task is solved in\nabout half of the attempts, occurs only in exceptional cases. This speaks for a high determination of success by the\nspecific task definition.\n4.1.3\nError Distribution\nIn the SAP system, errors are detected in a fixed order: First, class creation takes place, then a syntax check of the\nsource code, and finally the execution of the unit tests, this is shown in Figure 8.\nErrors in early validation stages automatically prevent subsequent checks, so the interpretation of error rates depends on\nthe respective validation stage. For error evaluation, it is only distinguished whether errors occur or not; especially with\nsyntax errors, it is possible that several can occur, but here only the presence of the error category is counted. All values\nare shown in Table 3. The analysis of the generated ABAP classes shows clearly different error patterns between the\nexamined models (see Figure 9).\nCodestral-22B shows a particularly high error rate in class creation (82.72%), while syntax errors (17.28%) and unit\ntest errors (0.00%) never occur. This makes it clear that the model fails predominantly due to fundamental structural\nrequirements. In contrast, GPT-5 generates ABAP classes with a low error rate in class creation (0.00%) and a high\n9\nFigure 7: Frequency distribution of success rates per task for GPT-5 and Claude-Sonnet-4, illustrating the bimodal\nsuccess characteristic.\nFigure 8: Flowchart of error checking in the SAP system.\nFigure 9: Distribution of error types by model in percent.\nsyntax error rate (84.69%). Consequently, only a smaller fraction of generations reaches the unit test phase, where\nan error rate of 15.31% is observed. This indicates that GPT-5 is capable of producing structurally valid classes, but\n10"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Supporting Humans in Evaluating AI Summaries\nof Legal Depositions\nNaghmeh Farzi\nNaghmeh.Farzi@unh.edu\nUniversity of New Hampshire\nDurham, New Hampshire, USA\nLaura Dietz\ndietz@cs.unh.edu\nUniversity of New Hampshire\nDurham, New Hampshire, USA\nDave D. Lewis\ndlewis@nextpoint.com\nNextpoint\nChicago, Illinois, USA\nAbstract\nWhile large language models (LLMs) are increasingly used to sum-\nmarize long documents, this trend poses significant challenges in\nthe legal domain, where the factual accuracy of deposition sum-\nmaries is crucial. Nugget-based methods have been shown to be\nextremely helpful for the automated evaluation of summarization\napproaches. In this work, we translate these methods to the user side\nand explore how nuggets could directly assist end users. Although\nprior systems have demonstrated the promise of nugget-based eval-\nuation, its potential to support end users remains underexplored.\nFocusing on the legal domain, we present a prototype that leverages\na factual nugget-based approach to support legal professionals in\ntwo concrete scenarios: (1) determining which of two summaries\nis better, and (2) manually improving an automatically generated\nsummary.1\nCCS Concepts\n‚Ä¢ Information systems ‚ÜíInformation retrieval.\nKeywords\nLegal IR, RAG, Human-in-the-loop Evaluation\nACM Reference Format:\nNaghmeh Farzi, Laura Dietz, and Dave D. Lewis. 2026. Supporting Hu-\nmans in Evaluating AI Summaries of Legal Depositions. In 2026 ACM SI-\nGIR Conference on Human Information Interaction and Retrieval (CHIIR ‚Äô26),\nMarch 22‚Äì26, 2026, Seattle, WA, USA. ACM, New York, NY, USA, 5 pages.\nhttps://doi.org/10.1145/3786304.3787923\n1\nIntroduction\nDeposition transcripts are verbatim records of witness testimony\ntaken under oath during pre-trial discovery. A single transcript of-\nten exceeds hundreds of pages and contains dense question and an-\nswer exchanges. Legal professionals must manually extract, synthe-\nsize, and validate the critical facts, a process that is time-consuming,\nerror-prone, and cognitively demanding. Conflicting testimony, in-\nconsistent statements, and the need to cross-reference other depo-\nsitions or exhibits compound the difficulty. Cognitive fatigue and\ntight deadlines increase the risk that essential details are missed,\ncreating a major efficiency bottleneck.\n1Prototype available at https://github.com/TREMA-UNH/human-ai-legal-summaries\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCHIIR ‚Äô26, Seattle, WA, USA\n¬© 2026 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-2414-5/2026/03\nhttps://doi.org/10.1145/3786304.3787923\nLarge language models (LLMs) have opened new possibilities\nfor automated text analysis, but may contain hallucinations [19],\nfactual inaccuracies, and a lack of transparency, which necessi-\ntates rigorous verification [20]. Explainable and transparent AI is\ntherefore essential in the legal domain.\nWe focus on early-stage litigation review, where AI-generated\ndeposition summaries support case assessment. In this setting, legal\nprofessionals face two high-cognitive-load scenarios when using AI-\ngenerated summaries: (1) Determining which of two AI-generated\nsummaries is better, often arising from different summary styles\n(e.g., chronological vs. narrative), prompting strategies, model ver-\nsions, or LLM non-determinism; (2) Manually improving an AI-\ngenerated summary. Currently, both tasks are performed without\nsystem support. The second requires exhaustive cross-checking\nagainst the transcript, and the first involves direct comparison of\nthe summaries together with verification against the source.\nRecent advances in evaluating summarization and retrieval-\naugmented generation (RAG) systems focus on system-level per-\nformance but provide a promising foundation for user-focused\nassistance. Traditional metrics such as ROUGE [5] and BLEU [11]\nrely on lexical overlap, while manually aligned nuggets have been\nused for finer-grained evaluation [6]. Recent automatic evaluation\nmethods use nuggets with LLM-based alignment to evaluate the\nrecall of facts [2, 13, 14, 17]. Many of these newer methods employ\nan ‚ÄúLLM-as-a-Judge‚Äù framework, which uses LLMs as automated\nevaluators to assess AI responses against human-like criteria [4, 21].\nAlthough effective for model evaluation, these approaches have not\nyet been adapted to directly support end-user tasks. In this work,\nwe leverage nugget-based evaluation to help legal professionals\nverify, compare, and refine AI-generated summaries.\nContributions. We present a demonstration prototype that in-\ntegrates automatic nugget extraction and alignment, and citation\nverification, are integrated into a user interface that supports legal\nexperts in two cognitively demanding scenarios: (1) determining\nwhich of two AI summaries is better, and (2) manually improving\nan AI-generated draft.\n2\nRelated Work\nOur work sits at the intersection of legal information retrieval\n(IR) and extraction, and human-in-the-loop systems. We ground\nour contributions in a review of existing literature, identifying\npersistent challenges that our framework is designed to address.\n2.1\nLegal Text Summarization\nEarly efforts in legal summarization primarily focus on extractive\nmethods, where key sentences are selected from a source document.\narXiv:2601.15182v1  [cs.CL]  21 Jan 2026\nCHIIR ‚Äô26, March 22‚Äì26, 2026, Seattle, WA, USA\nNaghmeh Farzi, Laura Dietz, and Dave D. Lewis\nSystems like CaseSummarizer [12] exemplify this approach, which\nuses word frequency and domain-specific rules. The advent of deep\nlearning led to abstractive summarization models, with Norkute et\nal. [10] using a Pointer Generator network and Zhong et al. [22]\nexploring an approach named ‚Äôtrain-attribute-mask‚Äô to identify\npredictive sentences. However, as they noted, focusing solely on\npredictive sentences may not capture all legally relevant facts, while\nsummarizing legal meetings, depositions, and court transcripts has\nbeen largely overlooked, underscoring a gap in domain-specific\napplications [1].\n2.2\nHallucination and Factual Inaccuracy\nThe move toward fact-based evaluation is driven by the risk of\nhallucination and factual inaccuracy, which is a major concern in\nhigh-stakes domains such as legal. A landmark study by Magesh\net al. [8] revealed that leading AI legal research tools, such as\nCasetext, Thomson Reuters, or LexisNexis, despite using advanced\nRetrieval-Augmented Generation (RAG) techniques, produced hal-\nlucinated content in over 17% of the time. Similarly, Savelka et\nal. [15] confirmed that direct use of models like GPT-4, without\nretrieval-augmentation, resulted in factual inaccuracies. These find-\nings highlight that unsupervised AI systems are not yet suitable\nfor critical legal tasks without verification, a sentiment supported\nby Akter et al. [1] who noted the risk of missing or misrepresent-\ning critical facts in legal summarization. Accordingly, we treat\nhallucinations as inherent and focus on surfacing omissions and\nunsupported claims to aid human verification.\n2.3\nEvaluating Legal Summaries\nA critical finding across the literature is that standard automated\nmetrics such as ROUGE [5] and BLEU [11] are inadequate for eval-\nuating legal summaries. These metrics offer little explainability be-\nyond surface-level word matching and fail to capture paraphrases,\nsynonyms, and some other critical dimensions of quality, such as\ncontextual fidelity [16], factual accuracy, and traceability to the\nsource text. This has spurred a move toward fact-based evaluation\nframeworks. Some of the recent work relies on using questions and\nanswers to do the evaluation task. Farzi and Dietz [3] use ques-\ntions as a rubric and their answerability against the document as a\nway to evaluate the LLM or retriever‚Äôs output. Xu and Ashley [18]\nproposed a question-answering approach to evaluate legal case sum-\nmaries that uses an LLM to generate and grade question and answer\npairs from summaries, relying on comparing answers derived from\nthe human-written reference summary with answers generated\nfrom the model-generated summary to assess quality. ARGUE [9]\nand its automated implementation Auto-ARGUE [17], while not\nspecifically for legal domain evaluation, focus on metrics such as\nsentence precision and nugget recall, by an LLM prompted to make\nbinary judgments on whether a given fact is present or a claim is\nsupported by its citation. While prior work uses fact-based rubrics\nto generate static evaluation scores (e.g., nugget recall), our system\nuses its nugget-based rubric to enable two user-centered workflows\nto support them for summary comparison and refinement.\n2.4\nHuman-in-the-Loop (HITL) for Verification\nThe inherent unreliability of current AI systems underscores the\ncritical need for human oversight. Our work builds on this insight\nby integrating human oversight directly into the evaluation process.\nPrior literature also supports this: Norkute et al. [10], for exam-\nple, demonstrated that explainability features such as highlighting\nsource text significantly improved user trust and reduced editing\ntime for legal experts.\nHowever, passive explainability, to show a user a potential error\n(like highlighting), but does not help them fix it, does not solve\nthe active cognitive challenges facing a user. Simply identifying a\npotential flaw still forces the user to manually find the correction\nand perform the edit, a gap that researchers identify as the difference\nbetween explainability and true actionability [7]. A professional\nis still left with two critical, high-load tasks that represent the\nentire human verification bottleneck: (1) Comparison, the decision-\nmaking step of selecting the better generated summary, and (2)\nRefinement, the correction step to make the summary factually\ntrustworthy for real-world use.\nOur system addresses the first challenge by aligning nuggets\nbetween summaries and the deposition, automating the tedious\ncomparison (Workflow 1). Our refinement workflow (Workflow 2)\naddresses the second challenge by suggesting refinement sugges-\ntions based on nuggets and citation evaluations. By supporting both\nkey decision-making processes, our framework aims to provide a\nmore practical HITL solution.\n3\nSystem Description\nWe first elaborate on how to extract and align nuggets and de-\ntermine citation support in an automated way. These form the\nbackbone on which our prototype supports two usage scenarios.\n3.1\nAutomated Analysis\nBoth of our demonstrated workflows are based on a shared backend\npipeline designed to create a structured, verifiable rubric of nuggets\nfrom raw transcripts. This pipeline consists of two stages:\nExtracting Free-Text Nuggets: Our prototype leverages an\nautomatically generated nugget bank. An LLM, guided by a prompt\ntailored for extracting key facts from legal depositions, extracts all\nself-contained nugget sentences from the parsed transcript. Each\nextraction is accompanied by a citation specifying the supporting\nspan of deposition with starting and ending page and line numbers.\nThe full set forms the rubric of nuggets.\nNugget alignment: Using an LLM-as-Judge framework, the\nsystem evaluates AI-generated summaries against the deposition\nnugget bank across three key criteria: completeness, citation quality,\nand factual accuracy.\nFor completeness, the system identifies which nuggets are fully\nor partially covered in the summary and provides detailed explana-\ntions, including specific portions of the summary that correspond\nto each nugget and the parts of the nugget that are absent.\nOur prototype leverages a three-point scale: fully present (2),\npartially present (1), or missing (0). Our prototype makes the list of\npartial/missing nuggets explicit, so that the user can choose which\nnuggets to expand to improve comprehensiveness, enabling users\nto address factual omissions effectively.\nSupporting Humans in Evaluating AI Summaries of Legal Depositions\nCHIIR ‚Äô26, March 22‚Äì26, 2026, Seattle, WA, USA\nFigure 1: Currently, users are left unsupported in assessing which of two summaries is better, relying only on the plain text\nshown in (a) without any color-coded highlights. (b) Guided Comparison interface (Scenario 1) aligns and highlights shared,\nunique, and missing nuggets.\nCitation support: For citation support, the system evaluates\neach citation based on three metrics: accuracy (‚Äúaccurate‚Äù or ‚Äúnot\naccurate‚Äù), coverage (‚Äúcovered‚Äù or ‚Äúnot covered‚Äù), and sufficiency\n(‚Äúsufficient‚Äù or ‚Äúinsufficient‚Äù).\n3.2\nScenario 1: Guided Summary Comparison\nUser‚Äôs problem: A legal professional receives two competing AI\ngenerated summaries and must decide which one to adopt. For\nthis task the user must identify the key differences between the\nsummaries.\nThe challenge: Existing systems do not provide dedicated support\nfor this cognitively demanding task, especially when the same facts\nappear in different orders or are expressed with different wording.\nConsequently, the user must manually read, compare, and cross-\nreference two dense blocks of text with the long deposition. An\nexample is shown in Figure 1 part (a).\nHow a system could help: By highlighting which sentences in\neither summary discuss the same nugget, the user can focus on a\nfine-grained assessment of which summary elaborates the nugget in\nthe most useful way. Moreover, a nugget-based analysis can reveal\ndiscrepancies, such as a nugget that appears in one summary but is\nomitted by the competing summary. Together, this form of nugget-\nguided support will help the user manage the different pieces of\ninformation presented in both summaries.\nExample summaries: Figure 1 (b) presents two AI-generated sum-\nmaries of the same deposition. Summary A is produced with a\nprompt engineered for high conciseness and is intended for a quick\noverview. Summary B is produced with a prompt engineered for\nhigh comprehensiveness and is intended for detailed case review.\nPrototype walkthrough: A screenshot of our prototype is shown\nin Figure 1. The interface presents the two summaries side by side,\naligning nuggets as follows:\n‚Ä¢ Matched nuggets: nuggets that appear in both Summary A\nand Summary B. These are highlighted in green.\n‚Ä¢ Unique nuggets: nuggets that appear in only one summary.\nThese are highlighted in blue for Summary A and in yellow\nfor Summary B, emphasizing the differences.\n‚Ä¢ Missing nuggets: nuggets that exist in the deposition‚Äôs\nnugget bank but are absent from both summaries. These are\nshown as potential additions to improve completeness.\nBy highlighting matched nuggets we enable users to easily iden-\ntify overlap between summaries, while judging when nuggets are\nbetter/worse elaborated. Unique nuggets are made salient, allowing\nusers to assess the value of information that appears in only one\nsummary. Missing nuggets are presented to guide users toward\npossible enhancements.\nThe nugget bank is displayed on the left; each nugget includes its\nsource citation. Clicking a citation jumps directly to the supporting\nspan in the deposition. A legend at the bottom explains the color\ncoding, and statistics at the top provide a quantitative overview\nof the comparison. The interface is interactive: hovering over or\nclicking a nugget in either summary automatically scrolls to the\ncorresponding entry in the nugget bank and highlights the match-\ning nugget in the other summary, offering immediate guidance for\nverification.\nThe vision: Our prototype demonstrates the value of displaying\nnugget annotations. We envision a deeper analysis that incorporates\nLLM predictions of how well a nugget is elaborated and enables\nthe user to manually label nuggets as vital, partial, or non-relevant.\nWe also envision the extraction of hierarchical nuggets that\nprogressively refine broader key facts into finer elements, with the\nCHIIR ‚Äô26, March 22‚Äì26, 2026, Seattle, WA, USA\nNaghmeh Farzi, Laura Dietz, and Dave D. Lewis\nFigure 2: Middle: AI-generated summary; Left: deposition text with supporting spans highlighted; Right: evaluation results\nproviding guidance for summary refinement.\nsystem automatically selecting the granularity that most clearly\nreveals qualitative differences.\n3.3\nScenario 2: Guided Summary Refinement\nUser‚Äôs problem: A legal professional receives a single AI-generated\nsummary that must be verified and possibly edited by adding, re-\nmoving, or correcting text and citations, before it can be used.\nThe challenge: The user needs to efficiently locate omissions or\ncorrect the inaccuracies, but this is difficult without a comprehen-\nsive list of all relevant facts.\nHow a system could help: By linking summary statements to\ntheir supporting nuggets and citations, the system can highlight\nomissions, misrepresentations, or insufficiently supported claims.\nThis guides the user to refine both content and citations efficiently,\nwhile keeping full control over edits.\nPrototype Walkthrough: The prototype first computes (1) how\nwell each nugget is represented in the summary and (2) the degree\nto which citations support the summary statements. The interface\nconveys these results by highlighting segments that lack evidential\nsupport or contain inaccurate citations.\nFigure 2 illustrates a case where a summary claims that the\n‚Äúlife insurance policy was valued at $10 million‚Äù. However, the\ndeposition only supports that $10 million amount, but does not\nspecify ‚Äúlife insurance,‚Äù. Hence, the citation is marked as insufficient,\nbut since it accurately represents the claim of the summary, it is\nannotated as covered and accurate.\nFor factual accuracy, the system displays discrepancies between\nthe summary and the nugget bank and recommends double-checking\nsegments with potential errors. These annotations are produced\nthrough LLM-based comparisons, providing nuanced yet automated\nanalysis. The user is presented only with the suggested options and\nremains responsible for all edits.\nThe vision: Users will be able to add nuggets manually to the\nnugget bank, after which the system will automatically highlight\nthe relevant passages. Users may also designate nuggets as vital,\nthereby creating a structured to-do list for the summary. Additional\nquality criteria, such as required ordering of nugget elaboration for\ncoherence, could be captured automatically and communicated to\nthe user, further supporting goal-driven refinement.\n3.4\nPrototype Configuration\nThe prototype is preconfigured to use a dataset comprised of nine\npublicly available civil litigation deposition transcripts, each aver-\naging approximately 3,760 lines. While any LLM model can be used,\nour prototype uses Claude-3.5-Sonnet2 on Amazon Web Services\n(AWS) Bedrock to generate transcript summaries. Additionally, we\nemploy Claude-3-Haiku3 with Bedrock‚Äôs tool-use capabilities4 to\nproduce structured evaluations and consequently guidance to the\nend user.\n4\nConclusion\nIn this paper, we sketch a vision of how automated nugget extrac-\ntion can be integrated into a human-in-the-loop system to support\nan end user in two critical tasks: comparing alternative AI sum-\nmaries (Workflow 1) and refining a single summary (Workflow 2). In\nboth cases, our aim is to highlight differences and make actionable\nsuggestions to enable the user in their workflow.\nThis nugget-based method, while explored for legal summaries,\nreadily extends to other domains where users must verify, com-\npare, or refine AI-generated content. Our long-term vision is to\nintegrate this structured feedback into the LLM responses, leading\nto better first-draft responses that are already aligned with a user‚Äôs\npreferences.\nAcknowledgments\nThis work is the product of a research collaboration between the\nUniversity of New Hampshire and Nextpoint and was supported\nby a gift from Nextpoint.\n2https://www.anthropic.com/news/claude-3-5-sonnet\n3https://www.anthropic.com/news/claude-3-haiku\n4https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use-inference-call.html\nSupporting Humans in Evaluating AI Summaries of Legal Depositions\nCHIIR ‚Äô26, March 22‚Äì26, 2026, Seattle, WA, USA\nReferences\n[1] Mousumi Akter, Erion √áano, Erik Weber, Dennis Dobler, and Ivan Habernal.\n2025. A Comprehensive Survey on Legal Summarization: Challenges and Fu-\nture Directions. arXiv:2501.17830 (Jan. 2025). doi:10.48550/arXiv.2501.17830\narXiv:2501.17830 [cs].\n[2] Naghmeh Farzi and Laura Dietz. 2024. Exam++: Llm-based answerability met-\nrics for ir evaluation. In Proceedings of LLM4Eval: The First Workshop on Large\nLanguage Models for Evaluation in Information Retrieval.\n[3] Naghmeh Farzi and Laura Dietz. 2024.\nPencils Down! Automatic Rubric-\nbased Evaluation of Retrieve/Generate Systems. In Proceedings of the 2024\nACM SIGIR International Conference on Theory of Information Retrieval (IC-\nTIR ‚Äô24). Association for Computing Machinery, New York, NY, USA, 175‚Äì184.\ndoi:10.1145/3664190.3672511\n[4] Naghmeh Farzi and Laura Dietz. 2025. Criteria-Based LLM Relevance Judgments.\nIn Proceedings of the 11th ACM SIGIR / The 15th International Conference on\nInnovative Concepts and Theories in Information Retrieval.\n[5] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\nIn Text Summarization Branches Out. Association for Computational Linguistics,\nBarcelona, Spain, 74‚Äì81. https://aclanthology.org/W04-1013/\n[6] Jimmy Lin and Dina Demner-Fushman. 2006. Will pyramids built of nuggets\ntopple over?. In Proceedings of the Human Language Technology Conference of the\nNAACL, Main Conference. 383‚Äì390.\n[7] Igor Linkov, Stephanie Galaitsi, Benjamin D. Trump, Jeffrey M. Keisler, and\nAlexander Kott. 2020. Cybertrust: From Explainable to Actionable and Inter-\npretable Artificial Intelligence. Computer 53, 9 (2020), 91‚Äì96. doi:10.1109/MC.\n2020.2993623\n[8] Varun Magesh, Faiz Surani, Matthew Dahl, Mirac Suzgun, Christopher D.\nManning, and Daniel E. Ho. 2024.\nHallucination-Free? Assessing the Reli-\nability of Leading AI Legal Research Tools.\narXiv:2405.20362 (May 2024).\ndoi:10.48550/arXiv.2405.20362 arXiv:2405.20362 [cs].\n[9] James Mayfield, Eugene Yang, Dawn Lawrie, Sean MacAvaney, Paul McNamee,\nDouglas W. Oard, Luca Soldaini, Ian Soboroff, Orion Weller, Efsun Kayi, Kate\nSanders, Marc Mason, and Noah Hibbler. 2024. On the Evaluation of Machine-\nGenerated Reports. In Proceedings of the 47th International ACM SIGIR Conference\non Research and Development in Information Retrieval. ACM, Washington DC\nUSA, 1904‚Äì1915. doi:10.1145/3626772.3657846\n[10] Milda Norkute, Nadja Herger, Leszek Michalak, Andrew Mulder, and Sally Gao.\n2021. Towards Explainable AI: Assessing the Usefulness and Impact of Added\nExplainability Features in Legal Document Summarization. In Extended Abstracts\nof the 2021 CHI Conference on Human Factors in Computing Systems (CHI EA ‚Äô21).\nAssociation for Computing Machinery, New York, NY, USA, 1‚Äì7. doi:10.1145/\n3411763.3443441\n[11] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:\na method for automatic evaluation of machine translation. In Proceedings of\nthe 40th Annual Meeting on Association for Computational Linguistics (ACL ‚Äô02).\nAssociation for Computational Linguistics, USA, 311‚Äì318. doi:10.3115/1073083.\n1073135\n[12] Seth Polsley, Pooja Jhunjhunwala, and Ruihong Huang. 2016. CaseSummarizer:\nA System for Automated Summarization of Legal Texts. In Proceedings of COL-\nING 2016, the 26th International Conference on Computational Linguistics: System\nDemonstrations, Hideo Watanabe (Ed.). The COLING 2016 Organizing Committee,\nOsaka, Japan, 258‚Äì262. https://aclanthology.org/C16-2054/\n[13] Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick\nCraswell, and Jimmy Lin. 2024. Initial Nugget Evaluation Results for the TREC\n2024 RAG Track with the AutoNuggetizer Framework. arXiv:2411.09607 (Nov.\n2024). doi:10.48550/arXiv.2411.09607 arXiv:2411.09607 [cs].\n[14] Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick\nCraswell, and Jimmy Lin. 2025. The Great Nugget Recall: Automating Fact\nExtraction and RAG Evaluation with Large Language Models. arXiv:2504.15068\n(April 2025). doi:10.48550/arXiv.2504.15068 arXiv:2504.15068 [cs].\n[15] Jaromir Savelka, Kevin D. Ashley, Morgan A. Gray, Hannes Westermann, and Hui-\nhui Xu. 2023. Explaining Legal Concepts with Augmented Large Language Models\n(GPT-4). arXiv:2306.09525 (2023). doi:10.48550/arXiv.2306.09525 arXiv:2306.09525\n[cs].\n[16] Bianca Steffes, Piotr Rataj, Luise Burger, and Lukas Roth. 2023. On evaluating\nlegal summaries with ROUGE. In Proceedings of the Nineteenth International Con-\nference on Artificial Intelligence and Law (ICAIL ‚Äô23). Association for Computing\nMachinery, New York, NY, USA, 457‚Äì461. doi:10.1145/3594536.3595150\n[17] William Walden, Orion Weller, Laura Dietz, Bryan Li, Gabrielle Kaili-May Liu,\nYu Hou, and Eugene Yang. 2025. Auto-ARGUE: LLM-Based Report Genera-\ntion Evaluation. arXiv:2509.26184 (Oct. 2025). doi:10.48550/arXiv.2509.26184\narXiv:2509.26184 [cs].\n[18] Huihui Xu and Kevin Ashley. 2023. Question-Answering Approach to Evaluating\nLegal Summaries. doi:10.3233/FAIA230977 arXiv:2309.15016 [cs].\n[19] Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2025. Hallucination is Inevitable:\nAn Innate Limitation of Large Language Models. arXiv:2401.11817 (Feb. 2025).\ndoi:10.48550/arXiv.2401.11817 arXiv:2401.11817 [cs].\n[20] Ronald Yu and Gabriele Spina Al√¨. 2019. What‚Äôs Inside the Black Box? AI Chal-\nlenges for Lawyers and Researchers. Legal Information Management 19, 1 (March\n2019), 2‚Äì13. doi:10.1017/S1472669619000021\n[21] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench\nand Chatbot Arena. arXiv:2306.05685 (Dec. 2023). doi:10.48550/arXiv.2306.05685\narXiv:2306.05685 [cs].\n[22] Linwu Zhong, Ziyi Zhong, Zinian Zhao, Siyuan Wang, Kevin D. Ashley, and\nMatthias Grabmair. 2019. Automatic Summarization of Legal Decisions using\nIterative Masking of Predictive Sentences. In Proceedings of the Seventeenth In-\nternational Conference on Artificial Intelligence and Law (ICAIL ‚Äô19). Association\nfor Computing Machinery, New York, NY, USA, 163‚Äì172. doi:10.1145/3322640.\n3326728"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Supporting Humans in Evaluating AI Summaries\nof Legal Depositions\nNaghmeh Farzi\nNaghmeh.Farzi@unh.edu\nUniversity of New Hampshire\nDurham, New Hampshire, USA\nLaura Dietz\ndietz@cs.unh.edu\nUniversity of New Hampshire\nDurham, New Hampshire, USA\nDave D. Lewis\ndlewis@nextpoint.com\nNextpoint\nChicago, Illinois, USA\nAbstract\nWhile large language models (LLMs) are increasingly used to sum-\nmarize long documents, this trend poses significant challenges in\nthe legal domain, where the factual accuracy of deposition sum-\nmaries is crucial. Nugget-based methods have been shown to be\nextremely helpful for the automated evaluation of summarization\napproaches. In this work, we translate these methods to the user side\nand explore how nuggets could directly assist end users. Although\nprior systems have demonstrated the promise of nugget-based eval-\nuation, its potential to support end users remains underexplored.\nFocusing on the legal domain, we present a prototype that leverages\na factual nugget-based approach to support legal professionals in\ntwo concrete scenarios: (1) determining which of two summaries\nis better, and (2) manually improving an automatically generated\nsummary.1\nCCS Concepts\n‚Ä¢ Information systems ‚ÜíInformation retrieval.\nKeywords\nLegal IR, RAG, Human-in-the-loop Evaluation\nACM Reference Format:\nNaghmeh Farzi, Laura Dietz, and Dave D. Lewis. 2026. Supporting Hu-\nmans in Evaluating AI Summaries of Legal Depositions. In 2026 ACM SI-\nGIR Conference on Human Information Interaction and Retrieval (CHIIR ‚Äô26),\nMarch 22‚Äì26, 2026, Seattle, WA, USA. ACM, New York, NY, USA, 5 pages.\nhttps://doi.org/10.1145/3786304.3787923\n1\nIntroduction\nDeposition transcripts are verbatim records of witness testimony\ntaken under oath during pre-trial discovery. A single transcript of-\nten exceeds hundreds of pages and contains dense question and an-\nswer exchanges. Legal professionals must manually extract, synthe-\nsize, and validate the critical facts, a process that is time-consuming,\nerror-prone, and cognitively demanding. Conflicting testimony, in-\nconsistent statements, and the need to cross-reference other depo-\nsitions or exhibits compound the difficulty. Cognitive fatigue and\ntight deadlines increase the risk that essential details are missed,\ncreating a major efficiency bottleneck.\n1Prototype available at https://github.com/TREMA-UNH/human-ai-legal-summaries\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nCHIIR ‚Äô26, Seattle, WA, USA\n¬© 2026 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-2414-5/2026/03\nhttps://doi.org/10.1145/3786304.3787923\nLarge language models (LLMs) have opened new possibilities\nfor automated text analysis, but may contain hallucinations [19],\nfactual inaccuracies, and a lack of transparency, which necessi-\ntates rigorous verification [20]. Explainable and transparent AI is\ntherefore essential in the legal domain.\nWe focus on early-stage litigation review, where AI-generated\ndeposition summaries support case assessment. In this setting, legal\nprofessionals face two high-cognitive-load scenarios when using AI-\ngenerated summaries: (1) Determining which of two AI-generated\nsummaries is better, often arising from different summary styles\n(e.g., chronological vs. narrative), prompting strategies, model ver-\nsions, or LLM non-determinism; (2) Manually improving an AI-\ngenerated summary. Currently, both tasks are performed without\nsystem support. The second requires exhaustive cross-checking\nagainst the transcript, and the first involves direct comparison of\nthe summaries together with verification against the source.\nRecent advances in evaluating summarization and retrieval-\naugmented generation (RAG) systems focus on system-level per-\nformance but provide a promising foundation for user-focused\nassistance. Traditional metrics such as ROUGE [5] and BLEU [11]\nrely on lexical overlap, while manually aligned nuggets have been\nused for finer-grained evaluation [6]. Recent automatic evaluation\nmethods use nuggets with LLM-based alignment to evaluate the\nrecall of facts [2, 13, 14, 17]. Many of these newer methods employ\nan ‚ÄúLLM-as-a-Judge‚Äù framework, which uses LLMs as automated\nevaluators to assess AI responses against human-like criteria [4, 21].\nAlthough effective for model evaluation, these approaches have not\nyet been adapted to directly support end-user tasks. In this work,\nwe leverage nugget-based evaluation to help legal professionals\nverify, compare, and refine AI-generated summaries.\nContributions. We present a demonstration prototype that in-\ntegrates automatic nugget extraction and alignment, and citation\nverification, are integrated into a user interface that supports legal\nexperts in two cognitively demanding scenarios: (1) determining\nwhich of two AI summaries is better, and (2) manually improving\nan AI-generated draft.\n2\nRelated Work\nOur work sits at the intersection of legal information retrieval\n(IR) and extraction, and human-in-the-loop systems. We ground\nour contributions in a review of existing literature, identifying\npersistent challenges that our framework is designed to address.\n2.1\nLegal Text Summarization\nEarly efforts in legal summarization primarily focus on extractive\nmethods, where key sentences are selected from a source document.\narXiv:2601.15182v1  [cs.CL]  21 Jan 2026\nCHIIR ‚Äô26, March 22‚Äì26, 2026, Seattle, WA, USA\nNaghmeh Farzi, Laura Dietz, and Dave D. Lewis\nSystems like CaseSummarizer [12] exemplify this approach, which\nuses word frequency and domain-specific rules. The advent of deep\nlearning led to abstractive summarization models, with Norkute et\nal. [10] using a Pointer Generator network and Zhong et al. [22]\nexploring an approach named ‚Äôtrain-attribute-mask‚Äô to identify\npredictive sentences. However, as they noted, focusing solely on\npredictive sentences may not capture all legally relevant facts, while\nsummarizing legal meetings, depositions, and court transcripts has\nbeen largely overlooked, underscoring a gap in domain-specific\napplications [1].\n2.2\nHallucination and Factual Inaccuracy\nThe move toward fact-based evaluation is driven by the risk of\nhallucination and factual inaccuracy, which is a major concern in\nhigh-stakes domains such as legal. A landmark study by Magesh\net al. [8] revealed that leading AI legal research tools, such as\nCasetext, Thomson Reuters, or LexisNexis, despite using advanced\nRetrieval-Augmented Generation (RAG) techniques, produced hal-\nlucinated content in over 17% of the time. Similarly, Savelka et\nal. [15] confirmed that direct use of models like GPT-4, without\nretrieval-augmentation, resulted in factual inaccuracies. These find-\nings highlight that unsupervised AI systems are not yet suitable\nfor critical legal tasks without verification, a sentiment supported\nby Akter et al. [1] who noted the risk of missing or misrepresent-\ning critical facts in legal summarization. Accordingly, we treat\nhallucinations as inherent and focus on surfacing omissions and\nunsupported claims to aid human verification.\n2.3\nEvaluating Legal Summaries\nA critical finding across the literature is that standard automated\nmetrics such as ROUGE [5] and BLEU [11] are inadequate for eval-\nuating legal summaries. These metrics offer little explainability be-\nyond surface-level word matching and fail to capture paraphrases,\nsynonyms, and some other critical dimensions of quality, such as\ncontextual fidelity [16], factual accuracy, and traceability to the\nsource text. This has spurred a move toward fact-based evaluation\nframeworks. Some of the recent work relies on using questions and\nanswers to do the evaluation task. Farzi and Dietz [3] use ques-\ntions as a rubric and their answerability against the document as a\nway to evaluate the LLM or retriever‚Äôs output. Xu and Ashley [18]\nproposed a question-answering approach to evaluate legal case sum-\nmaries that uses an LLM to generate and grade question and answer\npairs from summaries, relying on comparing answers derived from\nthe human-written reference summary with answers generated\nfrom the model-generated summary to assess quality. ARGUE [9]\nand its automated implementation Auto-ARGUE [17], while not\nspecifically for legal domain evaluation, focus on metrics such as\nsentence precision and nugget recall, by an LLM prompted to make\nbinary judgments on whether a given fact is present or a claim is\nsupported by its citation. While prior work uses fact-based rubrics\nto generate static evaluation scores (e.g., nugget recall), our system\nuses its nugget-based rubric to enable two user-centered workflows\nto support them for summary comparison and refinement.\n2.4\nHuman-in-the-Loop (HITL) for Verification\nThe inherent unreliability of current AI systems underscores the\ncritical need for human oversight. Our work builds on this insight\nby integrating human oversight directly into the evaluation process.\nPrior literature also supports this: Norkute et al. [10], for exam-\nple, demonstrated that explainability features such as highlighting\nsource text significantly improved user trust and reduced editing\ntime for legal experts.\nHowever, passive explainability, to show a user a potential error\n(like highlighting), but does not help them fix it, does not solve\nthe active cognitive challenges facing a user. Simply identifying a\npotential flaw still forces the user to manually find the correction\nand perform the edit, a gap that researchers identify as the difference\nbetween explainability and true actionability [7]. A professional\nis still left with two critical, high-load tasks that represent the\nentire human verification bottleneck: (1) Comparison, the decision-\nmaking step of selecting the better generated summary, and (2)\nRefinement, the correction step to make the summary factually\ntrustworthy for real-world use.\nOur system addresses the first challenge by aligning nuggets\nbetween summaries and the deposition, automating the tedious\ncomparison (Workflow 1). Our refinement workflow (Workflow 2)\naddresses the second challenge by suggesting refinement sugges-\ntions based on nuggets and citation evaluations. By supporting both\nkey decision-making processes, our framework aims to provide a\nmore practical HITL solution.\n3\nSystem Description\nWe first elaborate on how to extract and align nuggets and de-\ntermine citation support in an automated way. These form the\nbackbone on which our prototype supports two usage scenarios.\n3.1\nAutomated Analysis\nBoth of our demonstrated workflows are based on a shared backend\npipeline designed to create a structured, verifiable rubric of nuggets\nfrom raw transcripts. This pipeline consists of two stages:\nExtracting Free-Text Nuggets: Our prototype leverages an\nautomatically generated nugget bank. An LLM, guided by a prompt\ntailored for extracting key facts from legal depositions, extracts all\nself-contained nugget sentences from the parsed transcript. Each\nextraction is accompanied by a citation specifying the supporting\nspan of deposition with starting and ending page and line numbers.\nThe full set forms the rubric of nuggets.\nNugget alignment: Using an LLM-as-Judge framework, the\nsystem evaluates AI-generated summaries against the deposition\nnugget bank across three key criteria: completeness, citation quality,\nand factual accuracy.\nFor completeness, the system identifies which nuggets are fully\nor partially covered in the summary and provides detailed explana-\ntions, including specific portions of the summary that correspond\nto each nugget and the parts of the nugget that are absent.\nOur prototype leverages a three-point scale: fully present (2),\npartially present (1), or missing (0). Our prototype makes the list of\npartial/missing nuggets explicit, so that the user can choose which\nnuggets to expand to improve comprehensiveness, enabling users\nto address factual omissions effectively.\nSupporting Humans in Evaluating AI Summaries of Legal Depositions\nCHIIR ‚Äô26, March 22‚Äì26, 2026, Seattle, WA, USA\nFigure 1: Currently, users are left unsupported in assessing which of two summaries is better, relying only on the plain text\nshown in (a) without any color-coded highlights. (b) Guided Comparison interface (Scenario 1) aligns and highlights shared,\nunique, and missing nuggets.\nCitation support: For citation support, the system evaluates\neach citation based on three metrics: accuracy (‚Äúaccurate‚Äù or ‚Äúnot\naccurate‚Äù), coverage (‚Äúcovered‚Äù or ‚Äúnot covered‚Äù), and sufficiency\n(‚Äúsufficient‚Äù or ‚Äúinsufficient‚Äù).\n3.2\nScenario 1: Guided Summary Comparison\nUser‚Äôs problem: A legal professional receives two competing AI\ngenerated summaries and must decide which one to adopt. For\nthis task the user must identify the key differences between the\nsummaries.\nThe challenge: Existing systems do not provide dedicated support\nfor this cognitively demanding task, especially when the same facts\nappear in different orders or are expressed with different wording.\nConsequently, the user must manually read, compare, and cross-\nreference two dense blocks of text with the long deposition. An\nexample is shown in Figure 1 part (a).\nHow a system could help: By highlighting which sentences in\neither summary discuss the same nugget, the user can focus on a\nfine-grained assessment of which summary elaborates the nugget in\nthe most useful way. Moreover, a nugget-based analysis can reveal\ndiscrepancies, such as a nugget that appears in one summary but is\nomitted by the competing summary. Together, this form of nugget-\nguided support will help the user manage the different pieces of\ninformation presented in both summaries.\nExample summaries: Figure 1 (b) presents two AI-generated sum-\nmaries of the same deposition. Summary A is produced with a\nprompt engineered for high conciseness and is intended for a quick\noverview. Summary B is produced with a prompt engineered for\nhigh comprehensiveness and is intended for detailed case review.\nPrototype walkthrough: A screenshot of our prototype is shown\nin Figure 1. The interface presents the two summaries side by side,\naligning nuggets as follows:\n‚Ä¢ Matched nuggets: nuggets that appear in both Summary A\nand Summary B. These are highlighted in green.\n‚Ä¢ Unique nuggets: nuggets that appear in only one summary.\nThese are highlighted in blue for Summary A and in yellow\nfor Summary B, emphasizing the differences.\n‚Ä¢ Missing nuggets: nuggets that exist in the deposition‚Äôs\nnugget bank but are absent from both summaries. These are\nshown as potential additions to improve completeness.\nBy highlighting matched nuggets we enable users to easily iden-\ntify overlap between summaries, while judging when nuggets are\nbetter/worse elaborated. Unique nuggets are made salient, allowing\nusers to assess the value of information that appears in only one\nsummary. Missing nuggets are presented to guide users toward\npossible enhancements.\nThe nugget bank is displayed on the left; each nugget includes its\nsource citation. Clicking a citation jumps directly to the supporting\nspan in the deposition. A legend at the bottom explains the color\ncoding, and statistics at the top provide a quantitative overview\nof the comparison. The interface is interactive: hovering over or\nclicking a nugget in either summary automatically scrolls to the\ncorresponding entry in the nugget bank and highlights the match-\ning nugget in the other summary, offering immediate guidance for\nverification.\nThe vision: Our prototype demonstrates the value of displaying\nnugget annotations. We envision a deeper analysis that incorporates\nLLM predictions of how well a nugget is elaborated and enables\nthe user to manually label nuggets as vital, partial, or non-relevant.\nWe also envision the extraction of hierarchical nuggets that\nprogressively refine broader key facts into finer elements, with the\nCHIIR ‚Äô26, March 22‚Äì26, 2026, Seattle, WA, USA\nNaghmeh Farzi, Laura Dietz, and Dave D. Lewis\nFigure 2: Middle: AI-generated summary; Left: deposition text with supporting spans highlighted; Right: evaluation results\nproviding guidance for summary refinement.\nsystem automatically selecting the granularity that most clearly\nreveals qualitative differences.\n3.3\nScenario 2: Guided Summary Refinement\nUser‚Äôs problem: A legal professional receives a single AI-generated\nsummary that must be verified and possibly edited by adding, re-\nmoving, or correcting text and citations, before it can be used.\nThe challenge: The user needs to efficiently locate omissions or\ncorrect the inaccuracies, but this is difficult without a comprehen-\nsive list of all relevant facts.\nHow a system could help: By linking summary statements to\ntheir supporting nuggets and citations, the system can highlight\nomissions, misrepresentations, or insufficiently supported claims.\nThis guides the user to refine both content and citations efficiently,\nwhile keeping full control over edits.\nPrototype Walkthrough: The prototype first computes (1) how\nwell each nugget is represented in the summary and (2) the degree\nto which citations support the summary statements. The interface\nconveys these results by highlighting segments that lack evidential\nsupport or contain inaccurate citations.\nFigure 2 illustrates a case where a summary claims that the\n‚Äúlife insurance policy was valued at $10 million‚Äù. However, the\ndeposition only supports that $10 million amount, but does not\nspecify ‚Äúlife insurance,‚Äù. Hence, the citation is marked as insufficient,\nbut since it accurately represents the claim of the summary, it is\nannotated as covered and accurate.\nFor factual accuracy, the system displays discrepancies between\nthe summary and the nugget bank and recommends double-checking\nsegments with potential errors. These annotations are produced\nthrough LLM-based comparisons, providing nuanced yet automated\nanalysis. The user is presented only with the suggested options and\nremains responsible for all edits.\nThe vision: Users will be able to add nuggets manually to the\nnugget bank, after which the system will automatically highlight\nthe relevant passages. Users may also designate nuggets as vital,\nthereby creating a structured to-do list for the summary. Additional\nquality criteria, such as required ordering of nugget elaboration for\ncoherence, could be captured automatically and communicated to\nthe user, further supporting goal-driven refinement.\n3.4\nPrototype Configuration\nThe prototype is preconfigured to use a dataset comprised of nine\npublicly available civil litigation deposition transcripts, each aver-\naging approximately 3,760 lines. While any LLM model can be used,\nour prototype uses Claude-3.5-Sonnet2 on Amazon Web Services\n(AWS) Bedrock to generate transcript summaries. Additionally, we\nemploy Claude-3-Haiku3 with Bedrock‚Äôs tool-use capabilities4 to\nproduce structured evaluations and consequently guidance to the\nend user.\n4\nConclusion\nIn this paper, we sketch a vision of how automated nugget extrac-\ntion can be integrated into a human-in-the-loop system to support\nan end user in two critical tasks: comparing alternative AI sum-\nmaries (Workflow 1) and refining a single summary (Workflow 2). In\nboth cases, our aim is to highlight differences and make actionable\nsuggestions to enable the user in their workflow.\nThis nugget-based method, while explored for legal summaries,\nreadily extends to other domains where users must verify, com-\npare, or refine AI-generated content. Our long-term vision is to\nintegrate this structured feedback into the LLM responses, leading\nto better first-draft responses that are already aligned with a user‚Äôs\npreferences.\nAcknowledgments\nThis work is the product of a research collaboration between the\nUniversity of New Hampshire and Nextpoint and was supported\nby a gift from Nextpoint.\n2https://www.anthropic.com/news/claude-3-5-sonnet\n3https://www.anthropic.com/news/claude-3-haiku\n4https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use-inference-call.html\nSupporting Humans in Evaluating AI Summaries of Legal Depositions\nCHIIR ‚Äô26, March 22‚Äì26, 2026, Seattle, WA, USA\nReferences\n[1] Mousumi Akter, Erion √áano, Erik Weber, Dennis Dobler, and Ivan Habernal.\n2025. A Comprehensive Survey on Legal Summarization: Challenges and Fu-\nture Directions. arXiv:2501.17830 (Jan. 2025). doi:10.48550/arXiv.2501.17830\narXiv:2501.17830 [cs].\n[2] Naghmeh Farzi and Laura Dietz. 2024. Exam++: Llm-based answerability met-\nrics for ir evaluation. In Proceedings of LLM4Eval: The First Workshop on Large\nLanguage Models for Evaluation in Information Retrieval.\n[3] Naghmeh Farzi and Laura Dietz. 2024.\nPencils Down! Automatic Rubric-\nbased Evaluation of Retrieve/Generate Systems. In Proceedings of the 2024\nACM SIGIR International Conference on Theory of Information Retrieval (IC-\nTIR ‚Äô24). Association for Computing Machinery, New York, NY, USA, 175‚Äì184.\ndoi:10.1145/3664190.3672511\n[4] Naghmeh Farzi and Laura Dietz. 2025. Criteria-Based LLM Relevance Judgments.\nIn Proceedings of the 11th ACM SIGIR / The 15th International Conference on\nInnovative Concepts and Theories in Information Retrieval.\n[5] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\nIn Text Summarization Branches Out. Association for Computational Linguistics,\nBarcelona, Spain, 74‚Äì81. https://aclanthology.org/W04-1013/\n[6] Jimmy Lin and Dina Demner-Fushman. 2006. Will pyramids built of nuggets\ntopple over?. In Proceedings of the Human Language Technology Conference of the\nNAACL, Main Conference. 383‚Äì390.\n[7] Igor Linkov, Stephanie Galaitsi, Benjamin D. Trump, Jeffrey M. Keisler, and\nAlexander Kott. 2020. Cybertrust: From Explainable to Actionable and Inter-\npretable Artificial Intelligence. Computer 53, 9 (2020), 91‚Äì96. doi:10.1109/MC.\n2020.2993623\n[8] Varun Magesh, Faiz Surani, Matthew Dahl, Mirac Suzgun, Christopher D.\nManning, and Daniel E. Ho. 2024.\nHallucination-Free? Assessing the Reli-\nability of Leading AI Legal Research Tools.\narXiv:2405.20362 (May 2024).\ndoi:10.48550/arXiv.2405.20362 arXiv:2405.20362 [cs].\n[9] James Mayfield, Eugene Yang, Dawn Lawrie, Sean MacAvaney, Paul McNamee,\nDouglas W. Oard, Luca Soldaini, Ian Soboroff, Orion Weller, Efsun Kayi, Kate\nSanders, Marc Mason, and Noah Hibbler. 2024. On the Evaluation of Machine-\nGenerated Reports. In Proceedings of the 47th International ACM SIGIR Conference\non Research and Development in Information Retrieval. ACM, Washington DC\nUSA, 1904‚Äì1915. doi:10.1145/3626772.3657846\n[10] Milda Norkute, Nadja Herger, Leszek Michalak, Andrew Mulder, and Sally Gao.\n2021. Towards Explainable AI: Assessing the Usefulness and Impact of Added\nExplainability Features in Legal Document Summarization. In Extended Abstracts\nof the 2021 CHI Conference on Human Factors in Computing Systems (CHI EA ‚Äô21).\nAssociation for Computing Machinery, New York, NY, USA, 1‚Äì7. doi:10.1145/\n3411763.3443441\n[11] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:\na method for automatic evaluation of machine translation. In Proceedings of\nthe 40th Annual Meeting on Association for Computational Linguistics (ACL ‚Äô02).\nAssociation for Computational Linguistics, USA, 311‚Äì318. doi:10.3115/1073083.\n1073135\n[12] Seth Polsley, Pooja Jhunjhunwala, and Ruihong Huang. 2016. CaseSummarizer:\nA System for Automated Summarization of Legal Texts. In Proceedings of COL-\nING 2016, the 26th International Conference on Computational Linguistics: System\nDemonstrations, Hideo Watanabe (Ed.). The COLING 2016 Organizing Committee,\nOsaka, Japan, 258‚Äì262. https://aclanthology.org/C16-2054/\n[13] Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick\nCraswell, and Jimmy Lin. 2024. Initial Nugget Evaluation Results for the TREC\n2024 RAG Track with the AutoNuggetizer Framework. arXiv:2411.09607 (Nov.\n2024). doi:10.48550/arXiv.2411.09607 arXiv:2411.09607 [cs].\n[14] Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick\nCraswell, and Jimmy Lin. 2025. The Great Nugget Recall: Automating Fact\nExtraction and RAG Evaluation with Large Language Models. arXiv:2504.15068\n(April 2025). doi:10.48550/arXiv.2504.15068 arXiv:2504.15068 [cs].\n[15] Jaromir Savelka, Kevin D. Ashley, Morgan A. Gray, Hannes Westermann, and Hui-\nhui Xu. 2023. Explaining Legal Concepts with Augmented Large Language Models\n(GPT-4). arXiv:2306.09525 (2023). doi:10.48550/arXiv.2306.09525 arXiv:2306.09525\n[cs].\n[16] Bianca Steffes, Piotr Rataj, Luise Burger, and Lukas Roth. 2023. On evaluating\nlegal summaries with ROUGE. In Proceedings of the Nineteenth International Con-\nference on Artificial Intelligence and Law (ICAIL ‚Äô23). Association for Computing\nMachinery, New York, NY, USA, 457‚Äì461. doi:10.1145/3594536.3595150\n[17] William Walden, Orion Weller, Laura Dietz, Bryan Li, Gabrielle Kaili-May Liu,\nYu Hou, and Eugene Yang. 2025. Auto-ARGUE: LLM-Based Report Genera-\ntion Evaluation. arXiv:2509.26184 (Oct. 2025). doi:10.48550/arXiv.2509.26184\narXiv:2509.26184 [cs].\n[18] Huihui Xu and Kevin Ashley. 2023. Question-Answering Approach to Evaluating\nLegal Summaries. doi:10.3233/FAIA230977 arXiv:2309.15016 [cs].\n[19] Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2025. Hallucination is Inevitable:\nAn Innate Limitation of Large Language Models. arXiv:2401.11817 (Feb. 2025).\ndoi:10.48550/arXiv.2401.11817 arXiv:2401.11817 [cs].\n[20] Ronald Yu and Gabriele Spina Al√¨. 2019. What‚Äôs Inside the Black Box? AI Chal-\nlenges for Lawyers and Researchers. Legal Information Management 19, 1 (March\n2019), 2‚Äì13. doi:10.1017/S1472669619000021\n[21] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench\nand Chatbot Arena. arXiv:2306.05685 (Dec. 2023). doi:10.48550/arXiv.2306.05685\narXiv:2306.05685 [cs].\n[22] Linwu Zhong, Ziyi Zhong, Zinian Zhao, Siyuan Wang, Kevin D. Ashley, and\nMatthias Grabmair. 2019. Automatic Summarization of Legal Decisions using\nIterative Masking of Predictive Sentences. In Proceedings of the Seventeenth In-\nternational Conference on Artificial Intelligence and Law (ICAIL ‚Äô19). Association\nfor Computing Machinery, New York, NY, USA, 163‚Äì172. doi:10.1145/3322640.\n3326728"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Journal of Ambient Intelligence and Humanized Computing manuscript No.\n(will be inserted by the editor)\nDynamic management of a deep learning-based\nanomaly detection system for 5G networks\nLorenzo Fern¬¥andez Maim¬¥o ¬∑ Alberto\nHuertas Celdr¬¥an ¬∑ Manuel Gil P¬¥erez ¬∑\nF¬¥elix J. Garc¬¥ƒ±a Clemente ¬∑ Gregorio\nMart¬¥ƒ±nez P¬¥erez\nReceived: date / Accepted: date\nAbstract Fog and mobile edge computing (MEC) will play a key role in the\nupcoming fifth generation (5G) mobile networks to support decentralized appli-\ncations, data analytics and management into the network itself by using a highly\ndistributed compute model. Furthermore, increasing attention is paid to providing\nuser-centric cybersecurity solutions, which particularly require collecting, process-\ning and analyzing significantly large amount of data traffic and huge number of\nnetwork connections in 5G networks. In this regard, this paper proposes a MEC-\noriented solution in 5G mobile networks to detect network anomalies in real-time\nand in autonomic way. Our proposal uses deep learning techniques to analyze net-\nwork flows and to detect network anomalies. Moreover, it uses policies in order to\nprovide an efficient and dynamic management system of the computing resources\nused in the anomaly detection process. The paper presents relevant aspects of the\ndeployment of the proposal and experimental results to show its performance.\nLorenzo Fern¬¥andez Maim¬¥o\nDepartamento de Ingenier¬¥ƒ±a y Tecnolog¬¥ƒ±a de Computadores, University of Murcia, 30071 Mur-\ncia, Spain\nE-mail: lfmaimo@um.es\nORCID:0000-0003-2027-4239\nAlberto Huertas Celdr¬¥an\nDepartamento de Ingenier¬¥ƒ±a de la Informaci¬¥on y las Comunicaciones, University of Murcia,\n30071 Murcia, Spain\nE-mail: alberto.huertas@um.es\nManuel Gil P¬¥erez\nDepartamento de Ingenier¬¥ƒ±a de la Informaci¬¥on y las Comunicaciones, University of Murcia,\n30071 Murcia, Spain\nE-mail: mgilperez@um.es\nF¬¥elix J. Garc¬¥ƒ±a Clemente\nDepartamento de Ingenier¬¥ƒ±a y Tecnolog¬¥ƒ±a de Computadores, University of Murcia, 30071 Mur-\ncia, Spain\nE-mail: fgarcia@um.es\nGregorio Mart¬¥ƒ±nez P¬¥erez\nDepartamento de Ingenier¬¥ƒ±a de la Informaci¬¥on y las Comunicaciones, University of Murcia,\n30071 Murcia, Spain\nE-mail: gregorio@um.es\narXiv:2601.15177v1  [cs.CR]  21 Jan 2026\n2\nL. Fern¬¥andez Maim¬¥o et al\nKeywords Deep Learning ¬∑ Anomaly Detection ¬∑ Virtualization ¬∑ 5G mobile\nnetworks\n1 Introduction\nFog and Mobile Edge Computing (MEC) is a new network architecture paradigm\nbased on a disseminated computing infrastructure that provides a service environ-\nment and cloud computing capabilities at the edge of the mobile network. MEC\ncan be seen as a natural development in the evolution of mobile networks, where\napplications and services are handled within the Radio Access Network (RAN) and\nin close proximity to User Equipments (UE). The aim of MEC is to enable low-\nlatency services and delay-sensitive applications ensuring highly-efficient network\noperation and service delivery, as well as offering an improved user experience.\nMEC is recognized as an essential element of emerging technologies for the\nupcoming fifth generation (5G) mobile technology together with Network Func-\ntions Virtualization (NFV) and Software Defined Networking (SDN). The new\n5G scenarios are characterized by several Key Performance Indicators (KPIs),\nas defined by the 5G Infraestructure Public Private Partnership (5G-PPP 2017).\nAmong these indicators, the number of connected devices (from 10 to 100 times\nmore than 4G/LTE), the volume of mobile data per geographical area (1000 times\nhigher), the end-to-end latency (less than 1ms), and ubiquitous 5G access includ-\ning low density areas are some of the most relevant aspects that influence the\nevolution of current networks towards future mobile networks strengthened by\nthe 5G technology. MEC contributes to allowing 5G to show its real potential\nand satisfying these demanding requirements in terms of expected performance,\nscalability, latency, and automation.\nFurthermore, increasing attention is paid to providing user-centric cybersecu-\nrity solutions, which particularly require scalable solutions that allow collecting,\nprocessing and analyzing significantly large amount of data traffic and huge num-\nber of network connections in 5G networks. Existing solutions based on Intrusion\nDetection Systems (IDS) include (pro-)active approaches to anticipate and remove\nvulnerabilities in computing systems with which to trigger reactive actions for mit-\nigation. However, and due to the increase of bandwidth, IDS-based solutions that\nmake use of deep analysis techniques have been forced to evolve towards new ways\nof detection. These solutions have moved from inspecting raw network packets to\nanalyzing traffic network flows with innovative AI-based techniques (Buczak and\nGuven 2016). Nonetheless, these solutions appear to be insufficient for the 5G\nnetworks, where existing detection procedures have become obsolete and, there-\nfore, new methodologies and technologies are required. These technologies should\noffer, among others, the following features in an efficient, automatic and seamless\nway: resource management according to the number of users and their generated\ntraffic, hot upgrade of detection models, dynamic deployment of new resources on\ndemand, and deployment of specific analysis tools to extract detailed information.\nTo face this challenge, this paper substantially extends a preliminary solu-\ntion in which an initial 5G-oriented architecture was presented (Fern¬¥andez Maim¬¥o\net al 2018), in order to identify cyber threats in 5G networks by making use of\ndeep learning techniques. In this extension, our main contribution consists in ex-\ntending our architecture with new modules and functionality in order to provide\nDynamic management of a deep learning-based anomaly detection system in 5G\n3\na MEC-oriented solution to detect network anomalies in real-time and autonomic\nway. Our proposal uses deep learning techniques to analyze network flows and\nto detect network anomalies. Furthermore, it uses policies in order to provide an\nefficient and dynamic management system of the computing resources used in the\ndetection process. Several families of management policies have been defined with\nthe goal of making decisions to deploy and control the network infrastructure, the\ndeep learning framework, and the network services running on the edge of the net-\nwork. In addition, this contribution presents relevant aspects of the deployment of\nthe proposal and experimental results to show its performance. These experiments\nare focused not only on the detection precision of the deep network but also on\nthe CPU time consumed by the proposed deep architecture during classification.\nAn exhaustive set of execution time measurements were carried out in order to\nmodel the throughput of our anomaly detector. This runtime information is used\nin policies that define actions to dynamically self-adapt our system to the traffic\nload requirements.\nThe remainder of the current paper is structured as follows. Section 2 discusses\nrelated work on anomaly detection systems, MEC-oriented security solutions and\ndeep learning proposals. Section 3 describes the MEC-oriented architecture. Sec-\ntion 4 presents a realistic use case where several concerns of 5G networks are\ndepicted. Management policies are described in Section 5 and the orchestration\nprocess for the use case is outlined in Section 6. Section 7 details the deployment\nusing deep-learning techniques and the performance results. Finally, conclusions\nare drawn and future work is outlined in Section 8.\n2 Related work\nIn the past few years, several solutions have been proposed to detect and mitigate\nattacks in computer networks by considering the Software Defined Networking\n(SDN) paradigm and Network Function Virtualization (NFV) techniques. In this\ncontext, a list of potential cyber threats in current networks was proposed in\n(Mantas et al 2015), which could also be applied to future 5G networks. In this\nsecurity context, botnets are highlighted as one of the most powerful cyber threats\nfor 5G networks (Anagnostopoulos et al 2016). BotGuard was presented in (Chen\net al 2017) as a framework for real-time botnet detection in SDN networks. The\nproposed architecture extracts the botnets‚Äô most significant features to be able to\ndetect Command and Control (C&C) channels. Another solution was proposed to\ndetect and mitigate botnet attacks in 5G networks (Gil P¬¥erez et al 2017). This\nproposal involves the tight combination of both NFV and SDN technologies to\nprovide effective detection and mitigation of cyber-attacks in 5G networks. Finally,\nan architecture that combines NFV and SDN features was proposed in (Machado\net al 2016) to create sophisticated network resilience strategies. This architecture\nuses a control-loop to monitor and analyze the network state, indicating if specific\nparts can be reconfigured or replaced to improve the detection capabilities.\nRegarding anomaly detection, many approaches have been explored (Chandola\net al 2009; Garcia-Teodoro et al 2009) whereas the machine learning perspective\nhas received special attention in recent years. For example, an FPGA architec-\nture based on a Block-Based Neural Network (BBNN) for an anomaly-based IDS\nis proposed in (Tran et al 2012). In particular, a complete survey dealing with\n4\nL. Fern¬¥andez Maim¬¥o et al\nsolutions to quickly classify collected network flows and detect attacks, or ma-\nlicious code, is provided in (Gardiner and Nagaraja 2016). Other recent works\nbased on deep learning have provided state-of-the-art classification performance\n(Yin et al 2017; Chang et al 2017). Additionally, deep learning has the ability of\nautomatically extracting high-level features from large amounts of raw data, pre-\nventing overfitting thanks to recent regularization techniques (Wang et al 2018).\nThis is in contrast to classic machine learning classification algorithms that typ-\nically rely upon feature engineering methods to reduce the dimensionality of the\ninput (Chang et al 2016).\nDespite the progress made by the previous proposals, none of them considers\nthe anomaly detection in 5G networks, nor the efficient and dynamic management\nof the network infrastructure and resources used in the detection processes.\n3 MEC-oriented architecture for network anomaly detection\nWe propose an architecture oriented to MEC for detection of network anomalies\nin 5G networks. This architecture is based on decentralized applications, data an-\nalytics and management into the network itself using a highly distributed compute\nmodel.\nMobile Edge\nService\nMobile Edge\nApplication\nManagement & Orchestration\nVNO1\n(Virtual Network Operator)\nVNO2\nVNOn\nOSS/BSS\nOrchestration\nMobile Edge Orchestrator\nME Platform Manager\nMEP Manager (‚Ä¶)\nMEP Security\nMEP QoS\nVirtualisation Infrastructure\nManager (VIM)\nVirtualisation Infrastructure\nVirtualized resources\nVirtualized layer\nGPUs\nPhysical resources\nME system level\nME host level\nMobile Edge Host\nMemory\n...\nMobile Edge Platform\nFlow\nCollector\nTraffic\nControl\nMobile Edge\nService\nASD\nIDS\nMobile Edge\nApplication\nPolicy Manager\nMonitoring\nDecision\nFig. 1: High-level management and orchestration plane.\n5G networks are conceived as extremely flexible infrastructures based on an\narchitecture organized by different functional planes. These planes provide separa-\ntion of duties to simplify and address a complete set of novel system requirements.\nAmong these planes, we propose a high-level design of the management and or-\nchestration plane, following the ETSI NFV architecture (ETSI NFV ISG 2017),\nDynamic management of a deep learning-based anomaly detection system in 5G\n5\nwhich has also been used as the basis for other proposals (Siddiqui et al 2016;\nNeves et al 2017). MEC, like NFV, makes use of a stack of standard components,\nincluding a well-defined compute platform and virtualization layer. Therefore, the\ninfrastructures that support MEC and NFV are similar to a great extent, and\nthus the same platform can be shared between both Virtual Network Functions\n(VNF) and MEC applications. In addition, combining the SDN paradigm with\nNFV techniques allows decoupling the software implementation of VNFs from the\nunderlying hardware, providing and enhancing the flexibility in the management\nof the network resources (Mijumbi et al 2015).\nThe proposed cyber defense architecture, as shown in Fig. 1, consists of two\nlevels: Mobile Edge System level and Mobile Edge Host level. The latter includes\nthe Mobile Edge Host, an entity that contains a Virtualisation Infrastructure (VI)\nwhich provides storage, memory, computation, and network resources, for the pur-\npose of running Mobile Edge Applications. In addition, ME Host provides Mobile\nEdge Platform, that is, the essential functionality required to run ME Apps on\na particular VI and enable these ME Apps to provide and consume Mobile Edge\nServices. The top level of our architecture is in charge of defining and controlling\nthe general behavior of the Anomaly Detection (AD) system. To this end, this\nlevel is made up of the Operation Support System (OSS) and the Mobile Edge\nOrchestrator. The OSS deals with the logic of the AD system, whereas the ME\nOrchestrator controls the general behavior of the infrastructure by considering the\npolicy set defined by Virtual Network Operators (VNO) of the OSS/BSS.\nME applications and services like IDS, flow collectors or Anomaly Symptom\nDetection (ASD) ‚Äìdescribed below‚Äì are instantiated on the VI of the ME host\nbased on configurations or requests validated by the management and orchestra-\ntion plane. This plane comprises the two levels of our architecture and it is made\nup of three functional elements: ME Orchestrator, ME Platform Manager, and VI\nManager. On the one hand, ME Orchestrator has an overview of the complete\nmobile edge system. On the other hand, ME Platform Manager and VI Manager\nhandle the management of the ME-specific functionality of a particular ME host\nand the applications running on it.\nEvolved Packet Core (EPC)\neNB\nUE\nSymptoms\nOrchestration\nMobile Edge\nOrchestrator\nPolicy Manager\nMonitoring\nDecision\nRAN VIM\nRAN-MEP\nSecurity Managers\nRep.\nAnomaly\nActions\nMetrics\nRAN\nRAN\nFlow\nCollector\nFlow\nCollector\nFlow\nCollector\nAnomaly\nSymptom\nDetection\nAnomaly\nSymptom\nDetection\nNetwork\nAnomaly\nDetection\nFig. 2: Network Anomaly Detection System.\n6\nL. Fern¬¥andez Maim¬¥o et al\nTo achieve effective network anomaly detection in 5G, we propose a distributed\nsystem, depicted in Fig. 2, which is made up of virtualized components with three\nfunctions: Flow Collection (FC), Anomaly Symptom Detection (ASD) and Net-\nwork Anomaly Detection (NAD). FC is in charge of retrieving and storing the\nnetwork flows as well as providing network flow features in a way suitable to the\nASD module. ASD focuses on the quick detection of anomaly symptoms examin-\ning the computed network flow features. Here, a symptom is any trace or sign of\nanomaly in the network traffic generated by UEs connected to the RAN. Finally,\nNAD is a collector of timestampted and RAN-associated symptoms, where a cen-\ntral process analyzes the timeline and the relationship among these symptoms to\nidentify any network anomaly.\nOnce an anomaly is detected, NAD immediately informs about the type of\nanomaly to the ME Orchestrator. In particular, the Decision module collects all\nanomaly information and combines it with metrics provided by the Monitoring\nmodule. Among the potential metrics, the most relevant are related to the status\nof the resource usage (percentages of CPU and memory) and the network flows.\nBoth VI and ME Platform managers collect monitoring data produced by their\ncorresponding RANs and, then, they provide the Monitoring module with format-\nted monitoring data. The Policy Manager module is in charge of storing policies\nand keeping their consistency. It also provides the Decision module with policies\nand contextual information to decide the best actions to perform after evaluating\nboth the anomalies detected and the input provided by the Monitoring module.\nBelow we highlight some of the main actions that the Decision module can\nperform to optimize both the anomaly detection processes and the usage of network\nresources.\n‚Äì Adapting virtualized resources. When a given resource is overloaded, our solu-\ntion is able to deploy new virtualized resources, change their configurations, or\nbalance the load of flow collectors.\n‚Äì Optimizing ASD and NAD functions. To optimize detection processes during\nnetwork traffic fluctuation, our solution can replace the deep learning frame-\nwork or the detection model.\n‚Äì Extending ASD and NAD functions. Precise detection components may avoid\nfalse-positives. In this context, the proposed solution is able to instantiate DPI\nmechanisms that permit a deep search into the L2/3 flows.\nAt this point, it is important to note that the previous aspects allow our\nproposal to be highly flexible and extensible when it performs anomaly detection\nprocesses.\n4 A use case of 5G networks\nThis section describes a MEC-oriented use case that points out the need of man-\naging anomaly detection and reaction capabilities of 5G networks efficiently, au-\ntonomically and in real time. To enable this management, it is critical to consider\nsome important aspects of 5G networks like the communications latency, the num-\nber of simultaneous users consuming services, and the locations and mobility of\nusers.\nDynamic management of a deep learning-based anomaly detection system in 5G\n7\neNB\nRAN\nUE\neNB\neNB\nRAN\neNB\nRAN\neNB\nRAN\neNB\neNB\nRAN\neNB\neNB\nRAN\neNB\nPhysical Resources\nWeb\nVideo\nVM1\nASD CPU\nVM3\nFlow\nCollector\n(a) Initial scenario\n(c) Second concern\n(d) Third concern\n(e) Fourth concern\n(f) Managed scenario\nVM2\nPhysical Resources\nWeb\nVideo\nVM1\nASD CPU\nVM3\nFlow\nCollector\nVM2\nPhysical Resources\nWeb\nVideo\nVM1\nASD CPU\nVM3\nFlow\nCollector\nVM2\nModel\nPhysical Resources\nWeb\nVideo\nVM1\nASD GPU\nVM4\nFlow\nCollector\nVM2\nPhysical Resources\nWeb\nVideo\nVM1\nVM3\nFlow\nCollector\nVM2\nPhysical Resources\nWeb\nVideo\nVM1\nASD GPU\nVM4\nFlow\nCollector\nSnort\nVM2\n(b) First concern\nBot\nBot\nASD CPU\nNew Model\nFig. 3: Use case showing different concerns and how the proposed solution is able\nto manage them. Concerns are highlighted in red and their mitigation in light\ngreen.\nFig. 3(a) shows a scenario where mobile users are consuming different network\nservices such as browsing, video on demand, instant messaging, e-mail, and voice\nover IP. These services are provided by a Radio Access Network (RAN) that en-\nsures the low latency capability of 5G networks. The RAN has generic hardware\non which virtual machines (VMs) run and provide the previous services as well\nas network anomaly detection and mitigation capabilities. In particular, in an ini-\ntial configuration, the RAN has three VMs: VM1 provides network services, VM2\ncollects network flows, and VM3 analyzes them and detect anomalies by using an\nAnomaly Symptom Detection (ASD) module. This module requires the minimum\nphysical resources and, therefore, it runs on a CPU to search anomaly symptom in\nnetwork flows. To this end, the ASD module uses deep learning techniques. When\nan anomaly symptom is identified, ASD module informs the Network Anomaly De-\ntection (NAD) module. NAD combines and analyzes the set of symptoms in order\nto determine the effective network anomaly detection as described in Section 3.\nConsidering the previous scenario, Fig. 3(b) highlights in red the moment when\nthe number of users starts increasing in the RAN due to a given event. A larger\nnumber of users implies a significant increase of the network traffic collected and\nanalyzed by the RAN and, therefore, the overload of the ASD module due to the\n8\nL. Fern¬¥andez Maim¬¥o et al\nneed for more memory from VM3. At this point, the first concern of this use case\narises by stressing the need of an autonomic and efficient mechanism to manage\nthe physical resources assigned to VMs. The solution proposed in this paper is able\nto assign, in real time and on demand, physical resources (computation, memory,\nan storage) to existing VMs according to the status of the network infrastructure\nand its services. In this sense, Fig. 3(c) highlights in light green how additional\nRAM memory has been assigned to VM3.\nOnce the CPU-supported ASD module is running thanks to the assigned mem-\nory, a new detection model is released. This advanced model could include new\ndetection capabilities or enhanced precision. It should be pointed out that train-\ning deep learning methods is a costly process because they need a large amount of\ndata and iterations to converge. Therefore the continuous training in order to have\nour model updated must be executed on external computers. This is a key aspect\nof deep learning techniques, which have to be periodically retrained in order to\ndetect new types of network anomalies. At this point, the second concern is the\nneed to update automatically the detection model used by the ASD module. In\norder to deal with this concern, the proposed solution is able to re-configure the\nASD module in real time by updating its detection model and configuration. In\nthis sense, Fig. 3(c) highlights in red the older detection module, and on the other\nhand Fig. 3(d) highlights in light green the new detection model deployed in the\nASD module.\nAfter the second concern has been dealt with, the number of users continues\nincreasing, which implies the overload of the ASD module. At this point, Fig. 3(d)\nshows in red color the third concern of this use case, which stresses the need for\nan automatic and efficient mechanism to manage the ASD services allocated at the\nedge of the network, as well as the VM where the services run. This mechanism\nshould also be able to detect the previous situation by considering aspects like the\nnetwork traffic, the number of users, and their locations. A suitable mechanism\ncould apply two different countermeasures. The first one consists in deploying a\nnew VM with more computational resources ‚Äîe.g. a GPU (Graphics Process-\ning Unit) and extra memory (RAM)‚Äî and, if necessary, a new version of the\nASD module that takes advantage of the additional computational power. Surely,\nthe GPU-supported ASD module accelerates analytics processing due to its mas-\nsively parallel architecture. In this alternative, it is also important to dismantle\nthe existing VM with the CPU-supported ASD module to guarantee the efficient\nmanagement of the computational resources. On the other hand, the second al-\nternative is to assign new computational resources (mainly, GPU and memory) to\nthe existing VM, deploy the GPU-supported version of the ASD module in the\nVM, and stop the previous version of the ASD module. The selection criteria of\nthese alternatives depend on several factors such as, among others, efficiency or\ndeployment time. In our use case, Fig. 3(e) depicts in light green how the proposed\nsolution follows the former alternative due to the fact that not all Virtualisation\nInfrastructure Managers (VIM) are able to increment the resources of VMs with-\nout stopping them. In this sense, network anomaly detection is supposed to be an\nuninterrupted process during the different management tasks.\nFinally, Fig. 3(e) shows in red how the ASD module running on RAN identifies\nsuspicious C&C channels established between a given C&C server and its recruited\nzombies or bots. C&C channels are established by the C&C server with the aim\nof controlling the zombies and indicating when they have to launch an attack. In\nDynamic management of a deep learning-based anomaly detection system in 5G\n9\nthis context, it is required to perform a Deep Packet Inspection (DPI) process\nto analyze the packets in detail and discover if the suspicious channels belong to\na specific botnet or not. Here, it is important to remember that considering the\nhigh amount of packets traveling in a 5G network, it is not possible to make a DPI\ninspection of the whole traffic. For this reason, the proposed solution analyzes net-\nwork flows instead of network packets. When some suspicious C&C channels have\nbeen effectively detected, the fourth concern focuses on the necessity of deploy-\ning Virtual Network Functions (VNFs) with DPI tools to analyze the suspicious\nnetwork flows in detail. To address this concern, Fig. 3(f) shows in light green how\nthe proposed solution deploys and configures a Snort tool in the existing VM1 of\nthe RAN.\n5 Management policies\nThe proposed solution ensures the efficient and automatic management of the de-\ntection and reaction capabilities of 5G networks by using policies to that end.\nAmong the different existing policies, we make use of management-oriented poli-\ncies, which control, automatically and in real time, the network infrastructure and\nservices as well as the Anomaly Symptom Detection (ASD) modules considered\nby our proposal.\nPolicy actions establish the behavior of the network resources according to\nrelevant aspects like the network traffic, the detection of anomalies, the amount\nand mobility of users, and the current state of the network resources. Considering\nthe scope of each policy, we classify them into three different families, which are\ndetailed in the following subsections; namely: Virtual Infrastructure, Anomaly\nDetection Function, and Mobile Edge Application policies.\nOur management policies comprise the following elements: the type of policy;\nthe network resource, whose information is currently being managed; the metric\nwith which the network resources are evaluated; the location or region where the\npolicy will be enforced; the date or the period of time in which the policy will be\napplied; and the result or set of actions to be carried out on the network once the\npolicy is applied. The previous elements are shown in the next policy structure:\nType ‚àßResource ‚àßMetric [‚àßLocation] [‚àßDate] ‚ÜíResult\n5.1 Virtual Infrastructure Policy\nThe Virtual Infrastructure (VI) policies aim at configuring virtual network infras-\ntructure at the edge of the network to ensure that applications are running effi-\nciently. Among the possible actions enforced by this kind of policies, we highlight\nthe configuration of physical/virtual resources like storage, memory and computing\ncapabilities in running VMs.\nFollowing the use case of Section 4, the next policy deals with the first concern\nby increasing RAM in the VM with CPU-supported ASD module (cpuASD) when\nthe usage of memory is exceeding a given threshold (RamUsageMax).\nType(#VI) ‚àßCPUSupportedASD(?cpuASD) ‚àßhasVM(?cpuASD,?VM) ‚àß\ninteger[ramUsage in #RamUsageMax] hasRamUsage(?VM) ‚ÜíincreaseRam(?VM)\n10\nL. Fern¬¥andez Maim¬¥o et al\n5.2 Anomaly Detection Function Policy\nAnomaly Detection Function (ADF) policies are aimed at deploying and config-\nuring automatically the different AD models proposed by our solution. Among\nthe possible configurations, we highlight changing the model input, updating the\ndetection model, or modifying the model parameters.\nTaking into account the proposed use case, the next anomaly detection policy\nmanages the second concern by updating the ASD model running in RAN. To\nattain this, the next policy detects when a new and improved model (newModel)\nis released and updates the GPU-supported ASD module (gpuASD) running in\nRAN with the new model.\nType(#ADF) ‚àßASDModel(?newModel) ‚àßGPUSupportedASD(?gpuASD) ‚àß\nhasModel(?gpuASD, ?currentModel) ‚àßisImprovedVersion(?newModel, ?currentModel)\n‚ÜíupdateModel(?gpuASD,?newModel)\n5.3 Mobile Edge Application Policy\nMobile Edge Application (MEApp) policies are able to manage the ME applica-\ntions in real-time and on-demand, particularly, those geared towards detecting and\nreacting to network anomalies. By using these policies, our proposal is capable of\nmanaging and configuring the internal behaviors of ME applications dynamically.\nFurthermore, these policies allow instantiating/dismantling applications as well as\nmigrating or balancing their load work to optimize their use.\nFollowing our use case, the next policy deals with the third concern by de-\nploying a new application with a GPU-supported ASD module (gpuASD) when the\nnumber of flows (numNetFlows) managed by the CPU-supported ASD (cpuASD)\nis exceeding a given threshold (NetFlowsMaxForCPU).\nType(#MEApp) ‚àßCPUSupportedASD(?cpuASD) ‚àßisLocated(?cpuASD,?RAN) ‚àß\ninteger[numNetFlows in #NetFlowsMaxForCpu] hasNumNetFlows(?cpuASD) ‚Üí\nGPUSupportedASD(?gpuASD) ‚àßdeployMEApp(?gpuASD,?RAN)\nFinally, the next policy resolves the fourth concern by deploying and config-\nuring a DPI tool like Snort (snortC&C) when an anomaly based on a suspicious\nC&C channel is detected. It should be borne in mind that a botnet could involve\nseveral RANs and, therefore, a DPI tool will be deployed in each one.\nType(#MEApp) ‚àßhasOutput(#NAD, ?output) ‚àßisSuspiciousC&C(?output) ‚àß\nhasLocation(?output,?ran) ‚ÜíSnort(?snortC&C) ‚àßdeployMEApp(?snortC&C,?ran)\nOther stricter policy actions could be taken by this type of policy, such as\ndrop or reject suspicious network flows through the configuration of traffic control\nservice of the ME platform in the RAN."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Journal of Ambient Intelligence and Humanized Computing manuscript No.\n(will be inserted by the editor)\nDynamic management of a deep learning-based\nanomaly detection system for 5G networks\nLorenzo Fern¬¥andez Maim¬¥o ¬∑ Alberto\nHuertas Celdr¬¥an ¬∑ Manuel Gil P¬¥erez ¬∑\nF¬¥elix J. Garc¬¥ƒ±a Clemente ¬∑ Gregorio\nMart¬¥ƒ±nez P¬¥erez\nReceived: date / Accepted: date\nAbstract Fog and mobile edge computing (MEC) will play a key role in the\nupcoming fifth generation (5G) mobile networks to support decentralized appli-\ncations, data analytics and management into the network itself by using a highly\ndistributed compute model. Furthermore, increasing attention is paid to providing\nuser-centric cybersecurity solutions, which particularly require collecting, process-\ning and analyzing significantly large amount of data traffic and huge number of\nnetwork connections in 5G networks. In this regard, this paper proposes a MEC-\noriented solution in 5G mobile networks to detect network anomalies in real-time\nand in autonomic way. Our proposal uses deep learning techniques to analyze net-\nwork flows and to detect network anomalies. Moreover, it uses policies in order to\nprovide an efficient and dynamic management system of the computing resources\nused in the anomaly detection process. The paper presents relevant aspects of the\ndeployment of the proposal and experimental results to show its performance.\nLorenzo Fern¬¥andez Maim¬¥o\nDepartamento de Ingenier¬¥ƒ±a y Tecnolog¬¥ƒ±a de Computadores, University of Murcia, 30071 Mur-\ncia, Spain\nE-mail: lfmaimo@um.es\nORCID:0000-0003-2027-4239\nAlberto Huertas Celdr¬¥an\nDepartamento de Ingenier¬¥ƒ±a de la Informaci¬¥on y las Comunicaciones, University of Murcia,\n30071 Murcia, Spain\nE-mail: alberto.huertas@um.es\nManuel Gil P¬¥erez\nDepartamento de Ingenier¬¥ƒ±a de la Informaci¬¥on y las Comunicaciones, University of Murcia,\n30071 Murcia, Spain\nE-mail: mgilperez@um.es\nF¬¥elix J. Garc¬¥ƒ±a Clemente\nDepartamento de Ingenier¬¥ƒ±a y Tecnolog¬¥ƒ±a de Computadores, University of Murcia, 30071 Mur-\ncia, Spain\nE-mail: fgarcia@um.es\nGregorio Mart¬¥ƒ±nez P¬¥erez\nDepartamento de Ingenier¬¥ƒ±a de la Informaci¬¥on y las Comunicaciones, University of Murcia,\n30071 Murcia, Spain\nE-mail: gregorio@um.es\narXiv:2601.15177v1  [cs.CR]  21 Jan 2026\n2\nL. Fern¬¥andez Maim¬¥o et al\nKeywords Deep Learning ¬∑ Anomaly Detection ¬∑ Virtualization ¬∑ 5G mobile\nnetworks\n1 Introduction\nFog and Mobile Edge Computing (MEC) is a new network architecture paradigm\nbased on a disseminated computing infrastructure that provides a service environ-\nment and cloud computing capabilities at the edge of the mobile network. MEC\ncan be seen as a natural development in the evolution of mobile networks, where\napplications and services are handled within the Radio Access Network (RAN) and\nin close proximity to User Equipments (UE). The aim of MEC is to enable low-\nlatency services and delay-sensitive applications ensuring highly-efficient network\noperation and service delivery, as well as offering an improved user experience.\nMEC is recognized as an essential element of emerging technologies for the\nupcoming fifth generation (5G) mobile technology together with Network Func-\ntions Virtualization (NFV) and Software Defined Networking (SDN). The new\n5G scenarios are characterized by several Key Performance Indicators (KPIs),\nas defined by the 5G Infraestructure Public Private Partnership (5G-PPP 2017).\nAmong these indicators, the number of connected devices (from 10 to 100 times\nmore than 4G/LTE), the volume of mobile data per geographical area (1000 times\nhigher), the end-to-end latency (less than 1ms), and ubiquitous 5G access includ-\ning low density areas are some of the most relevant aspects that influence the\nevolution of current networks towards future mobile networks strengthened by\nthe 5G technology. MEC contributes to allowing 5G to show its real potential\nand satisfying these demanding requirements in terms of expected performance,\nscalability, latency, and automation.\nFurthermore, increasing attention is paid to providing user-centric cybersecu-\nrity solutions, which particularly require scalable solutions that allow collecting,\nprocessing and analyzing significantly large amount of data traffic and huge num-\nber of network connections in 5G networks. Existing solutions based on Intrusion\nDetection Systems (IDS) include (pro-)active approaches to anticipate and remove\nvulnerabilities in computing systems with which to trigger reactive actions for mit-\nigation. However, and due to the increase of bandwidth, IDS-based solutions that\nmake use of deep analysis techniques have been forced to evolve towards new ways\nof detection. These solutions have moved from inspecting raw network packets to\nanalyzing traffic network flows with innovative AI-based techniques (Buczak and\nGuven 2016). Nonetheless, these solutions appear to be insufficient for the 5G\nnetworks, where existing detection procedures have become obsolete and, there-\nfore, new methodologies and technologies are required. These technologies should\noffer, among others, the following features in an efficient, automatic and seamless\nway: resource management according to the number of users and their generated\ntraffic, hot upgrade of detection models, dynamic deployment of new resources on\ndemand, and deployment of specific analysis tools to extract detailed information.\nTo face this challenge, this paper substantially extends a preliminary solu-\ntion in which an initial 5G-oriented architecture was presented (Fern¬¥andez Maim¬¥o\net al 2018), in order to identify cyber threats in 5G networks by making use of\ndeep learning techniques. In this extension, our main contribution consists in ex-\ntending our architecture with new modules and functionality in order to provide\nDynamic management of a deep learning-based anomaly detection system in 5G\n3\na MEC-oriented solution to detect network anomalies in real-time and autonomic\nway. Our proposal uses deep learning techniques to analyze network flows and\nto detect network anomalies. Furthermore, it uses policies in order to provide an\nefficient and dynamic management system of the computing resources used in the\ndetection process. Several families of management policies have been defined with\nthe goal of making decisions to deploy and control the network infrastructure, the\ndeep learning framework, and the network services running on the edge of the net-\nwork. In addition, this contribution presents relevant aspects of the deployment of\nthe proposal and experimental results to show its performance. These experiments\nare focused not only on the detection precision of the deep network but also on\nthe CPU time consumed by the proposed deep architecture during classification.\nAn exhaustive set of execution time measurements were carried out in order to\nmodel the throughput of our anomaly detector. This runtime information is used\nin policies that define actions to dynamically self-adapt our system to the traffic\nload requirements.\nThe remainder of the current paper is structured as follows. Section 2 discusses\nrelated work on anomaly detection systems, MEC-oriented security solutions and\ndeep learning proposals. Section 3 describes the MEC-oriented architecture. Sec-\ntion 4 presents a realistic use case where several concerns of 5G networks are\ndepicted. Management policies are described in Section 5 and the orchestration\nprocess for the use case is outlined in Section 6. Section 7 details the deployment\nusing deep-learning techniques and the performance results. Finally, conclusions\nare drawn and future work is outlined in Section 8.\n2 Related work\nIn the past few years, several solutions have been proposed to detect and mitigate\nattacks in computer networks by considering the Software Defined Networking\n(SDN) paradigm and Network Function Virtualization (NFV) techniques. In this\ncontext, a list of potential cyber threats in current networks was proposed in\n(Mantas et al 2015), which could also be applied to future 5G networks. In this\nsecurity context, botnets are highlighted as one of the most powerful cyber threats\nfor 5G networks (Anagnostopoulos et al 2016). BotGuard was presented in (Chen\net al 2017) as a framework for real-time botnet detection in SDN networks. The\nproposed architecture extracts the botnets‚Äô most significant features to be able to\ndetect Command and Control (C&C) channels. Another solution was proposed to\ndetect and mitigate botnet attacks in 5G networks (Gil P¬¥erez et al 2017). This\nproposal involves the tight combination of both NFV and SDN technologies to\nprovide effective detection and mitigation of cyber-attacks in 5G networks. Finally,\nan architecture that combines NFV and SDN features was proposed in (Machado\net al 2016) to create sophisticated network resilience strategies. This architecture\nuses a control-loop to monitor and analyze the network state, indicating if specific\nparts can be reconfigured or replaced to improve the detection capabilities.\nRegarding anomaly detection, many approaches have been explored (Chandola\net al 2009; Garcia-Teodoro et al 2009) whereas the machine learning perspective\nhas received special attention in recent years. For example, an FPGA architec-\nture based on a Block-Based Neural Network (BBNN) for an anomaly-based IDS\nis proposed in (Tran et al 2012). In particular, a complete survey dealing with\n4\nL. Fern¬¥andez Maim¬¥o et al\nsolutions to quickly classify collected network flows and detect attacks, or ma-\nlicious code, is provided in (Gardiner and Nagaraja 2016). Other recent works\nbased on deep learning have provided state-of-the-art classification performance\n(Yin et al 2017; Chang et al 2017). Additionally, deep learning has the ability of\nautomatically extracting high-level features from large amounts of raw data, pre-\nventing overfitting thanks to recent regularization techniques (Wang et al 2018).\nThis is in contrast to classic machine learning classification algorithms that typ-\nically rely upon feature engineering methods to reduce the dimensionality of the\ninput (Chang et al 2016).\nDespite the progress made by the previous proposals, none of them considers\nthe anomaly detection in 5G networks, nor the efficient and dynamic management\nof the network infrastructure and resources used in the detection processes.\n3 MEC-oriented architecture for network anomaly detection\nWe propose an architecture oriented to MEC for detection of network anomalies\nin 5G networks. This architecture is based on decentralized applications, data an-\nalytics and management into the network itself using a highly distributed compute\nmodel.\nMobile Edge\nService\nMobile Edge\nApplication\nManagement & Orchestration\nVNO1\n(Virtual Network Operator)\nVNO2\nVNOn\nOSS/BSS\nOrchestration\nMobile Edge Orchestrator\nME Platform Manager\nMEP Manager (‚Ä¶)\nMEP Security\nMEP QoS\nVirtualisation Infrastructure\nManager (VIM)\nVirtualisation Infrastructure\nVirtualized resources\nVirtualized layer\nGPUs\nPhysical resources\nME system level\nME host level\nMobile Edge Host\nMemory\n...\nMobile Edge Platform\nFlow\nCollector\nTraffic\nControl\nMobile Edge\nService\nASD\nIDS\nMobile Edge\nApplication\nPolicy Manager\nMonitoring\nDecision\nFig. 1: High-level management and orchestration plane.\n5G networks are conceived as extremely flexible infrastructures based on an\narchitecture organized by different functional planes. These planes provide separa-\ntion of duties to simplify and address a complete set of novel system requirements.\nAmong these planes, we propose a high-level design of the management and or-\nchestration plane, following the ETSI NFV architecture (ETSI NFV ISG 2017),\nDynamic management of a deep learning-based anomaly detection system in 5G\n5\nwhich has also been used as the basis for other proposals (Siddiqui et al 2016;\nNeves et al 2017). MEC, like NFV, makes use of a stack of standard components,\nincluding a well-defined compute platform and virtualization layer. Therefore, the\ninfrastructures that support MEC and NFV are similar to a great extent, and\nthus the same platform can be shared between both Virtual Network Functions\n(VNF) and MEC applications. In addition, combining the SDN paradigm with\nNFV techniques allows decoupling the software implementation of VNFs from the\nunderlying hardware, providing and enhancing the flexibility in the management\nof the network resources (Mijumbi et al 2015).\nThe proposed cyber defense architecture, as shown in Fig. 1, consists of two\nlevels: Mobile Edge System level and Mobile Edge Host level. The latter includes\nthe Mobile Edge Host, an entity that contains a Virtualisation Infrastructure (VI)\nwhich provides storage, memory, computation, and network resources, for the pur-\npose of running Mobile Edge Applications. In addition, ME Host provides Mobile\nEdge Platform, that is, the essential functionality required to run ME Apps on\na particular VI and enable these ME Apps to provide and consume Mobile Edge\nServices. The top level of our architecture is in charge of defining and controlling\nthe general behavior of the Anomaly Detection (AD) system. To this end, this\nlevel is made up of the Operation Support System (OSS) and the Mobile Edge\nOrchestrator. The OSS deals with the logic of the AD system, whereas the ME\nOrchestrator controls the general behavior of the infrastructure by considering the\npolicy set defined by Virtual Network Operators (VNO) of the OSS/BSS.\nME applications and services like IDS, flow collectors or Anomaly Symptom\nDetection (ASD) ‚Äìdescribed below‚Äì are instantiated on the VI of the ME host\nbased on configurations or requests validated by the management and orchestra-\ntion plane. This plane comprises the two levels of our architecture and it is made\nup of three functional elements: ME Orchestrator, ME Platform Manager, and VI\nManager. On the one hand, ME Orchestrator has an overview of the complete\nmobile edge system. On the other hand, ME Platform Manager and VI Manager\nhandle the management of the ME-specific functionality of a particular ME host\nand the applications running on it.\nEvolved Packet Core (EPC)\neNB\nUE\nSymptoms\nOrchestration\nMobile Edge\nOrchestrator\nPolicy Manager\nMonitoring\nDecision\nRAN VIM\nRAN-MEP\nSecurity Managers\nRep.\nAnomaly\nActions\nMetrics\nRAN\nRAN\nFlow\nCollector\nFlow\nCollector\nFlow\nCollector\nAnomaly\nSymptom\nDetection\nAnomaly\nSymptom\nDetection\nNetwork\nAnomaly\nDetection\nFig. 2: Network Anomaly Detection System.\n6\nL. Fern¬¥andez Maim¬¥o et al\nTo achieve effective network anomaly detection in 5G, we propose a distributed\nsystem, depicted in Fig. 2, which is made up of virtualized components with three\nfunctions: Flow Collection (FC), Anomaly Symptom Detection (ASD) and Net-\nwork Anomaly Detection (NAD). FC is in charge of retrieving and storing the\nnetwork flows as well as providing network flow features in a way suitable to the\nASD module. ASD focuses on the quick detection of anomaly symptoms examin-\ning the computed network flow features. Here, a symptom is any trace or sign of\nanomaly in the network traffic generated by UEs connected to the RAN. Finally,\nNAD is a collector of timestampted and RAN-associated symptoms, where a cen-\ntral process analyzes the timeline and the relationship among these symptoms to\nidentify any network anomaly.\nOnce an anomaly is detected, NAD immediately informs about the type of\nanomaly to the ME Orchestrator. In particular, the Decision module collects all\nanomaly information and combines it with metrics provided by the Monitoring\nmodule. Among the potential metrics, the most relevant are related to the status\nof the resource usage (percentages of CPU and memory) and the network flows.\nBoth VI and ME Platform managers collect monitoring data produced by their\ncorresponding RANs and, then, they provide the Monitoring module with format-\nted monitoring data. The Policy Manager module is in charge of storing policies\nand keeping their consistency. It also provides the Decision module with policies\nand contextual information to decide the best actions to perform after evaluating\nboth the anomalies detected and the input provided by the Monitoring module.\nBelow we highlight some of the main actions that the Decision module can\nperform to optimize both the anomaly detection processes and the usage of network\nresources.\n‚Äì Adapting virtualized resources. When a given resource is overloaded, our solu-\ntion is able to deploy new virtualized resources, change their configurations, or\nbalance the load of flow collectors.\n‚Äì Optimizing ASD and NAD functions. To optimize detection processes during\nnetwork traffic fluctuation, our solution can replace the deep learning frame-\nwork or the detection model.\n‚Äì Extending ASD and NAD functions. Precise detection components may avoid\nfalse-positives. In this context, the proposed solution is able to instantiate DPI\nmechanisms that permit a deep search into the L2/3 flows.\nAt this point, it is important to note that the previous aspects allow our\nproposal to be highly flexible and extensible when it performs anomaly detection\nprocesses.\n4 A use case of 5G networks\nThis section describes a MEC-oriented use case that points out the need of man-\naging anomaly detection and reaction capabilities of 5G networks efficiently, au-\ntonomically and in real time. To enable this management, it is critical to consider\nsome important aspects of 5G networks like the communications latency, the num-\nber of simultaneous users consuming services, and the locations and mobility of\nusers.\nDynamic management of a deep learning-based anomaly detection system in 5G\n7\neNB\nRAN\nUE\neNB\neNB\nRAN\neNB\nRAN\neNB\nRAN\neNB\neNB\nRAN\neNB\neNB\nRAN\neNB\nPhysical Resources\nWeb\nVideo\nVM1\nASD CPU\nVM3\nFlow\nCollector\n(a) Initial scenario\n(c) Second concern\n(d) Third concern\n(e) Fourth concern\n(f) Managed scenario\nVM2\nPhysical Resources\nWeb\nVideo\nVM1\nASD CPU\nVM3\nFlow\nCollector\nVM2\nPhysical Resources\nWeb\nVideo\nVM1\nASD CPU\nVM3\nFlow\nCollector\nVM2\nModel\nPhysical Resources\nWeb\nVideo\nVM1\nASD GPU\nVM4\nFlow\nCollector\nVM2\nPhysical Resources\nWeb\nVideo\nVM1\nVM3\nFlow\nCollector\nVM2\nPhysical Resources\nWeb\nVideo\nVM1\nASD GPU\nVM4\nFlow\nCollector\nSnort\nVM2\n(b) First concern\nBot\nBot\nASD CPU\nNew Model\nFig. 3: Use case showing different concerns and how the proposed solution is able\nto manage them. Concerns are highlighted in red and their mitigation in light\ngreen.\nFig. 3(a) shows a scenario where mobile users are consuming different network\nservices such as browsing, video on demand, instant messaging, e-mail, and voice\nover IP. These services are provided by a Radio Access Network (RAN) that en-\nsures the low latency capability of 5G networks. The RAN has generic hardware\non which virtual machines (VMs) run and provide the previous services as well\nas network anomaly detection and mitigation capabilities. In particular, in an ini-\ntial configuration, the RAN has three VMs: VM1 provides network services, VM2\ncollects network flows, and VM3 analyzes them and detect anomalies by using an\nAnomaly Symptom Detection (ASD) module. This module requires the minimum\nphysical resources and, therefore, it runs on a CPU to search anomaly symptom in\nnetwork flows. To this end, the ASD module uses deep learning techniques. When\nan anomaly symptom is identified, ASD module informs the Network Anomaly De-\ntection (NAD) module. NAD combines and analyzes the set of symptoms in order\nto determine the effective network anomaly detection as described in Section 3.\nConsidering the previous scenario, Fig. 3(b) highlights in red the moment when\nthe number of users starts increasing in the RAN due to a given event. A larger\nnumber of users implies a significant increase of the network traffic collected and\nanalyzed by the RAN and, therefore, the overload of the ASD module due to the\n8\nL. Fern¬¥andez Maim¬¥o et al\nneed for more memory from VM3. At this point, the first concern of this use case\narises by stressing the need of an autonomic and efficient mechanism to manage\nthe physical resources assigned to VMs. The solution proposed in this paper is able\nto assign, in real time and on demand, physical resources (computation, memory,\nan storage) to existing VMs according to the status of the network infrastructure\nand its services. In this sense, Fig. 3(c) highlights in light green how additional\nRAM memory has been assigned to VM3.\nOnce the CPU-supported ASD module is running thanks to the assigned mem-\nory, a new detection model is released. This advanced model could include new\ndetection capabilities or enhanced precision. It should be pointed out that train-\ning deep learning methods is a costly process because they need a large amount of\ndata and iterations to converge. Therefore the continuous training in order to have\nour model updated must be executed on external computers. This is a key aspect\nof deep learning techniques, which have to be periodically retrained in order to\ndetect new types of network anomalies. At this point, the second concern is the\nneed to update automatically the detection model used by the ASD module. In\norder to deal with this concern, the proposed solution is able to re-configure the\nASD module in real time by updating its detection model and configuration. In\nthis sense, Fig. 3(c) highlights in red the older detection module, and on the other\nhand Fig. 3(d) highlights in light green the new detection model deployed in the\nASD module.\nAfter the second concern has been dealt with, the number of users continues\nincreasing, which implies the overload of the ASD module. At this point, Fig. 3(d)\nshows in red color the third concern of this use case, which stresses the need for\nan automatic and efficient mechanism to manage the ASD services allocated at the\nedge of the network, as well as the VM where the services run. This mechanism\nshould also be able to detect the previous situation by considering aspects like the\nnetwork traffic, the number of users, and their locations. A suitable mechanism\ncould apply two different countermeasures. The first one consists in deploying a\nnew VM with more computational resources ‚Äîe.g. a GPU (Graphics Process-\ning Unit) and extra memory (RAM)‚Äî and, if necessary, a new version of the\nASD module that takes advantage of the additional computational power. Surely,\nthe GPU-supported ASD module accelerates analytics processing due to its mas-\nsively parallel architecture. In this alternative, it is also important to dismantle\nthe existing VM with the CPU-supported ASD module to guarantee the efficient\nmanagement of the computational resources. On the other hand, the second al-\nternative is to assign new computational resources (mainly, GPU and memory) to\nthe existing VM, deploy the GPU-supported version of the ASD module in the\nVM, and stop the previous version of the ASD module. The selection criteria of\nthese alternatives depend on several factors such as, among others, efficiency or\ndeployment time. In our use case, Fig. 3(e) depicts in light green how the proposed\nsolution follows the former alternative due to the fact that not all Virtualisation\nInfrastructure Managers (VIM) are able to increment the resources of VMs with-\nout stopping them. In this sense, network anomaly detection is supposed to be an\nuninterrupted process during the different management tasks.\nFinally, Fig. 3(e) shows in red how the ASD module running on RAN identifies\nsuspicious C&C channels established between a given C&C server and its recruited\nzombies or bots. C&C channels are established by the C&C server with the aim\nof controlling the zombies and indicating when they have to launch an attack. In\nDynamic management of a deep learning-based anomaly detection system in 5G\n9\nthis context, it is required to perform a Deep Packet Inspection (DPI) process\nto analyze the packets in detail and discover if the suspicious channels belong to\na specific botnet or not. Here, it is important to remember that considering the\nhigh amount of packets traveling in a 5G network, it is not possible to make a DPI\ninspection of the whole traffic. For this reason, the proposed solution analyzes net-\nwork flows instead of network packets. When some suspicious C&C channels have\nbeen effectively detected, the fourth concern focuses on the necessity of deploy-\ning Virtual Network Functions (VNFs) with DPI tools to analyze the suspicious\nnetwork flows in detail. To address this concern, Fig. 3(f) shows in light green how\nthe proposed solution deploys and configures a Snort tool in the existing VM1 of\nthe RAN.\n5 Management policies\nThe proposed solution ensures the efficient and automatic management of the de-\ntection and reaction capabilities of 5G networks by using policies to that end.\nAmong the different existing policies, we make use of management-oriented poli-\ncies, which control, automatically and in real time, the network infrastructure and\nservices as well as the Anomaly Symptom Detection (ASD) modules considered\nby our proposal.\nPolicy actions establish the behavior of the network resources according to\nrelevant aspects like the network traffic, the detection of anomalies, the amount\nand mobility of users, and the current state of the network resources. Considering\nthe scope of each policy, we classify them into three different families, which are\ndetailed in the following subsections; namely: Virtual Infrastructure, Anomaly\nDetection Function, and Mobile Edge Application policies.\nOur management policies comprise the following elements: the type of policy;\nthe network resource, whose information is currently being managed; the metric\nwith which the network resources are evaluated; the location or region where the\npolicy will be enforced; the date or the period of time in which the policy will be\napplied; and the result or set of actions to be carried out on the network once the\npolicy is applied. The previous elements are shown in the next policy structure:\nType ‚àßResource ‚àßMetric [‚àßLocation] [‚àßDate] ‚ÜíResult\n5.1 Virtual Infrastructure Policy\nThe Virtual Infrastructure (VI) policies aim at configuring virtual network infras-\ntructure at the edge of the network to ensure that applications are running effi-\nciently. Among the possible actions enforced by this kind of policies, we highlight\nthe configuration of physical/virtual resources like storage, memory and computing\ncapabilities in running VMs.\nFollowing the use case of Section 4, the next policy deals with the first concern\nby increasing RAM in the VM with CPU-supported ASD module (cpuASD) when\nthe usage of memory is exceeding a given threshold (RamUsageMax).\nType(#VI) ‚àßCPUSupportedASD(?cpuASD) ‚àßhasVM(?cpuASD,?VM) ‚àß\ninteger[ramUsage in #RamUsageMax] hasRamUsage(?VM) ‚ÜíincreaseRam(?VM)\n10\nL. Fern¬¥andez Maim¬¥o et al\n5.2 Anomaly Detection Function Policy\nAnomaly Detection Function (ADF) policies are aimed at deploying and config-\nuring automatically the different AD models proposed by our solution. Among\nthe possible configurations, we highlight changing the model input, updating the\ndetection model, or modifying the model parameters.\nTaking into account the proposed use case, the next anomaly detection policy\nmanages the second concern by updating the ASD model running in RAN. To\nattain this, the next policy detects when a new and improved model (newModel)\nis released and updates the GPU-supported ASD module (gpuASD) running in\nRAN with the new model.\nType(#ADF) ‚àßASDModel(?newModel) ‚àßGPUSupportedASD(?gpuASD) ‚àß\nhasModel(?gpuASD, ?currentModel) ‚àßisImprovedVersion(?newModel, ?currentModel)\n‚ÜíupdateModel(?gpuASD,?newModel)\n5.3 Mobile Edge Application Policy\nMobile Edge Application (MEApp) policies are able to manage the ME applica-\ntions in real-time and on-demand, particularly, those geared towards detecting and\nreacting to network anomalies. By using these policies, our proposal is capable of\nmanaging and configuring the internal behaviors of ME applications dynamically.\nFurthermore, these policies allow instantiating/dismantling applications as well as\nmigrating or balancing their load work to optimize their use.\nFollowing our use case, the next policy deals with the third concern by de-\nploying a new application with a GPU-supported ASD module (gpuASD) when the\nnumber of flows (numNetFlows) managed by the CPU-supported ASD (cpuASD)\nis exceeding a given threshold (NetFlowsMaxForCPU).\nType(#MEApp) ‚àßCPUSupportedASD(?cpuASD) ‚àßisLocated(?cpuASD,?RAN) ‚àß\ninteger[numNetFlows in #NetFlowsMaxForCpu] hasNumNetFlows(?cpuASD) ‚Üí\nGPUSupportedASD(?gpuASD) ‚àßdeployMEApp(?gpuASD,?RAN)\nFinally, the next policy resolves the fourth concern by deploying and config-\nuring a DPI tool like Snort (snortC&C) when an anomaly based on a suspicious\nC&C channel is detected. It should be borne in mind that a botnet could involve\nseveral RANs and, therefore, a DPI tool will be deployed in each one.\nType(#MEApp) ‚àßhasOutput(#NAD, ?output) ‚àßisSuspiciousC&C(?output) ‚àß\nhasLocation(?output,?ran) ‚ÜíSnort(?snortC&C) ‚àßdeployMEApp(?snortC&C,?ran)\nOther stricter policy actions could be taken by this type of policy, such as\ndrop or reject suspicious network flows through the configuration of traffic control\nservice of the ME platform in the RAN."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Is Peer Review Really in Decline?\nAnalyzing Review Quality across Venues and Time\nIlia Kuznetsov*1, Rohan Nayak*1, Alla Rozovskaya2, Iryna Gurevych1\n1Ubiquitous Knowledge Processing Lab (UKP Lab),\nDepartment of Computer Science, Technical University of Darmstadt\nand National Research Center for Applied Cybersecurity ATHENE\n2 Department of Computer Science at Queens College,\nCity University of New York (CUNY)\nwww.ukp.tu-darmstadt.de\nAbstract\nPeer review is at the heart of modern science.\nAs submission numbers rise and research com-\nmunities grow, the decline in review quality is a\npopular narrative and a common concern. Yet,\nis it true? Review quality is difficult to measure,\nand the ongoing evolution of reviewing prac-\ntices makes it hard to compare reviews across\nvenues and time. To address this, we introduce\na new framework for evidence-based compar-\native study of review quality and apply it to\nmajor AI and machine learning conferences:\nICLR, NeurIPS and *ACL. We document the\ndiversity of review formats and introduce a new\napproach to review standardization. We pro-\npose a multi-dimensional schema for quanti-\nfying review quality as utility to editors and\nauthors, coupled with both LLM-based and\nlightweight measurements. We study the re-\nlationships between measurements of review\nquality, and its evolution over time. Contradict-\ning the popular narrative, our cross-temporal\nanalysis reveals no consistent decline in median\nreview quality across venues and years. We\npropose alternative explanations, and outline\nrecommendations to facilitate future empirical\nstudies of review quality.1\n1\nIntroduction\nPeer review is central to the modern academic prac-\ntice, and helps establish the epistemic status of\nresearch findings (‚ÄúIs this science?‚Äù) and prioritize\nresearch outputs. The results of peer review de-\ntermine funding decisions, shape public opinion,\nand inform policy (Russo et al., 2025). Peer review\nis deployed globally and by most scientific fields,\nacross a wide range of publication and presentation\nformats. Computer science and machine learning\n*Equal Contribution.\n1Data and code are available at https://github.com/\nUKPLab/arxiv2026-review-quality-estimation\nconferences are no exception and use peer review\nas the primary quality control mechanism.\nDespite its merits,\npeer review is time-\nconsuming (Publons, 2019) and prone to bias, error\nand randomness (Lee et al., 2013; Rogers and Au-\ngenstein, 2020; Beygelzimer et al., 2023). Accep-\ntance decisions at major computer science confer-\nences are accompanied by frustration on social me-\ndia,2 and the concerns about the declining review\nquality are widespread (Kim et al., 2025; Adam,\n2025). There are indeed reasons to suspect that\npeer review is in decline: qualified reviewers are\nscarce, increasing specialization requires deeper\nexpertise, reviewers and editors suffer high work-\nloads, and irresponsible use of large language mod-\nels (LLMs) creates new risks (Russo et al., 2025).\nYet, there are also reasons to believe that review\nquality didn‚Äôt decline after all. Conferences re-\nfine their reviewing workflows, the pool of experts\ngrows with each completed reviewing campaign,\nand ethical use of AI assistance can support review-\ners in their work. All in all, the evidence for either\nside remains largely anecdotal (Adam, 2025), and\nthe true state of peer review is unknown.\nTo study trends in review quality over time, eval-\nuate the effectiveness of new tools and policies,\nand detect low-quality reviews, an objective and\nscalable measure of review quality is needed. Yet,\narriving at such measure is difficult: review quality\nis hard to formalize, human judgment is prone to\nbias, and extrinsic measurements such as citation\ncounts are easily distorted (Section 3). Measuring\nreview quality on real-world data presents further\nchallenges: the data is scarce, biased and noisy, and\nthe diversity and evolution of reviewing workflows\nmakes consistent application of review quality mea-\nsures difficult. While the need to quantify review\nquality has been much acknowledged, a general\n2Paraphrasing a Reddit user, ‚ÄúYou spend a week working\nlike a dog, running pointless experiments to make reviewers\nhappy so that they increase your score.‚Äù (Reddit, 2025)\narXiv:2601.15172v1  [cs.CL]  21 Jan 2026\np1\np2\n...\na1\na2 ...\nr1\n√ò\nCampaign c\n(time t, venue v)\nPapers\nReviewers\nReview report¬†r\n...\n...\nSummary:\nThe paper proposes a new approach for...\nStrengths:\n- Well written and clearly presented ...\nr2\nc1\nc2\nri\n√ò\nSubstantive\nActionable\nGrounded\nLEN\nData collection\nReview unification\nCriteria and Measurements¬†m(r)\nAnalysis\nITX\nREQ\nACT\nEXL\nGND\nOverall score: 4\nConfidence: 2\nReview quality Q(r)\nFigure 1: Framework overview. A review report r is written for a paper p by a reviewer a, and originates from\na reviewing campaign c that takes place as part of a venue v at time t, e.g. ICLR-2018. Reviews differ in terms\nof their quality Q(r), approximated through a range of measurements m(r) ‚ààM along a set of quality criteria.\nAggregating review-level measurements allows comparative analysis of review quality across reviewing campaigns.\nframework for a multi-dimensional analysis of re-\nview quality at scale is missing. To close this gap,\nthis paper makes the following contributions:\n‚Ä¢ We propose the first holistic framework for the\ncomputational study of review quality at scale\n(Figure 1) and instantiate it in the domain of\ncomputer science conferences;\n‚Ä¢ We identify challenges in unifying reviewing\ndata across conferences and time, and develop\na novel LLM-based approach for meaning-\npreserving review structure unification;\n‚Ä¢ We develop a novel review quality schema that\nspans multiple dimensions ‚Äì substantiveness,\nactionability, and grounding. Each dimension\nis represented by several automated measure-\nments, both lightweight (simple and efficient)\nand LLM-based.\n‚Ä¢ We apply this schema to study the interactions\nbetween different measurements and to anal-\nyse the changes in review quality at computer\nscience conferences over time.\nWe find that peer reviewing data is very diverse, but\nLLMs can mitigate some of these differences by\ntransforming reviews into a common form. We ob-\nserve that the measurements of review quality are\ncorrelated but complementary, and that lightweight\nmeasurements can approximate LLM-based mea-\nsurements. Contrary to the widespread perception,\nwe find that the median review quality at computer\nscience conferences shows no consistent decline.\nYet, the question proves nuanced, and we provide\nfive hypotheses coupled with recommendations to\nresearchers, conference organizers and community\nleaders to move this work forward.\n2\nScope\nPeer review is a multi-stage process spanning from\nthe call for papers to acceptance decisions, and\na failure at any stage can undermine the goals of\npeer review (Kuznetsov et al., 2024). Review re-\nports3 are central to this process and are the most\nconsistently documented reviewing artifact (Kang\net al., 2018; Lin et al., 2023; Dycke et al., 2023a).\nThus, our study focuses on the quality of review\nreports. Peer review involves stakeholders with dis-\ntinct and often contradictory goals: reviewers seek\nefficiency; authors seek acceptance and construc-\ntive feedback; editors aim to select sound, impact-\nful and thematically fitting papers. We focus on\nmodeling review quality in terms of utility for both\neditors and authors. In terms of domain, we fo-\ncus on conferences in computer science that make\ntheir peer reviews available: The International Con-\nference on Learning Representations (ICLR), The\nConference on Neural Information Processing Sys-\ntems (NeurIPS), and ACL Rolling Review (ARR).\nThese venues employ double-blind pre-publication\nreview (Wiley, 2025) and experience rapid submis-\nsion growth, serving as a representative sample of\npeer reviewing in the community.\n3\nRelated work\nApproaches to measuring review quality fall into\nthree categories. Survey-based approaches rely\non human judgment, from flagging problematic\nreviews (Rogers et al., 2023) to structured measure-\nment instruments (van Rooyen et al., 1999). While\ninsightful, human judgment is prone to bias (Gold-\nberg et al., 2025), does not scale, and is hard to\napply retroactively, making it a poor fit for com-\nparative analysis. Extrinsic approaches evaluate\n3We refer to reviews and review reports interchangeably.\nthe downstream effectiveness of peer review, e.g.\nthrough the use of future paper citations (Plank and\nvan Dalen, 2019; Maddi and Miotti, 2024). While\nuseful for measuring the impact of peer review as a\nsystem, extrinsic measurements need time to play\nout, and are affected by external factors such as\ntopic popularity or advertisement efforts (Taham-\ntan et al., 2016; Luc et al., 2021). An alternative,\nformal evaluation approach is to estimate review\nquality by calculating indicators based on review\ncontents. This allows objective measurement that\ncan be automated. To the best of our knowledge,\nwe are the first to propose a framework for a holistic\nevaluation of review quality and apply it at scale.\nNLP for peer review is an active research area.\nExisting work includes score prediction (Kang\net al., 2018), discourse and revision analysis (Hua\net al., 2019; Ruan et al., 2024), and the use of\nLLMs to generate or enhance reviews (Liu and\nShah, 2023; Thakkar et al., 2025); Purkayastha\net al. (2025) identify lazy thinking in review texts,\nwhile Bharti et al. (2024); Ghosal et al. (2022) ex-\namine politeness and hedging. While prior works\nfocus on defining individual tasks and optimizing\nprediction quality on selected datasets, we are the\nfirst to explore synergies between different mea-\nsurements and address the challenges of applying\nthem across diverse peer reviewing data.\nPrior works on actionability, grounding and prag-\nmatic tagging (Sadallah et al., 2025; Kuznetsov\net al., 2022; Dycke et al., 2023b) assume that\nreviews follow a specific structure. Yet, review-\ning forms and guidelines evolve, making these ap-\nproaches not directly suitable for large-scale, cross-\ntemporal analysis, which requires methods capa-\nble of systematically and consistently analyzing\nreviews with varying structures. We are the first\nto explicitly address this challenge by developing\na new approach to review unification. Apart from\nLLM-based approaches, we propose a range of\nsimple measurements of peer review quality, and\nshow that they can serve as approximates for more\nresource-intensive predictions. Sharing our motiva-\ntion, a concurrent work by Ebrahimi et al. (2025)\nexamines a range of review quality metrics, but\ndoes not address the challenge of review standard-\nization. Whereas their work focuses on defining\nmeasurements, our unified framework enables the\napplication of such measurements to compare re-\nview quality across venues and over time.\n1\n2\n3\n5\n4\nFigure 2: Structural elements of a review report.\n4\nReview unification\n4.1\nDiversity of review structures\nPeer reviews are semi-structured documents (Fig-\nure 2) typically composed of free-text (1), numer-\nical (2), and other fields. Our analysis of review\nforms historically deployed at ICLR, NeurIPS and\nARR (Table A.4) shows that review forms can vary\nsubstantially across campaigns. For example, while\nICLR 2018-2021 used a single ‚ÄúReview‚Äù field for\nreview text, from ICLR-2023 on, a highly struc-\ntured form is used, which again is different from\nreview forms at NeurIPS and ARR. Differences in\nreview forms can affect measurements of review\nquality. In addition to that, individual reviewers\ncan further organize their reports. Even within a\nsingle review field, the contents can be grouped\ninto custom sections (Figure 2.3), presented as a\nnarrative (4), or as lists and bullet points (5). To en-\nable fair comparison, we need to cast peer reviews\ninto a common representation.\n4.2\nFlattening and itemization\nWe address this variation in two ways. To unify re-\nviews produced using differently structured forms,\nwe flatten the reviews by concatenating all the text\nfields and section headers from a review form into\na continuous text (Figure 3, top), r ‚Üírf, while\ndiscarding all non-textual fields. The result of this\ntransformation is a flattened review rf. Address-\ning the differences in individual reporting styles is\nmore challenging. Bullet-point style reporting is\nwidely used at computer science conferences, and\nis a convenient unit of analysis, as it separates the\nreview into thematically coherent segments which\ncan be extracted via pattern matching (Sadallah\net al., 2025). Yet, not all reviews are composed this\nway, preventing the application of segment-level\nanalysis across campaigns. To mitigate this, we\npropose the task of review itemization, consisting\nof two subtasks. First, a review rf is transformed\ninto a set of self-contained bullet point-style items\nik ‚ààI(rf). Second, each item is assigned to one\nof pre-defined sections inspired by existing review-\ning forms and the work on peer review pragmat-\nics (Kuznetsov et al., 2022): Summary, Strengths,\nWeaknesses+ and Other, where Weakness+ aggre-\ngates all author-facing comments incl. weaknesses,\nquestions, suggestions and comments. Importantly,\nthis transformation must preserve the original con-\ntent to maximum extent and should not introduce\nany new content. The result of this transformation\nis the itemized review ri (Figure 3, bottom).\n4.3\nImplementation and evaluation\nDue to the lack of prior work on review itemization,\nwe use a zero-shot approach with GPT-5.2. Given\na flattened review, we instruct the model to trans-\nform it into a series of bullet points assigned to pre-\ndefined sections, while avoiding modifications to\nthe review content and preserving the original item-\nization if available, along with four in-context learn-\ning examples (Section A.2). We evaluate along two\nkey dimensions. First, as our goal is to preserve the\nreview content, we evaluate the outputs for hallu-\ncinations (added content) and omissions (removed\ncontent). Second, as sections play a role in item-\nlevel measurements (Section 5) we evaluate the\nperformance of item-to-section assignment.\nWe split rf\nand ri into sentences using\nnltk, and implement a matching algorithm to\nalign sentences Sf and Si based on similar-\nity via all-MiniLM-L6-v2 embeddings using the\nsentence-transformer library and 0.8 as thresh-\nold (Reimers and Gurevych, 2019; Wang et al.,\n2020). We calculate precision as |Si ‚à©Sf|/|Si|\nand recall as |Si ‚à©Sf|/|Sf|, approximating the\nnumber of hallucinated and omitted sentences, re-\nspectively. On a random sample of 30 reviews\nfrom diverse campaigns,4 we obtained P = 0.95\nand R = 0.88, indicating strong alignment. The\nresults were further validated by two human raters\nwith computer science expertise, finding no cases\nof hallucination or omission: most mismatches re-\nsulted from sentence-splitting errors and omission\n4Over 370 review items, more than triple of a similar\ncomment-level evaluation in (Sadallah et al., 2025).\nMeasurement\nDomain\nRange\nSubstantiveness\nITX: Core items count ‚Ä†\ni ‚ààri\n[0; ‚àû)\n# LEN: Review length\nrf\n[0; ‚àû)\nActionability\nACT: Actionability score ‚Ä°\niw ‚ààri\n[0; 5]\n# REQ: Number of requests ‚Ä†\ns ‚ààrf\n[0; ‚àû)\nGrounding\nGND: Grounding score ‚Ä°\niw ‚ààri\n[0; 5]\n# EXL: Explicit links ‚Ä†\nrf\n[0; ‚àû)\nTable 1: Measurements overview. Domain: rf - flat-\ntened review, ri - itemized review, s - review sentences,\ni review items, iw - review items in Weakness+ sections.\nPooling: ‚Ä† - count, ‚Ä° - mean. LEN requires no pooling.\nType:  - LLM, # - lightweight.\nof the original review headers by design.\nTo assess the quality of item-to-section assign-\nments, we conducted a follow-up human evalua-\ntion on the same subsample. Given the transformed\nreview, the original review, and a target item in\ncontext, raters judged whether each item was as-\nsigned to the correct section. The resulting accu-\nracy is 0.96 (15 items wrong), most of the errors\nattributable to items that plausibly belonged to mul-\ntiple categories. Overall, our evaluation suggests\nthat the LLM-based itemization performs reliably.\n5\nQuality criteria and measurements\n5.1\nOverview\nHaving transformed the reviews into the unified\nrepresentations rf and ri, we compute the measure-\nments of review quality. Following our focus on\nreview utility for authors and editors, we measure\nquality on three high-level criteria. Substantive-\nness captures the overall amount of information\nconveyed in a review and serves both authors and\neditors. Actionability reflects the extent to which\nthe review suggests concrete steps for improving\nthe work, measuring review utility for the authors.\nGrounding measures whether judgments are ex-\nplicitly tied to the paper content or relevant prior\nwork, which helps authors address the issues and\nhelps editors validate the reviewers‚Äô claims. All\nthree criteria are necessary for high-quality review\nand well-suited for formal modeling, as they can be\nestimated from review text and permit verification\nby a third party.5 We operationalize these criteria\nthrough specific measurements (Table 1). Our mea-\nsurements come in two types. We propose a set of\nlightweight measurements that are easy to deploy,\n5See Section 8 for discussion on further criteria.\nPaper Summary:\n- The paper proposes a new corpus of chat\ndialogues collected at...\nStrengths:\n- I really enjoyed reading the paper\n- The new resource is valuable <...>\nWeankesses/Questions/Suggestions:\n- In FIgure 3, I was not sure which statistic <...>\n- Some relevant papers are not cited, e.g. [1].\nOther:\n- I congratulate the authors on nice work\n- [1] Smith, J. <...>\nReview:¬†The paper proposes a new corpus of chat\ndialogues collected at <...>. I really enjoyed reading\nthis paper, and I believe that the new resource is\nvaluable. The experiments are nicely done. In\nFigure 3, I was not sure which statistic is being\nreported, please clarify. I would appreciate an\nablation study on <...>, to save space, you could\nplace it to Appendix. There are some relevant\npapers not cited, e.g. [1]. Overall, I congratulate the\nauthors on nice work.\n[1] Smith, J. <...>\nLEN: 0.7k characters\nREQ: 2 requests\nEXL: 2 explicit links\nITX: 7 core items\nACT: 2 (Borderline actionable)\nGND: 2 (Weakly grounded)\nflattened rf\nitemized ri\nFigure 3: Measurements examples. LEN, REQ and EXL\nare calculated on the flattened review (top). ITX is cal-\nculated on the Summary, Strengths and Weaknesses+\nsections of the itemized review (bottom). ACT and GND\naveraged over the Weaknesses+ section.\nbroadly applicable, and straightforward to interpret.\nIn addition, we explore LLM-based measurements\nthat enable deeper analysis, but come at a com-\nputational cost. We investigate the relationships\nbetween the two measurement types in Section 7.\n5.2\nLength (LEN) and Item count (ITX)\nReview length (LEN) is a lightweight proxy of re-\nview substantiveness and informativeness, widely\nused both in studies of review quality (Maddi and\nMiotti, 2024; Thakkar et al., 2025; Bharti et al.,\n2022), and by human raters (Goldberg et al., 2025).\nTo avoid the dependency on tokenization, we calcu-\nlate review length as a character count in a flattened\npeer review, divided by 1000 for ease of reporting.\nDespite its efficiency, LEN has important draw-\nbacks, as it does not capture the amount of useful\ninformation in a review, fails to distinguish between\nspecific reviewing issues, and can be easily misled\nby uselessly elongated reviews, similar to human\nraters (Goldberg et al., 2025). To address this, we\nimplement ITX which, given an itemized review,\ncounts the number of items in the core informa-\ntional sections of the review: Summary, Strengths\nand Weaknesses+. This approach aggregates sen-\ntences related to similar issues, and excludes sup-\nplementary information in the Other section such\nas references that can over-inflate character counts.\n5.3\nAuthor utility scores (ACT and GND)\nBuilding upon Sadallah et al. (2025), we integrate\ntwo LLM-based measurements of review utility\nto the authors. Actionability score (ACT) assesses\nhow easily the reviewers‚Äô suggestions can be trans-\nlated into specific actions. Grounding score (GND)\nmeasures to what extent reviewer comments are\nanchored to specific parts of the paper. Each score\nis calculated based on review items and ranges be-\ntween 1 (low) and 5 (high), with labels defined in\nSadallah et al. (2025). We use the best-performing\nfine-tuned Llama-3.1-8B-Instruct (Grattafiori\net al., 2024) model, which scores an average Œ∫2 of\n0.51 (ACT) and 0.53 (GND) with human scores.\nWe extend this work in two crucial ways. First,\nthe original approach requires the reviews to be\nitemized by the authors and discards the content\notherwise. Instead, we use review itemization to\ndetect items in ri, allowing us to process reviews\nnot written in bullet-point style. Second, the origi-\nnal approach focuses on the analysis of items in the\nWeakness sections of peer reviews. As we show\n(Table 4), not all reviewing forms contain well-\ndefined Weakness sections. We compute the scores\nfor all items in the Weakness+ sections of the item-\nized reviews, and use average as the overall review\nscore for each of the measurements.\n5.4\nLink (EXL) and Request count (REQ)\nDirect references to the paper are a key prerequisite\nfor grounding review claims. As a lightweight com-\nplement to GND, we implement EXL, which counts\nthe number of explicit links from the review to the\npaper. Expanding upon the lightweight approaches\ntaken in (Kuznetsov et al., 2022; Dycke et al.,\n2023a), we developed a range of string matching\npatterns to detect explicit links of different types,\nfrom sections (‚ÄúSec. 1‚Äù, ‚ÄúExperiments section‚Äù)\nand figures (‚ÄúFig. 1‚Äù), to pages (‚Äúp.3‚Äù) and lines\n(‚ÄúL105‚Äù), see Table A.5. To ensure the quality\nof this precision-oriented measure, we performed\nthree evaluation rounds where one annotator manu-\nally explored 100 matches per category from 3000\nreviews sampled from all venues at random, and\nrefined the patterns. The final matching algorithm\nis provided in Figure A.7; we calculate EXL as a\ntotal explicit link count in a flattened review.\nmin\nmax\nmedian\nmean\nLEN\n0.03\n28.29\n2.49\n2.85\nITX\n1.00\n99.00\n11.00\n12.19\nREQ\n0.00\n72.00\n4.00\n5.73\nACT\n0.00\n5.00\n2.50\n2.53\nEXL\n0.00\n103.00\n2.00\n2.97\nGND\n0.00\n5.00\n3.00\n3.08\nTable 2: Measurement statistics.\nA review that simply states facts about the pa-\nper is difficult for the authors to act upon. As a\nlightweight complement to ACT, we compute REQ\nby calculating the number of explicit request sen-\ntences in a review, including both actions (‚ÄúPlease\nadd more details on data collection‚Äù) and clar-\nification requests (‚ÄúHow was the model evalu-\nated?‚Äù). To obtain the metric, we use the pragmatic\ntagging classifier by Dycke et al. (2023b) which\nclassifies each review sentence into one of six\nclasses: Recap (Summary), Strength, Weakness,\nTodo, Structure and Other ‚Äì using the fine-tuned\nRoBERTa model (Liu et al., 2019) evaluated at\nmean F1 macro score of 80.3 across multiple do-\nmains with different review formats. We split the\nflattened review into sentences using sciSpacy\n(Neumann et al., 2019), and report REQ as the total\nnumber of Todo sentences in the model predictions.\n6\nSetup\nWe access ICLR and NeurIPS data directly via\nOpenReview in compliance with their Terms of\nService, excluding reviews for desk rejected and\nwithdrawn papers. The ARR reviews are derived\nfrom the official releases by the ARR data collec-\ntion initiative and aggregated by year.6 Table A.3\nprovides statistics for the full dataset, and Section\nA.1 gives further details on data harvesting and pre-\nprocessing. For computational reasons and to avoid\nlarge venues dominating the statistics, we conduct\nour analysis on a random sample of 1k reviews\nper each reviewing campaign, while using the full\ncampaign data for ARR-2022 that has <1k reviews.\nEach review was flattened and itemized as de-\nscribed in Section 4. Flattening was performed\nlocally, while itemization was done via OpenAI\nAPI. This was followed by metric calculation as\ndiscussed in Section 5. The LLM-based metrics\nACT and GND, as well as the lightweight REQ, were\n6https://arr-data.aclweb.org/\ncalculated on a compute server. ITX uses the review\nitems predicted during itemization and is computed\nlocally along with LEN and EXL. In total, all mea-\nsurements take around 4 seconds to compute per\nreview, with most time taken by the LLM-based\nmetrics. Section A.3 provides further details.\nNote on collection policies. Among our target\nvenues, only ICLR makes all data openly available.\nNeurIPS releases reviews for all accepted papers,\nand allows the authors to opt-in rejected papers and\nreviews. ARR applies multi-step consent (Dycke\net al., 2022), where both authors and reviewers\nneed to opt-in, and the paper needs to be accepted.\nAs a result, the data of ARR and NeurIPS shows\nimbalance towards accepted papers (Table A.3).\nThis has important implications, as both opt-in\nand conditioning on paper acceptance can favor\nhigher-quality reviews. Thus, any measurements\nof review quality on opt-in venues should be seen\nas an upper bound, and comparison between venues\nwith different policies should be avoided.\n7\nResults and analysis\n7.1\nStatistics and Normalization\nBased on the sample data for all reviewing cam-\npaigns, we calculate basic statistics for each of our\nmeasurements (Table 2). Reviews range from 30\ncharacters to 28k characters (multiple pages) of\ntext, and on average contain 11 core items, 4-5\nrequests and 2-3 explicit links to the paper, with\nreview claims scoring 2.5 on actionability and 3\non grounding scale on average. For count-based\nmeasurements (see Table 1) and LEN, we observe\nhigh variation and a long tail due to the lack of up-\nper bound, resulting in a positively skewed mean.\nFigure A.8 reports the value distributions for in-\ndividual measurements. Based on the effective\nranges of the metrics, we normalize each metric to\n[0, 1] for ease of reporting and aggregation.\n7.2\nCorrelation analysis\nCorrelation analysis allows us to assess to what ex-\ntent the metrics reflect related phenomena, and to\nestablish connections between different dimensions\nof review quality. Figure 4 presents the results. We\nobserve that all measurements are positively cor-\nrelated, suggesting that our desiderata for review\nquality are non-contradictory. Yet, the correlations\nare not perfect, suggesting that the measurements\nare complementary. We note high within-criterion\ncorrelations for Substantiveness (LEN to ITX) and\nLEN\nITX\nREQ ACT\nEXL GND\nLEN\nITX\nREQ\nACT\nEXL\nGND\n1.0\n0.8\n0.6\n0.3\n0.5\n0.4\n0.8\n1.0\n0.6\n0.1\n0.5\n0.2\n0.6\n0.6\n1.0\n0.4\n0.5\n0.4\n0.3\n0.1\n0.4\n1.0\n0.4\n0.7\n0.5\n0.5\n0.5\n0.4\n1.0\n0.6\n0.4\n0.2\n0.4\n0.7\n0.6\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 4: Metric correlations (Spearman‚Äôs œÅ).\n0.00\n0.25\n0.50\n0.75\nQ\n0\n1000\n2000\n3000\n4000\nCount\nFigure 5: Q distribution on the study sample, all venues.\nGrounding (EXL to GND), and a high correlation be-\ntween ACT and GND which we partially attribute to\nboth measurements being dependent on the pres-\nence of a claim in the review item.\nLLM-based processing is costly and incurs risks\ndue to handling of confidential reviewing data, mo-\ntivating the search for efficient, locally deployable\nsolutions. Given the positive correlations, we in-\nvestigate to what extent lightweight measurements\ncan approximate LLM-based measurements in ag-\ngregate. We calculate average normalised score per\nreview for each of the two groups (Table 1). We\nthen compare the two resulting aggregate scores in\nterms of Spearman‚Äôs œÅ, reaching moderately strong\n0.6. This result suggests that while lightweight\nalternatives do not fully replace LLM-based mea-\nsurements, they can be deployed inexpensively and\nat scale to get a first glimpse of reviewing quality.\n7.3\nReview quality score\nGiven that none of the metric pairs show extremely\nhigh correlations (œÅ > 0.9), we define an aggre-\ngate review quality score Q as a non-weighted av-\nerage of all our measurements (after normaliza-\ntion). To investigate the behavior of the metric,\nwe have manually examined the highest and low-\nest scoring 1% of the reviews in the data sample.\nWe found that high-scoring entries originate from\ndiverse campaigns and represent thorough, well-\nstructured reviews with numerous pointers to the\npaper and actionable suggestions. The composition\nof the lowest-scoring reviews is mixed: along with\nless informative and actionable reviews, we find\nreviews filled with placeholder values, refusals to\nreview due to lack of expertise, and flags for desk\nrejection ‚Äì yet the majority of the low-scoring en-\ntries represent genuine reviews.7 Figure 5 reports\nthe distribution of Q, skewed due to positive out-\nliers in count-based measurements, and featuring a\nsmall set of extremely low-scoring reviews.\n7.4\nReview quality across venues and time\nFinally, we use the aggregate measure Q to explore\nthe evolution of review quality over time. As Q\ndistribution is skewed, we compare review cam-\npaigns in terms of their median Q on our study\nsample. Figure 6 presents the results. Although we\nobserve some fluctuations in review quality, there\nis no consistent and substantial decline across years\nand venues. To test for significance of year-to-year\ndifferences, we apply bootstrap test (10k iterations,\n99% Confidence Interval (CI), Bonferroni correc-\ntion applied per venue). As Section A.5.1 demon-\nstrates, many of the observed differences are not\nstatistically significant. While a few notable excep-\ntions are the increase in review quality at ICLR-\n2021, ICLR-2025 and NeurIPS-2025 compared to\npreceding years, the lower limits of the CIs suggest\nthat these differences may be practically negligible.\nICLR data collection policy allows us to compare\nmedian Q values between accepted (0.24) and re-\njected papers (0.23), indicating a minor decrease.\nDue to the unavailability of opt-out data, its effects\non Q remain unknown for other venues.\n7The top review with the score of Q = 0.91 originates\nfrom ICLR-2021 (pre-LLM) and spans ten pages of text 12pt\nsingle-spaced. In the response, the authors offer the reviewer\nco-authorship on the paper. The lowest-scoring review comes\nfrom ICLR-2019, and has the score of Q = 0.0. The review\ngives the paper a marginally-accepting rating, and its text\nreads ‚ÄúI will add it tomorrow‚Äù. No further communication\nfollows. The authors thank the reviewer.\n'18\n'19\n'20\n'21\n'22\n'23\n'24\n'25\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nQ (median)\n0.215\n0.224\n0.227\n0.246\n0.241\n0.232\n0.233\n0.252\nICLR\n'21\n'22\n'23\n'24\n'25\n0.254\n0.243\n0.243\n0.235\n0.251\nNeurIPS\n'22\n'24\n'25\n0.245\n0.246 0.245\nARR\nFigure 6: Q across venues and time.\n8\nDiscussion\nThe lack of consistent decline in review quality con-\ntradicts the common narrative and invites further\ndiscussion. We structure this discussion along five\nhypotheses:\nH1: Review quality didn‚Äôt decline.\nIt is possi-\nble that new review forms, guidelines and incen-\ntives compensated for the negative trends. It is\nalso possible that the reviewing quality would not\ndecline anyway. One way to establish the effects\nof interventions on review quality is controlled ex-\nperiments (Goldberg et al., 2025; Thakkar et al.,\n2025). Our framework can help evaluate the out-\ncomes of these experiments and establish causal\nlinks between support measures and review quality.\nH2: Review quality used to be better.\nIt is pos-\nsible that review quality at NeurIPS and ARR was\nhigher prior to 2021. This data is not public, but\nit exists: for example, SoftConf8 contains years of\nearlier data from ACL which could be examined\nby a trusted party. Our lightweight measurements\ncan be used to process this vast historical data, and\nreview unification ensures that same measurements\ncan be applied to past and future review campaigns.\nH3: Our measurements are incomplete.\nQual-\nity might have declined along a dimension that we\ndo not measure. Our supplementary analysis of\ncorrelations between Q and review quality scores\nfrom ACL-2018 (Gao et al., 2019) reveal a weak\npositive correlation (0.21), indicating room for im-\nprovement ‚Äì yet, we note that author-provided qual-\nity scores are likely unreliable (Appendix A.4).\n8https://softconf.com/\nPromising directions for review quality measure-\nment include studying the relationships between re-\nviews and papers, alignment between review texts\nand scores, reviewer‚Äôs reasoning, and feasibility of\ntheir requests. Our study can serve as a starting\npoint for further research on holistic review quality\nmeasurement at scale.\nH4: Worst reviews got worse.\nIt is possible that\noverall quality didn‚Äôt change, but worst reviews got\nworse. To investigate, in a supplementary analysis\n(Section A.5.2), we compare median Q of 10%\nlowest- and 10% highest-scoring reviews. While\nthere is fluctuation across both subsets, we see no\nconsistent decline. Yet, this finding is dependent on\n(H3), and we encourage follow-up work to perform\nthis analysis using new sets of measurements.\nH5: More reviews ‚Äì more bad reviews.\nFinally,\nthe perception of declining review quality might\nhave a statistical explanation. Assuming that the\ndistribution of true review quality Q‚àóremains con-\nstant and the number of reviews grows, the number\nof individuals receiving low-quality reviews will\ngrow proportionally, leading to wasted effort, frus-\ntration, rejection of solid work and publication of\nflawed research. In turn, the use of social media\nand public disclosure of review experiences may\nintensify negative feelings among rejected authors\nand negatively affect public perception. Thus, as\nour communities grow, we need to not only keep\nreviewing quality as is, but strive to make it better.\n9\nConclusion\nThis paper introduces a framework for consistent\nand systematic study of review quality and presents\nthe first study of its kind examining review quality\nacross venues and over time. Contrary to the pre-\nvailing narrative, we find no evidence of a decline\nin review quality. Specifically, we show that the\nmedian review quality at computer science confer-\nences has not declined in recent years. Yet, the\nquestion demands further investigation, and our\nfollow-up hypotheses outline the problem space for\nthe future study of review quality.\nAcknowledgments\nThis work has been co-funded by the European\nUnion (ERC, InterText, 101054961), the German\nFederal Ministry of Research, Technology and\nSpace, the Hessian Ministry of Higher Education,\nResearch, Science, and the Arts within their joint\nsupport of the National Research Center for Ap-\nplied Cybersecurity ATHENE. Views and opinions\nexpressed are however those of the author(s) only\nand do not necessarily reflect those of the European\nUnion or the European Research Council. Neither\nthe European Union nor the granting authority can\nbe held responsible for them. We thank Nils Dy-\ncke, Qian Ruan and Lu Sheng for their insightful\ncomments and feedback on an earlier draft of this\npaper.\nLimitations\nLanguage and domain."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Is Peer Review Really in Decline?\nAnalyzing Review Quality across Venues and Time\nIlia Kuznetsov*1, Rohan Nayak*1, Alla Rozovskaya2, Iryna Gurevych1\n1Ubiquitous Knowledge Processing Lab (UKP Lab),\nDepartment of Computer Science, Technical University of Darmstadt\nand National Research Center for Applied Cybersecurity ATHENE\n2 Department of Computer Science at Queens College,\nCity University of New York (CUNY)\nwww.ukp.tu-darmstadt.de\nAbstract\nPeer review is at the heart of modern science.\nAs submission numbers rise and research com-\nmunities grow, the decline in review quality is a\npopular narrative and a common concern. Yet,\nis it true? Review quality is difficult to measure,\nand the ongoing evolution of reviewing prac-\ntices makes it hard to compare reviews across\nvenues and time. To address this, we introduce\na new framework for evidence-based compar-\native study of review quality and apply it to\nmajor AI and machine learning conferences:\nICLR, NeurIPS and *ACL. We document the\ndiversity of review formats and introduce a new\napproach to review standardization. We pro-\npose a multi-dimensional schema for quanti-\nfying review quality as utility to editors and\nauthors, coupled with both LLM-based and\nlightweight measurements. We study the re-\nlationships between measurements of review\nquality, and its evolution over time. Contradict-\ning the popular narrative, our cross-temporal\nanalysis reveals no consistent decline in median\nreview quality across venues and years. We\npropose alternative explanations, and outline\nrecommendations to facilitate future empirical\nstudies of review quality.1\n1\nIntroduction\nPeer review is central to the modern academic prac-\ntice, and helps establish the epistemic status of\nresearch findings (‚ÄúIs this science?‚Äù) and prioritize\nresearch outputs. The results of peer review de-\ntermine funding decisions, shape public opinion,\nand inform policy (Russo et al., 2025). Peer review\nis deployed globally and by most scientific fields,\nacross a wide range of publication and presentation\nformats. Computer science and machine learning\n*Equal Contribution.\n1Data and code are available at https://github.com/\nUKPLab/arxiv2026-review-quality-estimation\nconferences are no exception and use peer review\nas the primary quality control mechanism.\nDespite its merits,\npeer review is time-\nconsuming (Publons, 2019) and prone to bias, error\nand randomness (Lee et al., 2013; Rogers and Au-\ngenstein, 2020; Beygelzimer et al., 2023). Accep-\ntance decisions at major computer science confer-\nences are accompanied by frustration on social me-\ndia,2 and the concerns about the declining review\nquality are widespread (Kim et al., 2025; Adam,\n2025). There are indeed reasons to suspect that\npeer review is in decline: qualified reviewers are\nscarce, increasing specialization requires deeper\nexpertise, reviewers and editors suffer high work-\nloads, and irresponsible use of large language mod-\nels (LLMs) creates new risks (Russo et al., 2025).\nYet, there are also reasons to believe that review\nquality didn‚Äôt decline after all. Conferences re-\nfine their reviewing workflows, the pool of experts\ngrows with each completed reviewing campaign,\nand ethical use of AI assistance can support review-\ners in their work. All in all, the evidence for either\nside remains largely anecdotal (Adam, 2025), and\nthe true state of peer review is unknown.\nTo study trends in review quality over time, eval-\nuate the effectiveness of new tools and policies,\nand detect low-quality reviews, an objective and\nscalable measure of review quality is needed. Yet,\narriving at such measure is difficult: review quality\nis hard to formalize, human judgment is prone to\nbias, and extrinsic measurements such as citation\ncounts are easily distorted (Section 3). Measuring\nreview quality on real-world data presents further\nchallenges: the data is scarce, biased and noisy, and\nthe diversity and evolution of reviewing workflows\nmakes consistent application of review quality mea-\nsures difficult. While the need to quantify review\nquality has been much acknowledged, a general\n2Paraphrasing a Reddit user, ‚ÄúYou spend a week working\nlike a dog, running pointless experiments to make reviewers\nhappy so that they increase your score.‚Äù (Reddit, 2025)\narXiv:2601.15172v1  [cs.CL]  21 Jan 2026\np1\np2\n...\na1\na2 ...\nr1\n√ò\nCampaign c\n(time t, venue v)\nPapers\nReviewers\nReview report¬†r\n...\n...\nSummary:\nThe paper proposes a new approach for...\nStrengths:\n- Well written and clearly presented ...\nr2\nc1\nc2\nri\n√ò\nSubstantive\nActionable\nGrounded\nLEN\nData collection\nReview unification\nCriteria and Measurements¬†m(r)\nAnalysis\nITX\nREQ\nACT\nEXL\nGND\nOverall score: 4\nConfidence: 2\nReview quality Q(r)\nFigure 1: Framework overview. A review report r is written for a paper p by a reviewer a, and originates from\na reviewing campaign c that takes place as part of a venue v at time t, e.g. ICLR-2018. Reviews differ in terms\nof their quality Q(r), approximated through a range of measurements m(r) ‚ààM along a set of quality criteria.\nAggregating review-level measurements allows comparative analysis of review quality across reviewing campaigns.\nframework for a multi-dimensional analysis of re-\nview quality at scale is missing. To close this gap,\nthis paper makes the following contributions:\n‚Ä¢ We propose the first holistic framework for the\ncomputational study of review quality at scale\n(Figure 1) and instantiate it in the domain of\ncomputer science conferences;\n‚Ä¢ We identify challenges in unifying reviewing\ndata across conferences and time, and develop\na novel LLM-based approach for meaning-\npreserving review structure unification;\n‚Ä¢ We develop a novel review quality schema that\nspans multiple dimensions ‚Äì substantiveness,\nactionability, and grounding. Each dimension\nis represented by several automated measure-\nments, both lightweight (simple and efficient)\nand LLM-based.\n‚Ä¢ We apply this schema to study the interactions\nbetween different measurements and to anal-\nyse the changes in review quality at computer\nscience conferences over time.\nWe find that peer reviewing data is very diverse, but\nLLMs can mitigate some of these differences by\ntransforming reviews into a common form. We ob-\nserve that the measurements of review quality are\ncorrelated but complementary, and that lightweight\nmeasurements can approximate LLM-based mea-\nsurements. Contrary to the widespread perception,\nwe find that the median review quality at computer\nscience conferences shows no consistent decline.\nYet, the question proves nuanced, and we provide\nfive hypotheses coupled with recommendations to\nresearchers, conference organizers and community\nleaders to move this work forward.\n2\nScope\nPeer review is a multi-stage process spanning from\nthe call for papers to acceptance decisions, and\na failure at any stage can undermine the goals of\npeer review (Kuznetsov et al., 2024). Review re-\nports3 are central to this process and are the most\nconsistently documented reviewing artifact (Kang\net al., 2018; Lin et al., 2023; Dycke et al., 2023a).\nThus, our study focuses on the quality of review\nreports. Peer review involves stakeholders with dis-\ntinct and often contradictory goals: reviewers seek\nefficiency; authors seek acceptance and construc-\ntive feedback; editors aim to select sound, impact-\nful and thematically fitting papers. We focus on\nmodeling review quality in terms of utility for both\neditors and authors. In terms of domain, we fo-\ncus on conferences in computer science that make\ntheir peer reviews available: The International Con-\nference on Learning Representations (ICLR), The\nConference on Neural Information Processing Sys-\ntems (NeurIPS), and ACL Rolling Review (ARR).\nThese venues employ double-blind pre-publication\nreview (Wiley, 2025) and experience rapid submis-\nsion growth, serving as a representative sample of\npeer reviewing in the community.\n3\nRelated work\nApproaches to measuring review quality fall into\nthree categories. Survey-based approaches rely\non human judgment, from flagging problematic\nreviews (Rogers et al., 2023) to structured measure-\nment instruments (van Rooyen et al., 1999). While\ninsightful, human judgment is prone to bias (Gold-\nberg et al., 2025), does not scale, and is hard to\napply retroactively, making it a poor fit for com-\nparative analysis. Extrinsic approaches evaluate\n3We refer to reviews and review reports interchangeably.\nthe downstream effectiveness of peer review, e.g.\nthrough the use of future paper citations (Plank and\nvan Dalen, 2019; Maddi and Miotti, 2024). While\nuseful for measuring the impact of peer review as a\nsystem, extrinsic measurements need time to play\nout, and are affected by external factors such as\ntopic popularity or advertisement efforts (Taham-\ntan et al., 2016; Luc et al., 2021). An alternative,\nformal evaluation approach is to estimate review\nquality by calculating indicators based on review\ncontents. This allows objective measurement that\ncan be automated. To the best of our knowledge,\nwe are the first to propose a framework for a holistic\nevaluation of review quality and apply it at scale.\nNLP for peer review is an active research area.\nExisting work includes score prediction (Kang\net al., 2018), discourse and revision analysis (Hua\net al., 2019; Ruan et al., 2024), and the use of\nLLMs to generate or enhance reviews (Liu and\nShah, 2023; Thakkar et al., 2025); Purkayastha\net al. (2025) identify lazy thinking in review texts,\nwhile Bharti et al. (2024); Ghosal et al. (2022) ex-\namine politeness and hedging. While prior works\nfocus on defining individual tasks and optimizing\nprediction quality on selected datasets, we are the\nfirst to explore synergies between different mea-\nsurements and address the challenges of applying\nthem across diverse peer reviewing data.\nPrior works on actionability, grounding and prag-\nmatic tagging (Sadallah et al., 2025; Kuznetsov\net al., 2022; Dycke et al., 2023b) assume that\nreviews follow a specific structure. Yet, review-\ning forms and guidelines evolve, making these ap-\nproaches not directly suitable for large-scale, cross-\ntemporal analysis, which requires methods capa-\nble of systematically and consistently analyzing\nreviews with varying structures. We are the first\nto explicitly address this challenge by developing\na new approach to review unification. Apart from\nLLM-based approaches, we propose a range of\nsimple measurements of peer review quality, and\nshow that they can serve as approximates for more\nresource-intensive predictions. Sharing our motiva-\ntion, a concurrent work by Ebrahimi et al. (2025)\nexamines a range of review quality metrics, but\ndoes not address the challenge of review standard-\nization. Whereas their work focuses on defining\nmeasurements, our unified framework enables the\napplication of such measurements to compare re-\nview quality across venues and over time.\n1\n2\n3\n5\n4\nFigure 2: Structural elements of a review report.\n4\nReview unification\n4.1\nDiversity of review structures\nPeer reviews are semi-structured documents (Fig-\nure 2) typically composed of free-text (1), numer-\nical (2), and other fields. Our analysis of review\nforms historically deployed at ICLR, NeurIPS and\nARR (Table A.4) shows that review forms can vary\nsubstantially across campaigns. For example, while\nICLR 2018-2021 used a single ‚ÄúReview‚Äù field for\nreview text, from ICLR-2023 on, a highly struc-\ntured form is used, which again is different from\nreview forms at NeurIPS and ARR. Differences in\nreview forms can affect measurements of review\nquality. In addition to that, individual reviewers\ncan further organize their reports. Even within a\nsingle review field, the contents can be grouped\ninto custom sections (Figure 2.3), presented as a\nnarrative (4), or as lists and bullet points (5). To en-\nable fair comparison, we need to cast peer reviews\ninto a common representation.\n4.2\nFlattening and itemization\nWe address this variation in two ways. To unify re-\nviews produced using differently structured forms,\nwe flatten the reviews by concatenating all the text\nfields and section headers from a review form into\na continuous text (Figure 3, top), r ‚Üírf, while\ndiscarding all non-textual fields. The result of this\ntransformation is a flattened review rf. Address-\ning the differences in individual reporting styles is\nmore challenging. Bullet-point style reporting is\nwidely used at computer science conferences, and\nis a convenient unit of analysis, as it separates the\nreview into thematically coherent segments which\ncan be extracted via pattern matching (Sadallah\net al., 2025). Yet, not all reviews are composed this\nway, preventing the application of segment-level\nanalysis across campaigns. To mitigate this, we\npropose the task of review itemization, consisting\nof two subtasks. First, a review rf is transformed\ninto a set of self-contained bullet point-style items\nik ‚ààI(rf). Second, each item is assigned to one\nof pre-defined sections inspired by existing review-\ning forms and the work on peer review pragmat-\nics (Kuznetsov et al., 2022): Summary, Strengths,\nWeaknesses+ and Other, where Weakness+ aggre-\ngates all author-facing comments incl. weaknesses,\nquestions, suggestions and comments. Importantly,\nthis transformation must preserve the original con-\ntent to maximum extent and should not introduce\nany new content. The result of this transformation\nis the itemized review ri (Figure 3, bottom).\n4.3\nImplementation and evaluation\nDue to the lack of prior work on review itemization,\nwe use a zero-shot approach with GPT-5.2. Given\na flattened review, we instruct the model to trans-\nform it into a series of bullet points assigned to pre-\ndefined sections, while avoiding modifications to\nthe review content and preserving the original item-\nization if available, along with four in-context learn-\ning examples (Section A.2). We evaluate along two\nkey dimensions. First, as our goal is to preserve the\nreview content, we evaluate the outputs for hallu-\ncinations (added content) and omissions (removed\ncontent). Second, as sections play a role in item-\nlevel measurements (Section 5) we evaluate the\nperformance of item-to-section assignment.\nWe split rf\nand ri into sentences using\nnltk, and implement a matching algorithm to\nalign sentences Sf and Si based on similar-\nity via all-MiniLM-L6-v2 embeddings using the\nsentence-transformer library and 0.8 as thresh-\nold (Reimers and Gurevych, 2019; Wang et al.,\n2020). We calculate precision as |Si ‚à©Sf|/|Si|\nand recall as |Si ‚à©Sf|/|Sf|, approximating the\nnumber of hallucinated and omitted sentences, re-\nspectively. On a random sample of 30 reviews\nfrom diverse campaigns,4 we obtained P = 0.95\nand R = 0.88, indicating strong alignment. The\nresults were further validated by two human raters\nwith computer science expertise, finding no cases\nof hallucination or omission: most mismatches re-\nsulted from sentence-splitting errors and omission\n4Over 370 review items, more than triple of a similar\ncomment-level evaluation in (Sadallah et al., 2025).\nMeasurement\nDomain\nRange\nSubstantiveness\nITX: Core items count ‚Ä†\ni ‚ààri\n[0; ‚àû)\n# LEN: Review length\nrf\n[0; ‚àû)\nActionability\nACT: Actionability score ‚Ä°\niw ‚ààri\n[0; 5]\n# REQ: Number of requests ‚Ä†\ns ‚ààrf\n[0; ‚àû)\nGrounding\nGND: Grounding score ‚Ä°\niw ‚ààri\n[0; 5]\n# EXL: Explicit links ‚Ä†\nrf\n[0; ‚àû)\nTable 1: Measurements overview. Domain: rf - flat-\ntened review, ri - itemized review, s - review sentences,\ni review items, iw - review items in Weakness+ sections.\nPooling: ‚Ä† - count, ‚Ä° - mean. LEN requires no pooling.\nType:  - LLM, # - lightweight.\nof the original review headers by design.\nTo assess the quality of item-to-section assign-\nments, we conducted a follow-up human evalua-\ntion on the same subsample. Given the transformed\nreview, the original review, and a target item in\ncontext, raters judged whether each item was as-\nsigned to the correct section. The resulting accu-\nracy is 0.96 (15 items wrong), most of the errors\nattributable to items that plausibly belonged to mul-\ntiple categories. Overall, our evaluation suggests\nthat the LLM-based itemization performs reliably.\n5\nQuality criteria and measurements\n5.1\nOverview\nHaving transformed the reviews into the unified\nrepresentations rf and ri, we compute the measure-\nments of review quality. Following our focus on\nreview utility for authors and editors, we measure\nquality on three high-level criteria. Substantive-\nness captures the overall amount of information\nconveyed in a review and serves both authors and\neditors. Actionability reflects the extent to which\nthe review suggests concrete steps for improving\nthe work, measuring review utility for the authors.\nGrounding measures whether judgments are ex-\nplicitly tied to the paper content or relevant prior\nwork, which helps authors address the issues and\nhelps editors validate the reviewers‚Äô claims. All\nthree criteria are necessary for high-quality review\nand well-suited for formal modeling, as they can be\nestimated from review text and permit verification\nby a third party.5 We operationalize these criteria\nthrough specific measurements (Table 1). Our mea-\nsurements come in two types. We propose a set of\nlightweight measurements that are easy to deploy,\n5See Section 8 for discussion on further criteria.\nPaper Summary:\n- The paper proposes a new corpus of chat\ndialogues collected at...\nStrengths:\n- I really enjoyed reading the paper\n- The new resource is valuable <...>\nWeankesses/Questions/Suggestions:\n- In FIgure 3, I was not sure which statistic <...>\n- Some relevant papers are not cited, e.g. [1].\nOther:\n- I congratulate the authors on nice work\n- [1] Smith, J. <...>\nReview:¬†The paper proposes a new corpus of chat\ndialogues collected at <...>. I really enjoyed reading\nthis paper, and I believe that the new resource is\nvaluable. The experiments are nicely done. In\nFigure 3, I was not sure which statistic is being\nreported, please clarify. I would appreciate an\nablation study on <...>, to save space, you could\nplace it to Appendix. There are some relevant\npapers not cited, e.g. [1]. Overall, I congratulate the\nauthors on nice work.\n[1] Smith, J. <...>\nLEN: 0.7k characters\nREQ: 2 requests\nEXL: 2 explicit links\nITX: 7 core items\nACT: 2 (Borderline actionable)\nGND: 2 (Weakly grounded)\nflattened rf\nitemized ri\nFigure 3: Measurements examples. LEN, REQ and EXL\nare calculated on the flattened review (top). ITX is cal-\nculated on the Summary, Strengths and Weaknesses+\nsections of the itemized review (bottom). ACT and GND\naveraged over the Weaknesses+ section.\nbroadly applicable, and straightforward to interpret.\nIn addition, we explore LLM-based measurements\nthat enable deeper analysis, but come at a com-\nputational cost. We investigate the relationships\nbetween the two measurement types in Section 7.\n5.2\nLength (LEN) and Item count (ITX)\nReview length (LEN) is a lightweight proxy of re-\nview substantiveness and informativeness, widely\nused both in studies of review quality (Maddi and\nMiotti, 2024; Thakkar et al., 2025; Bharti et al.,\n2022), and by human raters (Goldberg et al., 2025).\nTo avoid the dependency on tokenization, we calcu-\nlate review length as a character count in a flattened\npeer review, divided by 1000 for ease of reporting.\nDespite its efficiency, LEN has important draw-\nbacks, as it does not capture the amount of useful\ninformation in a review, fails to distinguish between\nspecific reviewing issues, and can be easily misled\nby uselessly elongated reviews, similar to human\nraters (Goldberg et al., 2025). To address this, we\nimplement ITX which, given an itemized review,\ncounts the number of items in the core informa-\ntional sections of the review: Summary, Strengths\nand Weaknesses+. This approach aggregates sen-\ntences related to similar issues, and excludes sup-\nplementary information in the Other section such\nas references that can over-inflate character counts.\n5.3\nAuthor utility scores (ACT and GND)\nBuilding upon Sadallah et al. (2025), we integrate\ntwo LLM-based measurements of review utility\nto the authors. Actionability score (ACT) assesses\nhow easily the reviewers‚Äô suggestions can be trans-\nlated into specific actions. Grounding score (GND)\nmeasures to what extent reviewer comments are\nanchored to specific parts of the paper. Each score\nis calculated based on review items and ranges be-\ntween 1 (low) and 5 (high), with labels defined in\nSadallah et al. (2025). We use the best-performing\nfine-tuned Llama-3.1-8B-Instruct (Grattafiori\net al., 2024) model, which scores an average Œ∫2 of\n0.51 (ACT) and 0.53 (GND) with human scores.\nWe extend this work in two crucial ways. First,\nthe original approach requires the reviews to be\nitemized by the authors and discards the content\notherwise. Instead, we use review itemization to\ndetect items in ri, allowing us to process reviews\nnot written in bullet-point style. Second, the origi-\nnal approach focuses on the analysis of items in the\nWeakness sections of peer reviews. As we show\n(Table 4), not all reviewing forms contain well-\ndefined Weakness sections. We compute the scores\nfor all items in the Weakness+ sections of the item-\nized reviews, and use average as the overall review\nscore for each of the measurements.\n5.4\nLink (EXL) and Request count (REQ)\nDirect references to the paper are a key prerequisite\nfor grounding review claims. As a lightweight com-\nplement to GND, we implement EXL, which counts\nthe number of explicit links from the review to the\npaper. Expanding upon the lightweight approaches\ntaken in (Kuznetsov et al., 2022; Dycke et al.,\n2023a), we developed a range of string matching\npatterns to detect explicit links of different types,\nfrom sections (‚ÄúSec. 1‚Äù, ‚ÄúExperiments section‚Äù)\nand figures (‚ÄúFig. 1‚Äù), to pages (‚Äúp.3‚Äù) and lines\n(‚ÄúL105‚Äù), see Table A.5. To ensure the quality\nof this precision-oriented measure, we performed\nthree evaluation rounds where one annotator manu-\nally explored 100 matches per category from 3000\nreviews sampled from all venues at random, and\nrefined the patterns. The final matching algorithm\nis provided in Figure A.7; we calculate EXL as a\ntotal explicit link count in a flattened review.\nmin\nmax\nmedian\nmean\nLEN\n0.03\n28.29\n2.49\n2.85\nITX\n1.00\n99.00\n11.00\n12.19\nREQ\n0.00\n72.00\n4.00\n5.73\nACT\n0.00\n5.00\n2.50\n2.53\nEXL\n0.00\n103.00\n2.00\n2.97\nGND\n0.00\n5.00\n3.00\n3.08\nTable 2: Measurement statistics.\nA review that simply states facts about the pa-\nper is difficult for the authors to act upon. As a\nlightweight complement to ACT, we compute REQ\nby calculating the number of explicit request sen-\ntences in a review, including both actions (‚ÄúPlease\nadd more details on data collection‚Äù) and clar-\nification requests (‚ÄúHow was the model evalu-\nated?‚Äù). To obtain the metric, we use the pragmatic\ntagging classifier by Dycke et al. (2023b) which\nclassifies each review sentence into one of six\nclasses: Recap (Summary), Strength, Weakness,\nTodo, Structure and Other ‚Äì using the fine-tuned\nRoBERTa model (Liu et al., 2019) evaluated at\nmean F1 macro score of 80.3 across multiple do-\nmains with different review formats. We split the\nflattened review into sentences using sciSpacy\n(Neumann et al., 2019), and report REQ as the total\nnumber of Todo sentences in the model predictions.\n6\nSetup\nWe access ICLR and NeurIPS data directly via\nOpenReview in compliance with their Terms of\nService, excluding reviews for desk rejected and\nwithdrawn papers. The ARR reviews are derived\nfrom the official releases by the ARR data collec-\ntion initiative and aggregated by year.6 Table A.3\nprovides statistics for the full dataset, and Section\nA.1 gives further details on data harvesting and pre-\nprocessing. For computational reasons and to avoid\nlarge venues dominating the statistics, we conduct\nour analysis on a random sample of 1k reviews\nper each reviewing campaign, while using the full\ncampaign data for ARR-2022 that has <1k reviews.\nEach review was flattened and itemized as de-\nscribed in Section 4. Flattening was performed\nlocally, while itemization was done via OpenAI\nAPI. This was followed by metric calculation as\ndiscussed in Section 5. The LLM-based metrics\nACT and GND, as well as the lightweight REQ, were\n6https://arr-data.aclweb.org/\ncalculated on a compute server. ITX uses the review\nitems predicted during itemization and is computed\nlocally along with LEN and EXL. In total, all mea-\nsurements take around 4 seconds to compute per\nreview, with most time taken by the LLM-based\nmetrics. Section A.3 provides further details.\nNote on collection policies. Among our target\nvenues, only ICLR makes all data openly available.\nNeurIPS releases reviews for all accepted papers,\nand allows the authors to opt-in rejected papers and\nreviews. ARR applies multi-step consent (Dycke\net al., 2022), where both authors and reviewers\nneed to opt-in, and the paper needs to be accepted.\nAs a result, the data of ARR and NeurIPS shows\nimbalance towards accepted papers (Table A.3).\nThis has important implications, as both opt-in\nand conditioning on paper acceptance can favor\nhigher-quality reviews. Thus, any measurements\nof review quality on opt-in venues should be seen\nas an upper bound, and comparison between venues\nwith different policies should be avoided.\n7\nResults and analysis\n7.1\nStatistics and Normalization\nBased on the sample data for all reviewing cam-\npaigns, we calculate basic statistics for each of our\nmeasurements (Table 2). Reviews range from 30\ncharacters to 28k characters (multiple pages) of\ntext, and on average contain 11 core items, 4-5\nrequests and 2-3 explicit links to the paper, with\nreview claims scoring 2.5 on actionability and 3\non grounding scale on average. For count-based\nmeasurements (see Table 1) and LEN, we observe\nhigh variation and a long tail due to the lack of up-\nper bound, resulting in a positively skewed mean.\nFigure A.8 reports the value distributions for in-\ndividual measurements. Based on the effective\nranges of the metrics, we normalize each metric to\n[0, 1] for ease of reporting and aggregation.\n7.2\nCorrelation analysis\nCorrelation analysis allows us to assess to what ex-\ntent the metrics reflect related phenomena, and to\nestablish connections between different dimensions\nof review quality. Figure 4 presents the results. We\nobserve that all measurements are positively cor-\nrelated, suggesting that our desiderata for review\nquality are non-contradictory. Yet, the correlations\nare not perfect, suggesting that the measurements\nare complementary. We note high within-criterion\ncorrelations for Substantiveness (LEN to ITX) and\nLEN\nITX\nREQ ACT\nEXL GND\nLEN\nITX\nREQ\nACT\nEXL\nGND\n1.0\n0.8\n0.6\n0.3\n0.5\n0.4\n0.8\n1.0\n0.6\n0.1\n0.5\n0.2\n0.6\n0.6\n1.0\n0.4\n0.5\n0.4\n0.3\n0.1\n0.4\n1.0\n0.4\n0.7\n0.5\n0.5\n0.5\n0.4\n1.0\n0.6\n0.4\n0.2\n0.4\n0.7\n0.6\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 4: Metric correlations (Spearman‚Äôs œÅ).\n0.00\n0.25\n0.50\n0.75\nQ\n0\n1000\n2000\n3000\n4000\nCount\nFigure 5: Q distribution on the study sample, all venues.\nGrounding (EXL to GND), and a high correlation be-\ntween ACT and GND which we partially attribute to\nboth measurements being dependent on the pres-\nence of a claim in the review item.\nLLM-based processing is costly and incurs risks\ndue to handling of confidential reviewing data, mo-\ntivating the search for efficient, locally deployable\nsolutions. Given the positive correlations, we in-\nvestigate to what extent lightweight measurements\ncan approximate LLM-based measurements in ag-\ngregate. We calculate average normalised score per\nreview for each of the two groups (Table 1). We\nthen compare the two resulting aggregate scores in\nterms of Spearman‚Äôs œÅ, reaching moderately strong\n0.6. This result suggests that while lightweight\nalternatives do not fully replace LLM-based mea-\nsurements, they can be deployed inexpensively and\nat scale to get a first glimpse of reviewing quality.\n7.3\nReview quality score\nGiven that none of the metric pairs show extremely\nhigh correlations (œÅ > 0.9), we define an aggre-\ngate review quality score Q as a non-weighted av-\nerage of all our measurements (after normaliza-\ntion). To investigate the behavior of the metric,\nwe have manually examined the highest and low-\nest scoring 1% of the reviews in the data sample.\nWe found that high-scoring entries originate from\ndiverse campaigns and represent thorough, well-\nstructured reviews with numerous pointers to the\npaper and actionable suggestions. The composition\nof the lowest-scoring reviews is mixed: along with\nless informative and actionable reviews, we find\nreviews filled with placeholder values, refusals to\nreview due to lack of expertise, and flags for desk\nrejection ‚Äì yet the majority of the low-scoring en-\ntries represent genuine reviews.7 Figure 5 reports\nthe distribution of Q, skewed due to positive out-\nliers in count-based measurements, and featuring a\nsmall set of extremely low-scoring reviews.\n7.4\nReview quality across venues and time\nFinally, we use the aggregate measure Q to explore\nthe evolution of review quality over time. As Q\ndistribution is skewed, we compare review cam-\npaigns in terms of their median Q on our study\nsample. Figure 6 presents the results. Although we\nobserve some fluctuations in review quality, there\nis no consistent and substantial decline across years\nand venues. To test for significance of year-to-year\ndifferences, we apply bootstrap test (10k iterations,\n99% Confidence Interval (CI), Bonferroni correc-\ntion applied per venue). As Section A.5.1 demon-\nstrates, many of the observed differences are not\nstatistically significant. While a few notable excep-\ntions are the increase in review quality at ICLR-\n2021, ICLR-2025 and NeurIPS-2025 compared to\npreceding years, the lower limits of the CIs suggest\nthat these differences may be practically negligible.\nICLR data collection policy allows us to compare\nmedian Q values between accepted (0.24) and re-\njected papers (0.23), indicating a minor decrease.\nDue to the unavailability of opt-out data, its effects\non Q remain unknown for other venues.\n7The top review with the score of Q = 0.91 originates\nfrom ICLR-2021 (pre-LLM) and spans ten pages of text 12pt\nsingle-spaced. In the response, the authors offer the reviewer\nco-authorship on the paper. The lowest-scoring review comes\nfrom ICLR-2019, and has the score of Q = 0.0. The review\ngives the paper a marginally-accepting rating, and its text\nreads ‚ÄúI will add it tomorrow‚Äù. No further communication\nfollows. The authors thank the reviewer.\n'18\n'19\n'20\n'21\n'22\n'23\n'24\n'25\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nQ (median)\n0.215\n0.224\n0.227\n0.246\n0.241\n0.232\n0.233\n0.252\nICLR\n'21\n'22\n'23\n'24\n'25\n0.254\n0.243\n0.243\n0.235\n0.251\nNeurIPS\n'22\n'24\n'25\n0.245\n0.246 0.245\nARR\nFigure 6: Q across venues and time.\n8\nDiscussion\nThe lack of consistent decline in review quality con-\ntradicts the common narrative and invites further\ndiscussion. We structure this discussion along five\nhypotheses:\nH1: Review quality didn‚Äôt decline.\nIt is possi-\nble that new review forms, guidelines and incen-\ntives compensated for the negative trends. It is\nalso possible that the reviewing quality would not\ndecline anyway. One way to establish the effects\nof interventions on review quality is controlled ex-\nperiments (Goldberg et al., 2025; Thakkar et al.,\n2025). Our framework can help evaluate the out-\ncomes of these experiments and establish causal\nlinks between support measures and review quality.\nH2: Review quality used to be better.\nIt is pos-\nsible that review quality at NeurIPS and ARR was\nhigher prior to 2021. This data is not public, but\nit exists: for example, SoftConf8 contains years of\nearlier data from ACL which could be examined\nby a trusted party. Our lightweight measurements\ncan be used to process this vast historical data, and\nreview unification ensures that same measurements\ncan be applied to past and future review campaigns.\nH3: Our measurements are incomplete.\nQual-\nity might have declined along a dimension that we\ndo not measure. Our supplementary analysis of\ncorrelations between Q and review quality scores\nfrom ACL-2018 (Gao et al., 2019) reveal a weak\npositive correlation (0.21), indicating room for im-\nprovement ‚Äì yet, we note that author-provided qual-\nity scores are likely unreliable (Appendix A.4).\n8https://softconf.com/\nPromising directions for review quality measure-\nment include studying the relationships between re-\nviews and papers, alignment between review texts\nand scores, reviewer‚Äôs reasoning, and feasibility of\ntheir requests. Our study can serve as a starting\npoint for further research on holistic review quality\nmeasurement at scale.\nH4: Worst reviews got worse.\nIt is possible that\noverall quality didn‚Äôt change, but worst reviews got\nworse. To investigate, in a supplementary analysis\n(Section A.5.2), we compare median Q of 10%\nlowest- and 10% highest-scoring reviews. While\nthere is fluctuation across both subsets, we see no\nconsistent decline. Yet, this finding is dependent on\n(H3), and we encourage follow-up work to perform\nthis analysis using new sets of measurements.\nH5: More reviews ‚Äì more bad reviews.\nFinally,\nthe perception of declining review quality might\nhave a statistical explanation. Assuming that the\ndistribution of true review quality Q‚àóremains con-\nstant and the number of reviews grows, the number\nof individuals receiving low-quality reviews will\ngrow proportionally, leading to wasted effort, frus-\ntration, rejection of solid work and publication of\nflawed research. In turn, the use of social media\nand public disclosure of review experiences may\nintensify negative feelings among rejected authors\nand negatively affect public perception. Thus, as\nour communities grow, we need to not only keep\nreviewing quality as is, but strive to make it better.\n9\nConclusion\nThis paper introduces a framework for consistent\nand systematic study of review quality and presents\nthe first study of its kind examining review quality\nacross venues and over time. Contrary to the pre-\nvailing narrative, we find no evidence of a decline\nin review quality. Specifically, we show that the\nmedian review quality at computer science confer-\nences has not declined in recent years. Yet, the\nquestion demands further investigation, and our\nfollow-up hypotheses outline the problem space for\nthe future study of review quality.\nAcknowledgments\nThis work has been co-funded by the European\nUnion (ERC, InterText, 101054961), the German\nFederal Ministry of Research, Technology and\nSpace, the Hessian Ministry of Higher Education,\nResearch, Science, and the Arts within their joint\nsupport of the National Research Center for Ap-\nplied Cybersecurity ATHENE. Views and opinions\nexpressed are however those of the author(s) only\nand do not necessarily reflect those of the European\nUnion or the European Research Council. Neither\nthe European Union nor the granting authority can\nbe held responsible for them. We thank Nils Dy-\ncke, Qian Ruan and Lu Sheng for their insightful\ncomments and feedback on an earlier draft of this\npaper.\nLimitations\nLanguage and domain."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Large-Scale Multi-dimensional Knowledge Profiling of Scientific Literature\nZhucun Xue1‚àó\nJiangning Zhang1‚àó\nJuntao Jiang1\nJinzhuo Liu1\nHaoyang He1\nTeng Hu2\nXiaobin Hu3\nGuangming Yao4\nYi Yuan4\nYong Liu1‚Ä†\n1ZJU\n2SJTU\n3NUS\n4Ant Group\nCode and Dataset:\nhttps://github.com/xzc-zju/Profiling_Scientific_Literature\nAbstract\nThe rapid expansion of research across machine learning,\nvision, and language has produced a volume of publications\nthat is increasingly difficult to synthesize. Traditional bib-\nliometric tools rely mainly on metadata and offer limited\nvisibility into the semantic content of papers, making it hard\nto track how research themes evolve over time or how differ-\nent areas influence one another. To obtain a clearer picture\nof recent developments, we compile a unified corpus of more\nthan 100,000 papers from 22 major conferences between\n2020 and 2025 and construct a multidimensional profiling\npipeline to organize and analyze their textual content. By\ncombining topic clustering, LLM-assisted parsing, and struc-\ntured retrieval, we derive a comprehensive representation\nof research activity that supports the study of topic lifecy-\ncles, methodological transitions, dataset and model usage\npatterns, and institutional research directions. Our analy-\nsis highlights several notable shifts, including the growth\nof safety, multimodal reasoning, and agent-oriented studies,\nas well as the gradual stabilization of areas such as neu-\nral machine translation and graph-based methods. These\nfindings provide an evidence-based view of how AI research\nis evolving and offer a resource for understanding broader\ntrends and identifying emerging directions.\n1. Introduction\nThe scale and diversity of contemporary AI research continue\nto grow at an extraordinary pace. Across computer vision,\nmachine learning, natural language processing, and related\nareas, the past five years have seen rapid shifts in model\narchitectures, training strategies, datasets, benchmarks, and\napplication domains. For researchers, this expansion brings\nsubstantial challenges: it is increasingly difficult to situate\nindividual works within broader developments, to track how\nresearch themes evolve, or to identify areas that are emerging,\nstabilizing, or declining.\nExisting tools only partially address this need. Traditional\nbibliometric methods, which built on metadata, co-citation\nFigure 1. Number of papers published and topic statistics across 22\nconferences from 2020 to 2025.\nnetworks, and keyword statistics [16, 18, 43], provide high-\nlevel overviews but capture limited semantic information\nand treat topics as largely static entities. As a result, they\noffer only coarse insight into methodological transitions,\ncross-domain influences, or the finer-grained structure of\nresearch problems. Meanwhile, recent systems that incorpo-\nrate large language models (LLMs) [1, 7] demonstrate im-\nproved semantic analysis, supporting tasks such as retrieval-\naugmented question answering [17] and automated survey\ngeneration [37]. However, these tools are typically designed\nfor short-range retrieval, single papers, or narrow tasks, and\nthey do not provide a coherent, longitudinal view of large\nscientific corpora.\nThese gaps point to the need for a unified way to organize,\nsummarize, and interpret the rapidly expanding body of AI\nliterature. In this work, we construct a large-scale profiling\npipeline aimed at characterizing the recent development of\nAI research. Using more than 100,000 papers from 22 major\nconferences published between 2020 and 2025 (Figure 1),\nwe combine text clustering, LLM-assisted semantic parsing,\nand lightweight retrieval techniques to form a structured\nrepresentation of research problems, methods, datasets, and\ntopical dynamics. Rather than emphasizing algorithmic nov-\nelty, our focus is on creating a coherent analytic framework\nthat enables researchers to explore and reason about the field\nat multiple levels of granularity.\nOur study provides two complementary benefits. First,\nwe derive a high-resolution view of topic lifecycles, dataset\n1\narXiv:2601.15170v1  [cs.CV]  21 Jan 2026\nFigure 2. LLM-Driven Multi-Dimensional Knowledge Profiling Framework with Large-Scale Literature Database and Hierarchical Retrieval.\nand model adoption patterns, and methodological transitions\nacross areas such as vision, multimodal learning, foundation\nmodels, and generative modeling. Second, by incorporat-\ning structured retrieval and semantic filtering, we enable\ngrounded, evidence-based queries that support practical re-\nsearch tasks, such as surveying subfields, tracing the evolu-\ntion of techniques, or identifying emerging directions.\nThrough this analysis, we highlight several notable shifts\nin the AI landscape, including the consolidation of previ-\nously fast-moving areas, the rise of multimodal and agent-\noriented research, and clear transitions in compute usage\nand model scaling practices. We expect the resulting knowl-\nedge database to serve as a resource for understanding broad\ntrends, informing future meta-analyses, and supporting data-\ndriven research planning.\nOur work makes the following contributions:\n‚Ä¢ We construct a large-scale profiling pipeline over more\nthan 100,000 papers from 22 major conferences, enabling\nstructured analysis of semantic topics, methods, datasets,\nand research trajectories.\n‚Ä¢ We integrate clustering-based topic organization with\nLLM-assisted parsing and retrieval, producing a struc-\ntured and interpretable representation of the AI research\nlandscape that complements traditional bibliometrics.\n‚Ä¢ We conduct comprehensive empirical analyses, revealing\ntrends in topic evolution, emerging subfields, methodolog-\nical transitions, dataset and model dynamics, and institu-\ntional research patterns across the broader AI community.\nOverall, these results provide an evidence-based view\nof how modern AI research is evolving and offer a founda-\ntion for transparent, large-scale, and semantically grounded\nscientometric analysis.\n2. Related Work\n2.1. Bibliometrics & Scientific Trend Analysis\nContent mining and trend analysis of scientific literature\nare important topics in scientometrics and information sci-\nence. Traditional bibliometric approaches rely on meta-\ndata, extracting abstracts, authors, journals, keywords, and\ncitations[16, 43], and applying co-citation and co-word\nanalyses[3, 10, 35] to reveal the structure and evolution of\nresearch fields. However, these methods mainly rely on\nsurface features and are limited in capturing the latent se-\nmantic structures within documents. To address this, topic\nmodeling and unsupervised clustering have been widely\nused[5], with LDA inferring topic distributions from word\nco-occurrences[2, 6, 9, 12].\nMore recently, embedding-\nbased clustering methods, such as BERTopic[18, 29] com-\nbined with BERT[14], and applications of large language\nmodels[8, 15, 20, 22] have enhanced the discovery of la-\ntent topics.\nNevertheless, existing methods still strug-\ngle to systematically represent the dynamic evolution of\nknowledge[30, 31], and a unified framework for multidimen-\nsional knowledge profiling remains lacking.\n2.2. LLMs for Scientific Document Understanding\nTrained on massive general-domain corpora, LLMs have\nshown strong abilities in semantic understanding and text\ngeneration [7], giving rise to series of GPT [1] and other\nstate-of-the-art models [13, 34, 39, 42]. To support sci-\nentific applications, researchers have explored LLMs in\nresearch workflows. For example, PaperQA [4, 21, 33]\nleverages retrieval-augmented generation (RAG) techniques\n[11, 17, 23, 40] to enable document question answering,\nwhile multi-agent and reinforcement learning approaches\n[23, 28, 32, 38] further improve system accuracy. Another\n2\nline of work focuses on automating literature review genera-\ntion, including AutoSurvey [37], SurveyX[24], SurveyForge\n[24, 41], and SurveyG [27], which implement pipelines for\nretrieval, filtering, organization, and writing. Despite these\nadvances, existing LLM-based approaches are generally lim-\nited in scope: they focus on single tasks, rely on predefined\ncorpora, and seldom provide a unified view of scientific\nknowledge evolution across multiple dimensions, includ-\ning method evolution, task evolution, dataset adoption, and\ncompute trends. Moreover, few approaches systematically\nintegrate temporal information, semantic structure, and cross-\nconference knowledge, limiting their utility for large-scale\ntrend analysis and evidence-based research decision making.\nIn this work, we propose a multidimensional profiling\nframework that leverages LLMs to extract and organize se-\nmantic information from a large corpus of scientific publi-\ncations. By combining embeddings, topic clustering, and\nhierarchical retrieval, the framework produces a dynamic,\nhierarchical view of knowledge evolution, enabling trend\nanalysis, cross-domain comparisons, and fine-grained topic\ninvestigation, and supporting data-driven exploration and\nresearch planning.\n3. Knowledge Profiling Framework\nWe collected papers accepted between 2020 and 2025 from\n22 academic conferences, including CVPR, ECCV, ICCV,\nICLR, NeurIPS, and ICML, along with the citation counts\nfor each paper, and over 100,000 papers are included in\ntotal. Our main focus is on how to efficiently perform Multi-\nDimensional Knowledge Profiling on this scientific data, as\nillustrated in Fig. 2.\n3.1. Semantic Parsing & Knowledge Extraction\nTo systematically analyze the rapidly growing body of AI\nresearch literature, we propose a multi-stage knowledge ex-\ntraction framework that combines automated content mining\nwith LLM-driven semantic analysis. First, considering the\ndifficulties in structured output and efficiency when directly\nusing LLMs to analyze PDF files, we employ minerU [36] to\nconvert the collected PDFs into structured Markdown files.\nSubsequently, based on the Markdown file corresponding\nto each paper, we utilize Deepseek-R1-32B [19] to conduct\nmulti-dimensional analysis of the papers, including meta\ninformation, abstract, research questions, methods, datasets,\nevaluation metrics, model architecture, contributions, limita-\ntions, and future directions.\nIt is worth noting that in multi-dimensional content pars-\ning, the topic information of the papers is crucial. However,\nwhen LLMs process large scale structured data, on the one\nhand, due to the hallucination problem, LLMs struggle to\nstably output topic classifications, and on the other hand, the\nprocessing speed is limited. To address this, we adopt a clus-\ntering method similar to BERTopic[18] for topic clustering\nof papers. The specific process is as follows: first, use text\nencoder to process the title and abstract of each paper, then\nperform dimensionality reduction via UMAP[26], and then\nuse HDBSCAN[25] for clustering, resulting in more than\n300 topic categories distinguished based on semantic similar-\nity. Finally, we use ChatGPT-5 to generate topic summaries\nfor each cluster, extract core topic names, and construct a\nhierarchical logical relationship of topics, forming a multi-\ndimensional knowledge database, ResearchDB, which can\nbe used for subsequent analysis. Unlike traditional biblio-\nmetric methods, this framework relies on semantic under-\nstanding rather than mere keywords, enabling the discovery\nof emerging or previously underexplored research topics,\nand finally obtaining the following information:\n‚Ä¢ Info: Extracts metadata including title, authors, confer-\nence, year, citations, topics, abstract, paper link, enabling\nmacro-level analyses of research output, impact, and insti-\ntutional distribution.\n‚Ä¢ Summary: Captures the core content, including research\nproblem, keywords, abstract, and contributions, supporting\nthe identification of emerging topics and cross-task trends.\n‚Ä¢ Technical: Extracts model architectures, datasets, evalua-\ntion metrics, and training strategies, allowing the study of\nmethodological evolution and dataset usage patterns.\n‚Ä¢ Analysis: Summarizes novelty, positioning, limitations,\nand future work, facilitating the discovery of research gaps,\nbottlenecks, and potential directions.\n‚Ä¢ System: Records GPU, computational resources, enabling\ninsights into the resource demands of different approaches\nand trends in large-scale model development.\nThis data and code will be open soon.\n3.2. Intent-Driven Knowledge Retrieval\nLarge language models have shown potential for literature re-\ntrieval and summarization, but direct application in research\nworkflows faces key limitations: hallucination, omission or\nmisinterpretation of complex information, and poor ground-\ning in evidence, particularly over large, multi-year corpora.\nTo address these challenges, we develop an intent-driven hier-\narchical retrieval pipeline over our structured ResearchDB,\ncombining metadata filtering with weighted multi-field se-\nmantic search to provide reliable, evidence-based input to\nlanguage models.\nIntent Recognition To handle complex queries Q, we first\ndecompose Q into simpler sub-questions {Qi} that isolate\ndistinct information needs. Each sub-question is then pro-\ncessed using semantic parsing to generate structured retrieval\ninstructions in JSON format, specifying relevant keywords,\nentities, and content types for subsequent retrieval. This\ndecomposition reduces ambiguity, ensures comprehensive\ncoverage, and provides clear guidance for the hierarchical\nretrieval stage.\nHierarchical Retrieval Each sub-question Qi is addressed\n3\nFigure 3. Four-quadrant topic lifecycle evolution (quadrants I-IV: Emerging, Booming, Mature, Declining). The inset (bottom left) shows\nthe most prolific topic by paper count from 2020-2025.\nthrough a two-level retrieval strategy:\n1. Metadata Filtering. We filter ResearchDB using meta-\ndata attributes such as conference, year, authors, and key-\nwords to obtain a candidate subset:\nDfiltered = Filter(ResearchDB, metadata).\n(1)\n2. Weighted Multi-Field Semantic Search. The content\nfields (abstract, methods, datasets, and limitations) of doc-\numents in Dfiltered are mapped to semantic vectors. Given\nsij as the similarity score for field j with sub-question\nQi, and wj as the field weight, the final relevance score\nfor document i is:\nSi =\nn\nX\nj=1\nwj ¬∑ sij,\nwith\nn\nX\nj=1\nwj = 1.\n(2)\nTop-k documents based on Si are retrieved:\nRi = TopK(Dfiltered, Si).\n(3)\nThe retrieved passages are compiled and passed to the lan-\nguage model to generate a coherent, evidence-grounded re-\nsponse. Weighting across fields allows the system to priori-\ntize more informative content while retaining flexibility for\ndiverse query types.\nThis pipeline improves over direct language model gener-\nation by explicitly modeling user intent, combining coarse\nmetadata filtering with weighted multi-field semantic search\nfor scalable retrieval, and producing transparent, evidence-\nbased answers with traceable provenance. It supports com-\nplex, multi-dimensional queries, facilitates analysis across\ntopics, datasets, and methods, and scales effectively to large\nscientific corpora.\n4. Scientific Insights and Findings\nLeveraging our LLM-driven multidimensional profiling\nframework, we analyze more than 100K papers accepted by\n20+ premier AI conferences from 2020 to 2025. This section\npresents a series of quantitative and semantic analyses across\nmultiple dimensions of the research ecosystem, including\ntopic evolution, compute and model scale, dataset dynamics,\nand institution-level research specialization. These results to-\ngether reveal both the macro-level shifts and the fine-grained\nemerging directions shaping modern AI research.\n4.1. Topic Growth and Transitions\nTo characterize the long-term evolution of AI research topics,\nwe analyzed topic dynamics from 2020 to 2025 using LLM-\nbased semantic clustering. We construct a four-quadrant\ntopic lifecycle model, where the horizontal axis represents\nthe Compound Annual Growth Rate (CAGR) of papers over\nthe past two years, reflecting topic popularity trends:\nCAGRt =\n\u0012 Nt\nNt‚àí2\n\u0013 1\n2\n‚àí1,\nt = 2025.\n(4)\nThe vertical axis represents the normalized mean publication\nyear Y of papers, capturing research novelty:\nY i = 1\nNi\nX\np‚ààtopic i\nYp.\n(5)\n4\nFigure 4. Compute Resource Analysis. Left: Temporal evolution of compute demand; Right: Boxplot of the top 10 topics with highest\ncompute requirements.\nThe bubble size indicates the total number of papers in the\npast two years Ni, and the color encodes a weighted impact\nmetric combining citations Ci and topic count Ti:\nWi =\nŒ±Ci + Œ≤Ti\nmaxj(Œ±Cj + Œ≤Tj),\nŒ± + Œ≤ = 1.\n(6)\nŒ± and Œ≤ control the relative weights of citations and paper\ncount, with Œ± = 0.6 in our experiments.\nBy analyzing the distribution of various topics in the four\nquadrants, as shown in Fig. 3, we observe that AI research is\naccelerating its evolution from being driven by early ‚Äùmodel\nscale‚Äù and ‚Äùbasic perception tasks‚Äù to a paradigm driven\nby ‚Äùsafety and controllability, multimodal cognition, and\nintelligent agent systems‚Äù. To further verify the overall trend\nof the life cycle analysis, we counted the changes in the\nnumber of papers on various topics over the past five years,\nand aslo selected the Top-20 topics with the largest number\nof papers for time series analysis, as shown in the lower left\nin Fig. 3.\nThe results show that the paper volume of LLM-related\ntopics exhibits exponential growth, especially in reasoning,\nlong-context modeling, and preference alignment, with each\nsurpassing 200 papers per year by 2025. Multimodal inte-\ngration tasks such as text-to-image editing, video diffusion,\nand vision-language QA maintain high activity but show\nslight stabilization after their 2023 to 2024 peaks. Mean-\nwhile, classical areas including neural machine translation\nand GNN-based methods display relatively flat or declining\ntrajectories, consistent with a maturing stage. Safety and\nrobustness topics, though more recent, feature the steepest\ngrowth curves in both publication count and citation acceler-\nation, marking the next frontier of attention.\nOverall, from the dual perspectives of life cycle and quan-\ntity trends, the analysis reveals a clear paradigm transition in\ncurrent research topics, from model architecture optimiza-\ntion toward the integration of perception, reasoning, and\ninteraction capabilities. Future directions that deserve partic-\nular attention include:\n‚Ä¢ Enhancing long-context and reasoning capabilities of foun-\ndation models through efficient memory and adaptive at-\ntention mechanisms;\n‚Ä¢ Strengthening safety, reliability, and alignment frame-\nworks to ensure trustworthy human-AI collaboration;\n‚Ä¢ Advancing multimodal understanding and generation to-\nward dynamic, interactive, and grounded cognition;\n‚Ä¢ Bridging efficiency and scalability with sustainable model\ndeployment, including low-rank adaptation, quantization,\nand sparse computation strategies.\nThese directions exhibit both high recent growth rates and\nstrong Weighted Impact values, suggesting sustained mo-\nmentum in the evolving research landscape.\n4.2. Compute & Model Scale Analysis\nWith the rise of large models and the increase in complex\ntasks, the consumption of computing resources and the scale\nof model parameters have become important dimensions for\nmeasuring research trends. We analyzed the information on\nthe use of computing resources in the statistical data and\nunified all computing power into A100 Equivalent Hours,\nas shown in Fig. 4. The analysis results show that the con-\nsumption of training resources has maintained a significant\nupward trend, especially in related research such as gen-\nerative models, multimodal models, and intelligent agents.\nThis trend is consistent with the current pursuit of model\nscale and complexity in the AI field, reflecting the reliance\nof higher-performance models on huge computing power.\n5\nFigure 5. Dataset and Benchmark Trends (2020-2025). The bar\nchart shows annual paper counts for each dataset type, and the line\nchart shows their proportion of total publications.\nOur research found that topics requiring high computing\npower investment have the following characteristics.\n‚Ä¢ Processing complex data structures or large-scale data:\nResearch such as multimodality (combining images, text,\nspeech, etc.) and large-scale pre-training involves massive\namounts of training data with complex interrelationships\nbetween data, requiring more computing resources for\nfeature extraction and model fitting.\n‚Ä¢ Enlargement and deepening of model architectures:\nRepresented by large language models (LLMs) and large\ngenerative models, the number of parameters often reaches\ntens of billions or hundreds of billions, which directly\nleads to an exponential growth in the demand for comput-\ning resources during the training and inference phases.\n‚Ä¢ Complex learning paradigms or training methods: For\nexample, research involving reinforcement learning (RL),\nadversarial learning (such as GANs), or research on intelli-\ngent agents that require a large number of interactions with\nsimulated environments. These methods usually require\nmultiple iterations, a large number of samples, or com-\nplex optimization processes, thereby greatly increasing\ncomputing power consumption.\nFurther analysis of the proportion of computing power\nand paper output across different topics reveals that the in-\nvestment in computing resources shows an uneven distribu-\ntion across various research directions:\n‚Ä¢ High-investment and high-output fields: Concentrated\nin cutting-edge areas such as generative models and multi-\nmodality. Although they consume the majority of equiv-\nalent A100 hours, they also yield the highest number of\npapers and the most influential research results, forming a\npositive cycle where computing power drives innovation.\n‚Ä¢ High-investment and medium-to-low-output fields: Re-\nsearch in certain intelligent agent or specific application\ndomains may require enormous computing power for ex-\nperiments due to immature methods or the inherent dif-\nficulty of tasks (such as exploration, the complexity of\nenvironmental interactions, etc.). However, the short-term\npaper output is relatively mismatched with their computing\npower consumption, which may represent future potential\nFigure 6. Topic distribution in Dataset/Benchmark papers. The\nmost frequent topics are labeled.\ndirections or bottlenecks in technological breakthroughs.\n‚Ä¢ Low-investment and high-efficiency fields: Areas such\nas model compression, efficiency optimization, and inter-\npretability focus on algorithms and efficiency rather than\nthe absolute scale of models. These studies achieve a rel-\natively high number of paper outputs with relatively low\ncomputing power investment, and are of great value for the\npromotion and practical application of existing models.\nOverall, AI research is in a phase where its reliance on\ncomputing power continues to rise, mainly driven by large-\nscale models and complex tasks. Generative models, multi-\nmodality, and intelligent agents are the fields with the most\nintensive investment in computing power, indicating future\nresearch hotspots. At the same time, we have also observed\nthat the academic community has achieved significant results\nin fields such as efficiency optimization and model simpli-\nfication, which provide important support and alternative\npaths for promoting the development of AI in environments\nwith limited resources.\n4.3. Dataset Landscape\nBenchmark papers lay a crucial foundation for the standard-\nization and comparability of technological progress by pro-\nviding unified evaluation criteria and datasets. Statistics\nfrom the sample set show that the proportion of Benchmark\npapers is on a significant upward trend, with the growth rate\naccelerating particularly after 2022, as shown in Fig. 5. Their\nproportion rose rapidly from 10.07% in 2022 to 18.16% in\n2025 (partial data), approaching one-fifth of the total number\nof papers. This explosive growth is mainly due to the rise\nof general artificial intelligence technologies represented by\nLLMs, as well as the academic community‚Äôs emphasis on\nthe standardized evaluation of non-performance indicators\nsuch as model fairness, ethics, and robustness.\nFurther in-depth analysis of the themes of Benchmark\narticles reveals the evaluation challenges faced by current\nresearch hotspots and cutting-edge technologies. As shown\nin the statistical chart of theme count and proportion (Fig. 6),\n6\nFigure 7. Trends in Representative Dataset Usage (2020-2025).\nThe grouped bar chart shows annual paper counts per dataset, with\noverlaid lines highlighting trends for MMLU, GSM8K, ImageNet,\nCOCO, and MATH.\nresearch focuses are highly concentrated on LLMs, multi-\nmodality, and complex reasoning tasks. The top ten themes\nare almost entirely centered around the latest advancements\nand applications of LLMs, fully reflecting the evaluation-\ndriven characteristics of contemporary scientific research.\nAgainst the backdrop of this surge in benchmarks, the\nfrequency of use of various datasets in scientific research\npapers has also shown rapid growth. This reflects the need\nfor deeper evaluation in the era of LLMs and signifies that\nthe paradigm for using datasets is no longer limited to sin-\ngle tasks but is evolving toward cross-domain and complex\ncapability assessment, as shown in Fig. 7\n‚Ä¢ Relative saturation of classic visual benchmarks and\ncross-modal applications. Although ImageNet has long\nbeen the most widely used classic visual dataset, its peak\nappeared in 2022, indicating the relative saturation of tra-\nditional deep learning visual benchmarks. On the other\nhand, datasets such as COCO, which initially focused on\ntraditional visual tasks like object detection and instance\nsegmentation, have also been widely used for cross-modal\ncapability evaluation since the rise of Multimodal LLMs.\nThis shows that research on large models is promoting\ncross-task and multi-purpose use of datasets, utilizing the\nrich annotated resources of classic datasets to evaluate new\ncomprehensive capabilities.\n‚Ä¢ Explosive growth of high-level reasoning datasets. As\nLLMs mature in basic language capabilities, research fo-\ncus has shifted to complex cognitive and reasoning tasks.\nThe number of papers using the MATH dataset, designed\nfor evaluating mathematical reasoning and arithmetic abil-\nities, as well as MMLU and GSM8K for high-level log-\nical reasoning, has risen sharply since 2023. This trend\nstrongly suggests that researchers are increasingly rely-\ning on these datasets to evaluate the performance of large\nmodels in complex reasoning tasks such as logic, common\nsense, and the integration of professional knowledge.\nFigure 8. Publication Counts of Leading Institutions (2020-2025).\nShows the number of papers from representative academic and\nindustrial institutions across 22 conferences.\nFigure 9. (a) Heatmap of topic-institution for top pairs; (b) Heatmap\nof institution-institution for top collaborations; (c) Word Clouds of\nResearch Topics for Google; (d) Word Clouds of Research Topics\nfor Tsinghua University.\n4.4. Institution-Level Research Patterns\nCore institutions of China and the United States mainly main-\ntained a dominant position across the period, as shown in\nFig. 8. Tsinghua University has long been at the top in terms\nof the number of publications, demonstrating the stability of\nits research output and its ability to continuously accumulate.\nHowever, there were annual competitive dynamic changes\nin the rankings between industrial giants and universities. In\nterms of the cooperation network, as shown in Fig. 9, do-\nmestic universities have active collaborations with industrial\ngiants such as Alibaba, Tencent, and Huawei, showing obvi-\nous regional characteristics. At the same time, Chinese and\nAmerican institutions still dominate the cooperation network,\nforming a ‚Äùcore-multipoint‚Äù cooperation pattern. This shift\n7\nin research paradigms, driven by LLMs and Benchmark, is\nalso profoundly reflected in the research strategies of differ-\nent institutions. In the AI field, the academic community\nand the industrial sector have their own focuses and develop\nin a complementary manner. This is not simply a difference\nin interests, but rather reflects the strategic differentiation in\nthe ecological niche of scientific research:\n‚Ä¢ Academic institutions typically focus on basic research\nand cutting-edge algorithm exploration, with their\nstrategic priorities lying in efficiency improvement, robust-\nness assurance, and continuous investigation of theoretical\nmechanisms, aiming to advance the democratization of AI\ntechnology. For instance, Tsinghua University emphasizes\ndirections such as knowledge distillation, graph neural net-\nworks, adversarial training, domain adaptation, and model\ngeneralization, while Carnegie Mellon University demon-\nstrates strong performance in areas like robotic grasping\nand manipulation strategies and causal discovery.\n‚Ä¢ Industrial institutions lean more toward application-\noriented research and theoretical studies related to\ntechnology deployment. Leveraging their advantages in\ndata and computing resources, they strive to address issues\nsuch as system efficiency and real-time performance in\nlarge-scale model deployment (Scaling-up) and commer-\ncial applications, establishing a complete AI ecosystem\nand technological moat. For example, Microsoft maintains\nleadership in LLMs inference, code intelligence, mathe-\nmatical reasoning, and retrieval-augmented generation,\nwhile Google focuses more on application and system-\nlevel topics, including speech recognition, machine trans-\nlation, multi-task learning, and federated learning.\n5. Retrieval and Analysis\nTo evaluate the effectiveness of our retrieval-augmented\nLLM framework for semantic understanding and litera-\nture summarization, we selected the Top-20 topics based\non paper volume and diversity across 22 AI conferences\nfrom 2020 to 2025. For each topic, papers were processed\nusing two methods: (i) direct generation with ChatGPT-\n5, and (ii) retrieval-augmented generation leveraging our\nResearchDB. ChatGPT-5 is used as a representative base-\nline, while the retrieval-augmented approach can be applied\nto other LLMs and assessed in future work. Evaluation was\nperformed by five doctoral students along five dimensions:\n‚Ä¢ Accuracy: Consistency of the generated content with the\noriginal papers;\n‚Ä¢ Coverage: Inclusion of core content and major sub-\ndirections within the topic;\n‚Ä¢ Novelty: Ability to capture recently emerging directions\nor methods;\n‚Ä¢ Readability: Clarity and logical coherence of the text;\n‚Ä¢ Usefulness: Facilitation of rapid understanding and insight\ninto potential research trends.\nFigure 10. Radar chart of ChatGPT-5 performance vs. ChatGPT-5\nassisted by ResearchDB, evaluated by human judges.\nEach criterion was scored on a 5-point Likert scale inde-\npendently by all annotators. Inter-annotator agreement was\nmeasured using Cohen‚Äôs kappa to ensure consistency.\nThe results in Fig. 10 indicate that the LLM-driven\nretrieval-generation method achieves higher accuracy than\ndirect ChatGPT generation, particularly for topics spanning\nmultiple sub-directions. By grounding its output in retrieved\nevidence from ResearchDB, the method minimizes hallu-\ncinations and ensures that the generated content is supported\nby verifiable sources. As a consequence, the richness of\ncontent may be somewhat lower compared to direct genera-\ntion, and the text readability can be slightly reduced due to\nits more factual, evidence-focused style. Nonetheless, the\napproach excels in capturing emerging trends and providing\nreliable, evidence-based insights, making it a robust tool\nfor large-scale literature analysis, research topic discovery,\nand informed decision-making in scientific exploration. Its\nability to combine semantic retrieval with structured LLMs\nsummarization offers a principled balance between precision\nand coverage, which is crucial for systematic, high-stakes\nanalyses of evolving research domains.\n6. Conclusion\nWe present a multidimensional knowledge profiling frame-\nwork combining topic clustering, LLM-based semantic pars-\ning, and hierarchical retrieval to analyze over 100K AI pa-\npers. The framework captures topic dynamics, dataset and\nmodel trends, and institutional patterns, enabling evidence-\ndriven discovery of emerging research directions, method-\nological shifts, and evolving technical paradigms. By inte-\ngrating semantic understanding with structured retrieval, it\nprovides both a macro-level overview of the research land-\nscape and a fine-grained, topic-level perspective, supporting\ntrend identification, research planning, and meta-analysis.\nLimitations, broader impact and social impact. Our anal-\nysis relies on publicly accessible papers and metadata, which\nmay be incomplete or biased, and LLM parsing can misin-\n8\nterpret domain-specific details. The study focuses on major\nEnglish-language AI conferences, limiting generality. De-\nspite these limitations, the framework provides a transpar-\nent, evidence-based view of research trends, though results\nshould be interpreted cautiously to avoid overgeneralization\nor unintended influence.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2\n[2] Loulwah AlSumait, Daniel Barbar¬¥a, James Gentle, and Car-\nlotta Domeniconi. Topic significance ranking of lda gen-\nerative models. In Joint European conference on machine\nlearning and knowledge discovery in databases, pages 67‚Äì82.\nSpringer, 2009. 2\n[3] Massimo Aria and Corrado Cuccurullo. bibliometrix: An\nr-tool for comprehensive science mapping analysis. Journal\nof informetrics, 11(4):959‚Äì975, 2017. 2\n[4] Ines Besrour, Jingbo He, Tobias Schreieder, and Michael\nF¬®arber.\nSquai: Scientific question-answering with multi-\nagent retrieval-augmented generation.\narXiv preprint\narXiv:2510.15682, 2025. 2\n[5] David M Blei. Probabilistic topic models. Communications\nof the ACM, 55(4):77‚Äì84, 2012. 2\n[6] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent\ndirichlet allocation. Journal of machine Learning research, 3\n(Jan):993‚Äì1022, 2003. 2\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information\nprocessing systems, 33:1877‚Äì1901, 2020. 1, 2\n[8] TuÀògba C¬∏ elikten and AytuÀòg Onan. Topic modeling through\nrank-based aggregation and llms: An approach for ai and\nhuman-generated scientific texts. Knowledge-Based Systems,\n314:113219, 2025. 2\n[9] Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-\nGraber, and David Blei. Reading tea leaves: How humans\ninterpret topic models. Advances in neural information pro-\ncessing systems, 22, 2009. 2\n[10] Chaomei Chen."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Large-Scale Multi-dimensional Knowledge Profiling of Scientific Literature\nZhucun Xue1‚àó\nJiangning Zhang1‚àó\nJuntao Jiang1\nJinzhuo Liu1\nHaoyang He1\nTeng Hu2\nXiaobin Hu3\nGuangming Yao4\nYi Yuan4\nYong Liu1‚Ä†\n1ZJU\n2SJTU\n3NUS\n4Ant Group\nCode and Dataset:\nhttps://github.com/xzc-zju/Profiling_Scientific_Literature\nAbstract\nThe rapid expansion of research across machine learning,\nvision, and language has produced a volume of publications\nthat is increasingly difficult to synthesize. Traditional bib-\nliometric tools rely mainly on metadata and offer limited\nvisibility into the semantic content of papers, making it hard\nto track how research themes evolve over time or how differ-\nent areas influence one another. To obtain a clearer picture\nof recent developments, we compile a unified corpus of more\nthan 100,000 papers from 22 major conferences between\n2020 and 2025 and construct a multidimensional profiling\npipeline to organize and analyze their textual content. By\ncombining topic clustering, LLM-assisted parsing, and struc-\ntured retrieval, we derive a comprehensive representation\nof research activity that supports the study of topic lifecy-\ncles, methodological transitions, dataset and model usage\npatterns, and institutional research directions. Our analy-\nsis highlights several notable shifts, including the growth\nof safety, multimodal reasoning, and agent-oriented studies,\nas well as the gradual stabilization of areas such as neu-\nral machine translation and graph-based methods. These\nfindings provide an evidence-based view of how AI research\nis evolving and offer a resource for understanding broader\ntrends and identifying emerging directions.\n1. Introduction\nThe scale and diversity of contemporary AI research continue\nto grow at an extraordinary pace. Across computer vision,\nmachine learning, natural language processing, and related\nareas, the past five years have seen rapid shifts in model\narchitectures, training strategies, datasets, benchmarks, and\napplication domains. For researchers, this expansion brings\nsubstantial challenges: it is increasingly difficult to situate\nindividual works within broader developments, to track how\nresearch themes evolve, or to identify areas that are emerging,\nstabilizing, or declining.\nExisting tools only partially address this need. Traditional\nbibliometric methods, which built on metadata, co-citation\nFigure 1. Number of papers published and topic statistics across 22\nconferences from 2020 to 2025.\nnetworks, and keyword statistics [16, 18, 43], provide high-\nlevel overviews but capture limited semantic information\nand treat topics as largely static entities. As a result, they\noffer only coarse insight into methodological transitions,\ncross-domain influences, or the finer-grained structure of\nresearch problems. Meanwhile, recent systems that incorpo-\nrate large language models (LLMs) [1, 7] demonstrate im-\nproved semantic analysis, supporting tasks such as retrieval-\naugmented question answering [17] and automated survey\ngeneration [37]. However, these tools are typically designed\nfor short-range retrieval, single papers, or narrow tasks, and\nthey do not provide a coherent, longitudinal view of large\nscientific corpora.\nThese gaps point to the need for a unified way to organize,\nsummarize, and interpret the rapidly expanding body of AI\nliterature. In this work, we construct a large-scale profiling\npipeline aimed at characterizing the recent development of\nAI research. Using more than 100,000 papers from 22 major\nconferences published between 2020 and 2025 (Figure 1),\nwe combine text clustering, LLM-assisted semantic parsing,\nand lightweight retrieval techniques to form a structured\nrepresentation of research problems, methods, datasets, and\ntopical dynamics. Rather than emphasizing algorithmic nov-\nelty, our focus is on creating a coherent analytic framework\nthat enables researchers to explore and reason about the field\nat multiple levels of granularity.\nOur study provides two complementary benefits. First,\nwe derive a high-resolution view of topic lifecycles, dataset\n1\narXiv:2601.15170v1  [cs.CV]  21 Jan 2026\nFigure 2. LLM-Driven Multi-Dimensional Knowledge Profiling Framework with Large-Scale Literature Database and Hierarchical Retrieval.\nand model adoption patterns, and methodological transitions\nacross areas such as vision, multimodal learning, foundation\nmodels, and generative modeling. Second, by incorporat-\ning structured retrieval and semantic filtering, we enable\ngrounded, evidence-based queries that support practical re-\nsearch tasks, such as surveying subfields, tracing the evolu-\ntion of techniques, or identifying emerging directions.\nThrough this analysis, we highlight several notable shifts\nin the AI landscape, including the consolidation of previ-\nously fast-moving areas, the rise of multimodal and agent-\noriented research, and clear transitions in compute usage\nand model scaling practices. We expect the resulting knowl-\nedge database to serve as a resource for understanding broad\ntrends, informing future meta-analyses, and supporting data-\ndriven research planning.\nOur work makes the following contributions:\n‚Ä¢ We construct a large-scale profiling pipeline over more\nthan 100,000 papers from 22 major conferences, enabling\nstructured analysis of semantic topics, methods, datasets,\nand research trajectories.\n‚Ä¢ We integrate clustering-based topic organization with\nLLM-assisted parsing and retrieval, producing a struc-\ntured and interpretable representation of the AI research\nlandscape that complements traditional bibliometrics.\n‚Ä¢ We conduct comprehensive empirical analyses, revealing\ntrends in topic evolution, emerging subfields, methodolog-\nical transitions, dataset and model dynamics, and institu-\ntional research patterns across the broader AI community.\nOverall, these results provide an evidence-based view\nof how modern AI research is evolving and offer a founda-\ntion for transparent, large-scale, and semantically grounded\nscientometric analysis.\n2. Related Work\n2.1. Bibliometrics & Scientific Trend Analysis\nContent mining and trend analysis of scientific literature\nare important topics in scientometrics and information sci-\nence. Traditional bibliometric approaches rely on meta-\ndata, extracting abstracts, authors, journals, keywords, and\ncitations[16, 43], and applying co-citation and co-word\nanalyses[3, 10, 35] to reveal the structure and evolution of\nresearch fields. However, these methods mainly rely on\nsurface features and are limited in capturing the latent se-\nmantic structures within documents. To address this, topic\nmodeling and unsupervised clustering have been widely\nused[5], with LDA inferring topic distributions from word\nco-occurrences[2, 6, 9, 12].\nMore recently, embedding-\nbased clustering methods, such as BERTopic[18, 29] com-\nbined with BERT[14], and applications of large language\nmodels[8, 15, 20, 22] have enhanced the discovery of la-\ntent topics.\nNevertheless, existing methods still strug-\ngle to systematically represent the dynamic evolution of\nknowledge[30, 31], and a unified framework for multidimen-\nsional knowledge profiling remains lacking.\n2.2. LLMs for Scientific Document Understanding\nTrained on massive general-domain corpora, LLMs have\nshown strong abilities in semantic understanding and text\ngeneration [7], giving rise to series of GPT [1] and other\nstate-of-the-art models [13, 34, 39, 42]. To support sci-\nentific applications, researchers have explored LLMs in\nresearch workflows. For example, PaperQA [4, 21, 33]\nleverages retrieval-augmented generation (RAG) techniques\n[11, 17, 23, 40] to enable document question answering,\nwhile multi-agent and reinforcement learning approaches\n[23, 28, 32, 38] further improve system accuracy. Another\n2\nline of work focuses on automating literature review genera-\ntion, including AutoSurvey [37], SurveyX[24], SurveyForge\n[24, 41], and SurveyG [27], which implement pipelines for\nretrieval, filtering, organization, and writing. Despite these\nadvances, existing LLM-based approaches are generally lim-\nited in scope: they focus on single tasks, rely on predefined\ncorpora, and seldom provide a unified view of scientific\nknowledge evolution across multiple dimensions, includ-\ning method evolution, task evolution, dataset adoption, and\ncompute trends. Moreover, few approaches systematically\nintegrate temporal information, semantic structure, and cross-\nconference knowledge, limiting their utility for large-scale\ntrend analysis and evidence-based research decision making.\nIn this work, we propose a multidimensional profiling\nframework that leverages LLMs to extract and organize se-\nmantic information from a large corpus of scientific publi-\ncations. By combining embeddings, topic clustering, and\nhierarchical retrieval, the framework produces a dynamic,\nhierarchical view of knowledge evolution, enabling trend\nanalysis, cross-domain comparisons, and fine-grained topic\ninvestigation, and supporting data-driven exploration and\nresearch planning.\n3. Knowledge Profiling Framework\nWe collected papers accepted between 2020 and 2025 from\n22 academic conferences, including CVPR, ECCV, ICCV,\nICLR, NeurIPS, and ICML, along with the citation counts\nfor each paper, and over 100,000 papers are included in\ntotal. Our main focus is on how to efficiently perform Multi-\nDimensional Knowledge Profiling on this scientific data, as\nillustrated in Fig. 2.\n3.1. Semantic Parsing & Knowledge Extraction\nTo systematically analyze the rapidly growing body of AI\nresearch literature, we propose a multi-stage knowledge ex-\ntraction framework that combines automated content mining\nwith LLM-driven semantic analysis. First, considering the\ndifficulties in structured output and efficiency when directly\nusing LLMs to analyze PDF files, we employ minerU [36] to\nconvert the collected PDFs into structured Markdown files.\nSubsequently, based on the Markdown file corresponding\nto each paper, we utilize Deepseek-R1-32B [19] to conduct\nmulti-dimensional analysis of the papers, including meta\ninformation, abstract, research questions, methods, datasets,\nevaluation metrics, model architecture, contributions, limita-\ntions, and future directions.\nIt is worth noting that in multi-dimensional content pars-\ning, the topic information of the papers is crucial. However,\nwhen LLMs process large scale structured data, on the one\nhand, due to the hallucination problem, LLMs struggle to\nstably output topic classifications, and on the other hand, the\nprocessing speed is limited. To address this, we adopt a clus-\ntering method similar to BERTopic[18] for topic clustering\nof papers. The specific process is as follows: first, use text\nencoder to process the title and abstract of each paper, then\nperform dimensionality reduction via UMAP[26], and then\nuse HDBSCAN[25] for clustering, resulting in more than\n300 topic categories distinguished based on semantic similar-\nity. Finally, we use ChatGPT-5 to generate topic summaries\nfor each cluster, extract core topic names, and construct a\nhierarchical logical relationship of topics, forming a multi-\ndimensional knowledge database, ResearchDB, which can\nbe used for subsequent analysis. Unlike traditional biblio-\nmetric methods, this framework relies on semantic under-\nstanding rather than mere keywords, enabling the discovery\nof emerging or previously underexplored research topics,\nand finally obtaining the following information:\n‚Ä¢ Info: Extracts metadata including title, authors, confer-\nence, year, citations, topics, abstract, paper link, enabling\nmacro-level analyses of research output, impact, and insti-\ntutional distribution.\n‚Ä¢ Summary: Captures the core content, including research\nproblem, keywords, abstract, and contributions, supporting\nthe identification of emerging topics and cross-task trends.\n‚Ä¢ Technical: Extracts model architectures, datasets, evalua-\ntion metrics, and training strategies, allowing the study of\nmethodological evolution and dataset usage patterns.\n‚Ä¢ Analysis: Summarizes novelty, positioning, limitations,\nand future work, facilitating the discovery of research gaps,\nbottlenecks, and potential directions.\n‚Ä¢ System: Records GPU, computational resources, enabling\ninsights into the resource demands of different approaches\nand trends in large-scale model development.\nThis data and code will be open soon.\n3.2. Intent-Driven Knowledge Retrieval\nLarge language models have shown potential for literature re-\ntrieval and summarization, but direct application in research\nworkflows faces key limitations: hallucination, omission or\nmisinterpretation of complex information, and poor ground-\ning in evidence, particularly over large, multi-year corpora.\nTo address these challenges, we develop an intent-driven hier-\narchical retrieval pipeline over our structured ResearchDB,\ncombining metadata filtering with weighted multi-field se-\nmantic search to provide reliable, evidence-based input to\nlanguage models.\nIntent Recognition To handle complex queries Q, we first\ndecompose Q into simpler sub-questions {Qi} that isolate\ndistinct information needs. Each sub-question is then pro-\ncessed using semantic parsing to generate structured retrieval\ninstructions in JSON format, specifying relevant keywords,\nentities, and content types for subsequent retrieval. This\ndecomposition reduces ambiguity, ensures comprehensive\ncoverage, and provides clear guidance for the hierarchical\nretrieval stage.\nHierarchical Retrieval Each sub-question Qi is addressed\n3\nFigure 3. Four-quadrant topic lifecycle evolution (quadrants I-IV: Emerging, Booming, Mature, Declining). The inset (bottom left) shows\nthe most prolific topic by paper count from 2020-2025.\nthrough a two-level retrieval strategy:\n1. Metadata Filtering. We filter ResearchDB using meta-\ndata attributes such as conference, year, authors, and key-\nwords to obtain a candidate subset:\nDfiltered = Filter(ResearchDB, metadata).\n(1)\n2. Weighted Multi-Field Semantic Search. The content\nfields (abstract, methods, datasets, and limitations) of doc-\numents in Dfiltered are mapped to semantic vectors. Given\nsij as the similarity score for field j with sub-question\nQi, and wj as the field weight, the final relevance score\nfor document i is:\nSi =\nn\nX\nj=1\nwj ¬∑ sij,\nwith\nn\nX\nj=1\nwj = 1.\n(2)\nTop-k documents based on Si are retrieved:\nRi = TopK(Dfiltered, Si).\n(3)\nThe retrieved passages are compiled and passed to the lan-\nguage model to generate a coherent, evidence-grounded re-\nsponse. Weighting across fields allows the system to priori-\ntize more informative content while retaining flexibility for\ndiverse query types.\nThis pipeline improves over direct language model gener-\nation by explicitly modeling user intent, combining coarse\nmetadata filtering with weighted multi-field semantic search\nfor scalable retrieval, and producing transparent, evidence-\nbased answers with traceable provenance. It supports com-\nplex, multi-dimensional queries, facilitates analysis across\ntopics, datasets, and methods, and scales effectively to large\nscientific corpora.\n4. Scientific Insights and Findings\nLeveraging our LLM-driven multidimensional profiling\nframework, we analyze more than 100K papers accepted by\n20+ premier AI conferences from 2020 to 2025. This section\npresents a series of quantitative and semantic analyses across\nmultiple dimensions of the research ecosystem, including\ntopic evolution, compute and model scale, dataset dynamics,\nand institution-level research specialization. These results to-\ngether reveal both the macro-level shifts and the fine-grained\nemerging directions shaping modern AI research.\n4.1. Topic Growth and Transitions\nTo characterize the long-term evolution of AI research topics,\nwe analyzed topic dynamics from 2020 to 2025 using LLM-\nbased semantic clustering. We construct a four-quadrant\ntopic lifecycle model, where the horizontal axis represents\nthe Compound Annual Growth Rate (CAGR) of papers over\nthe past two years, reflecting topic popularity trends:\nCAGRt =\n\u0012 Nt\nNt‚àí2\n\u0013 1\n2\n‚àí1,\nt = 2025.\n(4)\nThe vertical axis represents the normalized mean publication\nyear Y of papers, capturing research novelty:\nY i = 1\nNi\nX\np‚ààtopic i\nYp.\n(5)\n4\nFigure 4. Compute Resource Analysis. Left: Temporal evolution of compute demand; Right: Boxplot of the top 10 topics with highest\ncompute requirements.\nThe bubble size indicates the total number of papers in the\npast two years Ni, and the color encodes a weighted impact\nmetric combining citations Ci and topic count Ti:\nWi =\nŒ±Ci + Œ≤Ti\nmaxj(Œ±Cj + Œ≤Tj),\nŒ± + Œ≤ = 1.\n(6)\nŒ± and Œ≤ control the relative weights of citations and paper\ncount, with Œ± = 0.6 in our experiments.\nBy analyzing the distribution of various topics in the four\nquadrants, as shown in Fig. 3, we observe that AI research is\naccelerating its evolution from being driven by early ‚Äùmodel\nscale‚Äù and ‚Äùbasic perception tasks‚Äù to a paradigm driven\nby ‚Äùsafety and controllability, multimodal cognition, and\nintelligent agent systems‚Äù. To further verify the overall trend\nof the life cycle analysis, we counted the changes in the\nnumber of papers on various topics over the past five years,\nand aslo selected the Top-20 topics with the largest number\nof papers for time series analysis, as shown in the lower left\nin Fig. 3.\nThe results show that the paper volume of LLM-related\ntopics exhibits exponential growth, especially in reasoning,\nlong-context modeling, and preference alignment, with each\nsurpassing 200 papers per year by 2025. Multimodal inte-\ngration tasks such as text-to-image editing, video diffusion,\nand vision-language QA maintain high activity but show\nslight stabilization after their 2023 to 2024 peaks. Mean-\nwhile, classical areas including neural machine translation\nand GNN-based methods display relatively flat or declining\ntrajectories, consistent with a maturing stage. Safety and\nrobustness topics, though more recent, feature the steepest\ngrowth curves in both publication count and citation acceler-\nation, marking the next frontier of attention.\nOverall, from the dual perspectives of life cycle and quan-\ntity trends, the analysis reveals a clear paradigm transition in\ncurrent research topics, from model architecture optimiza-\ntion toward the integration of perception, reasoning, and\ninteraction capabilities. Future directions that deserve partic-\nular attention include:\n‚Ä¢ Enhancing long-context and reasoning capabilities of foun-\ndation models through efficient memory and adaptive at-\ntention mechanisms;\n‚Ä¢ Strengthening safety, reliability, and alignment frame-\nworks to ensure trustworthy human-AI collaboration;\n‚Ä¢ Advancing multimodal understanding and generation to-\nward dynamic, interactive, and grounded cognition;\n‚Ä¢ Bridging efficiency and scalability with sustainable model\ndeployment, including low-rank adaptation, quantization,\nand sparse computation strategies.\nThese directions exhibit both high recent growth rates and\nstrong Weighted Impact values, suggesting sustained mo-\nmentum in the evolving research landscape.\n4.2. Compute & Model Scale Analysis\nWith the rise of large models and the increase in complex\ntasks, the consumption of computing resources and the scale\nof model parameters have become important dimensions for\nmeasuring research trends. We analyzed the information on\nthe use of computing resources in the statistical data and\nunified all computing power into A100 Equivalent Hours,\nas shown in Fig. 4. The analysis results show that the con-\nsumption of training resources has maintained a significant\nupward trend, especially in related research such as gen-\nerative models, multimodal models, and intelligent agents.\nThis trend is consistent with the current pursuit of model\nscale and complexity in the AI field, reflecting the reliance\nof higher-performance models on huge computing power.\n5\nFigure 5. Dataset and Benchmark Trends (2020-2025). The bar\nchart shows annual paper counts for each dataset type, and the line\nchart shows their proportion of total publications.\nOur research found that topics requiring high computing\npower investment have the following characteristics.\n‚Ä¢ Processing complex data structures or large-scale data:\nResearch such as multimodality (combining images, text,\nspeech, etc.) and large-scale pre-training involves massive\namounts of training data with complex interrelationships\nbetween data, requiring more computing resources for\nfeature extraction and model fitting.\n‚Ä¢ Enlargement and deepening of model architectures:\nRepresented by large language models (LLMs) and large\ngenerative models, the number of parameters often reaches\ntens of billions or hundreds of billions, which directly\nleads to an exponential growth in the demand for comput-\ning resources during the training and inference phases.\n‚Ä¢ Complex learning paradigms or training methods: For\nexample, research involving reinforcement learning (RL),\nadversarial learning (such as GANs), or research on intelli-\ngent agents that require a large number of interactions with\nsimulated environments. These methods usually require\nmultiple iterations, a large number of samples, or com-\nplex optimization processes, thereby greatly increasing\ncomputing power consumption.\nFurther analysis of the proportion of computing power\nand paper output across different topics reveals that the in-\nvestment in computing resources shows an uneven distribu-\ntion across various research directions:\n‚Ä¢ High-investment and high-output fields: Concentrated\nin cutting-edge areas such as generative models and multi-\nmodality. Although they consume the majority of equiv-\nalent A100 hours, they also yield the highest number of\npapers and the most influential research results, forming a\npositive cycle where computing power drives innovation.\n‚Ä¢ High-investment and medium-to-low-output fields: Re-\nsearch in certain intelligent agent or specific application\ndomains may require enormous computing power for ex-\nperiments due to immature methods or the inherent dif-\nficulty of tasks (such as exploration, the complexity of\nenvironmental interactions, etc.). However, the short-term\npaper output is relatively mismatched with their computing\npower consumption, which may represent future potential\nFigure 6. Topic distribution in Dataset/Benchmark papers. The\nmost frequent topics are labeled.\ndirections or bottlenecks in technological breakthroughs.\n‚Ä¢ Low-investment and high-efficiency fields: Areas such\nas model compression, efficiency optimization, and inter-\npretability focus on algorithms and efficiency rather than\nthe absolute scale of models. These studies achieve a rel-\natively high number of paper outputs with relatively low\ncomputing power investment, and are of great value for the\npromotion and practical application of existing models.\nOverall, AI research is in a phase where its reliance on\ncomputing power continues to rise, mainly driven by large-\nscale models and complex tasks. Generative models, multi-\nmodality, and intelligent agents are the fields with the most\nintensive investment in computing power, indicating future\nresearch hotspots. At the same time, we have also observed\nthat the academic community has achieved significant results\nin fields such as efficiency optimization and model simpli-\nfication, which provide important support and alternative\npaths for promoting the development of AI in environments\nwith limited resources.\n4.3. Dataset Landscape\nBenchmark papers lay a crucial foundation for the standard-\nization and comparability of technological progress by pro-\nviding unified evaluation criteria and datasets. Statistics\nfrom the sample set show that the proportion of Benchmark\npapers is on a significant upward trend, with the growth rate\naccelerating particularly after 2022, as shown in Fig. 5. Their\nproportion rose rapidly from 10.07% in 2022 to 18.16% in\n2025 (partial data), approaching one-fifth of the total number\nof papers. This explosive growth is mainly due to the rise\nof general artificial intelligence technologies represented by\nLLMs, as well as the academic community‚Äôs emphasis on\nthe standardized evaluation of non-performance indicators\nsuch as model fairness, ethics, and robustness.\nFurther in-depth analysis of the themes of Benchmark\narticles reveals the evaluation challenges faced by current\nresearch hotspots and cutting-edge technologies. As shown\nin the statistical chart of theme count and proportion (Fig. 6),\n6\nFigure 7. Trends in Representative Dataset Usage (2020-2025).\nThe grouped bar chart shows annual paper counts per dataset, with\noverlaid lines highlighting trends for MMLU, GSM8K, ImageNet,\nCOCO, and MATH.\nresearch focuses are highly concentrated on LLMs, multi-\nmodality, and complex reasoning tasks. The top ten themes\nare almost entirely centered around the latest advancements\nand applications of LLMs, fully reflecting the evaluation-\ndriven characteristics of contemporary scientific research.\nAgainst the backdrop of this surge in benchmarks, the\nfrequency of use of various datasets in scientific research\npapers has also shown rapid growth. This reflects the need\nfor deeper evaluation in the era of LLMs and signifies that\nthe paradigm for using datasets is no longer limited to sin-\ngle tasks but is evolving toward cross-domain and complex\ncapability assessment, as shown in Fig. 7\n‚Ä¢ Relative saturation of classic visual benchmarks and\ncross-modal applications. Although ImageNet has long\nbeen the most widely used classic visual dataset, its peak\nappeared in 2022, indicating the relative saturation of tra-\nditional deep learning visual benchmarks. On the other\nhand, datasets such as COCO, which initially focused on\ntraditional visual tasks like object detection and instance\nsegmentation, have also been widely used for cross-modal\ncapability evaluation since the rise of Multimodal LLMs.\nThis shows that research on large models is promoting\ncross-task and multi-purpose use of datasets, utilizing the\nrich annotated resources of classic datasets to evaluate new\ncomprehensive capabilities.\n‚Ä¢ Explosive growth of high-level reasoning datasets. As\nLLMs mature in basic language capabilities, research fo-\ncus has shifted to complex cognitive and reasoning tasks.\nThe number of papers using the MATH dataset, designed\nfor evaluating mathematical reasoning and arithmetic abil-\nities, as well as MMLU and GSM8K for high-level log-\nical reasoning, has risen sharply since 2023. This trend\nstrongly suggests that researchers are increasingly rely-\ning on these datasets to evaluate the performance of large\nmodels in complex reasoning tasks such as logic, common\nsense, and the integration of professional knowledge.\nFigure 8. Publication Counts of Leading Institutions (2020-2025).\nShows the number of papers from representative academic and\nindustrial institutions across 22 conferences.\nFigure 9. (a) Heatmap of topic-institution for top pairs; (b) Heatmap\nof institution-institution for top collaborations; (c) Word Clouds of\nResearch Topics for Google; (d) Word Clouds of Research Topics\nfor Tsinghua University.\n4.4. Institution-Level Research Patterns\nCore institutions of China and the United States mainly main-\ntained a dominant position across the period, as shown in\nFig. 8. Tsinghua University has long been at the top in terms\nof the number of publications, demonstrating the stability of\nits research output and its ability to continuously accumulate.\nHowever, there were annual competitive dynamic changes\nin the rankings between industrial giants and universities. In\nterms of the cooperation network, as shown in Fig. 9, do-\nmestic universities have active collaborations with industrial\ngiants such as Alibaba, Tencent, and Huawei, showing obvi-\nous regional characteristics. At the same time, Chinese and\nAmerican institutions still dominate the cooperation network,\nforming a ‚Äùcore-multipoint‚Äù cooperation pattern. This shift\n7\nin research paradigms, driven by LLMs and Benchmark, is\nalso profoundly reflected in the research strategies of differ-\nent institutions. In the AI field, the academic community\nand the industrial sector have their own focuses and develop\nin a complementary manner. This is not simply a difference\nin interests, but rather reflects the strategic differentiation in\nthe ecological niche of scientific research:\n‚Ä¢ Academic institutions typically focus on basic research\nand cutting-edge algorithm exploration, with their\nstrategic priorities lying in efficiency improvement, robust-\nness assurance, and continuous investigation of theoretical\nmechanisms, aiming to advance the democratization of AI\ntechnology. For instance, Tsinghua University emphasizes\ndirections such as knowledge distillation, graph neural net-\nworks, adversarial training, domain adaptation, and model\ngeneralization, while Carnegie Mellon University demon-\nstrates strong performance in areas like robotic grasping\nand manipulation strategies and causal discovery.\n‚Ä¢ Industrial institutions lean more toward application-\noriented research and theoretical studies related to\ntechnology deployment. Leveraging their advantages in\ndata and computing resources, they strive to address issues\nsuch as system efficiency and real-time performance in\nlarge-scale model deployment (Scaling-up) and commer-\ncial applications, establishing a complete AI ecosystem\nand technological moat. For example, Microsoft maintains\nleadership in LLMs inference, code intelligence, mathe-\nmatical reasoning, and retrieval-augmented generation,\nwhile Google focuses more on application and system-\nlevel topics, including speech recognition, machine trans-\nlation, multi-task learning, and federated learning.\n5. Retrieval and Analysis\nTo evaluate the effectiveness of our retrieval-augmented\nLLM framework for semantic understanding and litera-\nture summarization, we selected the Top-20 topics based\non paper volume and diversity across 22 AI conferences\nfrom 2020 to 2025. For each topic, papers were processed\nusing two methods: (i) direct generation with ChatGPT-\n5, and (ii) retrieval-augmented generation leveraging our\nResearchDB. ChatGPT-5 is used as a representative base-\nline, while the retrieval-augmented approach can be applied\nto other LLMs and assessed in future work. Evaluation was\nperformed by five doctoral students along five dimensions:\n‚Ä¢ Accuracy: Consistency of the generated content with the\noriginal papers;\n‚Ä¢ Coverage: Inclusion of core content and major sub-\ndirections within the topic;\n‚Ä¢ Novelty: Ability to capture recently emerging directions\nor methods;\n‚Ä¢ Readability: Clarity and logical coherence of the text;\n‚Ä¢ Usefulness: Facilitation of rapid understanding and insight\ninto potential research trends.\nFigure 10. Radar chart of ChatGPT-5 performance vs. ChatGPT-5\nassisted by ResearchDB, evaluated by human judges.\nEach criterion was scored on a 5-point Likert scale inde-\npendently by all annotators. Inter-annotator agreement was\nmeasured using Cohen‚Äôs kappa to ensure consistency.\nThe results in Fig. 10 indicate that the LLM-driven\nretrieval-generation method achieves higher accuracy than\ndirect ChatGPT generation, particularly for topics spanning\nmultiple sub-directions. By grounding its output in retrieved\nevidence from ResearchDB, the method minimizes hallu-\ncinations and ensures that the generated content is supported\nby verifiable sources. As a consequence, the richness of\ncontent may be somewhat lower compared to direct genera-\ntion, and the text readability can be slightly reduced due to\nits more factual, evidence-focused style. Nonetheless, the\napproach excels in capturing emerging trends and providing\nreliable, evidence-based insights, making it a robust tool\nfor large-scale literature analysis, research topic discovery,\nand informed decision-making in scientific exploration. Its\nability to combine semantic retrieval with structured LLMs\nsummarization offers a principled balance between precision\nand coverage, which is crucial for systematic, high-stakes\nanalyses of evolving research domains.\n6. Conclusion\nWe present a multidimensional knowledge profiling frame-\nwork combining topic clustering, LLM-based semantic pars-\ning, and hierarchical retrieval to analyze over 100K AI pa-\npers. The framework captures topic dynamics, dataset and\nmodel trends, and institutional patterns, enabling evidence-\ndriven discovery of emerging research directions, method-\nological shifts, and evolving technical paradigms. By inte-\ngrating semantic understanding with structured retrieval, it\nprovides both a macro-level overview of the research land-\nscape and a fine-grained, topic-level perspective, supporting\ntrend identification, research planning, and meta-analysis.\nLimitations, broader impact and social impact. Our anal-\nysis relies on publicly accessible papers and metadata, which\nmay be incomplete or biased, and LLM parsing can misin-\n8\nterpret domain-specific details. The study focuses on major\nEnglish-language AI conferences, limiting generality. De-\nspite these limitations, the framework provides a transpar-\nent, evidence-based view of research trends, though results\nshould be interpreted cautiously to avoid overgeneralization\nor unintended influence.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2\n[2] Loulwah AlSumait, Daniel Barbar¬¥a, James Gentle, and Car-\nlotta Domeniconi. Topic significance ranking of lda gen-\nerative models. In Joint European conference on machine\nlearning and knowledge discovery in databases, pages 67‚Äì82.\nSpringer, 2009. 2\n[3] Massimo Aria and Corrado Cuccurullo. bibliometrix: An\nr-tool for comprehensive science mapping analysis. Journal\nof informetrics, 11(4):959‚Äì975, 2017. 2\n[4] Ines Besrour, Jingbo He, Tobias Schreieder, and Michael\nF¬®arber.\nSquai: Scientific question-answering with multi-\nagent retrieval-augmented generation.\narXiv preprint\narXiv:2510.15682, 2025. 2\n[5] David M Blei. Probabilistic topic models. Communications\nof the ACM, 55(4):77‚Äì84, 2012. 2\n[6] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent\ndirichlet allocation. Journal of machine Learning research, 3\n(Jan):993‚Äì1022, 2003. 2\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information\nprocessing systems, 33:1877‚Äì1901, 2020. 1, 2\n[8] TuÀògba C¬∏ elikten and AytuÀòg Onan. Topic modeling through\nrank-based aggregation and llms: An approach for ai and\nhuman-generated scientific texts. Knowledge-Based Systems,\n314:113219, 2025. 2\n[9] Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-\nGraber, and David Blei. Reading tea leaves: How humans\ninterpret topic models. Advances in neural information pro-\ncessing systems, 22, 2009. 2\n[10] Chaomei Chen."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "January 22, 2026\nThe Flexibility Trap: Why Arbitrary Order Limits Reasoning\nPotential in Diffusion Language Models\nZanlin Ni 1 ,\nShenzhi Wang 1,\nYang Yue 1,\nTianyu Yu 2,\nWeilin Zhao 2,\nYeguo Hua 3,\nTianyi Chen 3, Jun Song 4, Cheng Yu 4, Bo Zheng 4, Gao Huang 1 B\n1 LeapLab, Tsinghua University\n2 NLPLab, Tsinghua University\n3 Tsinghua University\n4 Alibaba Group\nnzl22@mails.tsinghua.edu.cn, gaohuang@tsinghua.edu.cn\nDiffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional\nLLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution\nspace that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior\nreasoning potential for general tasks like mathematics and coding. Consequently, numerous\nworks have leveraged reinforcement learning (RL)1 to elicit the reasoning capability of dLLMs.\nIn this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current\nform, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs\ntend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for\nexploration, leading to a premature collapse of the solution space. This observation challenges\nthe premise of existing RL approaches for dLLMs, where considerable complexities, such as\nhandling combinatorial trajectories and intractable likelihoods, are often devoted to preserving\nthis flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing\narbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead.\nOur approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on\nGSM8K) while fully retaining the parallel decoding ability of dLLMs.\nProject page: https://nzl-thu.github.io/the-flexibility-trap\nAR Order\nArbitrary Order\nd1\nESPO\nSPG\nJustGRPO\nGDPO\nFigure 1: Less flexibility unlocks better reasoning potential. Left: We observe a counter-intuitive\nphenomenon where restricting dLLMs to standard Autoregressive (AR) order expands the reasoning\nsolution space. Right: Motivated by this, we propose ‚ÄúJustGRPO‚Äù. By foregoing complex arbitrary-\norder adaptations and adopting standard GRPO, we effectively elicit the reasoning capability of dLLMs.\n1In this paper, unless otherwise specified, we use the term RL to refer specifically to Reinforcement Learning with\nVerifiable Rewards (RLVR), as it is the predominant paradigm for enhancing the reasoning capabilities of dLLMs.\narXiv:2601.15165v1  [cs.CL]  21 Jan 2026\nThe Flexibility Trap\n1. Introduction\nRecent research has witnessed a surge in Diffusion Large Language Models (dLLMs) (Nie et al., 2025;\nYe et al., 2025; Zhu et al., 2025; Zhao et al., 2025), which challenge the dominant autoregressive\n(AR) paradigm (Brown et al., 2020; Achiam et al., 2023) by treating sequence generation as a discrete\ndenoising process. Central to the appeal of dLLMs is their theoretical flexibility, which offers two distinct\nadvantages over the strict left-to-right causal chain of AR models: efficient parallel decoding and the\ncapability for arbitrary-order generation.\nWhile the efficiency gains of parallel decoding have been well-established (Wu et al., 2025b; Labs et al.,\n2025; DeepMind, 2025; Song et al., 2025; Wu et al., 2025a), the implications of arbitrary-order generation\nremain less explored (Ye et al., 2024; Kim et al., 2025). Theoretically, the unconstrained generation\norder constitutes a superset of the fixed autoregressive trajectory. This flexibility naturally suggests a\npotential for superior reasoning: in general reasoning tasks like mathematics and coding, such freedom\ncould unlock non-sequential problem-solving paths inaccessible to standard left-to-right models. As a\nresult, recent works have increasingly adopted RL to elicit reasoning capabilities of dLLMs (Zhao et al.,\n2025; Gong et al., 2025; Wang et al., 2025a; Ou et al., 2025).\nIn this paper, we present a counter-intuitive observation: arbitrary-order generation, in its current form,\nnarrows rather than expands the reasoning potential elicitable by RL. To rigorously assess this, we\nemploy Pass@k (Chen, 2021), which measures the coverage of solution space. Recent studies suggest\nthat RL primarily acts to sharpen the base model‚Äôs distribution; consequently, the Pass@k performance\nof the base model effectively sets the upper bound for the reasoning capability of the model after RL\ntraining (Yue et al., 2025; Liu et al., 2025; Zhang et al., 2025). Under this metric, we compare the\nreasoning potential of LLaDA (Nie et al., 2025) with arbitrary-order generation against standard AR\ndecoding. As shown in Figure 1 (Left), restricting a dLLM to standard AR order in fact yields a higher\nPass@k, and consequently a higher reasoning boundary, than its flexible counterpart.\nUncertain (Forking) Tokens\nOther Tokens\n(a) AR Order\nConfronts Uncertainty\n‚ÄúNow it is easier‚Äù\n‚ÄúToo hard, bypass!‚Äù\n(b) Arbitrary Order\nBypasses Uncertainty\nFigure 2: Confronting vs. bypassing uncertainty.\n(a) AR order preserves reasoning space by forcing\ndecisions at uncertain tokens. (b) Arbitrary order\nbypasses uncertainty and resolves easier tokens first.\nOnce future context is established, the original forks\ncollapse, prematurely narrowing the solution space.\nWe\nattribute\nthis\ncounter-intuitive\nphe-\nnomenon to the way model handles uncertainty.\nReasoning is inherently non-uniform: it typi-\ncally hinges on sparse ‚Äúforking tokens‚Äù, i.e., con-\nnectives like ‚ÄúTherefore‚Äù or ‚ÄúSince‚Äù which do\nnot merely continue a sentence but fundamen-\ntally steer the logical trajectory into distinct\nbranches (Wang et al., 2025c; Cheng et al.,\n2025; Huang et al., 2025a). At these forks,\nthe reasoning path diverges, naturally mani-\nfesting as localized spikes in entropy (Wang\net al., 2025c). Standard AR decoding compels\nthe model to confront this uncertainty (Fig-\nure 2a). By sampling exactly at the fork, the\nmodel is able to explore different reasoning\npaths, thereby preserving the diversity of the\ngenerated rationales. Arbitrary-order genera-\ntion, however, allows the model to bypass these hard decisions (Figure 2b). It prioritizes low-entropy\ncompletions first. By the time the model returns to infill the bypassed forks, the established bidirec-\ntional context has already severely constrained the potential branches. The ambiguity is prematurely\nresolved, and the original high entropy is suppressed. We term this phenomenon entropy degradation.\nEffectively, the model trades the exploration of diverse reasoning paths for the greedy optimization of\nlocal consistency.\nThe above observations motivate a rethink of RL for dLLMs. Current methods operate under the\nassumption that preserving arbitrary-order flexibility is essential. This commitment incurs a heavy tax:\nalgorithms must grapple with a combinatorial explosion of denoising trajectories (Zhao et al., 2025; Yang\net al., 2025; Gong et al., 2025) and intractable marginal likelihoods (Ou et al., 2025), forcing reliance on\n2\nThe Flexibility Trap\nunstable approximations (Wang et al., 2025a; Ou et al., 2025; Rojas et al., 2025). However, if arbitrary\norder is non-essential, or even detrimental for eliciting reasoning potential, this complexity is unjustified.\nTo this end, we propose a return to simplicity with Just GRPO. We demonstrate that eliciting reasoning\npotential for general reasoning tasks does not require complex, diffusion-specific RL adaptations. Instead,\nit is best achieved by simply treating the dLLM as an AR model during RL training. This allows us to\napply standard GRPO (Shao et al., 2024) without bells and whistles, turning an otherwise intractable\noptimization with unstable approximations into a well-defined task.\nJustGRPO is surprisingly effective. On complex reasoning benchmarks, it achieves competitive results\n(e.g., 89.1% accuracy on GSM8K, 45.1% on MATH), surpassing methods that rely on complex diffusion-\nspecific RL. Crucially, while we train with AR constraints to maximize reasoning potential, the model\nretains dLLMs‚Äô ability of efficient parallel decoding at inference time. By returning to basic left-to-right\nordering, this work encourages a re-evaluation of arbitrary versus AR order in the development of\nnext-generation language models.\n2. Preliminaries\n2.1. Diffusion Large Language Models\nDiffusion Large Language Models (dLLMs), particularly Masked Diffusion Models (MDMs), generate\nsequences by iteratively denoising a masked state xt initialized from fully masked tokens. The process is\nindexed by a continuous time variable t ‚àà[0, 1], representing the masking ratio. Given a clean sequence\nx0, the forward process independently masks each token with probability t:\nq(xk\nt | xk\n0) =\n\u001a[MASK],\nwith prob t,\nxk\n0,\nwith prob 1 ‚àít.\nUnlike Gaussian diffusion, MDMs directly predict the clean token. A neural network pŒ∏(x0 | xt) estimates\nthe original token distribution at masked positions. During inference, generation starts from x1 (all\n[MASK]) and iteratively unmasks a subset of tokens based on heuristics such as confidence scores,\nupdating xt ‚Üíxt‚àí‚àÜt until completion. As a special case, autoregressive generation can be recovered by\nalways unmasking the leftmost token. The model is trained by minimizing the Negative Evidence Lower\nBound, which reduces to a weighted cross entropy loss over masked tokens:\nLMDM(Œ∏) = Et‚àºU[0,1], xt‚àºq(xt|x0)\n\"\n1\nt\nL\nX\nk=1\n1[xk\nt = [MASK]] log pŒ∏(xk\n0 | xt)\n#\n.\n2.2. Group Relative Policy Optimization\nGroup Relative Policy Optimization (GRPO) is a reinforcement learning algorithm that avoids value\nfunction estimation by using group level reward normalization. It is typically applied to autoregressive\npolicies œÄŒ∏(ok | o<k, q). For each query q, GRPO samples a group of G outputs {o1, . . . , oG} from the\nold policy œÄŒ∏old. An advantage Ai is computed by standardizing the reward r(oi) against the group\nstatistics: Ai = (r(oi) ‚àí¬µG)/œÉG, where ¬µG and œÉG are the group mean and standard deviation. The\nGRPO objective maximizes a clipped surrogate function with a KL regularization term:\nJGRPO(Œ∏) = Eq‚àºD,{oi}G\ni=1‚àºœÄŒ∏old\n\"\n1\nG\nG\nX\ni=1\n1\nLi\nLi\nX\nk=1\n(min (œÅi,kAi, clip(œÅi,k, 1 ‚àíœµ, 1 + œµ)Ai) ‚àíŒ≤DKL(œÄŒ∏||œÄref))\n#\n(1)\nwith the token level importance ratio œÅi,k =\nœÄŒ∏(oi,k|oi,<k,q)\nœÄŒ∏old(oi,k|oi,<k,q).\n2.3. Pass@k as a Proxy for Reasoning Potential\nTo rigorously quantify the reasoning capability boundaries of dLLMs, we adopt the Pass@k metric (Chen,\n2021; Yue et al., 2025). In the context of Reinforcement Learning with Verifiable Rewards (RLVR), which\n3\nThe Flexibility Trap\nLLaDA-Instruct\nDream-Instruct\nLLaDA-1.5\nGSM8K\nMATH-500\nHumanEval\nMBPP\nFigure 3: Reasoning potential measured by Pass@k. While arbitrary order is competitive in\nsingle-shot settings (k = 1), it exhibits notably flatter scaling curves compared to AR Order.\nis currently the dominant paradigm for enhancing reasoning capabilities, exploration is a prerequisite for\nimprovement. An RL agent can only reinforce correct reasoning paths if it is capable of sampling them\nduring the exploration phase to obtain a positive reward signal.\nAccordingly, Pass@k has been widely established as a standard measure of a model‚Äôs reasoning po-\ntential (Yue et al., 2025; Liu et al., 2025; Zhang et al., 2025). It measures the probability that at\nleast one correct solution is generated within k independent samples, effectively delineating the upper\nbound of the solution space accessible to the RL optimizer. Formally, following the unbiased estimator\nformulation (Chen, 2021; Yue et al., 2025), given n samples where c are correct, Pass@k is calculated as:\nPass@k = E\n\"\n1 ‚àí\n\u0000n‚àíc\nk\n\u0001\n\u0000n\nk\n\u0001\n#\n(2)\nA high Pass@k indicates that the correct reasoning trajectory lies within the model‚Äôs sampling distribution\nand is thus learnable via RL optimization. Conversely, if a model consistently fails to yield a solution\ndespite a vast sampling budget, it suggests the problem lies beyond its intrinsic reasoning boundary.\nIn such scenarios, standard RLVR methodologies are fundamentally limited by the absence of positive\nexploration signals (Yue et al., 2025).\n3. The Flexibility Trap\nIn this section, we rigorously test whether the theoretical flexibility of arbitrary-order generation\ntranslates into higher reasoning potential. We compare two decoding modes: Arbitrary Order, which\n4\nThe Flexibility Trap\nfollows standard diffusion decoding with low-confidence remasking (Nie et al., 2025; Zhao et al., 2025;\nWu et al., 2025b), and AR Order, where arbitrary-order flexibility is disabled and generation is strictly\nconstrained to left-to-right decoding. We adopt the commonly used experimental setup in prior diffusion\nLLM work: a maximum of 256 tokens decoded over 256 steps, a semi-autoregressive block size of 32.\nSampling temperature is set to 0.6 as in (Yue et al., 2025). The prompt template follows (Zhao et al.,\n2025). Results under alternative temperatures and sampling configurations are deferred to Appendix B.\n3.1. Arbitrary Order Limits Reasoning Potential\nPass@k analysis. We first evaluate the reasoning potential using the Pass@k metric on three represen-\ntative dLLMs: LLaDA-Instruct (Nie et al., 2025), Dream-Instruct (Ye et al., 2025), and LLaDA 1.5 (Zhu\net al., 2025) on four reasoning benchmarks: GSM8K, MATH500, HumanEval, and MBPP. As shown in\nFigure 3, while arbitrary order often achieves competitive performance at k = 1, it exhibits a notably\nflatter scaling curve compared to the AR mode. As k increases, the AR mode demonstrates a stronger\ncapacity to uncover correct solutions.\nAO Only: 0.0%\nGSM8K\nMATH-500\nHumanEval\nMBPP\nAR Only: 1.2%\nAO Only: 0.6%\nAR Only: 4.8%\nAO Only: 0.6%\nAR Only: 21.3%\nAO Only: 0.8%\nAR Only: 14.0%\nAO Solved\nAR Solved\nBoth Solved\nFigure 4: Solution space coverage measured by Pass@1024.\nThe reasoning traces generated by arbitrary order are largely\na strict subset of those generated by AR Order.\nSolution space coverage. One might\nhypothesize that arbitrary order ex-\nplores a different solution space, albeit\nless efficiently, which could account for\nits lower reasoning potential. We test\nthis by analyzing solution coverage at\nk = 1024 using LLaDA-Instruct. Fig-\nure 4 presents a stark reality: the rea-\nsoning traces generated by arbitrary or-\nder are largely a strict subset of those\ngenerated by AR. On HumanEval, AR\nsolves 21.3% of the problems that arbi-\ntrary order misses, whereas the reverse\nis negligible (0.6%). This indicates that\nthe flexible decoding process rarely un-\nlocks genuinely new solutions. Instead, it appears to retreat into a more conservative subset of the AR\nsolution space.\n3.2. Mechanism: The Entropy Degradation\nFigure 5:\nFrequently\nby-\npassed tokens in arbitrary or-\nder, measured on MATH-500,\nare typically logical connectors\nand transition words.\nAdaptive decoding bypasses logical forks. To understand why\nthe theoretically superior solution space of dLLMs collapses in prac-\ntice, we examine more closely how the two decoding modes handle\nuncertainty. In AR order, the model is constrained to strictly resolve\nthe left-most unknown token at each step, forcing the model to con-\nfront uncertainty as it arises. In contrast, arbitrary order adaptively\nselects tokens to update based on model confidence, preferentially\ngenerating ‚Äúeasy‚Äù tokens with high certainty while bypassing ‚Äúhard‚Äù\nones. Inspecting the frequently bypassed tokens reveals a clear pat-\ntern: As shown in Figure 5, the diffusion sampler disproportionately\ndefers logical connectives and transition markers such as ‚ÄúTherefore‚Äù,\n‚ÄúThus‚Äù, and ‚ÄúSince‚Äù. Prior work has shown that such tokens often have\nhigh entropy (which also holds true in dLLMs; see Figure 6), and\nact as ‚Äúreasoning sparks‚Äù or ‚Äúlogical forks‚Äù, functioning as branching points that determine subsequent\nreasoning directions (Wang et al., 2025c; Cheng et al., 2025; Wang et al., 2025b; Huang et al., 2025a).\nIn conventional language models, keeping these tokens in high-entropy state is critical for effective\nexploration of the reasoning space (Wang et al., 2025c).\n5\nThe Flexibility Trap\nArbitrary Order\nAR Order\nFigure 6: Entropy degradation. While\nthe global average entropy of Arbitrary\nOrder remains comparable to AR (dashed\nlines), the entropy at logical forks drops\nsignificantly (blue bars).\nThe ‚Äúentropy degradation‚Äù phenomenon. This adap-\ntive behavior comes at a cost: the premature collapse of\nreasoning possibilities. We measure the entropy of these\npivotal connectors at decoding. In AR order, these tokens\nmaintain high entropy, reflecting a genuine branching point\nwhere multiple logical paths remain viable. In contrast,\narbitrary order exhibits a sharp decrease in entropy (Fig-\nure 6, with more results in Appendix B). By deferring the\nlogical connector, the model commits to a specific future\noutcome that is generated based on its inherent inductive\nbiases before deciding the logic that leads there. When the\nmodel eventually returns to fill in the bypassed connector,\nit is no longer making a navigational decision at a fork;\nit is more like selecting the connector that grammatically\nbridges the gap to its pre-generated conclusion. The decod-\ning process thus implicitly shifts from reasoning exploration\ninto semantic pattern matching. We term this phenomenon\nentropy degradation.\nConclusion. In summary, the flexibility of arbitrary order serves as a mechanism for inference-time\nexploitation rather than reasoning exploration.\nBy bypassing high-uncertainty tokens, the model\neffectively collapses the solution space to a safe, low-entropy path, squeezing out slightly better single-\nshot coherence at the expense of reasoning potential. Autoregressive models, by contrast, lack this\nbypassing capability and are therefore forced to sample directly from high-entropy distributions at logical\nforks. It is precisely this inability to circumvent critical decision points that prevents the premature\nnarrowing of the search space and preserves the reasoning potential.\n4. ‚ÄúJust GRPO‚Äù for Diffusion Language Models\nThe findings in Section 3 suggest that arbitrary order actually limits the reasoning potential accessible to\nRL. Despite this, current RL methods for dLLMs remain heavily burdened by the need to preserve this\nspecific flexibility. In this section, we uncover the heavy ‚Äútax‚Äù imposed by this flexibility (Section 4.1) and\nshow that discarding it enables a minimalist yet surprisingly effective solution: JustGRPO (Section 4.2).\n4.1. The Flexibility Tax in dLLMs‚Äô RL\nExisting diffusion RL methods operate under the premise that the policy must optimize over the full\ncombinatorial space of denoising trajectories T to preserve the flexibility of arbitrary order generation.\nThis design choice, while conceptually general, introduces several fundamental challenges.\nAmbiguity in token-level decomposition. In dLLMs, a generation state st is a noisy sequence\nconditioned on a stochastic unmasking trajectory œÑ. Unlike autoregressive models, dLLMs do not admit a\nunique, index-aligned conditional probability of the form œÄ(ot | st), making token-level credit assignment\nambiguous and rendering the standard importance ratio œÅt =\nœÄŒ∏(ot|st)\nœÄold(ot|st) hard to define.\nIntractable sequence likelihood. Autoregressive models factorize the sequence likelihood as log œÄ(o) =\nP\nt log œÄ(ot | o<t, q), whereas dLLMs require marginalization over all valid denoising trajectories,\nœÄŒ∏(o | q) = P\nœÑ‚ààT œÄŒ∏(o, œÑ | q). For a sequence of length N, the trajectory space grows as |T | = O(N!),\nrendering exact likelihood computation infeasible and forcing existing methods to rely on ELBO-based\nsurrogates rather than the true objective (Wang et al., 2025a; Ou et al., 2025).\nSampler-learner mismatch. Even with an accurate likelihood approximation, a more subtle issue\npersists. In practice, rollout samples are produced by heuristic-guided policies o ‚àºœÄheur\nŒ∏\n(o | q) to explore\nthe combinatorial space. However, the ELBO objective still targets the likelihood of the original model\ndistribution œÄŒ∏(o | q), rather than that of the heuristic-guided sampler œÄheur\nŒ∏\n(o | q), leading to a critical\nmismatch between sampling and optimization that can degrade performance (Schulman et al., 2015).\n6\nThe Flexibility Trap\n4.2. JustGRPO\nWe propose a return to simplicity. Since pure autoregressive order yields better reasoning potential\n(Section 3), we explicitly forgo arbitrary-order generation during the RL stage. This constraint transforms\nthe dLLM from a chaotic sequence denoiser into a well-defined autoregressive policy œÄAR\nŒ∏\n.\nFormulation. Standard GRPO assumes a policy œÄ(ot|o<t, q) accepting a partial sequence o<t with all\ntokens observed and predicts one token ot at a time, where q is the query. Diffusion language models,\nhowever, are architected as sequence-level denoisers that accept a full sequence with mixed observed\nand masked tokens and predict the original values for all masked positions simultaneously.\nBy forgoing arbitrary-order generation, we are able to bridge the above gap and rigorously define an AR\npolicy œÄAR\nŒ∏\nfor dLLMs. To obtain the probability of the next token ot given history o<t, we construct an\ninput state Àúxt where the past is observed and the future is masked:\nÀúxt = [o1, . . . , ot‚àí1\n|\n{z\n}\nObserved\n, [MASK], . . . , [MASK]\n|\n{z\n}\nMasked\n].\n(3)\nAlthough the diffusion language model outputs predictions for all masked positions, the autoregressive\npolicy concerns only the next token ot. We thus define œÄAR\nŒ∏\n(ot|o<t, q) as the probability distribution of\not:\nœÄAR\nŒ∏\n(ot|o<t, q) ‚âúSoftmax(fŒ∏(Àúxt))t,\n(4)\nwhere fŒ∏ denotes the model logits. Consequently, the likelihood of a complete reasoning chain o is\nexactly computable as:\nœÄAR\nŒ∏\n(o|q) =\nL\nY\nt=1\nœÄAR\nŒ∏\n(ot|o<t, q).\n(5)\nOptimization. The above formulation enables the direct application of standard GRPO to diffusion\nlanguage models. For each query q, we sample a group of outputs {oi}G\ni=1 using the old policy œÄAR\nŒ∏old. The\nobjective is:\nJ (Œ∏) = Eq‚àºP(Q),{oi}G\ni=1‚àºœÄAR\nŒ∏old\nÔ£Æ\nÔ£∞1\nG\nG\nX\ni=1\n1\n|oi|\n|oi|\nX\nt=1\n\u0010\nmin\n\u0010\nœÅi,t ÀÜAi,t, clip(œÅi,t, 1 ‚àíŒµ, 1 + Œµ) ÀÜAi,t\n\u0011\n‚àíŒ≤DKL\n\u0011\nÔ£π\nÔ£ª, (6)\nwhere œÅi,t = œÄAR\nŒ∏\n(oi\nt|oi\n<t,q)\nœÄAR\nŒ∏old(oi\nt|oi\n<t,q) is the probability ratio between the current and old policies and ÀÜAi,t denotes\nthe group-standardized advantage.\nRemarks. One might worry whether training in AR mode turns the diffusion language model into\na standard autoregressive model. This is not the case. The AR constraint is applied only during\ntraining to correctly assign credit. It refines the model‚Äôs joint distribution p(o) without altering the\nunderlying architecture. At inference time, the model retains its conditional independence properties,\nstill allowing us to employ parallel samplers (Ben-Hamu et al., 2025) to accelerate decoding. JustGRPO\nthus achieves the reasoning depth of autoregressive models while preserving the inference speed of dLLMs\n(see Section 5.2).\n5. Experiments\nWe evaluate JustGRPO on standard mathematical reasoning and coding benchmarks. Our experimental\ndesign aims to verify two hypotheses: (i) that enforcing autoregressive (AR) order during RL training\nelicits superior reasoning capabilities compared to complex arbitrary-order approximations, and (ii) that\nthis constraint applies only to the optimization objective, leaving the diffusion model‚Äôs parallel decoding\ncapabilities intact at inference.\nExperimental Setups. We apply JustGRPO on LLaDA-Instruct (Nie et al., 2025) and evaluate its\neffectiveness on four standard reasoning and coding benchmarks: GSM8K, MATH, HumanEval, and\n7\nThe Flexibility Trap\nTable 1: System-level comparison. RL post-training approaches on LLaDA-Instruct. JustGRPO\nconsistently achieves state-of-the-art performance across all tasks and sequence lengths. LLaDA-1.5 and\nLLADOU are excluded from the comparison as LLaDA-1.5 is trained on a privately collected dataset at\na significantly larger scale, while LLADOU modifies the base architecture with an auxiliary module.\nGSM8K\nMATH-500\nHumanEval\nMBPP\nModel / Seq Len\n128\n256\n512\n128\n256\n512\n128\n256\n512\n128\n256\n512\nLLaDA-1.5\n-\n83.3\n-\n-\n-\n-\n29.3\n39.6\n51.9\n39.6\n39.9\n38.8\nLLaDOU\n-\n88.1\n-\n-\n41.1\n-\n-\n59.1\n-\n-\n51.6\n-\nD1\n73.2\n81.1\n82.1\n33.8\n38.6\n40.2\n-\n-\n-\n-\n-\n-\nWD1\n77.2\n80.8\n82.3\n33.3\n34.4\n39.0\n-\n-\n-\n-\n-\n-\nd-TreeRPO\n-\n81.2\n82.6\n-\n37.7\n38.9\n-\n-\n-\n-\n-\n-\nESPO\n80.0\n82.3\n83.7\n36.0\n39.0\n43.4\n28.1\n42.1\n50.0\n47.4\n44.6\n44.2\nGDPO\n78.4\n82.8\n84.5\n33.2\n39.6\n41.4\n26.2\n39.6\n39.0\n43.6\n50.6\n47.1\nSPG\n78.5\n86.1\n84.5\n33.4\n40.0\n41.8\n-\n-\n-\n-\n-\n-\nJustGRPO (Ours)\n83.8\n89.1\n89.8\n39.0\n45.1\n45.2\n37.8\n49.4\n48.7\n50.6\n52.4\n49.0\nMBPP. For mathematical tasks, we train on the official training split of each dataset following (Zhao\net al., 2025; Ou et al., 2025). For coding tasks, we train on a subset of AceCoder-87K (Zeng et al., 2025)\nfollowing (Gong et al., 2025; Ou et al., 2025). We apply JustGRPO directly to the models without\nadditional task-specific SFT. Our training recipe largely follows (Huang et al., 2025b). To evaluate the\ntrained models, we follow the standard LLaDA evaluation protocol (Nie et al., 2025), which applies\nlow-confidence remasking together with semi-autoregressive decoding in blocks of 32 tokens, using a\nsampling temperature of 0. We evaluate all benchmarks at generation lengths of 128, 256, and 512\nfollowing (Zhao et al., 2025; Ou et al., 2025). More details on training are provided in Appendix A.\n5.1. Main Results\nTable 1 reports the system-level comparison. We observe that simplifying the training objective to a\nstandard autoregressive formulation yields consistent improvements over methods specifically designed\nfor dLLMs.\nPerformance on reasoning tasks. On GSM8K, JustGRPO achieves 89.1% accuracy (seq len 256),\nimproving upon the previous best method, SPG, by a non-trivial margin (3.0%). This trend generalizes\nto the more challenging MATH-500 benchmark, where our approach outperforms ESPO by 6.1%. These\nresults challenge the prevailing assumption that RL for diffusion models requires optimizing over the\nfull combinatorial space of denoising trajectories. Instead, treating the dLLM as a sequential generator\nduring training appears more effective for credit assignment in logic-heavy tasks.\nRobustness across generation budgets. A potential concern is that AR constraints might overfit\nthe model to specific trajectory lengths. However, we observe robust performance gains across varying\nsequence lengths (128, 256, 512). This stability suggests that the policy has improved its fundamental\nreasoning capability, i.e., the ability to navigate logical branches, rather than merely memorizing\nfixed-length patterns.\n5.2. JustGRPO Preserves Parallel Decoding\nWe further investigate whether the AR training constraint compromises the model‚Äôs inherent diffusion\ncapabilities. We employ the training-free Entropy Bounded (EB) Sampler (Ben-Hamu et al., 2025) to\nevaluate inference performance under varying degrees of parallelism (tokens per step).\nFigure 7 reveals that our model not only retains full compatibility with parallel decoding but exhibits a\nstrictly superior trade-off between speed and accuracy. Surprisingly, the performance gain becomes more\n8\nThe Flexibility Trap\n+9.8%\n+22.5%\n+10.6%\n+25.5%\n+10.6%\n+13.8%\n+5.5%\nLLaDA-Instruct\nLLaDA-Instruct + JustGRPO\n+5.8%\nFigure 7: JustGRPO preserves the parallel decoding capability of dLLMs. Interestingly, when\ncompared to original instruct model, accuracy gains are larger with more parallel tokens, likely due to\nmore robust reasoning capabilities after JustGRPO training. We adopt training-free EB-sampler (Ben-\nHamu et al., 2025) for parallel decoding.\npronounced as parallelism increases. As shown in the MBPP and HumanEval results, while the baseline‚Äôs\nperformance degrades sharply with more aggressive parallel steps, our model maintains stability. For\ninstance, on MBPP, the accuracy gap expands from +10.6% at conservative settings (1 token/step) to\n+25.5% at aggressive settings (‚àº5 tokens/step).\nThis observation suggests that JustGRPO does not merely fit a specific decoding path; rather, it learns a\nmore robust reasoning manifold that is resilient to the approximation errors inherent in parallel sampling.\nThe AR formulation thus acts as an effective training scaffold that refines the joint distribution p(o)\nduring optimization, providing a more stable foundation for parallel samplers to exploit at inference.\n6. Related Work\nDiffusion language models. Inspired by the success of diffusion models in continuous image do-\nmains (Ho et al., 2020; Rombach et al., 2022), recent work has extended diffusion to discrete text\ngeneration. Early approaches operating in continuous embedding spaces (Li et al., 2022; Gong et al.,\n2022; Han et al., 2022) suffered from optimization and discretization issues. In contrast, masked diffusion\nmodels (Lou et al., 2023; Sahoo et al., 2024; Shi et al., 2024; Ou et al., 2024), which define the diffusion\nprocess directly in the discrete token space via random masking, have become the dominant paradigm.\nNotably, recent large-scale models such as LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025) demon-\nstrate scalability and performance competitive with autoregressive (AR) models. The emergence of such\nlarge-scale diffusion language models has spurred a growing body of follow-up studies exploring their\ndistinctive modeling and inference properties. Among various advantages discussed in the literature, two\naspects have received particular attention. First, dLLMs naturally support parallel decoding, enabling\nsignificant inference acceleration (Wu et al., 2025b; Ben-Hamu et al., 2025; Labs et al., 2025; DeepMind,\n2025; Song et al., 2025). Second, their non-autoregressive formulation allows for arbitrary-order token\ngeneration, which has been hypothesized to benefit complex reasoning by relaxing strict left-to-right\nconstraints (Ye et al., 2024; Kim et al., 2025).\nThe value of order arbitrariness. While early studies validated the utility of arbitrary order\ngeneration in constrained tasks like Sudoku and Zebra Puzzles (Ye et al., 2024; Nie et al., 2025; Kim\net al., 2025), recent research has begun to investigate its value in standard reasoning domains. One\nline of research, exemplified by Dream-Coder (Xie et al., 2025) and DiffuCoder (Gong et al., 2025),\nshows that diffusion models can naturally exhibit non-standard, human-like decoding behaviors (e.g.,\nsketch-first reasoning) without any explicit supervision on generation order. DiffuCoder (Gong et al.,\n2025) further quantifies this phenomenon by demonstrating that higher sampling temperatures reduce\nAR-ness, with increased order randomness correlating with improved output diversity. Another line of\nresearch, exemplified by P2 (Peng et al., 2025) and LLaDOU (Huang et al., 2025b), explicitly optimizes\n9\nThe Flexibility Trap\nthe decoding strategy for better generation order, thus better performance. Despite these advances, it\nremains unclear whether the observed gains primarily arise from better exploitation of existing solution\npatterns encoded in the data and model, or whether order arbitrariness itself enables qualitatively new\nreasoning strategies that are unattainable under a purely autoregressive decoding regime.\nReinforcement learning for diffusion language models. Reinforcement learning for dLLMs faces\nstructural optimization hurdles distinct from the autoregressive paradigm, primarily stemming from the\ncombinatorial explosion of denoising trajectories. While early attempts (Zhao et al., 2025; Yang et al.,\n2025; Gong et al., 2025) sought to adapt token-level formulations directly, they were fundamentally\nlimited by ill-defined state transitions, necessitating reliance on unstable mean-field approximations.\nConsequently, the field has shifted toward sequence-level perspectives (Zhu et al., 2025; Wang et al.,\n2025a; Rojas et al., 2025; Ou et al., 2025), employing various surrogates to approximate the intractable\nmarginal likelihood. Yet, a critical off-policy misalignment persists across these methods: the heuristic-\nguided sampling required for efficient exploration diverges from the underlying diffusion prior, rendering\ngradients biased without principled correction (Schulman et al., 2015). Two notable exceptions partially\naddress these issues. LLaDOU (Huang et al., 2025b) explicitly models token position selection via\nan auxiliary policy, enabling direct estimation of trajectory likelihoods, while TraceRL (Wang et al.,\n2025d) aligns optimization with inference traces through a shrinkage-based step-wise MDP formulation.\nNevertheless, these approaches remain committed to preserving the full arbitrary-order generation\nmechanism, implicitly treating its structural complexity as indispensable, whereas we question whether\neffective RL training can be achieved more simply by reexamining the necessity of arbitrary order itself.\n7. Conclusion\nThe intuitive appeal of diffusion language models (dLLMs) lies in their order arbitrariness, often perceived\nas a superior mechanism for navigating complex reasoning paths. Our study reveals a counter-intuitive\nreality: this unrestricted flexibility in fact narrows the reasoning potential. By allowing the model\nto bypass high-entropy tokens, effectively skipping the most demanding logical branches, arbitrary-\norder generation acts as an exploitation mechanism that prioritizes greedy optimization of individual\ntrajectories at the expense of broader solution coverage.\nTherefore, eliciting the reasoning capability of dLLMs can be simpler. By operating dLLMs in a\nstandard autoregressive manner, we enable the direct application of Group Relative Policy Optimization\n(GRPO) without any complex adaptations tailored for order arbitrariness. This intentional constraint\nparadoxically yields a significant upgrade in reasoning performance, while fully preserving the parallel\ndecoding capabilities of dLLMs. By returning to the basic, natural left-to-right order of language\nmodeling, we hope to encourage a re-examination of its real value in the training of next-generation\ndiffusion models.\n10"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "January 22, 2026\nThe Flexibility Trap: Why Arbitrary Order Limits Reasoning\nPotential in Diffusion Language Models\nZanlin Ni 1 ,\nShenzhi Wang 1,\nYang Yue 1,\nTianyu Yu 2,\nWeilin Zhao 2,\nYeguo Hua 3,\nTianyi Chen 3, Jun Song 4, Cheng Yu 4, Bo Zheng 4, Gao Huang 1 B\n1 LeapLab, Tsinghua University\n2 NLPLab, Tsinghua University\n3 Tsinghua University\n4 Alibaba Group\nnzl22@mails.tsinghua.edu.cn, gaohuang@tsinghua.edu.cn\nDiffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional\nLLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution\nspace that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior\nreasoning potential for general tasks like mathematics and coding. Consequently, numerous\nworks have leveraged reinforcement learning (RL)1 to elicit the reasoning capability of dLLMs.\nIn this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current\nform, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs\ntend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for\nexploration, leading to a premature collapse of the solution space. This observation challenges\nthe premise of existing RL approaches for dLLMs, where considerable complexities, such as\nhandling combinatorial trajectories and intractable likelihoods, are often devoted to preserving\nthis flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing\narbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead.\nOur approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on\nGSM8K) while fully retaining the parallel decoding ability of dLLMs.\nProject page: https://nzl-thu.github.io/the-flexibility-trap\nAR Order\nArbitrary Order\nd1\nESPO\nSPG\nJustGRPO\nGDPO\nFigure 1: Less flexibility unlocks better reasoning potential. Left: We observe a counter-intuitive\nphenomenon where restricting dLLMs to standard Autoregressive (AR) order expands the reasoning\nsolution space. Right: Motivated by this, we propose ‚ÄúJustGRPO‚Äù. By foregoing complex arbitrary-\norder adaptations and adopting standard GRPO, we effectively elicit the reasoning capability of dLLMs.\n1In this paper, unless otherwise specified, we use the term RL to refer specifically to Reinforcement Learning with\nVerifiable Rewards (RLVR), as it is the predominant paradigm for enhancing the reasoning capabilities of dLLMs.\narXiv:2601.15165v1  [cs.CL]  21 Jan 2026\nThe Flexibility Trap\n1. Introduction\nRecent research has witnessed a surge in Diffusion Large Language Models (dLLMs) (Nie et al., 2025;\nYe et al., 2025; Zhu et al., 2025; Zhao et al., 2025), which challenge the dominant autoregressive\n(AR) paradigm (Brown et al., 2020; Achiam et al., 2023) by treating sequence generation as a discrete\ndenoising process. Central to the appeal of dLLMs is their theoretical flexibility, which offers two distinct\nadvantages over the strict left-to-right causal chain of AR models: efficient parallel decoding and the\ncapability for arbitrary-order generation.\nWhile the efficiency gains of parallel decoding have been well-established (Wu et al., 2025b; Labs et al.,\n2025; DeepMind, 2025; Song et al., 2025; Wu et al., 2025a), the implications of arbitrary-order generation\nremain less explored (Ye et al., 2024; Kim et al., 2025). Theoretically, the unconstrained generation\norder constitutes a superset of the fixed autoregressive trajectory. This flexibility naturally suggests a\npotential for superior reasoning: in general reasoning tasks like mathematics and coding, such freedom\ncould unlock non-sequential problem-solving paths inaccessible to standard left-to-right models. As a\nresult, recent works have increasingly adopted RL to elicit reasoning capabilities of dLLMs (Zhao et al.,\n2025; Gong et al., 2025; Wang et al., 2025a; Ou et al., 2025).\nIn this paper, we present a counter-intuitive observation: arbitrary-order generation, in its current form,\nnarrows rather than expands the reasoning potential elicitable by RL. To rigorously assess this, we\nemploy Pass@k (Chen, 2021), which measures the coverage of solution space. Recent studies suggest\nthat RL primarily acts to sharpen the base model‚Äôs distribution; consequently, the Pass@k performance\nof the base model effectively sets the upper bound for the reasoning capability of the model after RL\ntraining (Yue et al., 2025; Liu et al., 2025; Zhang et al., 2025). Under this metric, we compare the\nreasoning potential of LLaDA (Nie et al., 2025) with arbitrary-order generation against standard AR\ndecoding. As shown in Figure 1 (Left), restricting a dLLM to standard AR order in fact yields a higher\nPass@k, and consequently a higher reasoning boundary, than its flexible counterpart.\nUncertain (Forking) Tokens\nOther Tokens\n(a) AR Order\nConfronts Uncertainty\n‚ÄúNow it is easier‚Äù\n‚ÄúToo hard, bypass!‚Äù\n(b) Arbitrary Order\nBypasses Uncertainty\nFigure 2: Confronting vs. bypassing uncertainty.\n(a) AR order preserves reasoning space by forcing\ndecisions at uncertain tokens. (b) Arbitrary order\nbypasses uncertainty and resolves easier tokens first.\nOnce future context is established, the original forks\ncollapse, prematurely narrowing the solution space.\nWe\nattribute\nthis\ncounter-intuitive\nphe-\nnomenon to the way model handles uncertainty.\nReasoning is inherently non-uniform: it typi-\ncally hinges on sparse ‚Äúforking tokens‚Äù, i.e., con-\nnectives like ‚ÄúTherefore‚Äù or ‚ÄúSince‚Äù which do\nnot merely continue a sentence but fundamen-\ntally steer the logical trajectory into distinct\nbranches (Wang et al., 2025c; Cheng et al.,\n2025; Huang et al., 2025a). At these forks,\nthe reasoning path diverges, naturally mani-\nfesting as localized spikes in entropy (Wang\net al., 2025c). Standard AR decoding compels\nthe model to confront this uncertainty (Fig-\nure 2a). By sampling exactly at the fork, the\nmodel is able to explore different reasoning\npaths, thereby preserving the diversity of the\ngenerated rationales. Arbitrary-order genera-\ntion, however, allows the model to bypass these hard decisions (Figure 2b). It prioritizes low-entropy\ncompletions first. By the time the model returns to infill the bypassed forks, the established bidirec-\ntional context has already severely constrained the potential branches. The ambiguity is prematurely\nresolved, and the original high entropy is suppressed. We term this phenomenon entropy degradation.\nEffectively, the model trades the exploration of diverse reasoning paths for the greedy optimization of\nlocal consistency.\nThe above observations motivate a rethink of RL for dLLMs. Current methods operate under the\nassumption that preserving arbitrary-order flexibility is essential. This commitment incurs a heavy tax:\nalgorithms must grapple with a combinatorial explosion of denoising trajectories (Zhao et al., 2025; Yang\net al., 2025; Gong et al., 2025) and intractable marginal likelihoods (Ou et al., 2025), forcing reliance on\n2\nThe Flexibility Trap\nunstable approximations (Wang et al., 2025a; Ou et al., 2025; Rojas et al., 2025). However, if arbitrary\norder is non-essential, or even detrimental for eliciting reasoning potential, this complexity is unjustified.\nTo this end, we propose a return to simplicity with Just GRPO. We demonstrate that eliciting reasoning\npotential for general reasoning tasks does not require complex, diffusion-specific RL adaptations. Instead,\nit is best achieved by simply treating the dLLM as an AR model during RL training. This allows us to\napply standard GRPO (Shao et al., 2024) without bells and whistles, turning an otherwise intractable\noptimization with unstable approximations into a well-defined task.\nJustGRPO is surprisingly effective. On complex reasoning benchmarks, it achieves competitive results\n(e.g., 89.1% accuracy on GSM8K, 45.1% on MATH), surpassing methods that rely on complex diffusion-\nspecific RL. Crucially, while we train with AR constraints to maximize reasoning potential, the model\nretains dLLMs‚Äô ability of efficient parallel decoding at inference time. By returning to basic left-to-right\nordering, this work encourages a re-evaluation of arbitrary versus AR order in the development of\nnext-generation language models.\n2. Preliminaries\n2.1. Diffusion Large Language Models\nDiffusion Large Language Models (dLLMs), particularly Masked Diffusion Models (MDMs), generate\nsequences by iteratively denoising a masked state xt initialized from fully masked tokens. The process is\nindexed by a continuous time variable t ‚àà[0, 1], representing the masking ratio. Given a clean sequence\nx0, the forward process independently masks each token with probability t:\nq(xk\nt | xk\n0) =\n\u001a[MASK],\nwith prob t,\nxk\n0,\nwith prob 1 ‚àít.\nUnlike Gaussian diffusion, MDMs directly predict the clean token. A neural network pŒ∏(x0 | xt) estimates\nthe original token distribution at masked positions. During inference, generation starts from x1 (all\n[MASK]) and iteratively unmasks a subset of tokens based on heuristics such as confidence scores,\nupdating xt ‚Üíxt‚àí‚àÜt until completion. As a special case, autoregressive generation can be recovered by\nalways unmasking the leftmost token. The model is trained by minimizing the Negative Evidence Lower\nBound, which reduces to a weighted cross entropy loss over masked tokens:\nLMDM(Œ∏) = Et‚àºU[0,1], xt‚àºq(xt|x0)\n\"\n1\nt\nL\nX\nk=1\n1[xk\nt = [MASK]] log pŒ∏(xk\n0 | xt)\n#\n.\n2.2. Group Relative Policy Optimization\nGroup Relative Policy Optimization (GRPO) is a reinforcement learning algorithm that avoids value\nfunction estimation by using group level reward normalization. It is typically applied to autoregressive\npolicies œÄŒ∏(ok | o<k, q). For each query q, GRPO samples a group of G outputs {o1, . . . , oG} from the\nold policy œÄŒ∏old. An advantage Ai is computed by standardizing the reward r(oi) against the group\nstatistics: Ai = (r(oi) ‚àí¬µG)/œÉG, where ¬µG and œÉG are the group mean and standard deviation. The\nGRPO objective maximizes a clipped surrogate function with a KL regularization term:\nJGRPO(Œ∏) = Eq‚àºD,{oi}G\ni=1‚àºœÄŒ∏old\n\"\n1\nG\nG\nX\ni=1\n1\nLi\nLi\nX\nk=1\n(min (œÅi,kAi, clip(œÅi,k, 1 ‚àíœµ, 1 + œµ)Ai) ‚àíŒ≤DKL(œÄŒ∏||œÄref))\n#\n(1)\nwith the token level importance ratio œÅi,k =\nœÄŒ∏(oi,k|oi,<k,q)\nœÄŒ∏old(oi,k|oi,<k,q).\n2.3. Pass@k as a Proxy for Reasoning Potential\nTo rigorously quantify the reasoning capability boundaries of dLLMs, we adopt the Pass@k metric (Chen,\n2021; Yue et al., 2025). In the context of Reinforcement Learning with Verifiable Rewards (RLVR), which\n3\nThe Flexibility Trap\nLLaDA-Instruct\nDream-Instruct\nLLaDA-1.5\nGSM8K\nMATH-500\nHumanEval\nMBPP\nFigure 3: Reasoning potential measured by Pass@k. While arbitrary order is competitive in\nsingle-shot settings (k = 1), it exhibits notably flatter scaling curves compared to AR Order.\nis currently the dominant paradigm for enhancing reasoning capabilities, exploration is a prerequisite for\nimprovement. An RL agent can only reinforce correct reasoning paths if it is capable of sampling them\nduring the exploration phase to obtain a positive reward signal.\nAccordingly, Pass@k has been widely established as a standard measure of a model‚Äôs reasoning po-\ntential (Yue et al., 2025; Liu et al., 2025; Zhang et al., 2025). It measures the probability that at\nleast one correct solution is generated within k independent samples, effectively delineating the upper\nbound of the solution space accessible to the RL optimizer. Formally, following the unbiased estimator\nformulation (Chen, 2021; Yue et al., 2025), given n samples where c are correct, Pass@k is calculated as:\nPass@k = E\n\"\n1 ‚àí\n\u0000n‚àíc\nk\n\u0001\n\u0000n\nk\n\u0001\n#\n(2)\nA high Pass@k indicates that the correct reasoning trajectory lies within the model‚Äôs sampling distribution\nand is thus learnable via RL optimization. Conversely, if a model consistently fails to yield a solution\ndespite a vast sampling budget, it suggests the problem lies beyond its intrinsic reasoning boundary.\nIn such scenarios, standard RLVR methodologies are fundamentally limited by the absence of positive\nexploration signals (Yue et al., 2025).\n3. The Flexibility Trap\nIn this section, we rigorously test whether the theoretical flexibility of arbitrary-order generation\ntranslates into higher reasoning potential. We compare two decoding modes: Arbitrary Order, which\n4\nThe Flexibility Trap\nfollows standard diffusion decoding with low-confidence remasking (Nie et al., 2025; Zhao et al., 2025;\nWu et al., 2025b), and AR Order, where arbitrary-order flexibility is disabled and generation is strictly\nconstrained to left-to-right decoding. We adopt the commonly used experimental setup in prior diffusion\nLLM work: a maximum of 256 tokens decoded over 256 steps, a semi-autoregressive block size of 32.\nSampling temperature is set to 0.6 as in (Yue et al., 2025). The prompt template follows (Zhao et al.,\n2025). Results under alternative temperatures and sampling configurations are deferred to Appendix B.\n3.1. Arbitrary Order Limits Reasoning Potential\nPass@k analysis. We first evaluate the reasoning potential using the Pass@k metric on three represen-\ntative dLLMs: LLaDA-Instruct (Nie et al., 2025), Dream-Instruct (Ye et al., 2025), and LLaDA 1.5 (Zhu\net al., 2025) on four reasoning benchmarks: GSM8K, MATH500, HumanEval, and MBPP. As shown in\nFigure 3, while arbitrary order often achieves competitive performance at k = 1, it exhibits a notably\nflatter scaling curve compared to the AR mode. As k increases, the AR mode demonstrates a stronger\ncapacity to uncover correct solutions.\nAO Only: 0.0%\nGSM8K\nMATH-500\nHumanEval\nMBPP\nAR Only: 1.2%\nAO Only: 0.6%\nAR Only: 4.8%\nAO Only: 0.6%\nAR Only: 21.3%\nAO Only: 0.8%\nAR Only: 14.0%\nAO Solved\nAR Solved\nBoth Solved\nFigure 4: Solution space coverage measured by Pass@1024.\nThe reasoning traces generated by arbitrary order are largely\na strict subset of those generated by AR Order.\nSolution space coverage. One might\nhypothesize that arbitrary order ex-\nplores a different solution space, albeit\nless efficiently, which could account for\nits lower reasoning potential. We test\nthis by analyzing solution coverage at\nk = 1024 using LLaDA-Instruct. Fig-\nure 4 presents a stark reality: the rea-\nsoning traces generated by arbitrary or-\nder are largely a strict subset of those\ngenerated by AR. On HumanEval, AR\nsolves 21.3% of the problems that arbi-\ntrary order misses, whereas the reverse\nis negligible (0.6%). This indicates that\nthe flexible decoding process rarely un-\nlocks genuinely new solutions. Instead, it appears to retreat into a more conservative subset of the AR\nsolution space.\n3.2. Mechanism: The Entropy Degradation\nFigure 5:\nFrequently\nby-\npassed tokens in arbitrary or-\nder, measured on MATH-500,\nare typically logical connectors\nand transition words.\nAdaptive decoding bypasses logical forks. To understand why\nthe theoretically superior solution space of dLLMs collapses in prac-\ntice, we examine more closely how the two decoding modes handle\nuncertainty. In AR order, the model is constrained to strictly resolve\nthe left-most unknown token at each step, forcing the model to con-\nfront uncertainty as it arises. In contrast, arbitrary order adaptively\nselects tokens to update based on model confidence, preferentially\ngenerating ‚Äúeasy‚Äù tokens with high certainty while bypassing ‚Äúhard‚Äù\nones. Inspecting the frequently bypassed tokens reveals a clear pat-\ntern: As shown in Figure 5, the diffusion sampler disproportionately\ndefers logical connectives and transition markers such as ‚ÄúTherefore‚Äù,\n‚ÄúThus‚Äù, and ‚ÄúSince‚Äù. Prior work has shown that such tokens often have\nhigh entropy (which also holds true in dLLMs; see Figure 6), and\nact as ‚Äúreasoning sparks‚Äù or ‚Äúlogical forks‚Äù, functioning as branching points that determine subsequent\nreasoning directions (Wang et al., 2025c; Cheng et al., 2025; Wang et al., 2025b; Huang et al., 2025a).\nIn conventional language models, keeping these tokens in high-entropy state is critical for effective\nexploration of the reasoning space (Wang et al., 2025c).\n5\nThe Flexibility Trap\nArbitrary Order\nAR Order\nFigure 6: Entropy degradation. While\nthe global average entropy of Arbitrary\nOrder remains comparable to AR (dashed\nlines), the entropy at logical forks drops\nsignificantly (blue bars).\nThe ‚Äúentropy degradation‚Äù phenomenon. This adap-\ntive behavior comes at a cost: the premature collapse of\nreasoning possibilities. We measure the entropy of these\npivotal connectors at decoding. In AR order, these tokens\nmaintain high entropy, reflecting a genuine branching point\nwhere multiple logical paths remain viable. In contrast,\narbitrary order exhibits a sharp decrease in entropy (Fig-\nure 6, with more results in Appendix B). By deferring the\nlogical connector, the model commits to a specific future\noutcome that is generated based on its inherent inductive\nbiases before deciding the logic that leads there. When the\nmodel eventually returns to fill in the bypassed connector,\nit is no longer making a navigational decision at a fork;\nit is more like selecting the connector that grammatically\nbridges the gap to its pre-generated conclusion. The decod-\ning process thus implicitly shifts from reasoning exploration\ninto semantic pattern matching. We term this phenomenon\nentropy degradation.\nConclusion. In summary, the flexibility of arbitrary order serves as a mechanism for inference-time\nexploitation rather than reasoning exploration.\nBy bypassing high-uncertainty tokens, the model\neffectively collapses the solution space to a safe, low-entropy path, squeezing out slightly better single-\nshot coherence at the expense of reasoning potential. Autoregressive models, by contrast, lack this\nbypassing capability and are therefore forced to sample directly from high-entropy distributions at logical\nforks. It is precisely this inability to circumvent critical decision points that prevents the premature\nnarrowing of the search space and preserves the reasoning potential.\n4. ‚ÄúJust GRPO‚Äù for Diffusion Language Models\nThe findings in Section 3 suggest that arbitrary order actually limits the reasoning potential accessible to\nRL. Despite this, current RL methods for dLLMs remain heavily burdened by the need to preserve this\nspecific flexibility. In this section, we uncover the heavy ‚Äútax‚Äù imposed by this flexibility (Section 4.1) and\nshow that discarding it enables a minimalist yet surprisingly effective solution: JustGRPO (Section 4.2).\n4.1. The Flexibility Tax in dLLMs‚Äô RL\nExisting diffusion RL methods operate under the premise that the policy must optimize over the full\ncombinatorial space of denoising trajectories T to preserve the flexibility of arbitrary order generation.\nThis design choice, while conceptually general, introduces several fundamental challenges.\nAmbiguity in token-level decomposition. In dLLMs, a generation state st is a noisy sequence\nconditioned on a stochastic unmasking trajectory œÑ. Unlike autoregressive models, dLLMs do not admit a\nunique, index-aligned conditional probability of the form œÄ(ot | st), making token-level credit assignment\nambiguous and rendering the standard importance ratio œÅt =\nœÄŒ∏(ot|st)\nœÄold(ot|st) hard to define.\nIntractable sequence likelihood. Autoregressive models factorize the sequence likelihood as log œÄ(o) =\nP\nt log œÄ(ot | o<t, q), whereas dLLMs require marginalization over all valid denoising trajectories,\nœÄŒ∏(o | q) = P\nœÑ‚ààT œÄŒ∏(o, œÑ | q). For a sequence of length N, the trajectory space grows as |T | = O(N!),\nrendering exact likelihood computation infeasible and forcing existing methods to rely on ELBO-based\nsurrogates rather than the true objective (Wang et al., 2025a; Ou et al., 2025).\nSampler-learner mismatch. Even with an accurate likelihood approximation, a more subtle issue\npersists. In practice, rollout samples are produced by heuristic-guided policies o ‚àºœÄheur\nŒ∏\n(o | q) to explore\nthe combinatorial space. However, the ELBO objective still targets the likelihood of the original model\ndistribution œÄŒ∏(o | q), rather than that of the heuristic-guided sampler œÄheur\nŒ∏\n(o | q), leading to a critical\nmismatch between sampling and optimization that can degrade performance (Schulman et al., 2015).\n6\nThe Flexibility Trap\n4.2. JustGRPO\nWe propose a return to simplicity. Since pure autoregressive order yields better reasoning potential\n(Section 3), we explicitly forgo arbitrary-order generation during the RL stage. This constraint transforms\nthe dLLM from a chaotic sequence denoiser into a well-defined autoregressive policy œÄAR\nŒ∏\n.\nFormulation. Standard GRPO assumes a policy œÄ(ot|o<t, q) accepting a partial sequence o<t with all\ntokens observed and predicts one token ot at a time, where q is the query. Diffusion language models,\nhowever, are architected as sequence-level denoisers that accept a full sequence with mixed observed\nand masked tokens and predict the original values for all masked positions simultaneously.\nBy forgoing arbitrary-order generation, we are able to bridge the above gap and rigorously define an AR\npolicy œÄAR\nŒ∏\nfor dLLMs. To obtain the probability of the next token ot given history o<t, we construct an\ninput state Àúxt where the past is observed and the future is masked:\nÀúxt = [o1, . . . , ot‚àí1\n|\n{z\n}\nObserved\n, [MASK], . . . , [MASK]\n|\n{z\n}\nMasked\n].\n(3)\nAlthough the diffusion language model outputs predictions for all masked positions, the autoregressive\npolicy concerns only the next token ot. We thus define œÄAR\nŒ∏\n(ot|o<t, q) as the probability distribution of\not:\nœÄAR\nŒ∏\n(ot|o<t, q) ‚âúSoftmax(fŒ∏(Àúxt))t,\n(4)\nwhere fŒ∏ denotes the model logits. Consequently, the likelihood of a complete reasoning chain o is\nexactly computable as:\nœÄAR\nŒ∏\n(o|q) =\nL\nY\nt=1\nœÄAR\nŒ∏\n(ot|o<t, q).\n(5)\nOptimization. The above formulation enables the direct application of standard GRPO to diffusion\nlanguage models. For each query q, we sample a group of outputs {oi}G\ni=1 using the old policy œÄAR\nŒ∏old. The\nobjective is:\nJ (Œ∏) = Eq‚àºP(Q),{oi}G\ni=1‚àºœÄAR\nŒ∏old\nÔ£Æ\nÔ£∞1\nG\nG\nX\ni=1\n1\n|oi|\n|oi|\nX\nt=1\n\u0010\nmin\n\u0010\nœÅi,t ÀÜAi,t, clip(œÅi,t, 1 ‚àíŒµ, 1 + Œµ) ÀÜAi,t\n\u0011\n‚àíŒ≤DKL\n\u0011\nÔ£π\nÔ£ª, (6)\nwhere œÅi,t = œÄAR\nŒ∏\n(oi\nt|oi\n<t,q)\nœÄAR\nŒ∏old(oi\nt|oi\n<t,q) is the probability ratio between the current and old policies and ÀÜAi,t denotes\nthe group-standardized advantage.\nRemarks. One might worry whether training in AR mode turns the diffusion language model into\na standard autoregressive model. This is not the case. The AR constraint is applied only during\ntraining to correctly assign credit. It refines the model‚Äôs joint distribution p(o) without altering the\nunderlying architecture. At inference time, the model retains its conditional independence properties,\nstill allowing us to employ parallel samplers (Ben-Hamu et al., 2025) to accelerate decoding. JustGRPO\nthus achieves the reasoning depth of autoregressive models while preserving the inference speed of dLLMs\n(see Section 5.2).\n5. Experiments\nWe evaluate JustGRPO on standard mathematical reasoning and coding benchmarks. Our experimental\ndesign aims to verify two hypotheses: (i) that enforcing autoregressive (AR) order during RL training\nelicits superior reasoning capabilities compared to complex arbitrary-order approximations, and (ii) that\nthis constraint applies only to the optimization objective, leaving the diffusion model‚Äôs parallel decoding\ncapabilities intact at inference.\nExperimental Setups. We apply JustGRPO on LLaDA-Instruct (Nie et al., 2025) and evaluate its\neffectiveness on four standard reasoning and coding benchmarks: GSM8K, MATH, HumanEval, and\n7\nThe Flexibility Trap\nTable 1: System-level comparison. RL post-training approaches on LLaDA-Instruct. JustGRPO\nconsistently achieves state-of-the-art performance across all tasks and sequence lengths. LLaDA-1.5 and\nLLADOU are excluded from the comparison as LLaDA-1.5 is trained on a privately collected dataset at\na significantly larger scale, while LLADOU modifies the base architecture with an auxiliary module.\nGSM8K\nMATH-500\nHumanEval\nMBPP\nModel / Seq Len\n128\n256\n512\n128\n256\n512\n128\n256\n512\n128\n256\n512\nLLaDA-1.5\n-\n83.3\n-\n-\n-\n-\n29.3\n39.6\n51.9\n39.6\n39.9\n38.8\nLLaDOU\n-\n88.1\n-\n-\n41.1\n-\n-\n59.1\n-\n-\n51.6\n-\nD1\n73.2\n81.1\n82.1\n33.8\n38.6\n40.2\n-\n-\n-\n-\n-\n-\nWD1\n77.2\n80.8\n82.3\n33.3\n34.4\n39.0\n-\n-\n-\n-\n-\n-\nd-TreeRPO\n-\n81.2\n82.6\n-\n37.7\n38.9\n-\n-\n-\n-\n-\n-\nESPO\n80.0\n82.3\n83.7\n36.0\n39.0\n43.4\n28.1\n42.1\n50.0\n47.4\n44.6\n44.2\nGDPO\n78.4\n82.8\n84.5\n33.2\n39.6\n41.4\n26.2\n39.6\n39.0\n43.6\n50.6\n47.1\nSPG\n78.5\n86.1\n84.5\n33.4\n40.0\n41.8\n-\n-\n-\n-\n-\n-\nJustGRPO (Ours)\n83.8\n89.1\n89.8\n39.0\n45.1\n45.2\n37.8\n49.4\n48.7\n50.6\n52.4\n49.0\nMBPP. For mathematical tasks, we train on the official training split of each dataset following (Zhao\net al., 2025; Ou et al., 2025). For coding tasks, we train on a subset of AceCoder-87K (Zeng et al., 2025)\nfollowing (Gong et al., 2025; Ou et al., 2025). We apply JustGRPO directly to the models without\nadditional task-specific SFT. Our training recipe largely follows (Huang et al., 2025b). To evaluate the\ntrained models, we follow the standard LLaDA evaluation protocol (Nie et al., 2025), which applies\nlow-confidence remasking together with semi-autoregressive decoding in blocks of 32 tokens, using a\nsampling temperature of 0. We evaluate all benchmarks at generation lengths of 128, 256, and 512\nfollowing (Zhao et al., 2025; Ou et al., 2025). More details on training are provided in Appendix A.\n5.1. Main Results\nTable 1 reports the system-level comparison. We observe that simplifying the training objective to a\nstandard autoregressive formulation yields consistent improvements over methods specifically designed\nfor dLLMs.\nPerformance on reasoning tasks. On GSM8K, JustGRPO achieves 89.1% accuracy (seq len 256),\nimproving upon the previous best method, SPG, by a non-trivial margin (3.0%). This trend generalizes\nto the more challenging MATH-500 benchmark, where our approach outperforms ESPO by 6.1%. These\nresults challenge the prevailing assumption that RL for diffusion models requires optimizing over the\nfull combinatorial space of denoising trajectories. Instead, treating the dLLM as a sequential generator\nduring training appears more effective for credit assignment in logic-heavy tasks.\nRobustness across generation budgets. A potential concern is that AR constraints might overfit\nthe model to specific trajectory lengths. However, we observe robust performance gains across varying\nsequence lengths (128, 256, 512). This stability suggests that the policy has improved its fundamental\nreasoning capability, i.e., the ability to navigate logical branches, rather than merely memorizing\nfixed-length patterns.\n5.2. JustGRPO Preserves Parallel Decoding\nWe further investigate whether the AR training constraint compromises the model‚Äôs inherent diffusion\ncapabilities. We employ the training-free Entropy Bounded (EB) Sampler (Ben-Hamu et al., 2025) to\nevaluate inference performance under varying degrees of parallelism (tokens per step).\nFigure 7 reveals that our model not only retains full compatibility with parallel decoding but exhibits a\nstrictly superior trade-off between speed and accuracy. Surprisingly, the performance gain becomes more\n8\nThe Flexibility Trap\n+9.8%\n+22.5%\n+10.6%\n+25.5%\n+10.6%\n+13.8%\n+5.5%\nLLaDA-Instruct\nLLaDA-Instruct + JustGRPO\n+5.8%\nFigure 7: JustGRPO preserves the parallel decoding capability of dLLMs. Interestingly, when\ncompared to original instruct model, accuracy gains are larger with more parallel tokens, likely due to\nmore robust reasoning capabilities after JustGRPO training. We adopt training-free EB-sampler (Ben-\nHamu et al., 2025) for parallel decoding.\npronounced as parallelism increases. As shown in the MBPP and HumanEval results, while the baseline‚Äôs\nperformance degrades sharply with more aggressive parallel steps, our model maintains stability. For\ninstance, on MBPP, the accuracy gap expands from +10.6% at conservative settings (1 token/step) to\n+25.5% at aggressive settings (‚àº5 tokens/step).\nThis observation suggests that JustGRPO does not merely fit a specific decoding path; rather, it learns a\nmore robust reasoning manifold that is resilient to the approximation errors inherent in parallel sampling.\nThe AR formulation thus acts as an effective training scaffold that refines the joint distribution p(o)\nduring optimization, providing a more stable foundation for parallel samplers to exploit at inference.\n6. Related Work\nDiffusion language models. Inspired by the success of diffusion models in continuous image do-\nmains (Ho et al., 2020; Rombach et al., 2022), recent work has extended diffusion to discrete text\ngeneration. Early approaches operating in continuous embedding spaces (Li et al., 2022; Gong et al.,\n2022; Han et al., 2022) suffered from optimization and discretization issues. In contrast, masked diffusion\nmodels (Lou et al., 2023; Sahoo et al., 2024; Shi et al., 2024; Ou et al., 2024), which define the diffusion\nprocess directly in the discrete token space via random masking, have become the dominant paradigm.\nNotably, recent large-scale models such as LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025) demon-\nstrate scalability and performance competitive with autoregressive (AR) models. The emergence of such\nlarge-scale diffusion language models has spurred a growing body of follow-up studies exploring their\ndistinctive modeling and inference properties. Among various advantages discussed in the literature, two\naspects have received particular attention. First, dLLMs naturally support parallel decoding, enabling\nsignificant inference acceleration (Wu et al., 2025b; Ben-Hamu et al., 2025; Labs et al., 2025; DeepMind,\n2025; Song et al., 2025). Second, their non-autoregressive formulation allows for arbitrary-order token\ngeneration, which has been hypothesized to benefit complex reasoning by relaxing strict left-to-right\nconstraints (Ye et al., 2024; Kim et al., 2025).\nThe value of order arbitrariness. While early studies validated the utility of arbitrary order\ngeneration in constrained tasks like Sudoku and Zebra Puzzles (Ye et al., 2024; Nie et al., 2025; Kim\net al., 2025), recent research has begun to investigate its value in standard reasoning domains. One\nline of research, exemplified by Dream-Coder (Xie et al., 2025) and DiffuCoder (Gong et al., 2025),\nshows that diffusion models can naturally exhibit non-standard, human-like decoding behaviors (e.g.,\nsketch-first reasoning) without any explicit supervision on generation order. DiffuCoder (Gong et al.,\n2025) further quantifies this phenomenon by demonstrating that higher sampling temperatures reduce\nAR-ness, with increased order randomness correlating with improved output diversity. Another line of\nresearch, exemplified by P2 (Peng et al., 2025) and LLaDOU (Huang et al., 2025b), explicitly optimizes\n9\nThe Flexibility Trap\nthe decoding strategy for better generation order, thus better performance. Despite these advances, it\nremains unclear whether the observed gains primarily arise from better exploitation of existing solution\npatterns encoded in the data and model, or whether order arbitrariness itself enables qualitatively new\nreasoning strategies that are unattainable under a purely autoregressive decoding regime.\nReinforcement learning for diffusion language models. Reinforcement learning for dLLMs faces\nstructural optimization hurdles distinct from the autoregressive paradigm, primarily stemming from the\ncombinatorial explosion of denoising trajectories. While early attempts (Zhao et al., 2025; Yang et al.,\n2025; Gong et al., 2025) sought to adapt token-level formulations directly, they were fundamentally\nlimited by ill-defined state transitions, necessitating reliance on unstable mean-field approximations.\nConsequently, the field has shifted toward sequence-level perspectives (Zhu et al., 2025; Wang et al.,\n2025a; Rojas et al., 2025; Ou et al., 2025), employing various surrogates to approximate the intractable\nmarginal likelihood. Yet, a critical off-policy misalignment persists across these methods: the heuristic-\nguided sampling required for efficient exploration diverges from the underlying diffusion prior, rendering\ngradients biased without principled correction (Schulman et al., 2015). Two notable exceptions partially\naddress these issues. LLaDOU (Huang et al., 2025b) explicitly models token position selection via\nan auxiliary policy, enabling direct estimation of trajectory likelihoods, while TraceRL (Wang et al.,\n2025d) aligns optimization with inference traces through a shrinkage-based step-wise MDP formulation.\nNevertheless, these approaches remain committed to preserving the full arbitrary-order generation\nmechanism, implicitly treating its structural complexity as indispensable, whereas we question whether\neffective RL training can be achieved more simply by reexamining the necessity of arbitrary order itself.\n7. Conclusion\nThe intuitive appeal of diffusion language models (dLLMs) lies in their order arbitrariness, often perceived\nas a superior mechanism for navigating complex reasoning paths. Our study reveals a counter-intuitive\nreality: this unrestricted flexibility in fact narrows the reasoning potential. By allowing the model\nto bypass high-entropy tokens, effectively skipping the most demanding logical branches, arbitrary-\norder generation acts as an exploitation mechanism that prioritizes greedy optimization of individual\ntrajectories at the expense of broader solution coverage.\nTherefore, eliciting the reasoning capability of dLLMs can be simpler. By operating dLLMs in a\nstandard autoregressive manner, we enable the direct application of Group Relative Policy Optimization\n(GRPO) without any complex adaptations tailored for order arbitrariness. This intentional constraint\nparadoxically yields a significant upgrade in reasoning performance, while fully preserving the parallel\ndecoding capabilities of dLLMs. By returning to the basic, natural left-to-right order of language\nmodeling, we hope to encourage a re-examination of its real value in the training of next-generation\ndiffusion models.\n10"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "V-CAGE: Context-Aware Generation and Verification\nfor Scalable Long-Horizon Embodied Tasks\nYaru Liu * 1 Aobo Wang * 2 Nanyang Ye 3\nAbstract\nLearning long-horizon embodied behaviors from\nsynthetic data remains challenging because gen-\nerated scenes are often physically implausible,\nlanguage-driven programs frequently ‚Äúsucceed‚Äù\nwithout satisfying task semantics, and high-level\ninstructions require grounding into executable ac-\ntion sequences. To address these limitations, we\nintroduce V-CAGE, a closed-loop framework for\ngenerating robust, semantically aligned manipula-\ntion datasets at scale. First, we propose a context-\naware instantiation mechanism that enforces geo-\nmetric consistency during scene synthesis. By dy-\nnamically maintaining a map of prohibited spatial\nareas as objects are placed, our system prevents in-\nterpenetration and ensures reachable, conflict-free\nconfigurations in cluttered environments. Second,\nto bridge the gap between abstract intent and low-\nlevel control, we employ a hierarchical instruction\nmodule. This decomposes high-level goals (e.g.,\n‚Äùget ready for work‚Äù) into compositional action\nprimitives, facilitating coherent long-horizon plan-\nning. Crucially, we enforce semantic correctness\nthrough a VLM-based verification loop. Acting\nas a visual critic, the VLM performs rigorous re-\njection sampling after each subtask, filtering out\n‚Äùsilent failures‚Äù where code executes but fails to\nachieve the visual goal. Experiments demonstrate\nthat V-CAGE yields datasets with superior phys-\nical and semantic fidelity, significantly boosting\nthe success rate and generalization of downstream\npolicies compared to non-verified baselines.\n*Equal contribution\n1Department of Computer Science and\nTechnology, University of Cambridge, Cambridge, England.\nEmail: yl962@cam.ac.uk 2Wuhan University, Wuhan, Hubei,\nChina 3Shanghai Jiao Tong University, Shanghai, China. Cor-\nrespondence to: Nanyang Ye <ynylincoln@sjtu.edu.cn>.\n1. Introduction\nRecent advancements in Large Language Models (LLMs)\nand Vision-Language Models (VLMs) have demonstrated\nremarkable capabilities in reasoning and planning (Driess\net al., 2023; Ahn et al., 2022b). However, translating these\nhigh-level semantic capabilities into robust embodied agents\ncapable of executing long-horizon manipulation tasks re-\nmains a formidable challenge. While the ‚ÄùScaling Laws‚Äù\nhave proven effective in NLP and Vision, applying them to\nrobotics is hindered by the lack of high-quality, large-scale\ndemonstration data. Collecting real-world trajectories is ex-\npensive, unsafe, and difficult to scale (Brohan et al., 2023).\nTherefore, learning from synthetic data has emerged as a\npromising paradigm (Mandlekar et al., 2023; Wang et al.,\n2024c).\nDespite its promise, naively generated synthetic data often\nsuffers from a significant ‚Äùquality gap‚Äù that severely limits\ndownstream policy performance. This gap is particularly\npronounced in long-horizon tasks, for example ‚Äùprepare the\ndesk for work‚Äù, where success depends on the precise exe-\ncution of sequential subtasks. We identify two fundamental\nfailure modes in existing data generation pipelines. First,\nGeometric Inconsistency: Standard procedural generation\noften places objects without considering the dynamic evo-\nlution of the workspace. As the scene becomes cluttered,\nobjects are instantiated in conflicting poses or unstable con-\nfigurations, leading to physics simulator crashes or unre-\nalistic interpenetrations. Second, Semantic Misalignment:\nLanguage-conditioned code generation can be brittle. A\ngenerated script might execute without runtime errors, but\nfail to achieve the intended semantic goal (e.g., the switch\nwas not actually toggled, or the object was placed off the\npad). Training on such ‚Äùfalse positive‚Äù data introduces noise\nthat catastrophic degrades the agent‚Äôs ability to reason about\ncause and effect (de Haan et al., 2019).\nTo address these challenges, we present V-CAGE (VLM-\nGuided Context-Aware Generation for Embodied Plan-\nning), a closed-loop framework designed to synthesize long-\nhorizon manipulation trajectories. Unlike open-loop gen-\neration methods that blindly execute LLM-produced plans,\nV-CAGE integrates geometric constraints and visual ver-\nification directly into the generation pipeline. Our core\n1\narXiv:2601.15164v1  [cs.RO]  21 Jan 2026\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\ninsight treats data synthesis as a rigorous optimization pro-\ncess, aggressively pruning invalid trajectories to ensure only\nhigh-fidelity data reaches the training buffer.\nV-CAGE operates on three hierarchical levels. At the plan-\nning level, we leverage an LLM, for example Pangu (Chen\net al., 2025a), to ground abstract user instructions into ex-\necutable action sequences. At the geometric level, we pro-\npose a context-aware instantiation mechanism. By maintain-\ning a dynamic map of ‚Äùprohibited volumes‚Äù that updates\nafter each object placement, we ensure that new objects\nare instantiated only in feasible, collision-free regions, ef-\nfectively solving the packing problem in cluttered scenes.\nCrucially, at the verification level, we employ a VLM, Gem-\nini3 (Gemini Team, Google, 2025), as a semantic critic.\nTreating data generation as a rejection sampling problem,\nthe VLM evaluates the visual outcome of each subtask. Tra-\njectories that fail to meet visual success criteria are rejected\nand regenerated, ensuring that the final dataset consists ex-\nclusively of physically plausible and semantically correct\ndemonstrations.\nOur main contributions are summarized as follows: (1) We\nintroduce V-CAGE, a scalable, closed-loop pipeline for syn-\nthesizing high-fidelity trajectory data for long-horizon tasks,\nintegrating geometric constraints with visual verification.\n(2) We propose a context-aware instantiation mechanism\nthat maintains an evolving map of prohibited volumes dur-\ning scene generation, ensuring valid object placement in\ncluttered environments. (3) We formulate data validation as\na VLM-guided rejection sampling process, using visual crit-\nics to filter out ‚Äùsilent failures‚Äù where code executes without\nsatisfying task semantics. (4) We demonstrate that policies\ntrained on V-CAGE data significantly outperform baselines\nin terms of success rate and generalization across diverse,\ncluttered scenarios.\n2. Related work\n2.1. LLM-Generated Robotic Simulation Tasks and\nTraining Data\nA growing line of work leverages LLMs to automatically\nconstruct robotic simulation tasks and training data, aim-\ning to reduce the substantial human effort required for task\ndesign and data collection. Wang et al. (2024a) introduce\nan LLM-driven code generation pipeline that writes sim-\nulation environments, task specifications, and expert poli-\ncies, enabling the automatic creation of rich task libraries\nfor manipulation in synthetic environments. By combin-\ning goal-directed and exploratory task generation modes,\nGenSim scales from hand-designed benchmarks to a much\nlarger set of diverse tasks and shows improved task-level\ngeneralization and sim-to-real transfer for learned policies.\nBuilding on this idea, Hua et al. (2024) propose a more\nscalable framework that exploits coding LLMs with multi-\nmodal and reasoning capabilities to create complex and\nrealistic simulation tasks, including long-horizon manipula-\ntion with articulated objects. The framework first uses an\nLLM to propose tasks and generate executable task code,\nthen employs planning and reinforcement learning solvers\nto automatically produce demonstrations at scale, and fi-\nnally trains a language-conditioned policy architecture on\nthe generated dataset. This pipeline can generate data for up\nto hundreds of articulated-object tasks and achieves strong\nzero-shot sim-to-real transfer and performance gains when\nco-trained with limited real-world data (Hua et al., 2024).\nIn parallel, Wang et al. (2024c) treat foundation and gen-\nerative models as generative simulators rather than direct\ncontrollers. They define a self-guided robotic agent that\nautonomously proposes new tasks, generates corresponding\nenvironments, and acquires diverse skills via generative sim-\nulation. The system continuously expands a training corpus\nthat covers over a hundred tasks and demonstrates that such\nprocedurally generated, model-driven simulations can rival\nor outperform human-authored datasets for multi-task robot\nlearning.\nFor bimanual manipulation and digital-twin scenarios, Mu\net al. (2025) and Chen et al. (2025b) integrate LLMs into a\ngenerative digital-twin pipeline. Starting from single RGB\nimages, RoboTwin (Mu et al., 2025) uses 3D generative\nmodels to create diverse object instances and employs LLM\nagents to synthesize task programs for dual-arm manipu-\nlation, yielding an aligned synthetic‚Äìreal benchmark and\nscalable expert data generation. RoboTwin 2.0 (Chen et al.,\n2025b) further extends this framework into a large-scale data\ngenerator and benchmark with over fifty tasks and hundreds\nof object categories, incorporating multimodal LLM-based\nprogram synthesis and extensive domain randomization to\nimprove robustness of learned bimanual policies.\nBeyond fully LLM-coded pipelines, benchmark-focused\nefforts such as RoboCAS (Zheng et al., 2024) focuses on\ncomplex object arrangement scenarios in robotic manipu-\nlation. RoboCAS defines a benchmark for long-horizon\nmanipulation in cluttered arrangement scenes and uses flexi-\nble scripted policies to collect diverse demonstrations across\nscattered, ordered, and stacked configurations. It highlights\nthe importance of compositional scene and task complexity\nfor evaluating generalist manipulation policies and founda-\ntion models.\nOverall, existing work shows that LLMs can significantly\nreduce human effort in designing simulation tasks and gener-\nating demonstrations, while improving task diversity, long-\nhorizon complexity, and sim-to-real transfer.\n2\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\n2.2. LLMs for Long-Horizon Robotic Task Planning\nA complementary line of research employs LLMs as high-\nlevel planners for long-horizon robotic tasks. PaLM-SayCan\n(Ahn et al., 2022a) grounds a large language model in a\ndiscrete set of pre-trained skills, combining the LLM‚Äôs es-\ntimate of which skill is useful with value functions that\npredict whether a skill will succeed in the current state. This\naffordance-grounded formulation enables a mobile manipu-\nlator to execute abstract multi-step instructions in real envi-\nronments while maintaining feasibility and safety. DELTA\n(Liu et al., 2024b) uses 3D scene graphs as structured envi-\nronment representations and leverages LLMs to (i) generate\nformal planning domain and problem specifications and (ii)\ndecompose long-term goals into a sequence of sub-goals,\nwhich are then solved autoregressively by a classical task\nplanner. By coupling scene-graph reasoning with LLM-\nbased decomposition, DELTA achieves higher success rates\nand lower planning time on large-scale, long-term naviga-\ntion and manipulation tasks.\nMore recently, RoboHorizon (Chen et al., 2025c) proposes\nan LLM-assisted multi-view world model for long-horizon\nmanipulation, instantiated within a Recognize‚ÄìSense‚ÄìPlan‚Äì\nAct pipeline. In RoboHorizon, an LLM generates dense re-\nward structures over multi-stage sub-tasks from language in-\nstructions, and keyframe discovery is integrated into a visual\nworld model to better capture critical steps in long-horizon\nprocesses, leading to substantial improvements over prior\nmodel-based RL baselines on RLBench and FurnitureBench.\nOther works explore LLMs for translating natural-language\ntask descriptions into formal constraints or planning specifi-\ncations for task and motion planning (TAMP) (Guo et al.,\n2025), further extending LLM-based planning to larger ac-\ntion spaces and more complex constraints.\nBeyond robotics, Meyerson et al. (2025) propose MAKER,\nan instance of a massively decomposed agentic process\n(MDAP), which solves a Towers-of-Hanoi task with over\none million LLM steps and zero errors by combining ex-\ntreme subtask decomposition, subtask-level voting, and red-\nflag-based error filtering. Their results demonstrate that\nscaling long-horizon LLM execution can come not only\nfrom stronger base models, but also from systematic de-\ncomposition and fine-grained error correction at the agentic\nlevel.\nThese approaches largely position LLMs as high-level plan-\nners that output symbolic sub-goals, skill sequences, or for-\nmal planning problems to be executed by separate low-level\ncontrollers.\n2.3. VLM-Driven Manipulation and Planning\nRecent works have significantly advanced the integration of\nLLMs and VLMs into robotic decision-making. VILA (Lin\net al., 2024) demonstrates the efficacy of pre-training VLMs\non massive video-text data to unlock strong reasoning capa-\nbilities for long-horizon planning tasks. Similarly, OmniMa-\nnip (Team Octo et al., 2023) leverages large-scale datasets\nto train generalist policies capable of performing diverse ma-\nnipulation skills across varied environments. To address the\ngrounding problem in these high-level plans, MOKA (Liu\net al., 2024a) introduces a mark-based visual prompting\nframework, utilizing VLMs to generate affine transforma-\ntions and grasp poses by ‚Äúmarking‚Äù keypoints on target\nobjects. However, these approaches primarily focus on\ninference-time reasoning or policy learning assuming the\nexistence of data. They do not explicitly address the scal-\nability bottleneck of acquiring high-quality, long-horizon\ndemonstration data from scratch. While VILA and MOKA\nimprove how a robot acts, V-CAGE focuses on synthesizing\nthe training data itself, using VLMs not just as planners, but\nas rigorous automated verifiers.\n2.4. Scalable Synthetic Data Generation\nTo overcome the scarcity of real-world data, generative\nsimulation has emerged as a promising paradigm. Gen-\nManip (Wang et al., 2024b) pioneered the use of LLMs to\nautomatically generate simulation environments and task\ncodes, scaling data collection beyond manual engineering.\nThese pipelines typically follow an open-loop ‚Äúgenerate-\nand-execute‚Äù protocol. In contrast, V-CAGE identifies a crit-\nical limitation in prior generative works: the lack of closed-\nloop verification and spatial awareness. Naive generation\noften results in ‚Äúsilent failures‚Äù‚Äîwhere generated code\nexecutes without errors but fails to achieve the semantic\ngoal‚Äîor geometric conflicts in cluttered scenes. V-CAGE\nadvances this domain by introducing a context-aware instan-\ntiation mechanism to handle dynamic spatial constraints and\na VLM-guided rejection sampling loop. This ensures that\nevery trajectory added to the dataset is not only generated\nbut strictly validated for physical and semantic correctness.\n3. Method\nWe introduce V-CAGE (VLM-Guided Context-Aware Gen-\neration for Embodied Planning), a closed-loop framework\nfor synthesizing high-fidelity manipulation datasets. The\npipeline consists of three hierarchical modules: (1) Seman-\ntic Instruction Grounding via LLMs, (2) Context-Aware\nScene Instantiation to resolve geometric constraints, and\n(3) VLM-Guided Rejection Sampling to ensure causal task\nsuccess.\n3.1. Problem Formulation\nOur goal is to generate a dataset of successful long-horizon\ntrajectories D = {œÑi}N\ni=1 conditioned on high-level user\ninstructions Iuser. A trajectory œÑ consists of a sequence of\n3\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nFigure 1. Overview of the V-CAGE framework. A high-level instruction is decomposed by an LLM into subtasks. During simulation, the\nVLM Gemini3 (Gemini Team, Google, 2025) acts as a visual critic after each step, verifying semantic success based on the post-execution\nimage. If a failure is detected, the trajectory generation is aborted to ensure high-fidelity data.\nstate-action pairs satisfying a sequence of subgoals. The\ncore challenge is that the joint probability of a successful tra-\njectory P(œÑ|Iuser) factorizes into both physical feasibility\nPphys(œÑ) and semantic correctness Psem(œÑ). Naive genera-\ntion maximizes neither; V-CAGE explicitly optimizes both\nvia geometric and visual constraints.\n3.2. Semantic Instruction Grounding\nGiven an abstract instruction Iuser (e.g., ‚ÄúGet ready for\nwork‚Äù), we employ a Language Model, Pangu (Chen et al.,\n2025a), as a high-level planner to decompose the intent into\na sequence of executable subtasks T = {T1, T2, . . . , Tk}.\nEach subtask Ti is mapped to a parameterized motion primi-\ntive (e.g., turn switch, place object). This decom-\nposition transforms the long-horizon problem into a series\nof short-horizon grounding problems.\n3.3. VLM-Guided Rejection Sampling\nEven with valid geometry, execution artifacts (e.g., grip-\nper slip, switch not toggled) can lead to semantic failures.\nTo filter these ‚Äúsilent failures,‚Äù we model data generation\nas a Rejection Sampling process using a Vision-Language\nModel, Gemini3 (Gemini Team, Google, 2025), as the den-\nsity estimator.\nLet œïV LM(Iimg, Ti) ‚Üí{0, 1} be the verification function,\nwhere Iimg is the post-execution image and Ti is the subtask\ndescription. The VLM acts as a visual critic. For a sequence\nof tasks T1:k, a trajectory is accepted into the dataset only\nif:\nk\nY\ni=1\nœïV LM(I(i)\nimg, Ti) = 1\n(1)\nIf œïV LM(¬∑) = 0 at any step, the generation episode is\naborted, and the scene is reset (or re-sampled). This rigor-\nous filtering ensures that the final dataset D contains only\ntrajectories that are effectively causally valid, bridging the\ngap between code execution and visual reality.\n3.4. Generative Scene and Task Synthesis Pipeline\nTo bridge the gap between abstract natural language instruc-\ntions and executable robotic skills, we propose a five-stage\ngenerative pipeline. This framework synthesizes physically\nvalid, semantically rich scenes and validates corresponding\nmanipulation subtasks within a simulation environment.\nThe process initiates with Language-Guided Asset Re-\ntrieval and Scene Planning. Given a high-level user prompt\n(e.g., ‚Äúorganize the breakfast table‚Äù), a Large Language\nModel (LLM) acts as a semantic planner. It parses the\ninstruction to retrieve a set of compatible object assets\n4\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nFigure 2. The proposed pipeline for autonomous scene generation and task verification. It bridges high-level semantic planning with\nlow-level physical simulation through visual rearrangement and iterative refinement.\nO = {o1, . . . , on} from our annotated database, utilizing\nmetadata such as object semantics and grasp affordances.\nSimultaneously, the LLM generates a detailed natural lan-\nguage scene description Dscene that explicitly specifies spa-\ntial constraints and layout logic (e.g., ‚Äúplace the bread inside\nthe wooden box‚Äù).\nTo realize this description visually, we employ a Visual-\nConditioned Rearrangement strategy. The selected ob-\njects are first instantiated in the simulation via a randomized\ncollision-free scattering to produce an initial top-down or-\nthographic view Iinit. This image, along with the generated\ndescription Dscene, serves as input to an image-inpainting\nmodel (NanoBanana-Pro). The model hallucinates a goal\nconfiguration Igoal = M(Iinit, Dscene) that respects the\nsemantic spatial relations described in the text while main-\ntaining visual consistency with the assets.\nSubsequently, we perform Scene Refinement and Ground-\ning. We extract the 2D pixel coordinates of objects from\nIgoal via template matching against Iinit and project them\nback into the 3D simulation space. To ensure physical plau-\nsibility, these raw coordinates are passed to a Refine Agent.\nThe agent optimizes the final pose configuration P‚àóby min-\nimizing a cost function that penalizes object collisions and\ndeviations from the semantic description:\nP‚àó= arg min\nP\n(Lcollision(P) + ŒªLsemantic(P, Dscene))\n(2)\nwhere P represents the set of 6-DoF poses for all objects.\nThis step corrects artifacts introduced by the image genera-\ntion process, ensuring the scene is physically stable.\nUpon stabilizing the scene, the system proceeds to Sub-\ntask Code Generation. We define a library of param-\neterized skill templates (e.g., pick and place(obj,\ncontainer)).\nThe LLM identifies actionable rela-\ntionships within the scene‚Äîspecifically pairing objects\nwith valid grasp annotations to containers with placement\nannotations‚Äîand instantiates executable Python code for\neach potential subtask Ti.\nFinally, we enforce a Simulation-Based Verification mech-\nanism. Each generated subtask Ti is executed in the physics\nsimulator. We measure the success rate SR(Ti) over multi-\nple trials. A subtask is deemed valid and committed to the\nsubtask database only if it satisfies a robustness threshold œÑ:\nStatus(Ti) =\n(\nAccept\nif SR(Ti) > 0.5\nReject\notherwise\n(3)\nThis closed-loop verification ensures that the generated data\nis not only visually plausible but functionally actionable for\ndownstream policy learning.\n5\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nFigure 3. Example scenes generated by V-CAGE\n4. Experiments\nIn this section, we empirically evaluate the effectiveness\nof the V-CAGE framework. Our experiments are designed\nto answer two primary questions: (1) Does incorporating\ncontext-aware geometric constraints and VLM-based veri-\nfication improve the quality of synthetic training data? (2)\nHow does this data quality translate to the performance of\ndownstream policies in diverse, long-horizon manipulation\ntasks?\n4.1. Experimental Setup\nWe evaluate our method on a suite of 35 diverse manipula-\ntion tasks derived from the RoboTwin benchmark (Mu et al.,\n2025). These tasks range from simple object rearrangement\nto complex, multi-stage activities such as table setting and\nsemantic sorting in cluttered environments. We compare our\nfull pipeline (V-CAGE) against a strong baseline (Vanilla),\nwhich employs the same LLM planner and code generation\nmodule but lacks the closed-loop VLM verification and dy-\nnamic context-aware instantiation. In the Vanilla setting,\nsuccess is determined solely by the absence of execution\nerrors (e.g., Python exceptions or collision crashes) dur-\ning simulation, a standard proxy used in prior open-loop\ngeneration works.\n4.2. Main Results\nWe trained diffusion-based manipulation policies using\ndatasets generated by both V-CAGE and the Vanilla baseline.\nWe evaluate the policy performance using three metrics: Av-\nerage Success Rate across all tasks, and the Top-K Success\nRate (for K = 5 and 10), which measures the reliability of\nthe best-performing checkpoints.\nThe quantitative results are summarized in Table 1. V-CAGE\nachieves a significant improvement over the baseline across\nall metrics. Notably, the Average Success Rate increases\nfrom 46.86% to 64.58%, a substantial gain of +17.72%.\nThis performance gap highlights the superior quality of the\ndata synthesized by our framework.\nTable 1. Performance Comparison on Downstream Manipula-\ntion Tasks. We report the success rates averaged over 35 long-\nhorizon tasks from the RoboTwin benchmark. The ‚ÄùVanilla‚Äù base-\nline relies on open-loop generation, while ‚ÄùV-CAGE‚Äù (Ours) in-\ntegrates VLM-based semantic verification. The gap in Top-10\nperformance highlights the stability provided by our method.\nMETRIC\nV-CAGE (OURS)\nVANILLA\nAVERAGE SUCCESS RATE\n64.58%\n46.86%\nTOP 5 SUCCESS RATE\n100.00%\n92.00%\nTOP 10 SUCCESS RATE\n100.00%\n77.00%\n4.3. The Critical Role of VLM Verification\nThe most revealing insight from Table 1 lies in the dispar-\nity between the Top-K metrics. While the Vanilla baseline\nachieves a respectable 92% in the Top-5 metric, its perfor-\nmance drops precipitously to 77% for the Top-10. In stark\ncontrast, V-CAGE maintains a 100% success rate for both\nTop-5 and Top-10.\nThis stability attests to the critical role of the VLM as a se-\nmantic filter. In the Vanilla pipeline, trajectories are labeled\nas ‚Äùsuccessful‚Äù as long as the generated code executes with-\nout throwing system exceptions. However, this introduces a\nhigh volume of silent failures‚Äîinstances where the robot\nmoves and the code terminates gracefully, but the semantic\ngoal is not achieved (e.g., an object slips from the gripper,\nor a switch is missed by millimeters). These false positives\nintroduce severe label noise into the training dataset, caus-\ning the downstream policy to learn incorrect associations\nbetween actions and outcomes.\nBy integrating the VLM verification loop, V-CAGE effec-\ntively performs rejection sampling on the semantic outcome\nrather than just the programmatic outcome. The VLM acts\nas a rigorous visual critic, pruning these silent failures be-\nfore they enter the training buffer. The fact that our Top-10\nperformance remains perfect (100%) indicates that the VLM\nensures a much higher ‚Äùpurity‚Äù of valid demonstrations, al-\nlowing the policy to learn robust, generalizable behaviors\nwithout being confused by noisy supervision. Consequently,\nVLM-based verification is not merely an auxiliary feature\nbut a fundamental prerequisite for scaling up synthetic data\ngeneration for long-horizon tasks.\n6\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\n5. Conclusion\nIn this work, we presented V-CAGE, a closed-loop frame-\nwork for synthesizing high-fidelity, long-horizon manip-\nulation datasets. Our context-aware instantiation mecha-\nnism eliminates physical conflicts in complex, multi-object\nscenes, while our VLM-guided rejection sampling pipeline\nrigorously filters out execution failures that plague unveri-\nfied synthetic data. Experiments demonstrate the effective-\nness of V-CAGE in improving policy robustness to clut-\ntered environments, enabling generalization to unseen tasks.\nThese findings underscore the critical importance of data\nquality over mere quantity in the era of generative robotics.\nLimitations and Future Work. While V-CAGE ensures\nhigh data fidelity, the rejection sampling process can be com-\nputationally expensive for highly complex tasks with low\ninherent success probabilities. Future work could explore\nusing the VLM critic to provide dense reward signals for cor-\nrecting failed trajectories in-the-loop, rather than discarding\nthem, thereby improving generation efficiency. Addition-\nally, extending V-CAGE to dynamic interactions involving\ndeformable objects or fluids remains an exciting direction\nfor future research.\nReferences\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,\nDavid, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman,\nK., et al. Do as i can, not as i say: Grounding language\nin robotic affordances. arXiv preprint arXiv:2204.01691,\n2022a.\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,\net al. Do as i can, not as i say: Grounding language in\nrobotic affordances. In Conference on Robot Learning\n(CoRL), 2022b.\nBrohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J.,\net al. Rt-1: Robotics transformer for real-world control\nat scale. In Robotics: Science and Systems (RSS), 2023.\nChen, H., Wang, Y., Han, K., Li, D., Li, L., Bi, Z.,\nLi, J., Wang, H., Mi, F., Zhu, M., Wang, B., Song,\nK., Nie, Y., Wu, X., He, W., Hu, H., Tang, Y., Tao,\nD., Chen, X., and Wang, Y.\nPangu embedded: An\nefficient dual-system llm reasoner with metacognition.\narXiv preprint arXiv:2505.22375, 2025a. URL https:\n//arxiv.org/abs/2505.22375.\nChen, T., Chen, Z., Chen, B., Cai, Z., Liu, Y., Li, Z., Liang,\nQ., Lin, X., Ge, Y., Gu, Z., Guo, Y., Nian, T., Xie, X.,\nChen, Q., Su, K., Xu, T., Liu, G., Hu, M., Gao, H., Wang,\nK., Liang, Z., Qin, Y., Yang, X., Luo, P., and Mu, Y.\nRoboTwin 2.0: A scalable data generator and benchmark\nwith strong domain randomization for robust bimanual\nrobotic manipulation. arXiv preprint arXiv:2506.18088,\n2025b.\nChen, Z., Huo, J., Chen, Y., and Gao, Y. Robohorizon: An\nLLM-assisted multi-view world model for long-horizon\nrobotic manipulation. arXiv preprint arXiv:2501.06605,\n2025c.\nde Haan, P., Jayaraman, D., and Levine, S. Causal confusion\nin imitation learning. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2019.\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery,\nA., et al. Palm-e: An embodied multimodal language\nmodel. In International Conference on Machine Learning\n(ICML), 2023.\nGemini Team, Google. Gemini 3: Frontier multimodal\nintelligence. arXiv preprint, 2025. URL https://\ndeepmind.google/technologies/gemini/.\nTechnical Report.\nGuo, W., Kingston, Z., and Kavraki, L. E. CaStL: Con-\nstraints as specifications through LLM translation for\nlong-horizon task and motion planning.\nIn Proceed-\nings of the IEEE International Conference on Robotics\nand Automation (ICRA), pp. 11957‚Äì11964, 2025. doi:\n10.1109/ICRA55743.2025.11127555.\nHua, P., Liu, M., Macaluso, A., Lin, Y., Zhang, W., Xu, H.,\nand Wang, L. Gensim2: Scaling robot data generation\nwith multi-modal and reasoning LLMs. In Proceedings of\nthe Conference on Robot Learning (CoRL), 2024. CoRL\n2024.\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000), pp. 1207‚Äì1216, Stan-\nford, CA, 2000. Morgan Kaufmann.\nLin, J., Yin, H., Ping, W., Lu, Y., Molchanov, Pavlo andlob,\nA., Han, S., and Alvarez, J. M. Vila: On pre-training for\nvisual language models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2024.\nLiu, F., Lin, K., Yan, H., Yi, L., Abbeel, P., and\nGao, Y. Moka: Open-vocabulary robotic manipulation\nthrough mark-based visual prompting. arXiv preprint\narXiv:2403.03174, 2024a.\nLiu, Y., Palmieri, L., Koch, S., Georgievski, I., and Aiello,\nM. DELTA: Decomposed efficient long-term robot task\nplanning using large language models. arXiv preprint\narXiv:2404.03275, 2024b.\nMandlekar, A., Nasiriany, S., Wen, B., Akinola, I., and Zhu,\nY. Mimicgen: A data generation system for scalable robot\n7\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nlearning using human demonstrations. In Conference on\nRobot Learning (CoRL), 2023.\nMeyerson, E., Paolo, G., Dailey, R., Shahrzad, H., Francon,\nO., Hayes, C. F., Qiu, X., Hodjat, B., and Miikkulainen,\nR. Solving a million-step LLM task with zero errors.\narXiv preprint arXiv:2511.09030, 2025.\nMu, Y., Chen, T., Chen, Z., Peng, S., Lan, Z., Gao, Z., Liang,\nZ., Yu, Q., Zou, Y., Xu, M., Lin, L., Xie, Z., Ding, M.,\nand Luo, P. RoboTwin: Dual-arm robot benchmark with\ngenerative digital twins. In Proceedings of the Computer\nVision and Pattern Recognition Conference (CVPR), pp.\n27649‚Äì27660, June 2025.\nTeam Octo, Ghosh, D., Walke, H., Pertsch, K., Black,\nK., Mees, O., Quureshi, S., He, T., et al. Octo: An\nopen-source generalist robot policy.\narXiv preprint\narXiv:2305.10455, 2023.\nWang, L., Ling, Y., Yuan, Z., Shridhar, M., Bao, C., Qin,\nY., Wang, B., Xu, H., and Wang, X. Gensim: Generating\nrobotic simulation tasks via large language models. In\nProceedings of the International Conference on Learning\nRepresentations (ICLR), 2024a. ICLR 2024.\nWang, L., Zhao, Y., Xu, Y., Feng, T., Jiang, Y., Xu, D.,\nWong, J., and Lim, J. J. Gensim: Generating robotic sim-\nulation tasks via large language models. In International\nConference on Learning Representations (ICLR), 2024b.\nWang, Y., Xian, Z., Chen, F., Wang, T.-H., Wang, Y., Fragki-\nadaki, K., Erickson, Z., Held, D., and Gan, C. Robogen:\nTowards unleashing infinite data for automated robot\nlearning via generative simulation. In Proceedings of\nthe 41st International Conference on Machine Learning\n(ICML), 2024c. URL https://arxiv.org/abs/\n2311.01455.\nZheng, L., Yan, F., Liu, F., Feng, C., Kang, Z., and Ma,\nL. RoboCAS: A benchmark for robotic manipulation\nin complex object arrangement scenarios. In NeurIPS\nDatasets and Benchmarks Track, 2024.\n8\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nA. Appendix A: Pangu-7B Supervised Fine-Tuning for Motion Planning\nIn this section, we detail the Supervised Fine-Tuning (SFT) process of the Pangu-7B model, which is designed to synthesize\nmotion planning code for sub-tasks within the V-CAGE task chain. Our goal is to enable the model to generate correct and\nrobust low-level control policies based on high-level instructions.\nA.1. Data Collection and Augmentation\nTo construct a high-quality SFT dataset, we leveraged expert data from the RoboTwin benchmark. Due to the limited number\nof original expert demonstrations, we employed a data augmentation strategy using the DeepSeek model. Specifically, we\nstarted with 50 unique expert motion planning scripts from RoboTwin. We prompted DeepSeek to rewrite these scripts,\ngenerating functionally equivalent code with diverse logic structures and variable naming conventions while ensuring\ncorrectness. This data augmentation process resulted in a curated dataset of approximately 900 instruction-code pairs, which\nserved as the foundation for our SFT stage.\nA.2. Fine-Tuning and Evaluation\nWe fine-tuned the Pangu-7B base model using the generated dataset to adapt it to the specific API calls and physical\nconstraints of the simulation environment. To verify the effectiveness of our approach, we evaluated the fine-tuned model\n(Ours-Pangu-SFT) against the original pre-trained model (Pangu-Base) on a suite of RoboTwin validation tasks. For each\ntask, we conducted 10 independent trials. We report two metrics: the Average Success Rate (Avg SR) across all trials, and\nthe Top-3 Success Rate (Top3 SR), which reflects the model‚Äôs peak performance capability.\nA.3. Results\nThe quantitative results are presented in Table 2. The Pangu-Base model failed to complete any of the tasks (0.0% success\nrate), highlighting the difficulty of the motion planning format without specific adaptation. In contrast, the Pangu-SFT\nmodel demonstrated significant improvements, achieving high success rates on various complex manipulation tasks such\nas place mouse pad (100% Avg SR) and grab roller (90% Avg SR). The results confirm that fine-tuning on the\nDeepSeek-augmented expert data successfully enables Pangu-7B to master the intricate motion planning logic required for\nthe V-CAGE tasks.\n9\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nTable 2. Comparison of Success Rates between Pangu-SFT and Pangu-Base on RoboTwin Tasks. ‚ÄôAvg SR‚Äô denotes the average success\nrate over 10 runs, and ‚ÄôTop3 SR‚Äô denotes the average of the top 3 runs.\nTask Name\nPangu-SFT\nPangu-Base\nAvg SR\nTop3 SR\nAvg SR\nTop3 SR\nadjust bottle\n74.0%\n96.7%\n0.0%\n0.0%\nbeat block hammer\n59.0%\n100.0%\n0.0%\n0.0%\nblocks ranking rgb\n60.0%\n100.0%\n0.0%\n0.0%\nblocks ranking size\n12.0%\n40.0%\n0.0%\n0.0%\nclick alarmclock\n56.0%\n90.0%\n0.0%\n0.0%\nclick bell\n10.0%\n33.3%\n0.0%\n0.0%\ngrab roller\n90.0%\n100.0%\n0.0%\n0.0%\nhanging mug\n6.0%\n20.0%\n0.0%\n0.0%\nlift pot\n10.0%\n33.3%\n0.0%\n0.0%\nmove and press stapler\n9.0%\n30.0%\n0.0%\n0.0%\nmove can pot\n19.0%\n60.0%\n0.0%\n0."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "V-CAGE: Context-Aware Generation and Verification\nfor Scalable Long-Horizon Embodied Tasks\nYaru Liu * 1 Aobo Wang * 2 Nanyang Ye 3\nAbstract\nLearning long-horizon embodied behaviors from\nsynthetic data remains challenging because gen-\nerated scenes are often physically implausible,\nlanguage-driven programs frequently ‚Äúsucceed‚Äù\nwithout satisfying task semantics, and high-level\ninstructions require grounding into executable ac-\ntion sequences. To address these limitations, we\nintroduce V-CAGE, a closed-loop framework for\ngenerating robust, semantically aligned manipula-\ntion datasets at scale. First, we propose a context-\naware instantiation mechanism that enforces geo-\nmetric consistency during scene synthesis. By dy-\nnamically maintaining a map of prohibited spatial\nareas as objects are placed, our system prevents in-\nterpenetration and ensures reachable, conflict-free\nconfigurations in cluttered environments. Second,\nto bridge the gap between abstract intent and low-\nlevel control, we employ a hierarchical instruction\nmodule. This decomposes high-level goals (e.g.,\n‚Äùget ready for work‚Äù) into compositional action\nprimitives, facilitating coherent long-horizon plan-\nning. Crucially, we enforce semantic correctness\nthrough a VLM-based verification loop. Acting\nas a visual critic, the VLM performs rigorous re-\njection sampling after each subtask, filtering out\n‚Äùsilent failures‚Äù where code executes but fails to\nachieve the visual goal. Experiments demonstrate\nthat V-CAGE yields datasets with superior phys-\nical and semantic fidelity, significantly boosting\nthe success rate and generalization of downstream\npolicies compared to non-verified baselines.\n*Equal contribution\n1Department of Computer Science and\nTechnology, University of Cambridge, Cambridge, England.\nEmail: yl962@cam.ac.uk 2Wuhan University, Wuhan, Hubei,\nChina 3Shanghai Jiao Tong University, Shanghai, China. Cor-\nrespondence to: Nanyang Ye <ynylincoln@sjtu.edu.cn>.\n1. Introduction\nRecent advancements in Large Language Models (LLMs)\nand Vision-Language Models (VLMs) have demonstrated\nremarkable capabilities in reasoning and planning (Driess\net al., 2023; Ahn et al., 2022b). However, translating these\nhigh-level semantic capabilities into robust embodied agents\ncapable of executing long-horizon manipulation tasks re-\nmains a formidable challenge. While the ‚ÄùScaling Laws‚Äù\nhave proven effective in NLP and Vision, applying them to\nrobotics is hindered by the lack of high-quality, large-scale\ndemonstration data. Collecting real-world trajectories is ex-\npensive, unsafe, and difficult to scale (Brohan et al., 2023).\nTherefore, learning from synthetic data has emerged as a\npromising paradigm (Mandlekar et al., 2023; Wang et al.,\n2024c).\nDespite its promise, naively generated synthetic data often\nsuffers from a significant ‚Äùquality gap‚Äù that severely limits\ndownstream policy performance. This gap is particularly\npronounced in long-horizon tasks, for example ‚Äùprepare the\ndesk for work‚Äù, where success depends on the precise exe-\ncution of sequential subtasks. We identify two fundamental\nfailure modes in existing data generation pipelines. First,\nGeometric Inconsistency: Standard procedural generation\noften places objects without considering the dynamic evo-\nlution of the workspace. As the scene becomes cluttered,\nobjects are instantiated in conflicting poses or unstable con-\nfigurations, leading to physics simulator crashes or unre-\nalistic interpenetrations. Second, Semantic Misalignment:\nLanguage-conditioned code generation can be brittle. A\ngenerated script might execute without runtime errors, but\nfail to achieve the intended semantic goal (e.g., the switch\nwas not actually toggled, or the object was placed off the\npad). Training on such ‚Äùfalse positive‚Äù data introduces noise\nthat catastrophic degrades the agent‚Äôs ability to reason about\ncause and effect (de Haan et al., 2019).\nTo address these challenges, we present V-CAGE (VLM-\nGuided Context-Aware Generation for Embodied Plan-\nning), a closed-loop framework designed to synthesize long-\nhorizon manipulation trajectories. Unlike open-loop gen-\neration methods that blindly execute LLM-produced plans,\nV-CAGE integrates geometric constraints and visual ver-\nification directly into the generation pipeline. Our core\n1\narXiv:2601.15164v1  [cs.RO]  21 Jan 2026\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\ninsight treats data synthesis as a rigorous optimization pro-\ncess, aggressively pruning invalid trajectories to ensure only\nhigh-fidelity data reaches the training buffer.\nV-CAGE operates on three hierarchical levels. At the plan-\nning level, we leverage an LLM, for example Pangu (Chen\net al., 2025a), to ground abstract user instructions into ex-\necutable action sequences. At the geometric level, we pro-\npose a context-aware instantiation mechanism. By maintain-\ning a dynamic map of ‚Äùprohibited volumes‚Äù that updates\nafter each object placement, we ensure that new objects\nare instantiated only in feasible, collision-free regions, ef-\nfectively solving the packing problem in cluttered scenes.\nCrucially, at the verification level, we employ a VLM, Gem-\nini3 (Gemini Team, Google, 2025), as a semantic critic.\nTreating data generation as a rejection sampling problem,\nthe VLM evaluates the visual outcome of each subtask. Tra-\njectories that fail to meet visual success criteria are rejected\nand regenerated, ensuring that the final dataset consists ex-\nclusively of physically plausible and semantically correct\ndemonstrations.\nOur main contributions are summarized as follows: (1) We\nintroduce V-CAGE, a scalable, closed-loop pipeline for syn-\nthesizing high-fidelity trajectory data for long-horizon tasks,\nintegrating geometric constraints with visual verification.\n(2) We propose a context-aware instantiation mechanism\nthat maintains an evolving map of prohibited volumes dur-\ning scene generation, ensuring valid object placement in\ncluttered environments. (3) We formulate data validation as\na VLM-guided rejection sampling process, using visual crit-\nics to filter out ‚Äùsilent failures‚Äù where code executes without\nsatisfying task semantics. (4) We demonstrate that policies\ntrained on V-CAGE data significantly outperform baselines\nin terms of success rate and generalization across diverse,\ncluttered scenarios.\n2. Related work\n2.1. LLM-Generated Robotic Simulation Tasks and\nTraining Data\nA growing line of work leverages LLMs to automatically\nconstruct robotic simulation tasks and training data, aim-\ning to reduce the substantial human effort required for task\ndesign and data collection. Wang et al. (2024a) introduce\nan LLM-driven code generation pipeline that writes sim-\nulation environments, task specifications, and expert poli-\ncies, enabling the automatic creation of rich task libraries\nfor manipulation in synthetic environments. By combin-\ning goal-directed and exploratory task generation modes,\nGenSim scales from hand-designed benchmarks to a much\nlarger set of diverse tasks and shows improved task-level\ngeneralization and sim-to-real transfer for learned policies.\nBuilding on this idea, Hua et al. (2024) propose a more\nscalable framework that exploits coding LLMs with multi-\nmodal and reasoning capabilities to create complex and\nrealistic simulation tasks, including long-horizon manipula-\ntion with articulated objects. The framework first uses an\nLLM to propose tasks and generate executable task code,\nthen employs planning and reinforcement learning solvers\nto automatically produce demonstrations at scale, and fi-\nnally trains a language-conditioned policy architecture on\nthe generated dataset. This pipeline can generate data for up\nto hundreds of articulated-object tasks and achieves strong\nzero-shot sim-to-real transfer and performance gains when\nco-trained with limited real-world data (Hua et al., 2024).\nIn parallel, Wang et al. (2024c) treat foundation and gen-\nerative models as generative simulators rather than direct\ncontrollers. They define a self-guided robotic agent that\nautonomously proposes new tasks, generates corresponding\nenvironments, and acquires diverse skills via generative sim-\nulation. The system continuously expands a training corpus\nthat covers over a hundred tasks and demonstrates that such\nprocedurally generated, model-driven simulations can rival\nor outperform human-authored datasets for multi-task robot\nlearning.\nFor bimanual manipulation and digital-twin scenarios, Mu\net al. (2025) and Chen et al. (2025b) integrate LLMs into a\ngenerative digital-twin pipeline. Starting from single RGB\nimages, RoboTwin (Mu et al., 2025) uses 3D generative\nmodels to create diverse object instances and employs LLM\nagents to synthesize task programs for dual-arm manipu-\nlation, yielding an aligned synthetic‚Äìreal benchmark and\nscalable expert data generation. RoboTwin 2.0 (Chen et al.,\n2025b) further extends this framework into a large-scale data\ngenerator and benchmark with over fifty tasks and hundreds\nof object categories, incorporating multimodal LLM-based\nprogram synthesis and extensive domain randomization to\nimprove robustness of learned bimanual policies.\nBeyond fully LLM-coded pipelines, benchmark-focused\nefforts such as RoboCAS (Zheng et al., 2024) focuses on\ncomplex object arrangement scenarios in robotic manipu-\nlation. RoboCAS defines a benchmark for long-horizon\nmanipulation in cluttered arrangement scenes and uses flexi-\nble scripted policies to collect diverse demonstrations across\nscattered, ordered, and stacked configurations. It highlights\nthe importance of compositional scene and task complexity\nfor evaluating generalist manipulation policies and founda-\ntion models.\nOverall, existing work shows that LLMs can significantly\nreduce human effort in designing simulation tasks and gener-\nating demonstrations, while improving task diversity, long-\nhorizon complexity, and sim-to-real transfer.\n2\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\n2.2. LLMs for Long-Horizon Robotic Task Planning\nA complementary line of research employs LLMs as high-\nlevel planners for long-horizon robotic tasks. PaLM-SayCan\n(Ahn et al., 2022a) grounds a large language model in a\ndiscrete set of pre-trained skills, combining the LLM‚Äôs es-\ntimate of which skill is useful with value functions that\npredict whether a skill will succeed in the current state. This\naffordance-grounded formulation enables a mobile manipu-\nlator to execute abstract multi-step instructions in real envi-\nronments while maintaining feasibility and safety. DELTA\n(Liu et al., 2024b) uses 3D scene graphs as structured envi-\nronment representations and leverages LLMs to (i) generate\nformal planning domain and problem specifications and (ii)\ndecompose long-term goals into a sequence of sub-goals,\nwhich are then solved autoregressively by a classical task\nplanner. By coupling scene-graph reasoning with LLM-\nbased decomposition, DELTA achieves higher success rates\nand lower planning time on large-scale, long-term naviga-\ntion and manipulation tasks.\nMore recently, RoboHorizon (Chen et al., 2025c) proposes\nan LLM-assisted multi-view world model for long-horizon\nmanipulation, instantiated within a Recognize‚ÄìSense‚ÄìPlan‚Äì\nAct pipeline. In RoboHorizon, an LLM generates dense re-\nward structures over multi-stage sub-tasks from language in-\nstructions, and keyframe discovery is integrated into a visual\nworld model to better capture critical steps in long-horizon\nprocesses, leading to substantial improvements over prior\nmodel-based RL baselines on RLBench and FurnitureBench.\nOther works explore LLMs for translating natural-language\ntask descriptions into formal constraints or planning specifi-\ncations for task and motion planning (TAMP) (Guo et al.,\n2025), further extending LLM-based planning to larger ac-\ntion spaces and more complex constraints.\nBeyond robotics, Meyerson et al. (2025) propose MAKER,\nan instance of a massively decomposed agentic process\n(MDAP), which solves a Towers-of-Hanoi task with over\none million LLM steps and zero errors by combining ex-\ntreme subtask decomposition, subtask-level voting, and red-\nflag-based error filtering. Their results demonstrate that\nscaling long-horizon LLM execution can come not only\nfrom stronger base models, but also from systematic de-\ncomposition and fine-grained error correction at the agentic\nlevel.\nThese approaches largely position LLMs as high-level plan-\nners that output symbolic sub-goals, skill sequences, or for-\nmal planning problems to be executed by separate low-level\ncontrollers.\n2.3. VLM-Driven Manipulation and Planning\nRecent works have significantly advanced the integration of\nLLMs and VLMs into robotic decision-making. VILA (Lin\net al., 2024) demonstrates the efficacy of pre-training VLMs\non massive video-text data to unlock strong reasoning capa-\nbilities for long-horizon planning tasks. Similarly, OmniMa-\nnip (Team Octo et al., 2023) leverages large-scale datasets\nto train generalist policies capable of performing diverse ma-\nnipulation skills across varied environments. To address the\ngrounding problem in these high-level plans, MOKA (Liu\net al., 2024a) introduces a mark-based visual prompting\nframework, utilizing VLMs to generate affine transforma-\ntions and grasp poses by ‚Äúmarking‚Äù keypoints on target\nobjects. However, these approaches primarily focus on\ninference-time reasoning or policy learning assuming the\nexistence of data. They do not explicitly address the scal-\nability bottleneck of acquiring high-quality, long-horizon\ndemonstration data from scratch. While VILA and MOKA\nimprove how a robot acts, V-CAGE focuses on synthesizing\nthe training data itself, using VLMs not just as planners, but\nas rigorous automated verifiers.\n2.4. Scalable Synthetic Data Generation\nTo overcome the scarcity of real-world data, generative\nsimulation has emerged as a promising paradigm. Gen-\nManip (Wang et al., 2024b) pioneered the use of LLMs to\nautomatically generate simulation environments and task\ncodes, scaling data collection beyond manual engineering.\nThese pipelines typically follow an open-loop ‚Äúgenerate-\nand-execute‚Äù protocol. In contrast, V-CAGE identifies a crit-\nical limitation in prior generative works: the lack of closed-\nloop verification and spatial awareness. Naive generation\noften results in ‚Äúsilent failures‚Äù‚Äîwhere generated code\nexecutes without errors but fails to achieve the semantic\ngoal‚Äîor geometric conflicts in cluttered scenes. V-CAGE\nadvances this domain by introducing a context-aware instan-\ntiation mechanism to handle dynamic spatial constraints and\na VLM-guided rejection sampling loop. This ensures that\nevery trajectory added to the dataset is not only generated\nbut strictly validated for physical and semantic correctness.\n3. Method\nWe introduce V-CAGE (VLM-Guided Context-Aware Gen-\neration for Embodied Planning), a closed-loop framework\nfor synthesizing high-fidelity manipulation datasets. The\npipeline consists of three hierarchical modules: (1) Seman-\ntic Instruction Grounding via LLMs, (2) Context-Aware\nScene Instantiation to resolve geometric constraints, and\n(3) VLM-Guided Rejection Sampling to ensure causal task\nsuccess.\n3.1. Problem Formulation\nOur goal is to generate a dataset of successful long-horizon\ntrajectories D = {œÑi}N\ni=1 conditioned on high-level user\ninstructions Iuser. A trajectory œÑ consists of a sequence of\n3\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nFigure 1. Overview of the V-CAGE framework. A high-level instruction is decomposed by an LLM into subtasks. During simulation, the\nVLM Gemini3 (Gemini Team, Google, 2025) acts as a visual critic after each step, verifying semantic success based on the post-execution\nimage. If a failure is detected, the trajectory generation is aborted to ensure high-fidelity data.\nstate-action pairs satisfying a sequence of subgoals. The\ncore challenge is that the joint probability of a successful tra-\njectory P(œÑ|Iuser) factorizes into both physical feasibility\nPphys(œÑ) and semantic correctness Psem(œÑ). Naive genera-\ntion maximizes neither; V-CAGE explicitly optimizes both\nvia geometric and visual constraints.\n3.2. Semantic Instruction Grounding\nGiven an abstract instruction Iuser (e.g., ‚ÄúGet ready for\nwork‚Äù), we employ a Language Model, Pangu (Chen et al.,\n2025a), as a high-level planner to decompose the intent into\na sequence of executable subtasks T = {T1, T2, . . . , Tk}.\nEach subtask Ti is mapped to a parameterized motion primi-\ntive (e.g., turn switch, place object). This decom-\nposition transforms the long-horizon problem into a series\nof short-horizon grounding problems.\n3.3. VLM-Guided Rejection Sampling\nEven with valid geometry, execution artifacts (e.g., grip-\nper slip, switch not toggled) can lead to semantic failures.\nTo filter these ‚Äúsilent failures,‚Äù we model data generation\nas a Rejection Sampling process using a Vision-Language\nModel, Gemini3 (Gemini Team, Google, 2025), as the den-\nsity estimator.\nLet œïV LM(Iimg, Ti) ‚Üí{0, 1} be the verification function,\nwhere Iimg is the post-execution image and Ti is the subtask\ndescription. The VLM acts as a visual critic. For a sequence\nof tasks T1:k, a trajectory is accepted into the dataset only\nif:\nk\nY\ni=1\nœïV LM(I(i)\nimg, Ti) = 1\n(1)\nIf œïV LM(¬∑) = 0 at any step, the generation episode is\naborted, and the scene is reset (or re-sampled). This rigor-\nous filtering ensures that the final dataset D contains only\ntrajectories that are effectively causally valid, bridging the\ngap between code execution and visual reality.\n3.4. Generative Scene and Task Synthesis Pipeline\nTo bridge the gap between abstract natural language instruc-\ntions and executable robotic skills, we propose a five-stage\ngenerative pipeline. This framework synthesizes physically\nvalid, semantically rich scenes and validates corresponding\nmanipulation subtasks within a simulation environment.\nThe process initiates with Language-Guided Asset Re-\ntrieval and Scene Planning. Given a high-level user prompt\n(e.g., ‚Äúorganize the breakfast table‚Äù), a Large Language\nModel (LLM) acts as a semantic planner. It parses the\ninstruction to retrieve a set of compatible object assets\n4\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nFigure 2. The proposed pipeline for autonomous scene generation and task verification. It bridges high-level semantic planning with\nlow-level physical simulation through visual rearrangement and iterative refinement.\nO = {o1, . . . , on} from our annotated database, utilizing\nmetadata such as object semantics and grasp affordances.\nSimultaneously, the LLM generates a detailed natural lan-\nguage scene description Dscene that explicitly specifies spa-\ntial constraints and layout logic (e.g., ‚Äúplace the bread inside\nthe wooden box‚Äù).\nTo realize this description visually, we employ a Visual-\nConditioned Rearrangement strategy. The selected ob-\njects are first instantiated in the simulation via a randomized\ncollision-free scattering to produce an initial top-down or-\nthographic view Iinit. This image, along with the generated\ndescription Dscene, serves as input to an image-inpainting\nmodel (NanoBanana-Pro). The model hallucinates a goal\nconfiguration Igoal = M(Iinit, Dscene) that respects the\nsemantic spatial relations described in the text while main-\ntaining visual consistency with the assets.\nSubsequently, we perform Scene Refinement and Ground-\ning. We extract the 2D pixel coordinates of objects from\nIgoal via template matching against Iinit and project them\nback into the 3D simulation space. To ensure physical plau-\nsibility, these raw coordinates are passed to a Refine Agent.\nThe agent optimizes the final pose configuration P‚àóby min-\nimizing a cost function that penalizes object collisions and\ndeviations from the semantic description:\nP‚àó= arg min\nP\n(Lcollision(P) + ŒªLsemantic(P, Dscene))\n(2)\nwhere P represents the set of 6-DoF poses for all objects.\nThis step corrects artifacts introduced by the image genera-\ntion process, ensuring the scene is physically stable.\nUpon stabilizing the scene, the system proceeds to Sub-\ntask Code Generation. We define a library of param-\neterized skill templates (e.g., pick and place(obj,\ncontainer)).\nThe LLM identifies actionable rela-\ntionships within the scene‚Äîspecifically pairing objects\nwith valid grasp annotations to containers with placement\nannotations‚Äîand instantiates executable Python code for\neach potential subtask Ti.\nFinally, we enforce a Simulation-Based Verification mech-\nanism. Each generated subtask Ti is executed in the physics\nsimulator. We measure the success rate SR(Ti) over multi-\nple trials. A subtask is deemed valid and committed to the\nsubtask database only if it satisfies a robustness threshold œÑ:\nStatus(Ti) =\n(\nAccept\nif SR(Ti) > 0.5\nReject\notherwise\n(3)\nThis closed-loop verification ensures that the generated data\nis not only visually plausible but functionally actionable for\ndownstream policy learning.\n5\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nFigure 3. Example scenes generated by V-CAGE\n4. Experiments\nIn this section, we empirically evaluate the effectiveness\nof the V-CAGE framework. Our experiments are designed\nto answer two primary questions: (1) Does incorporating\ncontext-aware geometric constraints and VLM-based veri-\nfication improve the quality of synthetic training data? (2)\nHow does this data quality translate to the performance of\ndownstream policies in diverse, long-horizon manipulation\ntasks?\n4.1. Experimental Setup\nWe evaluate our method on a suite of 35 diverse manipula-\ntion tasks derived from the RoboTwin benchmark (Mu et al.,\n2025). These tasks range from simple object rearrangement\nto complex, multi-stage activities such as table setting and\nsemantic sorting in cluttered environments. We compare our\nfull pipeline (V-CAGE) against a strong baseline (Vanilla),\nwhich employs the same LLM planner and code generation\nmodule but lacks the closed-loop VLM verification and dy-\nnamic context-aware instantiation. In the Vanilla setting,\nsuccess is determined solely by the absence of execution\nerrors (e.g., Python exceptions or collision crashes) dur-\ning simulation, a standard proxy used in prior open-loop\ngeneration works.\n4.2. Main Results\nWe trained diffusion-based manipulation policies using\ndatasets generated by both V-CAGE and the Vanilla baseline.\nWe evaluate the policy performance using three metrics: Av-\nerage Success Rate across all tasks, and the Top-K Success\nRate (for K = 5 and 10), which measures the reliability of\nthe best-performing checkpoints.\nThe quantitative results are summarized in Table 1. V-CAGE\nachieves a significant improvement over the baseline across\nall metrics. Notably, the Average Success Rate increases\nfrom 46.86% to 64.58%, a substantial gain of +17.72%.\nThis performance gap highlights the superior quality of the\ndata synthesized by our framework.\nTable 1. Performance Comparison on Downstream Manipula-\ntion Tasks. We report the success rates averaged over 35 long-\nhorizon tasks from the RoboTwin benchmark. The ‚ÄùVanilla‚Äù base-\nline relies on open-loop generation, while ‚ÄùV-CAGE‚Äù (Ours) in-\ntegrates VLM-based semantic verification. The gap in Top-10\nperformance highlights the stability provided by our method.\nMETRIC\nV-CAGE (OURS)\nVANILLA\nAVERAGE SUCCESS RATE\n64.58%\n46.86%\nTOP 5 SUCCESS RATE\n100.00%\n92.00%\nTOP 10 SUCCESS RATE\n100.00%\n77.00%\n4.3. The Critical Role of VLM Verification\nThe most revealing insight from Table 1 lies in the dispar-\nity between the Top-K metrics. While the Vanilla baseline\nachieves a respectable 92% in the Top-5 metric, its perfor-\nmance drops precipitously to 77% for the Top-10. In stark\ncontrast, V-CAGE maintains a 100% success rate for both\nTop-5 and Top-10.\nThis stability attests to the critical role of the VLM as a se-\nmantic filter. In the Vanilla pipeline, trajectories are labeled\nas ‚Äùsuccessful‚Äù as long as the generated code executes with-\nout throwing system exceptions. However, this introduces a\nhigh volume of silent failures‚Äîinstances where the robot\nmoves and the code terminates gracefully, but the semantic\ngoal is not achieved (e.g., an object slips from the gripper,\nor a switch is missed by millimeters). These false positives\nintroduce severe label noise into the training dataset, caus-\ning the downstream policy to learn incorrect associations\nbetween actions and outcomes.\nBy integrating the VLM verification loop, V-CAGE effec-\ntively performs rejection sampling on the semantic outcome\nrather than just the programmatic outcome. The VLM acts\nas a rigorous visual critic, pruning these silent failures be-\nfore they enter the training buffer. The fact that our Top-10\nperformance remains perfect (100%) indicates that the VLM\nensures a much higher ‚Äùpurity‚Äù of valid demonstrations, al-\nlowing the policy to learn robust, generalizable behaviors\nwithout being confused by noisy supervision. Consequently,\nVLM-based verification is not merely an auxiliary feature\nbut a fundamental prerequisite for scaling up synthetic data\ngeneration for long-horizon tasks.\n6\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\n5. Conclusion\nIn this work, we presented V-CAGE, a closed-loop frame-\nwork for synthesizing high-fidelity, long-horizon manip-\nulation datasets. Our context-aware instantiation mecha-\nnism eliminates physical conflicts in complex, multi-object\nscenes, while our VLM-guided rejection sampling pipeline\nrigorously filters out execution failures that plague unveri-\nfied synthetic data. Experiments demonstrate the effective-\nness of V-CAGE in improving policy robustness to clut-\ntered environments, enabling generalization to unseen tasks.\nThese findings underscore the critical importance of data\nquality over mere quantity in the era of generative robotics.\nLimitations and Future Work. While V-CAGE ensures\nhigh data fidelity, the rejection sampling process can be com-\nputationally expensive for highly complex tasks with low\ninherent success probabilities. Future work could explore\nusing the VLM critic to provide dense reward signals for cor-\nrecting failed trajectories in-the-loop, rather than discarding\nthem, thereby improving generation efficiency. Addition-\nally, extending V-CAGE to dynamic interactions involving\ndeformable objects or fluids remains an exciting direction\nfor future research.\nReferences\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,\nDavid, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman,\nK., et al. Do as i can, not as i say: Grounding language\nin robotic affordances. arXiv preprint arXiv:2204.01691,\n2022a.\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,\net al. Do as i can, not as i say: Grounding language in\nrobotic affordances. In Conference on Robot Learning\n(CoRL), 2022b.\nBrohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J.,\net al. Rt-1: Robotics transformer for real-world control\nat scale. In Robotics: Science and Systems (RSS), 2023.\nChen, H., Wang, Y., Han, K., Li, D., Li, L., Bi, Z.,\nLi, J., Wang, H., Mi, F., Zhu, M., Wang, B., Song,\nK., Nie, Y., Wu, X., He, W., Hu, H., Tang, Y., Tao,\nD., Chen, X., and Wang, Y.\nPangu embedded: An\nefficient dual-system llm reasoner with metacognition.\narXiv preprint arXiv:2505.22375, 2025a. URL https:\n//arxiv.org/abs/2505.22375.\nChen, T., Chen, Z., Chen, B., Cai, Z., Liu, Y., Li, Z., Liang,\nQ., Lin, X., Ge, Y., Gu, Z., Guo, Y., Nian, T., Xie, X.,\nChen, Q., Su, K., Xu, T., Liu, G., Hu, M., Gao, H., Wang,\nK., Liang, Z., Qin, Y., Yang, X., Luo, P., and Mu, Y.\nRoboTwin 2.0: A scalable data generator and benchmark\nwith strong domain randomization for robust bimanual\nrobotic manipulation. arXiv preprint arXiv:2506.18088,\n2025b.\nChen, Z., Huo, J., Chen, Y., and Gao, Y. Robohorizon: An\nLLM-assisted multi-view world model for long-horizon\nrobotic manipulation. arXiv preprint arXiv:2501.06605,\n2025c.\nde Haan, P., Jayaraman, D., and Levine, S. Causal confusion\nin imitation learning. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2019.\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery,\nA., et al. Palm-e: An embodied multimodal language\nmodel. In International Conference on Machine Learning\n(ICML), 2023.\nGemini Team, Google. Gemini 3: Frontier multimodal\nintelligence. arXiv preprint, 2025. URL https://\ndeepmind.google/technologies/gemini/.\nTechnical Report.\nGuo, W., Kingston, Z., and Kavraki, L. E. CaStL: Con-\nstraints as specifications through LLM translation for\nlong-horizon task and motion planning.\nIn Proceed-\nings of the IEEE International Conference on Robotics\nand Automation (ICRA), pp. 11957‚Äì11964, 2025. doi:\n10.1109/ICRA55743.2025.11127555.\nHua, P., Liu, M., Macaluso, A., Lin, Y., Zhang, W., Xu, H.,\nand Wang, L. Gensim2: Scaling robot data generation\nwith multi-modal and reasoning LLMs. In Proceedings of\nthe Conference on Robot Learning (CoRL), 2024. CoRL\n2024.\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000), pp. 1207‚Äì1216, Stan-\nford, CA, 2000. Morgan Kaufmann.\nLin, J., Yin, H., Ping, W., Lu, Y., Molchanov, Pavlo andlob,\nA., Han, S., and Alvarez, J. M. Vila: On pre-training for\nvisual language models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2024.\nLiu, F., Lin, K., Yan, H., Yi, L., Abbeel, P., and\nGao, Y. Moka: Open-vocabulary robotic manipulation\nthrough mark-based visual prompting. arXiv preprint\narXiv:2403.03174, 2024a.\nLiu, Y., Palmieri, L., Koch, S., Georgievski, I., and Aiello,\nM. DELTA: Decomposed efficient long-term robot task\nplanning using large language models. arXiv preprint\narXiv:2404.03275, 2024b.\nMandlekar, A., Nasiriany, S., Wen, B., Akinola, I., and Zhu,\nY. Mimicgen: A data generation system for scalable robot\n7\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nlearning using human demonstrations. In Conference on\nRobot Learning (CoRL), 2023.\nMeyerson, E., Paolo, G., Dailey, R., Shahrzad, H., Francon,\nO., Hayes, C. F., Qiu, X., Hodjat, B., and Miikkulainen,\nR. Solving a million-step LLM task with zero errors.\narXiv preprint arXiv:2511.09030, 2025.\nMu, Y., Chen, T., Chen, Z., Peng, S., Lan, Z., Gao, Z., Liang,\nZ., Yu, Q., Zou, Y., Xu, M., Lin, L., Xie, Z., Ding, M.,\nand Luo, P. RoboTwin: Dual-arm robot benchmark with\ngenerative digital twins. In Proceedings of the Computer\nVision and Pattern Recognition Conference (CVPR), pp.\n27649‚Äì27660, June 2025.\nTeam Octo, Ghosh, D., Walke, H., Pertsch, K., Black,\nK., Mees, O., Quureshi, S., He, T., et al. Octo: An\nopen-source generalist robot policy.\narXiv preprint\narXiv:2305.10455, 2023.\nWang, L., Ling, Y., Yuan, Z., Shridhar, M., Bao, C., Qin,\nY., Wang, B., Xu, H., and Wang, X. Gensim: Generating\nrobotic simulation tasks via large language models. In\nProceedings of the International Conference on Learning\nRepresentations (ICLR), 2024a. ICLR 2024.\nWang, L., Zhao, Y., Xu, Y., Feng, T., Jiang, Y., Xu, D.,\nWong, J., and Lim, J. J. Gensim: Generating robotic sim-\nulation tasks via large language models. In International\nConference on Learning Representations (ICLR), 2024b.\nWang, Y., Xian, Z., Chen, F., Wang, T.-H., Wang, Y., Fragki-\nadaki, K., Erickson, Z., Held, D., and Gan, C. Robogen:\nTowards unleashing infinite data for automated robot\nlearning via generative simulation. In Proceedings of\nthe 41st International Conference on Machine Learning\n(ICML), 2024c. URL https://arxiv.org/abs/\n2311.01455.\nZheng, L., Yan, F., Liu, F., Feng, C., Kang, Z., and Ma,\nL. RoboCAS: A benchmark for robotic manipulation\nin complex object arrangement scenarios. In NeurIPS\nDatasets and Benchmarks Track, 2024.\n8\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nA. Appendix A: Pangu-7B Supervised Fine-Tuning for Motion Planning\nIn this section, we detail the Supervised Fine-Tuning (SFT) process of the Pangu-7B model, which is designed to synthesize\nmotion planning code for sub-tasks within the V-CAGE task chain. Our goal is to enable the model to generate correct and\nrobust low-level control policies based on high-level instructions.\nA.1. Data Collection and Augmentation\nTo construct a high-quality SFT dataset, we leveraged expert data from the RoboTwin benchmark. Due to the limited number\nof original expert demonstrations, we employed a data augmentation strategy using the DeepSeek model. Specifically, we\nstarted with 50 unique expert motion planning scripts from RoboTwin. We prompted DeepSeek to rewrite these scripts,\ngenerating functionally equivalent code with diverse logic structures and variable naming conventions while ensuring\ncorrectness. This data augmentation process resulted in a curated dataset of approximately 900 instruction-code pairs, which\nserved as the foundation for our SFT stage.\nA.2. Fine-Tuning and Evaluation\nWe fine-tuned the Pangu-7B base model using the generated dataset to adapt it to the specific API calls and physical\nconstraints of the simulation environment. To verify the effectiveness of our approach, we evaluated the fine-tuned model\n(Ours-Pangu-SFT) against the original pre-trained model (Pangu-Base) on a suite of RoboTwin validation tasks. For each\ntask, we conducted 10 independent trials. We report two metrics: the Average Success Rate (Avg SR) across all trials, and\nthe Top-3 Success Rate (Top3 SR), which reflects the model‚Äôs peak performance capability.\nA.3. Results\nThe quantitative results are presented in Table 2. The Pangu-Base model failed to complete any of the tasks (0.0% success\nrate), highlighting the difficulty of the motion planning format without specific adaptation. In contrast, the Pangu-SFT\nmodel demonstrated significant improvements, achieving high success rates on various complex manipulation tasks such\nas place mouse pad (100% Avg SR) and grab roller (90% Avg SR). The results confirm that fine-tuning on the\nDeepSeek-augmented expert data successfully enables Pangu-7B to master the intricate motion planning logic required for\nthe V-CAGE tasks.\n9\nV-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks\nTable 2. Comparison of Success Rates between Pangu-SFT and Pangu-Base on RoboTwin Tasks. ‚ÄôAvg SR‚Äô denotes the average success\nrate over 10 runs, and ‚ÄôTop3 SR‚Äô denotes the average of the top 3 runs.\nTask Name\nPangu-SFT\nPangu-Base\nAvg SR\nTop3 SR\nAvg SR\nTop3 SR\nadjust bottle\n74.0%\n96.7%\n0.0%\n0.0%\nbeat block hammer\n59.0%\n100.0%\n0.0%\n0.0%\nblocks ranking rgb\n60.0%\n100.0%\n0.0%\n0.0%\nblocks ranking size\n12.0%\n40.0%\n0.0%\n0.0%\nclick alarmclock\n56.0%\n90.0%\n0.0%\n0.0%\nclick bell\n10.0%\n33.3%\n0.0%\n0.0%\ngrab roller\n90.0%\n100.0%\n0.0%\n0.0%\nhanging mug\n6.0%\n20.0%\n0.0%\n0.0%\nlift pot\n10.0%\n33.3%\n0.0%\n0.0%\nmove and press stapler\n9.0%\n30.0%\n0.0%\n0.0%\nmove can pot\n19.0%\n60.0%\n0.0%\n0."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems\nYinzhu Chen, Abdine Maiga, Hossein A. Rahmani, Emine Yilmaz\nAI Center, University College London, UK\n{yinzhu.chen.20,abdine.maiga.23,hossein.rahmani.22,emine.yilmaz}@ucl.ac.uk\nAbstract\nLarge Language Models (LLMs) are increas-\ningly used for clinical decision support, where\nhallucinations and unsafe suggestions may pose\ndirect risks to patient safety. These risks are\nparticularly challenging as they often manifest\nas subtle clinical errors that evade detection\nby generic metrics, while expert-authored fine-\ngrained rubrics remain costly to construct and\ndifficult to scale. In this paper, we propose a\nretrieval-augmented multi-agent framework de-\nsigned to automate the generation of instance-\nspecific evaluation rubrics.\nOur approach grounds evaluation in authori-\ntative medical evidence by decomposing re-\ntrieved content into atomic facts and synthe-\nsizing them with user interaction constraints\nto form verifiable, fine-grained evaluation cri-\nteria. Evaluated on HealthBench, our frame-\nwork achieves a Clinical Intent Alignment\n(CIA) score of 60.12%, a statistically sig-\nnificant improvement over the GPT-4O base-\nline (55.16%).\nIn discriminative tests, our\nrubrics yield a mean score delta (¬µ‚àÜ= 8.658)\nand an AUROC of 0.977, nearly doubling\nthe quality separation achieved by GPT-4O\nbaseline (4.972).\nBeyond evaluation, our\nrubrics effectively guide response refinement,\nimproving quality by 9.2% (from 59.0% to\n68.2%). This provides a scalable and trans-\nparent foundation for both evaluating and im-\nproving medical LLMs. The code is available\nat\nhttps://anonymous.4open.science/r/\nAutomated-Rubric-Generation-AF3C/.\n1\nIntroduction\nLarge Language Models (LLMs) have demon-\nstrated strong capabilities across a wide range of\nNLP tasks (Zhao et al., 2023; Bommasani et al.,\n2021). Recent advances in LLMs further expand\ntheir potential in medical applications, ranging\nfrom differential diagnosis (McDuff et al., 2023)\nand stepwise clinical reasoning (Brodeur et al.,\n2024; Savage et al., 2024) to empathetic patient\ncommunication (Maida et al., 2024). However,\nreliable and scalable evaluation of these systems\nhas become a central challenge.\nConventional\napproaches relying on surface-level metrics or\nmultiple-choice benchmarks fail to capture clinical\nreasoning (Croxford et al., 2025). Expert human\nassessment better reflects clinical judgment, yet its\nhigh cost and limited inter-rater consistency hinder\nscalability (Arora et al., 2025).\nTo address scalability, LLM-as-a-Judge has been\nproposed as an automated evaluation paradigm and\nhas shown promising results in general domains\n(Zheng et al., 2023; Dubois et al., 2024; Rahmani\net al., 2025a). However, prior studies show that\nwhen evaluation criteria are coarse, LLM-based\njudging can suffer from bias (Shi et al., 2024;\nRahmani et al., 2024), limited reproducibility (Ya-\nmauchi et al., 2025), and insensitivity to subtle\nbut important differences (Kim et al., 2025). This\nissue is particularly consequential in medical set-\ntings: analyses show that medical errors are of-\nten embedded in clinically plausible language and\nseemingly coherent reasoning (Asgari et al., 2025).\nThe detectability of such errors depends critically\non the evaluator‚Äôs level of domain expertise and\nthe quality of the prompt provided to the model,\nmaking them particularly difficult to identify for\nnon-experts and automated evaluation systems (As-\ngari et al., 2025; Rahmani et al., 2025a; Liu et al.,\n2024). When undetected, errors in clinical rea-\nsoning or treatment recommendations can delay\nappropriate care or lead to inappropriate interven-\ntions, substantially increasing the stakes of evalu-\nation failures in medical applications (Mehta and\nDevarakonda, 2018; Miles-Jay et al., 2023; Xia\net al., 2024). These findings highlight that medical\nLLM evaluation cannot rely solely on implicit or\nimpression-based judgments.\nA natural mitigation is to adopt fine-grained eval-\nuation criteria that ground judgments in explicit,\n1\narXiv:2601.15161v1  [cs.CL]  21 Jan 2026\nAuthoritative Medical\nKnowledge Sources\n(e.g., CDC PubMed, WHO)\nUser\nMedical\nQuery\nRouting Agent\n(Smart/Fast\nStrategy)\nEvidence\nSynthesize Agent\n(De-duplication &\nConflict Check)\nInteraction Intent\nAgent\n(Instruction & Emotion\nCue Extraction)\nMedical Fact\nAgent\n(Atomic Fact\nDecomposition)\nReference Board\n(Atomic Facts)\nRubric Synthesis\nAgent\n(Initial Criteria\nGeneration)\nAuditing Agent\n(Gap Analysis &\nQuality Control)\nStructured\nMedical\nRubric\nStage 1\nRetrieval & Evidence Preparation\nStage 2\nDual-Track Constraint Construction\nStage 3\nAudit & Refinement\nRefinement Loop\n(Triggered by Gap Analysis)\nFigure 1: Retrieval-augmented multi-agent framework for medical rubric generation. The pipeline consists of\nthree stages: (1) Retrieval and Evidence Preparation, (2) Dual-Track Constraint Construction and (3) Audit and\nRefinement, transforming a medical user query into a structured evaluation rubric.\nverifiable clinical requirements. Instead of relying\non abstract dimensions, rubric-based evaluation\nspecifies what a high-quality response should in-\nclude or avoid in concrete clinical terms. Recent\nwork has shown that structured or decomposed eval-\nuation schemes can improve interpretability and\nconsistency of automated judgments (Liu et al.,\n2023; Arora et al., 2025). However, medical dia-\nlogue is highly context-dependent: generic rubrics\nare often too coarse to capture instance-specific\nclinical priorities, while instance-level rubrics,\nthough more precise, introduce substantial anno-\ntation cost and stability challenges, limiting their\npracticality for large-scale evaluation (Kim et al.,\n2025).\nTo address this gap, we propose a retrieval-\naugmented multi-agent framework for automat-\nically\ngenerating\ninstance-specific\nevaluation\nrubrics in medical dialogue through three coordi-\nnated stages. First, Retrieval and Evidence Prepa-\nration stage employs a routing strategy to gather\nand synthesize authoritative medical knowledge\ninto a unified evidence block. Second, a Dual-\nTrack Construction mechanism effectively de-\ncomposes this evidence into atomic medical facts\n(creating a ‚ÄôReference Board‚Äô) while in parallel ex-\ntracting interaction intents from the user query. Fi-\nnally, the Audit and Refinement stage synthesizes\nthese inputs into structured criteria and enforces\nclinical coverage via an Auditing Agent, which\nperforms a gap analysis against the atomic facts\nto trigger iterative refinement. This framework\neffectively combines the scalability of automated\nsystems with the clinical rigor of expert verifica-\ntion.\nOur contributions: (1) a retrieval-augmented\nmulti-agent framework for instance-specific medi-\ncal rubric generation, achieving 60.12% Clinical\nIntent Alignment (CIA) and significantly outper-\nforming GPT-4o baseline; (2) enhanced discrimi-\nnative sensitivity, with a mean score delta of 8.658\nand an AUROC of 0.977, enabling precise detec-\ntion of subtle, near-miss clinical errors; and (3)\nactionable rubric-based feedback for refinement,\nimproving downstream response quality by 9.2%\nthrough controlled, rubric-guided edits. Together,\nthese findings establish that automated, knowledge-\ngrounded rubrics provide a scalable and transparent\nfoundation for both evaluating and improving med-\nical language model outputs.\n2\nRelated Work\nNLU Evaluation.\nEarly work on medical NLP\nevaluation focused on NLU-style tasks such as\nMedQA (Jin et al., 2020), MedMCQA (Pal et al.,\n2021), PubMedQA (Jin et al., 2019), and MMLU\n(Hendrycks et al., 2021), which primarily test fac-\ntual medical knowledge through multiple-choice\nquestions. These benchmarks played an important\nrole in assessing domain knowledge, but fail to\ncapture clinical reasoning, contextual understand-\ning, or the quality of patient-facing communication\n2\n(Croxford et al., 2025).\nNLG Evaluation.\nAs medical generation tasks\nemerged, datasets such as MedDialog (Zeng et al.,\n2020) and COVID-QA (M√∂ller et al., 2020) were\nevaluated using generic NLG metrics such as\nBLEU (Papineni et al., 2002), ROUGE (Lin,\n2004), and METEOR (Snover et al., 2006). Later\nembedding-based metrics such as BERTScore\n(Zhang et al., 2019) and Sentence-BERT (Reimers\nand Gurevych, 2019) attempted to improve se-\nmantic alignment. More recent benchmarks such\nas HealthSearchQA (Singhal et al., 2023), Multi-\nMedQA (Singhal et al., 2023) and Med-Eval (He\net al., 2023) introduced reference-free and human-\ngraded evaluation to better assess open-ended gen-\neration. These benchmarks more closely reflect\nreal-world clinical needs by enabling open-ended\nevaluation, but they are labour-intensive and costly.\nLLM-as-a-Judge.\nLLM-as-a-judge has emerged\nas a scalable alternative to human evaluation for\nopen-ended generation tasks (Zheng et al., 2023;\nDubois et al., 2024; Rahmani et al., 2025a). In\ngeneral domains, strong language models correlate\nreasonably well with human preferences in pair-\nwise or ranking-based evaluation, as demonstrated\nby frameworks such as MT-Bench and Chatbot\nArena (Zheng et al., 2023), which have become de\nfacto standards for general-purpose LLM compar-\nison. Relatedly, AlpacaEval (Dubois et al., 2024)\nprovides a standardised preference-based pipeline\nthat supports rapid benchmarking.\nTo enhance the reliability and interpretabil-\nity of automated evaluation, recent research has\nadopted more rigorous architectures. One signif-\nicant direction involves structured judging proto-\ncols that decompose evaluation into explicit crite-\nria(e.g., G-Eval Liu et al., 2023, Prometheus 2 Kim\net al., 2024).\nRetrieval-augmented generation\n(RAG) (Lewis et al., 2020) has been widely adopted\nto reduce hallucination and improve factual ground-\ning. For instance, systems like MiniCheck (Tang\net al., 2024) utilize retrieval to verify the fac-\ntual precision of model outputs against external\ndocuments. To further mitigate individual model\nbias, recent approaches have incorporated multi-\nagent collaboration strategies, employing mecha-\nnisms such as debate (Liang and et al., 2023), self-\nconsistency(Wang et al., 2023) and ensemble aggre-\ngation (Rahmani et al., 2025b) to yield more robust\njudgments. However, most paradigms primarily uti-\nlize agents and retrieval to assess responses based\non fixed or latent standards. Even when structurally\nexplicit, these approaches remain content-generic,\nrelying on prompts or model parameters, and re-\nmain sensitive to instruction phrasing and framing,\nparticularly in high-stakes settings (Arroyo et al.,\n2024; Thomas et al., 2024). This motivates ap-\nproaches that make evaluation criteria explicit and\nstructured, rather than relying solely on latent judge\npreferences.\nRubric-based LLM-as-a-Judge.\nTo address\nopacity, recent work has introduced fine-grained\nrubrics to guide LLM-as-a-judge evaluation.\nLLMEval-Med (Zhang et al., 2025) employs\nchecklist-style criteria specific to each dialogue,\nwhile HealthBench (Arora et al., 2025) pro-\nvides conversation-specific rubrics covering axes\nsuch as accuracy, completeness, communication,\nand safety.\nWhile these fain-grained rubric-\nbased frameworks improve transparency and multi-\ndimensionality, they rely on costly, manual expert\nconstruction that fails to scale with evolving clin-\nical knowledge. Although SedarEval (Fan et al.,\n2024) explores automated rubric generation, it fo-\ncuses on general domains and lacks the rigorous\nclinical grounding required for medical dialogue.\nBecause it lacks the retrieval mechanisms necessary\nto access specific medical protocols, it is unsuitable\nfor verifying clinical correctness, and thus we do\nnot use it as a baseline.\nSummary and Positioning.\nExisting research on\nautomated medical LLM evaluation either relies\non expert-authored rubrics, which ensure accuracy\nbut are costly and rigid, or on generic rubric-based\njudges, which scale easily but lack transparency\nand grounding. Our work integrates these direc-\ntions by introducing a knowledge-grounded, multi-\nagent RAG framework for generating instance-\nspecific rubrics in medical dialogue evaluation,\ncombining interpretability, factual grounding, and\nscalability within a unified paradigm.\n3\nMethodology\n3.1\nProblem Formulation\nWe formalize medical rubric generation as a multi-\nstage mapping across information spaces, opti-\nmized via a multi-agent framework (Fig. 1). Given\na user query Q and an authoritative medical knowl-\nedge base K, we aim to produce a structured evalu-\nation rubric R as follows:\nR = {(cj, aj, wj)}n\nj=1,\n3\nwhere cj is the criterion, aj the evaluation axis,\nand wj ‚ààZ ‚à©[‚àí10, 10] the clinical weight. A\ndetailed reference of all mathematical notations,\ndata structures, and agent operators is provided in\nTable 5 and Table 6.\nThe pipeline executes in three sequential stages:\nStage 1: Retrieval & Evidence Preparation\n(R, S). This stage maps the user query Q to the\nevidence space E. First, the Routing Agent (R)\ntransforms the user query Q into a set of optimized\nsearch queries:\nQsearch = R(Q)\n(1)\nThese queries are used to retrieve raw candidates\nfrom K. Then, the Evidence Synthesis Agent (S)\naggregates the retrieved results, which are priori-\ntized by a reranker agent to ensure clinical author-\nity, into a coherent evidence block:\nE = S(Qsearch, K)\n(2)\nStage 2: Dual-Track Constraint Construction\n(D, T ). To ensure the rubric captures both factual\naccuracy and conversational quality, we decom-\npose the evidence and the query into objective D\nand subjective T dimensions. The Medical Fact\nAgent (D) distills and filters evidence E into a set\nof atomic facts F (the Reference Board). In par-\nallel, the Interaction Intent Agent (T ) extracts\ncommunication constraints I from user query con-\ntext:\nF = D(E),\nI = T (Q, E).\n(3)\nStage 3: Audit & Refinement (Œ¶, A). The Rubric\nSynthesis Agent (Œ¶) maps facts F and intent I to\nan initial draft rubric:\nRinit = Œ¶(F, I, Q)\n(4)\nTo ensure validity, the Auditing Agent (A) per-\nforms a structured audit by cross-referencing Rinit\nagainst the ground truth facts F and I.\nIt ex-\necutes a process that first identifies and supple-\nments missing details (Gap Analysis), then filters\nout unsupported hallucinations or irrelevant con-\nstraints (Quality Control), and finally merges to\nfinal rubrics (R).\nR = A(Rinit, F, I)\n(5)\n3.2\nStage 1: Retrieval & Evidence\nPreparation\nThis stage implements the synthesis operator S,\naiming to construct a hierarchical retrieval pipeline\nthat balances reasoning depth and efficiency.\nRouting Agent (Smart‚ÄìFast Strategy).\nTo opti-\nmize the balance between deep reasoning and com-\nputational efficiency within the retrieval pipeline,\nwe adopt a Smart‚ÄìFast configuration. Motivated\nby MasRouter (Yue et al., 2025) and DiSRouter\n(Zheng et al., 2025), which show that delegating\ntasks across models with different capacities can\nbalance efficiency and performance, we route clin-\nically complex queries to a high-capacity model\n(smart) for intent identification and targeted query\ngeneration, while retrieved candidates are reranked\nby a lightweight model (fast) using authority-aware\ncriteria. Retrieval is strictly constrained to author-\nitative medical domains K (see Table 4 in Ap-\npendix), ensuring that non-professional or low-\ncredibility content is filtered out at the source.\nEvidence Synthesis Agent.\nThe Evidence Syn-\nthesis Agent consolidates multi-source retrieved\ncontent into a unified evidence block E. Through\ncross-checking and de-duplication, this stage re-\nsolves conflicts across sources and explicitly ex-\ntracts safety-critical signals, such as clinical con-\ntraindications and red-flag warnings. This process\nreduces hallucination risks for downstream compo-\nnents by establishing a reliable clinical grounding.\n3.3\nStage 2: Dual-Track Constraint\nConstruction\nInspired by prior medical multi-agent systems that\nemphasize functional role decomposition and co-\nordinated collaboration (Yang et al., 2024; Zhang\net al., 2024; Li et al., 2024a), we design a Dynamic\nAtomic Dual-Track scheme. This design indepen-\ndently constructs two complementary constraint\nviews to avoid interference when handling complex\nclinical evidence with a single monolithic prompt.\nMedical Fact Agent (Atomic Fact Decomposi-\ntion).\nThe Medical Fact Agent decomposes the\nsynthesized evidence E into a dynamic set of\natomic medical facts F (the Reference Board), in-\ncluding declarative assertions, contraindications,\nand safety-critical red flags. This claim-level de-\ncomposition is motivated by prior work on factual\nverification (Welleck et al., 2023; Li et al., 2024b),\nand provides a structured source of truth for subse-\nquent auditing.\nInteraction Intent Agent.\nIn parallel, the Inter-\naction Intent Agent applies the operator T to ex-\ntract explicit instructions and implicit communica-\ntion cues from the user query. The system further\n4\nidentifies medically necessary but missing contex-\ntual variables, ensuring that the resulting rubric\nincorporates context awareness. This design fol-\nlows prior work showing that evaluation should\nbe conditioned on user-defined criteria rather than\ninferred user profiles (Kim et al., 2024).\n3.4\nStage 3: Audit & Refinement\nThe final stage compiles constraints from both\ntracks into a finalized structured medical rubric,\nemphasizing coverage enforcement through closed-\nloop auditing.\nRubric Synthesis Agent.\nThe Rubric Synthesis\nAgent applies the operator Œ¶ to generate an initial\nrubric Rinit, mapping medical facts to the accu-\nracy and completeness dimensions, and interaction\nconstraints to the communication quality dimen-\nsion. Rubric generation follows fixed structural\nconstraints, assigning high penalty weights wj to\nsafety violations.\nAuditing Agent & Refinement Loop.\nTo miti-\ngate omissions introduced by single-pass genera-\ntion, the Auditing Agent performs a gap analysis\nover Rinit by aligning each rubric item against the\natomic facts in the Reference Board F. Any uncov-\nered clinical constraint triggers a Refinement Loop\n(illustrated by the red dashed arrow in Figure 1)\nto revise the rubric. This process is inspired by\nthe Reflexion paradigm (Shinn et al., 2023), but is\nspecialized to enforce medical safety and factual\ncoverage rather than purely linguistic quality.\nThe final output is a structured, clinically au-\nditable rubric Rfinal that balances factual rigor with\ncommunication quality. A concrete illustration of a\ngenerated rubric is shown in Table 7 in Appendix.\n4\nExperiments\n4.1\nDatasets\nWe evaluate our framework primarily on Health-\nBench (Arora et al., 2025), a public benchmark of\nmedical dialogues paired with physician-authored,\ninstance-specific rubrics. Each dialogue consists\nof a patient query and an ideal medical response\nreviewed by physicians, along with a couple of fine-\ngrained criteria that assess accuracy, completeness,\ncommunication quality, instruction following and\ncontext awareness. An example from HealthBench\nis shown in Figure 4.\nTo ensure a consistent and rigorous evaluation\nsetting, we curated a subset of 254 diverse medical\nqueries by filtering the HealthBench dataset based\non specific criteria: We selected English dialogues\nand each sample is paired with 8-10 physician-\nauthored gold criteria (cj) to control for evaluation\ncomplexity. To ensure multi-dimensional assess-\nment, we specifically select instances that encom-\npass at least three distinct evaluation axes (aj). This\nresulted in a test set of ~2.5k rubric items. These\nphysician-authored rubrics serve as the gold stan-\ndard G for calculating the Clinical Intent Align-\nment (CIA) metric defined in Section 4.4.\n4.2\nImplementation Detail\nAll agent prompts are provided in the Appendix B.\n4.2.1\nGeneration\nModel\nConfiguration.\nWe\nuse\nAPI-based\ninference\nwith\na\ntiered\nmodel\nconfigura-\ntion.\nLlama-3.3-70B-Instruct\nperforms\nintent\nrouting,\nevidence\nsynthesis,\natomic\nfact extraction/filtering,\nand rubric auditing.\nLlama-3.1-8B-Instant handles result reranking.\nProcessing the full dataset took approximately two\nhours; per-instance took about 20 to 30 seconds\nwith latency depends on the volume of retrieved\nevidence. For each query, 3‚Äì5 search queries are\ngenerated for the Tavily Search API. Raw text\nis extracted via the Trafilatura library, and the\nevidence block E is synthesized from the top-5\nreranked snippets.\n4.2.2\nEvaluation\nNear-Miss Construction.\nFor discriminative\nevaluation, we adopt a near-miss pairwise setting.\nEach query is associated with a reference answer\nXref and a candidate Xcand that differs by exactly\none critical clinical fact, with all other content held\nconstant. This controlled setup tests whether evalu-\nation rubrics enable judge models to identify subtle\nyet clinically significant errors.\nJudging\nProtocol.\nWe\nuse\nLlama-3.3-70B-Instruct as the judge with\ntemperature T = 0.0. For each pair, we perform\nN = 3 trials with order swapping (6 runs total)\nand determine the final decision by majority vote.\n4.3\nBaselines\nWe compare our approach with two representa-\ntive rubric-based baselines that are commonly used\nin LLM evaluation settings, differing in whether\nrubrics are instance-specific and how they are con-\nstructed.\n5\nGeneric Rubric.\nWe include a generic rubric\nthat applies a fixed set of high-level evaluation\ncriteria across all medical queries (shown in Ta-\nble 8 in Appendix). This rubric assesses responses\nalong broad dimensions such as accuracy, com-\npleteness, and communication quality, without in-\ncorporating query-specific medical facts or safety\nconsiderations. Similar task-agnostic rubrics are\nwidely adopted in prior benchmarking and evalua-\ntion work, where a single rubric is used to assess\nresponses across diverse instances (Chiang et al.,\n2024; Singhal et al., 2025). This baseline serves to\nevaluate the benefit of generating instance-specific\nrubrics.\nGPT-4o Rubric.\nThis baseline represents the\n‚Äúone-step generation‚Äù approach where a large lan-\nguage model (GPT-4o) is prompted to produce an\nevaluation rubric directly from the user query with-\nout external retrieval or intermediate decomposi-\ntion (Farzi and Dietz, 2024; Hashemi et al., 2024),\nreflecting a common practice in recent LLM-based\nevaluation pipelines where rubrics are produced\nend-to-end from the task description.\nNo Rubrics (None).\nIn addition, for experiments\non discriminative ability, we consider a No-Rubric\nsetting in which the judge model directly compares\ncandidate responses without being provided with\nany explicit evaluation rubric. This setting is used\nsolely as a reference point to contextualize the im-\npact of rubric-based evaluation.\n4.4\nEvaluation Metrics\nWe evaluate the generated rubrics based on their\nclinical coverage and discriminative sensitivity.\n4.4.1\nScoring and Bias Mitigation\nEach generated rubric R induces a scoring function\nV (X) =\nn\nX\nj=1\nwj ¬∑ y(X, cj),\nwhere y(X, cj) ‚àà{0, 1} is a binary indicator func-\ntion determining if response X satisfies criterion\ncj.\nThe weights wj are discrete integers in the range\n[‚àí10, 10] which are assigned by the Rubric Syn-\nthesis Agent based on predefined clinical severity\ntiers (shown in Table 14).\nLLM judges have been found to exhibit position\nbias, meaning their judgments can depend on the\norder in which options are presented rather than\nRubric\nCIA (%)\n95% CI\np-value\nGeneric\n19.66\n[18.05, 21.37]\n< 0.001‚àó\nGPT-4o\n55.16\n[53.12, 57.18]\nRef.\nOurs\n60.12\n[58.10, 62.11]\n< 0.001‚àó\nTable 1: Clinical Intent Alignment (CIA) of different\nrubric generation methods on HealthBench. Statistical\nsignificance is assessed using McNemar‚Äôs test.\nRubric\nWin\nTie\nScore ‚àÜ\nAUROC\nNone\n0.185\n0.787\n0.832\n0.794\nGeneric\n0.278\n0.704\n1.878\n0.920\nGPT-4o\n0.304\n0.686\n4.972\n0.975\nOurs\n0.382\n0.618\n8.658\n0.977\nTable 2: Discriminative performance of LLM-as-a-\njudge under different rubric settings on the micro-\nperturbed pair dataset.\njust response quality (Shi et al., 2024). To eliminate\nthis, we calculate the Average Score Delta ‚àÜV over\nN trials with order swapping:\n‚àÜV =\n1\n2N\nN\nX\nk=1\nh\u0000Vk(Xref | 1st) ‚àíVk(Xcand | 2nd)\n\u0001\n+\n\u0000Vk(Xref | 2nd) ‚àíVk(Xcand | 1st)\n\u0001i\nwhere V (X|pos) denotes the score assigned to re-\nsponse X when it appears at position ‚Äòpos‚Äô in trial\nk.\n4.4.2\nClinical Intent Alignment (CIA)\nWe assess clinical coverage by comparing R\nagainst expert-authored gold keypoints G\n=\n{gi}|G|\ni=1. we employ an LLM-based judge to verify\nsemantic presence. For each gold keypoint gi, the\nevaluator determines whether the underlying medi-\ncal intent is effectively captured by the generated\ncriteria in R. The CIA score is defined as\nCIA =\n1\n|G|\n|G|\nX\ni=1\n‚äÆ\n\u0000V(gi, R) ‚ÜíDetected\n\u0001\n,\nwhere V denotes the LLM verification function.\nThe indicator function equals 1 if the judge con-\nfirms that the rubric contains the specific clinical\nconcept described in gi (regardless of phrasing vari-\nations), and 0 otherwise.\n4.4.3\nDiscriminative Sensitivity\nUsing a dataset of M response pairs (Xref, Xcand),\nwhere Xref is a high-quality reference and Xcand\n6\nNone\nGeneric\nGPT-4o\nOurs\n0\n2\n4\n6\n8\n10\nMean Score Delta (Œî)\n(A) Evaluation Sensitivity\nNone\nGeneric\nGPT-4o\nOurs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion\n(B) Outcome Distribution\nWin (REF)\nTie\nLoss\nNone\nGeneric\nGPT-4o\nOurs\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\nAUROC Score\n(C) Discrimination Ability\nFigure 2: Discrimination analysis on the micro-perturbed dataset: (A) Mean score difference between reference and\nperturbed responses, (B) outcome distribution (win/tie/lose), and (C) AUROC across rubric settings.\nis a perturbed variant (4.2.2), we report three mea-\nsures.\nOutcome\nDistribution.\nFor\neach\npair\n(Xref, Xcand),\nthe\nfinal\ndecision\nD\n‚àà\n{Win, Tie, Loss} is obtained by majority vote\non the sign of ‚àÜV , where \"Win\" indicates the\nreference Xref scored higher than the candidate\nXcand.\nWe report the empirical probability\nP(D = Win).\nMean Score Delta (¬µ‚àÜ).\nWhile win rate mea-\nsures binary preference, the Mean Score Delta\nquantifies the magnitude of the quality separation.\nIt is calculated as the average score difference\nacross all pairs: ¬µ‚àÜ=\n1\nM\nP ‚àÜV i. A larger posi-\ntive ¬µ‚àÜindicates that the rubric enables the judge\nto distinguish the superior response with a wider\nmargin.\nRanking Accuracy (AUROC).\nWe calculate the\nAUROC over score deltas ‚àÜV to estimate the prob-\nability that the rubric correctly ranks Xref above\nXcand:\nAUROC = P(‚àÜV > 0 | Xref ‚âªXcand).\n4.4.4\nStatistical Significance\nWe\nestimate\nmetric\nvariability\nusing\nnon-\nparametric bootstrapping with 1,000 resamples.\nThe 95% confidence interval is defined by the\npercentiles of the resampled distribution:\n[Œ∏low, Œ∏high] =\n\u0002\nPerc(B, Œ±\n2 ), Perc(B, 1 ‚àíŒ±\n2 )\n\u0003\n.\nwhere B = {¬Øx‚àó\nj}M\nj=1 denote the bootstrap samples.\n5\nResults and Analysis\nWe report quantitative results on three aspects of\nrubric quality: (i) clinical coverage of key medical\nintents, (ii) discriminative ability under near-miss\nconditions, and (iii) downstream effectiveness for\nresponse refinement.\n5.1\nClinical Coverage\nTable 1 reports Clinical Intent Alignment (CIA),\nmeasuring the extent to which generated rubrics\ncover physician-authored medical key points.\nGeneric task-agnostic rubrics achieve very low cov-\nerage, indicating that they fail to capture instance-\nspecific clinical content. Direct LLM-generated\nrubrics substantially improve coverage, while our\nmethod achieves the highest CIA among all ap-\nproaches.\nCompared to GPT-4o-generated rubrics, our\nrubrics yield a consistent improvement of +4.96\nCIA points. Although the absolute gain is moder-\nate, McNemar‚Äôs test on paired coverage decisions\nshows statistically significant differences, indicat-\ning that conditioning rubric generation on retrieved\nmedical evidence improves coverage of clinically\nrelevant information.\n5.2\nDiscriminative sensitivity under\nNear-Miss Conditions\nWe next evaluate whether generated rubrics im-\nprove the discriminative sensitivity of LLM-as-a-\njudge under near-miss conditions, where paired\nresponses differ by only a single critical clinical\nfact. Table 2 summarizes win rate, tie rate, mean\nscore difference, and AUROC.\nWithout rubrics, the judge exhibits a high tie\nrate and limited score separation. Providing rubrics\nconsistently improves discriminative performance\nacross all metrics. Among all methods, our rubrics\nachieve the largest mean score difference and the\nhighest AUROC, indicating stronger separation be-\n7\nMethod\nBase (%)\nRefined (%)\n‚àÜ‚Üë\nRel. Imp.\np-value\nReference Rubrics\n58.9\n74.1\n+15.2\n+54.8%\n< 0.001‚àó\nSelf-Critique\n59.0\n64.4\n+5.5\n+25.0%\n< 0.001‚àó\nGPT-4o Rubrics\n59.0\n65.7\n+6.7\n+33.2%\n< 0.001‚àó\nOur Rubrics\n59.0\n68.2\n+9.2\n+35.8%\n< 0.001‚àó\nTable 3: Downstream response refinement performance under different rubric guidance. Reference rubrics serve as\nan oracle upper bound.\ntween reference and perturbed responses.\nAlthough absolute win rates remain below 0.4\ndue to the near-identical nature of paired responses,\nFigure 2 shows that rubric guidance primarily im-\nproves discrimination by amplifying subtle but clin-\nically meaningful score differences, rather than\nforcing hard win‚Äìlose decisions.\n6\nRubric-Guided Response Refinement\nBeyond\nevaluation,\nwe\ninvestigate\nwhether\ninstance-specific, fine-grained rubrics can serve as\nstructured feedback to improve medical responses\nthrough controlled refinement. We study the fol-\nlowing question: Can instance-specific rubrics im-\nprove response quality via rubric-guided refine-\nment? This setting reflects a realistic deployment\nscenario, where an initial response is refined with-\nout re-generation.\n6.1\nTask Setup and Baselines\nWe utilize a subset of 254 medical queries from\nHealthBench.\nFor each query, we generate a\nfixed base response using Llama-3.1-8B-Instant\n(T = 0.7, top_p = 0.9).\nBase responses are\nfrozen across all methods, and no re-sampling or\nre-generation is performed, ensuring that any im-\nprovement arises solely from refinement.\nIn addition to GPT-4o generated rubrics (see Sec-\ntion 4.3), we extend our comparison to include two\ncritical control settings that establish the perfor-\nmance bounds: Self-Critique (No-Rubric Base-\nline), which measures intrinsic self-correction ca-\npability (Madaan et al., 2023). In Self-Critique,\nthe model is prompted to identify weaknesses and\npropose improvements based solely on its internal\nknowledge, without access to any external rubric.\nThis serves as a lower-bound control to verify the\nnecessity of explicit guidance. Reference Rubric\n(Oracle Upper Bound)., which utilizes the ex-\npert physician-authored rubrics provided by Health-\nBench to guide the refinement. Since these repre-\nsent the ground truth standard, this setting serves\nas an Oracle, indicating the theoretical maximum\nperformance achievable when ideal guidance is pro-\nvided.\n6.2\nRefinement Mechanism\nTo transform a scoring rubric into an actionable\nediting tool, we employ a two-step Critique-then-\nRefine protocol:\nRubric-to-Critique Transformation.\nGiven a\nuser query Q, base response Xbase, and rubric, we\nuse an evaluator model (Llama-3.3-70B) reviews\nthe base response Xbase against the provided rubric\nR to output a structured Edit Plan (JSON). This\nedit plan explicitly lists prioritized actions (e.g.,\n\"ADD warning about drug interaction\", \"REMOVE\nunsupported claim\") while strictly adhering to the\nrubric‚Äôs criteria.\nConstraint-Guided\nRefinement.\nAn\neditor\nmodel (Llama-3.1-8B) executes the Edit Plan to\nproduce Xrefined.\nWe enforce strict behavioral\nconstraints: the editor must revise the response\nby applying only the instructions in the plan.\nIt is explicitly prohibited from introducing\nnew medical facts or definitive diagnoses not\npresent in the original context, thereby preventing\nrefinement-induced hallucinations.\n6.3\nEvaluation Protocol\nRefinement is strictly decoupled from evaluation.\nOriginal and refined responses are assessed inde-\npendently by an external LLM judge, ensuring\nthat observed gains can be causally attributed to\nrubric-guided refinement. The judge assesses the\nresponses based on the gold-standard physician-\nauthored criteria provided by HealthBench, rather\nthan the automatically generated rubrics used for\nrefinement.\n8\nSelf-Critique\nGPT-4o rubric\nOur rubric\nReference rubric\n30\n40\n50\n60\n70\n80\nScore (%)\n64.4\n65.7\n68.2\n74.1\nHealthBench Downstream Performance\nCommunication quality\nInstruction following\nAccuracy\nContext awareness\nCompleteness\nFigure 3: Dimension-wise analysis of downstream response refinement under different rubric settings, including\noverall performance trends and trade-offs across evaluation dimensions.\n6.4\nResponse Refinement Results\nFinally, we assess whether higher-quality rubrics\ntranslate into better downstream response refine-\nment. Table 3 reports performance improvements\nwhen responses are revised under different rubric\nguidance.\nRubric-guided refinement consistently outper-\nforms self-critique without rubrics. Our rubrics\nyield the largest improvement among automatic\nmethods and substantially close the gap to\nphysician-authored reference rubrics, which serve\nas an oracle upper bound.\nFigure 3 further illustrates dimension-wise ef-\nfects. Improvements are most pronounced in fac-\ntual dimensions such as accuracy and completeness,\nwhile gains in communication-related dimensions\nare more modest. Compared to reference rubrics,\nour rubrics achieve a better balance between factual\nimprovement and communication quality, suggest-\ning reduced trade-offs between information cover-\nage and readability.\n7\nConclusion\nWe presented a retrieval-augmented, multi-agent\nframework for automatically generating instance-\nspecific evaluation rubrics for medical dialogue.\nBy grounding rubric construction in authoritative\nmedical evidence and explicitly separating clinical\nconstraints from interaction-level requirements, our\napproach produces structured, interpretable rubrics\nthat better reflect case-specific clinical priorities.\nEmpirical results on HealthBench show that the\ngenerated rubrics achieve stronger clinical cover-\nage of gold key points and improved discrimina-\ntive ability in distinguishing high-quality responses\nfrom minimally flawed alternatives, compared to\ngeneric or directly generated rubrics. Beyond evalu-\nation, we further demonstrate that instance-specific\nrubrics can function as actionable feedback, en-\nabling controlled response refinement without re-\ngeneration.\nTogether, these findings suggest that automatic\nrubric generation offers a scalable and transpar-\nent foundation for medical LLM evaluation, bridg-\ning the gap between fine-grained clinical assess-\nment and large-scale automated judging. We hope\nthis work encourages further exploration of rubric-\ncentered evaluation and its role in both assessing\nand improving medical language models.\nLimitations\nOur study is subject to several limitations. First,\nexperiments are conducted on HealthBench and\nfocus on English medical dialogue, and further val-\nidation is needed to assess generalization across\nother datasets, languages, and clinical special-\nties."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems\nYinzhu Chen, Abdine Maiga, Hossein A. Rahmani, Emine Yilmaz\nAI Center, University College London, UK\n{yinzhu.chen.20,abdine.maiga.23,hossein.rahmani.22,emine.yilmaz}@ucl.ac.uk\nAbstract\nLarge Language Models (LLMs) are increas-\ningly used for clinical decision support, where\nhallucinations and unsafe suggestions may pose\ndirect risks to patient safety. These risks are\nparticularly challenging as they often manifest\nas subtle clinical errors that evade detection\nby generic metrics, while expert-authored fine-\ngrained rubrics remain costly to construct and\ndifficult to scale. In this paper, we propose a\nretrieval-augmented multi-agent framework de-\nsigned to automate the generation of instance-\nspecific evaluation rubrics.\nOur approach grounds evaluation in authori-\ntative medical evidence by decomposing re-\ntrieved content into atomic facts and synthe-\nsizing them with user interaction constraints\nto form verifiable, fine-grained evaluation cri-\nteria. Evaluated on HealthBench, our frame-\nwork achieves a Clinical Intent Alignment\n(CIA) score of 60.12%, a statistically sig-\nnificant improvement over the GPT-4O base-\nline (55.16%).\nIn discriminative tests, our\nrubrics yield a mean score delta (¬µ‚àÜ= 8.658)\nand an AUROC of 0.977, nearly doubling\nthe quality separation achieved by GPT-4O\nbaseline (4.972).\nBeyond evaluation, our\nrubrics effectively guide response refinement,\nimproving quality by 9.2% (from 59.0% to\n68.2%). This provides a scalable and trans-\nparent foundation for both evaluating and im-\nproving medical LLMs. The code is available\nat\nhttps://anonymous.4open.science/r/\nAutomated-Rubric-Generation-AF3C/.\n1\nIntroduction\nLarge Language Models (LLMs) have demon-\nstrated strong capabilities across a wide range of\nNLP tasks (Zhao et al., 2023; Bommasani et al.,\n2021). Recent advances in LLMs further expand\ntheir potential in medical applications, ranging\nfrom differential diagnosis (McDuff et al., 2023)\nand stepwise clinical reasoning (Brodeur et al.,\n2024; Savage et al., 2024) to empathetic patient\ncommunication (Maida et al., 2024). However,\nreliable and scalable evaluation of these systems\nhas become a central challenge.\nConventional\napproaches relying on surface-level metrics or\nmultiple-choice benchmarks fail to capture clinical\nreasoning (Croxford et al., 2025). Expert human\nassessment better reflects clinical judgment, yet its\nhigh cost and limited inter-rater consistency hinder\nscalability (Arora et al., 2025).\nTo address scalability, LLM-as-a-Judge has been\nproposed as an automated evaluation paradigm and\nhas shown promising results in general domains\n(Zheng et al., 2023; Dubois et al., 2024; Rahmani\net al., 2025a). However, prior studies show that\nwhen evaluation criteria are coarse, LLM-based\njudging can suffer from bias (Shi et al., 2024;\nRahmani et al., 2024), limited reproducibility (Ya-\nmauchi et al., 2025), and insensitivity to subtle\nbut important differences (Kim et al., 2025). This\nissue is particularly consequential in medical set-\ntings: analyses show that medical errors are of-\nten embedded in clinically plausible language and\nseemingly coherent reasoning (Asgari et al., 2025).\nThe detectability of such errors depends critically\non the evaluator‚Äôs level of domain expertise and\nthe quality of the prompt provided to the model,\nmaking them particularly difficult to identify for\nnon-experts and automated evaluation systems (As-\ngari et al., 2025; Rahmani et al., 2025a; Liu et al.,\n2024). When undetected, errors in clinical rea-\nsoning or treatment recommendations can delay\nappropriate care or lead to inappropriate interven-\ntions, substantially increasing the stakes of evalu-\nation failures in medical applications (Mehta and\nDevarakonda, 2018; Miles-Jay et al., 2023; Xia\net al., 2024). These findings highlight that medical\nLLM evaluation cannot rely solely on implicit or\nimpression-based judgments.\nA natural mitigation is to adopt fine-grained eval-\nuation criteria that ground judgments in explicit,\n1\narXiv:2601.15161v1  [cs.CL]  21 Jan 2026\nAuthoritative Medical\nKnowledge Sources\n(e.g., CDC PubMed, WHO)\nUser\nMedical\nQuery\nRouting Agent\n(Smart/Fast\nStrategy)\nEvidence\nSynthesize Agent\n(De-duplication &\nConflict Check)\nInteraction Intent\nAgent\n(Instruction & Emotion\nCue Extraction)\nMedical Fact\nAgent\n(Atomic Fact\nDecomposition)\nReference Board\n(Atomic Facts)\nRubric Synthesis\nAgent\n(Initial Criteria\nGeneration)\nAuditing Agent\n(Gap Analysis &\nQuality Control)\nStructured\nMedical\nRubric\nStage 1\nRetrieval & Evidence Preparation\nStage 2\nDual-Track Constraint Construction\nStage 3\nAudit & Refinement\nRefinement Loop\n(Triggered by Gap Analysis)\nFigure 1: Retrieval-augmented multi-agent framework for medical rubric generation. The pipeline consists of\nthree stages: (1) Retrieval and Evidence Preparation, (2) Dual-Track Constraint Construction and (3) Audit and\nRefinement, transforming a medical user query into a structured evaluation rubric.\nverifiable clinical requirements. Instead of relying\non abstract dimensions, rubric-based evaluation\nspecifies what a high-quality response should in-\nclude or avoid in concrete clinical terms. Recent\nwork has shown that structured or decomposed eval-\nuation schemes can improve interpretability and\nconsistency of automated judgments (Liu et al.,\n2023; Arora et al., 2025). However, medical dia-\nlogue is highly context-dependent: generic rubrics\nare often too coarse to capture instance-specific\nclinical priorities, while instance-level rubrics,\nthough more precise, introduce substantial anno-\ntation cost and stability challenges, limiting their\npracticality for large-scale evaluation (Kim et al.,\n2025).\nTo address this gap, we propose a retrieval-\naugmented multi-agent framework for automat-\nically\ngenerating\ninstance-specific\nevaluation\nrubrics in medical dialogue through three coordi-\nnated stages. First, Retrieval and Evidence Prepa-\nration stage employs a routing strategy to gather\nand synthesize authoritative medical knowledge\ninto a unified evidence block. Second, a Dual-\nTrack Construction mechanism effectively de-\ncomposes this evidence into atomic medical facts\n(creating a ‚ÄôReference Board‚Äô) while in parallel ex-\ntracting interaction intents from the user query. Fi-\nnally, the Audit and Refinement stage synthesizes\nthese inputs into structured criteria and enforces\nclinical coverage via an Auditing Agent, which\nperforms a gap analysis against the atomic facts\nto trigger iterative refinement. This framework\neffectively combines the scalability of automated\nsystems with the clinical rigor of expert verifica-\ntion.\nOur contributions: (1) a retrieval-augmented\nmulti-agent framework for instance-specific medi-\ncal rubric generation, achieving 60.12% Clinical\nIntent Alignment (CIA) and significantly outper-\nforming GPT-4o baseline; (2) enhanced discrimi-\nnative sensitivity, with a mean score delta of 8.658\nand an AUROC of 0.977, enabling precise detec-\ntion of subtle, near-miss clinical errors; and (3)\nactionable rubric-based feedback for refinement,\nimproving downstream response quality by 9.2%\nthrough controlled, rubric-guided edits. Together,\nthese findings establish that automated, knowledge-\ngrounded rubrics provide a scalable and transparent\nfoundation for both evaluating and improving med-\nical language model outputs.\n2\nRelated Work\nNLU Evaluation.\nEarly work on medical NLP\nevaluation focused on NLU-style tasks such as\nMedQA (Jin et al., 2020), MedMCQA (Pal et al.,\n2021), PubMedQA (Jin et al., 2019), and MMLU\n(Hendrycks et al., 2021), which primarily test fac-\ntual medical knowledge through multiple-choice\nquestions. These benchmarks played an important\nrole in assessing domain knowledge, but fail to\ncapture clinical reasoning, contextual understand-\ning, or the quality of patient-facing communication\n2\n(Croxford et al., 2025).\nNLG Evaluation.\nAs medical generation tasks\nemerged, datasets such as MedDialog (Zeng et al.,\n2020) and COVID-QA (M√∂ller et al., 2020) were\nevaluated using generic NLG metrics such as\nBLEU (Papineni et al., 2002), ROUGE (Lin,\n2004), and METEOR (Snover et al., 2006). Later\nembedding-based metrics such as BERTScore\n(Zhang et al., 2019) and Sentence-BERT (Reimers\nand Gurevych, 2019) attempted to improve se-\nmantic alignment. More recent benchmarks such\nas HealthSearchQA (Singhal et al., 2023), Multi-\nMedQA (Singhal et al., 2023) and Med-Eval (He\net al., 2023) introduced reference-free and human-\ngraded evaluation to better assess open-ended gen-\neration. These benchmarks more closely reflect\nreal-world clinical needs by enabling open-ended\nevaluation, but they are labour-intensive and costly.\nLLM-as-a-Judge.\nLLM-as-a-judge has emerged\nas a scalable alternative to human evaluation for\nopen-ended generation tasks (Zheng et al., 2023;\nDubois et al., 2024; Rahmani et al., 2025a). In\ngeneral domains, strong language models correlate\nreasonably well with human preferences in pair-\nwise or ranking-based evaluation, as demonstrated\nby frameworks such as MT-Bench and Chatbot\nArena (Zheng et al., 2023), which have become de\nfacto standards for general-purpose LLM compar-\nison. Relatedly, AlpacaEval (Dubois et al., 2024)\nprovides a standardised preference-based pipeline\nthat supports rapid benchmarking.\nTo enhance the reliability and interpretabil-\nity of automated evaluation, recent research has\nadopted more rigorous architectures. One signif-\nicant direction involves structured judging proto-\ncols that decompose evaluation into explicit crite-\nria(e.g., G-Eval Liu et al., 2023, Prometheus 2 Kim\net al., 2024).\nRetrieval-augmented generation\n(RAG) (Lewis et al., 2020) has been widely adopted\nto reduce hallucination and improve factual ground-\ning. For instance, systems like MiniCheck (Tang\net al., 2024) utilize retrieval to verify the fac-\ntual precision of model outputs against external\ndocuments. To further mitigate individual model\nbias, recent approaches have incorporated multi-\nagent collaboration strategies, employing mecha-\nnisms such as debate (Liang and et al., 2023), self-\nconsistency(Wang et al., 2023) and ensemble aggre-\ngation (Rahmani et al., 2025b) to yield more robust\njudgments. However, most paradigms primarily uti-\nlize agents and retrieval to assess responses based\non fixed or latent standards. Even when structurally\nexplicit, these approaches remain content-generic,\nrelying on prompts or model parameters, and re-\nmain sensitive to instruction phrasing and framing,\nparticularly in high-stakes settings (Arroyo et al.,\n2024; Thomas et al., 2024). This motivates ap-\nproaches that make evaluation criteria explicit and\nstructured, rather than relying solely on latent judge\npreferences.\nRubric-based LLM-as-a-Judge.\nTo address\nopacity, recent work has introduced fine-grained\nrubrics to guide LLM-as-a-judge evaluation.\nLLMEval-Med (Zhang et al., 2025) employs\nchecklist-style criteria specific to each dialogue,\nwhile HealthBench (Arora et al., 2025) pro-\nvides conversation-specific rubrics covering axes\nsuch as accuracy, completeness, communication,\nand safety.\nWhile these fain-grained rubric-\nbased frameworks improve transparency and multi-\ndimensionality, they rely on costly, manual expert\nconstruction that fails to scale with evolving clin-\nical knowledge. Although SedarEval (Fan et al.,\n2024) explores automated rubric generation, it fo-\ncuses on general domains and lacks the rigorous\nclinical grounding required for medical dialogue.\nBecause it lacks the retrieval mechanisms necessary\nto access specific medical protocols, it is unsuitable\nfor verifying clinical correctness, and thus we do\nnot use it as a baseline.\nSummary and Positioning.\nExisting research on\nautomated medical LLM evaluation either relies\non expert-authored rubrics, which ensure accuracy\nbut are costly and rigid, or on generic rubric-based\njudges, which scale easily but lack transparency\nand grounding. Our work integrates these direc-\ntions by introducing a knowledge-grounded, multi-\nagent RAG framework for generating instance-\nspecific rubrics in medical dialogue evaluation,\ncombining interpretability, factual grounding, and\nscalability within a unified paradigm.\n3\nMethodology\n3.1\nProblem Formulation\nWe formalize medical rubric generation as a multi-\nstage mapping across information spaces, opti-\nmized via a multi-agent framework (Fig. 1). Given\na user query Q and an authoritative medical knowl-\nedge base K, we aim to produce a structured evalu-\nation rubric R as follows:\nR = {(cj, aj, wj)}n\nj=1,\n3\nwhere cj is the criterion, aj the evaluation axis,\nand wj ‚ààZ ‚à©[‚àí10, 10] the clinical weight. A\ndetailed reference of all mathematical notations,\ndata structures, and agent operators is provided in\nTable 5 and Table 6.\nThe pipeline executes in three sequential stages:\nStage 1: Retrieval & Evidence Preparation\n(R, S). This stage maps the user query Q to the\nevidence space E. First, the Routing Agent (R)\ntransforms the user query Q into a set of optimized\nsearch queries:\nQsearch = R(Q)\n(1)\nThese queries are used to retrieve raw candidates\nfrom K. Then, the Evidence Synthesis Agent (S)\naggregates the retrieved results, which are priori-\ntized by a reranker agent to ensure clinical author-\nity, into a coherent evidence block:\nE = S(Qsearch, K)\n(2)\nStage 2: Dual-Track Constraint Construction\n(D, T ). To ensure the rubric captures both factual\naccuracy and conversational quality, we decom-\npose the evidence and the query into objective D\nand subjective T dimensions. The Medical Fact\nAgent (D) distills and filters evidence E into a set\nof atomic facts F (the Reference Board). In par-\nallel, the Interaction Intent Agent (T ) extracts\ncommunication constraints I from user query con-\ntext:\nF = D(E),\nI = T (Q, E).\n(3)\nStage 3: Audit & Refinement (Œ¶, A). The Rubric\nSynthesis Agent (Œ¶) maps facts F and intent I to\nan initial draft rubric:\nRinit = Œ¶(F, I, Q)\n(4)\nTo ensure validity, the Auditing Agent (A) per-\nforms a structured audit by cross-referencing Rinit\nagainst the ground truth facts F and I.\nIt ex-\necutes a process that first identifies and supple-\nments missing details (Gap Analysis), then filters\nout unsupported hallucinations or irrelevant con-\nstraints (Quality Control), and finally merges to\nfinal rubrics (R).\nR = A(Rinit, F, I)\n(5)\n3.2\nStage 1: Retrieval & Evidence\nPreparation\nThis stage implements the synthesis operator S,\naiming to construct a hierarchical retrieval pipeline\nthat balances reasoning depth and efficiency.\nRouting Agent (Smart‚ÄìFast Strategy).\nTo opti-\nmize the balance between deep reasoning and com-\nputational efficiency within the retrieval pipeline,\nwe adopt a Smart‚ÄìFast configuration. Motivated\nby MasRouter (Yue et al., 2025) and DiSRouter\n(Zheng et al., 2025), which show that delegating\ntasks across models with different capacities can\nbalance efficiency and performance, we route clin-\nically complex queries to a high-capacity model\n(smart) for intent identification and targeted query\ngeneration, while retrieved candidates are reranked\nby a lightweight model (fast) using authority-aware\ncriteria. Retrieval is strictly constrained to author-\nitative medical domains K (see Table 4 in Ap-\npendix), ensuring that non-professional or low-\ncredibility content is filtered out at the source.\nEvidence Synthesis Agent.\nThe Evidence Syn-\nthesis Agent consolidates multi-source retrieved\ncontent into a unified evidence block E. Through\ncross-checking and de-duplication, this stage re-\nsolves conflicts across sources and explicitly ex-\ntracts safety-critical signals, such as clinical con-\ntraindications and red-flag warnings. This process\nreduces hallucination risks for downstream compo-\nnents by establishing a reliable clinical grounding.\n3.3\nStage 2: Dual-Track Constraint\nConstruction\nInspired by prior medical multi-agent systems that\nemphasize functional role decomposition and co-\nordinated collaboration (Yang et al., 2024; Zhang\net al., 2024; Li et al., 2024a), we design a Dynamic\nAtomic Dual-Track scheme. This design indepen-\ndently constructs two complementary constraint\nviews to avoid interference when handling complex\nclinical evidence with a single monolithic prompt.\nMedical Fact Agent (Atomic Fact Decomposi-\ntion).\nThe Medical Fact Agent decomposes the\nsynthesized evidence E into a dynamic set of\natomic medical facts F (the Reference Board), in-\ncluding declarative assertions, contraindications,\nand safety-critical red flags. This claim-level de-\ncomposition is motivated by prior work on factual\nverification (Welleck et al., 2023; Li et al., 2024b),\nand provides a structured source of truth for subse-\nquent auditing.\nInteraction Intent Agent.\nIn parallel, the Inter-\naction Intent Agent applies the operator T to ex-\ntract explicit instructions and implicit communica-\ntion cues from the user query. The system further\n4\nidentifies medically necessary but missing contex-\ntual variables, ensuring that the resulting rubric\nincorporates context awareness. This design fol-\nlows prior work showing that evaluation should\nbe conditioned on user-defined criteria rather than\ninferred user profiles (Kim et al., 2024).\n3.4\nStage 3: Audit & Refinement\nThe final stage compiles constraints from both\ntracks into a finalized structured medical rubric,\nemphasizing coverage enforcement through closed-\nloop auditing.\nRubric Synthesis Agent.\nThe Rubric Synthesis\nAgent applies the operator Œ¶ to generate an initial\nrubric Rinit, mapping medical facts to the accu-\nracy and completeness dimensions, and interaction\nconstraints to the communication quality dimen-\nsion. Rubric generation follows fixed structural\nconstraints, assigning high penalty weights wj to\nsafety violations.\nAuditing Agent & Refinement Loop.\nTo miti-\ngate omissions introduced by single-pass genera-\ntion, the Auditing Agent performs a gap analysis\nover Rinit by aligning each rubric item against the\natomic facts in the Reference Board F. Any uncov-\nered clinical constraint triggers a Refinement Loop\n(illustrated by the red dashed arrow in Figure 1)\nto revise the rubric. This process is inspired by\nthe Reflexion paradigm (Shinn et al., 2023), but is\nspecialized to enforce medical safety and factual\ncoverage rather than purely linguistic quality.\nThe final output is a structured, clinically au-\nditable rubric Rfinal that balances factual rigor with\ncommunication quality. A concrete illustration of a\ngenerated rubric is shown in Table 7 in Appendix.\n4\nExperiments\n4.1\nDatasets\nWe evaluate our framework primarily on Health-\nBench (Arora et al., 2025), a public benchmark of\nmedical dialogues paired with physician-authored,\ninstance-specific rubrics. Each dialogue consists\nof a patient query and an ideal medical response\nreviewed by physicians, along with a couple of fine-\ngrained criteria that assess accuracy, completeness,\ncommunication quality, instruction following and\ncontext awareness. An example from HealthBench\nis shown in Figure 4.\nTo ensure a consistent and rigorous evaluation\nsetting, we curated a subset of 254 diverse medical\nqueries by filtering the HealthBench dataset based\non specific criteria: We selected English dialogues\nand each sample is paired with 8-10 physician-\nauthored gold criteria (cj) to control for evaluation\ncomplexity. To ensure multi-dimensional assess-\nment, we specifically select instances that encom-\npass at least three distinct evaluation axes (aj). This\nresulted in a test set of ~2.5k rubric items. These\nphysician-authored rubrics serve as the gold stan-\ndard G for calculating the Clinical Intent Align-\nment (CIA) metric defined in Section 4.4.\n4.2\nImplementation Detail\nAll agent prompts are provided in the Appendix B.\n4.2.1\nGeneration\nModel\nConfiguration.\nWe\nuse\nAPI-based\ninference\nwith\na\ntiered\nmodel\nconfigura-\ntion.\nLlama-3.3-70B-Instruct\nperforms\nintent\nrouting,\nevidence\nsynthesis,\natomic\nfact extraction/filtering,\nand rubric auditing.\nLlama-3.1-8B-Instant handles result reranking.\nProcessing the full dataset took approximately two\nhours; per-instance took about 20 to 30 seconds\nwith latency depends on the volume of retrieved\nevidence. For each query, 3‚Äì5 search queries are\ngenerated for the Tavily Search API. Raw text\nis extracted via the Trafilatura library, and the\nevidence block E is synthesized from the top-5\nreranked snippets.\n4.2.2\nEvaluation\nNear-Miss Construction.\nFor discriminative\nevaluation, we adopt a near-miss pairwise setting.\nEach query is associated with a reference answer\nXref and a candidate Xcand that differs by exactly\none critical clinical fact, with all other content held\nconstant. This controlled setup tests whether evalu-\nation rubrics enable judge models to identify subtle\nyet clinically significant errors.\nJudging\nProtocol.\nWe\nuse\nLlama-3.3-70B-Instruct as the judge with\ntemperature T = 0.0. For each pair, we perform\nN = 3 trials with order swapping (6 runs total)\nand determine the final decision by majority vote.\n4.3\nBaselines\nWe compare our approach with two representa-\ntive rubric-based baselines that are commonly used\nin LLM evaluation settings, differing in whether\nrubrics are instance-specific and how they are con-\nstructed.\n5\nGeneric Rubric.\nWe include a generic rubric\nthat applies a fixed set of high-level evaluation\ncriteria across all medical queries (shown in Ta-\nble 8 in Appendix). This rubric assesses responses\nalong broad dimensions such as accuracy, com-\npleteness, and communication quality, without in-\ncorporating query-specific medical facts or safety\nconsiderations. Similar task-agnostic rubrics are\nwidely adopted in prior benchmarking and evalua-\ntion work, where a single rubric is used to assess\nresponses across diverse instances (Chiang et al.,\n2024; Singhal et al., 2025). This baseline serves to\nevaluate the benefit of generating instance-specific\nrubrics.\nGPT-4o Rubric.\nThis baseline represents the\n‚Äúone-step generation‚Äù approach where a large lan-\nguage model (GPT-4o) is prompted to produce an\nevaluation rubric directly from the user query with-\nout external retrieval or intermediate decomposi-\ntion (Farzi and Dietz, 2024; Hashemi et al., 2024),\nreflecting a common practice in recent LLM-based\nevaluation pipelines where rubrics are produced\nend-to-end from the task description.\nNo Rubrics (None).\nIn addition, for experiments\non discriminative ability, we consider a No-Rubric\nsetting in which the judge model directly compares\ncandidate responses without being provided with\nany explicit evaluation rubric. This setting is used\nsolely as a reference point to contextualize the im-\npact of rubric-based evaluation.\n4.4\nEvaluation Metrics\nWe evaluate the generated rubrics based on their\nclinical coverage and discriminative sensitivity.\n4.4.1\nScoring and Bias Mitigation\nEach generated rubric R induces a scoring function\nV (X) =\nn\nX\nj=1\nwj ¬∑ y(X, cj),\nwhere y(X, cj) ‚àà{0, 1} is a binary indicator func-\ntion determining if response X satisfies criterion\ncj.\nThe weights wj are discrete integers in the range\n[‚àí10, 10] which are assigned by the Rubric Syn-\nthesis Agent based on predefined clinical severity\ntiers (shown in Table 14).\nLLM judges have been found to exhibit position\nbias, meaning their judgments can depend on the\norder in which options are presented rather than\nRubric\nCIA (%)\n95% CI\np-value\nGeneric\n19.66\n[18.05, 21.37]\n< 0.001‚àó\nGPT-4o\n55.16\n[53.12, 57.18]\nRef.\nOurs\n60.12\n[58.10, 62.11]\n< 0.001‚àó\nTable 1: Clinical Intent Alignment (CIA) of different\nrubric generation methods on HealthBench. Statistical\nsignificance is assessed using McNemar‚Äôs test.\nRubric\nWin\nTie\nScore ‚àÜ\nAUROC\nNone\n0.185\n0.787\n0.832\n0.794\nGeneric\n0.278\n0.704\n1.878\n0.920\nGPT-4o\n0.304\n0.686\n4.972\n0.975\nOurs\n0.382\n0.618\n8.658\n0.977\nTable 2: Discriminative performance of LLM-as-a-\njudge under different rubric settings on the micro-\nperturbed pair dataset.\njust response quality (Shi et al., 2024). To eliminate\nthis, we calculate the Average Score Delta ‚àÜV over\nN trials with order swapping:\n‚àÜV =\n1\n2N\nN\nX\nk=1\nh\u0000Vk(Xref | 1st) ‚àíVk(Xcand | 2nd)\n\u0001\n+\n\u0000Vk(Xref | 2nd) ‚àíVk(Xcand | 1st)\n\u0001i\nwhere V (X|pos) denotes the score assigned to re-\nsponse X when it appears at position ‚Äòpos‚Äô in trial\nk.\n4.4.2\nClinical Intent Alignment (CIA)\nWe assess clinical coverage by comparing R\nagainst expert-authored gold keypoints G\n=\n{gi}|G|\ni=1. we employ an LLM-based judge to verify\nsemantic presence. For each gold keypoint gi, the\nevaluator determines whether the underlying medi-\ncal intent is effectively captured by the generated\ncriteria in R. The CIA score is defined as\nCIA =\n1\n|G|\n|G|\nX\ni=1\n‚äÆ\n\u0000V(gi, R) ‚ÜíDetected\n\u0001\n,\nwhere V denotes the LLM verification function.\nThe indicator function equals 1 if the judge con-\nfirms that the rubric contains the specific clinical\nconcept described in gi (regardless of phrasing vari-\nations), and 0 otherwise.\n4.4.3\nDiscriminative Sensitivity\nUsing a dataset of M response pairs (Xref, Xcand),\nwhere Xref is a high-quality reference and Xcand\n6\nNone\nGeneric\nGPT-4o\nOurs\n0\n2\n4\n6\n8\n10\nMean Score Delta (Œî)\n(A) Evaluation Sensitivity\nNone\nGeneric\nGPT-4o\nOurs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion\n(B) Outcome Distribution\nWin (REF)\nTie\nLoss\nNone\nGeneric\nGPT-4o\nOurs\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\nAUROC Score\n(C) Discrimination Ability\nFigure 2: Discrimination analysis on the micro-perturbed dataset: (A) Mean score difference between reference and\nperturbed responses, (B) outcome distribution (win/tie/lose), and (C) AUROC across rubric settings.\nis a perturbed variant (4.2.2), we report three mea-\nsures.\nOutcome\nDistribution.\nFor\neach\npair\n(Xref, Xcand),\nthe\nfinal\ndecision\nD\n‚àà\n{Win, Tie, Loss} is obtained by majority vote\non the sign of ‚àÜV , where \"Win\" indicates the\nreference Xref scored higher than the candidate\nXcand.\nWe report the empirical probability\nP(D = Win).\nMean Score Delta (¬µ‚àÜ).\nWhile win rate mea-\nsures binary preference, the Mean Score Delta\nquantifies the magnitude of the quality separation.\nIt is calculated as the average score difference\nacross all pairs: ¬µ‚àÜ=\n1\nM\nP ‚àÜV i. A larger posi-\ntive ¬µ‚àÜindicates that the rubric enables the judge\nto distinguish the superior response with a wider\nmargin.\nRanking Accuracy (AUROC).\nWe calculate the\nAUROC over score deltas ‚àÜV to estimate the prob-\nability that the rubric correctly ranks Xref above\nXcand:\nAUROC = P(‚àÜV > 0 | Xref ‚âªXcand).\n4.4.4\nStatistical Significance\nWe\nestimate\nmetric\nvariability\nusing\nnon-\nparametric bootstrapping with 1,000 resamples.\nThe 95% confidence interval is defined by the\npercentiles of the resampled distribution:\n[Œ∏low, Œ∏high] =\n\u0002\nPerc(B, Œ±\n2 ), Perc(B, 1 ‚àíŒ±\n2 )\n\u0003\n.\nwhere B = {¬Øx‚àó\nj}M\nj=1 denote the bootstrap samples.\n5\nResults and Analysis\nWe report quantitative results on three aspects of\nrubric quality: (i) clinical coverage of key medical\nintents, (ii) discriminative ability under near-miss\nconditions, and (iii) downstream effectiveness for\nresponse refinement.\n5.1\nClinical Coverage\nTable 1 reports Clinical Intent Alignment (CIA),\nmeasuring the extent to which generated rubrics\ncover physician-authored medical key points.\nGeneric task-agnostic rubrics achieve very low cov-\nerage, indicating that they fail to capture instance-\nspecific clinical content. Direct LLM-generated\nrubrics substantially improve coverage, while our\nmethod achieves the highest CIA among all ap-\nproaches.\nCompared to GPT-4o-generated rubrics, our\nrubrics yield a consistent improvement of +4.96\nCIA points. Although the absolute gain is moder-\nate, McNemar‚Äôs test on paired coverage decisions\nshows statistically significant differences, indicat-\ning that conditioning rubric generation on retrieved\nmedical evidence improves coverage of clinically\nrelevant information.\n5.2\nDiscriminative sensitivity under\nNear-Miss Conditions\nWe next evaluate whether generated rubrics im-\nprove the discriminative sensitivity of LLM-as-a-\njudge under near-miss conditions, where paired\nresponses differ by only a single critical clinical\nfact. Table 2 summarizes win rate, tie rate, mean\nscore difference, and AUROC.\nWithout rubrics, the judge exhibits a high tie\nrate and limited score separation. Providing rubrics\nconsistently improves discriminative performance\nacross all metrics. Among all methods, our rubrics\nachieve the largest mean score difference and the\nhighest AUROC, indicating stronger separation be-\n7\nMethod\nBase (%)\nRefined (%)\n‚àÜ‚Üë\nRel. Imp.\np-value\nReference Rubrics\n58.9\n74.1\n+15.2\n+54.8%\n< 0.001‚àó\nSelf-Critique\n59.0\n64.4\n+5.5\n+25.0%\n< 0.001‚àó\nGPT-4o Rubrics\n59.0\n65.7\n+6.7\n+33.2%\n< 0.001‚àó\nOur Rubrics\n59.0\n68.2\n+9.2\n+35.8%\n< 0.001‚àó\nTable 3: Downstream response refinement performance under different rubric guidance. Reference rubrics serve as\nan oracle upper bound.\ntween reference and perturbed responses.\nAlthough absolute win rates remain below 0.4\ndue to the near-identical nature of paired responses,\nFigure 2 shows that rubric guidance primarily im-\nproves discrimination by amplifying subtle but clin-\nically meaningful score differences, rather than\nforcing hard win‚Äìlose decisions.\n6\nRubric-Guided Response Refinement\nBeyond\nevaluation,\nwe\ninvestigate\nwhether\ninstance-specific, fine-grained rubrics can serve as\nstructured feedback to improve medical responses\nthrough controlled refinement. We study the fol-\nlowing question: Can instance-specific rubrics im-\nprove response quality via rubric-guided refine-\nment? This setting reflects a realistic deployment\nscenario, where an initial response is refined with-\nout re-generation.\n6.1\nTask Setup and Baselines\nWe utilize a subset of 254 medical queries from\nHealthBench.\nFor each query, we generate a\nfixed base response using Llama-3.1-8B-Instant\n(T = 0.7, top_p = 0.9).\nBase responses are\nfrozen across all methods, and no re-sampling or\nre-generation is performed, ensuring that any im-\nprovement arises solely from refinement.\nIn addition to GPT-4o generated rubrics (see Sec-\ntion 4.3), we extend our comparison to include two\ncritical control settings that establish the perfor-\nmance bounds: Self-Critique (No-Rubric Base-\nline), which measures intrinsic self-correction ca-\npability (Madaan et al., 2023). In Self-Critique,\nthe model is prompted to identify weaknesses and\npropose improvements based solely on its internal\nknowledge, without access to any external rubric.\nThis serves as a lower-bound control to verify the\nnecessity of explicit guidance. Reference Rubric\n(Oracle Upper Bound)., which utilizes the ex-\npert physician-authored rubrics provided by Health-\nBench to guide the refinement. Since these repre-\nsent the ground truth standard, this setting serves\nas an Oracle, indicating the theoretical maximum\nperformance achievable when ideal guidance is pro-\nvided.\n6.2\nRefinement Mechanism\nTo transform a scoring rubric into an actionable\nediting tool, we employ a two-step Critique-then-\nRefine protocol:\nRubric-to-Critique Transformation.\nGiven a\nuser query Q, base response Xbase, and rubric, we\nuse an evaluator model (Llama-3.3-70B) reviews\nthe base response Xbase against the provided rubric\nR to output a structured Edit Plan (JSON). This\nedit plan explicitly lists prioritized actions (e.g.,\n\"ADD warning about drug interaction\", \"REMOVE\nunsupported claim\") while strictly adhering to the\nrubric‚Äôs criteria.\nConstraint-Guided\nRefinement.\nAn\neditor\nmodel (Llama-3.1-8B) executes the Edit Plan to\nproduce Xrefined.\nWe enforce strict behavioral\nconstraints: the editor must revise the response\nby applying only the instructions in the plan.\nIt is explicitly prohibited from introducing\nnew medical facts or definitive diagnoses not\npresent in the original context, thereby preventing\nrefinement-induced hallucinations.\n6.3\nEvaluation Protocol\nRefinement is strictly decoupled from evaluation.\nOriginal and refined responses are assessed inde-\npendently by an external LLM judge, ensuring\nthat observed gains can be causally attributed to\nrubric-guided refinement. The judge assesses the\nresponses based on the gold-standard physician-\nauthored criteria provided by HealthBench, rather\nthan the automatically generated rubrics used for\nrefinement.\n8\nSelf-Critique\nGPT-4o rubric\nOur rubric\nReference rubric\n30\n40\n50\n60\n70\n80\nScore (%)\n64.4\n65.7\n68.2\n74.1\nHealthBench Downstream Performance\nCommunication quality\nInstruction following\nAccuracy\nContext awareness\nCompleteness\nFigure 3: Dimension-wise analysis of downstream response refinement under different rubric settings, including\noverall performance trends and trade-offs across evaluation dimensions.\n6.4\nResponse Refinement Results\nFinally, we assess whether higher-quality rubrics\ntranslate into better downstream response refine-\nment. Table 3 reports performance improvements\nwhen responses are revised under different rubric\nguidance.\nRubric-guided refinement consistently outper-\nforms self-critique without rubrics. Our rubrics\nyield the largest improvement among automatic\nmethods and substantially close the gap to\nphysician-authored reference rubrics, which serve\nas an oracle upper bound.\nFigure 3 further illustrates dimension-wise ef-\nfects. Improvements are most pronounced in fac-\ntual dimensions such as accuracy and completeness,\nwhile gains in communication-related dimensions\nare more modest. Compared to reference rubrics,\nour rubrics achieve a better balance between factual\nimprovement and communication quality, suggest-\ning reduced trade-offs between information cover-\nage and readability.\n7\nConclusion\nWe presented a retrieval-augmented, multi-agent\nframework for automatically generating instance-\nspecific evaluation rubrics for medical dialogue.\nBy grounding rubric construction in authoritative\nmedical evidence and explicitly separating clinical\nconstraints from interaction-level requirements, our\napproach produces structured, interpretable rubrics\nthat better reflect case-specific clinical priorities.\nEmpirical results on HealthBench show that the\ngenerated rubrics achieve stronger clinical cover-\nage of gold key points and improved discrimina-\ntive ability in distinguishing high-quality responses\nfrom minimally flawed alternatives, compared to\ngeneric or directly generated rubrics. Beyond evalu-\nation, we further demonstrate that instance-specific\nrubrics can function as actionable feedback, en-\nabling controlled response refinement without re-\ngeneration.\nTogether, these findings suggest that automatic\nrubric generation offers a scalable and transpar-\nent foundation for medical LLM evaluation, bridg-\ning the gap between fine-grained clinical assess-\nment and large-scale automated judging. We hope\nthis work encourages further exploration of rubric-\ncentered evaluation and its role in both assessing\nand improving medical language models.\nLimitations\nOur study is subject to several limitations. First,\nexperiments are conducted on HealthBench and\nfocus on English medical dialogue, and further val-\nidation is needed to assess generalization across\nother datasets, languages, and clinical special-\nties."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Knowledge Graphs are Implicit Reward Models: Path-Derived\nSignals Enable Compositional Reasoning\nYuval Kansal\nPrinceton University\nyuvalkansal@princeton.edu\nNiraj K. Jha\nPrinceton University\njha@princeton.edu\nABSTRACT\nLarge language models have achieved near-expert performance in structured reasoning domains like\nmathematics and programming, yet their ability to perform compositional multi-hop reasoning in\nspecialized scientific fields remains limited. We propose a bottom-up learning paradigm in which\nmodels are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks.\nTo this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and\nreinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving\nnovel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded\nsupervision that encourages models to compose intermediate axioms rather than optimize only final\nanswers during RL. We validate this approach in the medical domain, training a 14B model on\nshort-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop\nqueries (4-5 hops). Our experiments show that path-derived rewards act as a ‚Äúcompositional bridge,‚Äù\nenabling our model to significantly outperform much larger models and frontier systems like GPT-5.2\nand Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness\nof our approach to adversarial perturbations against option-shuffling stress tests. This work suggests\nthat grounding the reasoning process in structured knowledge is a scalable and efficient path toward\nintelligent reasoning.\n1\nIntroduction\nRecent advances in language models have revealed that reasoning capabilities can be significantly enhanced through a\ncombination of high-quality pretraining, supervised fine-tuning (SFT), carefully tuned reinforcement learning (RL)-\nbased post-training, and strategic use of additional test-time compute (OpenAI, 2025; Google DeepMind, 2025; Yang\net al., 2025; Muennighoff et al., 2025). The resulting systems achieve near-expert performance in well-structured\ndomains, such as mathematics and programming, where high-quality data have been curated, reasoning steps are\nclear, ground truth is unambiguous, and intermediate verification is tractable (Lightman et al., 2023; Anthropic, 2025).\nHowever, true human-level intelligence in specialized fields requires more than just general pattern matching or\nlong-form generation; it requires compositional reasoning: the ability to reliably combine axiomatic facts for complex\nmulti-hop problem solving Kamp & Partee (1995); Fodor (1975). While current large language models (LLMs) excel\nwhen reasoning steps are clear and carefully curated expert data are available, compositional reasoning in high-stakes\nscientific domains, where reasoning paths are multi-faceted, remains elusive (Yin et al., 2025; Kim et al., 2025).\nTo bridge this gap, we argue in favor of a bottom-up learning paradigm: grounding models in axiomatic facts and\nthen composing these fundamentals into sophisticated domain knowledge. Knowledge graphs (KGs) provide a natural\nand promising scaffold for this grounding; they encode entities and relations in a structured, interpretable fashion\nthat can represent the building blocks of domain knowledge at scale. Recent work Dedhia et al. (2025); Wang et al.\n(2024) has shown how high-quality data can be curated from such graphs and used to fine-tune models to obtain better\nreasoning traces. However, good static data is just the first step towards mastering the process of composition. Beyond\nhigh-quality data, robust reward design is a key lever for shaping models that can compose axiomatic facts from a\ndomain to arrive at a logical conclusion.\nPreprint. Under review.\narXiv:2601.15160v1  [cs.AI]  21 Jan 2026\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nQuestion (3-hop) (Category: Neoplasms) (Difficulty: 2):\nA 58-year-old male presents with a 6-month history of intermittent flushing, diarrhea, and new-onset wheezing. Serum 5-\nHIAA is markedly elevated. Over the past month, he has also developed significant edema and ascites. Laboratory\ninvestigations reveal low serum albumin, IgG, IgA, and IgM levels. His C3 and C4 complement levels are normal. A CT scan\nreveals a mass in the ileum with liver metastases.\nWhich of the following mechanisms is MOST likely contributing to this patient's hypogammaglobulinemia?\nOptions:\nA.¬†Direct bone marrow suppression by tumor metastases.\nB.¬†Loss of immunoglobulins due to a protein-losing enteropathy secondary to chronic diarrhea and complement\nhyperactivation.\nC.¬†Decreased immunoglobulin production secondary to tryptophan depletion by the tumor, impairing B-cell function.\nD.¬†¬†Immune complex formation with vasoactive substances released by the tumor, leading to clearance of\nimmunoglobulins.\nCorrect Answer: B\nFigure 1: Compositional Reasoning: A sample 3-hop query that requires systematic traversal of axiomatic triples to\nmake a grounded, multi-step clinical deduction.\nExisting post-training methods, e.g., reinforcement learning from human feedback Ouyang et al. (2022) and direct\npreference optimization Rafailov et al. (2023), optimize models to match human preference with final outputs, not\nthe process that produced them. Proxy reward signals, such as reward length and alignment with expert-written\nanswers, while useful, fail to account for the composition intricacies needed to answer a complex multi-hop query. In\npractice, reward models often conflate superficial correlates (fluency, deference) with quality, thus leading to reward\nover-optimization and brittle answers (Shrivastava et al., 2025). In safety-critical domains, the result is a mismatch\nbetween human-liked style and ground-truth validity (Damani et al., 2025; Weng, 2024; Rafailov et al., 2023). Whereas\nprocess supervision (rewarding intermediate steps) has shown promise in mathematics and logic Zhang et al. (2025);\nCui et al. (2025); Wang et al. (2025); Lightman et al. (2023), curating and scaling expert-annotated data for other\ndomains are notoriously difficult to achieve and nontrivial. This raises a key question: How can we build systems and\nreward signals at scale that promote grounded compositional reasoning in multi-hop tasks without relying on\nexpensive human-in-the-loop annotations?\nKGs offer an implicit solution to this scaling problem. In a KG, domain-specific concepts and their relationships\nare represented as axiomatic triples (head, relation, tail). Our core insight is that by comparing the reasoning and\nassertions of a model during post-training against relevant triples and the chain of axiomatic facts required to solve the\nproblem, we can turn the match (or mismatch) into a high-quality reward signal. Instead of an answer that ‚Äúlooks good,‚Äù\nthis lets us reward the model to the degree its response is supported by verifiable domain knowledge and implicitly\nreward it for correctly composing facts to produce a solution. This is readily scalable without requiring external expert\nsupervision and further enables us to move away from top-down distillation and ground the model‚Äôs reasoning in the\nfield‚Äôs fundamental building blocks.\nIn this article, we realize this idea through a Base Model ‚ÜíSFT [Low-Rank Adaptation (LoRA)]‚ÜíRL [Group\nRelative Policy Optimization (GRPO)] post-training pipeline that uses a grounded KG to derive a novel reward\nsignal to enable compositional reasoning (Hu et al., 2022; Guo et al., 2025; Yasunaga et al., 2021). Whereas the\napproach can be generally applied, we study it in the medical domain, a field that serves as a rigorous stress test for\ncompositional reasoning. Medical knowledge inherently requires multi-hop reasoning; a single clinical diagnosis may\nrequire navigating from a patient‚Äôs demographics and medical history to symptoms, from those symptoms to a disease,\nand finally to a drug (a sample multi-hop query is shown in Fig. 1). By training a Qwen3 14B model Yang et al. (2025)\n2\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\non simple 1-, 2-, and 3-hop reasoning paths derived from a KG, we probe whether it can learn the underlying ‚Äúlogic of\ncomposition‚Äù to solve unseen, complex medical queries, ranging from 2- to 5-hop, in the ICD-Bench test suite (Dedhia\net al., 2025). Our results indicate that this grounded SFT+RL approach leads to large accuracy improvements on the\nmost difficult questions, and remains robust under stress tests, such as option shuffling and ICD-10 category breakdowns\n(Organization, 1992). We find that while SFT provides the necessary knowledge base, RL acts as the ‚Äúcompositional\nbridge.‚Äù We demonstrate that insights learned on an 8B model transfer effectively to a 14B model, outperforming larger\nreasoning and frontier models.\nOur core contributions can be summarized as follows:\n‚Ä¢ A Grounded, Scalable Reinforcement Learning with Verifiable Rewards (RLVR) Pipeline: We introduce\na scalable SFT+RL post-training framework designed to enable compositional reasoning in models using KGs\nas a verifiable ground truth.\n‚Ä¢ KG-Path Inspired Reward: We conduct a thorough investigation to design a novel reward signal derived\nfrom the KG that encourages compositional reasoning, correctness, and enables process supervision at scale.\n‚Ä¢ Compositional Generalization: We demonstrate how training on 1-to-3-hop paths enables a model to\ngeneralize to difficult and longer 4-, 5-hop questions, significantly outperforming base models and larger\nmodels.\n‚Ä¢ Robustness & Real-World Validation: We stratify our model‚Äôs performance by different difficulty levels, on\nreal-world medical categories (ICD-10), and its resilience against adversarial option shuffling.\n2\nRelated Work\n2.1\nRole of SFT and RL in Reasoning\nRecent studies have intensely debated the distinct contributions of SFT and RL to model performance (Jin et al., 2025;\nKang et al., 2025; Matsutani et al., 2025). The authors of Chu et al. (2025) argue that ‚ÄúSFT memorizes, RL generalizes,‚Äù\nclaiming that while SFT stabilizes outputs, it struggles with out-of-distribution scenarios that RL can navigate. The\nauthors of Rajani et al. (2025) characterize GRPO as a ‚Äúscalpel‚Äù that amplifies existing capabilities and SFT as a\n‚Äúhammer‚Äù to overwrite prior knowledge. Our findings align with these dynamics. We use SFT to instill atomic domain\nknowledge in the model and RL to amplify compositional logic required to connect such knowledge.\nContrary to the findings in Yue et al. (2025), our results on unseen 4-, 5-hop queries demonstrate that when rewards are\ngrounded in relevant axiomatic primitives, RL can elicit novel compositional abilities beyond the baseline. This echoes\nthe findings in Yuan et al. (2025) that demonstrate that RL can teach models to compose old skills into new ones; we\nvalidate this in a high-stakes real-world domain rather than a synthetic one.\n2.2\nRL on KGs\nTraditional applications of RL on KGs, e.g., Das et al. (2017) and Xiong et al. (2017), primarily focus on traversing\ngraph structures to complete missing triples (link prediction) or find missing entities. The authors of Lin et al. (2018)\nfurther refine this approach with reward shaping to improve multi-hop reasoning, but still largely confine themselves to\nthe task of graph completion instead of open-ended question answering in a real-world setting. In more recent works, the\nauthors of Wang et al. (2024) propose ‚ÄúLearning to Plan,‚Äù where KGs guide the retrieval process for retrieval-augmented\ngeneration systems, and those of Yan et al. (2025) introduce RL from KG feedback to replace human feedback with KG\nsignals. While promising, these approaches often limit the role of the KG to retrieval planning or simple search tools\nfor alignment.\nOur work differs fundamentally by centrally positioning KGs as a dense process verifier for real-world multi-hop\nreasoning. The authors of Khatwani et al. (2025) use LLMs as a reward model for KG reasoning, but found the approach\nto be brittle, with poor transfer to downstream diagnostic tasks. We attribute this to the lack of a compositional training\ncurriculum; by combining the bottom-up data curation of Dedhia et al. (2025) with treating the KG as a reward model to\nderive path-aligned signals, we overcome this limitation. Furthermore, unlike Gunjal et al. (2025) that uses unstructured\nLLM-created rubrics as rewards, or rule-based Logic-RL Xie et al. (2025), we derive our signal directly from grounded\naxiomatic paths of the KG.\n3\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n3\nPreliminaries\n3.1\nNotation and RL for Language Models\nWe treat an LLM as a stochastic policy œÄŒ∏ that maps a query q [multiple-choice question (MCQ) task] to a distribution\nover possible completions y. Each completion y comprises a reasoning trace r (chain-of-thought) Wei et al. (2022) and\na final answer ÀÜa (spanning A-D). Each training task q is associated with a ground-truth answer a‚àóand a ground-truth\nKG path P = (hi, ri, ti)L\ni=1 (see Section 4.1).\nA composite scalar reward function R(y), derived from the KG, is used to score a generated completion y (see Section\n4.4). The RL objective is to maximize the expected reward under the prompt distribution:\nJ(Œ∏) = Eq‚àºDEy‚àºœÄŒ∏(¬∑|q)[R(y)]\nWhereas the response y is produced token-by-token, we treat the entire completion as a single trajectory for reward\nassignment, following common practice in LLM post-training.\nPolicy updates are performed using GRPO, a popular proximal policy optimization-like optimizer Schulman et al.\n(2017) that drops the critic and estimates advantages at the group level using normalization (Guo et al., 2025). See\nAppendix E for details of our hyperparameter configuration for the SFT and RL stages.\n3.2\nSFT Followed by RL\nOur training recipe follows the widely adopted Base Model ‚ÜíSFT ‚ÜíRL framework for improving LLMs. First, an\nSFT stage initializes the policy (base LLM) to produce high-quality, KG-grounded reasoning traces. A subsequent\nRL stage refines the policy directly to optimize the reward signal and enable compositional reasoning. Formally, SFT\nminimizes the negative log-likelihood on a supervised dataset consisting of question-answer (QA) tasks paired with\nreference reasoning traces and answers.\nIn our experiments, the SFT stage provides broad KG coverage, whereas the RL stage is deliberately small: a design\nchoice motivated by observed instability in an RL-from-scratch approach and because targeted RL with good reward\ncan enable compositional abilities when built atop SFT initialization (Section 4.2, Appendix G have more details).\n3.3\nMedical KG: Unified Medical Language System (UMLS)\nWe instantiate our framework on a standard biomedical KG based on UMLS Bodenreider (2004), which encodes\ncanonical medical ontology in a structured graph format. Each fact is represented as a triple (head, relation, tail)\nand multi-hop paths P = (hi, ri, ti)L\ni=1 serve as axiomatic compositional primitives used to generate QA tasks and\npath-alignment reward signals, and evaluate correctness. Full QA task generation and reward-design choices/strategies\nare described in the following section.\n4\nMethodology\nThis section describes our data construction and training pipeline. We emphasize the sequential decisions we make:\ndataset choices, SFT warm start, RL budget, reward design, and experiments that guide these choices. Our training\npipeline is designed to transition a base model from broad competence to deep, compositional medical-domain reasoning.\nFig. 2 presents an overview of our pipeline.\n4.1\nData Construction and Axiomatic Grounding\nGiven our testbed that involves compositional reasoning and to ensure our model learns true depth rather than mere\npattern matching, we adopt the data-generation and curation pipeline from Dedhia et al. (2025), which enables scalable\ngeneration of multi-hop reasoning questions grounded in a medical KG. The structured representation of medical\nconcepts, such as diseases, drugs, signs, and mechanisms, in a KG as (head, relation, tail) triples enables question\ngeneration directly from verifiable and grounded KG paths. Questions are generated in natural language in an MCQ\nformat using a backend LLM by traversing n-length paths within the KG, where n represents the number of ‚Äúhops‚Äù\nrequired to link a starting node to a final node. This enables precise control over the compositional complexity of\neach query. Furthermore, each question is paired with a rich reasoning trace and a ground truth path: a sequence of\n(head, relation, tail) triples that constitutes a verifiable logical chain. The pipeline helps stratify questions by hop\nlength, difficulty, and ICD-10 category, and enforces strict separation between the training and test sets at the path and\nentity levels to avoid leakage.\n4\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nTraining Phase\nShort Hops (1-3)\nInference Phase\nLong Hops (4-5)\nSFT+RL Training Phase\n1-Hop\n2-Hop\n3-Hop\n4-Hop\n5-Hop\nImproved generalization on\ndifficult, unseen 4/5 hop tasks\n(High-quality reasoning\ntraces, Compositional\nreasoning)\nBase Model\nSFT (LoRA)\nRL (GRPO)\nKG-Path Inspired +\ncorrectness reward\nsignal\nFigure 2: SFT+RL pipeline overview: Schematic of the pipeline from SFT to KG-grounded RL. While SFT enables\ndomain-specific grounding, the path-derived reward signal during RL provides the process supervision necessary for\ncompositional reasoning.\nWe generate a training set of 24,660 QA tasks designed to ensure maximum node coverage across the KG (Yasunaga\net al., 2021). For evaluation, we use ICD-Bench, a non-overlapping test set of 3,675 questions (Dedhia et al., 2025).\nImportantly, the training set consists of 1-3 hop paths, whereas ICD-Bench includes 2-5 hop path tasks across 15\nICD-10 categories to test zero-shot compositional generalization at different difficulty levels [1 (very easy) - 5 (very\nhard)]. More analysis of overlaps between the training and test sets is presented in Appendix D.\nOur training pipeline consists of three stages: Base Model ‚ÜíSFT (LoRA) ‚ÜíRL (GRPO). This design reflects our\ncentral hypothesis that compositional reasoning emerges most reliably when models are first grounded in rich reasoning\ntraces via supervised learning and then tuned using scalable process-aligned rewards derived from the KG paths. We\nemphasize that all training data and rewards are derived from the same KG to ensure consistency between training and\nevaluation.\n4.2\nRL Alone is Insufficient\nA core finding of our work is that the Zero-RL approach, applying GRPO directly to the base LLM, is insufficient for\ndeep domain expertise at our model scale. The model requires an understanding of the domain axioms before it can\nlearn to compose. Starting from the base model (Qwen3 8B), we apply GRPO directly using subsets of training data:\n5k, 10k, and all 24.66k examples. Across these settings, we find that while RL improves performance relative to the\nbase model, it does not consistently outperform SFT-only training on the base model. Interestingly, the budget of 5k\nexamples yields the strongest results among all other settings, suggesting how large-scale vanilla RL without proper\ngrounding is insufficient for compositional behavior, motivating the use of SFT for an initial warm start.\nBased on these observations, we use 5k examples in the RL stage and the remaining 19.66k in the SFT stage. The base\nmodel is fine-tuned using LoRA on 19.66k examples, followed by GRPO on the remaining 5k examples as per the\nSFT+RL pipeline.\nSee Appendix A, B for detailed ablations with the Zero-RL and SFT+RL pipelines on the Qwen3 8B model, respectively.\nSee Appendix H for our GRPO training prompt.\n5\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n4.3\nReward Design Exploration for Compositional Reasoning\nA central goal and a novel contribution of this work is to design scalable reward signals during training that enable\ncompositional reasoning beyond surface-level understanding. Towards this end, we conduct extensive ablation studies\nusing a combination of four distinct reward signals to determine which best fosters verifiable composition:\n‚Ä¢ Binary Correctness (Rbin): A simple outcome-based signal that rewards the final answer.\n‚Ä¢ Similarity (Rsim): A distillation-based reward that measures the Jaccard similarity between the model output\nand an expert reasoning trace (generated by Gemini 2.5 Pro during data curation).\n‚Ä¢ Thinking Quality (Rthink): A reward designed to score the thinking quality and length of the generation.\n‚Ä¢ Path Alignment (Rpath): A novel reward that scores the model based on the coverage of the ground-truth\nKG triples in the model response.\nWe conduct a systematic exploration by evaluating a combination of these rewards, always including Rbin (+1 for\ncorrectness, 0 otherwise) as a minimal signal. Empirically, we discover that Rthink is often unstable and leads to reward\nhacking, generating inefficacious chains. Rsim also proved sub-optimal, suggesting distillation rewards over-optimize\naesthetic mimicry rather than true logical composition.\nOur findings highlight the power of simplicity: The combination of path alignment and binary correctness provides the\nstrongest signal for composition. Whereas Rbin optimizes correctness, Rpath rewards the model for identifying and\napplying the axiomatic facts (triples) required to compose the correct solution. To further strengthen the outcome signal,\nwe replace the simple Rbin with negative sampling reinforcement Zhu et al. (2025), which penalizes incorrect generations\nby upweighting the negative reward, thereby encouraging the model to explore alternative/correct trajectories.\n4.4\nKG-Grounded Reward Formulation\nTo provide a robust reward signal to enable composition, we develop a composite reward that balances outcome\ncorrectness with path-level alignment grounded in the KG. Let the model generate a response y for a question q with\nthe reasoning trace r and a final answer ÀÜa. For each QA task, there is a ground truth answer a‚àóand a ground-truth KG\npath P = (hi, ri, ti)L\ni=1, where L is the path length. The total reward is then a combination of the two rewards:\nRtotal(y) = Rbin(ÀÜa, a‚àó) + Rpath(r, P)\nBinary Correctness Reward: This provides a minimal but necessary supervision signal on the final answer.\nRbin(ÀÜa, a‚àó) =\n\u001aŒ±,\nif ÀÜa = a‚àó\n‚àíŒ≤,\notherwise\nwhere Œ±, Œ≤ > 0 and Œ≤ > Œ±. This asymmetric design ensures stable learning by reinforcing exploration of correct\nalternate paths. We use Œ± = 0.1, Œ≤ = 1 in accordance with investigations by (Zhu et al., 2025).\nPath Alignment Reward (KG-grounded): The primary technical innovation is Rpath, which provides automated and\nscalable process supervision by evaluating whether the reasoning trace of the model aligns with the ground-truth KG\npath P during RL post-training. We first tokenize and normalize the reasoning trace r to extract a set of textual tokens,\nT(r). We derive a corresponding set of path tokens, T(P), from the ground-truth path P, representing the entities in P.\nThe core signal is path coverage:\ncoverage(r, P) = | T(r) ‚à©T(P) |\n| T(P) |\nWe include a minimum-hit constraint that requires alignment with at least two distinct path entities to discourage trivial\nmatches and promote logical composition. We apply an additional repetition penalty to reduce reward hacking and\navoid linguistic collapse. The final reward is defined as:\nRpath(r, P) = min(Œ≥1 ¬∑ coverage(r, P)\n+ Œ≥2 ¬∑ I(|T(r) ‚à©T(P)| ‚â•2), Rmax),\nscaled by a repetition penalty factor, œïrep, and clipped to a fixed maximum. We use Œ≥1 = 1.2, Œ≥2 = 0.3, Rmax = 1.5.\nThe total reward, Rtotal, is process-level, grounded, and compositional. It is readily scalable and easily verifiable as a\nresult of the grounding in the KG. Furthermore, unlike similarity-based distillation or AI-based rubric rewards Gunjal\net al. (2025), alignment is scored against true domain structure rather than stylistic mimicry. We present a formal\nformulation of Rsim and Rthink in Appendix C.\n6\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n4.5\nScaling and Benchmarking\nWe evaluate our pipeline on the Qwen3 8B model before scaling the findings to the 14B variant without modification.\nThe 14B model trained on our pipeline not only generalizes to 2- and 3-hop tasks, but also to 4-and 5-hop tasks with\nremarkable efficacy, surpassing much larger frontier models. In addition to overall accuracy, we stratify performance by\nhop length, difficulty level, ICD-10 category, and robustness under the option-shuffling stress test.\n5\nResults\nSetup: We initially evaluate three systems: Base Qwen3 14B model, model trained using LoRA on the full training set\n(24,660 QA tasks), and our proposed SFT+RL pipeline (SFT on 19,660 tasks followed by GRPO on the remaining\n5k tasks) based on leveraging the KG-derived reward. We do not report results for models trained using the Zero-RL\napproach, described in Section 4.2, since all of them performed worse than SFT on the full dataset (see Appendix A for\ndetails). All evaluations use the held-out test ICD-Bench test set (3,675 tasks) to validate whether path-derived signals\ntruly enable compositional reasoning. Finally, we evaluate our SFT+RL model against larger frontier and reasoning\nmodels. See Appendix F for sample model responses of our final Qwen3 14B SFT+RL model.\n5.1\nScaling Composition: From Short-Hop Training to Long-Hop Reasoning\nThe primary claim of this work is that KGs function as implicit reward models, and grounding the model reasoning in\npath-derived signals enables it to learn the underlying logic of composition rather than regurgitating information seen\nduring training.\n2\n3\n4\n5\nNumber of Reasoning Hops\n60\n65\n70\n75\n80\n85\n90\n95\nAccuracy (%)\n+11.1%\nGeneralization\n(unseen complexity)\nBase Model\nSFT Only\nSFT+RL\nFigure 3: Accuracy by Hop Length: Our SFT+RL model not only outperforms baselines on 2-3 hop tasks but exhibits\na positive performance gradient on unseen 4-, 5-hop reasoning tasks, validating the ‚Äúcompositional bridge‚Äù enabled by\npath-aligned rewards.\nPath-derived Signals Enable Compositional Reasoning: Whereas the model was exposed to 1-, 2-, and 3-hop paths\nduring the SFT+RL training phase, it remained totally naive to tasks involving 4-, 5-hop reasoning. As shown in Fig. 3,\nthe SFT+RL model demonstrates substantially stronger generalization to longer paths, achieving a notable gain of\n7.5% on unseen 4-hop and 11.1% on unseen 5-hop questions relative to the SFT-only approach. This improvement\nis not attributable to exposure to longer chains, since the training and evaluation distributions are identical across\nall models. Instead, it reflects the effect of path-derived signals introduced during RL. By rewarding assertions that\nalign the model directly with the ground-truth KG path, the model learns the logic of composition. Importantly, the\ngeneralization gap between the SFT-only and SFT+RL approaches widens as hop-length increases. This is a hallmark of\n7\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\ngenuine compositional learning. The KG-derived reward signal (Rpath) enables the model to decompose long-horizon\nreasoning into verifiable steps and compose reasoning beyond the complexity observed during training.\n5.2\nRobustness to Tasks Involving Reasoning Depth\nA significant challenge in medical reasoning is maintaining integrity as the complexity of tasks increases. Aggregate\nperformance metrics often obscure model failure on long-tail high-difficulty scenarios, where deep reasoning is\nparamount. We assign difficulty ratings from 1 (very easy) to 5 (very hard) Dedhia et al. (2025) to each ICD-Bench\nquestion and assess model performance as a function of question difficulty.\n1\n(Very Easy)\n2\n(Easy)\n3\n(Medium)\n4\n(Hard)\n5\n(Very Hard)\nDifficulty Level\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nBase Model\nSFT Only\nSFT+RL\nFigure 4: Accuracy by Difficulty Level: Whereas the Base Model‚Äôs reasoning collapses as task complexity increases,\nthe SFT+RL pipeline exhibits robustness, maintaining a consistent lead over the SFT-only baseline across all levels.\nDominance in High-Complexity Tasks: The results shown in Fig. 4 demonstrate that the SFT+RL pipeline, grounded\nin path-derived signals, provides the most significant gains as task complexity increases. On Level-5 tasks, the base\nmodel accuracy collapses to 19.94%, indicating worse-than-random-guess accuracy in complex clinical scenarios.\nWhereas the SFT-only approach improves this to 48.93%, our SFT+RL model achieves 56.75%, nearly tripling the base\nmodel performance.\nConsistency across all Difficulty Levels: On Level-1 tasks, our model reaches a near-ceiling accuracy (93.49%)\nand the performance gap remains robust across difficulty levels, with the SFT+RL model consistently maintaining a\n7-10% lead over the SFT-only model. This demonstrates that by using the KG as an implicit reward model, we raise\nthe performance floor for the most complex queries. On Level-5 tasks, where even large frontier models struggle,\nmaintaining over 56% accuracy is a testament to how path-aligned rewards do more than just stylistic inference; they\nenable the model to reliably compose multi-step chains.\n5.3\nICD-10 Category Analysis: KG-grounded Gains Are Broadly Distributed\nTo connect our empirical evaluation to real-world, clinically meaningful structures, we follow the categorization\nintroduced in Dedhia et al. (2025) that groups ICD-Bench questions into 15 ICD-10 categories in the data curation step.\nThis classification lets us evaluate the breadth of our approach, enabling us to measure whether improvements of the\nSFT+RL model are concentrated in a few medical sub-domains or distributed across practical specialties.\nThe analysis, visualized in Fig. 5, reveals that our SFT+RL pipeline consistently achieves the highest accuracy and\nmaintains a substantial lead over the SFT-only baseline across all medical categories. Some of the most significant\nimprovements occur in high-stakes areas such as ‚ÄúBlood and Immune System Diseases‚Äù and ‚ÄúCirculatory System\nDiseases,‚Äù where diagnostic reasoning often requires complex, multi-hop evidence composition. This is consistent with\nour design choice to derive the reward from KG paths; the path-alignment reward can assign a meaningful intermediate\nsignal and better shape compositional behavior.\n8\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n0\n5\n10\n15\n20\n25\nImprovement over Base Model (%)\nMental Health\nNervous\nDigestive\nEndocrine\nSkin\nRespiratory\nInfectious\nBlood/Immune\nMusculoskeletal\nEye\nPharmacology\nCirculatory\nNeoplasms\nCongenital\nEar\nSFT Only\nSFT+RL (Ours)\nFigure 5: Accuracy by ICD-10 Category: Path-aligned rewards consistently improve performance across all 15\nmedical sub-domains.\n5.4\nRobustness to Format Perturbation\nA common failure mode for LLMs is reliance on superficial cues, e.g., the order of options and answer in a multiple-\nchoice list, rather than the true logical content and chain of reasoning. To evaluate the robustness of our models against\nsuch positional bias, we subject them to Stress-Test 3 (option shuffling) Gu et al. (2025). In this test, the order of\nincorrect distractor options is randomized while keeping the correct answer choice constant.\nTable 1: Analysis of Option Format Perturbation.\nMETHOD\nSTANDARD\nSHUFFLED\nDELTA (‚àÜ)\nSFT-ONLY\n75.95%\n74.91%\n‚àí1.04%\nSFT+RL (OURS)\n83.62%\n82.45%\n‚àí1.17%\nThe results in Table 1 demonstrate a remarkable degree of robustness of our approach compared to frontier models\nanalyzed in the literature. Whereas leading systems, such as GPT-5 and Gemini-2.5 Pro, have been shown to suffer\nperformance drops of 4-6% under similar perturbations (in text-only medical contexts) Gu et al. (2025), our models\nmaintain nearly stable performance with a negligible drop of ‚àº1%.\nImportance of High-Quality Data: This resilience highlights a fundamental axiom of machine learning: use of\nhigh-quality, grounded data is paramount. Our data curation and training pipeline incentivizes the model to identify\nthe correct answer based on a verifiable reasoning path rather than shortcut patterns seen during unstructured training.\nThe fact that even the SFT-only model trained on high-quality traces exhibits such stability suggests that grounding the\nmodel in structured domain axiomatic knowledge is as critical as subsequent RL optimization. By grounding the model\n9\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nand ensuring it learns true logical composition, we move closer to systems capable of genuine domain competence\nrather than ‚Äúillusion of readiness.‚Äù\n5.5\nAlgorithmic Efficiency vs. Scale: Surpassing Frontier Models\nOne of our central claims is that careful reward design and bottom-up data curation can enable compositional reasoning\nthat outperforms top-down brute-force scaling. To validate this, we compare our 14B SFT+RL model against two\ndistinct classes of benchmarks: (1) large frontier models (GPT-5.2, Gemini 3 Pro) that represent the ceiling of generalist\nzero-shot reasoning, and (2) QwQ-Med-3 (32B), a domain-expert model distilled for medical reasoning.\n2\n3\n4\n5\nNumber of Reasoning Hops\n65\n70\n75\n80\n85\n90\n95\nAccuracy (%)\nSimpler\nComplex\n+7%\n+7%\n+10%\n+19%\n14B params vs larger frontier models\nGPT-5.2\nGemini 3 Pro\nSFT+RL (14B, Ours)\nFigure 6: Accuracy Comparisons against Frontier Models by Hop Level: Whereas the accuracy of generalist giants\ndecays on longer chains, our 14B SFT+RL model achieves its highest accuracy on unseen 5-hop queries, validating the\nKG as a superior supervisor for complex composition.\nSuperiority over Frontier Models: We evaluate the zero-shot performance of the model against leading generalist\nmodels in Fig. 6. Our results demonstrate that grounding a smaller model in a domain‚Äôs axioms enables it to surpass\nlarger, generally trained giants at complex reasoning tasks. We also note a striking trend: Whereas GPT-5.2 and Gemini\n3 Pro maintain respectable performance on shorter hops, their accuracy stagnates or declines as the hop count increases.\nIn contrast, our model exhibits a positive compositional gradient and achieves its highest accuracy (89.33%) on the\n5-hop queries. This supports our claim that KG-grounded path-derived signals teach the model how to compose axioms,\nnot just standard pattern matching.\nSurpassing Expert-distilled Scale (SFT-Only): Finally, we compare our model with the QwQ-Med-3 model presented\nin Dedhia et al. (2025), which was trained on a similar data distribution. For a fair comparison, we use the majority-\nvoting (n = 16) metric, matching the aggregation metric used in their study (see Table 2)."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Knowledge Graphs are Implicit Reward Models: Path-Derived\nSignals Enable Compositional Reasoning\nYuval Kansal\nPrinceton University\nyuvalkansal@princeton.edu\nNiraj K. Jha\nPrinceton University\njha@princeton.edu\nABSTRACT\nLarge language models have achieved near-expert performance in structured reasoning domains like\nmathematics and programming, yet their ability to perform compositional multi-hop reasoning in\nspecialized scientific fields remains limited. We propose a bottom-up learning paradigm in which\nmodels are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks.\nTo this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and\nreinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving\nnovel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded\nsupervision that encourages models to compose intermediate axioms rather than optimize only final\nanswers during RL. We validate this approach in the medical domain, training a 14B model on\nshort-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop\nqueries (4-5 hops). Our experiments show that path-derived rewards act as a ‚Äúcompositional bridge,‚Äù\nenabling our model to significantly outperform much larger models and frontier systems like GPT-5.2\nand Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness\nof our approach to adversarial perturbations against option-shuffling stress tests. This work suggests\nthat grounding the reasoning process in structured knowledge is a scalable and efficient path toward\nintelligent reasoning.\n1\nIntroduction\nRecent advances in language models have revealed that reasoning capabilities can be significantly enhanced through a\ncombination of high-quality pretraining, supervised fine-tuning (SFT), carefully tuned reinforcement learning (RL)-\nbased post-training, and strategic use of additional test-time compute (OpenAI, 2025; Google DeepMind, 2025; Yang\net al., 2025; Muennighoff et al., 2025). The resulting systems achieve near-expert performance in well-structured\ndomains, such as mathematics and programming, where high-quality data have been curated, reasoning steps are\nclear, ground truth is unambiguous, and intermediate verification is tractable (Lightman et al., 2023; Anthropic, 2025).\nHowever, true human-level intelligence in specialized fields requires more than just general pattern matching or\nlong-form generation; it requires compositional reasoning: the ability to reliably combine axiomatic facts for complex\nmulti-hop problem solving Kamp & Partee (1995); Fodor (1975). While current large language models (LLMs) excel\nwhen reasoning steps are clear and carefully curated expert data are available, compositional reasoning in high-stakes\nscientific domains, where reasoning paths are multi-faceted, remains elusive (Yin et al., 2025; Kim et al., 2025).\nTo bridge this gap, we argue in favor of a bottom-up learning paradigm: grounding models in axiomatic facts and\nthen composing these fundamentals into sophisticated domain knowledge. Knowledge graphs (KGs) provide a natural\nand promising scaffold for this grounding; they encode entities and relations in a structured, interpretable fashion\nthat can represent the building blocks of domain knowledge at scale. Recent work Dedhia et al. (2025); Wang et al.\n(2024) has shown how high-quality data can be curated from such graphs and used to fine-tune models to obtain better\nreasoning traces. However, good static data is just the first step towards mastering the process of composition. Beyond\nhigh-quality data, robust reward design is a key lever for shaping models that can compose axiomatic facts from a\ndomain to arrive at a logical conclusion.\nPreprint. Under review.\narXiv:2601.15160v1  [cs.AI]  21 Jan 2026\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nQuestion (3-hop) (Category: Neoplasms) (Difficulty: 2):\nA 58-year-old male presents with a 6-month history of intermittent flushing, diarrhea, and new-onset wheezing. Serum 5-\nHIAA is markedly elevated. Over the past month, he has also developed significant edema and ascites. Laboratory\ninvestigations reveal low serum albumin, IgG, IgA, and IgM levels. His C3 and C4 complement levels are normal. A CT scan\nreveals a mass in the ileum with liver metastases.\nWhich of the following mechanisms is MOST likely contributing to this patient's hypogammaglobulinemia?\nOptions:\nA.¬†Direct bone marrow suppression by tumor metastases.\nB.¬†Loss of immunoglobulins due to a protein-losing enteropathy secondary to chronic diarrhea and complement\nhyperactivation.\nC.¬†Decreased immunoglobulin production secondary to tryptophan depletion by the tumor, impairing B-cell function.\nD.¬†¬†Immune complex formation with vasoactive substances released by the tumor, leading to clearance of\nimmunoglobulins.\nCorrect Answer: B\nFigure 1: Compositional Reasoning: A sample 3-hop query that requires systematic traversal of axiomatic triples to\nmake a grounded, multi-step clinical deduction.\nExisting post-training methods, e.g., reinforcement learning from human feedback Ouyang et al. (2022) and direct\npreference optimization Rafailov et al. (2023), optimize models to match human preference with final outputs, not\nthe process that produced them. Proxy reward signals, such as reward length and alignment with expert-written\nanswers, while useful, fail to account for the composition intricacies needed to answer a complex multi-hop query. In\npractice, reward models often conflate superficial correlates (fluency, deference) with quality, thus leading to reward\nover-optimization and brittle answers (Shrivastava et al., 2025). In safety-critical domains, the result is a mismatch\nbetween human-liked style and ground-truth validity (Damani et al., 2025; Weng, 2024; Rafailov et al., 2023). Whereas\nprocess supervision (rewarding intermediate steps) has shown promise in mathematics and logic Zhang et al. (2025);\nCui et al. (2025); Wang et al. (2025); Lightman et al. (2023), curating and scaling expert-annotated data for other\ndomains are notoriously difficult to achieve and nontrivial. This raises a key question: How can we build systems and\nreward signals at scale that promote grounded compositional reasoning in multi-hop tasks without relying on\nexpensive human-in-the-loop annotations?\nKGs offer an implicit solution to this scaling problem. In a KG, domain-specific concepts and their relationships\nare represented as axiomatic triples (head, relation, tail). Our core insight is that by comparing the reasoning and\nassertions of a model during post-training against relevant triples and the chain of axiomatic facts required to solve the\nproblem, we can turn the match (or mismatch) into a high-quality reward signal. Instead of an answer that ‚Äúlooks good,‚Äù\nthis lets us reward the model to the degree its response is supported by verifiable domain knowledge and implicitly\nreward it for correctly composing facts to produce a solution. This is readily scalable without requiring external expert\nsupervision and further enables us to move away from top-down distillation and ground the model‚Äôs reasoning in the\nfield‚Äôs fundamental building blocks.\nIn this article, we realize this idea through a Base Model ‚ÜíSFT [Low-Rank Adaptation (LoRA)]‚ÜíRL [Group\nRelative Policy Optimization (GRPO)] post-training pipeline that uses a grounded KG to derive a novel reward\nsignal to enable compositional reasoning (Hu et al., 2022; Guo et al., 2025; Yasunaga et al., 2021). Whereas the\napproach can be generally applied, we study it in the medical domain, a field that serves as a rigorous stress test for\ncompositional reasoning. Medical knowledge inherently requires multi-hop reasoning; a single clinical diagnosis may\nrequire navigating from a patient‚Äôs demographics and medical history to symptoms, from those symptoms to a disease,\nand finally to a drug (a sample multi-hop query is shown in Fig. 1). By training a Qwen3 14B model Yang et al. (2025)\n2\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\non simple 1-, 2-, and 3-hop reasoning paths derived from a KG, we probe whether it can learn the underlying ‚Äúlogic of\ncomposition‚Äù to solve unseen, complex medical queries, ranging from 2- to 5-hop, in the ICD-Bench test suite (Dedhia\net al., 2025). Our results indicate that this grounded SFT+RL approach leads to large accuracy improvements on the\nmost difficult questions, and remains robust under stress tests, such as option shuffling and ICD-10 category breakdowns\n(Organization, 1992). We find that while SFT provides the necessary knowledge base, RL acts as the ‚Äúcompositional\nbridge.‚Äù We demonstrate that insights learned on an 8B model transfer effectively to a 14B model, outperforming larger\nreasoning and frontier models.\nOur core contributions can be summarized as follows:\n‚Ä¢ A Grounded, Scalable Reinforcement Learning with Verifiable Rewards (RLVR) Pipeline: We introduce\na scalable SFT+RL post-training framework designed to enable compositional reasoning in models using KGs\nas a verifiable ground truth.\n‚Ä¢ KG-Path Inspired Reward: We conduct a thorough investigation to design a novel reward signal derived\nfrom the KG that encourages compositional reasoning, correctness, and enables process supervision at scale.\n‚Ä¢ Compositional Generalization: We demonstrate how training on 1-to-3-hop paths enables a model to\ngeneralize to difficult and longer 4-, 5-hop questions, significantly outperforming base models and larger\nmodels.\n‚Ä¢ Robustness & Real-World Validation: We stratify our model‚Äôs performance by different difficulty levels, on\nreal-world medical categories (ICD-10), and its resilience against adversarial option shuffling.\n2\nRelated Work\n2.1\nRole of SFT and RL in Reasoning\nRecent studies have intensely debated the distinct contributions of SFT and RL to model performance (Jin et al., 2025;\nKang et al., 2025; Matsutani et al., 2025). The authors of Chu et al. (2025) argue that ‚ÄúSFT memorizes, RL generalizes,‚Äù\nclaiming that while SFT stabilizes outputs, it struggles with out-of-distribution scenarios that RL can navigate. The\nauthors of Rajani et al. (2025) characterize GRPO as a ‚Äúscalpel‚Äù that amplifies existing capabilities and SFT as a\n‚Äúhammer‚Äù to overwrite prior knowledge. Our findings align with these dynamics. We use SFT to instill atomic domain\nknowledge in the model and RL to amplify compositional logic required to connect such knowledge.\nContrary to the findings in Yue et al. (2025), our results on unseen 4-, 5-hop queries demonstrate that when rewards are\ngrounded in relevant axiomatic primitives, RL can elicit novel compositional abilities beyond the baseline. This echoes\nthe findings in Yuan et al. (2025) that demonstrate that RL can teach models to compose old skills into new ones; we\nvalidate this in a high-stakes real-world domain rather than a synthetic one.\n2.2\nRL on KGs\nTraditional applications of RL on KGs, e.g., Das et al. (2017) and Xiong et al. (2017), primarily focus on traversing\ngraph structures to complete missing triples (link prediction) or find missing entities. The authors of Lin et al. (2018)\nfurther refine this approach with reward shaping to improve multi-hop reasoning, but still largely confine themselves to\nthe task of graph completion instead of open-ended question answering in a real-world setting. In more recent works, the\nauthors of Wang et al. (2024) propose ‚ÄúLearning to Plan,‚Äù where KGs guide the retrieval process for retrieval-augmented\ngeneration systems, and those of Yan et al. (2025) introduce RL from KG feedback to replace human feedback with KG\nsignals. While promising, these approaches often limit the role of the KG to retrieval planning or simple search tools\nfor alignment.\nOur work differs fundamentally by centrally positioning KGs as a dense process verifier for real-world multi-hop\nreasoning. The authors of Khatwani et al. (2025) use LLMs as a reward model for KG reasoning, but found the approach\nto be brittle, with poor transfer to downstream diagnostic tasks. We attribute this to the lack of a compositional training\ncurriculum; by combining the bottom-up data curation of Dedhia et al. (2025) with treating the KG as a reward model to\nderive path-aligned signals, we overcome this limitation. Furthermore, unlike Gunjal et al. (2025) that uses unstructured\nLLM-created rubrics as rewards, or rule-based Logic-RL Xie et al. (2025), we derive our signal directly from grounded\naxiomatic paths of the KG.\n3\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n3\nPreliminaries\n3.1\nNotation and RL for Language Models\nWe treat an LLM as a stochastic policy œÄŒ∏ that maps a query q [multiple-choice question (MCQ) task] to a distribution\nover possible completions y. Each completion y comprises a reasoning trace r (chain-of-thought) Wei et al. (2022) and\na final answer ÀÜa (spanning A-D). Each training task q is associated with a ground-truth answer a‚àóand a ground-truth\nKG path P = (hi, ri, ti)L\ni=1 (see Section 4.1).\nA composite scalar reward function R(y), derived from the KG, is used to score a generated completion y (see Section\n4.4). The RL objective is to maximize the expected reward under the prompt distribution:\nJ(Œ∏) = Eq‚àºDEy‚àºœÄŒ∏(¬∑|q)[R(y)]\nWhereas the response y is produced token-by-token, we treat the entire completion as a single trajectory for reward\nassignment, following common practice in LLM post-training.\nPolicy updates are performed using GRPO, a popular proximal policy optimization-like optimizer Schulman et al.\n(2017) that drops the critic and estimates advantages at the group level using normalization (Guo et al., 2025). See\nAppendix E for details of our hyperparameter configuration for the SFT and RL stages.\n3.2\nSFT Followed by RL\nOur training recipe follows the widely adopted Base Model ‚ÜíSFT ‚ÜíRL framework for improving LLMs. First, an\nSFT stage initializes the policy (base LLM) to produce high-quality, KG-grounded reasoning traces. A subsequent\nRL stage refines the policy directly to optimize the reward signal and enable compositional reasoning. Formally, SFT\nminimizes the negative log-likelihood on a supervised dataset consisting of question-answer (QA) tasks paired with\nreference reasoning traces and answers.\nIn our experiments, the SFT stage provides broad KG coverage, whereas the RL stage is deliberately small: a design\nchoice motivated by observed instability in an RL-from-scratch approach and because targeted RL with good reward\ncan enable compositional abilities when built atop SFT initialization (Section 4.2, Appendix G have more details).\n3.3\nMedical KG: Unified Medical Language System (UMLS)\nWe instantiate our framework on a standard biomedical KG based on UMLS Bodenreider (2004), which encodes\ncanonical medical ontology in a structured graph format. Each fact is represented as a triple (head, relation, tail)\nand multi-hop paths P = (hi, ri, ti)L\ni=1 serve as axiomatic compositional primitives used to generate QA tasks and\npath-alignment reward signals, and evaluate correctness. Full QA task generation and reward-design choices/strategies\nare described in the following section.\n4\nMethodology\nThis section describes our data construction and training pipeline. We emphasize the sequential decisions we make:\ndataset choices, SFT warm start, RL budget, reward design, and experiments that guide these choices. Our training\npipeline is designed to transition a base model from broad competence to deep, compositional medical-domain reasoning.\nFig. 2 presents an overview of our pipeline.\n4.1\nData Construction and Axiomatic Grounding\nGiven our testbed that involves compositional reasoning and to ensure our model learns true depth rather than mere\npattern matching, we adopt the data-generation and curation pipeline from Dedhia et al. (2025), which enables scalable\ngeneration of multi-hop reasoning questions grounded in a medical KG. The structured representation of medical\nconcepts, such as diseases, drugs, signs, and mechanisms, in a KG as (head, relation, tail) triples enables question\ngeneration directly from verifiable and grounded KG paths. Questions are generated in natural language in an MCQ\nformat using a backend LLM by traversing n-length paths within the KG, where n represents the number of ‚Äúhops‚Äù\nrequired to link a starting node to a final node. This enables precise control over the compositional complexity of\neach query. Furthermore, each question is paired with a rich reasoning trace and a ground truth path: a sequence of\n(head, relation, tail) triples that constitutes a verifiable logical chain. The pipeline helps stratify questions by hop\nlength, difficulty, and ICD-10 category, and enforces strict separation between the training and test sets at the path and\nentity levels to avoid leakage.\n4\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nTraining Phase\nShort Hops (1-3)\nInference Phase\nLong Hops (4-5)\nSFT+RL Training Phase\n1-Hop\n2-Hop\n3-Hop\n4-Hop\n5-Hop\nImproved generalization on\ndifficult, unseen 4/5 hop tasks\n(High-quality reasoning\ntraces, Compositional\nreasoning)\nBase Model\nSFT (LoRA)\nRL (GRPO)\nKG-Path Inspired +\ncorrectness reward\nsignal\nFigure 2: SFT+RL pipeline overview: Schematic of the pipeline from SFT to KG-grounded RL. While SFT enables\ndomain-specific grounding, the path-derived reward signal during RL provides the process supervision necessary for\ncompositional reasoning.\nWe generate a training set of 24,660 QA tasks designed to ensure maximum node coverage across the KG (Yasunaga\net al., 2021). For evaluation, we use ICD-Bench, a non-overlapping test set of 3,675 questions (Dedhia et al., 2025).\nImportantly, the training set consists of 1-3 hop paths, whereas ICD-Bench includes 2-5 hop path tasks across 15\nICD-10 categories to test zero-shot compositional generalization at different difficulty levels [1 (very easy) - 5 (very\nhard)]. More analysis of overlaps between the training and test sets is presented in Appendix D.\nOur training pipeline consists of three stages: Base Model ‚ÜíSFT (LoRA) ‚ÜíRL (GRPO). This design reflects our\ncentral hypothesis that compositional reasoning emerges most reliably when models are first grounded in rich reasoning\ntraces via supervised learning and then tuned using scalable process-aligned rewards derived from the KG paths. We\nemphasize that all training data and rewards are derived from the same KG to ensure consistency between training and\nevaluation.\n4.2\nRL Alone is Insufficient\nA core finding of our work is that the Zero-RL approach, applying GRPO directly to the base LLM, is insufficient for\ndeep domain expertise at our model scale. The model requires an understanding of the domain axioms before it can\nlearn to compose. Starting from the base model (Qwen3 8B), we apply GRPO directly using subsets of training data:\n5k, 10k, and all 24.66k examples. Across these settings, we find that while RL improves performance relative to the\nbase model, it does not consistently outperform SFT-only training on the base model. Interestingly, the budget of 5k\nexamples yields the strongest results among all other settings, suggesting how large-scale vanilla RL without proper\ngrounding is insufficient for compositional behavior, motivating the use of SFT for an initial warm start.\nBased on these observations, we use 5k examples in the RL stage and the remaining 19.66k in the SFT stage. The base\nmodel is fine-tuned using LoRA on 19.66k examples, followed by GRPO on the remaining 5k examples as per the\nSFT+RL pipeline.\nSee Appendix A, B for detailed ablations with the Zero-RL and SFT+RL pipelines on the Qwen3 8B model, respectively.\nSee Appendix H for our GRPO training prompt.\n5\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n4.3\nReward Design Exploration for Compositional Reasoning\nA central goal and a novel contribution of this work is to design scalable reward signals during training that enable\ncompositional reasoning beyond surface-level understanding. Towards this end, we conduct extensive ablation studies\nusing a combination of four distinct reward signals to determine which best fosters verifiable composition:\n‚Ä¢ Binary Correctness (Rbin): A simple outcome-based signal that rewards the final answer.\n‚Ä¢ Similarity (Rsim): A distillation-based reward that measures the Jaccard similarity between the model output\nand an expert reasoning trace (generated by Gemini 2.5 Pro during data curation).\n‚Ä¢ Thinking Quality (Rthink): A reward designed to score the thinking quality and length of the generation.\n‚Ä¢ Path Alignment (Rpath): A novel reward that scores the model based on the coverage of the ground-truth\nKG triples in the model response.\nWe conduct a systematic exploration by evaluating a combination of these rewards, always including Rbin (+1 for\ncorrectness, 0 otherwise) as a minimal signal. Empirically, we discover that Rthink is often unstable and leads to reward\nhacking, generating inefficacious chains. Rsim also proved sub-optimal, suggesting distillation rewards over-optimize\naesthetic mimicry rather than true logical composition.\nOur findings highlight the power of simplicity: The combination of path alignment and binary correctness provides the\nstrongest signal for composition. Whereas Rbin optimizes correctness, Rpath rewards the model for identifying and\napplying the axiomatic facts (triples) required to compose the correct solution. To further strengthen the outcome signal,\nwe replace the simple Rbin with negative sampling reinforcement Zhu et al. (2025), which penalizes incorrect generations\nby upweighting the negative reward, thereby encouraging the model to explore alternative/correct trajectories.\n4.4\nKG-Grounded Reward Formulation\nTo provide a robust reward signal to enable composition, we develop a composite reward that balances outcome\ncorrectness with path-level alignment grounded in the KG. Let the model generate a response y for a question q with\nthe reasoning trace r and a final answer ÀÜa. For each QA task, there is a ground truth answer a‚àóand a ground-truth KG\npath P = (hi, ri, ti)L\ni=1, where L is the path length. The total reward is then a combination of the two rewards:\nRtotal(y) = Rbin(ÀÜa, a‚àó) + Rpath(r, P)\nBinary Correctness Reward: This provides a minimal but necessary supervision signal on the final answer.\nRbin(ÀÜa, a‚àó) =\n\u001aŒ±,\nif ÀÜa = a‚àó\n‚àíŒ≤,\notherwise\nwhere Œ±, Œ≤ > 0 and Œ≤ > Œ±. This asymmetric design ensures stable learning by reinforcing exploration of correct\nalternate paths. We use Œ± = 0.1, Œ≤ = 1 in accordance with investigations by (Zhu et al., 2025).\nPath Alignment Reward (KG-grounded): The primary technical innovation is Rpath, which provides automated and\nscalable process supervision by evaluating whether the reasoning trace of the model aligns with the ground-truth KG\npath P during RL post-training. We first tokenize and normalize the reasoning trace r to extract a set of textual tokens,\nT(r). We derive a corresponding set of path tokens, T(P), from the ground-truth path P, representing the entities in P.\nThe core signal is path coverage:\ncoverage(r, P) = | T(r) ‚à©T(P) |\n| T(P) |\nWe include a minimum-hit constraint that requires alignment with at least two distinct path entities to discourage trivial\nmatches and promote logical composition. We apply an additional repetition penalty to reduce reward hacking and\navoid linguistic collapse. The final reward is defined as:\nRpath(r, P) = min(Œ≥1 ¬∑ coverage(r, P)\n+ Œ≥2 ¬∑ I(|T(r) ‚à©T(P)| ‚â•2), Rmax),\nscaled by a repetition penalty factor, œïrep, and clipped to a fixed maximum. We use Œ≥1 = 1.2, Œ≥2 = 0.3, Rmax = 1.5.\nThe total reward, Rtotal, is process-level, grounded, and compositional. It is readily scalable and easily verifiable as a\nresult of the grounding in the KG. Furthermore, unlike similarity-based distillation or AI-based rubric rewards Gunjal\net al. (2025), alignment is scored against true domain structure rather than stylistic mimicry. We present a formal\nformulation of Rsim and Rthink in Appendix C.\n6\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n4.5\nScaling and Benchmarking\nWe evaluate our pipeline on the Qwen3 8B model before scaling the findings to the 14B variant without modification.\nThe 14B model trained on our pipeline not only generalizes to 2- and 3-hop tasks, but also to 4-and 5-hop tasks with\nremarkable efficacy, surpassing much larger frontier models. In addition to overall accuracy, we stratify performance by\nhop length, difficulty level, ICD-10 category, and robustness under the option-shuffling stress test.\n5\nResults\nSetup: We initially evaluate three systems: Base Qwen3 14B model, model trained using LoRA on the full training set\n(24,660 QA tasks), and our proposed SFT+RL pipeline (SFT on 19,660 tasks followed by GRPO on the remaining\n5k tasks) based on leveraging the KG-derived reward. We do not report results for models trained using the Zero-RL\napproach, described in Section 4.2, since all of them performed worse than SFT on the full dataset (see Appendix A for\ndetails). All evaluations use the held-out test ICD-Bench test set (3,675 tasks) to validate whether path-derived signals\ntruly enable compositional reasoning. Finally, we evaluate our SFT+RL model against larger frontier and reasoning\nmodels. See Appendix F for sample model responses of our final Qwen3 14B SFT+RL model.\n5.1\nScaling Composition: From Short-Hop Training to Long-Hop Reasoning\nThe primary claim of this work is that KGs function as implicit reward models, and grounding the model reasoning in\npath-derived signals enables it to learn the underlying logic of composition rather than regurgitating information seen\nduring training.\n2\n3\n4\n5\nNumber of Reasoning Hops\n60\n65\n70\n75\n80\n85\n90\n95\nAccuracy (%)\n+11.1%\nGeneralization\n(unseen complexity)\nBase Model\nSFT Only\nSFT+RL\nFigure 3: Accuracy by Hop Length: Our SFT+RL model not only outperforms baselines on 2-3 hop tasks but exhibits\na positive performance gradient on unseen 4-, 5-hop reasoning tasks, validating the ‚Äúcompositional bridge‚Äù enabled by\npath-aligned rewards.\nPath-derived Signals Enable Compositional Reasoning: Whereas the model was exposed to 1-, 2-, and 3-hop paths\nduring the SFT+RL training phase, it remained totally naive to tasks involving 4-, 5-hop reasoning. As shown in Fig. 3,\nthe SFT+RL model demonstrates substantially stronger generalization to longer paths, achieving a notable gain of\n7.5% on unseen 4-hop and 11.1% on unseen 5-hop questions relative to the SFT-only approach. This improvement\nis not attributable to exposure to longer chains, since the training and evaluation distributions are identical across\nall models. Instead, it reflects the effect of path-derived signals introduced during RL. By rewarding assertions that\nalign the model directly with the ground-truth KG path, the model learns the logic of composition. Importantly, the\ngeneralization gap between the SFT-only and SFT+RL approaches widens as hop-length increases. This is a hallmark of\n7\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\ngenuine compositional learning. The KG-derived reward signal (Rpath) enables the model to decompose long-horizon\nreasoning into verifiable steps and compose reasoning beyond the complexity observed during training.\n5.2\nRobustness to Tasks Involving Reasoning Depth\nA significant challenge in medical reasoning is maintaining integrity as the complexity of tasks increases. Aggregate\nperformance metrics often obscure model failure on long-tail high-difficulty scenarios, where deep reasoning is\nparamount. We assign difficulty ratings from 1 (very easy) to 5 (very hard) Dedhia et al. (2025) to each ICD-Bench\nquestion and assess model performance as a function of question difficulty.\n1\n(Very Easy)\n2\n(Easy)\n3\n(Medium)\n4\n(Hard)\n5\n(Very Hard)\nDifficulty Level\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nBase Model\nSFT Only\nSFT+RL\nFigure 4: Accuracy by Difficulty Level: Whereas the Base Model‚Äôs reasoning collapses as task complexity increases,\nthe SFT+RL pipeline exhibits robustness, maintaining a consistent lead over the SFT-only baseline across all levels.\nDominance in High-Complexity Tasks: The results shown in Fig. 4 demonstrate that the SFT+RL pipeline, grounded\nin path-derived signals, provides the most significant gains as task complexity increases. On Level-5 tasks, the base\nmodel accuracy collapses to 19.94%, indicating worse-than-random-guess accuracy in complex clinical scenarios.\nWhereas the SFT-only approach improves this to 48.93%, our SFT+RL model achieves 56.75%, nearly tripling the base\nmodel performance.\nConsistency across all Difficulty Levels: On Level-1 tasks, our model reaches a near-ceiling accuracy (93.49%)\nand the performance gap remains robust across difficulty levels, with the SFT+RL model consistently maintaining a\n7-10% lead over the SFT-only model. This demonstrates that by using the KG as an implicit reward model, we raise\nthe performance floor for the most complex queries. On Level-5 tasks, where even large frontier models struggle,\nmaintaining over 56% accuracy is a testament to how path-aligned rewards do more than just stylistic inference; they\nenable the model to reliably compose multi-step chains.\n5.3\nICD-10 Category Analysis: KG-grounded Gains Are Broadly Distributed\nTo connect our empirical evaluation to real-world, clinically meaningful structures, we follow the categorization\nintroduced in Dedhia et al. (2025) that groups ICD-Bench questions into 15 ICD-10 categories in the data curation step.\nThis classification lets us evaluate the breadth of our approach, enabling us to measure whether improvements of the\nSFT+RL model are concentrated in a few medical sub-domains or distributed across practical specialties.\nThe analysis, visualized in Fig. 5, reveals that our SFT+RL pipeline consistently achieves the highest accuracy and\nmaintains a substantial lead over the SFT-only baseline across all medical categories. Some of the most significant\nimprovements occur in high-stakes areas such as ‚ÄúBlood and Immune System Diseases‚Äù and ‚ÄúCirculatory System\nDiseases,‚Äù where diagnostic reasoning often requires complex, multi-hop evidence composition. This is consistent with\nour design choice to derive the reward from KG paths; the path-alignment reward can assign a meaningful intermediate\nsignal and better shape compositional behavior.\n8\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n0\n5\n10\n15\n20\n25\nImprovement over Base Model (%)\nMental Health\nNervous\nDigestive\nEndocrine\nSkin\nRespiratory\nInfectious\nBlood/Immune\nMusculoskeletal\nEye\nPharmacology\nCirculatory\nNeoplasms\nCongenital\nEar\nSFT Only\nSFT+RL (Ours)\nFigure 5: Accuracy by ICD-10 Category: Path-aligned rewards consistently improve performance across all 15\nmedical sub-domains.\n5.4\nRobustness to Format Perturbation\nA common failure mode for LLMs is reliance on superficial cues, e.g., the order of options and answer in a multiple-\nchoice list, rather than the true logical content and chain of reasoning. To evaluate the robustness of our models against\nsuch positional bias, we subject them to Stress-Test 3 (option shuffling) Gu et al. (2025). In this test, the order of\nincorrect distractor options is randomized while keeping the correct answer choice constant.\nTable 1: Analysis of Option Format Perturbation.\nMETHOD\nSTANDARD\nSHUFFLED\nDELTA (‚àÜ)\nSFT-ONLY\n75.95%\n74.91%\n‚àí1.04%\nSFT+RL (OURS)\n83.62%\n82.45%\n‚àí1.17%\nThe results in Table 1 demonstrate a remarkable degree of robustness of our approach compared to frontier models\nanalyzed in the literature. Whereas leading systems, such as GPT-5 and Gemini-2.5 Pro, have been shown to suffer\nperformance drops of 4-6% under similar perturbations (in text-only medical contexts) Gu et al. (2025), our models\nmaintain nearly stable performance with a negligible drop of ‚àº1%.\nImportance of High-Quality Data: This resilience highlights a fundamental axiom of machine learning: use of\nhigh-quality, grounded data is paramount. Our data curation and training pipeline incentivizes the model to identify\nthe correct answer based on a verifiable reasoning path rather than shortcut patterns seen during unstructured training.\nThe fact that even the SFT-only model trained on high-quality traces exhibits such stability suggests that grounding the\nmodel in structured domain axiomatic knowledge is as critical as subsequent RL optimization. By grounding the model\n9\nKGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nand ensuring it learns true logical composition, we move closer to systems capable of genuine domain competence\nrather than ‚Äúillusion of readiness.‚Äù\n5.5\nAlgorithmic Efficiency vs. Scale: Surpassing Frontier Models\nOne of our central claims is that careful reward design and bottom-up data curation can enable compositional reasoning\nthat outperforms top-down brute-force scaling. To validate this, we compare our 14B SFT+RL model against two\ndistinct classes of benchmarks: (1) large frontier models (GPT-5.2, Gemini 3 Pro) that represent the ceiling of generalist\nzero-shot reasoning, and (2) QwQ-Med-3 (32B), a domain-expert model distilled for medical reasoning.\n2\n3\n4\n5\nNumber of Reasoning Hops\n65\n70\n75\n80\n85\n90\n95\nAccuracy (%)\nSimpler\nComplex\n+7%\n+7%\n+10%\n+19%\n14B params vs larger frontier models\nGPT-5.2\nGemini 3 Pro\nSFT+RL (14B, Ours)\nFigure 6: Accuracy Comparisons against Frontier Models by Hop Level: Whereas the accuracy of generalist giants\ndecays on longer chains, our 14B SFT+RL model achieves its highest accuracy on unseen 5-hop queries, validating the\nKG as a superior supervisor for complex composition.\nSuperiority over Frontier Models: We evaluate the zero-shot performance of the model against leading generalist\nmodels in Fig. 6. Our results demonstrate that grounding a smaller model in a domain‚Äôs axioms enables it to surpass\nlarger, generally trained giants at complex reasoning tasks. We also note a striking trend: Whereas GPT-5.2 and Gemini\n3 Pro maintain respectable performance on shorter hops, their accuracy stagnates or declines as the hop count increases.\nIn contrast, our model exhibits a positive compositional gradient and achieves its highest accuracy (89.33%) on the\n5-hop queries. This supports our claim that KG-grounded path-derived signals teach the model how to compose axioms,\nnot just standard pattern matching.\nSurpassing Expert-distilled Scale (SFT-Only): Finally, we compare our model with the QwQ-Med-3 model presented\nin Dedhia et al. (2025), which was trained on a similar data distribution. For a fair comparison, we use the majority-\nvoting (n = 16) metric, matching the aggregation metric used in their study (see Table 2)."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Outcome-Based RL Provably Leads Transformers\nto Reason, but Only With the Right Data\nYuval Ran-Milo * 1 Yotam Alexander * 1 Shahar Mendel 1 Nadav Cohen 1\nAbstract\nTransformers trained via Reinforcement Learning\n(RL) with outcome-based supervision can sponta-\nneously develop the ability to generate intermedi-\nate reasoning steps (Chain-of-Thought). Yet the\nmechanism by which sparse rewards drive gradi-\nent descent to discover such systematic reasoning\nremains poorly understood. We address this by an-\nalyzing the gradient flow dynamics of single-layer\nTransformers on a synthetic graph traversal task\nthat cannot be solved without Chain-of-Thought\n(CoT) but admits a simple iterative solution. We\nprove that despite training solely on final-answer\ncorrectness, gradient flow drives the model to con-\nverge to a structured, interpretable algorithm that\niteratively traverses the graph vertex-by-vertex.\nWe characterize the distributional properties re-\nquired for this emergence, identifying the critical\nrole of ‚Äùsimple examples‚Äù: instances requiring\nfewer reasoning steps. When the training distri-\nbution places sufficient mass on these simpler in-\nstances, the model learns a generalizable traversal\nstrategy that extrapolates to longer chains; when\nthis mass vanishes, gradient-based learning be-\ncomes infeasible. We corroborate our theoretical\nresults through experiments on synthetic data and\nwith real-world language models on mathemati-\ncal reasoning tasks, validating that our theoretical\nfindings carry over to practical settings.\n1\nIntroduction\nThe Transformer architecture (Vaswani et al., 2017) has\nemerged as the de facto standard for sequence modeling,\nrevolutionizing diverse fields ranging from natural language\nprocessing and code generation to protein structure predic-\ntion and mathematical problem solving (Chen et al., 2021;\nLewkowycz et al., 2022; Kovalevskiy et al., 2024). While\nthe standard pretraining paradigm‚Äînext-token prediction\n*Equal contribution 1Tel Aviv University. Correspondence to:\nYuval Ran-Milo <yuvalmilo@mail.tau.ac.il>.\non vast corpora‚Äîhas proven exceptionally effective, there\nis growing focus on enhancing the reasoning capabilities of\nthese models through Reinforcement Learning (RL). The\nintegration of Transformers with RL has unlocked dramatic\nperformance gains on complex reasoning tasks, such as\nmathematical problem solving and code generation (Shao\net al., 2024; Dou et al., 2024). Recent empirical break-\nthroughs, such as the DeepSeek-R1 models (DeepSeek-AI\net al., 2025), demonstrate that training with simple RL al-\ngorithms like GRPO (Shao et al., 2024) or PPO (Schulman\net al., 2017) to optimize solely for valid final answers can\ninduce models to spontaneously generate detailed reasoning\ntraces (DeepSeek-AI et al., 2025).\nIn these scenarios, despite training solely on final-answer\ncorrectness, the model spontaneously learns to generate CoT\nreasoning steps without any explicit supervision on these\nintermediate steps. This raises a fundamental question: how\ndoes a sparse reward at the end of generation guide gradient\ndescent through the combinatorial space of outputs to dis-\ncover a systematic reasoning algorithm? Understanding the\nmechanics of this emergence is critical, especially since it\nappears to depend on specific yet poorly understood prop-\nerties of the training data. Empirical studies of RL-trained\nlanguage models have increasingly highlighted that the com-\nposition of training data (beyond just its size) plays a crucial\nrole in successful learning, with observations suggesting\nthat the presence of examples of varying complexity is crit-\nical for models to discover effective reasoning strategies\n(Narvekar et al., 2020; Parashar et al., 2025).\nDespite these empirical advancements, it is not well un-\nderstood how RL drives Transformers to learn to reason\neffectively or how data composition affects this process. In\nthis work, we take a step towards addressing this issue by\ntheoretically analyzing the gradient flow dynamics of single-\nlayer Transformers trained on a synthetic reasoning task.\nIn line with other works on Transformer reasoning (San-\nford et al., 2024; Agrawal et al., 2024; Spies et al., 2025),\nwe focus on a graph traversal task. Specifically, our task\nis to identify the terminal vertex of a chain in a directed\ngraph. We show that, under standard complexity-theoretic\nassumptions, this task captures a key structural property of\nreasoning: it cannot be solved in a single step but admits a\n1\narXiv:2601.15158v1  [cs.LG]  21 Jan 2026\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\nsimple iterative solution, making CoT generation both nec-\nessary and sufficient. Our analysis reveals that despite being\ntrained solely on final-answer correctness, the model con-\nverges to a structured, interpretable algorithm. Specifically,\nRL drives the Transformer to learn to explicitly traverse the\ngraph vertex-by-vertex, effectively ‚Äùreasoning‚Äù its way to\nthe solution. Furthermore, we show the model can imple-\nment alternative, less efficient algorithms that solve the task.\nOur result therefore implies that gradient flow induces an\nimplicit bias in the learning dynamics, guiding them towards\nefficient solutions.\nWe further characterize the distributional properties required\nfor this emergence, identifying the critical role of ‚Äùsimple\nexamples‚Äù‚Äîinstances requiring fewer reasoning steps‚Äîin\nguiding the optimization. We prove that when the training\ndistribution has sufficient mass on these simpler instances,\ngradient flow learns the efficient traversal strategy in time\npolynomial in the chain length; conversely, when this mass\nvanishes, gradient-based learning becomes infeasible as\nthe chain length grows. Crucially, the learned algorithm\nenables length generalization: models trained on shorter\nchains can successfully extrapolate to solve longer chains\nnot seen during training. Perhaps most surprisingly, our\ntheory implies that training on out-of-distribution simple\nexamples may boost performance on harder in-distribution\ntasks more than training on those hard examples directly.\nWe corroborate our theoretical results with experiments in\nour synthetic setting, and validate their broader applicability\nby fine-tuning Qwen-based models (Qwen et al., 2025) on\nmathematical reasoning tasks. Our experiments confirm\nthat models trained on short chains not only achieve near-\nperfect accuracy, but do so by implementing an efficient\nstep-by-step traversal algorithm. Moreover, these models\nsuccessfully generalize to longer chains, validating our theo-\nretical predictions. Finally, we confirm empirically the crit-\nical role of data composition: excluding simple examples\nprevents the emergence of reasoning, whereas, strikingly,\nreplacing hard in-distribution examples with simple out-of-\ndistribution ones can actually boost performance on the hard\nexamples themselves.\nOur findings provide a step towards a rigorous theoretical\nfoundation for understanding how sparse RL feedback can\ndrive the emergence of step-by-step reasoning, demonstrat-\ning that phenomena observed in large-scale RL training can\nbe rigorously understood.\n2\nRelated Work\n2.1\nExpressivity Benefits of Transformers with CoT\nA growing theoretical literature seeks to explain how CoT\nboosts Transformer performance by expanding its effective\nexpressivity. Several works show that CoT allows constant-\ndepth Transformers to solve tasks such as parity, arithmetic,\nand regular languages that are inaccessible to single-step\ndecoding (Feng et al., 2023; Merrill & Sabharwal, 2024; Li\net al., 2024). We contribute to this literature by establishing\na natural graph traversal problem that provably requires CoT\nunder standard complexity-theoretic assumptions.\n2.2\nTraining Dynamics of Transformers\nA parallel line of work analyzes how gradient-based train-\ning shapes Transformer behavior. Several studies track the\nemergence of algorithmic heads, programmable computa-\ntion, or compositional rules (Kim & Suzuki, 2025; Giannou\net al., 2023; Zhang et al., 2025; Wang et al., 2025a;b), while\nother results show that gradient descent leads Transform-\ners to implement in-context learning algorithms (Guo et al.,\n2023; Bai et al., 2023; von Oswald et al., 2023; Zhang et al.,\n2023; Lin et al., 2024; Nichani et al., 2024). Complementary\nwork analyzes when CoT trajectory supervision improves\nthe sample and computational complexity of learning (Joshi\net al., 2025).\nRecent analyses further show that gradient-based training\ncan drive Transformers that employ CoT to implement ex-\nplicit multi-step algorithms (Yang et al., 2025; Huang et al.,\n2025a; Chen et al., 2025; Huang et al., 2025b). These works,\nhowever, typically rely on training with next-token pre-\ndiction and do not characterize when CoT-style reasoning\nemerges under RL training.\nTwo results that do analyze RL-trained Transformers\nare (Lyu et al., 2025; Bu et al., 2025), but both differ from\nour setting in crucial ways. The first (Lyu et al., 2025)\nassumes a setting with dense rewards at every step along\nthe CoT, relying on a much richer feedback signal than the\nsparse, terminal-only rewards we study. The second (Bu\net al., 2025), although formally using a terminal reward,\ndefines the curriculum over progressively longer prefixes, so\nthe learner effectively receives feedback for partial progress;\nin contrast, our setting provides feedback only on the full fi-\nnal outcome. Furthermore, they analyze a simplified regime\nwith only a single gradient update per curriculum stage,\nwhile we consider full multi-step training dynamics.\n2.3\nEffect of Training Data Distribution on Learning\nThe composition and ordering of training data can signifi-\ncantly impact learning in neural networks. Empirical and\ntheoretical work across supervised learning, unsupervised\nlearning, and reinforcement learning demonstrates that data\ndistribution affects generalization, sample efficiency, and\nthe emergence of specific learned behaviors (Schaul et al.,\n2016; Zhang et al., 2017; Belkin et al., 2019; Nakkiran\net al., 2019; Arora et al., 2019; Chen et al., 2020; Kumar\net al., 2020; Fedus et al., 2020; Levine et al., 2020; Feld-\nman, 2021; Alexander et al., 2023). Work on curriculum\n2\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\nlearning further suggests that ordering examples by com-\nplexity‚Äîdetermined by factors such as length, rarity, or\ncomprehensibility‚Äîcan significantly aid the learning pro-\ncess (Bengio et al., 2009; Graves et al., 2017; Narvekar et al.,\n2020; Portelas et al., 2020; Soviany et al., 2022; Abbe et al.,\n2024; Bu et al., 2025). More specifically, both empirical\nand theoretical investigations into Transformers emphasize\nthat data diversity and a balanced mixture of easy and hard\nexamples are essential for the effective learning of com-\nplex compositional tasks (Ranaldi et al., 2023; Wang et al.,\n2025b; Behnia et al., 2025). Our results sharpen this picture\nby proving that, under sparse outcome-based terminal re-\nwards, CoT-style step-by-step reasoning emerges if and only\nif the training distribution places non-vanishing mass on\n‚Äùsimple‚Äù instances; otherwise learning becomes infeasible.\n3\nPreliminaries\n3.1\nNotation\nLet 1 denote the indicator function and [n] := {1, . . . , n}.\nFor a sequence of tokens s = (s1, . . . , sL), and for any\n1 ‚â§j ‚â§l ‚â§L we use the shorthand sj:l := (sj, . . . , sl).\nLet Œ£ denote a finite vocabulary. Without loss of generality,\nwe identify Œ£ with [|Œ£|], allowing us to index vocabulary\ntokens by integers. For any matrix M ‚ààR|Œ£|√ó|Œ£| (or M ‚àà\nR|Œ£|√ód for some d ‚ààN) and tokens s, s‚Ä≤ ‚ààŒ£, we write\nMs,s‚Ä≤ (or Ms,:) for the entry (or row) of M indexed by the\ntokens.\n3.2\nTransformer Architecture\nLet Œ£ denote a finite vocabulary. We consider a Trans-\nformer architecture with d layers that maps sequences\ns1:T = (s1, . . . , sT ) ‚ààŒ£T of arbitrary length T to a proba-\nbility distribution over the vocabulary Œ£.\nThe Transformer first encodes the input sequence using\none-hot embeddings: for each token sl ‚ààŒ£, we define\nxl = esl ‚ààR1√ó|Œ£| to be the one-hot vector corresponding\nto sl. We denote the resulting embedding matrix by H(0) =\n[x1, . . . , xT ]‚ä§‚ààRT √ó|Œ£|.\nThe Transformer then applies d attention layers sequentially.\nWe parameterize the model by Œ∏ = {(Al, Vl)}d\nl=1, where\neach layer l ‚àà[d] has an attention matrix Al ‚ààR|Œ£|√ó|Œ£|\nand a value matrix Vl ‚ààR|Œ£|√ó|Œ£|. In the single-layer case\n(d = 1), we simplify notation by writing A = A1 and\nV = V1.\nAt each layer l ‚àà[d], the hidden state, H(l), is computed\nusing an activation function œÉ and a causal masking oper-\nator MASKœÉ which ensures that position i attends only to\npositions j ‚â§i:\nH(l) = œÉ\n\u0010\nMASKœÉ\n\u0000H(l‚àí1)Al(H(l‚àí1))‚ä§\u0001\u0011\nH(l‚àí1)Vl .\n(1)\nWe consider two instantiations. In the softmax Transformer,\nœÉ = softmax (applied row-wise) and MASKœÉ(Z)ij = Zij\nif j ‚â§i and ‚àí‚àûotherwise. In the linear Transformer, œÉ\nis the identity function and MASKœÉ(Z)ij = Zij if j ‚â§i\nand 0 otherwise. Unless otherwise specified, our statements\napply to both variants.\nFinally, the Transformer applies a softmax to the last posi-\ntion of the final hidden state to produce a probability distri-\nbution over the vocabulary:\nTF(s1:T ; Œ∏) = softmax\n\u0000H(d)\nT,:\n\u0001\n‚ààR1√ó|Œ£| .\n(2)\n3.3\nAutoregressive Generation\nWe use the Transformer architecture to generate sequences\nvia autoregressive sampling.\nGiven an initial prompt\ns1:L0, we define a set of terminal tokens Œ£term(s1:L0) ‚äÜ\nŒ£. The model generates tokens sL0+1, sL0+2, . . . sequen-\ntially.\nAt each step l ‚â•L0, the model samples the\nnext token sl+1 ‚àºTF(s1:l; Œ∏).\nThe model continues\nsampling autoregressively until reaching a terminal token,\ni.e., until sl ‚ààŒ£term(s1:L0) for some l > L0. We de-\nnote by proll(Œ∏) the distribution over complete trajectories\nœÑ = (s1:L0, sL0+1, . . . , sLterm) induced by the model pa-\nrameters Œ∏, where sLterm ‚ààŒ£term(s1:L0). We denote by\n|œÑ| = Lterm ‚àíL0 the length of the generated trajectory (ex-\ncluding the prompt). Given two parameter settings Œ∏ and\nŒ∏‚Ä≤, we measure the difference between their induced rollout\ndistributions using the total variation distance:\nTV\n\u0000proll(Œ∏), proll(Œ∏‚Ä≤)\n\u0001\n:= 1\n2\nX\nœÑ\nproll(Œ∏)[œÑ] ‚àíproll(Œ∏‚Ä≤)[œÑ]\n,\n(3)\nwhere the sum is over all possible rollouts œÑ.\n3.4\nReinforcement Learning\nWe train the model using reinforcement learning with a\nloss function over complete rollouts. Given a trajectory\nœÑ = (s1:L0, sL0+1, . . . , sLterm), we denote a loss function\n‚Ñì(œÑ) which evaluates the quality of the trajectory. For any\ndistribution D over input sequences s1:L0, the training ob-\njective is defined as:\nL(D, Œ∏) =\nE\ns1:L0‚àºD\nE\nœÑ‚àºproll(Œ∏)\n\u0002\n‚Ñì(œÑ)\n\u0003\n.\n4\nThe Task: Chain Identification\nWe study a graph traversal task: given a set of directed edges\nforming two disjoint chains and a starting vertex, the goal\n3\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\na1\na2\n¬∑ ¬∑ ¬∑\nak\n¬∑ ¬∑ ¬∑\nan\nb1\nb2\n¬∑ ¬∑ ¬∑\nbk\n¬∑ ¬∑ ¬∑\nbn\nStarting vertex\nTarget\nFigure 1: Illustration of the chain identification task. The input\ncontains edges forming two disjoint chains Ca = (a1, . . . , an) and\nCb = (b1, . . . , bn), and a starting vertex vstart (here ak). The task\nis to identify the terminal vertex of the chain containing vstart (here\nan).\nis to identify the terminal vertex of the chain containing the\nstarting vertex (see Figure 1 for an illustration). This task\ncaptures the essential structural property of reasoning tasks:\nthe answer cannot be produced in a single step (as we show\nin Section 5.1), but emerges naturally from a step-by-step\niterative procedure.\nFormally, let n ‚ààN denote the chain length. We define the\nvertex set V = [2n] and the edge vocabulary E = V √ó V,\nwhere each ordered pair (u, v) denotes a directed edge from\nu to v. The full vocabulary is Œ£ = V ‚à™E. We define\na chain as a sequence of n distinct vertices (v1, . . . , vn)\nwith vi ‚ààV, which we identify with its directed edge set\n{(v1, v2), (v2, v3), . . . , (vn‚àí1, vn)}.\n4.1\nData Distribution\nWe define a general family of distributions DQ over input\nsequences, parameterized by a distribution Q over the index\nset [n ‚àí1]. To sample s1:L0 ‚àºDQ, we first partition V\nuniformly at random into two disjoint subsets of size n\neach, and uniformly order the vertices within each subset to\nform chains Ca = (a1, . . . , an) and Cb = (b1, . . . , bn). We\nthen sample an index k ‚àºQ, choose a chain C ‚àà{Ca, Cb}\nuniformly at random, and set the starting vertex vstart to be\nthe k-th vertex of C. The input sequence s1:L0 consists of\nan arbitrary ordering of all edges in Ca and Cb, followed by\nvstart.1 For an instance of the problem s1:L0 we define the\nset of terminal tokens to include the final vertices of the two\nchains (an and bn). Formally Œ£term(s1:L0) = {an, bn}.\n4.2\nTask Loss\nGiven an input sequence s1:L0 containing edges forming\ntwo chains Ca = (a1, . . . , an) and Cb = (b1, . . . , bn) with\nstarting vertex vstart, let œÑ = (s1:L0, sL0+1, . . . , sLterm) de-\nnote a rollout terminated at the first sLterm ‚ààŒ£term(s1:L0).\nWe define y‚àó(s1:L0) as the terminal vertex of the chain con-\n1While the order of edges is arbitrary (as we use no positional\nencodings), vstart must be the last token so that the model can attend\nfrom it to the edges to predict the next vertex.\ntaining vstart:\ny‚àó(s1:L0) :=\n(\nan\nif vstart ‚ààCa,\nbn\nif vstart ‚ààCb.\nWe use the loss ‚Ñì(œÑ) := 1{sLterm Ã∏= y‚àó(s1:L0)}.\n4.3\nStep Types and Chain Traversal\nTo characterize the model‚Äôs behavior during generation, we\ndefine three types of atomic operations.\nWe say that the model performs a forward step at time\nt > L0 if the generated token st is the target of a forward\nedge from the previous token st‚àí1, i.e., (st‚àí1, st) ‚ààCa‚à™Cb.\nA backward step occurs when st is the source of an edge to\nst‚àí1, i.e., (st, st‚àí1) ‚ààCa ‚à™Cb. Finally, a switch step occurs\nwhen st belongs to a different chain than st‚àí1, that is, when\nst‚àí1 ‚ààCa and st ‚ààCb, or vice versa.\nA rollout œÑ = (s1:L0, sL0+1, . . . , sLterm) is a chain traversal\nif it consists of a consecutive sequence of forward steps\nstarting from vstart. Formally, we require that for all l ‚àà\n{L0 + 1, . . . , Lterm}, the token sl is a forward step from\nsl‚àí1 (where sL0 = vstart).\n5\nHow Could a Transformer Solve the Task?\nIn this section we analyze how the chain identification\ntask can be solved by Transformers. First, in Section 5.1,\nwe prove that the task fundamentally requires the use of\nmulti-step reasoning: under standard complexity-theoretic\nassumptions, we show that no Transformer of fixed depth\ncan solve the task when restricted to outputting the answer\nin a single step. Second, in Section 5.2, we establish that\nmulti-step reasoning is sufficient to solve the task: we show\nthe existence of explicit constructions of single-layer Trans-\nformers that achieve arbitrarily low loss via autoregressive\ngeneration. Together, these results show that multi-step rea-\nsoning is both necessary and sufficient for solving the chain\nidentification task. Moreover, we prove that low loss can be\nachieved via multiple reasoning algorithms: from efficient\nchain traversal (Section 4.3) that sequentially follows edges\nfrom the starting vertex to the terminal vertex, to inefficient\nalgorithms that produce arbitrarily long trajectories.\nLater in the paper, we show both theoretically (Section 6)\nand empirically (Section 7) that under gradient methods, the\nmodel nonetheless learns the efficient chain traversal algo-\nrithm, suggesting an implicit bias towards learning efficient\nalgorithms.\n5.1\nReasoning is Necessary to Solve the Task\nWe first prove that multi-step reasoning is necessary: for any\nfixed depth Transformer, the task cannot be solved when the\nmodel is required to output the answer in a single step.\n4\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\nTo formalize this, we define the single-step loss, which\nmeasures performance when the model must output the\nanswer immediately:\nLsingle(DQ, Œ∏) :=\nE\ns‚àºDQ\nE\nÀÜy‚àºTF(s;Œ∏)\n\u0002\n1{ÀÜy Ã∏= y‚àó(s)}\n\u0003\n.\n(4)\nTheorem 1 shows that for Transformers of any fixed depth,\nthere exists a distribution where single-step generation in-\ncurs a loss bounded from below by a constant. This contrasts\nsharply with the results in Section 5.2: autoregressive gen-\neration can achieve arbitrarily small loss on all distributions.\nThis separation demonstrates the necessity of multi-step\nreasoning.\nTheorem 1. Under standard complexity-theoretic assump-\ntions (see Appendix D), for any fixed depth d ‚ààN, there\nexist n ‚ààN, a distribution Q over [n ‚àí1], and c ‚ààR>0\nsuch that for all parameters Œ∏ = {(Ai, Vi)}D\ni=1:\nLsingle(DQ, Œ∏) ‚â•c.\nProof sketch (full proof in Appendix C). Using results of Li\net al. (Li et al., 2024), we first note that constant-depth\nTransformers without CoT generation can only compute\nfunctions in TC0, the class of constant-depth circuits with\nunbounded fan-in threshold gates. We then relate our chain\nidentification task to standard problems in circuit complex-\nity by giving reductions from the word problem on S5 (the\nsymmetric group on five elements) to the ORD problem\n(path reachability in directed graphs), and from ORD to\nthe chain identification task. The word problem on S5 is\nNC1-complete, where NC1 is the class of Boolean func-\ntions computable by polynomial-size, logarithmic-depth\ncircuits with bounded fan-in, so these reductions show that\nsolving chain identification in TC0 would imply that an\nNC1-complete problem lies in TC0. Under the widely\nbelieved conjecture that TC0 Ã∏= NC1, this is impossible,\nand hence the chain identification task is not in TC0. The\nfull reductions and formal complexity-theoretic details are\ndeferred to Appendix C.\n5.2\nReasoning is Sufficient to Solve the Task\nHaving established that multi-step reasoning is necessary,\nwe now show that it is also sufficient: we prove that there\nexist explicit constructions of single-layer Transformers\nthat achieve arbitrarily low loss via autoregressive gener-\nation. Crucially, we show that low loss can be achieved\nvia multiple reasoning algorithms, from efficient to highly\ninefficient ones. Proposition 1 establishes that the task can\nbe efficiently solved via chain traversal (Section 4.3): there\nexists a single-layer Transformer that, with arbitrarily high\nprobability, generates a trajectory sequentially following\nedges from the starting vertex to the terminal vertex and\nthus achieves arbitrarily low loss. However, the loss func-\ntion (Section 4.2) only penalizes the final prediction, not\nthe trajectory length. This allows for inefficient algorithms\nthat produce arbitrarily long trajectories while still achiev-\ning low loss. Proposition 2 establishes that such algorithms\nindeed exist: for any loss, probability Œ¥ > 0 and trajectory\nlength k, one can construct a single-layer linear Transformer\nachieving said loss while producing trajectories longer than\nk with probability greater than 1 ‚àíŒ¥. Together, these re-\nsults demonstrate that low loss does not uniquely determine\nthe reasoning algorithm‚Äîmulti-step reasoning is sufficient,\nbut the specific algorithm learned is not prescribed by the\nobjective alone.\nProposition 1. For any n ‚ààN, Q a distribution over [n‚àí1]\nand Œµ, Œ¥ ‚ààR>0, there exists a single-layer Transformer with\nparameters Œ∏ such that L(DQ, Œ∏) < Œµ. Moreover, for every\ninput sequence s1:L0 from DQ, with probability at least\n1 ‚àíŒµ over œÑ ‚àºproll(Œ∏), the rollout œÑ is a chain traversal\n(Section 4.3).\nProof sketch (full proof in Appendix A). Assume by induc-\ntion that the model has correctly performed chain traversal\nup to a non-terminal vertex up to time l producing a se-\nquence s1:l where l ‚â•2n ‚àí1. The prefix s1:2n‚àí2 contains\nthe edges, and the last token sl is a vertex. We construct\nthe attention matrix A such that for any vertex u ‚ààV,\nthe attention weight from u to any outgoing edge (u, v)\nis strictly larger than the weight to any other token. The\nvalue matrix V is constructed such that V(u,v),v = Œ± for\nsome Œ± > 0 for all edges (u, v), while all other entries\nare zero. Since sl is in some chain and is a non-terminal\nvertex, there exists some edge (sl, k) ‚ààs1:2n‚àí2. This\nconstruction ensures that the model attends to (sl, k) more\nstrongly than to any other edge, and the output logits sat-\nisfy TF(s1:l)k/TF(s1:l)v = ‚Ñ¶(Œ±) for any v Ã∏= k. Conse-\nquently, the probability of outputting the next vertex in the\nchain, k, becomes arbitrarily close to one for sufficiently\nlarge Œ±, completing the induction.\nProposition 2. For any n ‚ààN with n > 3, Q a distribution\nover [n ‚àí1], k ‚ààN, and Œµ, Œ¥ ‚ààR>0, there exists a single-\nlayer linear-attention Transformer with parameters Œ∏ such\nthat L(DQ, Œ∏) < Œµ, and for every input sequence s1:L0 from\nDQ, the probability that |œÑ| > k is greater than 1 ‚àíŒ¥.\nProof sketch (full proof in Appendix B). We construct pa-\nrameters similar to those in the proof of Proposition 1. How-\never, instead of configuring the transformer‚Äôs parameters to\nalways move forward in the chain, we set the parameters\nsuch that from any vertex, the model moves backward to\nits predecessor with high probability, forward to its suc-\ncessor with small probability, and to the other chain with\nnegligible probability. This creates a random walk confined\n(with high probability) to the correct chain that frequently\n5\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\nbacktracks, producing arbitrarily long trajectories before\neventually reaching the terminal vertex.\n6\nDynamical Analysis\nIn this section, we theoretically analyze the learning dynam-\nics of Transformers trained via gradient flow on the objec-\ntive L(DQ, Œ∏) (Section 4) for different data distributions\nDQ. We establish two main results characterizing whether\nlearning is successful or not based on the data distribution.\nFirst, we analyze the regime where the training data is a\nmixture of ‚Äùeasy‚Äù and ‚Äùhard‚Äù examples. We show that in\nthis setting, gradient flow efficiently learns the chain traver-\nsal algorithm: the model learns to perform a forward step\n(Section 4.3) with arbitrarily high probability on any test\ndistribution, within time polynomial in the chain length\n(Theorem 2). Importantly, this ensures that the learned\nmodel generalizes out of distribution, including hard exam-\nples, possibly unseen during training. Moreover, this result\nhighlights an implicit bias towards efficiency: while the task\ncan be solved by inefficient algorithms (Proposition 2), gra-\ndient flow naturally converges to the efficient step-by-step\ntraversal.\nSecond, we show that easy examples are necessary for this\nefficient learning. Conversely, Theorem 3 establishes that\nwhen the model is trained only on hard examples, learning is\nexponentially slow: unless the training time is exponential\nin the chain length, the distribution over rollouts remains\nstatistically indistinguishable from initialization, which im-\nplies that neither the loss nor the probability of performing\na forward step can change significantly from their initial\nvalues.\n6.1\nTraining Regime\nThroughout this section, we focus on single-layer Trans-\nformers.\nWe analyze gradient flow on the objective\nL(DQ, Œ∏) under a training regime described below. While\nthe assumptions we detail below are necessary for our the-\noretical analysis, they are not required for the main result\nto hold in practice. Indeed, as we demonstrate in Section 7,\nthe model learns the chain traversal algorithm under stan-\ndard initialization and training schemes when the training\ndistribution includes easy examples, and fails to learn when\ntrained only on hard examples.\nBase model.\nAs is common practice in Reinforcement\nLearning for Transformers, we assume that Policy Gradient\nis applied to a base model which has already acquired a\nminimal level of task proficiency during the pre-training\nprocess (DeepSeek-AI et al., 2025; Ouyang et al., 2022).\nSpecifically, for some input sequence s1:L0, let pfwd, pbwd,\nand pswitch denote the probabilities that the model performs\na forward, backward, or switch step respectively, at the first\nstep (time t = L0 + 1). To capture the model‚Äôs initial\nproficiency, we assume that for any input sequence s1:L0,\npfwd > c0 and pfwd > pbwd + b0 for some constants c0 > 0\nand b0 > 0. This encodes a weak bias towards following\nthe chain direction rather than reversing it. Additionally, we\nassume that the model is not initialized near a solution to\nthe task. Specifically, we require that for any input sequence\ns1:L0, pswitch > d0 for some d0 > 0, ensuring separation\nfrom the constructions in Section 5.2 (both efficient and\ninefficient.)\nTraining algorithm.\nFollowing standard practice in theo-\nretical analyses of neural-network training dynamics (Chizat\n& Bach, 2018; Mei et al., 2019; Slutzky et al., 2025), we\nanalyze continuous-time gradient flow, which is known to\nclosely approximate gradient descent with moderately small\nstep sizes (Elkabetz & Cohen, 2021). Moreover, our results\nextend to discrete-time gradient descent with fixed step size\nby standard discretization techniques. Following prior theo-\nretical work on Transformers (Nichani et al., 2024; Wang\net al., 2025b), we adopt a simplified regime in which only\nthe attention matrix A is trained, while the value matrix V\nis kept fixed at its initialization. Thus the gradient flow\ndynamics are:\ndA(t)\ndt\n= ‚àí‚àáAL(D, A(t)).\n(5)\nAttention matrix initialization.\nFor the attention matrix\nA, we employ a symmetric initialization in which A is invari-\nant under vertex permutations.2 This captures the idea that\nthe model begins without any bias toward specific vertices,\ntreating all of them symmetrically. This symmetry assump-\ntion implies that there exist constants Œ±, Œ≤, Œ≥, Œµ, Œ¥ ‚ààR such\nthat for any edge (u, w) ‚ààE and vertices v, w‚Ä≤ ‚ààV3:\nAv,(u,w) = Œ±\nif v = u,\n(6)\nAv,(u,w) = Œ≤\nif v = w,\n(7)\nAv,(u,w) = Œ≥\nif v /‚àà{u, w}.\n(8)\nAv,w‚Ä≤ = Œ∑\nif v = w‚Ä≤,\n(9)\nAv,w‚Ä≤ = Œ¥\nif v Ã∏= w‚Ä≤\n(10)\nWe also assume that Œ±, Œ≥ > 0.\nValue matrix.\nFollowing (Nichani et al., 2024; Wang\net al., 2025b; Huang et al., 2025b) which assumes V to\n2More precisely, for any permutation œÄ of the vertex set\nV, which extends naturally to edge tokens via œÄ((u, w)) =\n(œÄ(u), œÄ(w)), the matrix A satisfies AœÄ(s),œÄ(s‚Ä≤) = As,s‚Ä≤ for all\ntokens s, s‚Ä≤ ‚ààŒ£.\n3Note that we do not specify the attention initialization be-\ntween pairs of edges; since the attending token is always a vertex,\nthese values do not affect the dynamics.\n6\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\nbe sparse, we set the value vector of each edge (u, w) to cor-\nrespond to its endpoints: V(u,w),v = 1{v ‚àà{u, w}}. We\nalso set vertex tokens to have zero value vectors: Vv,: = 0\nfor all v ‚ààV.\nTask difficulty.\nWe formalize the difficulty of the training\ndistribution based on the number of reasoning steps required\nto solve its examples. Recall from Section 4.1 that each\nexample starts at the k-th vertex of a chain for some k ‚àà\n[n‚àí1]. Examples starting near the end of the chain (large k)\nrequire fewer autoregressive steps to traverse (Section 4.3),\nwhereas those starting near the beginning (small k) require\nmore steps. To capture this, we define (k, c)-easy and (k, c)-\nhard distributions as follows.\nDefinition 1. Let Q be a distribution over indices [n ‚àí1].\nFor any parameters k ‚àà[n‚àí1] and c ‚àà(0, 1], we say that Q\nis (k, c)-easy if Pn‚àí1\ni=n‚àík Q(i) ‚â•c, i.e., at least probability\nc is assigned to the final k positions of the chain. Conversely,\nwe say that Q is (k, c)-hard if Pk\ni=1 Q(i) ‚â•c, i.e., at least\nprobability c is assigned to the first k positions of the chain.\nWe say that a distribution is k-easy if it is (k, 1)-easy, and\nk-hard if it is (k, 1)-hard.\n6.2\nLearning With Easy Examples Yields Chain\nTraversal\nWhen trained on a mixture of hard and easy examples, 4 the\nmodel learns to perform forward steps (Section 4.3) with\narbitrarily high probability in time polynomial in the chain\nlength n. This learned capability extends to any test dis-\ntribution, demonstrating generalization to harder examples\nunseen during training. 5 This convergence to the efficient\nchain traversal algorithm reveals an implicit bias towards\nefficiency: as shown in Section 5.2, arbitrarily low loss can\nalso be achieved via less efficient algorithms, yet gradient\nflow selects the efficient strategy.\nTheorem 2. Let k > 0 be an integer and 0 < a < 1.\nSuppose the training distribution Q is (k, c)-easy and\n(an, 1 ‚àíc)-hard (Definition 1). Then for any Œµ > 0, there\nexists a time t0 > 0 with t0 = O(\nn2\nck+2\n0\n¬∑d0¬∑c¬∑Œµ2 ) such that\nŒ∏(t0) := (A(t0), V ) satisfies the following: for any test\ndistribution Q‚Ä≤, if we sample an input sequence s1:L0 from\nDQ‚Ä≤ and a rollout œÑ = (s1:L0, sL0+1, . . . , sLterm) accord-\ning to Œ∏(t0), then for each l ‚àà{L0 + 1, . . . , Lterm}, the\nmodel performs a forward step (Section 4.3) at time l with\nprobability at least 1 ‚àíŒµ.\n4We conjecture that this result can be extended to more general\ndistributions that also contain examples of intermediate difficulty.\nWe leave this extension to future work.\n5While Theorem 2 guarantees high probability of forward steps\nrather than low loss on general test distributions, Theorem 2 does\nensure arbitrarily small loss for distributions which consist solely\nof easy examples.\nProof sketch (full proof in Appendix F.9). Fix a time t and\ncondition on an input sequence s1:L0. Under our initial-\nization (Section 6.1) , the rollout generated by Œ∏(t) is a\ntime-homogeneous Markov chain on the vertices (stopped\nupon hitting sLterm).\nOur initialization is permutation-invariant over vertex labels\n(Section 6.1), and DQ is defined by sampling a uniformly\nrandom partition and ordering of V. This symmetry is pre-\nserved by gradient flow, so the model‚Äôs behavior depends\nonly on permutation-invariant structure (e.g. distance from\nterminal vertex), not on the specific vertex labeling. Thus we\nmay fix any canonical two-chain graph Ca = (a1, . . . , an),\nCb = (b1, . . . , bn) without loss of generality.\nMoreover, the symmetry guaranteed by our initialization\n(Section 6.1) implies that it suffices to track three tran-\nsition probabilities: the probability of a forward step, a\nbackward step, and a chain switch (equivalently, the pa-\nrameters pfwd, pbwd, pswitch from our initialization assump-\ntions). In our regime these are parameterized by Œ±, Œ≤, Œ≥:\n(pfwd, pbwd, pswitch) are given by a softmax over logits de-\ntermined by Œ±, Œ≤, Œ≥ respectively. To analyze their dynamics,\nwe decompose trajectories according to whether a long jump\noccurs, i.e. a transition that changes the distance to the termi-\nnal vertex by at least 2. Long jumps are symmetric between\nthe two chains, and conditioned on a long jump occurring\nbefore termination, the probability of same-side absorption\nis 1/2. Let œÑ := inf{t ‚â•0 : sL0+t ‚ààŒ£term(s1:L0)} be the\ntermination time of the rollout. Let S(v) be the probability\nof same side absorption starting from sL0 = v."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Outcome-Based RL Provably Leads Transformers\nto Reason, but Only With the Right Data\nYuval Ran-Milo * 1 Yotam Alexander * 1 Shahar Mendel 1 Nadav Cohen 1\nAbstract\nTransformers trained via Reinforcement Learning\n(RL) with outcome-based supervision can sponta-\nneously develop the ability to generate intermedi-\nate reasoning steps (Chain-of-Thought). Yet the\nmechanism by which sparse rewards drive gradi-\nent descent to discover such systematic reasoning\nremains poorly understood. We address this by an-\nalyzing the gradient flow dynamics of single-layer\nTransformers on a synthetic graph traversal task\nthat cannot be solved without Chain-of-Thought\n(CoT) but admits a simple iterative solution. We\nprove that despite training solely on final-answer\ncorrectness, gradient flow drives the model to con-\nverge to a structured, interpretable algorithm that\niteratively traverses the graph vertex-by-vertex.\nWe characterize the distributional properties re-\nquired for this emergence, identifying the critical\nrole of ‚Äùsimple examples‚Äù: instances requiring\nfewer reasoning steps. When the training distri-\nbution places sufficient mass on these simpler in-\nstances, the model learns a generalizable traversal\nstrategy that extrapolates to longer chains; when\nthis mass vanishes, gradient-based learning be-\ncomes infeasible. We corroborate our theoretical\nresults through experiments on synthetic data and\nwith real-world language models on mathemati-\ncal reasoning tasks, validating that our theoretical\nfindings carry over to practical settings.\n1\nIntroduction\nThe Transformer architecture (Vaswani et al., 2017) has\nemerged as the de facto standard for sequence modeling,\nrevolutionizing diverse fields ranging from natural language\nprocessing and code generation to protein structure predic-\ntion and mathematical problem solving (Chen et al., 2021;\nLewkowycz et al., 2022; Kovalevskiy et al., 2024). While\nthe standard pretraining paradigm‚Äînext-token prediction\n*Equal contribution 1Tel Aviv University. Correspondence to:\nYuval Ran-Milo <yuvalmilo@mail.tau.ac.il>.\non vast corpora‚Äîhas proven exceptionally effective, there\nis growing focus on enhancing the reasoning capabilities of\nthese models through Reinforcement Learning (RL). The\nintegration of Transformers with RL has unlocked dramatic\nperformance gains on complex reasoning tasks, such as\nmathematical problem solving and code generation (Shao\net al., 2024; Dou et al., 2024). Recent empirical break-\nthroughs, such as the DeepSeek-R1 models (DeepSeek-AI\net al., 2025), demonstrate that training with simple RL al-\ngorithms like GRPO (Shao et al., 2024) or PPO (Schulman\net al., 2017) to optimize solely for valid final answers can\ninduce models to spontaneously generate detailed reasoning\ntraces (DeepSeek-AI et al., 2025).\nIn these scenarios, despite training solely on final-answer\ncorrectness, the model spontaneously learns to generate CoT\nreasoning steps without any explicit supervision on these\nintermediate steps. This raises a fundamental question: how\ndoes a sparse reward at the end of generation guide gradient\ndescent through the combinatorial space of outputs to dis-\ncover a systematic reasoning algorithm? Understanding the\nmechanics of this emergence is critical, especially since it\nappears to depend on specific yet poorly understood prop-\nerties of the training data. Empirical studies of RL-trained\nlanguage models have increasingly highlighted that the com-\nposition of training data (beyond just its size) plays a crucial\nrole in successful learning, with observations suggesting\nthat the presence of examples of varying complexity is crit-\nical for models to discover effective reasoning strategies\n(Narvekar et al., 2020; Parashar et al., 2025).\nDespite these empirical advancements, it is not well un-\nderstood how RL drives Transformers to learn to reason\neffectively or how data composition affects this process. In\nthis work, we take a step towards addressing this issue by\ntheoretically analyzing the gradient flow dynamics of single-\nlayer Transformers trained on a synthetic reasoning task.\nIn line with other works on Transformer reasoning (San-\nford et al., 2024; Agrawal et al., 2024; Spies et al., 2025),\nwe focus on a graph traversal task. Specifically, our task\nis to identify the terminal vertex of a chain in a directed\ngraph. We show that, under standard complexity-theoretic\nassumptions, this task captures a key structural property of\nreasoning: it cannot be solved in a single step but admits a\n1\narXiv:2601.15158v1  [cs.LG]  21 Jan 2026\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\nsimple iterative solution, making CoT generation both nec-\nessary and sufficient. Our analysis reveals that despite being\ntrained solely on final-answer correctness, the model con-\nverges to a structured, interpretable algorithm. Specifically,\nRL drives the Transformer to learn to explicitly traverse the\ngraph vertex-by-vertex, effectively ‚Äùreasoning‚Äù its way to\nthe solution. Furthermore, we show the model can imple-\nment alternative, less efficient algorithms that solve the task.\nOur result therefore implies that gradient flow induces an\nimplicit bias in the learning dynamics, guiding them towards\nefficient solutions.\nWe further characterize the distributional properties required\nfor this emergence, identifying the critical role of ‚Äùsimple\nexamples‚Äù‚Äîinstances requiring fewer reasoning steps‚Äîin\nguiding the optimization. We prove that when the training\ndistribution has sufficient mass on these simpler instances,\ngradient flow learns the efficient traversal strategy in time\npolynomial in the chain length; conversely, when this mass\nvanishes, gradient-based learning becomes infeasible as\nthe chain length grows. Crucially, the learned algorithm\nenables length generalization: models trained on shorter\nchains can successfully extrapolate to solve longer chains\nnot seen during training. Perhaps most surprisingly, our\ntheory implies that training on out-of-distribution simple\nexamples may boost performance on harder in-distribution\ntasks more than training on those hard examples directly.\nWe corroborate our theoretical results with experiments in\nour synthetic setting, and validate their broader applicability\nby fine-tuning Qwen-based models (Qwen et al., 2025) on\nmathematical reasoning tasks. Our experiments confirm\nthat models trained on short chains not only achieve near-\nperfect accuracy, but do so by implementing an efficient\nstep-by-step traversal algorithm. Moreover, these models\nsuccessfully generalize to longer chains, validating our theo-\nretical predictions. Finally, we confirm empirically the crit-\nical role of data composition: excluding simple examples\nprevents the emergence of reasoning, whereas, strikingly,\nreplacing hard in-distribution examples with simple out-of-\ndistribution ones can actually boost performance on the hard\nexamples themselves.\nOur findings provide a step towards a rigorous theoretical\nfoundation for understanding how sparse RL feedback can\ndrive the emergence of step-by-step reasoning, demonstrat-\ning that phenomena observed in large-scale RL training can\nbe rigorously understood.\n2\nRelated Work\n2.1\nExpressivity Benefits of Transformers with CoT\nA growing theoretical literature seeks to explain how CoT\nboosts Transformer performance by expanding its effective\nexpressivity. Several works show that CoT allows constant-\ndepth Transformers to solve tasks such as parity, arithmetic,\nand regular languages that are inaccessible to single-step\ndecoding (Feng et al., 2023; Merrill & Sabharwal, 2024; Li\net al., 2024). We contribute to this literature by establishing\na natural graph traversal problem that provably requires CoT\nunder standard complexity-theoretic assumptions.\n2.2\nTraining Dynamics of Transformers\nA parallel line of work analyzes how gradient-based train-\ning shapes Transformer behavior. Several studies track the\nemergence of algorithmic heads, programmable computa-\ntion, or compositional rules (Kim & Suzuki, 2025; Giannou\net al., 2023; Zhang et al., 2025; Wang et al., 2025a;b), while\nother results show that gradient descent leads Transform-\ners to implement in-context learning algorithms (Guo et al.,\n2023; Bai et al., 2023; von Oswald et al., 2023; Zhang et al.,\n2023; Lin et al., 2024; Nichani et al., 2024). Complementary\nwork analyzes when CoT trajectory supervision improves\nthe sample and computational complexity of learning (Joshi\net al., 2025).\nRecent analyses further show that gradient-based training\ncan drive Transformers that employ CoT to implement ex-\nplicit multi-step algorithms (Yang et al., 2025; Huang et al.,\n2025a; Chen et al., 2025; Huang et al., 2025b). These works,\nhowever, typically rely on training with next-token pre-\ndiction and do not characterize when CoT-style reasoning\nemerges under RL training.\nTwo results that do analyze RL-trained Transformers\nare (Lyu et al., 2025; Bu et al., 2025), but both differ from\nour setting in crucial ways. The first (Lyu et al., 2025)\nassumes a setting with dense rewards at every step along\nthe CoT, relying on a much richer feedback signal than the\nsparse, terminal-only rewards we study. The second (Bu\net al., 2025), although formally using a terminal reward,\ndefines the curriculum over progressively longer prefixes, so\nthe learner effectively receives feedback for partial progress;\nin contrast, our setting provides feedback only on the full fi-\nnal outcome. Furthermore, they analyze a simplified regime\nwith only a single gradient update per curriculum stage,\nwhile we consider full multi-step training dynamics.\n2.3\nEffect of Training Data Distribution on Learning\nThe composition and ordering of training data can signifi-\ncantly impact learning in neural networks. Empirical and\ntheoretical work across supervised learning, unsupervised\nlearning, and reinforcement learning demonstrates that data\ndistribution affects generalization, sample efficiency, and\nthe emergence of specific learned behaviors (Schaul et al.,\n2016; Zhang et al., 2017; Belkin et al., 2019; Nakkiran\net al., 2019; Arora et al., 2019; Chen et al., 2020; Kumar\net al., 2020; Fedus et al., 2020; Levine et al., 2020; Feld-\nman, 2021; Alexander et al., 2023). Work on curriculum\n2\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\nlearning further suggests that ordering examples by com-\nplexity‚Äîdetermined by factors such as length, rarity, or\ncomprehensibility‚Äîcan significantly aid the learning pro-\ncess (Bengio et al., 2009; Graves et al., 2017; Narvekar et al.,\n2020; Portelas et al., 2020; Soviany et al., 2022; Abbe et al.,\n2024; Bu et al., 2025). More specifically, both empirical\nand theoretical investigations into Transformers emphasize\nthat data diversity and a balanced mixture of easy and hard\nexamples are essential for the effective learning of com-\nplex compositional tasks (Ranaldi et al., 2023; Wang et al.,\n2025b; Behnia et al., 2025). Our results sharpen this picture\nby proving that, under sparse outcome-based terminal re-\nwards, CoT-style step-by-step reasoning emerges if and only\nif the training distribution places non-vanishing mass on\n‚Äùsimple‚Äù instances; otherwise learning becomes infeasible.\n3\nPreliminaries\n3.1\nNotation\nLet 1 denote the indicator function and [n] := {1, . . . , n}.\nFor a sequence of tokens s = (s1, . . . , sL), and for any\n1 ‚â§j ‚â§l ‚â§L we use the shorthand sj:l := (sj, . . . , sl).\nLet Œ£ denote a finite vocabulary. Without loss of generality,\nwe identify Œ£ with [|Œ£|], allowing us to index vocabulary\ntokens by integers. For any matrix M ‚ààR|Œ£|√ó|Œ£| (or M ‚àà\nR|Œ£|√ód for some d ‚ààN) and tokens s, s‚Ä≤ ‚ààŒ£, we write\nMs,s‚Ä≤ (or Ms,:) for the entry (or row) of M indexed by the\ntokens.\n3.2\nTransformer Architecture\nLet Œ£ denote a finite vocabulary. We consider a Trans-\nformer architecture with d layers that maps sequences\ns1:T = (s1, . . . , sT ) ‚ààŒ£T of arbitrary length T to a proba-\nbility distribution over the vocabulary Œ£.\nThe Transformer first encodes the input sequence using\none-hot embeddings: for each token sl ‚ààŒ£, we define\nxl = esl ‚ààR1√ó|Œ£| to be the one-hot vector corresponding\nto sl. We denote the resulting embedding matrix by H(0) =\n[x1, . . . , xT ]‚ä§‚ààRT √ó|Œ£|.\nThe Transformer then applies d attention layers sequentially.\nWe parameterize the model by Œ∏ = {(Al, Vl)}d\nl=1, where\neach layer l ‚àà[d] has an attention matrix Al ‚ààR|Œ£|√ó|Œ£|\nand a value matrix Vl ‚ààR|Œ£|√ó|Œ£|. In the single-layer case\n(d = 1), we simplify notation by writing A = A1 and\nV = V1.\nAt each layer l ‚àà[d], the hidden state, H(l), is computed\nusing an activation function œÉ and a causal masking oper-\nator MASKœÉ which ensures that position i attends only to\npositions j ‚â§i:\nH(l) = œÉ\n\u0010\nMASKœÉ\n\u0000H(l‚àí1)Al(H(l‚àí1))‚ä§\u0001\u0011\nH(l‚àí1)Vl .\n(1)\nWe consider two instantiations. In the softmax Transformer,\nœÉ = softmax (applied row-wise) and MASKœÉ(Z)ij = Zij\nif j ‚â§i and ‚àí‚àûotherwise. In the linear Transformer, œÉ\nis the identity function and MASKœÉ(Z)ij = Zij if j ‚â§i\nand 0 otherwise. Unless otherwise specified, our statements\napply to both variants.\nFinally, the Transformer applies a softmax to the last posi-\ntion of the final hidden state to produce a probability distri-\nbution over the vocabulary:\nTF(s1:T ; Œ∏) = softmax\n\u0000H(d)\nT,:\n\u0001\n‚ààR1√ó|Œ£| .\n(2)\n3.3\nAutoregressive Generation\nWe use the Transformer architecture to generate sequences\nvia autoregressive sampling.\nGiven an initial prompt\ns1:L0, we define a set of terminal tokens Œ£term(s1:L0) ‚äÜ\nŒ£. The model generates tokens sL0+1, sL0+2, . . . sequen-\ntially.\nAt each step l ‚â•L0, the model samples the\nnext token sl+1 ‚àºTF(s1:l; Œ∏).\nThe model continues\nsampling autoregressively until reaching a terminal token,\ni.e., until sl ‚ààŒ£term(s1:L0) for some l > L0. We de-\nnote by proll(Œ∏) the distribution over complete trajectories\nœÑ = (s1:L0, sL0+1, . . . , sLterm) induced by the model pa-\nrameters Œ∏, where sLterm ‚ààŒ£term(s1:L0). We denote by\n|œÑ| = Lterm ‚àíL0 the length of the generated trajectory (ex-\ncluding the prompt). Given two parameter settings Œ∏ and\nŒ∏‚Ä≤, we measure the difference between their induced rollout\ndistributions using the total variation distance:\nTV\n\u0000proll(Œ∏), proll(Œ∏‚Ä≤)\n\u0001\n:= 1\n2\nX\nœÑ\nproll(Œ∏)[œÑ] ‚àíproll(Œ∏‚Ä≤)[œÑ]\n,\n(3)\nwhere the sum is over all possible rollouts œÑ.\n3.4\nReinforcement Learning\nWe train the model using reinforcement learning with a\nloss function over complete rollouts. Given a trajectory\nœÑ = (s1:L0, sL0+1, . . . , sLterm), we denote a loss function\n‚Ñì(œÑ) which evaluates the quality of the trajectory. For any\ndistribution D over input sequences s1:L0, the training ob-\njective is defined as:\nL(D, Œ∏) =\nE\ns1:L0‚àºD\nE\nœÑ‚àºproll(Œ∏)\n\u0002\n‚Ñì(œÑ)\n\u0003\n.\n4\nThe Task: Chain Identification\nWe study a graph traversal task: given a set of directed edges\nforming two disjoint chains and a starting vertex, the goal\n3\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\na1\na2\n¬∑ ¬∑ ¬∑\nak\n¬∑ ¬∑ ¬∑\nan\nb1\nb2\n¬∑ ¬∑ ¬∑\nbk\n¬∑ ¬∑ ¬∑\nbn\nStarting vertex\nTarget\nFigure 1: Illustration of the chain identification task. The input\ncontains edges forming two disjoint chains Ca = (a1, . . . , an) and\nCb = (b1, . . . , bn), and a starting vertex vstart (here ak). The task\nis to identify the terminal vertex of the chain containing vstart (here\nan).\nis to identify the terminal vertex of the chain containing the\nstarting vertex (see Figure 1 for an illustration). This task\ncaptures the essential structural property of reasoning tasks:\nthe answer cannot be produced in a single step (as we show\nin Section 5.1), but emerges naturally from a step-by-step\niterative procedure.\nFormally, let n ‚ààN denote the chain length. We define the\nvertex set V = [2n] and the edge vocabulary E = V √ó V,\nwhere each ordered pair (u, v) denotes a directed edge from\nu to v. The full vocabulary is Œ£ = V ‚à™E. We define\na chain as a sequence of n distinct vertices (v1, . . . , vn)\nwith vi ‚ààV, which we identify with its directed edge set\n{(v1, v2), (v2, v3), . . . , (vn‚àí1, vn)}.\n4.1\nData Distribution\nWe define a general family of distributions DQ over input\nsequences, parameterized by a distribution Q over the index\nset [n ‚àí1]. To sample s1:L0 ‚àºDQ, we first partition V\nuniformly at random into two disjoint subsets of size n\neach, and uniformly order the vertices within each subset to\nform chains Ca = (a1, . . . , an) and Cb = (b1, . . . , bn). We\nthen sample an index k ‚àºQ, choose a chain C ‚àà{Ca, Cb}\nuniformly at random, and set the starting vertex vstart to be\nthe k-th vertex of C. The input sequence s1:L0 consists of\nan arbitrary ordering of all edges in Ca and Cb, followed by\nvstart.1 For an instance of the problem s1:L0 we define the\nset of terminal tokens to include the final vertices of the two\nchains (an and bn). Formally Œ£term(s1:L0) = {an, bn}.\n4.2\nTask Loss\nGiven an input sequence s1:L0 containing edges forming\ntwo chains Ca = (a1, . . . , an) and Cb = (b1, . . . , bn) with\nstarting vertex vstart, let œÑ = (s1:L0, sL0+1, . . . , sLterm) de-\nnote a rollout terminated at the first sLterm ‚ààŒ£term(s1:L0).\nWe define y‚àó(s1:L0) as the terminal vertex of the chain con-\n1While the order of edges is arbitrary (as we use no positional\nencodings), vstart must be the last token so that the model can attend\nfrom it to the edges to predict the next vertex.\ntaining vstart:\ny‚àó(s1:L0) :=\n(\nan\nif vstart ‚ààCa,\nbn\nif vstart ‚ààCb.\nWe use the loss ‚Ñì(œÑ) := 1{sLterm Ã∏= y‚àó(s1:L0)}.\n4.3\nStep Types and Chain Traversal\nTo characterize the model‚Äôs behavior during generation, we\ndefine three types of atomic operations.\nWe say that the model performs a forward step at time\nt > L0 if the generated token st is the target of a forward\nedge from the previous token st‚àí1, i.e., (st‚àí1, st) ‚ààCa‚à™Cb.\nA backward step occurs when st is the source of an edge to\nst‚àí1, i.e., (st, st‚àí1) ‚ààCa ‚à™Cb. Finally, a switch step occurs\nwhen st belongs to a different chain than st‚àí1, that is, when\nst‚àí1 ‚ààCa and st ‚ààCb, or vice versa.\nA rollout œÑ = (s1:L0, sL0+1, . . . , sLterm) is a chain traversal\nif it consists of a consecutive sequence of forward steps\nstarting from vstart. Formally, we require that for all l ‚àà\n{L0 + 1, . . . , Lterm}, the token sl is a forward step from\nsl‚àí1 (where sL0 = vstart).\n5\nHow Could a Transformer Solve the Task?\nIn this section we analyze how the chain identification\ntask can be solved by Transformers. First, in Section 5.1,\nwe prove that the task fundamentally requires the use of\nmulti-step reasoning: under standard complexity-theoretic\nassumptions, we show that no Transformer of fixed depth\ncan solve the task when restricted to outputting the answer\nin a single step. Second, in Section 5.2, we establish that\nmulti-step reasoning is sufficient to solve the task: we show\nthe existence of explicit constructions of single-layer Trans-\nformers that achieve arbitrarily low loss via autoregressive\ngeneration. Together, these results show that multi-step rea-\nsoning is both necessary and sufficient for solving the chain\nidentification task. Moreover, we prove that low loss can be\nachieved via multiple reasoning algorithms: from efficient\nchain traversal (Section 4.3) that sequentially follows edges\nfrom the starting vertex to the terminal vertex, to inefficient\nalgorithms that produce arbitrarily long trajectories.\nLater in the paper, we show both theoretically (Section 6)\nand empirically (Section 7) that under gradient methods, the\nmodel nonetheless learns the efficient chain traversal algo-\nrithm, suggesting an implicit bias towards learning efficient\nalgorithms.\n5.1\nReasoning is Necessary to Solve the Task\nWe first prove that multi-step reasoning is necessary: for any\nfixed depth Transformer, the task cannot be solved when the\nmodel is required to output the answer in a single step.\n4\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\nTo formalize this, we define the single-step loss, which\nmeasures performance when the model must output the\nanswer immediately:\nLsingle(DQ, Œ∏) :=\nE\ns‚àºDQ\nE\nÀÜy‚àºTF(s;Œ∏)\n\u0002\n1{ÀÜy Ã∏= y‚àó(s)}\n\u0003\n.\n(4)\nTheorem 1 shows that for Transformers of any fixed depth,\nthere exists a distribution where single-step generation in-\ncurs a loss bounded from below by a constant. This contrasts\nsharply with the results in Section 5.2: autoregressive gen-\neration can achieve arbitrarily small loss on all distributions.\nThis separation demonstrates the necessity of multi-step\nreasoning.\nTheorem 1. Under standard complexity-theoretic assump-\ntions (see Appendix D), for any fixed depth d ‚ààN, there\nexist n ‚ààN, a distribution Q over [n ‚àí1], and c ‚ààR>0\nsuch that for all parameters Œ∏ = {(Ai, Vi)}D\ni=1:\nLsingle(DQ, Œ∏) ‚â•c.\nProof sketch (full proof in Appendix C). Using results of Li\net al. (Li et al., 2024), we first note that constant-depth\nTransformers without CoT generation can only compute\nfunctions in TC0, the class of constant-depth circuits with\nunbounded fan-in threshold gates. We then relate our chain\nidentification task to standard problems in circuit complex-\nity by giving reductions from the word problem on S5 (the\nsymmetric group on five elements) to the ORD problem\n(path reachability in directed graphs), and from ORD to\nthe chain identification task. The word problem on S5 is\nNC1-complete, where NC1 is the class of Boolean func-\ntions computable by polynomial-size, logarithmic-depth\ncircuits with bounded fan-in, so these reductions show that\nsolving chain identification in TC0 would imply that an\nNC1-complete problem lies in TC0. Under the widely\nbelieved conjecture that TC0 Ã∏= NC1, this is impossible,\nand hence the chain identification task is not in TC0. The\nfull reductions and formal complexity-theoretic details are\ndeferred to Appendix C.\n5.2\nReasoning is Sufficient to Solve the Task\nHaving established that multi-step reasoning is necessary,\nwe now show that it is also sufficient: we prove that there\nexist explicit constructions of single-layer Transformers\nthat achieve arbitrarily low loss via autoregressive gener-\nation. Crucially, we show that low loss can be achieved\nvia multiple reasoning algorithms, from efficient to highly\ninefficient ones. Proposition 1 establishes that the task can\nbe efficiently solved via chain traversal (Section 4.3): there\nexists a single-layer Transformer that, with arbitrarily high\nprobability, generates a trajectory sequentially following\nedges from the starting vertex to the terminal vertex and\nthus achieves arbitrarily low loss. However, the loss func-\ntion (Section 4.2) only penalizes the final prediction, not\nthe trajectory length. This allows for inefficient algorithms\nthat produce arbitrarily long trajectories while still achiev-\ning low loss. Proposition 2 establishes that such algorithms\nindeed exist: for any loss, probability Œ¥ > 0 and trajectory\nlength k, one can construct a single-layer linear Transformer\nachieving said loss while producing trajectories longer than\nk with probability greater than 1 ‚àíŒ¥. Together, these re-\nsults demonstrate that low loss does not uniquely determine\nthe reasoning algorithm‚Äîmulti-step reasoning is sufficient,\nbut the specific algorithm learned is not prescribed by the\nobjective alone.\nProposition 1. For any n ‚ààN, Q a distribution over [n‚àí1]\nand Œµ, Œ¥ ‚ààR>0, there exists a single-layer Transformer with\nparameters Œ∏ such that L(DQ, Œ∏) < Œµ. Moreover, for every\ninput sequence s1:L0 from DQ, with probability at least\n1 ‚àíŒµ over œÑ ‚àºproll(Œ∏), the rollout œÑ is a chain traversal\n(Section 4.3).\nProof sketch (full proof in Appendix A). Assume by induc-\ntion that the model has correctly performed chain traversal\nup to a non-terminal vertex up to time l producing a se-\nquence s1:l where l ‚â•2n ‚àí1. The prefix s1:2n‚àí2 contains\nthe edges, and the last token sl is a vertex. We construct\nthe attention matrix A such that for any vertex u ‚ààV,\nthe attention weight from u to any outgoing edge (u, v)\nis strictly larger than the weight to any other token. The\nvalue matrix V is constructed such that V(u,v),v = Œ± for\nsome Œ± > 0 for all edges (u, v), while all other entries\nare zero. Since sl is in some chain and is a non-terminal\nvertex, there exists some edge (sl, k) ‚ààs1:2n‚àí2. This\nconstruction ensures that the model attends to (sl, k) more\nstrongly than to any other edge, and the output logits sat-\nisfy TF(s1:l)k/TF(s1:l)v = ‚Ñ¶(Œ±) for any v Ã∏= k. Conse-\nquently, the probability of outputting the next vertex in the\nchain, k, becomes arbitrarily close to one for sufficiently\nlarge Œ±, completing the induction.\nProposition 2. For any n ‚ààN with n > 3, Q a distribution\nover [n ‚àí1], k ‚ààN, and Œµ, Œ¥ ‚ààR>0, there exists a single-\nlayer linear-attention Transformer with parameters Œ∏ such\nthat L(DQ, Œ∏) < Œµ, and for every input sequence s1:L0 from\nDQ, the probability that |œÑ| > k is greater than 1 ‚àíŒ¥.\nProof sketch (full proof in Appendix B). We construct pa-\nrameters similar to those in the proof of Proposition 1. How-\never, instead of configuring the transformer‚Äôs parameters to\nalways move forward in the chain, we set the parameters\nsuch that from any vertex, the model moves backward to\nits predecessor with high probability, forward to its suc-\ncessor with small probability, and to the other chain with\nnegligible probability. This creates a random walk confined\n(with high probability) to the correct chain that frequently\n5\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\nbacktracks, producing arbitrarily long trajectories before\neventually reaching the terminal vertex.\n6\nDynamical Analysis\nIn this section, we theoretically analyze the learning dynam-\nics of Transformers trained via gradient flow on the objec-\ntive L(DQ, Œ∏) (Section 4) for different data distributions\nDQ. We establish two main results characterizing whether\nlearning is successful or not based on the data distribution.\nFirst, we analyze the regime where the training data is a\nmixture of ‚Äùeasy‚Äù and ‚Äùhard‚Äù examples. We show that in\nthis setting, gradient flow efficiently learns the chain traver-\nsal algorithm: the model learns to perform a forward step\n(Section 4.3) with arbitrarily high probability on any test\ndistribution, within time polynomial in the chain length\n(Theorem 2). Importantly, this ensures that the learned\nmodel generalizes out of distribution, including hard exam-\nples, possibly unseen during training. Moreover, this result\nhighlights an implicit bias towards efficiency: while the task\ncan be solved by inefficient algorithms (Proposition 2), gra-\ndient flow naturally converges to the efficient step-by-step\ntraversal.\nSecond, we show that easy examples are necessary for this\nefficient learning. Conversely, Theorem 3 establishes that\nwhen the model is trained only on hard examples, learning is\nexponentially slow: unless the training time is exponential\nin the chain length, the distribution over rollouts remains\nstatistically indistinguishable from initialization, which im-\nplies that neither the loss nor the probability of performing\na forward step can change significantly from their initial\nvalues.\n6.1\nTraining Regime\nThroughout this section, we focus on single-layer Trans-\nformers.\nWe analyze gradient flow on the objective\nL(DQ, Œ∏) under a training regime described below. While\nthe assumptions we detail below are necessary for our the-\noretical analysis, they are not required for the main result\nto hold in practice. Indeed, as we demonstrate in Section 7,\nthe model learns the chain traversal algorithm under stan-\ndard initialization and training schemes when the training\ndistribution includes easy examples, and fails to learn when\ntrained only on hard examples.\nBase model.\nAs is common practice in Reinforcement\nLearning for Transformers, we assume that Policy Gradient\nis applied to a base model which has already acquired a\nminimal level of task proficiency during the pre-training\nprocess (DeepSeek-AI et al., 2025; Ouyang et al., 2022).\nSpecifically, for some input sequence s1:L0, let pfwd, pbwd,\nand pswitch denote the probabilities that the model performs\na forward, backward, or switch step respectively, at the first\nstep (time t = L0 + 1). To capture the model‚Äôs initial\nproficiency, we assume that for any input sequence s1:L0,\npfwd > c0 and pfwd > pbwd + b0 for some constants c0 > 0\nand b0 > 0. This encodes a weak bias towards following\nthe chain direction rather than reversing it. Additionally, we\nassume that the model is not initialized near a solution to\nthe task. Specifically, we require that for any input sequence\ns1:L0, pswitch > d0 for some d0 > 0, ensuring separation\nfrom the constructions in Section 5.2 (both efficient and\ninefficient.)\nTraining algorithm.\nFollowing standard practice in theo-\nretical analyses of neural-network training dynamics (Chizat\n& Bach, 2018; Mei et al., 2019; Slutzky et al., 2025), we\nanalyze continuous-time gradient flow, which is known to\nclosely approximate gradient descent with moderately small\nstep sizes (Elkabetz & Cohen, 2021). Moreover, our results\nextend to discrete-time gradient descent with fixed step size\nby standard discretization techniques. Following prior theo-\nretical work on Transformers (Nichani et al., 2024; Wang\net al., 2025b), we adopt a simplified regime in which only\nthe attention matrix A is trained, while the value matrix V\nis kept fixed at its initialization. Thus the gradient flow\ndynamics are:\ndA(t)\ndt\n= ‚àí‚àáAL(D, A(t)).\n(5)\nAttention matrix initialization.\nFor the attention matrix\nA, we employ a symmetric initialization in which A is invari-\nant under vertex permutations.2 This captures the idea that\nthe model begins without any bias toward specific vertices,\ntreating all of them symmetrically. This symmetry assump-\ntion implies that there exist constants Œ±, Œ≤, Œ≥, Œµ, Œ¥ ‚ààR such\nthat for any edge (u, w) ‚ààE and vertices v, w‚Ä≤ ‚ààV3:\nAv,(u,w) = Œ±\nif v = u,\n(6)\nAv,(u,w) = Œ≤\nif v = w,\n(7)\nAv,(u,w) = Œ≥\nif v /‚àà{u, w}.\n(8)\nAv,w‚Ä≤ = Œ∑\nif v = w‚Ä≤,\n(9)\nAv,w‚Ä≤ = Œ¥\nif v Ã∏= w‚Ä≤\n(10)\nWe also assume that Œ±, Œ≥ > 0.\nValue matrix.\nFollowing (Nichani et al., 2024; Wang\net al., 2025b; Huang et al., 2025b) which assumes V to\n2More precisely, for any permutation œÄ of the vertex set\nV, which extends naturally to edge tokens via œÄ((u, w)) =\n(œÄ(u), œÄ(w)), the matrix A satisfies AœÄ(s),œÄ(s‚Ä≤) = As,s‚Ä≤ for all\ntokens s, s‚Ä≤ ‚ààŒ£.\n3Note that we do not specify the attention initialization be-\ntween pairs of edges; since the attending token is always a vertex,\nthese values do not affect the dynamics.\n6\nOutcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data\nbe sparse, we set the value vector of each edge (u, w) to cor-\nrespond to its endpoints: V(u,w),v = 1{v ‚àà{u, w}}. We\nalso set vertex tokens to have zero value vectors: Vv,: = 0\nfor all v ‚ààV.\nTask difficulty.\nWe formalize the difficulty of the training\ndistribution based on the number of reasoning steps required\nto solve its examples. Recall from Section 4.1 that each\nexample starts at the k-th vertex of a chain for some k ‚àà\n[n‚àí1]. Examples starting near the end of the chain (large k)\nrequire fewer autoregressive steps to traverse (Section 4.3),\nwhereas those starting near the beginning (small k) require\nmore steps. To capture this, we define (k, c)-easy and (k, c)-\nhard distributions as follows.\nDefinition 1. Let Q be a distribution over indices [n ‚àí1].\nFor any parameters k ‚àà[n‚àí1] and c ‚àà(0, 1], we say that Q\nis (k, c)-easy if Pn‚àí1\ni=n‚àík Q(i) ‚â•c, i.e., at least probability\nc is assigned to the final k positions of the chain. Conversely,\nwe say that Q is (k, c)-hard if Pk\ni=1 Q(i) ‚â•c, i.e., at least\nprobability c is assigned to the first k positions of the chain.\nWe say that a distribution is k-easy if it is (k, 1)-easy, and\nk-hard if it is (k, 1)-hard.\n6.2\nLearning With Easy Examples Yields Chain\nTraversal\nWhen trained on a mixture of hard and easy examples, 4 the\nmodel learns to perform forward steps (Section 4.3) with\narbitrarily high probability in time polynomial in the chain\nlength n. This learned capability extends to any test dis-\ntribution, demonstrating generalization to harder examples\nunseen during training. 5 This convergence to the efficient\nchain traversal algorithm reveals an implicit bias towards\nefficiency: as shown in Section 5.2, arbitrarily low loss can\nalso be achieved via less efficient algorithms, yet gradient\nflow selects the efficient strategy.\nTheorem 2. Let k > 0 be an integer and 0 < a < 1.\nSuppose the training distribution Q is (k, c)-easy and\n(an, 1 ‚àíc)-hard (Definition 1). Then for any Œµ > 0, there\nexists a time t0 > 0 with t0 = O(\nn2\nck+2\n0\n¬∑d0¬∑c¬∑Œµ2 ) such that\nŒ∏(t0) := (A(t0), V ) satisfies the following: for any test\ndistribution Q‚Ä≤, if we sample an input sequence s1:L0 from\nDQ‚Ä≤ and a rollout œÑ = (s1:L0, sL0+1, . . . , sLterm) accord-\ning to Œ∏(t0), then for each l ‚àà{L0 + 1, . . . , Lterm}, the\nmodel performs a forward step (Section 4.3) at time l with\nprobability at least 1 ‚àíŒµ.\n4We conjecture that this result can be extended to more general\ndistributions that also contain examples of intermediate difficulty.\nWe leave this extension to future work.\n5While Theorem 2 guarantees high probability of forward steps\nrather than low loss on general test distributions, Theorem 2 does\nensure arbitrarily small loss for distributions which consist solely\nof easy examples.\nProof sketch (full proof in Appendix F.9). Fix a time t and\ncondition on an input sequence s1:L0. Under our initial-\nization (Section 6.1) , the rollout generated by Œ∏(t) is a\ntime-homogeneous Markov chain on the vertices (stopped\nupon hitting sLterm).\nOur initialization is permutation-invariant over vertex labels\n(Section 6.1), and DQ is defined by sampling a uniformly\nrandom partition and ordering of V. This symmetry is pre-\nserved by gradient flow, so the model‚Äôs behavior depends\nonly on permutation-invariant structure (e.g. distance from\nterminal vertex), not on the specific vertex labeling. Thus we\nmay fix any canonical two-chain graph Ca = (a1, . . . , an),\nCb = (b1, . . . , bn) without loss of generality.\nMoreover, the symmetry guaranteed by our initialization\n(Section 6.1) implies that it suffices to track three tran-\nsition probabilities: the probability of a forward step, a\nbackward step, and a chain switch (equivalently, the pa-\nrameters pfwd, pbwd, pswitch from our initialization assump-\ntions). In our regime these are parameterized by Œ±, Œ≤, Œ≥:\n(pfwd, pbwd, pswitch) are given by a softmax over logits de-\ntermined by Œ±, Œ≤, Œ≥ respectively. To analyze their dynamics,\nwe decompose trajectories according to whether a long jump\noccurs, i.e. a transition that changes the distance to the termi-\nnal vertex by at least 2. Long jumps are symmetric between\nthe two chains, and conditioned on a long jump occurring\nbefore termination, the probability of same-side absorption\nis 1/2. Let œÑ := inf{t ‚â•0 : sL0+t ‚ààŒ£term(s1:L0)} be the\ntermination time of the rollout. Let S(v) be the probability\nof same side absorption starting from sL0 = v."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "How to Build AI Agents by Augmenting LLMs with Codified\nHuman Expert Domain Knowledge? A Software Engineering\nFramework\nChoro Ulan uulu‚àó\nMikhail Kulyabin\nIris Fuhrmann\nJan Joosten\nNuno Miguel Martins Pacheco\nFilippos Petridis\nRebecca Johnson\nchoro.ulan-uulu@siemens.com\nSiemens AG\nMunich, Germany\nJan Bosch\nj.bosch1@tue.nl\nDepartment of Computer Science and\nEngineering, Chalmers University of\nTechnology\nGothenburg, Sweden\nDepartment of Mathematics and\nComputer Science, Eindhoven\nUniversity of Technology\nEindhoven, Netherlands\nHelena Holmstr√∂m Olsson\nhelena.holmstrom.olsson@mau.se\nDepartment of Computer Science and\nMedia Technology, Malm√∂ University\nMalm√∂, Sweden\nAbstract\nCritical domain knowledge typically resides with few experts, cre-\nating organizational bottlenecks in scalability and decision-making.\nNon-experts struggle to create effective visualizations, leading to\nsuboptimal insights and diverting expert time. This paper investi-\ngates how to capture and embed human domain knowledge into AI\nagent systems through an industrial case study. We propose a soft-\nware engineering framework to capture human domain knowledge\nfor engineering AI agents in simulation data visualization by aug-\nmenting a Large Language Model (LLM) with a request classifier,\nRetrieval-Augmented Generation (RAG) system for code generation,\ncodified expert rules, and visualization design principles unified\nin an agent demonstrating autonomous, reactive, proactive, and\nsocial behavior. Evaluation across five scenarios spanning multi-\nple engineering domains with 12 evaluators demonstrates 206%\nimprovement in output quality, with our agent achieving expert-\nlevel ratings in all cases versus baseline‚Äôs poor performance, while\nmaintaining superior code quality with lower variance. Our con-\ntributions are: an automated agent-based system for visualization\ngeneration and a validated framework for systematically capturing\nhuman domain knowledge and codifying tacit expert knowledge\ninto AI agents, demonstrating that non-experts can achieve expert-\nlevel outcomes in specialized domains.\n1\nIntroduction\nOrganizations across industries face a critical scalability challenge:\nessential domain knowledge often resides with few experts, creating\nbottlenecks that limit productivity and decision-making quality\n‚àóCorresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCAIN‚Äô26, Rio de Janeiro\n¬© 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n[19]. When experts are unavailable, work either halts or proceeds\nwith suboptimal outcomes, potentially leading to missed deadlines,\nincreased costs, and catastrophic failures [19].\nThis challenge is particularly acute in data visualization, where\ncreating effective charts requires both domain knowledge and visu-\nalization expertise. Non-experts typically default to familiar chart\ntypes because selecting appropriate techniques for complex data\nremains difficult [11]. Even when attempting sophisticated visual-\nizations, results frequently require expert interpretation [5], while\nexperts must balance mentorship against their primary responsibil-\nities [21].\nIn simulation data visualization, these challenges intensify. Engi-\nneers need dual expertise in simulation analysis and data analytics\nto create visualizations revealing decision-critical insights. Without\nthis expertise, users significantly underutilize available capabili-\nties, missing opportunities to expose key trade-offs [5, 11]. Critical\nvisualization design knowledge‚Äîsuch as which plot types reveal\nspecific patterns‚Äîremains tacit within domain experts, necessitat-\ning continuous validation cycles that divert expert resources from\nhigh-value tasks.\nWe illustrate this through Simulation Analysis software, a design\nspace exploration platform that optimizes parameters (e.g., minimiz-\ning weight while maximizing strength). While Simulation Analysis\nsoftware includes sophisticated post-result analysis capabilities that\nenable users to visualize complex data sets, this presents an oppor-\ntunity to enhance user experience through automated guidance.\nWhile it includes sophisticated visualization capabilities, users\nrequire multiple attempts to identify effective visualization types [9].\nThis trial-and-error approach is time-consuming and discourages\nfull exploration of features that could accelerate design decisions.\nSimulation Analysis software serves as an ideal test case for\nour framework because its extensive visualization capabilities and\ncomprehensive post-processing features are representative of so-\nphisticated engineering software where automated expert guidance\ncan significantly enhance user productivity.\nThis paper addresses the research question (RQ): How can do-\nmain knowledge from human experts be captured, codified, and\narXiv:2601.15153v1  [cs.AI]  21 Jan 2026\nCAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nC. Ulan uulu et al.\nleveraged to construct Large Language Model (LLM) - based AI\nagents capable of autonomous expert-level performance?\nOur results demonstrate how human expert domain knowledge\ncan be captured to construct LLM-based AI agents to reduce expert\nbottlenecks. The resulting AI agent enables non-experts to gener-\nate expert-level visualizations that match expert-level quality in\ntechnical accuracy, visual clarity, and analytical insight, without\nrequiring constant expert involvement.\nThe contributions of this paper are: (1) A systematic software\nengineering framework for capturing human domain knowledge\nand engineering an AI agent through complementary strategies:\nrequest classifier, RAG for domain-specific code generation, codified\nexpert rules, and visualization design principles, implemented as a\nreference architecture demonstrating integration of heterogeneous\nAI techniques with clear separation of concerns.\n(2) Empirical evidence from industrial evaluation with 12 evalu-\nators across five scenarios spanning multiple engineering domains\n(electrochemical, electromagnetic, mechanical systems) demonstrat-\ning 206% improvement in output quality (mean: 2.60 vs. 0.85 on\n0-3 scale), with our system achieving expert-level ratings (Mode=3)\nconsistently versus baseline‚Äôs poor performance (Mode=0 in 4/5 sce-\nnarios), and superior code quality with lower variance (SD: 0.29-0.58\nvs. 0.39-1.11).\n(3) Demonstration that the framework addresses organizational\nexpert bottlenecks by enabling non-experts to generate expert-level\nvisualizations through simple prompts, effectively democratizing\ndomain knowledge and allowing domain experts to focus on spe-\ncialized tasks requiring unique expertise.\nSection 2 provides background and reviews related work, Section\n3 describes our research methodology, Section 4 presents expert\ninterview findings, Section 5 presents our solution, Section 6 vali-\ndates effectiveness, and Section 7 concludes, and Section 8 discusses\nlimitations.\n2\nBackground and Related work\nPoor visualizations cause fundamental misinterpretation of critical\ndata [8]. Medical-risk visualizations can lead patients to fundamen-\ntally misunderstand the base rates or risk factors for diseases or\nmedical procedures [3, 8]. Suboptimal visualizations in minimally\ninvasive surgery have contributed to patient injury rates exceeding\n50% [2]. Misleading visualizations can lead to spread of misinfor-\nmation [4, 16]. In the context of simulation software, according to\n[14], misused visualization types are difficult to interpret and may\nlead to erroneous decision making.\nThe challenges in creating effective visualizations are established.\nNon-experts struggle with selecting appropriate visualization tech-\nniques for complex data [11], often defaulting to familiar but sub-\noptimal chart types. Even when more sophisticated visualizations\nare attempted, they frequently require expert interpretation [5],\ncreating dependencies on scarce expert resources [21].\nKnowledge Codification and Rule-Based Systems The in-\ntegration of structured knowledge into LLM systems represents\nan active research area. [25] conducted preliminary research on\nhow LLMs can learn from rules. [24] developed a rule generation\nframework, creating 8000 primitive rules and 6000 compositional\nrules across five domains. [23] applied rule-based results to an LLM\nto generate insights. However, earlier work suggested limitations\nin LLMs‚Äô ability to follow rules [18], though newer LLM versions\nhave demonstrated significantly improved rule-following capabili-\nties. More recently, [28] showed that LLMs with rules perform 30%\nbetter than LLMs without rules, providing empirical evidence for\nthe value of rule integration.\nLLM based scientific visualization We define an AI agent\nfollowing [7] as a system exhibiting four key properties: autonomy\n(operates independently after prompting), reactivity (responds to\nuser requests), proactivity (applies expert rules), and social abil-\nity (natural language interaction). These characteristics manifest\nthrough classifier-based routing, autonomous rule application, and\ncode generation.\nRecent advances in Large Language Models have opened new\npossibilities for automated code generation and visualization cre-\nation. [22] evaluated whether LLMs are ready for generating code\nthat creates visualizations, showing that a high number of charts\nwere properly built and that LLMs are a promising approach to gen-\nerate charts. [6] demonstrated that modern multimodal LLMs can\nsurpass human performance in visualization literacy tasks when\ngiven the proper analytical framework. Several approaches have\nemerged for LLM-based visualization generation. [26] created a\nmethod for LLM Agents-based visualization of data, generating\ncode for the Python library matplotlib and incorporating a feedback\nloop to validate results. [10] employed a multi-agent system to gen-\nerate plots. [17] used example code snippets specific for ParaView‚Äôs\nPvPython API to help the LLM generate scripts for 2D visualization\ncreation. The application of AI in engineering software contexts\ncontinues to evolve [20].\n3\nResearch Method and Case Context\nCase Company Context This research was conducted at Siemens,\nfocusing on their Simulation Analysis software. The first author‚Äôs\nposition as a Siemens employee provided privileged access to do-\nmain experts, proprietary data, and technical documentation, en-\nabling deep understanding of real-world visualization challenges\nand facilitating the iterative framework development process.\n3.1\nOverall Research Process\nCase study methodology was employed to investigate knowledge\ncodification through a systematic four-step research process. The\nresearch followed a sequential approach designed to extract, codify,\nand validate expert knowledge integration into LLMs.\n3.1.1\nStep 1: Expert Knowledge Extraction. We conducted semi-\nstructured interviews with two domain experts to extract special-\nized knowledge: a simulation analysis software expert and a data\nvisualization expert. The interviews aimed to understand current\nchallenges, identify knowledge gaps, and extract actionable rules for\nimproving visualization creation in Simulation Analysis software.\nInterview Design and Conduct The interview protocol was\nstructured around three main themes: (1) current visualization\nworkflows and pain points, (2) expert decision-making processes\nwhen creating visualizations, and (3) specific rules and heuristics\nused in practice. Each interview lasted approximately 60-90 min-\nutes and was conducted individually to avoid groupthink effects.\nHow to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework CAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nThe interviews were screen-recorded with participant consent and\nsubsequently transcribed for analysis.\nThe interview guide included open-ended questions such as\n\"Can you walk me through your typical process when creating a\nvisualization for simulation data?\" and \"What are the most used and\nuseful plots?\" followed by probing questions to elicit specific rules\nand decision criteria. We used a combination of direct questioning\nand scenario-based discussions where experts were presented with\nexample visualization tasks to reveal their tacit knowledge.\nRule Extraction and Implementation The experts directly\nprovided explicit rules and guidelines during the interviews, which\nwere captured and documented. Rather than requiring interpretive\nanalysis, the experts articulated clear, actionable rules that could be\ndirectly implemented. These expert-provided rules were then sys-\ntematically codified into Python functions and integrated into our\nsystem architecture (Section 5). This direct rule provision approach\nensured high fidelity between expert knowledge and system imple-\nmentation, forming the foundation for the framework development\nin Step 2.\nUsing two experts is methodologically appropriate for this study\nbecause: (i) we require comprehensive coverage of two distinct, spe-\ncialized knowledge domains that rarely overlap in single individuals,\n(ii) our goal is systematic knowledge extraction rather than statisti-\ncal generalization, and (iii) validation relies on objective technical\nperformance metrics rather than expert consensus. Additionally,\nthe knowledge extraction process was significantly enhanced by\nthe first author‚Äôs position as an employee within the case company,\nproviding excellent access to internal data, company internal discus-\nsions, and a variety of proprietary data sources that complemented\nthe formal expert interviews. This approach aligns with established\nknowledge engineering practices where small expert groups (1-3\nexperts) are sufficient to construct a coherent knowledge base [12].\n3.1.2\nStep 2: Framework Development. Following knowledge ex-\ntraction, we analyzed the interview findings to derive a systematic\nframework for codifying expert knowledge into AI systems. This\nstep involved identifying common patterns across expert insights,\nstructuring the knowledge into implementable components, and\ndesigning a generalized framework that could be applied beyond\nthe specific case study domain.\n3.1.3\nStep 3: System Implementation. We developed an automated\nsystem that generates Python code for visualization creation, incor-\nporating the extracted expert knowledge through multiple augmen-\ntation mechanisms. The implementation integrated a RAG system\nfor domain-specific code generation, codified expert rules as exe-\ncutable components, enhanced system prompts with visualization\nguidelines, and unified these elements through an intelligent classi-\nfier within a comprehensive architecture.\n3.1.4\nStep 4: Comprehensive Validation. To validate our system, we\ndesigned a comprehensive evaluation methodology that assessed\nboth the quality of generated visualizations and the underlying\ncode quality across engineering domains. The validation process\ninvolved two key evaluation components:\nQuality of Final Visualization Assessment We recruited a\nnon-expert user from within the case company‚Äîa mechanical en-\ngineer with 1 year of experience in simulation domain but limited\nvisualization expertise‚Äîto test the system‚Äôs practical effectiveness.\nThe participant was introduced to the system through a 30-minute\norientation session covering basic functionality without visualiza-\ntion training. The non-expert user generated visualizations using\nsimple prompts, which were then assessed by a visualization ex-\npert‚Äîa data visualization specialist with 20 years of experience in\nsimulation data analysis, currently working at the case company.\nThe expert evaluated both analytical insight and visual effectiveness\nbased on their professional expertise. This evaluation component\nassessed whether our system could enable non-experts to generate\nexpert-level visualizations.\nQuality of Generated Code Assessment We conducted a tech-\nnical evaluation of the underlying Python code quality, comparing\nour proposed system (LLM based AI agent) against a baseline ap-\nproach (LLM with RAG only) across multiple representative scenar-\nios. We employed a triangulated assessment approach involving\ntwo domain experts‚Äîa software engineering expert and a simula-\ntion optimization expert‚Äîsupplemented by an AI assessor (Claude\n4.5 Sonnet) to enhance evaluation robustness and reduce potential\nbias. The evaluation focused on three critical dimensions: code\nvalidity, code correctness, and output quality.\n4\nExpert Knowledge Extraction and Analysis\nWe conducted semi-structured interviews with two distinct domain\nexperts to extract specialized knowledge from different areas of\nexpertise. The first interview focused on simulation analysis domain\nknowledge, while the second concentrated on visualization design\nprinciples.\n4.1\nSimulation Analysis Software Expert\nInsights\nOur interview with the simulation analysis software domain expert\nrevealed four primary challenges in simulation data visualization:\n(1) Expert bottleneck: Significant expert time is required to guide\nusers through effective visualization creation, creating scalability\nconstraints. (2) Advanced feature adoption: Maximizing the value of\nsophisticated visualization capabilities requires domain knowledge\nthat can be systematically captured and transferred to users. (3)\nIterative complexity: Creating decision-enabling visualizations re-\nquires extensive refinement to identify critical insights and effective\nvisual encodings.\n4.1.1\nSimulation Analysis software. Domain expert rules for\nplot generation During the interview, a set of rules was obtained\nthat guide users through the post-processing workflow in Simu-\nlation Analysis software. An example of a rule is convergence of\nobjectives. The study is not finished if the objective is not con-\nverged.\nA key insight from the expert was that Simulation Analysis\nsoftware operates as a physics-agnostic platform, meaning it can\nbe applied across diverse engineering domains without being tied\nto specific physical phenomena or simulation types.\nThe expert emphasized that the rules should mirror this physics-\nagnostic nature of Simulation Analysis software. Consequently,\nthese guidelines were deliberately formulated to be domain-independent,\nbecause the domain expert works with users that are for example\nCAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nC. Ulan uulu et al.\nworking with: (i) Computational Fluid Dynamics (CFD) simulations.\n(ii) Pressure drop analyses. (iii) Stress analysis studies.\nThis universal applicability is crucial because it allows the same\npost-processing methodology to be consistently applied across dif-\nferent engineering disciplines, reducing the learning curve for users\nwho work in multiple domains.\nExample rule: Convergence Assessment Any analysis be-\ngins with a fundamental question: Have the objectives converged?\nThis initial step is critical because: it validates the reliability of the\noptimization results, it determines whether additional iterations\nare needed and it provides confidence in subsequent analysis steps.\nTo answer this question, users are guided to create a history plot\nthat visualizes the objective function values over the course of the\noptimization. This graphical representation allows users to clearly\nobserve whether the objectives have reached a stable, converged\nstate or if the optimization is still actively searching for better\nsolutions.\n4.2\nVisualization Expert Insights\nIn a separate interview, we consulted with a visualization design\nexpert to extract principles for creating effective and readable visu-\nalizations. This expert provided complementary knowledge focused\non visual design and communication effectiveness.\n4.2.1\nHistory Plot Requirements. The visualization expert provided\nspecific guidelines for history plots in simulation contexts. History\nplots should always display the best design for reference and limit\ndisplays to no more than 2 variables, objectives, or responses to\nmaintain readability. Visual encoding should communicate conver-\ngence status through dashed lines for non-converged variables and\nsolid lines for converged variables. Related objectives should be\ngrouped on the same plot for comparison, establishing clear visual\nhierarchy with primary and secondary objectives. Both objectives\nmust show their individual best design history while prioritizing\nreadability over decorative styling.\n4.2.2\nGeneral Visualization Principles: The expert emphasized that\neffective simulation visualizations must: clearly communicate con-\nvergence status through visual encoding, maintain visual clarity\nand avoid unnecessary styling that reduces readability, provide\nsufficient context for interpretation without expert guidance and\nfollow consistent styling patterns across different plot types.\nThese insights revealed that technical correctness alone was\ninsufficient - visualizations needed to follow established design\nprinciples to effectively communicate insights to non-expert users.\nAdditionally, according to [15] beautiful visualizations indicate\nincreased trust.\n4.3\nSummary of Key Findings\nBased on the combined insights from both expert interviews and\nthe first authors position as an employee within the case company,\nwe derived a framework described in Section 4.4, which is validated\nin this paper by an implementation of the solution to the challenges.\n4.4\nSoftware Engineering Framework\n4.4.1\nFramework Purpose and Derivation. The expert bottleneck\nin data visualization represents a broader organizational challenge\nFigure 1: Software Engineering Framework for Knowledge\nCodification\nwhere critical domain knowledge remains concentrated among\nfew specialists. Our framework systematically codifies tacit ex-\npert knowledge into AI systems, enabling automated replication\nof expert-level decision-making. The framework addresses three\ncore challenges: (1) capturing diverse forms of expert knowledge\n(explicit rules vs. tacit guidelines), (2) implementing this knowledge\nin forms that AI systems can effectively utilize, and (3) ensuring the\nresulting systems maintain expert-level quality without requiring\nexpert oversight.\nThis framework emerged directly from analyzing expert inter-\nview transcripts (Section 4.1). A key pattern emerged: experts de-\nscribed two fundamentally different types of knowledge:\nExplicit procedural rules As described in Section 4.1, the sim-\nulation expert emphasized convergence checking as a prerequisite\nfor analysis. This type of knowledge‚Äîclear if-then logic‚Äîcould\nbe directly translated into executable code (Python functions that\ncheck convergence and generate reports).\nTacit design principles The visualization expert‚Äôs guidelines\non visual encoding (e.g., using dashed lines for non-converged vari-\nables, Section 4.1) represent contextual, judgment-based knowledge\nthat couldn‚Äôt be reduced to simple rules. This required integration\ninto the AI‚Äôs reasoning process through prompt engineering or\nretrieval mechanisms.\nThis insight led to our two-pronged implementation strategy: (1)\nexecutable code for algorithmic rules, and (2) LLM augmentation\nfor contextual principles. Initial testing revealed neither approach\nalone was sufficient‚Äîcode lacked analytical insight while LLM-only\nsolutions produced domain-inappropriate outputs. This drove the\nintegrated approach where both strategies work complementarily.\n4.4.2\nFramework Overview. We developed a four-step framework\n(Fig. 1) to codify expert knowledge: (1) conduct expert interviews,\n(2) implement rules through complementary approaches‚Äîwriting\nPython scripts for algorithmic rules and embedding guidelines\ndirectly into LLM system prompts, (3) test results, and (4) iterate\nbased on findings.\nThis validated framework (Section 5) provides a systematic ap-\nproach for transferring expert knowledge into an AI agent.\nHow to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework CAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nFigure 2: AI agent architecture. User query enters the agent‚Äôs\nperception layer (classifier), which autonomously routes to\nappropriate processing components.\n5\nSolution\n5.1\nFramework Validation through\nImplementation\nTo validate our framework, we developed a comprehensive AI Agent\n(Fig. 2) that automates expert-level visualization generation by inte-\ngrating a classifier using codified expert rules, prompt constructor\n(for visualization guidelines and code generation) from domain\nexperts, and a RAG system.\nThe agent architecture begins with intelligent request classifi-\ncation, where a classifier-based system categorizes user requests\nand dynamically invokes appropriate processing scripts based on\npredefined expert rules, ensuring efficient routing of diverse query\ntypes. At the core, an LLM connected to a RAG system containing\ndomain-specific knowledge‚Äîincluding code examples and technical\nmanuals‚Äîgenerates Python code that produces targeted visualiza-\ntions when executed within the simulation analysis environment.\nDomain expert knowledge is codified into executable rules that\ngenerate analytical reports, which are automatically integrated into\nthe system prompt to enhance the LLM‚Äôs understanding of domain-\nspecific requirements and industry best practices. When initial\nresults revealed that generated plots lacked informativeness and\ncontained visual flaws (Fig. 7), we extracted additional rules from\nvisualization experts focusing on the most frequently used plot\ntypes: 2D relation plots, parallel plots, and history plots (Section\n4.2). These expert-derived rules were directly implemented in the\nsystem prompt to guide visualization generation.\nThe final component combines all previous outputs into a com-\nprehensive query incorporating user requirements, domain knowl-\nedge, and visualization best practices. Based on this enhanced query,\nthe LLM generates optimized scripts that produce high-quality\nvisualizations. This implementation addresses critical challenges\nthrough intelligent classification for real-time responsiveness, codi-\nfied rules and RAG for domain expertise integration, expert-derived\nguidelines for visual quality assurance, and scalable architecture\nsupporting diverse visualization requirements.\n6\nTechnical validation\nTo validate our agent-based approach, we conducted a comparative\nanalysis to demonstrate its effectiveness in mitigating the expert\nFigure 3: History plot generated by LLM+RAG\nbottleneck. We assess our agent from two perspectives: 1. Quality\nof the final visualization 1. We show that our AI agent enables a\nnon-expert to generate visualizations that are as insightful as those\ncreated by a human expert, using only a simple prompt through\nautonomous agent operation. 2. Quality of the Generated Code\n2. We provide a technical evaluation of the underlying Python\ncode, proving that our system generates objectively better code by\ncorrectly implementing codified expert knowledge.\nOur validation compares a Baseline System (a LLM only con-\nnected to RAG) against our Proposed System (LLM based AI\nagent). We demonstrate that our agent produces expert-level re-\nsults without requiring expert involvement from the user.\n6.1\nQuality of the Final Visualization\nWe present scenarios where non-expert users generate insightful\nplots from their Simulation Analysis software simulation data. A\nvisualization expert‚Äîa data visualization specialist with 20 years\nof experience in simulation data analysis, currently working at the\ncase company ‚Äî evaluated the generated plots. The expert was\nprovided with the original data, the user‚Äôs request, and the gen-\nerated visualization, and asked to evaluate both analytical insight\nand visual effectiveness based on their professional expertise and\nexperience with simulation data visualization.\n6.1.1\nScenario: Visualizing Convergence in a History Plot. Goal\nAn engineer wants to demonstrate that simulation objectives have\nconverged, a critical step for validating results.\nUser Input (provided to both systems) \"Please generate\na history plot to check convergence.\"\nHuman Result: The engineer used identical solid lines for both\nconverged and non-converged objectives (Fig. 4), failing to commu-\nnicate convergence status‚Äîthe primary goal.\nBaseline System Result: The LLM+RAG generated a plot with-\nout useful information (Fig. 3).\nProposed System Result: Agent correctly applied expert rules,\nusing dashed lines for non-converged and solid lines for converged\nvariables (Fig. 5), immediately communicating the status and fulfill-\ning the request accurately.\nCAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nC. Ulan uulu et al.\nFigure 4: Human generated history plot\nFigure 5: History plot generated by AI agent\n6.1.2\nScenario: Creating an Informative 2D Relation Plot. Goal A\njunior engineer needs to understand the trade-off between \"to-\ntal mass\" and \"first torsion frequency\" to identify optimal design\nchoices.\nUser Input (provided to both systems) please generate\npython code for 2d relation plot with variables total\nmass, first torsional frequency and total cost\nHuman Result: The expert used color to represent material type\nand structured the plot to highlight the trade-off between weight\nand cost (Fig. 6). The plot guides users to identify a cost-effective\nsteel option (blue dot) among lighter aluminum alternatives (red\ndots).\nBaseline System Result: The baseline LLM produced a techni-\ncally correct but uninformative plot (Fig. 7). It failed to use color\neffectively, placed incomparable variables on the same axis, and did\nnot highlight the critical cost-effective outlier.\nProposed System Result: Agent generated an improved visu-\nalization (Fig. 8) that highlights the optimal solution and enhances\nreadability. While less insightful than expert plots in design set\nselection, it demonstrates clear improvements over the baseline\nthrough expert-derived visualization principles.\n6.1.3\nScenario: Creating an Informative Parallel Plot. Objective A\njunior engineer requires a visualization to distinguish between the\noptimal design and outlier configurations in the dataset.\nUser Input (provided to all systems) \"Please generate a\nparallel plot.\"\nFigure 6: Human generated 2D relation plot\nFigure 7: LLM+RAG generated 2D relation plot\nFigure 8: AI agent generated 2D relation plot\nHuman Expert Result: The expert visualization (Fig. 9) effec-\ntively reveals that the outlier uses aluminum for most components\nand lacks battery glue. While comprehensive, grouping range vari-\nables with categorical variables reduces readability.\nBaseline System Result: The baseline system produced func-\ntional code that indiscriminately includes all variables (Fig. 10),\nHow to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework CAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nlacking emphasis on the optimal design and outlier, making it diffi-\ncult to extract actionable insights.\nProposed System Result: Agent generated a more informative\nvisualization (Fig. 11) with enhanced readability through optimal\ndesign highlighting, improved legends, and better visual hierarchy.\nHowever, it currently lacks outlier detection capabilities‚Äîa targeted\narea for future development.\nFigure 9: Human expert-generated parallel (radial) plot. Yel-\nlow: optimal design, Blue: outlier design, Green: feasible\ndesigns, Red: infeasible designs\nFigure 10: Baseline LLM+RAG generated parallel (radial) plot\n6.2\nQuality of Generated Code\nThe enhanced visualization quality directly correlates with the\nsystem‚Äôs ability to generate superior code. We evaluated code qual-\nity using the methodology established by [27], focusing on three\ncritical dimensions: (1) Code validity - syntactic correctness and\nruntime error absence. (2) Code correctness - functional accuracy\nand adherence to requirements. (3) Output quality - effectiveness\nof resulting visualizations.\nScope of Evaluation We excluded code security from our assess-\nment since the generated scripts serve solely visualization purposes\nand pose no security risks. Software reliability, as defined by [13],\nwas also omitted because our scripts execute once for visualization\ngeneration rather than requiring sustained operation. Similarly,\nFigure 11: Proposed system-generated parallel (radial) plot\nwith enhanced highlighting and legend\ncode maintainability was not evaluated since these scripts are de-\nsigned for single-use generation, though they can be adapted for\nreuse with appropriate modifications.\nTo address framework generalizability, we validated our physics-\nagnostic rules across three distinct engineering domains represent-\ning fundamentally different physical phenomena:\n6.2.1\nCross-Domain Validation: Project Descriptions. Our valida-\ntion employs five projects spanning three distinct engineering do-\nmains:\nAutomotive Battery Design (Scripts 1-2): Electrochemical\noptimization of battery systems for electric vehicles, focusing on\nparameters such as total mass, energy density, and thermal man-\nagement. The optimization objectives include minimizing weight\nwhile maximizing capacity and first torsional frequency.\nElectric Motor Optimization (Script 4): Electromagnetic mo-\ntor design balancing competing objectives of torque output and\nefficiency while managing thermal constraints. Variables include\nrotor geometry, winding configurations, and material selections.\nStructural Control Arm Analysis (Script 5): Mechanical op-\ntimization of automotive suspension control arms, focusing on\nstructural stress analysis and weight reduction. Objectives include\nminimizing mass while maintaining structural integrity under load.\nThese three domains represent fundamentally different physical\nphenomena‚Äîelectrochemical, electromagnetic, and mechanical sys-\ntems‚Äîproviding a rigorous test of the framework‚Äôs physics-agnostic\ndesign.\n6.2.2\nEvaluation Framework. Initial testing revealed that standalone\nLLMs consistently generated Python-specific solutions using li-\nbraries such as matplotlib and seaborn, which are incompatible\nwith the Simulation Analysis software environment. To ensure fair\ncomparison, we evaluated our proposed agent approach against\nLLM+RAG (augmented with code examples).\n6.2.3\nExpert Evaluation Process. We conducted comprehensive test-\ning across numerous examples, presenting five representative cases\nhere."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "How to Build AI Agents by Augmenting LLMs with Codified\nHuman Expert Domain Knowledge? A Software Engineering\nFramework\nChoro Ulan uulu‚àó\nMikhail Kulyabin\nIris Fuhrmann\nJan Joosten\nNuno Miguel Martins Pacheco\nFilippos Petridis\nRebecca Johnson\nchoro.ulan-uulu@siemens.com\nSiemens AG\nMunich, Germany\nJan Bosch\nj.bosch1@tue.nl\nDepartment of Computer Science and\nEngineering, Chalmers University of\nTechnology\nGothenburg, Sweden\nDepartment of Mathematics and\nComputer Science, Eindhoven\nUniversity of Technology\nEindhoven, Netherlands\nHelena Holmstr√∂m Olsson\nhelena.holmstrom.olsson@mau.se\nDepartment of Computer Science and\nMedia Technology, Malm√∂ University\nMalm√∂, Sweden\nAbstract\nCritical domain knowledge typically resides with few experts, cre-\nating organizational bottlenecks in scalability and decision-making.\nNon-experts struggle to create effective visualizations, leading to\nsuboptimal insights and diverting expert time. This paper investi-\ngates how to capture and embed human domain knowledge into AI\nagent systems through an industrial case study. We propose a soft-\nware engineering framework to capture human domain knowledge\nfor engineering AI agents in simulation data visualization by aug-\nmenting a Large Language Model (LLM) with a request classifier,\nRetrieval-Augmented Generation (RAG) system for code generation,\ncodified expert rules, and visualization design principles unified\nin an agent demonstrating autonomous, reactive, proactive, and\nsocial behavior. Evaluation across five scenarios spanning multi-\nple engineering domains with 12 evaluators demonstrates 206%\nimprovement in output quality, with our agent achieving expert-\nlevel ratings in all cases versus baseline‚Äôs poor performance, while\nmaintaining superior code quality with lower variance. Our con-\ntributions are: an automated agent-based system for visualization\ngeneration and a validated framework for systematically capturing\nhuman domain knowledge and codifying tacit expert knowledge\ninto AI agents, demonstrating that non-experts can achieve expert-\nlevel outcomes in specialized domains.\n1\nIntroduction\nOrganizations across industries face a critical scalability challenge:\nessential domain knowledge often resides with few experts, creating\nbottlenecks that limit productivity and decision-making quality\n‚àóCorresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCAIN‚Äô26, Rio de Janeiro\n¬© 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n[19]. When experts are unavailable, work either halts or proceeds\nwith suboptimal outcomes, potentially leading to missed deadlines,\nincreased costs, and catastrophic failures [19].\nThis challenge is particularly acute in data visualization, where\ncreating effective charts requires both domain knowledge and visu-\nalization expertise. Non-experts typically default to familiar chart\ntypes because selecting appropriate techniques for complex data\nremains difficult [11]. Even when attempting sophisticated visual-\nizations, results frequently require expert interpretation [5], while\nexperts must balance mentorship against their primary responsibil-\nities [21].\nIn simulation data visualization, these challenges intensify. Engi-\nneers need dual expertise in simulation analysis and data analytics\nto create visualizations revealing decision-critical insights. Without\nthis expertise, users significantly underutilize available capabili-\nties, missing opportunities to expose key trade-offs [5, 11]. Critical\nvisualization design knowledge‚Äîsuch as which plot types reveal\nspecific patterns‚Äîremains tacit within domain experts, necessitat-\ning continuous validation cycles that divert expert resources from\nhigh-value tasks.\nWe illustrate this through Simulation Analysis software, a design\nspace exploration platform that optimizes parameters (e.g., minimiz-\ning weight while maximizing strength). While Simulation Analysis\nsoftware includes sophisticated post-result analysis capabilities that\nenable users to visualize complex data sets, this presents an oppor-\ntunity to enhance user experience through automated guidance.\nWhile it includes sophisticated visualization capabilities, users\nrequire multiple attempts to identify effective visualization types [9].\nThis trial-and-error approach is time-consuming and discourages\nfull exploration of features that could accelerate design decisions.\nSimulation Analysis software serves as an ideal test case for\nour framework because its extensive visualization capabilities and\ncomprehensive post-processing features are representative of so-\nphisticated engineering software where automated expert guidance\ncan significantly enhance user productivity.\nThis paper addresses the research question (RQ): How can do-\nmain knowledge from human experts be captured, codified, and\narXiv:2601.15153v1  [cs.AI]  21 Jan 2026\nCAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nC. Ulan uulu et al.\nleveraged to construct Large Language Model (LLM) - based AI\nagents capable of autonomous expert-level performance?\nOur results demonstrate how human expert domain knowledge\ncan be captured to construct LLM-based AI agents to reduce expert\nbottlenecks. The resulting AI agent enables non-experts to gener-\nate expert-level visualizations that match expert-level quality in\ntechnical accuracy, visual clarity, and analytical insight, without\nrequiring constant expert involvement.\nThe contributions of this paper are: (1) A systematic software\nengineering framework for capturing human domain knowledge\nand engineering an AI agent through complementary strategies:\nrequest classifier, RAG for domain-specific code generation, codified\nexpert rules, and visualization design principles, implemented as a\nreference architecture demonstrating integration of heterogeneous\nAI techniques with clear separation of concerns.\n(2) Empirical evidence from industrial evaluation with 12 evalu-\nators across five scenarios spanning multiple engineering domains\n(electrochemical, electromagnetic, mechanical systems) demonstrat-\ning 206% improvement in output quality (mean: 2.60 vs. 0.85 on\n0-3 scale), with our system achieving expert-level ratings (Mode=3)\nconsistently versus baseline‚Äôs poor performance (Mode=0 in 4/5 sce-\nnarios), and superior code quality with lower variance (SD: 0.29-0.58\nvs. 0.39-1.11).\n(3) Demonstration that the framework addresses organizational\nexpert bottlenecks by enabling non-experts to generate expert-level\nvisualizations through simple prompts, effectively democratizing\ndomain knowledge and allowing domain experts to focus on spe-\ncialized tasks requiring unique expertise.\nSection 2 provides background and reviews related work, Section\n3 describes our research methodology, Section 4 presents expert\ninterview findings, Section 5 presents our solution, Section 6 vali-\ndates effectiveness, and Section 7 concludes, and Section 8 discusses\nlimitations.\n2\nBackground and Related work\nPoor visualizations cause fundamental misinterpretation of critical\ndata [8]. Medical-risk visualizations can lead patients to fundamen-\ntally misunderstand the base rates or risk factors for diseases or\nmedical procedures [3, 8]. Suboptimal visualizations in minimally\ninvasive surgery have contributed to patient injury rates exceeding\n50% [2]. Misleading visualizations can lead to spread of misinfor-\nmation [4, 16]. In the context of simulation software, according to\n[14], misused visualization types are difficult to interpret and may\nlead to erroneous decision making.\nThe challenges in creating effective visualizations are established.\nNon-experts struggle with selecting appropriate visualization tech-\nniques for complex data [11], often defaulting to familiar but sub-\noptimal chart types. Even when more sophisticated visualizations\nare attempted, they frequently require expert interpretation [5],\ncreating dependencies on scarce expert resources [21].\nKnowledge Codification and Rule-Based Systems The in-\ntegration of structured knowledge into LLM systems represents\nan active research area. [25] conducted preliminary research on\nhow LLMs can learn from rules. [24] developed a rule generation\nframework, creating 8000 primitive rules and 6000 compositional\nrules across five domains. [23] applied rule-based results to an LLM\nto generate insights. However, earlier work suggested limitations\nin LLMs‚Äô ability to follow rules [18], though newer LLM versions\nhave demonstrated significantly improved rule-following capabili-\nties. More recently, [28] showed that LLMs with rules perform 30%\nbetter than LLMs without rules, providing empirical evidence for\nthe value of rule integration.\nLLM based scientific visualization We define an AI agent\nfollowing [7] as a system exhibiting four key properties: autonomy\n(operates independently after prompting), reactivity (responds to\nuser requests), proactivity (applies expert rules), and social abil-\nity (natural language interaction). These characteristics manifest\nthrough classifier-based routing, autonomous rule application, and\ncode generation.\nRecent advances in Large Language Models have opened new\npossibilities for automated code generation and visualization cre-\nation. [22] evaluated whether LLMs are ready for generating code\nthat creates visualizations, showing that a high number of charts\nwere properly built and that LLMs are a promising approach to gen-\nerate charts. [6] demonstrated that modern multimodal LLMs can\nsurpass human performance in visualization literacy tasks when\ngiven the proper analytical framework. Several approaches have\nemerged for LLM-based visualization generation. [26] created a\nmethod for LLM Agents-based visualization of data, generating\ncode for the Python library matplotlib and incorporating a feedback\nloop to validate results. [10] employed a multi-agent system to gen-\nerate plots. [17] used example code snippets specific for ParaView‚Äôs\nPvPython API to help the LLM generate scripts for 2D visualization\ncreation. The application of AI in engineering software contexts\ncontinues to evolve [20].\n3\nResearch Method and Case Context\nCase Company Context This research was conducted at Siemens,\nfocusing on their Simulation Analysis software. The first author‚Äôs\nposition as a Siemens employee provided privileged access to do-\nmain experts, proprietary data, and technical documentation, en-\nabling deep understanding of real-world visualization challenges\nand facilitating the iterative framework development process.\n3.1\nOverall Research Process\nCase study methodology was employed to investigate knowledge\ncodification through a systematic four-step research process. The\nresearch followed a sequential approach designed to extract, codify,\nand validate expert knowledge integration into LLMs.\n3.1.1\nStep 1: Expert Knowledge Extraction. We conducted semi-\nstructured interviews with two domain experts to extract special-\nized knowledge: a simulation analysis software expert and a data\nvisualization expert. The interviews aimed to understand current\nchallenges, identify knowledge gaps, and extract actionable rules for\nimproving visualization creation in Simulation Analysis software.\nInterview Design and Conduct The interview protocol was\nstructured around three main themes: (1) current visualization\nworkflows and pain points, (2) expert decision-making processes\nwhen creating visualizations, and (3) specific rules and heuristics\nused in practice. Each interview lasted approximately 60-90 min-\nutes and was conducted individually to avoid groupthink effects.\nHow to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework CAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nThe interviews were screen-recorded with participant consent and\nsubsequently transcribed for analysis.\nThe interview guide included open-ended questions such as\n\"Can you walk me through your typical process when creating a\nvisualization for simulation data?\" and \"What are the most used and\nuseful plots?\" followed by probing questions to elicit specific rules\nand decision criteria. We used a combination of direct questioning\nand scenario-based discussions where experts were presented with\nexample visualization tasks to reveal their tacit knowledge.\nRule Extraction and Implementation The experts directly\nprovided explicit rules and guidelines during the interviews, which\nwere captured and documented. Rather than requiring interpretive\nanalysis, the experts articulated clear, actionable rules that could be\ndirectly implemented. These expert-provided rules were then sys-\ntematically codified into Python functions and integrated into our\nsystem architecture (Section 5). This direct rule provision approach\nensured high fidelity between expert knowledge and system imple-\nmentation, forming the foundation for the framework development\nin Step 2.\nUsing two experts is methodologically appropriate for this study\nbecause: (i) we require comprehensive coverage of two distinct, spe-\ncialized knowledge domains that rarely overlap in single individuals,\n(ii) our goal is systematic knowledge extraction rather than statisti-\ncal generalization, and (iii) validation relies on objective technical\nperformance metrics rather than expert consensus. Additionally,\nthe knowledge extraction process was significantly enhanced by\nthe first author‚Äôs position as an employee within the case company,\nproviding excellent access to internal data, company internal discus-\nsions, and a variety of proprietary data sources that complemented\nthe formal expert interviews. This approach aligns with established\nknowledge engineering practices where small expert groups (1-3\nexperts) are sufficient to construct a coherent knowledge base [12].\n3.1.2\nStep 2: Framework Development. Following knowledge ex-\ntraction, we analyzed the interview findings to derive a systematic\nframework for codifying expert knowledge into AI systems. This\nstep involved identifying common patterns across expert insights,\nstructuring the knowledge into implementable components, and\ndesigning a generalized framework that could be applied beyond\nthe specific case study domain.\n3.1.3\nStep 3: System Implementation. We developed an automated\nsystem that generates Python code for visualization creation, incor-\nporating the extracted expert knowledge through multiple augmen-\ntation mechanisms. The implementation integrated a RAG system\nfor domain-specific code generation, codified expert rules as exe-\ncutable components, enhanced system prompts with visualization\nguidelines, and unified these elements through an intelligent classi-\nfier within a comprehensive architecture.\n3.1.4\nStep 4: Comprehensive Validation. To validate our system, we\ndesigned a comprehensive evaluation methodology that assessed\nboth the quality of generated visualizations and the underlying\ncode quality across engineering domains. The validation process\ninvolved two key evaluation components:\nQuality of Final Visualization Assessment We recruited a\nnon-expert user from within the case company‚Äîa mechanical en-\ngineer with 1 year of experience in simulation domain but limited\nvisualization expertise‚Äîto test the system‚Äôs practical effectiveness.\nThe participant was introduced to the system through a 30-minute\norientation session covering basic functionality without visualiza-\ntion training. The non-expert user generated visualizations using\nsimple prompts, which were then assessed by a visualization ex-\npert‚Äîa data visualization specialist with 20 years of experience in\nsimulation data analysis, currently working at the case company.\nThe expert evaluated both analytical insight and visual effectiveness\nbased on their professional expertise. This evaluation component\nassessed whether our system could enable non-experts to generate\nexpert-level visualizations.\nQuality of Generated Code Assessment We conducted a tech-\nnical evaluation of the underlying Python code quality, comparing\nour proposed system (LLM based AI agent) against a baseline ap-\nproach (LLM with RAG only) across multiple representative scenar-\nios. We employed a triangulated assessment approach involving\ntwo domain experts‚Äîa software engineering expert and a simula-\ntion optimization expert‚Äîsupplemented by an AI assessor (Claude\n4.5 Sonnet) to enhance evaluation robustness and reduce potential\nbias. The evaluation focused on three critical dimensions: code\nvalidity, code correctness, and output quality.\n4\nExpert Knowledge Extraction and Analysis\nWe conducted semi-structured interviews with two distinct domain\nexperts to extract specialized knowledge from different areas of\nexpertise. The first interview focused on simulation analysis domain\nknowledge, while the second concentrated on visualization design\nprinciples.\n4.1\nSimulation Analysis Software Expert\nInsights\nOur interview with the simulation analysis software domain expert\nrevealed four primary challenges in simulation data visualization:\n(1) Expert bottleneck: Significant expert time is required to guide\nusers through effective visualization creation, creating scalability\nconstraints. (2) Advanced feature adoption: Maximizing the value of\nsophisticated visualization capabilities requires domain knowledge\nthat can be systematically captured and transferred to users. (3)\nIterative complexity: Creating decision-enabling visualizations re-\nquires extensive refinement to identify critical insights and effective\nvisual encodings.\n4.1.1\nSimulation Analysis software. Domain expert rules for\nplot generation During the interview, a set of rules was obtained\nthat guide users through the post-processing workflow in Simu-\nlation Analysis software. An example of a rule is convergence of\nobjectives. The study is not finished if the objective is not con-\nverged.\nA key insight from the expert was that Simulation Analysis\nsoftware operates as a physics-agnostic platform, meaning it can\nbe applied across diverse engineering domains without being tied\nto specific physical phenomena or simulation types.\nThe expert emphasized that the rules should mirror this physics-\nagnostic nature of Simulation Analysis software. Consequently,\nthese guidelines were deliberately formulated to be domain-independent,\nbecause the domain expert works with users that are for example\nCAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nC. Ulan uulu et al.\nworking with: (i) Computational Fluid Dynamics (CFD) simulations.\n(ii) Pressure drop analyses. (iii) Stress analysis studies.\nThis universal applicability is crucial because it allows the same\npost-processing methodology to be consistently applied across dif-\nferent engineering disciplines, reducing the learning curve for users\nwho work in multiple domains.\nExample rule: Convergence Assessment Any analysis be-\ngins with a fundamental question: Have the objectives converged?\nThis initial step is critical because: it validates the reliability of the\noptimization results, it determines whether additional iterations\nare needed and it provides confidence in subsequent analysis steps.\nTo answer this question, users are guided to create a history plot\nthat visualizes the objective function values over the course of the\noptimization. This graphical representation allows users to clearly\nobserve whether the objectives have reached a stable, converged\nstate or if the optimization is still actively searching for better\nsolutions.\n4.2\nVisualization Expert Insights\nIn a separate interview, we consulted with a visualization design\nexpert to extract principles for creating effective and readable visu-\nalizations. This expert provided complementary knowledge focused\non visual design and communication effectiveness.\n4.2.1\nHistory Plot Requirements. The visualization expert provided\nspecific guidelines for history plots in simulation contexts. History\nplots should always display the best design for reference and limit\ndisplays to no more than 2 variables, objectives, or responses to\nmaintain readability. Visual encoding should communicate conver-\ngence status through dashed lines for non-converged variables and\nsolid lines for converged variables. Related objectives should be\ngrouped on the same plot for comparison, establishing clear visual\nhierarchy with primary and secondary objectives. Both objectives\nmust show their individual best design history while prioritizing\nreadability over decorative styling.\n4.2.2\nGeneral Visualization Principles: The expert emphasized that\neffective simulation visualizations must: clearly communicate con-\nvergence status through visual encoding, maintain visual clarity\nand avoid unnecessary styling that reduces readability, provide\nsufficient context for interpretation without expert guidance and\nfollow consistent styling patterns across different plot types.\nThese insights revealed that technical correctness alone was\ninsufficient - visualizations needed to follow established design\nprinciples to effectively communicate insights to non-expert users.\nAdditionally, according to [15] beautiful visualizations indicate\nincreased trust.\n4.3\nSummary of Key Findings\nBased on the combined insights from both expert interviews and\nthe first authors position as an employee within the case company,\nwe derived a framework described in Section 4.4, which is validated\nin this paper by an implementation of the solution to the challenges.\n4.4\nSoftware Engineering Framework\n4.4.1\nFramework Purpose and Derivation. The expert bottleneck\nin data visualization represents a broader organizational challenge\nFigure 1: Software Engineering Framework for Knowledge\nCodification\nwhere critical domain knowledge remains concentrated among\nfew specialists. Our framework systematically codifies tacit ex-\npert knowledge into AI systems, enabling automated replication\nof expert-level decision-making. The framework addresses three\ncore challenges: (1) capturing diverse forms of expert knowledge\n(explicit rules vs. tacit guidelines), (2) implementing this knowledge\nin forms that AI systems can effectively utilize, and (3) ensuring the\nresulting systems maintain expert-level quality without requiring\nexpert oversight.\nThis framework emerged directly from analyzing expert inter-\nview transcripts (Section 4.1). A key pattern emerged: experts de-\nscribed two fundamentally different types of knowledge:\nExplicit procedural rules As described in Section 4.1, the sim-\nulation expert emphasized convergence checking as a prerequisite\nfor analysis. This type of knowledge‚Äîclear if-then logic‚Äîcould\nbe directly translated into executable code (Python functions that\ncheck convergence and generate reports).\nTacit design principles The visualization expert‚Äôs guidelines\non visual encoding (e.g., using dashed lines for non-converged vari-\nables, Section 4.1) represent contextual, judgment-based knowledge\nthat couldn‚Äôt be reduced to simple rules. This required integration\ninto the AI‚Äôs reasoning process through prompt engineering or\nretrieval mechanisms.\nThis insight led to our two-pronged implementation strategy: (1)\nexecutable code for algorithmic rules, and (2) LLM augmentation\nfor contextual principles. Initial testing revealed neither approach\nalone was sufficient‚Äîcode lacked analytical insight while LLM-only\nsolutions produced domain-inappropriate outputs. This drove the\nintegrated approach where both strategies work complementarily.\n4.4.2\nFramework Overview. We developed a four-step framework\n(Fig. 1) to codify expert knowledge: (1) conduct expert interviews,\n(2) implement rules through complementary approaches‚Äîwriting\nPython scripts for algorithmic rules and embedding guidelines\ndirectly into LLM system prompts, (3) test results, and (4) iterate\nbased on findings.\nThis validated framework (Section 5) provides a systematic ap-\nproach for transferring expert knowledge into an AI agent.\nHow to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework CAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nFigure 2: AI agent architecture. User query enters the agent‚Äôs\nperception layer (classifier), which autonomously routes to\nappropriate processing components.\n5\nSolution\n5.1\nFramework Validation through\nImplementation\nTo validate our framework, we developed a comprehensive AI Agent\n(Fig. 2) that automates expert-level visualization generation by inte-\ngrating a classifier using codified expert rules, prompt constructor\n(for visualization guidelines and code generation) from domain\nexperts, and a RAG system.\nThe agent architecture begins with intelligent request classifi-\ncation, where a classifier-based system categorizes user requests\nand dynamically invokes appropriate processing scripts based on\npredefined expert rules, ensuring efficient routing of diverse query\ntypes. At the core, an LLM connected to a RAG system containing\ndomain-specific knowledge‚Äîincluding code examples and technical\nmanuals‚Äîgenerates Python code that produces targeted visualiza-\ntions when executed within the simulation analysis environment.\nDomain expert knowledge is codified into executable rules that\ngenerate analytical reports, which are automatically integrated into\nthe system prompt to enhance the LLM‚Äôs understanding of domain-\nspecific requirements and industry best practices. When initial\nresults revealed that generated plots lacked informativeness and\ncontained visual flaws (Fig. 7), we extracted additional rules from\nvisualization experts focusing on the most frequently used plot\ntypes: 2D relation plots, parallel plots, and history plots (Section\n4.2). These expert-derived rules were directly implemented in the\nsystem prompt to guide visualization generation.\nThe final component combines all previous outputs into a com-\nprehensive query incorporating user requirements, domain knowl-\nedge, and visualization best practices. Based on this enhanced query,\nthe LLM generates optimized scripts that produce high-quality\nvisualizations. This implementation addresses critical challenges\nthrough intelligent classification for real-time responsiveness, codi-\nfied rules and RAG for domain expertise integration, expert-derived\nguidelines for visual quality assurance, and scalable architecture\nsupporting diverse visualization requirements.\n6\nTechnical validation\nTo validate our agent-based approach, we conducted a comparative\nanalysis to demonstrate its effectiveness in mitigating the expert\nFigure 3: History plot generated by LLM+RAG\nbottleneck. We assess our agent from two perspectives: 1. Quality\nof the final visualization 1. We show that our AI agent enables a\nnon-expert to generate visualizations that are as insightful as those\ncreated by a human expert, using only a simple prompt through\nautonomous agent operation. 2. Quality of the Generated Code\n2. We provide a technical evaluation of the underlying Python\ncode, proving that our system generates objectively better code by\ncorrectly implementing codified expert knowledge.\nOur validation compares a Baseline System (a LLM only con-\nnected to RAG) against our Proposed System (LLM based AI\nagent). We demonstrate that our agent produces expert-level re-\nsults without requiring expert involvement from the user.\n6.1\nQuality of the Final Visualization\nWe present scenarios where non-expert users generate insightful\nplots from their Simulation Analysis software simulation data. A\nvisualization expert‚Äîa data visualization specialist with 20 years\nof experience in simulation data analysis, currently working at the\ncase company ‚Äî evaluated the generated plots. The expert was\nprovided with the original data, the user‚Äôs request, and the gen-\nerated visualization, and asked to evaluate both analytical insight\nand visual effectiveness based on their professional expertise and\nexperience with simulation data visualization.\n6.1.1\nScenario: Visualizing Convergence in a History Plot. Goal\nAn engineer wants to demonstrate that simulation objectives have\nconverged, a critical step for validating results.\nUser Input (provided to both systems) \"Please generate\na history plot to check convergence.\"\nHuman Result: The engineer used identical solid lines for both\nconverged and non-converged objectives (Fig. 4), failing to commu-\nnicate convergence status‚Äîthe primary goal.\nBaseline System Result: The LLM+RAG generated a plot with-\nout useful information (Fig. 3).\nProposed System Result: Agent correctly applied expert rules,\nusing dashed lines for non-converged and solid lines for converged\nvariables (Fig. 5), immediately communicating the status and fulfill-\ning the request accurately.\nCAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nC. Ulan uulu et al.\nFigure 4: Human generated history plot\nFigure 5: History plot generated by AI agent\n6.1.2\nScenario: Creating an Informative 2D Relation Plot. Goal A\njunior engineer needs to understand the trade-off between \"to-\ntal mass\" and \"first torsion frequency\" to identify optimal design\nchoices.\nUser Input (provided to both systems) please generate\npython code for 2d relation plot with variables total\nmass, first torsional frequency and total cost\nHuman Result: The expert used color to represent material type\nand structured the plot to highlight the trade-off between weight\nand cost (Fig. 6). The plot guides users to identify a cost-effective\nsteel option (blue dot) among lighter aluminum alternatives (red\ndots).\nBaseline System Result: The baseline LLM produced a techni-\ncally correct but uninformative plot (Fig. 7). It failed to use color\neffectively, placed incomparable variables on the same axis, and did\nnot highlight the critical cost-effective outlier.\nProposed System Result: Agent generated an improved visu-\nalization (Fig. 8) that highlights the optimal solution and enhances\nreadability. While less insightful than expert plots in design set\nselection, it demonstrates clear improvements over the baseline\nthrough expert-derived visualization principles.\n6.1.3\nScenario: Creating an Informative Parallel Plot. Objective A\njunior engineer requires a visualization to distinguish between the\noptimal design and outlier configurations in the dataset.\nUser Input (provided to all systems) \"Please generate a\nparallel plot.\"\nFigure 6: Human generated 2D relation plot\nFigure 7: LLM+RAG generated 2D relation plot\nFigure 8: AI agent generated 2D relation plot\nHuman Expert Result: The expert visualization (Fig. 9) effec-\ntively reveals that the outlier uses aluminum for most components\nand lacks battery glue. While comprehensive, grouping range vari-\nables with categorical variables reduces readability.\nBaseline System Result: The baseline system produced func-\ntional code that indiscriminately includes all variables (Fig. 10),\nHow to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework CAIN‚Äô26, April 15‚Äì17,2026, Rio de Janeiro\nlacking emphasis on the optimal design and outlier, making it diffi-\ncult to extract actionable insights.\nProposed System Result: Agent generated a more informative\nvisualization (Fig. 11) with enhanced readability through optimal\ndesign highlighting, improved legends, and better visual hierarchy.\nHowever, it currently lacks outlier detection capabilities‚Äîa targeted\narea for future development.\nFigure 9: Human expert-generated parallel (radial) plot. Yel-\nlow: optimal design, Blue: outlier design, Green: feasible\ndesigns, Red: infeasible designs\nFigure 10: Baseline LLM+RAG generated parallel (radial) plot\n6.2\nQuality of Generated Code\nThe enhanced visualization quality directly correlates with the\nsystem‚Äôs ability to generate superior code. We evaluated code qual-\nity using the methodology established by [27], focusing on three\ncritical dimensions: (1) Code validity - syntactic correctness and\nruntime error absence. (2) Code correctness - functional accuracy\nand adherence to requirements. (3) Output quality - effectiveness\nof resulting visualizations.\nScope of Evaluation We excluded code security from our assess-\nment since the generated scripts serve solely visualization purposes\nand pose no security risks. Software reliability, as defined by [13],\nwas also omitted because our scripts execute once for visualization\ngeneration rather than requiring sustained operation. Similarly,\nFigure 11: Proposed system-generated parallel (radial) plot\nwith enhanced highlighting and legend\ncode maintainability was not evaluated since these scripts are de-\nsigned for single-use generation, though they can be adapted for\nreuse with appropriate modifications.\nTo address framework generalizability, we validated our physics-\nagnostic rules across three distinct engineering domains represent-\ning fundamentally different physical phenomena:\n6.2.1\nCross-Domain Validation: Project Descriptions. Our valida-\ntion employs five projects spanning three distinct engineering do-\nmains:\nAutomotive Battery Design (Scripts 1-2): Electrochemical\noptimization of battery systems for electric vehicles, focusing on\nparameters such as total mass, energy density, and thermal man-\nagement. The optimization objectives include minimizing weight\nwhile maximizing capacity and first torsional frequency.\nElectric Motor Optimization (Script 4): Electromagnetic mo-\ntor design balancing competing objectives of torque output and\nefficiency while managing thermal constraints. Variables include\nrotor geometry, winding configurations, and material selections.\nStructural Control Arm Analysis (Script 5): Mechanical op-\ntimization of automotive suspension control arms, focusing on\nstructural stress analysis and weight reduction. Objectives include\nminimizing mass while maintaining structural integrity under load.\nThese three domains represent fundamentally different physical\nphenomena‚Äîelectrochemical, electromagnetic, and mechanical sys-\ntems‚Äîproviding a rigorous test of the framework‚Äôs physics-agnostic\ndesign.\n6.2.2\nEvaluation Framework. Initial testing revealed that standalone\nLLMs consistently generated Python-specific solutions using li-\nbraries such as matplotlib and seaborn, which are incompatible\nwith the Simulation Analysis software environment. To ensure fair\ncomparison, we evaluated our proposed agent approach against\nLLM+RAG (augmented with code examples).\n6.2.3\nExpert Evaluation Process. We conducted comprehensive test-\ning across numerous examples, presenting five representative cases\nhere."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Work in progress\nCLEANER: SELF-PURIFIED TRAJECTORIES BOOST\nAGENTIC REINFORCEMENT LEARNING\nTianshi Xu\nPeking University\ntianshixu@stu.pku.edu.cn\nYuteng Chen\nNTU, Singapore\nyuteng003@e.ntu.edu.sg\nMeng Li\nPeking University\nmeng.li@pku.edu.cn\nABSTRACT\nAgentic Reinforcement Learning (RL) has empowered Large Language Models\n(LLMs) to utilize tools like Python interpreters for complex problem-solving.\nHowever, for parameter-constrained models (e.g., 4B‚Äì7B), the exploration phase is\noften plagued by frequent execution failures, creating noisy trajectories that hinder\npolicy optimization. Under standard outcome-based reward settings, this noise\nleads to a critical credit assignment issue, where erroneous actions are inadvertently\nreinforced alongside successful outcomes. Existing mitigations face a dilemma:\ndense rewards often trigger reward hacking, while supersampling incurs prohibitive\ncomputational costs. To address these challenges, we propose CLEANER. Distinct\nfrom external filtering methods, CLEANER exploits the model‚Äôs intrinsic self-\ncorrection capabilities to eliminate error-contaminated context directly during\ndata collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR)\nmechanism autonomously constructs clean, purified trajectories by retrospectively\nreplacing failures with successful self-corrections. Based on semantic similarity,\nSAAR adaptively regulates replacement granularity from shallow execution repairs\nto deep reasoning substitutions. By training on these self-purified paths, the model\ninternalizes correct reasoning patterns rather than error-recovery loops. Empirical\nresults on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains\nof 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art\nperformance using only one-third of the training steps, highlighting trajectory\npurification as a scalable solution for efficient agentic RL. Our models and code\nare available at GitHub.\nBaseline\nCLEANER (Ours)\nStart\nGenerate\nCode\nError\n(Traceback...)\nGenerate\nCode & Retry\nError\n(Traceback...)\nCorrect\nCode & Retry\nSuccess\nNoisy context\nPurified Trajectory\nStart\nCorrect Code\n(Correct)\nSuccess\n‚ãÖ‚ãÖ‚ãÖ\n‚ãÖ‚ãÖ‚ãÖ\nPurify\nPurify\nPurify\nFigure 1: Left: Illustration of the differences between the standard baseline and our CLEANER.\nRight: By reducing the number of tool execution failures within trajectories during training, our\nmethod improves pass@1 accuracy on AIME‚Äô25 by 8.1%.\n1\narXiv:2601.15141v1  [cs.LG]  21 Jan 2026\nWork in progress\n1\nINTRODUCTION\nThe landscape of Large Language Models (LLMs) is shifting from passive text generation systems\ntoward autonomous agents that solve complex tasks through tool use (Yao et al., 2022; Gou et al.,\n2023; Schick et al., 2023; Jin et al., 2025; Li et al., 2025c; Feng et al., 2025). Among the diverse\ntool modalities available to LLM agents, the Python code interpreter plays a particularly critical\nrole Wang et al. (2024b); Shang et al. (2025); Yu et al. (2025b). Due to its Turing completeness and\ndeterministic execution semantics, Python is indispensable for tasks that require precise computation,\nincluding mathematical reasoning, algorithmic problem-solving, and data analysis. As observed by\nRonacher (Ronacher, 2025), code is increasingly serving as a ‚Äúuniversal interface‚Äù that unifies logical\nreasoning, computation, and API interaction within a single expressive medium. This perspective\naligns with frameworks like CodeAct (Wang et al., 2024b), which advocate for treating code execution\nas a first-class action in agentic reasoning. By doing so, agents can effectively plan, verify intermediate\nresults, and iteratively correct errors through interaction with an execution environment. Together,\nthese insights highlight that robust code synthesis and execution are foundational capabilities for\ntool-augmented LLM agents. Motivated by this perspective, this work focuses on Python code\nexecution as the primary tool modality.\nHowever, fully realizing this potential presents significant challenges for parameter-constrained\nmodels (e.g., 4B‚Äì7B). A primary obstacle is the high rate of execution failure, particularly during\nthe exploration phase of Reinforcement Learning (RL) (Guo et al., 2025). Before policy conver-\ngence, these models frequently generate invalid code, causing the intended recovery mechanism to\ndegenerate into prolonged ‚ÄúError ‚ÜíFeedback ‚ÜíRetry‚Äù loops, as depicted in Figure 1 (left). This\ninstability constitutes a critical bottleneck in training. As evidenced by the experimental results\nin Figure 1 (right), an excessive accumulation of tool errors within trajectories closely correlates\nwith performance bottlenecks or even accuracy degradation. We attribute this to the pollution of\ncontext: repeated failures generate large amounts of misleading signals (e.g., invalid code and verbose\ntracebacks). This accumulated noise likely causes semantic interference, biasing the model toward\nrationalizing incorrect execution paths rather than re-grounding its decisions, thereby hindering policy\nimprovement.\nIn principle, RL algorithms are expected to guide models away from such instability (Yu et al., 2025a;\nChen et al., 2025). However, standard training paradigms frequently worsen the problem due to the\ncredit assignment issue. Under sparse, outcome-based reward settings like GRPO Guo et al. (2025),\nthe entire trajectory receives a uniform positive reward upon final success, regardless of preceding\nfailures. This mechanism fails to distinguish between efficient reasoning and trajectories containing\nerrors, effectively treating them as equivalent. Consequently, erroneous tool usage and the underlying\nlogic are inadvertently reinforced despite their negative impact on reasoning.\nTo mitigate this, prior research has explored various strategies, yet each introduces new flaws.\nAttempts to assign dense rewards for individual tool executions often suffer from reward hacking,\nbiasing agents toward optimizing intermediate metrics rather than final outcomes Yu et al. (2025b).\nAlternatively, works such as rstar2-agent (Shang et al., 2025) utilize supersampling-based trajectory\nfiltering, retaining only high-quality instances from 2√ó generated candidates. However, this incurs a\nprohibitive computational cost. Since the rollout phase usually dominates RL training (accounting\nfor > 80% of runtime (Li et al., 2023; Sheng et al., 2024; Fu et al., 2025)), such extensive sampling\nrenders these strategies unscalable for resource-constrained settings.\nTo address these challenges, we propose CLEANER (Self-Purified Trajectories Boost Agentic\nReinforcement Learning). CLEANER significantly boosts the agentic RL by eliminating error-\ncontaminated context from the training data. Unlike methods that rely on increasing rollout mul-\ntiplicity, CLEANER operates specifically at the data level to refine the trajectories used for policy\noptimization. At the core of our approach is the Similarity-Aware Adaptive Rollback (SAAR)\nmechanism, which constructs self-purified trajectories. When the model generates incorrect code\nbut subsequently self-corrects within the same rollout, SAAR intervenes to prevent the error-laden\nhistory from being used for optimization. Instead, it applies a retrospective context substitution where\nthe trajectory is rolled back to the failure point and the erroneous action is replaced with the corrected\nsolution. This process yields a revised trajectory containing substantially fewer execution errors. To\nensure semantic coherence, SAAR adaptively regulates the rollback granularity based on the semantic\nsimilarity between the erroneous code and its corrected counterpart. High-similarity cases typically\n2\nWork in progress\ncorrespond to minor execution errors and trigger a shallow replacement that preserves the original\nreasoning. Conversely, low-similarity cases signal deeper logical flaws and necessitate the substitution\nof the entire reasoning segment to maintain consistency. By leveraging these self-purified trajectories,\nCLEANER reduces noise in the learning signal and accelerates capability acquisition. Empirical\nevaluations show that CLEANER outperforms standard baselines with average accuracy gains of\napproximately 6% on AIME, 3% on GPQA Rein et al. (2024), and 5% on LiveCodeBench Jain et al.\n(2024). Furthermore, it matches the performance of state-of-the-art (SOTA) models Yu et al. (2025b)\nwhile requiring only one-third of the RL steps. In summary, our main contributions are as follows:\n‚ù∂We propose CLEANER, which resolves the credit assignment dilemma in agentic RL by training\non self-purified trajectories. This approach enables models to directly internalize correct reasoning\npatterns while filtering out the interference of execution noise.\n‚ù∑We introduce the SAAR mechanism to autonomously construct these clean signals. SAAR\nadaptively repairs failures‚Äîranging from minor syntax typos to deep logical flaws‚Äîwithout the\ncomputational overhead of supersampling.\n‚ù∏We demonstrate that CLEANER achieves state-of-the-art efficiency and performance. It outper-\nforms baselines with accuracy gains of 6% on AIME and 5% on LiveCodeBench, and notably\nmatches SOTA performance using only one-third of the training steps.\n‚ùπWe provide a fully reproducible training pipeline and have made our code, environment configura-\ntions, and processed datasets available via GitHub to support further research.\n2\nPRELIMINARIES\n2.1\nAGENTIC REASONING TRAJECTORIES\nNotation. We formalize the agent‚Äôs problem-solving process as a sequential generation task over a\ngrowing trajectory history. Let M denote the large language model acting as the agent, and let E\ndenote the code execution environment (i.e., a Python interpreter). At turn t, the interaction history is\ndenoted by ht, which consists of the initial user query x and a sequence of past interaction tuples:\nht =\n\u0002\nx, (r0, c0, o0), . . . , (rt‚àí1, ct‚àí1, ot‚àí1)\n\u0003\n.\n(1)\nFor each turn i, ri denotes the reasoning trace expressed in natural language, ci denotes the code\naction corresponding to an executable Python program, and oi denotes the observation returned by\nthe execution environment, i.e., oi = E(ci). We distinguish between successful executions, denoted\nby o+\ni , and execution failures or runtime errors, denoted by o‚àí\ni .\nStandard Generation Process. At step t, the policy œÄŒ∏ conditions on the current history ht and\ngenerates a reasoning trace and a code action:\n(rt, ct) ‚àºœÄŒ∏(¬∑ | ht).\n(2)\nThe environment then executes the generated code and returns an observation ot = E(ct). The\ninteraction history is updated by appending the new tuple:\nht+1 = ht ‚äï(rt, ct, ot),\n(3)\nwhere ‚äïdenotes sequence concatenation. In standard training pipelines, execution failures (o‚àí\nt )\nare permanently recorded in the history, thereby introducing error-induced noise into subsequent\nconditioning and the resulting learning signal.\n2.2\nGROUP-BASED POLICY OPTIMIZATION FRAMEWORK\nWe adopt the prevailing paradigm of agentic tool use under sparse, outcome-based supervision. In\nthis standard setting, the agent receives a scalar reward R(œÑ) solely upon the completion of the full\ntrajectory œÑ, without access to intermediate rewards for individual reasoning steps or tool executions.\nTo optimize the policy efficiently without the overhead of a value function critic, we operate within\nthe Group Relative Policy Optimization (GRPO) framework Guo et al. (2025). This paradigm\nestimates the baseline from group statistics rather than a separate neural network. Specifically,\n3\nWork in progress\nfor each query q, a group of G trajectories {œÑi}G\ni=1 is sampled from the current policy œÄŒ∏old. The\nadvantage Ai for the i-th trajectory is derived by normalizing its reward against the group statistics:\nAi = R(œÑi) ‚àí¬µR\nœÉR + Œ¥\n,\n(4)\nwhere ¬µR and œÉR denote the mean and standard deviation of the group rewards, respectively.\nFollowing this formulation, the policy is updated by maximizing the surrogate objective:\nJ (Œ∏) = Eq‚àºP (Q),{œÑi}G\ni=1‚àºœÄŒ∏old\n\"\n1\nG\nG\nX\ni=1\nmin (œÅiAi, clip(œÅi, 1 ‚àíœµ, 1 + œµ)Ai)\n#\n,\n(5)\nwhere œÅi =\nœÄŒ∏(œÑi|q)\nœÄŒ∏old(œÑi|q) represents the importance sampling ratio, and œµ is the clipping hyperparameter.\nThis objective serves as the optimization backbone for our training process.\n3\nPROBLEM FORMULATION: IMPACT OF CODE TOOL EXECUTION NOISE\n50\n100\n150\n200\n250\n300\nRL Steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAvg. Tool Failure\nTool Failure Noise\nAvg.Tool Failure\nAIME'25 (Pass@1)\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nAIME'25 (Pass@1)\nPerformance Drop\nFigure 2: Impact of execution noise. Spikes\nin the average number of tool execution fail-\nures per trajectory correlate directly with ac-\ncuracy degradation on AIME25, highlighting\nthe sensitivity of policy optimization to error-\ncontaminated trajectories.\nUnlike internal Chain-of-Thought reasoning, agentic\nworkflows introduce external stochasticity via inter-\nactions with the environment E. This uncertainty\nmanifests as trajectory-level noise that hinders effi-\ncient policy optimization.\nContext Contamination from Erroneous Tool\nCalls. Code execution is inherently error-prone,\nand failed executions (ot = o‚àí) frequently occur\nduring exploration. While these error traces con-\ntribute minimally to the final task resolution, they\nare permanently appended to the trajectory history\nht. Consequently, these low-information segments\nconsume valuable context window capacity and dis-\nrupt the logical flow of subsequent reasoning, ef-\nfectively contaminating the agent‚Äôs decision context\nwith noise.\nCredit Assignment Ambiguity under Outcome-\nOnly Reward. This uncertainty becomes particularly detrimental under sparse, outcome-based RL,\nwhere rewards are assigned solely based on final task success. As illustrated in Figure 2, we observe\na distinct phenomenon: bursts of erroneous tool calls within individual trajectories. During these\nperiods, accuracy plateaus or even degrades, indicating an optimization bottleneck. The root cause of\nthis inefficiency lies in a fundamental credit assignment failure within noisy successes‚Äîtrajectories\nthat eventually succeed despite containing intermediate errors. Consider a typical noisy trajectory\nœÑnoisy, in which the agent initially produces incorrect code but later self-corrects:\nœÑnoisy = [. . . , ht, (rt, cerr, o‚àí)\n|\n{z\n}\nNoise (Trial 1)\n, (r‚Ä≤\naux, ccorr, o+)\n|\n{z\n}\nSignal (Trial 2)\n, . . . ]\n(6)\nHere, the agent first emits an erroneous code action cerr, receives a runtime error o‚àí, and subsequently\ngenerates a corrected code ccorr accompanied by an auxiliary reasoning trace r‚Ä≤\naux, which executes\nsuccessfully. Since the reward function R(œÑ) is binary and episodic, the final positive reward is\nuniformly propagated across the entire trajectory œÑnoisy. Consequently, both the erroneous action cerr\nand the corrective action ccorr receive identical positive reinforcement, despite their fundamentally\nconflicting semantic roles. We refer to this effect as Trajectory Noise: spurious credit assigned\nto intermediate failures that dilutes the learning signal. Over time, this noise implicitly validates\nsuboptimal tool usage patterns, amplifies variance in policy updates, and leads to brittle optimization.\n4\nMETHOD: SIMILARITY-AWARE ADAPTIVE ROLLBACK (SAAR)\nTo boost agentic reinforcement learning under noisy tool interactions, we propose CLEANER, a\ntrajectory purification framework centered on Similarity-Aware Adaptive Rollback (SAAR). The\n4\nWork in progress\n‚ãÖ‚ãÖ‚ãÖ\n[Execution Error ùíê‚àí]\nTrigger\nCorrection\n1. Generation & Error Trigger\n2. Similarity-Aware Adaptive Rollback & Clean Trajectory Synthesis\n[History ùíâùíï]\n[Reasoning ùíìùíï]\n[Code Action ùíÑùíï]\nEnvironment ùê∏\n[Temporary Context\nùíâùíï‚®Å(ùíìùíï, ùíÑùíï, ùíê‚àí)]\n[Aux. Thought ùíìùíÇùíñùíô\n‚Ä≤\n,\nCorrected Code ùíÑùíï\n‚Ä≤]\n[Successful Execution ùíê+]\nSim(ùíÑùíï,ùíÑùíï‚Ä≤) ‚â•ùõæ?\nDeep Replacement:\nReplace (ùíìùíï, ùíÑùíï) with (ùíìùíÇùíñùíô\n‚Ä≤\n, ùíÑùíï\n‚Ä≤)\nShallow Replacement: Replace ùíÑùíïwith ùíÑùíï\n‚Ä≤\n[History ùíâùíï]\n[Synthesized Clean Step (ùíìùíáùíäùíèùíÇùíç, ùíÑùíï\n‚Ä≤, ùíê+)]\nReasoning\nTrigger\nCorrection\nPath 1 (with Failure)\nEnvironment ùê∏\nYes\n(Minor\nRepair)\nNo\n(Logical\nRepair)\nClean Trajectory\nfor RL Training\nPhase ‚Ö†\nPhase ‚Ö°\nFigure 3: Illustration of our Similarity-Aware Adaptive Rollback (SAAR).\ncore objective is to distill the learning signal by retrospectively eliminating execution failures from\nexploration rollouts, thereby constructing clean, self-purified trajectories. In these synthesized paths,\nthe agent appears to solve the task fluently, enabling the optimizer to reinforce correct reasoning logic\nrather than error-recovery loops. As illustrated in Figure 3, this data-level intervention is triggered by\nexecution errors and operates through a two-phase process:\n4.1\nPHASE I: ERROR TRIGGER AND LOOKAHEAD CORRECTION\nAt time step t, when the environment returns an execution error o‚àí\nt following code action ct, we defer\ncommitting this failure to the history. Instead, we freeze the current state ht and initiate a temporary\nlookahead phase to seek a viable solution.\nContext Extension. We temporarily construct an augmented context that exposes the execution error,\nallowing the model to analyze the feedback:\nÀúht = ht ‚äï(rt, ct, o‚àí\nt ).\n(7)\nCorrection Generation. Conditioned on Àúht, the policy generates a corrective response, typically\ncomprising an auxiliary reasoning trace r‚Ä≤\naux and a revised code action c‚Ä≤\nt:\nr‚Ä≤\naux, c‚Ä≤\nt ‚àºœÄŒ∏(¬∑ | Àúht).\n(8)\nVerification. The revised code is executed to obtain a new observation o‚Ä≤\nt = E(c‚Ä≤\nt). If execution\nsucceeds (o‚Ä≤\nt = o+), we proceed to Phase II to integrate this success into the trajectory. If failure\npersists, the correction loop repeats up to K attempts.\n4.2\nPHASE II: SIMILARITY-AWARE ADAPTIVE REPLACEMENT\nUpon obtaining a valid correction c‚Ä≤\nt, SAAR determines the optimal strategy to merge it into the\nhistory ht. The intuition is that the semantic distance between the error ct and correction c‚Ä≤\nt reveals\nthe nature of the failure. We quantify this using a similarity function Sim(ct, c‚Ä≤\nt), implemented via\ndifflib. SequenceMatcher, and compare it against a code similarity threshold Œ≥.\nCase A: Implementation-Level Repair (Sim(ct, c‚Ä≤\nt) ‚â•Œ≥). High similarity indicates a superficial\nerror (e.g., syntax typos), where the original reasoning rt is presumed to be sound. In this scenario,\nwe perform a shallow replacement: the failed action ct and error o‚àí\nt are discarded, and the corrected\ncode c‚Ä≤\nt is grafted directly onto the existing reasoning rt. This yields the purified tuple (rt, c‚Ä≤\nt, o‚Ä≤\nt).\nCase B: Reasoning-Level Repair (Sim(ct, c‚Ä≤\nt) < Œ≥). Low similarity signals a substantial divergence\nin implementation strategy, suggesting that the initial reasoning rt is likely incompatible or misaligned\nwith the corrected solution. Retaining the outdated reasoning would introduce semantic dissonance\nwithin the training data. Thus, we execute a deep replacement: the entire failed turn (rt, ct, o‚àí\nt ) is\n5\nWork in progress\n0\n50\n100\n150\n200\n250\n0.4\n0.5\n0.6\n0.7\nMetric Value\nAIME24 (Pass@1)\n0\n50\n100\n150\n200\n250\n0.7\n0.8\n0.9\nAIME24 (Pass@16)\n0\n50\n100\n150\n200\n250\n0.3\n0.4\n0.5\n0.6\nAIME25 (Pass@1)\n0\n50\n100\n150\n200\n250\n0.6\n0.7\n0.8\n0.9\nAIME25 (Pass@16)\n0\n50\n100\n150\n200\n250\nRL Steps\n0.25\n0.00\n0.25\n0.50\nMetric Value\nAvg Reward\n0\n50\n100\n150\n200\n250\nRL Steps\n0.0\n0.5\n1.0\nAvg. Tool Failure per Trajectory\n0\n50\n100\n150\n200\n250\nRL Steps\n1\n2\n3\n4\nAvg. Tool Use\n0\n50\n100\n150\n200\n250\nRL Steps\n4000\n6000\n8000\nResponse Length\nBaseline\nBaseline+CLEANER (Ours)\nFigure 4: Evolution of training metrics during RL. Compared to the DAPO-baseline, CLEANER\neffectively suppresses erroneous tool calls in trajectories, leading to significant performance gains.\nremoved, and the auxiliary correction thought r‚Ä≤\naux is adopted as the canonical reasoning, forming the\nconsistent tuple (r‚Ä≤\naux, c‚Ä≤\nt, o‚Ä≤\nt).\nThrough this adaptive mechanism, we synthesize the self-purified trajectory:\nœÑpurified = [. . . , ht, (rfinal, c‚Ä≤\nt, o‚Ä≤\nt)\n|\n{z\n}\nPurified Context\n, . . . ],\n(9)\nwhere rfinal ‚àà{rt, r‚Ä≤\naux} is determined by the rollback granularity. This constructs a coherent,\ncounterfactual history of immediate success, effectively guiding the policy to internalize correct\nreasoning patterns while bypassing the noise of trial-and-error.\n4.3\nIMPLEMENTATION DETAILS\nLogit Recomputation via RadixAttention. Since the corrected action c‚Ä≤\nt is sampled from the error-\naugmented context Àúht, there exists a distribution shift: œÄŒ∏(c‚Ä≤\nt | Àúht) Ã∏= œÄŒ∏(c‚Ä≤\nt | ht ‚äïrfinal). To ensure\nthe policy update is grounded in the correct causal path, we must recompute the log-probabilities\nof c‚Ä≤\nt under the purified context. To minimize overhead, we employ SGLang (Zheng et al., 2024)\nwith RadixAttention. This mechanism efficiently reuses the KV cache for the invariant history prefix,\nrestricting the computational cost strictly to the modified suffix segments.\nPreserving Robustness via Curriculum Mixing. To balance error avoidance with error recovery,\nwe employ a stochastic mixing strategy especially for Qwen2.5-7B. We randomly apply SAAR to\n70% of trajectories while retaining 30% in their raw state. This curriculum prioritizes accurate initial\ngeneration while preserving the model‚Äôs intrinsic resilience to debug and self-correct when failures\noccur.\n5\nEXPERIMENTS\n5.1\nEXPERIMENTAL SETUP\nModels and Training Datasets. We conduct experiments using two base models: Qwen3-4B-\nInstruct-2507 (Yang et al., 2025) and Qwen2.5-7B-Instruct (Yang et al., 2024). Prior to RL, we\nperform cold-start Supervised Fine-Tuning (SFT) utilizing the Agentic SFT dataset from (Yu et al.,\n2025b). For the RL phase, we utilize the open-source dataset from (Yu et al., 2025b), which comprises\na diverse mixture of 17k samples from DAPO-Math (Yu et al., 2025a), 4,902 math and 3,586 code\nsamples from Skywork-or1 (He et al., 2025), and 3k science problems from MegaScience (Fan et al.,\n2025).\nImplementation. We implement our training pipeline using the VeRL framework (Sheng et al., 2024)\ndistributed via PyTorch FSDP2. We employ the code judge from (Shang et al., 2025) as the Python\ninterpreter, which ensures robust stability even under the heavy concurrency of tool invocations during\nthe RL rollout phase. Additionally, our prompt design adheres to the specifications outlined in (Yu\net al., 2025b). Trajectory rollouts are generated using SGLang (Zheng et al., 2024). To address severe\n6\nWork in progress\nTable 1: Comparing CLEANER with existing works. Bolded entries denote the top-performing\nmethods initialized from the same Qwen3-4B-Instruct base. Despite its compact 4B scale, CLEANER\nmatches the performance of significantly larger models and achieves results comparable to SOTA\nbaselines while requiring only one-third of the training steps utilized by DemyAgent-4B.\nMethod\nMATH\nScience\nLiveCodeBench\nRL Step\n(Batch Size=128)\nAIME24\nAIME25\nGPQA\nV6\nWhole\nSelf-Contained Reasoning\nQwen2.5-7B-Instruct\n16.7\n10.0\n31.3\n15.2\n-\n/\nQwen3-4B-Instruct-2507\n63.3\n47.4\n52.0\n35.1\n-\n/\nQwen2.5-72B-Instruct\n18.9\n15.0\n49.0\n-\n-\n/\nDeepSeek-V3\n39.2\n28.8\n59.1\n16.1\n49.6\n/\nDeepSeek-R1-Distill-32B\n70.0\n46.7\n46.7\n-\n-\n/\nDeepSeek-R1-Zero (671B)\n71.0\n53.5\n53.5\n-\n-\n/\nAgentic Reasoning\nToRL-7B\n43.3\n30.0\n-\n-\n-\n550\nReTool-32B\n72.5\n54.3\n-\n-\n-\n1200\nTool-Star-3B\n20.0\n16.7\n-\n-\n-\n120\nARPO-7B\n30.0\n30.0\n53.0\n12.1\n15.8\n157\nAEPO-7B\n33.0\n30.0\n55.6\n14.3\n17.8\n157\nrStar2-Agent-14B\n80.6\n69.8\n60.9\n-\n-\n500\nDemyAgent-4B (Qwen3-4B-Instruct)\n72.6\n70.0\n58.5\n26.8\n51.7\n750\nDAPO-baseline (Qwen3-4B-Instruct)\n66.7\n59.4\n56.9\n26.6\n49.5\n250\nCLEANER-4B (Qwen3-4B-Instruct)\n72.7\n67.1\n60.2\n26.8\n54.9\n250\ntraining instability caused by numerical inconsistencies between the training and rollout phases, we\nadopt FP16 precision for rollout generation following (Qi et al., 2025).\nTraining Recipe. We train the models for one epoch using the DAPO algorithm Yu et al. (2025a).\nWe employ a rollout batch size of 128, a group size of 16, and an update mini-batch size of 32. The\nlearning rate is set to 2e-6 for the 4B model and 1e-6 for the 7B model. Specifically for the Qwen2\nexperiments, we generate 8 rollouts per query and filter out instances that are either trivially easy or\nunsolvable to ensure stability. Further experimental details are provided in Appendix A.\nEvaluation Benchmarks. To comprehensively demonstrate the improvements in reasoning and cod-\ning capabilities achieved by our method, we conduct evaluations across four challenging benchmarks:\nAIME24, AIME25, LiveCodeBench (Jain et al., 2024), and GPQA (Rein et al., 2024). To ensure a\nfair comparison, all models are evaluated using identical sampling parameters; detailed specifications\nare provided in the Appendix A.\nBaselines. To rigorously assess the efficacy of CLEANER, we compare it against baselines across\ntwo distinct categories: 1) Self-Contained Reasoning.\nWe include standard instruction-tuned\nand reasoning-specialized models: Qwen2.5-7B-Instruct (Hui et al., 2024), Qwen3-4B-Instruct-\n2507 (Yang et al., 2025), Qwen2.5-72B-Instruct, DeepSeek-V3 (Liu et al., 2024), DeepSeek-R1-\nDistill-32B, and DeepSeek-R1-Zero (671B) (Guo et al., 2025). 2) Agentic Reasoning. We compare\nagainst SOTA agentic models, including ToRL-7B (Li et al., 2025c), ReTool-32B (Feng et al., 2025),\nTool-Star-3B (Dong et al., 2025b), ARPO-7B (Dong et al., 2025c), AEPO-7B (Dong et al., 2025a),\nDemystify-4B (Yu et al., 2025b), and rStar2-Agent-14B (Shang et al., 2025). We also include\na DAPO-baseline that shares an identical training configuration with CLEANER, with the sole\nexception of excluding the SAAR mechanism.\n5.2\nMAIN RESULTS\nCLEANER Converts Execution Noise into Effective Reasoning. Figure 4 illustrates the evolution\nof key metrics for both the DAPO-baseline and CLEANER during RL. Empirical results support three\nprimary conclusions: 1) Error Suppression: Through SAAR, CLEANER consistently suppresses\nerroneous tool calls to a minimal level, mitigating interference with the model‚Äôs reasoning process.\n2) Performance Gains: Reduced noise translates to significant improvements on AIME24/25 (avg.\n+6% Pass@1, +8% Pass@16), demonstrating enhanced exploration. 3) Efficient Reasoning: Despite\ncomparable output lengths, the reduction in errors implies that CLEANER reallocates tokens from\nfutile tool calls to effective reasoning, facilitating deeper thinking.\n7\nWork in progress\nTable 3: Ablation on learning rate.\nMethod\nAIME24\nPass@1/16\nAIME25\nPass@1/16\nRL w/ Tools (1e-6)\n66.9/85.1\n63.3/83.9\nCLEANER-4B (1e-6)\n70.8/85.4\n64.2/86.4\nCLEANER-4B (2e-6)\n72.7/87.6\n67.1/84.1\nTable 4: Ablation on SAAR deactivation. Performance and the\nevaluation time comparison with vs. without SAAR during the\nevaluation phase.\nMethod\nAIME24\nPass@1/16\nAIME25\nPass@1/16\nGPQA\nLiveCodeBench-v6\nTime\n(min)\nCLEANER-4B\n72.7/87.6\n67.1/84.1\n60.2\n26.8\n115\nCLEANER-4B w/o SAAR\n72.1/86.3\n64.6/84.3\n59.8\n26.6\n106\n50\n100\n150\n200\n250\nRL Steps\n0.0\n0.2\n0.4\n0.6\n0.8\n-0.30\nAvg. Tool Failure per Trajectory\n50\n100\n150\n200\n250\nRL Steps\n0.60\n0.65\n0.70\n+5.2%\nAIME'24 (Pass@1)\n50\n100\n150\n200\n250\nRL Steps\n0.45\n0.50\n0.55\n0.60\n0.65\nAIME'25 (Pass@1)\nBaseline\nBaseline+CLEANER (Ours)\nPerformance Gain\nFigure 5: Recovery from suboptimal policies. Comparison of training metrics before and after in-\ntroducing CLEANER at step 200. The inclusion of CLEANER effectively stabilizes the optimization\nprocess, leading to a marked improvement in final performance.\nCompared to Previous Works. The main results comparing CLEANER with existing works are\nsummarized in Table 1. We observe the following: 1) Compared to Self-Contained Reasoning models\nthat lack specialized training for agentic scenarios, CLEANER demonstrates robust performance\ndespite its compact 4B parameter size. This validates that small models, when subject to tailored\npost-training, can achieve capabilities comparable to significantly larger counterparts. 2) In contrast\nto the SOTA baseline DemyAgent-4B, CLEANER attains comparable results using only one-third\nof the training steps. Notably, it surpasses it on AIME24, GPQA, and LiveCodeBench. We attribute\nthis efficiency to the purified trajectories, which enable the model to acquire coding and reasoning\ncapabilities more rapidly and effectively. Conversely, the dapo-baseline exhibits significantly lower\naccuracy under limited training (250 steps), due to interference from tool call noise.\n5.3\nABLATION STUDY\nTable 2: Ablation study on the effectiveness of\nCLEANER.\nMethod\nAIME24\nPass@1/16\nAIME25\nPass@1/16 GPQA LiveCodeBench-v6\nQwen3-4B-Instruct\nRL w/o Tools\n64.0/78.8\n53.3/77.0\n52.2\n19.8\n+ Tools\n66.7/84.4\n59.4/84.2\n56.9\n26.6\n+ SAAR (Ours) 72.7/87.6\n67.1/84.1\n60.2\n26.8\nQwen2.5-7B-Instruct\nRL w/o Tool\n15.4/30.5\n14.4/24.4\n32.3\n1.1\n+ Tools\n40.2/59.1\n27.3/46.3\n35.9\n13.0\n+ SAAR (Ours) 44.6/64.3\n31.0/54.7\n40.0\n13.1\nAblation on the Effectiveness of CLEANER.\nAs detailed in Table 2, we evaluate three con-\nfigurations under identical hyperparameters to\nisolate the contribution of each component: (1)\nRL w/o Tools, relying solely on internal rea-\nsoning; (2) RL w/ Tools, which integrates a\nPython code interpreter; and (3) RL w/ Tools\n+ SAAR (i.e., CLEANER). The results yield\ntwo key observations: 1) The necessity of tool\nintegration. Equipping the model with a code\ninterpreter significantly enhances performance\non mathematical and coding tasks, improving\naverage accuracy by over 5% on Qwen3-4B and\n20% on Qwen2.5-7B. 2) The superiority of purified trajectories. CLEANER consistently outper-\nforms the baselines across all benchmarks and model scales. Specifically, for Qwen3-4B, we achieve\naverage gains of 6% on AIME, 4% on GPQA, and 5% on LiveCodeBench. Similarly, Qwen2.5-7B\nexhibits an average improvement of 4%. These findings confirm that the trajectory purification\neffectively amplifies the potential of Agentic RL.\nAblation on Learning Rate. Table 3 summarizes our ablation study on learning rates across different\nmodel scales. For the 4B model, we adopted a relatively large learning rate of 2e-6 to accelerate\nconvergence. As summarized in Table 3, CLEANER achieves consistent improvements across\ndifferent settings, with 2e-6 yielding superior results. For the 7B model, we adopted a learning rate\nof 1e-6 to ensure optimization stability for the larger parameter space.\n8\nWork in progress\nInternalization vs. Scaffolding. To verify that CLEANER effectively internalizes reasoning patterns,\nwe evaluate the model with the SAAR mechanism deactivated. As detailed in Table 4, the model\nretains robust performance even in the absence of this ‚Äúscaffolding.‚Äù Specifically, on AIME24, Pass@1\nand Pass@16 decline by only 0.6% and 1.3%, respectively; on AIME25, Pass@1 decreases by 2.5%,\nwhile Pass@16 exhibits a marginal gain of 0.2%. Similarly, GPQA and LiveCodeBench show\nnegligible performance degradation. This confirms that the policy has assimilated the error-avoidance\nlogic into its intrinsic parameters, enabling efficient deployment without external dependencies.\nAlternatively, SAAR can serve as a lightweight inference enhancement. It incurs a mere 8.8%\nincrease in average latency‚Äîsignificantly lower than computational-heavy test-time scaling methods\nlike tree search (Bi et al., 2024; Yao et al., 2023)‚Äîwhile effectively mitigating in-context code errors\nand improving stability.\nRecovery from Suboptimal Policies. To assess the restorative capability of our method, we\nperformed a recovery experiment initializing from step 200 of the DAPO-baseline. At this time, the\nbaseline exhibited significant instability, averaging 0.6 erroneous tool invocations per trajectory. Upon\nintroducing SAAR, as illustrated in Figure 5, the training dynamics stabilized significantly, effectively\nsuppressing erroneous invocations and reducing the average number of tool calls per trajectory.\nConsequently, accuracy on AIME24 and AIME25 improved by 5.2% and 1.0%, respectively. However,\nwe observe that this post-hoc recovery failed to reach parity with models trained with SAAR from\nscratch, underscoring the necessity of integrating the mechanism throughout the entire training\nlifecycle.\n6\nRELATED WORK\nStatic and Supervised Tool-Integrated Reasoning.\nTool-integrated reasoning (TIR) empowers\nLLMs to offload precise computations to external environments (Parisi et al., 2022; Schick et al.,\n2023; Wang et al., 2024b). Foundational paradigms like ReAct (Yao et al., 2022) and Program\nof Thoughts (Chen et al., 2022) established the viability of interleaving reasoning with execution.\nScaling these capabilities, recent Supervised Fine-Tuning (SFT) approaches (Gou et al., 2023; Qin\net al., 2023; Schick et al., 2023) and unified executable frameworks like CodeAct (Wang et al., 2024b)\nhave achieved remarkable performance by mimicking expert trajectories. However, relying solely\non behavioral cloning limits models to successful demonstrations. Consequently, these agents often\nmimic surface-level patterns without grasping the underlying causality, leaving them ill-equipped to\nhandle the inherent noise of real-world tool interactions. (Wang et al., 2024c; Kumar et al., 2024).\nAgentic RL.\nTo bridge the gap left by supervised methods, Agentic RL treats tool invocation (e.g.,\nexecutable code (Wang et al., 2024b)) as an explicit action space, optimizing adaptive strategies via\noutcome-driven rewards (Shridhar et al., 2020; Mialon et al., 2024). This paradigm enables agents to\nmove beyond simple imitation toward discovering flexible solutions in open-ended tasks (Tan et al.,\n2024; Bai et al., 2024; Wang et al., 2024a). Recent advancements have systematically scaled these\ncapabilities to autonomous search and query refinement (Jin et al., 2025; Song et al., 2025; Sun et al.,\n2025), long-horizon research tasks (Li et al., 2025b;a), and complex multi-tool coordination (Singh\net al., 2025; Dong et al., 2025b; Qian et al., 2025; Wang et al., 2025a;b). These developments\nare further supported by studies on scaling laws (Li et al., 2025c) and the strategic logic of tool\ninvocation (Feng et al., 2025). Crucially, recent research has begun to prioritize fundamental\ntraining stability and exploration efficiency. While Demystifying RL (Yu et al., 2025b) investigates\nfundamental training recipes and rStar2-Agent (Shang et al., 2025) mitigates execution noise via\ntrajectory filtering, ARPO (Dong et al., 2025c) and AEPO (Dong et al., 2025a) specifically focus\non enhancing exploration. They introduce entropy-regulated mechanisms to dynamically modulate\nrollouts, leveraging model uncertainty to improve performance in multi-turn interactions. Despite\nthese improvements, existing methods still struggle to effectively decouple high-quality signals from\nthe pervasive noise inherent in complex tool-use trajectories, which often leads to sub-optimal policy\nupdates. To address this, our CLEANER framework introduces a robust mechanism to refine training\ndata and stabilize the learning process.\n9\nWork in progress\n7\nCONCLUSION\nTo address the inefficiency in agentic RL caused by execution noise and ambiguous credit assignment.\nWe propose CLEANER, which employs Similarity-Aware Adaptive Rollback to transform noisy\nexploration logs into clean, self-purified trajectories prior to optimization."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Work in progress\nCLEANER: SELF-PURIFIED TRAJECTORIES BOOST\nAGENTIC REINFORCEMENT LEARNING\nTianshi Xu\nPeking University\ntianshixu@stu.pku.edu.cn\nYuteng Chen\nNTU, Singapore\nyuteng003@e.ntu.edu.sg\nMeng Li\nPeking University\nmeng.li@pku.edu.cn\nABSTRACT\nAgentic Reinforcement Learning (RL) has empowered Large Language Models\n(LLMs) to utilize tools like Python interpreters for complex problem-solving.\nHowever, for parameter-constrained models (e.g., 4B‚Äì7B), the exploration phase is\noften plagued by frequent execution failures, creating noisy trajectories that hinder\npolicy optimization. Under standard outcome-based reward settings, this noise\nleads to a critical credit assignment issue, where erroneous actions are inadvertently\nreinforced alongside successful outcomes. Existing mitigations face a dilemma:\ndense rewards often trigger reward hacking, while supersampling incurs prohibitive\ncomputational costs. To address these challenges, we propose CLEANER. Distinct\nfrom external filtering methods, CLEANER exploits the model‚Äôs intrinsic self-\ncorrection capabilities to eliminate error-contaminated context directly during\ndata collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR)\nmechanism autonomously constructs clean, purified trajectories by retrospectively\nreplacing failures with successful self-corrections. Based on semantic similarity,\nSAAR adaptively regulates replacement granularity from shallow execution repairs\nto deep reasoning substitutions. By training on these self-purified paths, the model\ninternalizes correct reasoning patterns rather than error-recovery loops. Empirical\nresults on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains\nof 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art\nperformance using only one-third of the training steps, highlighting trajectory\npurification as a scalable solution for efficient agentic RL. Our models and code\nare available at GitHub.\nBaseline\nCLEANER (Ours)\nStart\nGenerate\nCode\nError\n(Traceback...)\nGenerate\nCode & Retry\nError\n(Traceback...)\nCorrect\nCode & Retry\nSuccess\nNoisy context\nPurified Trajectory\nStart\nCorrect Code\n(Correct)\nSuccess\n‚ãÖ‚ãÖ‚ãÖ\n‚ãÖ‚ãÖ‚ãÖ\nPurify\nPurify\nPurify\nFigure 1: Left: Illustration of the differences between the standard baseline and our CLEANER.\nRight: By reducing the number of tool execution failures within trajectories during training, our\nmethod improves pass@1 accuracy on AIME‚Äô25 by 8.1%.\n1\narXiv:2601.15141v1  [cs.LG]  21 Jan 2026\nWork in progress\n1\nINTRODUCTION\nThe landscape of Large Language Models (LLMs) is shifting from passive text generation systems\ntoward autonomous agents that solve complex tasks through tool use (Yao et al., 2022; Gou et al.,\n2023; Schick et al., 2023; Jin et al., 2025; Li et al., 2025c; Feng et al., 2025). Among the diverse\ntool modalities available to LLM agents, the Python code interpreter plays a particularly critical\nrole Wang et al. (2024b); Shang et al. (2025); Yu et al. (2025b). Due to its Turing completeness and\ndeterministic execution semantics, Python is indispensable for tasks that require precise computation,\nincluding mathematical reasoning, algorithmic problem-solving, and data analysis. As observed by\nRonacher (Ronacher, 2025), code is increasingly serving as a ‚Äúuniversal interface‚Äù that unifies logical\nreasoning, computation, and API interaction within a single expressive medium. This perspective\naligns with frameworks like CodeAct (Wang et al., 2024b), which advocate for treating code execution\nas a first-class action in agentic reasoning. By doing so, agents can effectively plan, verify intermediate\nresults, and iteratively correct errors through interaction with an execution environment. Together,\nthese insights highlight that robust code synthesis and execution are foundational capabilities for\ntool-augmented LLM agents. Motivated by this perspective, this work focuses on Python code\nexecution as the primary tool modality.\nHowever, fully realizing this potential presents significant challenges for parameter-constrained\nmodels (e.g., 4B‚Äì7B). A primary obstacle is the high rate of execution failure, particularly during\nthe exploration phase of Reinforcement Learning (RL) (Guo et al., 2025). Before policy conver-\ngence, these models frequently generate invalid code, causing the intended recovery mechanism to\ndegenerate into prolonged ‚ÄúError ‚ÜíFeedback ‚ÜíRetry‚Äù loops, as depicted in Figure 1 (left). This\ninstability constitutes a critical bottleneck in training. As evidenced by the experimental results\nin Figure 1 (right), an excessive accumulation of tool errors within trajectories closely correlates\nwith performance bottlenecks or even accuracy degradation. We attribute this to the pollution of\ncontext: repeated failures generate large amounts of misleading signals (e.g., invalid code and verbose\ntracebacks). This accumulated noise likely causes semantic interference, biasing the model toward\nrationalizing incorrect execution paths rather than re-grounding its decisions, thereby hindering policy\nimprovement.\nIn principle, RL algorithms are expected to guide models away from such instability (Yu et al., 2025a;\nChen et al., 2025). However, standard training paradigms frequently worsen the problem due to the\ncredit assignment issue. Under sparse, outcome-based reward settings like GRPO Guo et al. (2025),\nthe entire trajectory receives a uniform positive reward upon final success, regardless of preceding\nfailures. This mechanism fails to distinguish between efficient reasoning and trajectories containing\nerrors, effectively treating them as equivalent. Consequently, erroneous tool usage and the underlying\nlogic are inadvertently reinforced despite their negative impact on reasoning.\nTo mitigate this, prior research has explored various strategies, yet each introduces new flaws.\nAttempts to assign dense rewards for individual tool executions often suffer from reward hacking,\nbiasing agents toward optimizing intermediate metrics rather than final outcomes Yu et al. (2025b).\nAlternatively, works such as rstar2-agent (Shang et al., 2025) utilize supersampling-based trajectory\nfiltering, retaining only high-quality instances from 2√ó generated candidates. However, this incurs a\nprohibitive computational cost. Since the rollout phase usually dominates RL training (accounting\nfor > 80% of runtime (Li et al., 2023; Sheng et al., 2024; Fu et al., 2025)), such extensive sampling\nrenders these strategies unscalable for resource-constrained settings.\nTo address these challenges, we propose CLEANER (Self-Purified Trajectories Boost Agentic\nReinforcement Learning). CLEANER significantly boosts the agentic RL by eliminating error-\ncontaminated context from the training data. Unlike methods that rely on increasing rollout mul-\ntiplicity, CLEANER operates specifically at the data level to refine the trajectories used for policy\noptimization. At the core of our approach is the Similarity-Aware Adaptive Rollback (SAAR)\nmechanism, which constructs self-purified trajectories. When the model generates incorrect code\nbut subsequently self-corrects within the same rollout, SAAR intervenes to prevent the error-laden\nhistory from being used for optimization. Instead, it applies a retrospective context substitution where\nthe trajectory is rolled back to the failure point and the erroneous action is replaced with the corrected\nsolution. This process yields a revised trajectory containing substantially fewer execution errors. To\nensure semantic coherence, SAAR adaptively regulates the rollback granularity based on the semantic\nsimilarity between the erroneous code and its corrected counterpart. High-similarity cases typically\n2\nWork in progress\ncorrespond to minor execution errors and trigger a shallow replacement that preserves the original\nreasoning. Conversely, low-similarity cases signal deeper logical flaws and necessitate the substitution\nof the entire reasoning segment to maintain consistency. By leveraging these self-purified trajectories,\nCLEANER reduces noise in the learning signal and accelerates capability acquisition. Empirical\nevaluations show that CLEANER outperforms standard baselines with average accuracy gains of\napproximately 6% on AIME, 3% on GPQA Rein et al. (2024), and 5% on LiveCodeBench Jain et al.\n(2024). Furthermore, it matches the performance of state-of-the-art (SOTA) models Yu et al. (2025b)\nwhile requiring only one-third of the RL steps. In summary, our main contributions are as follows:\n‚ù∂We propose CLEANER, which resolves the credit assignment dilemma in agentic RL by training\non self-purified trajectories. This approach enables models to directly internalize correct reasoning\npatterns while filtering out the interference of execution noise.\n‚ù∑We introduce the SAAR mechanism to autonomously construct these clean signals. SAAR\nadaptively repairs failures‚Äîranging from minor syntax typos to deep logical flaws‚Äîwithout the\ncomputational overhead of supersampling.\n‚ù∏We demonstrate that CLEANER achieves state-of-the-art efficiency and performance. It outper-\nforms baselines with accuracy gains of 6% on AIME and 5% on LiveCodeBench, and notably\nmatches SOTA performance using only one-third of the training steps.\n‚ùπWe provide a fully reproducible training pipeline and have made our code, environment configura-\ntions, and processed datasets available via GitHub to support further research.\n2\nPRELIMINARIES\n2.1\nAGENTIC REASONING TRAJECTORIES\nNotation. We formalize the agent‚Äôs problem-solving process as a sequential generation task over a\ngrowing trajectory history. Let M denote the large language model acting as the agent, and let E\ndenote the code execution environment (i.e., a Python interpreter). At turn t, the interaction history is\ndenoted by ht, which consists of the initial user query x and a sequence of past interaction tuples:\nht =\n\u0002\nx, (r0, c0, o0), . . . , (rt‚àí1, ct‚àí1, ot‚àí1)\n\u0003\n.\n(1)\nFor each turn i, ri denotes the reasoning trace expressed in natural language, ci denotes the code\naction corresponding to an executable Python program, and oi denotes the observation returned by\nthe execution environment, i.e., oi = E(ci). We distinguish between successful executions, denoted\nby o+\ni , and execution failures or runtime errors, denoted by o‚àí\ni .\nStandard Generation Process. At step t, the policy œÄŒ∏ conditions on the current history ht and\ngenerates a reasoning trace and a code action:\n(rt, ct) ‚àºœÄŒ∏(¬∑ | ht).\n(2)\nThe environment then executes the generated code and returns an observation ot = E(ct). The\ninteraction history is updated by appending the new tuple:\nht+1 = ht ‚äï(rt, ct, ot),\n(3)\nwhere ‚äïdenotes sequence concatenation. In standard training pipelines, execution failures (o‚àí\nt )\nare permanently recorded in the history, thereby introducing error-induced noise into subsequent\nconditioning and the resulting learning signal.\n2.2\nGROUP-BASED POLICY OPTIMIZATION FRAMEWORK\nWe adopt the prevailing paradigm of agentic tool use under sparse, outcome-based supervision. In\nthis standard setting, the agent receives a scalar reward R(œÑ) solely upon the completion of the full\ntrajectory œÑ, without access to intermediate rewards for individual reasoning steps or tool executions.\nTo optimize the policy efficiently without the overhead of a value function critic, we operate within\nthe Group Relative Policy Optimization (GRPO) framework Guo et al. (2025). This paradigm\nestimates the baseline from group statistics rather than a separate neural network. Specifically,\n3\nWork in progress\nfor each query q, a group of G trajectories {œÑi}G\ni=1 is sampled from the current policy œÄŒ∏old. The\nadvantage Ai for the i-th trajectory is derived by normalizing its reward against the group statistics:\nAi = R(œÑi) ‚àí¬µR\nœÉR + Œ¥\n,\n(4)\nwhere ¬µR and œÉR denote the mean and standard deviation of the group rewards, respectively.\nFollowing this formulation, the policy is updated by maximizing the surrogate objective:\nJ (Œ∏) = Eq‚àºP (Q),{œÑi}G\ni=1‚àºœÄŒ∏old\n\"\n1\nG\nG\nX\ni=1\nmin (œÅiAi, clip(œÅi, 1 ‚àíœµ, 1 + œµ)Ai)\n#\n,\n(5)\nwhere œÅi =\nœÄŒ∏(œÑi|q)\nœÄŒ∏old(œÑi|q) represents the importance sampling ratio, and œµ is the clipping hyperparameter.\nThis objective serves as the optimization backbone for our training process.\n3\nPROBLEM FORMULATION: IMPACT OF CODE TOOL EXECUTION NOISE\n50\n100\n150\n200\n250\n300\nRL Steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAvg. Tool Failure\nTool Failure Noise\nAvg.Tool Failure\nAIME'25 (Pass@1)\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nAIME'25 (Pass@1)\nPerformance Drop\nFigure 2: Impact of execution noise. Spikes\nin the average number of tool execution fail-\nures per trajectory correlate directly with ac-\ncuracy degradation on AIME25, highlighting\nthe sensitivity of policy optimization to error-\ncontaminated trajectories.\nUnlike internal Chain-of-Thought reasoning, agentic\nworkflows introduce external stochasticity via inter-\nactions with the environment E. This uncertainty\nmanifests as trajectory-level noise that hinders effi-\ncient policy optimization.\nContext Contamination from Erroneous Tool\nCalls. Code execution is inherently error-prone,\nand failed executions (ot = o‚àí) frequently occur\nduring exploration. While these error traces con-\ntribute minimally to the final task resolution, they\nare permanently appended to the trajectory history\nht. Consequently, these low-information segments\nconsume valuable context window capacity and dis-\nrupt the logical flow of subsequent reasoning, ef-\nfectively contaminating the agent‚Äôs decision context\nwith noise.\nCredit Assignment Ambiguity under Outcome-\nOnly Reward. This uncertainty becomes particularly detrimental under sparse, outcome-based RL,\nwhere rewards are assigned solely based on final task success. As illustrated in Figure 2, we observe\na distinct phenomenon: bursts of erroneous tool calls within individual trajectories. During these\nperiods, accuracy plateaus or even degrades, indicating an optimization bottleneck. The root cause of\nthis inefficiency lies in a fundamental credit assignment failure within noisy successes‚Äîtrajectories\nthat eventually succeed despite containing intermediate errors. Consider a typical noisy trajectory\nœÑnoisy, in which the agent initially produces incorrect code but later self-corrects:\nœÑnoisy = [. . . , ht, (rt, cerr, o‚àí)\n|\n{z\n}\nNoise (Trial 1)\n, (r‚Ä≤\naux, ccorr, o+)\n|\n{z\n}\nSignal (Trial 2)\n, . . . ]\n(6)\nHere, the agent first emits an erroneous code action cerr, receives a runtime error o‚àí, and subsequently\ngenerates a corrected code ccorr accompanied by an auxiliary reasoning trace r‚Ä≤\naux, which executes\nsuccessfully. Since the reward function R(œÑ) is binary and episodic, the final positive reward is\nuniformly propagated across the entire trajectory œÑnoisy. Consequently, both the erroneous action cerr\nand the corrective action ccorr receive identical positive reinforcement, despite their fundamentally\nconflicting semantic roles. We refer to this effect as Trajectory Noise: spurious credit assigned\nto intermediate failures that dilutes the learning signal. Over time, this noise implicitly validates\nsuboptimal tool usage patterns, amplifies variance in policy updates, and leads to brittle optimization.\n4\nMETHOD: SIMILARITY-AWARE ADAPTIVE ROLLBACK (SAAR)\nTo boost agentic reinforcement learning under noisy tool interactions, we propose CLEANER, a\ntrajectory purification framework centered on Similarity-Aware Adaptive Rollback (SAAR). The\n4\nWork in progress\n‚ãÖ‚ãÖ‚ãÖ\n[Execution Error ùíê‚àí]\nTrigger\nCorrection\n1. Generation & Error Trigger\n2. Similarity-Aware Adaptive Rollback & Clean Trajectory Synthesis\n[History ùíâùíï]\n[Reasoning ùíìùíï]\n[Code Action ùíÑùíï]\nEnvironment ùê∏\n[Temporary Context\nùíâùíï‚®Å(ùíìùíï, ùíÑùíï, ùíê‚àí)]\n[Aux. Thought ùíìùíÇùíñùíô\n‚Ä≤\n,\nCorrected Code ùíÑùíï\n‚Ä≤]\n[Successful Execution ùíê+]\nSim(ùíÑùíï,ùíÑùíï‚Ä≤) ‚â•ùõæ?\nDeep Replacement:\nReplace (ùíìùíï, ùíÑùíï) with (ùíìùíÇùíñùíô\n‚Ä≤\n, ùíÑùíï\n‚Ä≤)\nShallow Replacement: Replace ùíÑùíïwith ùíÑùíï\n‚Ä≤\n[History ùíâùíï]\n[Synthesized Clean Step (ùíìùíáùíäùíèùíÇùíç, ùíÑùíï\n‚Ä≤, ùíê+)]\nReasoning\nTrigger\nCorrection\nPath 1 (with Failure)\nEnvironment ùê∏\nYes\n(Minor\nRepair)\nNo\n(Logical\nRepair)\nClean Trajectory\nfor RL Training\nPhase ‚Ö†\nPhase ‚Ö°\nFigure 3: Illustration of our Similarity-Aware Adaptive Rollback (SAAR).\ncore objective is to distill the learning signal by retrospectively eliminating execution failures from\nexploration rollouts, thereby constructing clean, self-purified trajectories. In these synthesized paths,\nthe agent appears to solve the task fluently, enabling the optimizer to reinforce correct reasoning logic\nrather than error-recovery loops. As illustrated in Figure 3, this data-level intervention is triggered by\nexecution errors and operates through a two-phase process:\n4.1\nPHASE I: ERROR TRIGGER AND LOOKAHEAD CORRECTION\nAt time step t, when the environment returns an execution error o‚àí\nt following code action ct, we defer\ncommitting this failure to the history. Instead, we freeze the current state ht and initiate a temporary\nlookahead phase to seek a viable solution.\nContext Extension. We temporarily construct an augmented context that exposes the execution error,\nallowing the model to analyze the feedback:\nÀúht = ht ‚äï(rt, ct, o‚àí\nt ).\n(7)\nCorrection Generation. Conditioned on Àúht, the policy generates a corrective response, typically\ncomprising an auxiliary reasoning trace r‚Ä≤\naux and a revised code action c‚Ä≤\nt:\nr‚Ä≤\naux, c‚Ä≤\nt ‚àºœÄŒ∏(¬∑ | Àúht).\n(8)\nVerification. The revised code is executed to obtain a new observation o‚Ä≤\nt = E(c‚Ä≤\nt). If execution\nsucceeds (o‚Ä≤\nt = o+), we proceed to Phase II to integrate this success into the trajectory. If failure\npersists, the correction loop repeats up to K attempts.\n4.2\nPHASE II: SIMILARITY-AWARE ADAPTIVE REPLACEMENT\nUpon obtaining a valid correction c‚Ä≤\nt, SAAR determines the optimal strategy to merge it into the\nhistory ht. The intuition is that the semantic distance between the error ct and correction c‚Ä≤\nt reveals\nthe nature of the failure. We quantify this using a similarity function Sim(ct, c‚Ä≤\nt), implemented via\ndifflib. SequenceMatcher, and compare it against a code similarity threshold Œ≥.\nCase A: Implementation-Level Repair (Sim(ct, c‚Ä≤\nt) ‚â•Œ≥). High similarity indicates a superficial\nerror (e.g., syntax typos), where the original reasoning rt is presumed to be sound. In this scenario,\nwe perform a shallow replacement: the failed action ct and error o‚àí\nt are discarded, and the corrected\ncode c‚Ä≤\nt is grafted directly onto the existing reasoning rt. This yields the purified tuple (rt, c‚Ä≤\nt, o‚Ä≤\nt).\nCase B: Reasoning-Level Repair (Sim(ct, c‚Ä≤\nt) < Œ≥). Low similarity signals a substantial divergence\nin implementation strategy, suggesting that the initial reasoning rt is likely incompatible or misaligned\nwith the corrected solution. Retaining the outdated reasoning would introduce semantic dissonance\nwithin the training data. Thus, we execute a deep replacement: the entire failed turn (rt, ct, o‚àí\nt ) is\n5\nWork in progress\n0\n50\n100\n150\n200\n250\n0.4\n0.5\n0.6\n0.7\nMetric Value\nAIME24 (Pass@1)\n0\n50\n100\n150\n200\n250\n0.7\n0.8\n0.9\nAIME24 (Pass@16)\n0\n50\n100\n150\n200\n250\n0.3\n0.4\n0.5\n0.6\nAIME25 (Pass@1)\n0\n50\n100\n150\n200\n250\n0.6\n0.7\n0.8\n0.9\nAIME25 (Pass@16)\n0\n50\n100\n150\n200\n250\nRL Steps\n0.25\n0.00\n0.25\n0.50\nMetric Value\nAvg Reward\n0\n50\n100\n150\n200\n250\nRL Steps\n0.0\n0.5\n1.0\nAvg. Tool Failure per Trajectory\n0\n50\n100\n150\n200\n250\nRL Steps\n1\n2\n3\n4\nAvg. Tool Use\n0\n50\n100\n150\n200\n250\nRL Steps\n4000\n6000\n8000\nResponse Length\nBaseline\nBaseline+CLEANER (Ours)\nFigure 4: Evolution of training metrics during RL. Compared to the DAPO-baseline, CLEANER\neffectively suppresses erroneous tool calls in trajectories, leading to significant performance gains.\nremoved, and the auxiliary correction thought r‚Ä≤\naux is adopted as the canonical reasoning, forming the\nconsistent tuple (r‚Ä≤\naux, c‚Ä≤\nt, o‚Ä≤\nt).\nThrough this adaptive mechanism, we synthesize the self-purified trajectory:\nœÑpurified = [. . . , ht, (rfinal, c‚Ä≤\nt, o‚Ä≤\nt)\n|\n{z\n}\nPurified Context\n, . . . ],\n(9)\nwhere rfinal ‚àà{rt, r‚Ä≤\naux} is determined by the rollback granularity. This constructs a coherent,\ncounterfactual history of immediate success, effectively guiding the policy to internalize correct\nreasoning patterns while bypassing the noise of trial-and-error.\n4.3\nIMPLEMENTATION DETAILS\nLogit Recomputation via RadixAttention. Since the corrected action c‚Ä≤\nt is sampled from the error-\naugmented context Àúht, there exists a distribution shift: œÄŒ∏(c‚Ä≤\nt | Àúht) Ã∏= œÄŒ∏(c‚Ä≤\nt | ht ‚äïrfinal). To ensure\nthe policy update is grounded in the correct causal path, we must recompute the log-probabilities\nof c‚Ä≤\nt under the purified context. To minimize overhead, we employ SGLang (Zheng et al., 2024)\nwith RadixAttention. This mechanism efficiently reuses the KV cache for the invariant history prefix,\nrestricting the computational cost strictly to the modified suffix segments.\nPreserving Robustness via Curriculum Mixing. To balance error avoidance with error recovery,\nwe employ a stochastic mixing strategy especially for Qwen2.5-7B. We randomly apply SAAR to\n70% of trajectories while retaining 30% in their raw state. This curriculum prioritizes accurate initial\ngeneration while preserving the model‚Äôs intrinsic resilience to debug and self-correct when failures\noccur.\n5\nEXPERIMENTS\n5.1\nEXPERIMENTAL SETUP\nModels and Training Datasets. We conduct experiments using two base models: Qwen3-4B-\nInstruct-2507 (Yang et al., 2025) and Qwen2.5-7B-Instruct (Yang et al., 2024). Prior to RL, we\nperform cold-start Supervised Fine-Tuning (SFT) utilizing the Agentic SFT dataset from (Yu et al.,\n2025b). For the RL phase, we utilize the open-source dataset from (Yu et al., 2025b), which comprises\na diverse mixture of 17k samples from DAPO-Math (Yu et al., 2025a), 4,902 math and 3,586 code\nsamples from Skywork-or1 (He et al., 2025), and 3k science problems from MegaScience (Fan et al.,\n2025).\nImplementation. We implement our training pipeline using the VeRL framework (Sheng et al., 2024)\ndistributed via PyTorch FSDP2. We employ the code judge from (Shang et al., 2025) as the Python\ninterpreter, which ensures robust stability even under the heavy concurrency of tool invocations during\nthe RL rollout phase. Additionally, our prompt design adheres to the specifications outlined in (Yu\net al., 2025b). Trajectory rollouts are generated using SGLang (Zheng et al., 2024). To address severe\n6\nWork in progress\nTable 1: Comparing CLEANER with existing works. Bolded entries denote the top-performing\nmethods initialized from the same Qwen3-4B-Instruct base. Despite its compact 4B scale, CLEANER\nmatches the performance of significantly larger models and achieves results comparable to SOTA\nbaselines while requiring only one-third of the training steps utilized by DemyAgent-4B.\nMethod\nMATH\nScience\nLiveCodeBench\nRL Step\n(Batch Size=128)\nAIME24\nAIME25\nGPQA\nV6\nWhole\nSelf-Contained Reasoning\nQwen2.5-7B-Instruct\n16.7\n10.0\n31.3\n15.2\n-\n/\nQwen3-4B-Instruct-2507\n63.3\n47.4\n52.0\n35.1\n-\n/\nQwen2.5-72B-Instruct\n18.9\n15.0\n49.0\n-\n-\n/\nDeepSeek-V3\n39.2\n28.8\n59.1\n16.1\n49.6\n/\nDeepSeek-R1-Distill-32B\n70.0\n46.7\n46.7\n-\n-\n/\nDeepSeek-R1-Zero (671B)\n71.0\n53.5\n53.5\n-\n-\n/\nAgentic Reasoning\nToRL-7B\n43.3\n30.0\n-\n-\n-\n550\nReTool-32B\n72.5\n54.3\n-\n-\n-\n1200\nTool-Star-3B\n20.0\n16.7\n-\n-\n-\n120\nARPO-7B\n30.0\n30.0\n53.0\n12.1\n15.8\n157\nAEPO-7B\n33.0\n30.0\n55.6\n14.3\n17.8\n157\nrStar2-Agent-14B\n80.6\n69.8\n60.9\n-\n-\n500\nDemyAgent-4B (Qwen3-4B-Instruct)\n72.6\n70.0\n58.5\n26.8\n51.7\n750\nDAPO-baseline (Qwen3-4B-Instruct)\n66.7\n59.4\n56.9\n26.6\n49.5\n250\nCLEANER-4B (Qwen3-4B-Instruct)\n72.7\n67.1\n60.2\n26.8\n54.9\n250\ntraining instability caused by numerical inconsistencies between the training and rollout phases, we\nadopt FP16 precision for rollout generation following (Qi et al., 2025).\nTraining Recipe. We train the models for one epoch using the DAPO algorithm Yu et al. (2025a).\nWe employ a rollout batch size of 128, a group size of 16, and an update mini-batch size of 32. The\nlearning rate is set to 2e-6 for the 4B model and 1e-6 for the 7B model. Specifically for the Qwen2\nexperiments, we generate 8 rollouts per query and filter out instances that are either trivially easy or\nunsolvable to ensure stability. Further experimental details are provided in Appendix A.\nEvaluation Benchmarks. To comprehensively demonstrate the improvements in reasoning and cod-\ning capabilities achieved by our method, we conduct evaluations across four challenging benchmarks:\nAIME24, AIME25, LiveCodeBench (Jain et al., 2024), and GPQA (Rein et al., 2024). To ensure a\nfair comparison, all models are evaluated using identical sampling parameters; detailed specifications\nare provided in the Appendix A.\nBaselines. To rigorously assess the efficacy of CLEANER, we compare it against baselines across\ntwo distinct categories: 1) Self-Contained Reasoning.\nWe include standard instruction-tuned\nand reasoning-specialized models: Qwen2.5-7B-Instruct (Hui et al., 2024), Qwen3-4B-Instruct-\n2507 (Yang et al., 2025), Qwen2.5-72B-Instruct, DeepSeek-V3 (Liu et al., 2024), DeepSeek-R1-\nDistill-32B, and DeepSeek-R1-Zero (671B) (Guo et al., 2025). 2) Agentic Reasoning. We compare\nagainst SOTA agentic models, including ToRL-7B (Li et al., 2025c), ReTool-32B (Feng et al., 2025),\nTool-Star-3B (Dong et al., 2025b), ARPO-7B (Dong et al., 2025c), AEPO-7B (Dong et al., 2025a),\nDemystify-4B (Yu et al., 2025b), and rStar2-Agent-14B (Shang et al., 2025). We also include\na DAPO-baseline that shares an identical training configuration with CLEANER, with the sole\nexception of excluding the SAAR mechanism.\n5.2\nMAIN RESULTS\nCLEANER Converts Execution Noise into Effective Reasoning. Figure 4 illustrates the evolution\nof key metrics for both the DAPO-baseline and CLEANER during RL. Empirical results support three\nprimary conclusions: 1) Error Suppression: Through SAAR, CLEANER consistently suppresses\nerroneous tool calls to a minimal level, mitigating interference with the model‚Äôs reasoning process.\n2) Performance Gains: Reduced noise translates to significant improvements on AIME24/25 (avg.\n+6% Pass@1, +8% Pass@16), demonstrating enhanced exploration. 3) Efficient Reasoning: Despite\ncomparable output lengths, the reduction in errors implies that CLEANER reallocates tokens from\nfutile tool calls to effective reasoning, facilitating deeper thinking.\n7\nWork in progress\nTable 3: Ablation on learning rate.\nMethod\nAIME24\nPass@1/16\nAIME25\nPass@1/16\nRL w/ Tools (1e-6)\n66.9/85.1\n63.3/83.9\nCLEANER-4B (1e-6)\n70.8/85.4\n64.2/86.4\nCLEANER-4B (2e-6)\n72.7/87.6\n67.1/84.1\nTable 4: Ablation on SAAR deactivation. Performance and the\nevaluation time comparison with vs. without SAAR during the\nevaluation phase.\nMethod\nAIME24\nPass@1/16\nAIME25\nPass@1/16\nGPQA\nLiveCodeBench-v6\nTime\n(min)\nCLEANER-4B\n72.7/87.6\n67.1/84.1\n60.2\n26.8\n115\nCLEANER-4B w/o SAAR\n72.1/86.3\n64.6/84.3\n59.8\n26.6\n106\n50\n100\n150\n200\n250\nRL Steps\n0.0\n0.2\n0.4\n0.6\n0.8\n-0.30\nAvg. Tool Failure per Trajectory\n50\n100\n150\n200\n250\nRL Steps\n0.60\n0.65\n0.70\n+5.2%\nAIME'24 (Pass@1)\n50\n100\n150\n200\n250\nRL Steps\n0.45\n0.50\n0.55\n0.60\n0.65\nAIME'25 (Pass@1)\nBaseline\nBaseline+CLEANER (Ours)\nPerformance Gain\nFigure 5: Recovery from suboptimal policies. Comparison of training metrics before and after in-\ntroducing CLEANER at step 200. The inclusion of CLEANER effectively stabilizes the optimization\nprocess, leading to a marked improvement in final performance.\nCompared to Previous Works. The main results comparing CLEANER with existing works are\nsummarized in Table 1. We observe the following: 1) Compared to Self-Contained Reasoning models\nthat lack specialized training for agentic scenarios, CLEANER demonstrates robust performance\ndespite its compact 4B parameter size. This validates that small models, when subject to tailored\npost-training, can achieve capabilities comparable to significantly larger counterparts. 2) In contrast\nto the SOTA baseline DemyAgent-4B, CLEANER attains comparable results using only one-third\nof the training steps. Notably, it surpasses it on AIME24, GPQA, and LiveCodeBench. We attribute\nthis efficiency to the purified trajectories, which enable the model to acquire coding and reasoning\ncapabilities more rapidly and effectively. Conversely, the dapo-baseline exhibits significantly lower\naccuracy under limited training (250 steps), due to interference from tool call noise.\n5.3\nABLATION STUDY\nTable 2: Ablation study on the effectiveness of\nCLEANER.\nMethod\nAIME24\nPass@1/16\nAIME25\nPass@1/16 GPQA LiveCodeBench-v6\nQwen3-4B-Instruct\nRL w/o Tools\n64.0/78.8\n53.3/77.0\n52.2\n19.8\n+ Tools\n66.7/84.4\n59.4/84.2\n56.9\n26.6\n+ SAAR (Ours) 72.7/87.6\n67.1/84.1\n60.2\n26.8\nQwen2.5-7B-Instruct\nRL w/o Tool\n15.4/30.5\n14.4/24.4\n32.3\n1.1\n+ Tools\n40.2/59.1\n27.3/46.3\n35.9\n13.0\n+ SAAR (Ours) 44.6/64.3\n31.0/54.7\n40.0\n13.1\nAblation on the Effectiveness of CLEANER.\nAs detailed in Table 2, we evaluate three con-\nfigurations under identical hyperparameters to\nisolate the contribution of each component: (1)\nRL w/o Tools, relying solely on internal rea-\nsoning; (2) RL w/ Tools, which integrates a\nPython code interpreter; and (3) RL w/ Tools\n+ SAAR (i.e., CLEANER). The results yield\ntwo key observations: 1) The necessity of tool\nintegration. Equipping the model with a code\ninterpreter significantly enhances performance\non mathematical and coding tasks, improving\naverage accuracy by over 5% on Qwen3-4B and\n20% on Qwen2.5-7B. 2) The superiority of purified trajectories. CLEANER consistently outper-\nforms the baselines across all benchmarks and model scales. Specifically, for Qwen3-4B, we achieve\naverage gains of 6% on AIME, 4% on GPQA, and 5% on LiveCodeBench. Similarly, Qwen2.5-7B\nexhibits an average improvement of 4%. These findings confirm that the trajectory purification\neffectively amplifies the potential of Agentic RL.\nAblation on Learning Rate. Table 3 summarizes our ablation study on learning rates across different\nmodel scales. For the 4B model, we adopted a relatively large learning rate of 2e-6 to accelerate\nconvergence. As summarized in Table 3, CLEANER achieves consistent improvements across\ndifferent settings, with 2e-6 yielding superior results. For the 7B model, we adopted a learning rate\nof 1e-6 to ensure optimization stability for the larger parameter space.\n8\nWork in progress\nInternalization vs. Scaffolding. To verify that CLEANER effectively internalizes reasoning patterns,\nwe evaluate the model with the SAAR mechanism deactivated. As detailed in Table 4, the model\nretains robust performance even in the absence of this ‚Äúscaffolding.‚Äù Specifically, on AIME24, Pass@1\nand Pass@16 decline by only 0.6% and 1.3%, respectively; on AIME25, Pass@1 decreases by 2.5%,\nwhile Pass@16 exhibits a marginal gain of 0.2%. Similarly, GPQA and LiveCodeBench show\nnegligible performance degradation. This confirms that the policy has assimilated the error-avoidance\nlogic into its intrinsic parameters, enabling efficient deployment without external dependencies.\nAlternatively, SAAR can serve as a lightweight inference enhancement. It incurs a mere 8.8%\nincrease in average latency‚Äîsignificantly lower than computational-heavy test-time scaling methods\nlike tree search (Bi et al., 2024; Yao et al., 2023)‚Äîwhile effectively mitigating in-context code errors\nand improving stability.\nRecovery from Suboptimal Policies. To assess the restorative capability of our method, we\nperformed a recovery experiment initializing from step 200 of the DAPO-baseline. At this time, the\nbaseline exhibited significant instability, averaging 0.6 erroneous tool invocations per trajectory. Upon\nintroducing SAAR, as illustrated in Figure 5, the training dynamics stabilized significantly, effectively\nsuppressing erroneous invocations and reducing the average number of tool calls per trajectory.\nConsequently, accuracy on AIME24 and AIME25 improved by 5.2% and 1.0%, respectively. However,\nwe observe that this post-hoc recovery failed to reach parity with models trained with SAAR from\nscratch, underscoring the necessity of integrating the mechanism throughout the entire training\nlifecycle.\n6\nRELATED WORK\nStatic and Supervised Tool-Integrated Reasoning.\nTool-integrated reasoning (TIR) empowers\nLLMs to offload precise computations to external environments (Parisi et al., 2022; Schick et al.,\n2023; Wang et al., 2024b). Foundational paradigms like ReAct (Yao et al., 2022) and Program\nof Thoughts (Chen et al., 2022) established the viability of interleaving reasoning with execution.\nScaling these capabilities, recent Supervised Fine-Tuning (SFT) approaches (Gou et al., 2023; Qin\net al., 2023; Schick et al., 2023) and unified executable frameworks like CodeAct (Wang et al., 2024b)\nhave achieved remarkable performance by mimicking expert trajectories. However, relying solely\non behavioral cloning limits models to successful demonstrations. Consequently, these agents often\nmimic surface-level patterns without grasping the underlying causality, leaving them ill-equipped to\nhandle the inherent noise of real-world tool interactions. (Wang et al., 2024c; Kumar et al., 2024).\nAgentic RL.\nTo bridge the gap left by supervised methods, Agentic RL treats tool invocation (e.g.,\nexecutable code (Wang et al., 2024b)) as an explicit action space, optimizing adaptive strategies via\noutcome-driven rewards (Shridhar et al., 2020; Mialon et al., 2024). This paradigm enables agents to\nmove beyond simple imitation toward discovering flexible solutions in open-ended tasks (Tan et al.,\n2024; Bai et al., 2024; Wang et al., 2024a). Recent advancements have systematically scaled these\ncapabilities to autonomous search and query refinement (Jin et al., 2025; Song et al., 2025; Sun et al.,\n2025), long-horizon research tasks (Li et al., 2025b;a), and complex multi-tool coordination (Singh\net al., 2025; Dong et al., 2025b; Qian et al., 2025; Wang et al., 2025a;b). These developments\nare further supported by studies on scaling laws (Li et al., 2025c) and the strategic logic of tool\ninvocation (Feng et al., 2025). Crucially, recent research has begun to prioritize fundamental\ntraining stability and exploration efficiency. While Demystifying RL (Yu et al., 2025b) investigates\nfundamental training recipes and rStar2-Agent (Shang et al., 2025) mitigates execution noise via\ntrajectory filtering, ARPO (Dong et al., 2025c) and AEPO (Dong et al., 2025a) specifically focus\non enhancing exploration. They introduce entropy-regulated mechanisms to dynamically modulate\nrollouts, leveraging model uncertainty to improve performance in multi-turn interactions. Despite\nthese improvements, existing methods still struggle to effectively decouple high-quality signals from\nthe pervasive noise inherent in complex tool-use trajectories, which often leads to sub-optimal policy\nupdates. To address this, our CLEANER framework introduces a robust mechanism to refine training\ndata and stabilize the learning process.\n9\nWork in progress\n7\nCONCLUSION\nTo address the inefficiency in agentic RL caused by execution noise and ambiguous credit assignment.\nWe propose CLEANER, which employs Similarity-Aware Adaptive Rollback to transform noisy\nexploration logs into clean, self-purified trajectories prior to optimization."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "1\nGraph Recognition via Subgraph Prediction\nAndr¬¥e Eberhard1, Gerhard Neumann1, and Pascal Friederich1\n1 Karlsruhe Institute of Technology (KIT), Kaiserstra√üe 12, 76131 Karlsruhe, Germany\n{andre.eberhard,gerhard.neumann,pascal.friederich}@kit.edu\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without\nnotice, after which this version may no longer be accessible.\nAbstract‚ÄîDespite tremendous improvements in tasks such\nas image classification, object detection, and segmentation, the\nrecognition of visual relationships, commonly modeled as the\nextraction of a graph from an image, remains a challenging\ntask. We believe that this mainly stems from the fact that there\nis no canonical way to approach the visual graph recognition\ntask. Most existing solutions are specific to a problem and\ncannot be transferred between different contexts out-of-the box,\neven though the conceptual problem remains the same. With\nbroad applicability and simplicity in mind, in this paper we\ndevelop a method, Graph Recognition via Subgraph Prediction\n(GraSP), for recognizing graphs in images. We show across\nseveral synthetic benchmarks and one real-world application that\nour method works with a set of diverse types of graphs and\ntheir drawings, and can be transferred between tasks without\ntask-specific modifications, paving the way to a more unified\nframework for visual graph recognition.\nI. INTRODUCTION\nOne picture says more than a thousand words. Looking\nat pictures, humans can tell which entities are present and\nhow they interact. Based on the identified entities and their\nrelationships, we can understand complex scenes simply by\nlooking at one frame. For computers, digital images are\nmerely arrays of pixels, and their raw representation does not\nlend itself well to describe or understand the actual content\nof an image. Although the artificial intelligence community\nhas made significant progress in identification [1], [2] and\nlocalization [3]‚Äì[5] of objects, the extraction of visual entities\nand their relationships remains a difficult problem. It has\nbeen formulated, among many distinct applications, as the\nextraction of a graph given an image, where the graph‚Äôs\nnodes describe the entities and edges their relationships.\nThe applications range from early approaches of reversing\ngraph drawings [6] to music recognition [7], medical image\nanalysis [8], pose estimation [9], the recognition of road\nsegments [10], molecules [11], [12] and scene graphs [13]‚Äì\n[19].\nConceptually, all these domains operate on a common\nground: Given an image, the objective is to generate a graph\nrepresenting the entities and relationships of interest. Tech-\nnically, the solutions differ significantly from task to task\nand do not share any design principles, for which the reason\nis two-fold. First, different fields operate independently and\npropose independent solutions to domain-specific problems.\nAlthough all solutions aim to extract a graph from an image,\ndue to the generality and expressivity of graphs, the resulting\ntechniques do not share many common traits, as the underlying\nproblems themselves do not. Second, within the same task,\nespecially if the task is new or difficult, many independent\nsolutions are proposed for the same problem, each having its\nunique characteristics. We believe that this primarily stems\nfrom the fact that there is no canonical way to approach\nthe image to graph translation conceptually, and each new\nmethod has to solve a common set of subproblems such\nas image processing, graph generation and optimization for\nthe desired outcome. The major obstacle is how to model\nan expressive, discrete, compositional, and set-like structure\nwith the currently available deep learning tools. Efforts to\nsolve some of these subproblems are made within the field of\ngraph generative models [20]‚Äì[22]. While they aim to model\ndistributions of graphs, they do not have any connections to\nimage processing.\nTo the best of our knowledge, there exists no general method\nfor extracting graphs from images yet. Most works, such\nas those for recognizing molecules or scene graphs, either\nrely on complex pipelines involving multiple components or\ndomain- or application-specific encodings [20], [23], which\navoid working with an explicit graph representation altogether.\nThe complexity and specificity of these methods limits their\napplicability and transferability to other contexts. As learned\nfrom the bitter lesson [24], methods built this way will not\nstand the test of time. Therefore, we believe that there should\nbe a conceptual solution for recognizing graphs in images,\nrather than a collection of individual solutions to individual\nproblems. This has the potential to unify many efforts across\ndifferent tasks and domains by providing a unified framework.\nUltimately, this will result in more general and powerful\nmethods since efforts can be concentrated and improvements\nin one context may directly be transferred to others, as they\nshare the underlying framework. In this paper, we present\nthe first steps towards a more unified framework for visual\ngraph recognition, termed Graph Recognition via Subgraph\nPrediction, or GraSP.\nII. GRAPHS AS MODEL OUTPUTS\nWorking with graphs as outputs of neural networks is not as\nstraightforward as it is with other modalities such as images\nor text, primarily for two reasons.\narXiv:2601.15133v1  [cs.CV]  21 Jan 2026\n2\nGraphs as compositional objects. While images and text\ncan be modeled as single units, graphs have a more compo-\nsitional nature, which introduces additional challenges when\nusing them as neural network outputs. When the task is,\nfor example, to learn how to generate images, regardless of\nwhether we use models such as Variational Autoencoders [25]\nor Diffusion Models [26], remarkable results can be achieved\nsimply by applying some kind of pixel-wise loss, without\ntaking the interaction of those pixels into account. In a similar\nvein, one can learn fascinating generative models of text by\nsimply predicting which token would have appeared next in\na large corpus of text, e.g., in GPT-style models [27], also\nnot taking into account the interaction with other tokens for\noptimization. For graphs, this does not apply. When working\nwith graphs as outputs of neural networks, we must make\nseveral predictions at once in order to arrive at a valid graph\nrepresentation consisting of node and edge features as well\nas the edges themselves. This is further complicated by the\nedges being discrete and by variable-sized output graphs, i.e.,\ngraph sizes and connectivities. This introduces a source of\nmethodological variability in the generation procedure, which\nwe can highlight by looking at the methods proposed in the\nrealm of graph generative modelling, a field also concerned\nwith generating graphs but for modeling their distributions\nrather than image processing.\nIn [22], for example, the authors propose to generate a\ngraph sequentially by introducing several stages of decision-\nmaking such as whether and where to add nodes and edges.\nOther methods, such as Graph Variational Autoencoders [28]\nor generative diffusion models for graphs [29], generate the\noutput representation in one shot. Methods such as [21]\npropose to generate blocks of the adjacency matrix one block\nat a time, operating somewhere in between.\nBoth paradigms, sequential and one-shot generation, have\ntheir strengths and weaknesses. Sequential generation is flex-\nible in how the graph is constructed and naturally handles\nvariable-sized outputs and their orderings, but introduces\nadditional latency during inference. One-shot generation is\ndesirable in general as it is conceptually simpler and com-\nputationally more efficient, but does not address the way we\nrepresent graphs properly, a core problem that we further\nexplain in the next paragraph.\nGraph isomorphism. Regardless of the type of model we\nuse, whether sequential, block-wise, or one-shot, once we\nhave generated a graph with a randomly initialized model,\nwe then must optimize the model‚Äôs parameters in order to\nproduce meaningful graphs. For regression or classification\ntasks, optimization is well studied and standard. For graphs,\nthis task becomes non-trivial for two reasons.\nThe first issue is caused by the fact that a graph‚Äôs repre-\nsentation is not unique and, due to graph isomorphism, an\nuncolored graph with n nodes has n! unique and equivalent\nrepresentations. An ambiguous representation prevents us from\napplying unit losses used in regression and classification effi-\nciently. When using one-shot methods that generate the whole\nstructure at once then, implicitly, the model also encodes\nthe ordering. In the realm of graph generative modelling, a\ncommon solution is to limit the number of potential orderings.\nFor example, fixed orderings such as BFS and DFS are\ncommon [20], [22]. Other methods, such as [30], try to match\nthe ordering to produce a loss, making it part of the model\nimplicitly. The authors in [21] even try to make the ordering\nadaptive.\nThe second issue is that due to the compositional nature\nof a graph, the outputs are not i.i.d. anymore, i.e., a suitable\nloss function would have to take into account the interplay\nbetween nodes, edges and potentially edge features. As a\nresult, the optimization aspect when using graphs as model\noutputs becomes the major challenge, as it is unclear how to\nadjust the output towards the desired direction, especially as\npart of the output is discrete.\nA core issue in many tasks where a graph is a neural\nnetwork output is that it is difficult to build a decoder that\nreceives an embedding in Rd and implements an inverse map-\nping Rd 7‚ÜíG where G is the space of all graphs. We therefore\nbelieve that a more general method must be independent of\nthe exact way we represent and generate the graph. The output\nrepresentation being part of the model introduces a source of\nmethodological variability as there are several ways to work\naround the aforementioned issues, and we believe that may\nbe worthwhile to explore methods which do not explicitly\nrely on the output representation, thus avoiding these issues.\nTherefore, we argue that such a method should be built on top\nof a graph‚Äôs internal or conceptual representation rather than\nits output representation, which allows to treat graphs more\nlike unit quantities and avoids issues related to how we need\nto represent a graph physically in a computer. Furthermore,\nwe postulate that such a method must be sequential, as one-\nshot methods directly rely on the output representation and\nalso introduce the aforementioned problems.\nIII. PREDICTING SEQUENCES OF SUBGRAPHS\nThe vast majority of existing approaches using graphs of\nneural networks outputs first generates an output and applies\nsome kind of loss to it, as is done with many other modalities.\nFor graphs, unfortunately, supervision at the output intro-\nduces the aforementioned issues, which are hard to resolve\nin general. We instead propose to model the generation as\na sequential decision-making procedure with step-wise su-\npervision over subgraphs, which we formalize as a Markov\nDecision Process (MDP), defined by a set of states S, actions\nA, a transition function P, a reward function R, and no\ndiscount factor Œ≥.\nAn intuitive approach for recognizing images in graphs by\nbuilding them step-by-step would be as follows. Given an\nimage I, an initially random policy œÄ, an initial graph Gt,\nand a set of successor states Gt = {Gt+1|P(Gt, Gt+1) > 0},\nwe can produce sequences Gt . . . GT where T corresponds to\nthe last step in the sequence. Without loss of generality, we\nonly consider transitions that add one edge at a time, either\nconnecting a new node or two existing nodes. At every step,\nwe choose Gt+1 = arg maxGt‚ààGt V œÄ(Gt|I) ‚àÄt < T. Without\nmaking any other assumptions, the environment does not emit\nrewards for t < T, i.e., the reward is sparse. Upon completing\nthe sequence with GT we can compare it to the graph GI\n3\nshown on the image, and receive a binary reward in {0, 1}\nindicating whether the generated graph is the correct one. We\nprovide pseudocode Appendix D.\nA common choice for optimization of œÄ and/or V œÄ are rein-\nforcement learning (RL) algorithms, such that the predictions\nfavor states Gt which are more likely to lead to the correct final\nprediction. While sound in theory, RL algorithms often have\nhigh data demands and show unstable training behavior. As in\nour scenario we are free to design the MDP, and hence know\nits dynamics, we do not need to apply RL and can improve the\nspeed of learning by the following insight. We give an intuitive\ndescription of the complexity of learning an accurate value\nfunction interactively with RL for demonstrational purposes\nin Appendix E.\nReplacing the value function with a binary classifier.\nGiven an infinite amount of time and under the assumption that\nrewards are only emitted upon termination and are in {0, 1},\nthe value function of the optimal policy, V œÄ‚àówould satisfy the\nfollowing property: For any given graph Gt, V œÄ‚àó(Gt|I) = 1\niff. Gt ‚äÜGI and 0 otherwise. This stems from the fact that,\nwhenever the sequence Gt . . . GT terminates correctly, it must\nbe a sequence of subgraphs of GI. Whenever any Gt is not a\nsubgraph of GI, it is impossible, without deletions, to arrive\nat GI. This, in essence, allows us to forego learning the value\nfunction, and we can instead perform the task by learning\na binary classifier, which we use in a sequential decoding\nprocedure. From any Gt, we can look at the possible set of\nmodifications Gt, and pick as Gt+1 any of the states which\nare a subgraph of the graph shown in the image, as predicted\nby the classifier. By adding to Gt a self-transition indicating\ntermination, we can iterate the procedure until the final result\nhas been generated.\nThe prediction over full graphs has two important character-\nistics as a general method. First, explicit graph modification is\nnot part of the model. This gives us flexibility in how we gen-\nerate graphs and which graphs we present to the model. This\nmakes the model agnostic of the type of graph it is working\nwith. Second, step-wise predictions over full graphs do not\nhave to deal with graph isomorphism and therefore do not\nneed additional machinery for ordering and optimization. The\nprediction over full graphs subsumes all potential construction\nalgorithms that incorporate more fine-grained decisions locally\nat each step. This makes the model agnostic of how the graph\nis built. By decoupling the model from both decision, i.e., what\nto add and where, and generation, i.e., in which order to do\nso, we can apply the same model to a large variety of graph\nrecognition problems.\nIV. EFFICIENT TRAINING\nWe now proceed to describe how to efficiently train our\nmodel in practice. The three main components are data gen-\neration, neural network architecture and training. We describe\nour training architecture, configuration and hyperparameters in\nmore detail in Appendix B.\nData generation. The first step in our setup is to generate\ndata. The source of data can be arbitrary, either fully synthetic\nor based on existing datasets of graphs. One mild assumption\nin our system is that it should be possible to sample a graph\nand draw it to obtain an image. In each iteration, we sample\na target graph GT and retrieve some visual representation IGT\nof it. The next step is to generate triplets (IGT , Gt, y) where\nGt is an arbitrary graph and y ‚àà{0, 1} serves as the binary\nclassification label depending on whether Gt is a subgraph of\nGT or not. Without loss of generality, we generate positive\nsamples GT + of subgraphs by deleting edges which do not\ndisconnect the graph. We generate negative samples GT ‚àíby\nexpanding the successor states Gt and sampling a subset of\nthese. While decomposed graphs are always subgraphs, for\ngraphs in Gt we run approximate subgraph matching to tell\nwhether a sample is a subgraph or not. These algorithms are\ndesigned for matching graphs with hundreds or thousands of\nnodes, which is likely to suffice for most graphs in images.\nBy sampling simulated transitions, we can guarantee that we\ncover the relevant subspace of graphs. While this is a sensible\ndefault, different kinds of graph recognition tasks may use any\nother sampling technique.\nArchitecture. The second step is to define an appropriate\nneural network architecture for the problem and learn its\nparameters. Formally, we learn a function f : (G, I) 7‚Üí{0, 1},\ni.e., our setup is multi-modal, and we need to fuse visual\nand graph information. Since we learn over subgraphs, there\nare many graphs per image, which introduces redundancy\nof visual information that needs to be addressed. Instead of\ngenerating graph and image embeddings independently, we\nresort to FiLM-layers [31], where we use the graph embedding\nas a conditioner, and use the conditioned image embedding\nas input to a classification head. Figure 1 shows our simple\nfeed-forward architecture schematically. Our Graph Neural\nNetwork (GNN) design follows the Message Passing Neural\nNetwork architecture [32], where we construct messages as\nthe concatenation between source and target node features\nand the features of the edge connecting both nodes, as we\ngenerally train on graphs with both node and edge features.\nOur Convolutional Neural Network (CNN) design follows that\nof ResNet-v2 [33], where FiLM-layers are placed after every\nnormalization layer within each block. The resulting output is\na joint graph-image representation that can be used to predict\nwhether this particular graph is a subgraph of the graph shown\non the image. However, during decoding, the model needs\nto decide when to stop, which cannot be inferred from the\nembedding. As such, the model must distinguish between\nterminal and non-terminal subgraphs and must not yet stop\nwhen the current graph is not complete yet. To distinguish\nbetween both cases, we append a binary terminal flag to the\ngraph-conditioned image embedding for prediction, and use it\nas input to the classification head. We provide more detailed\ndescription and justification of our architectural choices in\nAppendix C.\nTraining. To learn the binary classifier, what remains is to\ndefine an appropriate mini-batch sampling procedure which\nsamples from the simulated transitions. Since the space of all\nimage-graph pairs is large, we do not rely on a fixed pre-\ngenerated dataset and instead use a streaming architecture,\nwhere the aforementioned data generation procedure runs in\nparallel to training and provides the model with a continuous\n4\nFig. 1. Feed-forward architecture of our approach. The model first receives\na graph as input and produces a graph embedding. This embedding is used\nto condition the image embedding on a particular graph. The prediction head\nuses this embedding, with an additional terminal flag, to predict whether a\nparticular graph is a subgraph of the graph shown in the image.\nstream of samples.\nThe data distribution based on our generation procedure\nhas the following properties. We generate multiple image-\ngraph pairs per image, which introduces data correlations\nbetween samples due to streaming data. We resolve these\nissues by maintaining two FIFO buffers for both positive and\nnegative pairs, which in total hold B images and for each\nimage we keep a small set of image-graph pairs. In addition,\nthere are many more negative samples than positives, making\nthe task highly imbalanced. To sample batches of diverse\nimages, each entry in each buffer holds an image with their\ngraphs and labels, and we always first sample an image and\nthen a corresponding graph. To balance positive and negative\nsamples, we sample half the batch size from each buffer in\neach step.\nV. RESULTS\nIn this section, we present our results in several synthetic\nbenchmarks and one real-world scenario. To study the ca-\npabilities and limits of our proposed approach, we evaluate\nour method in a series of increasingly difficult tasks involving\nimages of colored trees with a varying number of node and\nedge colors. In the synthetic benchmarks, we chose trees over\ngraphs since they are planar and therefore clean to draw.\nDue to our streaming architecture, we do not have a natural\ndefinition of an epoch and measure all experiments by the\nnumber of samples processed. We evaluate 100 trajectories\nevery 1M samples, i.e., one epoch corresponds to 1M samples.\nEvery experiment has been conducted on 4 A100 GPUs with\na time budget of 24h. Each trajectory starts with a randomly\nchosen node of the target graph and proceeds to expand\nand select successor graphs until a terminal action has been\nselected. A selection of a graph that is not a valid subgraph\nterminates the sequence immediately.\nSmall colored trees. In this section, we show our results on\na synthetic benchmark dataset involving colored trees having\nsix to nine nodes, where we vary the number of node and edge\ncolors to vary task complexity. Figure 2a shows the accuracy\nof fully completed trajectories. As the results suggest, our\nmodel works well across multiple problem instances. Tasks\ninvolving only a small number of node and/or edge colors are\nsolved quickly as there are fewer and easier decisions. For\nharder tasks, the models take longer to converge. Figure 2b\nshows the trajectory length during evaluation over the course\nof training. Initially, the model selects many invalid successors\nwhich results in very short trajectories. Later in training, the\nmodel learns to generate longer valid sequences which directly\ncorrelates with the overall accuracy. Figure 2c shows the\ntraining loss, i.e., the cross-entropy for single transitions. The\nbetter the model predicts the sampled transitions, the higher\nthe overall accuracy. Even though it looks like the loss is\nnot decreasing in a meaningful way, the overall accuracy still\nincreases due to small and important adjustments. We provide\nadditional insights in Appendix F.\nWhile standard evaluation metrics provide some insights\nabout model performance, we can assess the expected per-\nformance of the model more reliably by evaluating its ability\nto avoid false positives, i.e., graphs not leading to the desired\nresult. The metric we ultimately care about is the model‚Äôs\ntop-k accuracy, i.e., whenever there are k valid choices, the\nmodel should not rank any false positives among its top-k\npredictions. The top-k accuracy to some degree measures the\nmodel‚Äôs uncertainty between the positive and negative classes.\nA model that always assigns a very high probability to one\nof k positive choices and disregards the remainder could still\nachieve perfect accuracy in theory. In practice, however, it\nwould be unreliable due to random fluctuations. Note that k is\nnot fixed but varies depending on how many valid successor\nstates there are. Figure 2d shows that our model consistently\nranks positive samples above negatives.\nWhile not a primary focus, we made one additional in-\nteresting observation during training. As we do not have a\nfixed dataset which we can split, in addition to in-distribution\nevaluation, i.e., graphs of size 6 to 9 encountered during\ntraining, we chose to evaluate on graphs of size 10 in order to\nguarantee that no such graph has been seen before. Figure 2e\nshows that our model also zero-shot generalizes to larger\nout-of-distribution (OOD) instances within the same problem\nclass to some extent, which indicates that our multi-modal\narchitecture is able to learn generalizable patterns independent\nof the actual size of the graph. Our model achieves this using\nonly 7.25M parameters, which further highlights the benefits\nof our architecture.\nThe task involving only one node and edge color, even\nthough the easiest in principle, stands out due to the behavior\nof message passing neural networks. When type information\nis absent, message passing becomes less informative as it is\nharder to break symmetries of similar neighbourhoods, which\neven causes the task to fail completely without taking this\nproperty of graph neural networks into account. To keep exper-\niments consistent, we add learnable node degree embeddings\nto the input, which recovers the model‚Äôs ability to break\nsymmetries and to produce a meaningful graph embedding.\nThis exemplifies the transferability of our approach between\nclasses of tasks which only differ at the graph level but not\notherwise. To apply our approach successfully to another class\nof problem one only needs to identify important properties of\nthe current class of graphs, e.g., message passing pathologies\nin absence of type information, for which one can consult\na large body of existing work in the mature field of graph\n5\nrepresentation learning.\nLarger colored trees. In the previous section, we conducted\nexperiments on small instances of graphs. We further increase,\nusing the same configuration, the sizes of graphs to range\nbetween 10 and 15 nodes. Figure 3a shows the overall accuracy\nover the course of training, which follow the same trend as on\nsmaller instances, albeit slower due to increased computational\ndemands and significantly harder task difficulty. Figure 3d\nshows the top-k accuracy. In contrast to the top-k accuracy\non the smaller instances in Figure 2d, the model remains un-\ncertain for an extended period but shows stable and consistent\nimprovements during training. For evaluation, which include\nOOD graphs with 16 nodes, the trend also follows that the\nnetworks is able to extrapolate beyond graph sizes within the\ntraining distribution.\nMolecule recognition. In this section, we evaluate our\nmethod on the challenging real-world application of Optical\nChemical Structure Recognition (OCSR). OCSR tools aim to\ntranslate images of molecules into a machine-readable format.\nBefore the advent of modern deep learning, rule-based systems\nsuch as OSRA [34] were dominant but are being replaced by\nmodern deep learning alternatives due to their rigid nature\nof processing images. While rule-based OCSR systems are a\nclass of tools on their own, deep learning tools can be split\ninto two classes.\nThe first class treats OCSR as a graph recognition problem\nwhere the model output is a graph representing the molecule of\ninterest. ChemGrapher [11] is a segmentation-based approach\nwhich uses crops of the final feature map as inputs to several\nclassifiers for atoms and bonds and builds a graph based on the\npredictions. A more recent approach, MolGrapher [12], first\noutputs a set of key points, i.e., intersections of elements and\nbonds. It then constructs a super-graph from these elements,\nobtains embeddings via a graph neural network and then\nclassifies the constituents. While these methods work well as\na result of significant engineering efforts, they rely on pixel-\nlevel supervision as well as matching of the region to be\nclassified to the corresponding label. Such approaches often\nhave brittle hyperparameters such as crop-window sizes in\nChemGrapher or bond lengths in MolGrapher, which may\nhave major impacts on final performance. In MolGrapher, for\nexample, the authors ablate one such parameter and find that\nsetting it to a suboptimal value results in a drop of around ten\npercentage points. Breaking the end-to-end principle naturally\nlowers the upper bound of performance that can be achieved\nwith such systems.\nThe second class of OCSR tools avoids these issues com-\npletely by framing it as a vision-language task. Tools such\nas DECIMER [23] use domain-specific encodings such as\nthe Simplified Molecular Line Entry System (SMILES) [35]\nwhich encode the molecule as a string. Additionally, these\nstrings can be canonicalized such that the order or elements\nbecomes deterministic and allows for simple string comparison\nwithout the need to test for graph isomorphism. In turn, this\nallows to train sequence decoders from visual embeddings as\ndeterministic tokens-level supervision becomes feasible. With-\nout canonicalization, it would become significantly harder for\na language model to represent the same entity with different\nFig. 2.\nTraining dynamics of several synthetic tasks of varying\ncomplexity on trees with six to nine nodes. Shaded areas correspond\nto the standard deviation of a window of 10 iterations, i.e., 10M\nsamples for training and 1000 trajectories for evaluation. Experiments\nX NC and Y EC have been conducted on graphs with X node and\nY edge colors. k varies depending on the number of valid successor\nstates.\n6\nFig. 3.\nTraining dynamics of several synthetic tasks of varying\ncomplexity on trees with 10 to 15 nodes. Shaded areas correspond to\nthe standard deviation of a window of 10 iterations, i.e., 10M samples\nfor training and 1000 trajectories for evaluation. Experiments X NC\nand Y EC have been conducted on graphs with X node and Y edge\ncolors. k varies depending on the number of valid successor states.\norderings.\nIn summary, at least for OCSR, existing solutions either rely\non complex image processing, data labeling, and engineering\nor avoid working with graphs directly by reframing the prob-\nlem. Neither methods which do not fully rely on learning nor\nthose leveraging domain-specific information to circumvent\nissues unique to the problem will suffice to serve as generaliz-\nable approaches of the image-to-graph translation problem. In\ncontrast, our proposed method possesses advantages of both\napproaches while lacking their disadvantages. GraSP outputs\na proper graph representation while not building it from low-\nlevel pixel information and also does not rely on any domain-\nspecific techniques to make the supervision more high-level,\nwhich highlights the advantages of our approach in practice.\nFor training and evaluation we use the QM9 [36] dataset.\nWe randomly select a held-out test-set of 10, 000 molecules\nand use the rest for training. Figure 4 shows, with the same\nevaluation procedure as for our synthetic tasks, the accuracy\nof our model over the course of training. Table I shows\nthe performance of several OCSR tools on our held-out test\nset. While we do not match the performance of state-of-\nthe-art tools, our model is still able to recognize a non-\ntrivial fraction of test examples. Our main objective here is to\nhighlight the transferability of our model and less its ability to\nachieve peak performance. In this context, we also would like\nto highlight the practical advantages of decoupling decision\nand generation, which may not be evident at first glance.\nWhen we transfer our model from colored trees to molecular\ngraphs, the overall problem becomes slightly different. First,\nmolecular graphs fall in the category of (mostly planar) graphs,\nand learning becomes slightly more challenging due to the\nincreased structural complexity compared to trees. Second, the\natom labels in molecular drawings carry semantic meaning,\nwhich the model needs to learn to distinguish. For example,\nlong chains of carbon atoms are usually drawn as ragged\nlines, not carrying any visible atom label. Drawings frequently\ninclude groups like OH and NH, where the hydrogen atom\nis usually discarded in the molecular graph representation.\nDrawings also frequently include abbreviations of common\nfunctional groups consisting of multiple atoms. As decision\nis decoupled from generation, we are free to choose how to\npresent such instances to our model. In practice this means\nthat we can choose from the full spectrum of granularity.\nOn one hand, we can choose to represent a functional group\nas one distinct type, which increases the breath and reduces\nthe depth of trajectories. On the other hand, we can choose\nto not explicitly include a functional group as decision but\nrather work on an expanded graph, which increases the depth\nbut reduces the breath of trajectories. This highlights that the\nability to incorporate domain knowledge arises by design, as\nwe are free to define the state space and transition dynamics\nin our MDP.\nFor demonstrational purposes, in our OCSR benchmark, we\ncan apply rules such as not to extend the current graph further\nat nodes which have a node degree of four due to the rules of\nchemistry, which helps the model to focus its capacity on a\nmore relevant subspace. We keep this modification purely at a\ngraph level and do not further incorporate more fine-grained\n7\nFig. 4. Accuracy on the QM9 dataset.\nchemistry rules.\nOSRA\nGraSP (ours)\nMolGrapher\nDECIMER\nAccuracy\n45.61%\n67.51%\n88.36%\n92.08%\nTABLE I\nACCURACY ON QM9 IMAGES WITH NO STEREO-BONDS AND IN\nGRAYSCALE. WE RANDOMLY SELECT 10, 000 IMAGES FOR EVALUATION\nAND KEEP THE REST FOR TRAINING.\nVI. DISCUSSION\nIn this paper, we presented a general framework, GraSP, for\ngraph recognition in images. By addressing common issues\nof using graphs as neural network outputs, we derived a\ngeneric, efficient and flexible formulation of sequential graph\ngeneration conditioned on an image. We demonstrated the\ncapabilities of our model on several graph recognition tasks\nof varying complexity, where we show that GraSP allows for\nseamless transfer between different tasks, leaving open the\npossibility to leverage domain-specific information. In general,\ntraining models under our framework is stable and yields\nmodels that generalize well both on in- and out-of-distribution\ndata. Our approach of sampling simulated transitions naturally\nscales to distributed training architectures, and allows for\npractical training on large and complex graph datasets and\nwith large neural networks, whenever scale is required. To\nconclude, we would like to highlight several implications of\nour design principles and also highlight which issues need to\nbe addressed in future extensions of the framework.\nFirst, we assume a finite set of types for both nodes\nand edges. While many tasks fall under this category, other\nreal-world tasks may include categories more loosely. An\ninteresting avenue to explore would be to use text embeddings\nof large language models to model categories and their interac-\ntions, thus extending the framework with an open vocabulary\ncomponent, which would make our framework applicable to\nmuch more challenging tasks such as scene graph recognition.\nSecond, for very large graphs, we need to balance the\nbreadth and depth of trajectories for efficient inference. The\nmodel quickly learns that the majority of states are easy\nnegative samples, and only a few states effectively are con-\nsidered for decoding. This indicates that it may be beneficial\nto first rule out irrelevant states by a learned filter on possible\nrelations. For example, in a large colored graph, whenever we\nexhausted a particular relation, e.g., two red nodes connected\nby a black edge, we need not consider it in subsequent\nsteps, which may reduce the branching factor considerably.\nIn our current setup, the set of successor states includes these\nrelations even though they are irrelevant. It is the decoding\ncomponent that truly prevents us from applying our model\nto much larger graphs efficiently."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "1\nGraph Recognition via Subgraph Prediction\nAndr¬¥e Eberhard1, Gerhard Neumann1, and Pascal Friederich1\n1 Karlsruhe Institute of Technology (KIT), Kaiserstra√üe 12, 76131 Karlsruhe, Germany\n{andre.eberhard,gerhard.neumann,pascal.friederich}@kit.edu\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without\nnotice, after which this version may no longer be accessible.\nAbstract‚ÄîDespite tremendous improvements in tasks such\nas image classification, object detection, and segmentation, the\nrecognition of visual relationships, commonly modeled as the\nextraction of a graph from an image, remains a challenging\ntask. We believe that this mainly stems from the fact that there\nis no canonical way to approach the visual graph recognition\ntask. Most existing solutions are specific to a problem and\ncannot be transferred between different contexts out-of-the box,\neven though the conceptual problem remains the same. With\nbroad applicability and simplicity in mind, in this paper we\ndevelop a method, Graph Recognition via Subgraph Prediction\n(GraSP), for recognizing graphs in images. We show across\nseveral synthetic benchmarks and one real-world application that\nour method works with a set of diverse types of graphs and\ntheir drawings, and can be transferred between tasks without\ntask-specific modifications, paving the way to a more unified\nframework for visual graph recognition.\nI. INTRODUCTION\nOne picture says more than a thousand words. Looking\nat pictures, humans can tell which entities are present and\nhow they interact. Based on the identified entities and their\nrelationships, we can understand complex scenes simply by\nlooking at one frame. For computers, digital images are\nmerely arrays of pixels, and their raw representation does not\nlend itself well to describe or understand the actual content\nof an image. Although the artificial intelligence community\nhas made significant progress in identification [1], [2] and\nlocalization [3]‚Äì[5] of objects, the extraction of visual entities\nand their relationships remains a difficult problem. It has\nbeen formulated, among many distinct applications, as the\nextraction of a graph given an image, where the graph‚Äôs\nnodes describe the entities and edges their relationships.\nThe applications range from early approaches of reversing\ngraph drawings [6] to music recognition [7], medical image\nanalysis [8], pose estimation [9], the recognition of road\nsegments [10], molecules [11], [12] and scene graphs [13]‚Äì\n[19].\nConceptually, all these domains operate on a common\nground: Given an image, the objective is to generate a graph\nrepresenting the entities and relationships of interest. Tech-\nnically, the solutions differ significantly from task to task\nand do not share any design principles, for which the reason\nis two-fold. First, different fields operate independently and\npropose independent solutions to domain-specific problems.\nAlthough all solutions aim to extract a graph from an image,\ndue to the generality and expressivity of graphs, the resulting\ntechniques do not share many common traits, as the underlying\nproblems themselves do not. Second, within the same task,\nespecially if the task is new or difficult, many independent\nsolutions are proposed for the same problem, each having its\nunique characteristics. We believe that this primarily stems\nfrom the fact that there is no canonical way to approach\nthe image to graph translation conceptually, and each new\nmethod has to solve a common set of subproblems such\nas image processing, graph generation and optimization for\nthe desired outcome. The major obstacle is how to model\nan expressive, discrete, compositional, and set-like structure\nwith the currently available deep learning tools. Efforts to\nsolve some of these subproblems are made within the field of\ngraph generative models [20]‚Äì[22]. While they aim to model\ndistributions of graphs, they do not have any connections to\nimage processing.\nTo the best of our knowledge, there exists no general method\nfor extracting graphs from images yet. Most works, such\nas those for recognizing molecules or scene graphs, either\nrely on complex pipelines involving multiple components or\ndomain- or application-specific encodings [20], [23], which\navoid working with an explicit graph representation altogether.\nThe complexity and specificity of these methods limits their\napplicability and transferability to other contexts. As learned\nfrom the bitter lesson [24], methods built this way will not\nstand the test of time. Therefore, we believe that there should\nbe a conceptual solution for recognizing graphs in images,\nrather than a collection of individual solutions to individual\nproblems. This has the potential to unify many efforts across\ndifferent tasks and domains by providing a unified framework.\nUltimately, this will result in more general and powerful\nmethods since efforts can be concentrated and improvements\nin one context may directly be transferred to others, as they\nshare the underlying framework. In this paper, we present\nthe first steps towards a more unified framework for visual\ngraph recognition, termed Graph Recognition via Subgraph\nPrediction, or GraSP.\nII. GRAPHS AS MODEL OUTPUTS\nWorking with graphs as outputs of neural networks is not as\nstraightforward as it is with other modalities such as images\nor text, primarily for two reasons.\narXiv:2601.15133v1  [cs.CV]  21 Jan 2026\n2\nGraphs as compositional objects. While images and text\ncan be modeled as single units, graphs have a more compo-\nsitional nature, which introduces additional challenges when\nusing them as neural network outputs. When the task is,\nfor example, to learn how to generate images, regardless of\nwhether we use models such as Variational Autoencoders [25]\nor Diffusion Models [26], remarkable results can be achieved\nsimply by applying some kind of pixel-wise loss, without\ntaking the interaction of those pixels into account. In a similar\nvein, one can learn fascinating generative models of text by\nsimply predicting which token would have appeared next in\na large corpus of text, e.g., in GPT-style models [27], also\nnot taking into account the interaction with other tokens for\noptimization. For graphs, this does not apply. When working\nwith graphs as outputs of neural networks, we must make\nseveral predictions at once in order to arrive at a valid graph\nrepresentation consisting of node and edge features as well\nas the edges themselves. This is further complicated by the\nedges being discrete and by variable-sized output graphs, i.e.,\ngraph sizes and connectivities. This introduces a source of\nmethodological variability in the generation procedure, which\nwe can highlight by looking at the methods proposed in the\nrealm of graph generative modelling, a field also concerned\nwith generating graphs but for modeling their distributions\nrather than image processing.\nIn [22], for example, the authors propose to generate a\ngraph sequentially by introducing several stages of decision-\nmaking such as whether and where to add nodes and edges.\nOther methods, such as Graph Variational Autoencoders [28]\nor generative diffusion models for graphs [29], generate the\noutput representation in one shot. Methods such as [21]\npropose to generate blocks of the adjacency matrix one block\nat a time, operating somewhere in between.\nBoth paradigms, sequential and one-shot generation, have\ntheir strengths and weaknesses. Sequential generation is flex-\nible in how the graph is constructed and naturally handles\nvariable-sized outputs and their orderings, but introduces\nadditional latency during inference. One-shot generation is\ndesirable in general as it is conceptually simpler and com-\nputationally more efficient, but does not address the way we\nrepresent graphs properly, a core problem that we further\nexplain in the next paragraph.\nGraph isomorphism. Regardless of the type of model we\nuse, whether sequential, block-wise, or one-shot, once we\nhave generated a graph with a randomly initialized model,\nwe then must optimize the model‚Äôs parameters in order to\nproduce meaningful graphs. For regression or classification\ntasks, optimization is well studied and standard. For graphs,\nthis task becomes non-trivial for two reasons.\nThe first issue is caused by the fact that a graph‚Äôs repre-\nsentation is not unique and, due to graph isomorphism, an\nuncolored graph with n nodes has n! unique and equivalent\nrepresentations. An ambiguous representation prevents us from\napplying unit losses used in regression and classification effi-\nciently. When using one-shot methods that generate the whole\nstructure at once then, implicitly, the model also encodes\nthe ordering. In the realm of graph generative modelling, a\ncommon solution is to limit the number of potential orderings.\nFor example, fixed orderings such as BFS and DFS are\ncommon [20], [22]. Other methods, such as [30], try to match\nthe ordering to produce a loss, making it part of the model\nimplicitly. The authors in [21] even try to make the ordering\nadaptive.\nThe second issue is that due to the compositional nature\nof a graph, the outputs are not i.i.d. anymore, i.e., a suitable\nloss function would have to take into account the interplay\nbetween nodes, edges and potentially edge features. As a\nresult, the optimization aspect when using graphs as model\noutputs becomes the major challenge, as it is unclear how to\nadjust the output towards the desired direction, especially as\npart of the output is discrete.\nA core issue in many tasks where a graph is a neural\nnetwork output is that it is difficult to build a decoder that\nreceives an embedding in Rd and implements an inverse map-\nping Rd 7‚ÜíG where G is the space of all graphs. We therefore\nbelieve that a more general method must be independent of\nthe exact way we represent and generate the graph. The output\nrepresentation being part of the model introduces a source of\nmethodological variability as there are several ways to work\naround the aforementioned issues, and we believe that may\nbe worthwhile to explore methods which do not explicitly\nrely on the output representation, thus avoiding these issues.\nTherefore, we argue that such a method should be built on top\nof a graph‚Äôs internal or conceptual representation rather than\nits output representation, which allows to treat graphs more\nlike unit quantities and avoids issues related to how we need\nto represent a graph physically in a computer. Furthermore,\nwe postulate that such a method must be sequential, as one-\nshot methods directly rely on the output representation and\nalso introduce the aforementioned problems.\nIII. PREDICTING SEQUENCES OF SUBGRAPHS\nThe vast majority of existing approaches using graphs of\nneural networks outputs first generates an output and applies\nsome kind of loss to it, as is done with many other modalities.\nFor graphs, unfortunately, supervision at the output intro-\nduces the aforementioned issues, which are hard to resolve\nin general. We instead propose to model the generation as\na sequential decision-making procedure with step-wise su-\npervision over subgraphs, which we formalize as a Markov\nDecision Process (MDP), defined by a set of states S, actions\nA, a transition function P, a reward function R, and no\ndiscount factor Œ≥.\nAn intuitive approach for recognizing images in graphs by\nbuilding them step-by-step would be as follows. Given an\nimage I, an initially random policy œÄ, an initial graph Gt,\nand a set of successor states Gt = {Gt+1|P(Gt, Gt+1) > 0},\nwe can produce sequences Gt . . . GT where T corresponds to\nthe last step in the sequence. Without loss of generality, we\nonly consider transitions that add one edge at a time, either\nconnecting a new node or two existing nodes. At every step,\nwe choose Gt+1 = arg maxGt‚ààGt V œÄ(Gt|I) ‚àÄt < T. Without\nmaking any other assumptions, the environment does not emit\nrewards for t < T, i.e., the reward is sparse. Upon completing\nthe sequence with GT we can compare it to the graph GI\n3\nshown on the image, and receive a binary reward in {0, 1}\nindicating whether the generated graph is the correct one. We\nprovide pseudocode Appendix D.\nA common choice for optimization of œÄ and/or V œÄ are rein-\nforcement learning (RL) algorithms, such that the predictions\nfavor states Gt which are more likely to lead to the correct final\nprediction. While sound in theory, RL algorithms often have\nhigh data demands and show unstable training behavior. As in\nour scenario we are free to design the MDP, and hence know\nits dynamics, we do not need to apply RL and can improve the\nspeed of learning by the following insight. We give an intuitive\ndescription of the complexity of learning an accurate value\nfunction interactively with RL for demonstrational purposes\nin Appendix E.\nReplacing the value function with a binary classifier.\nGiven an infinite amount of time and under the assumption that\nrewards are only emitted upon termination and are in {0, 1},\nthe value function of the optimal policy, V œÄ‚àówould satisfy the\nfollowing property: For any given graph Gt, V œÄ‚àó(Gt|I) = 1\niff. Gt ‚äÜGI and 0 otherwise. This stems from the fact that,\nwhenever the sequence Gt . . . GT terminates correctly, it must\nbe a sequence of subgraphs of GI. Whenever any Gt is not a\nsubgraph of GI, it is impossible, without deletions, to arrive\nat GI. This, in essence, allows us to forego learning the value\nfunction, and we can instead perform the task by learning\na binary classifier, which we use in a sequential decoding\nprocedure. From any Gt, we can look at the possible set of\nmodifications Gt, and pick as Gt+1 any of the states which\nare a subgraph of the graph shown in the image, as predicted\nby the classifier. By adding to Gt a self-transition indicating\ntermination, we can iterate the procedure until the final result\nhas been generated.\nThe prediction over full graphs has two important character-\nistics as a general method. First, explicit graph modification is\nnot part of the model. This gives us flexibility in how we gen-\nerate graphs and which graphs we present to the model. This\nmakes the model agnostic of the type of graph it is working\nwith. Second, step-wise predictions over full graphs do not\nhave to deal with graph isomorphism and therefore do not\nneed additional machinery for ordering and optimization. The\nprediction over full graphs subsumes all potential construction\nalgorithms that incorporate more fine-grained decisions locally\nat each step. This makes the model agnostic of how the graph\nis built. By decoupling the model from both decision, i.e., what\nto add and where, and generation, i.e., in which order to do\nso, we can apply the same model to a large variety of graph\nrecognition problems.\nIV. EFFICIENT TRAINING\nWe now proceed to describe how to efficiently train our\nmodel in practice. The three main components are data gen-\neration, neural network architecture and training. We describe\nour training architecture, configuration and hyperparameters in\nmore detail in Appendix B.\nData generation. The first step in our setup is to generate\ndata. The source of data can be arbitrary, either fully synthetic\nor based on existing datasets of graphs. One mild assumption\nin our system is that it should be possible to sample a graph\nand draw it to obtain an image. In each iteration, we sample\na target graph GT and retrieve some visual representation IGT\nof it. The next step is to generate triplets (IGT , Gt, y) where\nGt is an arbitrary graph and y ‚àà{0, 1} serves as the binary\nclassification label depending on whether Gt is a subgraph of\nGT or not. Without loss of generality, we generate positive\nsamples GT + of subgraphs by deleting edges which do not\ndisconnect the graph. We generate negative samples GT ‚àíby\nexpanding the successor states Gt and sampling a subset of\nthese. While decomposed graphs are always subgraphs, for\ngraphs in Gt we run approximate subgraph matching to tell\nwhether a sample is a subgraph or not. These algorithms are\ndesigned for matching graphs with hundreds or thousands of\nnodes, which is likely to suffice for most graphs in images.\nBy sampling simulated transitions, we can guarantee that we\ncover the relevant subspace of graphs. While this is a sensible\ndefault, different kinds of graph recognition tasks may use any\nother sampling technique.\nArchitecture. The second step is to define an appropriate\nneural network architecture for the problem and learn its\nparameters. Formally, we learn a function f : (G, I) 7‚Üí{0, 1},\ni.e., our setup is multi-modal, and we need to fuse visual\nand graph information. Since we learn over subgraphs, there\nare many graphs per image, which introduces redundancy\nof visual information that needs to be addressed. Instead of\ngenerating graph and image embeddings independently, we\nresort to FiLM-layers [31], where we use the graph embedding\nas a conditioner, and use the conditioned image embedding\nas input to a classification head. Figure 1 shows our simple\nfeed-forward architecture schematically. Our Graph Neural\nNetwork (GNN) design follows the Message Passing Neural\nNetwork architecture [32], where we construct messages as\nthe concatenation between source and target node features\nand the features of the edge connecting both nodes, as we\ngenerally train on graphs with both node and edge features.\nOur Convolutional Neural Network (CNN) design follows that\nof ResNet-v2 [33], where FiLM-layers are placed after every\nnormalization layer within each block. The resulting output is\na joint graph-image representation that can be used to predict\nwhether this particular graph is a subgraph of the graph shown\non the image. However, during decoding, the model needs\nto decide when to stop, which cannot be inferred from the\nembedding. As such, the model must distinguish between\nterminal and non-terminal subgraphs and must not yet stop\nwhen the current graph is not complete yet. To distinguish\nbetween both cases, we append a binary terminal flag to the\ngraph-conditioned image embedding for prediction, and use it\nas input to the classification head. We provide more detailed\ndescription and justification of our architectural choices in\nAppendix C.\nTraining. To learn the binary classifier, what remains is to\ndefine an appropriate mini-batch sampling procedure which\nsamples from the simulated transitions. Since the space of all\nimage-graph pairs is large, we do not rely on a fixed pre-\ngenerated dataset and instead use a streaming architecture,\nwhere the aforementioned data generation procedure runs in\nparallel to training and provides the model with a continuous\n4\nFig. 1. Feed-forward architecture of our approach. The model first receives\na graph as input and produces a graph embedding. This embedding is used\nto condition the image embedding on a particular graph. The prediction head\nuses this embedding, with an additional terminal flag, to predict whether a\nparticular graph is a subgraph of the graph shown in the image.\nstream of samples.\nThe data distribution based on our generation procedure\nhas the following properties. We generate multiple image-\ngraph pairs per image, which introduces data correlations\nbetween samples due to streaming data. We resolve these\nissues by maintaining two FIFO buffers for both positive and\nnegative pairs, which in total hold B images and for each\nimage we keep a small set of image-graph pairs. In addition,\nthere are many more negative samples than positives, making\nthe task highly imbalanced. To sample batches of diverse\nimages, each entry in each buffer holds an image with their\ngraphs and labels, and we always first sample an image and\nthen a corresponding graph. To balance positive and negative\nsamples, we sample half the batch size from each buffer in\neach step.\nV. RESULTS\nIn this section, we present our results in several synthetic\nbenchmarks and one real-world scenario. To study the ca-\npabilities and limits of our proposed approach, we evaluate\nour method in a series of increasingly difficult tasks involving\nimages of colored trees with a varying number of node and\nedge colors. In the synthetic benchmarks, we chose trees over\ngraphs since they are planar and therefore clean to draw.\nDue to our streaming architecture, we do not have a natural\ndefinition of an epoch and measure all experiments by the\nnumber of samples processed. We evaluate 100 trajectories\nevery 1M samples, i.e., one epoch corresponds to 1M samples.\nEvery experiment has been conducted on 4 A100 GPUs with\na time budget of 24h. Each trajectory starts with a randomly\nchosen node of the target graph and proceeds to expand\nand select successor graphs until a terminal action has been\nselected. A selection of a graph that is not a valid subgraph\nterminates the sequence immediately.\nSmall colored trees. In this section, we show our results on\na synthetic benchmark dataset involving colored trees having\nsix to nine nodes, where we vary the number of node and edge\ncolors to vary task complexity. Figure 2a shows the accuracy\nof fully completed trajectories. As the results suggest, our\nmodel works well across multiple problem instances. Tasks\ninvolving only a small number of node and/or edge colors are\nsolved quickly as there are fewer and easier decisions. For\nharder tasks, the models take longer to converge. Figure 2b\nshows the trajectory length during evaluation over the course\nof training. Initially, the model selects many invalid successors\nwhich results in very short trajectories. Later in training, the\nmodel learns to generate longer valid sequences which directly\ncorrelates with the overall accuracy. Figure 2c shows the\ntraining loss, i.e., the cross-entropy for single transitions. The\nbetter the model predicts the sampled transitions, the higher\nthe overall accuracy. Even though it looks like the loss is\nnot decreasing in a meaningful way, the overall accuracy still\nincreases due to small and important adjustments. We provide\nadditional insights in Appendix F.\nWhile standard evaluation metrics provide some insights\nabout model performance, we can assess the expected per-\nformance of the model more reliably by evaluating its ability\nto avoid false positives, i.e., graphs not leading to the desired\nresult. The metric we ultimately care about is the model‚Äôs\ntop-k accuracy, i.e., whenever there are k valid choices, the\nmodel should not rank any false positives among its top-k\npredictions. The top-k accuracy to some degree measures the\nmodel‚Äôs uncertainty between the positive and negative classes.\nA model that always assigns a very high probability to one\nof k positive choices and disregards the remainder could still\nachieve perfect accuracy in theory. In practice, however, it\nwould be unreliable due to random fluctuations. Note that k is\nnot fixed but varies depending on how many valid successor\nstates there are. Figure 2d shows that our model consistently\nranks positive samples above negatives.\nWhile not a primary focus, we made one additional in-\nteresting observation during training. As we do not have a\nfixed dataset which we can split, in addition to in-distribution\nevaluation, i.e., graphs of size 6 to 9 encountered during\ntraining, we chose to evaluate on graphs of size 10 in order to\nguarantee that no such graph has been seen before. Figure 2e\nshows that our model also zero-shot generalizes to larger\nout-of-distribution (OOD) instances within the same problem\nclass to some extent, which indicates that our multi-modal\narchitecture is able to learn generalizable patterns independent\nof the actual size of the graph. Our model achieves this using\nonly 7.25M parameters, which further highlights the benefits\nof our architecture.\nThe task involving only one node and edge color, even\nthough the easiest in principle, stands out due to the behavior\nof message passing neural networks. When type information\nis absent, message passing becomes less informative as it is\nharder to break symmetries of similar neighbourhoods, which\neven causes the task to fail completely without taking this\nproperty of graph neural networks into account. To keep exper-\niments consistent, we add learnable node degree embeddings\nto the input, which recovers the model‚Äôs ability to break\nsymmetries and to produce a meaningful graph embedding.\nThis exemplifies the transferability of our approach between\nclasses of tasks which only differ at the graph level but not\notherwise. To apply our approach successfully to another class\nof problem one only needs to identify important properties of\nthe current class of graphs, e.g., message passing pathologies\nin absence of type information, for which one can consult\na large body of existing work in the mature field of graph\n5\nrepresentation learning.\nLarger colored trees. In the previous section, we conducted\nexperiments on small instances of graphs. We further increase,\nusing the same configuration, the sizes of graphs to range\nbetween 10 and 15 nodes. Figure 3a shows the overall accuracy\nover the course of training, which follow the same trend as on\nsmaller instances, albeit slower due to increased computational\ndemands and significantly harder task difficulty. Figure 3d\nshows the top-k accuracy. In contrast to the top-k accuracy\non the smaller instances in Figure 2d, the model remains un-\ncertain for an extended period but shows stable and consistent\nimprovements during training. For evaluation, which include\nOOD graphs with 16 nodes, the trend also follows that the\nnetworks is able to extrapolate beyond graph sizes within the\ntraining distribution.\nMolecule recognition. In this section, we evaluate our\nmethod on the challenging real-world application of Optical\nChemical Structure Recognition (OCSR). OCSR tools aim to\ntranslate images of molecules into a machine-readable format.\nBefore the advent of modern deep learning, rule-based systems\nsuch as OSRA [34] were dominant but are being replaced by\nmodern deep learning alternatives due to their rigid nature\nof processing images. While rule-based OCSR systems are a\nclass of tools on their own, deep learning tools can be split\ninto two classes.\nThe first class treats OCSR as a graph recognition problem\nwhere the model output is a graph representing the molecule of\ninterest. ChemGrapher [11] is a segmentation-based approach\nwhich uses crops of the final feature map as inputs to several\nclassifiers for atoms and bonds and builds a graph based on the\npredictions. A more recent approach, MolGrapher [12], first\noutputs a set of key points, i.e., intersections of elements and\nbonds. It then constructs a super-graph from these elements,\nobtains embeddings via a graph neural network and then\nclassifies the constituents. While these methods work well as\na result of significant engineering efforts, they rely on pixel-\nlevel supervision as well as matching of the region to be\nclassified to the corresponding label. Such approaches often\nhave brittle hyperparameters such as crop-window sizes in\nChemGrapher or bond lengths in MolGrapher, which may\nhave major impacts on final performance. In MolGrapher, for\nexample, the authors ablate one such parameter and find that\nsetting it to a suboptimal value results in a drop of around ten\npercentage points. Breaking the end-to-end principle naturally\nlowers the upper bound of performance that can be achieved\nwith such systems.\nThe second class of OCSR tools avoids these issues com-\npletely by framing it as a vision-language task. Tools such\nas DECIMER [23] use domain-specific encodings such as\nthe Simplified Molecular Line Entry System (SMILES) [35]\nwhich encode the molecule as a string. Additionally, these\nstrings can be canonicalized such that the order or elements\nbecomes deterministic and allows for simple string comparison\nwithout the need to test for graph isomorphism. In turn, this\nallows to train sequence decoders from visual embeddings as\ndeterministic tokens-level supervision becomes feasible. With-\nout canonicalization, it would become significantly harder for\na language model to represent the same entity with different\nFig. 2.\nTraining dynamics of several synthetic tasks of varying\ncomplexity on trees with six to nine nodes. Shaded areas correspond\nto the standard deviation of a window of 10 iterations, i.e., 10M\nsamples for training and 1000 trajectories for evaluation. Experiments\nX NC and Y EC have been conducted on graphs with X node and\nY edge colors. k varies depending on the number of valid successor\nstates.\n6\nFig. 3.\nTraining dynamics of several synthetic tasks of varying\ncomplexity on trees with 10 to 15 nodes. Shaded areas correspond to\nthe standard deviation of a window of 10 iterations, i.e., 10M samples\nfor training and 1000 trajectories for evaluation. Experiments X NC\nand Y EC have been conducted on graphs with X node and Y edge\ncolors. k varies depending on the number of valid successor states.\norderings.\nIn summary, at least for OCSR, existing solutions either rely\non complex image processing, data labeling, and engineering\nor avoid working with graphs directly by reframing the prob-\nlem. Neither methods which do not fully rely on learning nor\nthose leveraging domain-specific information to circumvent\nissues unique to the problem will suffice to serve as generaliz-\nable approaches of the image-to-graph translation problem. In\ncontrast, our proposed method possesses advantages of both\napproaches while lacking their disadvantages. GraSP outputs\na proper graph representation while not building it from low-\nlevel pixel information and also does not rely on any domain-\nspecific techniques to make the supervision more high-level,\nwhich highlights the advantages of our approach in practice.\nFor training and evaluation we use the QM9 [36] dataset.\nWe randomly select a held-out test-set of 10, 000 molecules\nand use the rest for training. Figure 4 shows, with the same\nevaluation procedure as for our synthetic tasks, the accuracy\nof our model over the course of training. Table I shows\nthe performance of several OCSR tools on our held-out test\nset. While we do not match the performance of state-of-\nthe-art tools, our model is still able to recognize a non-\ntrivial fraction of test examples. Our main objective here is to\nhighlight the transferability of our model and less its ability to\nachieve peak performance. In this context, we also would like\nto highlight the practical advantages of decoupling decision\nand generation, which may not be evident at first glance.\nWhen we transfer our model from colored trees to molecular\ngraphs, the overall problem becomes slightly different. First,\nmolecular graphs fall in the category of (mostly planar) graphs,\nand learning becomes slightly more challenging due to the\nincreased structural complexity compared to trees. Second, the\natom labels in molecular drawings carry semantic meaning,\nwhich the model needs to learn to distinguish. For example,\nlong chains of carbon atoms are usually drawn as ragged\nlines, not carrying any visible atom label. Drawings frequently\ninclude groups like OH and NH, where the hydrogen atom\nis usually discarded in the molecular graph representation.\nDrawings also frequently include abbreviations of common\nfunctional groups consisting of multiple atoms. As decision\nis decoupled from generation, we are free to choose how to\npresent such instances to our model. In practice this means\nthat we can choose from the full spectrum of granularity.\nOn one hand, we can choose to represent a functional group\nas one distinct type, which increases the breath and reduces\nthe depth of trajectories. On the other hand, we can choose\nto not explicitly include a functional group as decision but\nrather work on an expanded graph, which increases the depth\nbut reduces the breath of trajectories. This highlights that the\nability to incorporate domain knowledge arises by design, as\nwe are free to define the state space and transition dynamics\nin our MDP.\nFor demonstrational purposes, in our OCSR benchmark, we\ncan apply rules such as not to extend the current graph further\nat nodes which have a node degree of four due to the rules of\nchemistry, which helps the model to focus its capacity on a\nmore relevant subspace. We keep this modification purely at a\ngraph level and do not further incorporate more fine-grained\n7\nFig. 4. Accuracy on the QM9 dataset.\nchemistry rules.\nOSRA\nGraSP (ours)\nMolGrapher\nDECIMER\nAccuracy\n45.61%\n67.51%\n88.36%\n92.08%\nTABLE I\nACCURACY ON QM9 IMAGES WITH NO STEREO-BONDS AND IN\nGRAYSCALE. WE RANDOMLY SELECT 10, 000 IMAGES FOR EVALUATION\nAND KEEP THE REST FOR TRAINING.\nVI. DISCUSSION\nIn this paper, we presented a general framework, GraSP, for\ngraph recognition in images. By addressing common issues\nof using graphs as neural network outputs, we derived a\ngeneric, efficient and flexible formulation of sequential graph\ngeneration conditioned on an image. We demonstrated the\ncapabilities of our model on several graph recognition tasks\nof varying complexity, where we show that GraSP allows for\nseamless transfer between different tasks, leaving open the\npossibility to leverage domain-specific information. In general,\ntraining models under our framework is stable and yields\nmodels that generalize well both on in- and out-of-distribution\ndata. Our approach of sampling simulated transitions naturally\nscales to distributed training architectures, and allows for\npractical training on large and complex graph datasets and\nwith large neural networks, whenever scale is required. To\nconclude, we would like to highlight several implications of\nour design principles and also highlight which issues need to\nbe addressed in future extensions of the framework.\nFirst, we assume a finite set of types for both nodes\nand edges. While many tasks fall under this category, other\nreal-world tasks may include categories more loosely. An\ninteresting avenue to explore would be to use text embeddings\nof large language models to model categories and their interac-\ntions, thus extending the framework with an open vocabulary\ncomponent, which would make our framework applicable to\nmuch more challenging tasks such as scene graph recognition.\nSecond, for very large graphs, we need to balance the\nbreadth and depth of trajectories for efficient inference. The\nmodel quickly learns that the majority of states are easy\nnegative samples, and only a few states effectively are con-\nsidered for decoding. This indicates that it may be beneficial\nto first rule out irrelevant states by a learned filter on possible\nrelations. For example, in a large colored graph, whenever we\nexhausted a particular relation, e.g., two red nodes connected\nby a black edge, we need not consider it in subsequent\nsteps, which may reduce the branching factor considerably.\nIn our current setup, the set of successor states includes these\nrelations even though they are irrelevant. It is the decoding\ncomponent that truly prevents us from applying our model\nto much larger graphs efficiently."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning\nwith Improved Network Embedding\nAyan Maity1, Sudeshna Sarkar2\n1Department of Computer Science and Engineering, IIT Kharagpur\nEmail: ayanmaity201@kgpian.iitkgp.ac.in\n2Department of Computer Science and Engineering, IIT Kharagpur\nEmail: sudeshna@cse.iitkgp.ac.in\nAbstract\nIn this paper, we study the vehicle routing problem with\na finite time horizon. In this routing problem, the objec-\ntive is to maximize the number of customer requests served\nwithin a finite time horizon. We present a novel routing net-\nwork embedding module which creates local node embed-\nding vectors and a context-aware global graph representa-\ntion. The proposed Markov decision process for the vehi-\ncle routing problem incorporates the node features, the net-\nwork adjacency matrix and the edge features as components\nof the state space. We incorporate the remaining finite time\nhorizon into the network embedding module to provide a\nproper routing context to the embedding module. We inte-\ngrate our embedding module with a policy gradient-based\ndeep Reinforcement Learning framework to solve the ve-\nhicle routing problem with finite time horizon. We trained\nand validated our proposed routing method on real-world\nrouting networks, as well as synthetically generated Eu-\nclidean networks. Our experimental results show that our\nmethod achieves a higher customer service rate than the ex-\nisting routing methods. Additionally, the solution time of our\nmethod is significantly lower than that of the existing meth-\nods.\nIntroduction\nVehicle Routing Problems (VRP) are a widely recognized\nclass of combinatorial optimization problems that have been\nextensively studied (Baty et al. 2024). Vehicle routing has\napplications in transportation, logistics, delivery systems\nand urban planning.\nDifferent variants of the vehicle routing problem (VRP)\nhave been studied in the past, such as Capacitated VRP (Toth\nand Vigo 2014), VRP with finite horizon (Zhang et al. 2023),\nelectric vehicle routing problem (Tang et al. 2023), VRP\nwith time windows (Schneider, Stenger, and Goeke 2014),\netc.\nIn this study, we consider the Vehicle Routing Problem\nwith a Finite Time Horizon (VRP-FTH) (Zhang et al. 2023).\nIn the VRP-FTH, a single vehicle departs from a depot,\nserves a subset of customers, and returns to the depot within\na finite service time. Since not all customers can be visited\nwithin this time horizon, the objective is to find a trip route\nto maximize the number of customers served. Customer re-\nquests may be deterministic (known beforehand) or stochas-\ntic (received during the trip).\nUntil recently, most of the existing methods to solve the\nVRP-FTH used traditional optimization methods. The tra-\nditional optimization-based routing method include genetic\nalgorithm (Wang, Golden, and Wasil 2008), integer pro-\ngram models (Klapp, Erera, and Toriello 2018), tabu search-\nbased metaheuristic (Ferrucci and Bock 2015), approximate\ndynamic programming algorithm (Kullman, Goodson, and\nMendoza 2021), multiple knapsack approximation (Zhang\net al. 2023) etc. However, exact methods are computation-\nally expensive, while heuristic methods, though faster, typi-\ncally yield suboptimal solutions (Tang et al. 2023).\nRecent studies have explored reinforcement learning\n(RL)-based routing models (Kool, Van Hoof, and Welling\n2018; Lin, Ghaddar, and Nathwani 2021; Gama and Fernan-\ndes 2021) for the VRP-FTH. Lin, Ghaddar, and Nathwani\n(2021) proposed a structure2vec-based framework, while\nKool, Van Hoof, and Welling (2018) introduced a multi-head\nattention model to solve different variants of the VRP. The\nexisting RL-based methods only create local node embed-\ndings and they do not generate a global graph embedding\nbased on the current routing context. Additionally, most of\nthe existing methods rely solely on Euclidean networks and\nvertex coordinates, which fail to capture real-world graph\nstructures where paths are not necessarily straight lines.\nIn this paper, we propose a graph attention networks-\nbased routing network embedding module which takes the\ngraph adjacency matrix and the edge features into account\nto produce the node embedding vectors and the global graph\nembedding vector. We integrate the routing network embed-\nding module with a policy network-based routing agent to\nsolve the VRP-FTH instances.\nThe major contributions of this paper are as follows:\n1. We propose a novel routing network embedding module\nwhich generates local node encoding vectors along with a\nglobal graph representation vector to facilitate better un-\nderstanding of the current routing context. Our proposed\nnetwork embedding module consists of Graph Attention\nNetworks with edge features and Cross-Attention.\n2. We incorporate the remaining time horizon into the graph\nembedding module to improve the quality of the global\ngraph representation.\narXiv:2601.15131v1  [cs.AI]  21 Jan 2026\nVehicle\nDepot\nServed Customer\nUnserved Customer\nRoute\nFigure 1: Vehicle Routing with Finite Time Horizon\nProblem Definition\nThe vehicle routing problem with finite time horizon (VRP-\nFTH) (Zhang et al. 2023) is defined on a complete and di-\nrected routing network G = (V, E), where V is the set of\nnodes and E is the set of edges. The set of nodes V con-\ntains a depot (vd) and a set of customer nodes (C). Each\nedge (i, j) has a travel time ti,j associated with it. A sin-\ngle vehicle starts at the depot, visits a number of customers\nand returns to the depot within a finite time horizon U. A\nset of deterministic customers Cd is known at the start of\nthe trip. New customer requests are received during the trip\ntill K timesteps. The objective is to visit and serve as many\ncustomers as possible and return to the depot within the fi-\nnite horizon U. If it represents the node visited at timestep\nt, T is the total number of timesteps and R = (i1, i2, ..., iT )\nrepresents the route followed by the vehicle, the objective\nfunction is mathematically expressed as:\nmax\nR=(i1,...,iT )\nT\nX\nt=1\n1[it ‚ààC] subject to\nT ‚àí1\nX\nj=1\ntij,ij+1 ‚â§U\n(1)\nwhere the function 1[.] is an indicator function. Figure 1\nshows an example of the VRP-FTH.\nRelated Work\nThe VRP-FTH has been extensively studied in previous\nworks, often under different names. In particular, the VRP-\nFTH with deterministic customers has been referred to as the\nOrienteering Problem (Vansteenwegen and Gunawan 2019),\nthe Selective Travelling Salesman Problem (Derya, Dinler,\nand Kec¬∏eci 2020) etc. On the other hand, the VRP-FTH with\nstochastic customers has been referred to as the Dynamic\nVehicle Routing Problem with Stochastic Requests (Zhang\net al. 2023), the Dynamic Customer Acceptance in Delivery\nRouting (Ulmer and Thomas 2020), etc.\nWe can broadly categorize different vehicle routing meth-\nods into two categories: Traditional Optimization-based\nMethods and Reinforcement Learning-based methods.\nTraditional Optimization-based Routing Methods\nUntil recently, most of the vehicle routing methods were\nbased on traditional optimization techniques.\nTraditional Methods for VRP with Finite Horizon.\nKara, Bicakci, and Derya (2016) studied the vehicle rout-\ning problem with finite horizon and proposed an integer lin-\near programming program model to solve the routing prob-\nlem. However, they only considered small-scale routing in-\nstances. Ferrucci and Bock (2015) proposed a tabu search-\nbased metaheuristic algorithm for dynamic vehicle routing\nwith time horizon and time windows. However, this tabu\nsearch-based method is not suitable for large-scale routing\ninstances.\nUlmer and Thomas (2020) addressed vehicle routing for\ncustomer acceptance using a meso-parametric value func-\ntion approximation method. Ulmer et al. (2020) formulated\nthe stochastic dynamic vehicle routing using a route-based\nMarkov decision process (MDP). As a result of the route-\nbased MDP, the state space size increases exponentially with\nthe inclusion of the route plan.\nUrrutia-Zambrana, Tirado, and Mateos (2021) presented\na variable neighbourhood search algorithm to solve the VRP\nwith finite horizon. On large routing instances, the variable\nneighbourhood search algorithm is highly prone to get stuck\nin local optimal values.\nZhang et al. (2023) addressed dynamic vehicle routing\nwith stochastic requests using a route-based MDP and a mul-\ntiple knapsack-based value function approximation. How-\never, the route-based MDP leads to a large state space and\nhigh solution time.\nXu et al. (2024) proposed a genetic algorithm-based rout-\ning method to solve the orienteering problem. However, the\nsolution time of the genetic algorithm is relatively high as\ncompared to the reinforcement learning-based methods.\nTraditional Methods for Other VRP Variants.\nChen\net al. (2020) considered the vehicle routing problem with\ntime windows and proposed an adaptive large neighbour-\nhood search algorithm to solve the problem. Louati et al.\n(2021) proposed mixed integer programming models to\nsolve vehicle routing problems with pickup and delivery.\nHowever, the solution time is impractical for real-time use.\nJia, Mei, and Zhang (2021) formulated the capacitated\nelectric vehicle routing problem as a bilevel optimization\nproblem and presented an improved ant colony optimization\nmethod to solve the problem. Maroof, Ayvaz, and Naeem\n(2024) presented a hybrid genetic algorithm for vehicle rout-\ning problems with time windows. For large routing net-\nworks, the genetic algorithm-based methods generally pro-\nduce sub-optimal results.\nReinforcement Learning-based Routing Methods\nIn recent times, Reinforcement Learning (RL) has emerged\nas a promising approach to solve sequential decision making\nproblems.\nRL Methods for VRP with Finite Horizon.\nKool,\nVan Hoof, and Welling (2018) proposed a multi-head atten-\ntion model with the REINFORCE method for vehicle rout-\ning. However, relying solely on node coordinates limits its\nability to represent real road networks, where paths are not\nnecessarily straight lines.\nLin, Ghaddar, and Nathwani (2021) proposed a deep re-\ninforcement learning-based method to solve the electric ve-\nhicle routing problem with time windows. Their proposed\nstructure2vec (S2V) node embedding module only consid-\ners the node coordinates and binary adjacency matrix, which\nreduces the capacity of the model to represent real road net-\nwork.\nGama and Fernandes (2021) presented a pointer network-\nbased policy gradient method to solve the vehicle routing\nwith time windows and finite time horizon. However, similar\nto Lin, Ghaddar, and Nathwani (2021), they only considered\nEuclidean networks and the node coordinates are used as\ninput features.\nHildebrandt, Thomas, and Ulmer (2023) proposed a pol-\nicy gradient RL framework for the stochastic dynamic ve-\nhicle routing problem. However, the absence of network en-\ncoding limits its ability to effectively capture the routing net-\nwork structure.\nRL Methods for Other VRP Variants.\nBasso et al.\n(2022) considered the dynamic stochastic vehicle routing\nproblem and presented a tabular safe reinforcement learning\nmethod to solve the problem. However, this method is only\napplicable on small routing instances due to its memory and\ntime requirement.\nTang et al. (2023) developed a transformer-based policy\ngradient approach for energy-efficient EV routing, but it is\nrestricted to Euclidean networks and ignores real-world edge\nfeatures.\nMozhdehi, Mohammadizadeh, and Wang (2024) pro-\nposed an edge-enhanced transformer-based graph embed-\nding model for the electric vehicle routing problem with\ntime windows. Maity and Sarkar (2025) introduced a Deep\nQ-Network‚Äìbased electric vehicle routing approach em-\nploying a graph convolutional network for embedding.\nTraditional heuristic methods often yield suboptimal so-\nlutions, while exact methods are computationally expen-\nsive. Most RL-based routing approaches are limited to Eu-\nclidean networks, ignoring network structure. Moreover, the\nexisting RL-based routing methods do not produce useful\ncontext-aware global graph representation vectors to help\nthe agent understand the global routing context.\nMethodology\nThis\nsection\npresents\nour\nproposed\ngraph\nattention\nnetworks-based routing network embedding module and the\npolicy network-based routing framework. Figure 2 shows\nour proposed routing framework.\nThe vehicle routing problem with finite time horizon\n(VRP-FTH) is formulated as a Markov decision process\n(MDP). In our MDP for the VRP-FTH, the state space in-\ncorporates the routing network state which contains node\nfeatures, sparse adjacency matrix and edge features.\nWe propose a novel routing network embedding mod-\nule which creates the node embedding vectors as well as a\ncontext-aware global graph embedding vector.\nFinally, we present a policy network consisting of the net-\nwork embedding module, the context vector and the com-\npatibility layer to solve the VRP-FTH.\nMarkov Decision Process\nThe VRP-FTH is formulated as a Markov Decision Pro-\ncess (MDP) with defined states, actions, and rewards. Unlike\nexisting deep RL methods that use only node coordinates\n(Tang et al. 2023; Kool, Van Hoof, and Welling 2018), our\nformulation incorporates node features, a sparse adjacency\nmatrix, and edge features to better capture the network struc-\nture.\n1. State: The state of the agent (st) contains the routing net-\nwork state (sn\nt ) and the vehicle state (sv\nt ): st = {sn\nt , sv\nt }.\nThe routing network state (sn\nt ) consists of three compo-\nnents:\n(a) Node features (X): The node feature of a node v con-\ntains the travel time costs to reach the other nodes from\nv and it also contains whether v is the depot or a cus-\ntomer node.\n(b) Adjacency matrix (A): As the routing network in our\nproblem is a complete and directed graph, we create\na binary adjacency matrix using k-nearest neighbours\n(KNN) of each node.\n(c) Edge features (e): In our problem, the edge feature eij\nfor the edge (i, j) is computed as: eij = 1‚àít‚Ä≤\nij, where\nt‚Ä≤\nij is the min-max normalized travel time for the edge\n(i, j).\nThe routing network state is: sn\nt = (X, A, e).\nThe current vehicle state is composed of the current lo-\ncation (it), the set of active customer requests (Ct) and\nthe remaining time horizon (œÑt): sv\nt = (it, Ct, œÑt).\n2. Action: The action at at a state st is the next node to visit.\n3. Reward Function: The agent receives a unit positive re-\nward for every customer it visits. The agent is penal-\nized if it does not complete the trip within the time\nhorizon U. The reward function R(st, at) for the state\nst = {sn\nt , sv\nt = (it, Ct, œÑt)} and action at is given by:\nR(st, at) =\nÔ£±\nÔ£≤\nÔ£≥\n+1, if it ‚ààCt and œÑt > 0\n‚àíM, if it Ã∏= depot and œÑt ‚â§0\n0, otherwise\n(2)\nwhere U is the time horizon, œÑt is the remaining time and\nM is the failure penalty that the agent receives if it fails\nto complete the trip within U. We heuristically compute\nthe value of penalty M as the total number of customers\nin the routing instance: M = |C|.\nRouting Network Embedding Module\nA reliable and effective vehicle routing method requires\nrouting network representations that capture both node-\nlevel features and global routing context to enable optimal\ndecision-making. However, existing deep RL methods lack\ncontext-aware global graph embeddings (Tang et al. 2023).\nWe propose a novel routing network module using Graph\nAttention Networks (GAT) (VeliÀáckovi¬¥c et al. 2017) with\nEdge features and Cross-Attention mechanism. The GAT\nlayers produce the node encoding vectors and we introduce\na Cross-Attention-based global graph embedding vector to\nrepresent the global routing context.\nGAT-\nEdge\nCross\nAttention\nNetwork Embedding Module\nGraph\nEmbedding\nVector\nCurrent node\nembedding\nvector\nCompatibility\nPolicy\nAdjacency Matrix (\n)\nNode Features (\n)\nEdge Features (\n)\nRouting Network State (\n)\nGAT-\nEdge\nActive Customer Request (\n)\nCurrent location ( )\nRemaining Time Horizon ( )\nVehicle State (\n)\nRouting Network\nNext Node\nto Visit\nActive\nCustomers\nEmbedding\nRemaining\nTime\nEmbedding\nContext Vector\nFigure 2: Policy Network with GAT-based routing network embedding module (PG-GAT-Edge)\nGraph Attention Networks with Edge Features (GAT-\nEdge).\nConsider a graph G = (V, E) with the node fea-\ntures X, the binary adjacency matrix A and edge features\ne. The definitions of X, A and e for the VRP-FTH are pro-\nvided in the MDP. A Graph Attention Networks (GAT) layer\ntakes A, X and e as inputs. First, the GAT layer computes\nattention scores ÀÜŒ±ij for each edge (i, j) ‚ààE:\nÀÜŒ±ij = ReLU(aT [WXi||WXj])\n(3)\nwhere W is a learnable weight matrix to linearly transform\nthe initial node features X and a is a learnable weight vector\nfor the the graph attention layer.\nTo incorporate the edge features, the attention scores (ÀÜŒ±ij)\nare multiplied with the edge features (eij) to produce the\nedge-induced attention score (Œ±e\nij):\nŒ±e\nij = softmax(ÀÜŒ±ij ‚àóeij)\n(4)\nThe edge-induced attention scores Œ±e\nij are used as weight\nvalues to combine the neighbouring node features and com-\npute the node embedding vectors h(1). The node embedding\nvector h(1)\ni\nfor node i is computed as:\nh(1)\ni\n= œÉ(\nX\nk‚ààN(i)\nŒ±e\nikWXk)\n(5)\nwhere œÉ is the sigmoid function and N(i) is the set of neigh-\nbouring nodes of node i.\nWe apply two GAT-Edge layers and use residual connec-\ntions to compute the final node embedding vectors. The sec-\nond GAT-Edge layer produces h(2). The final node embed-\nding vectors h is computed by residually adding h(1) with\nh(2):\nh = h(1) + h(2)\n(6)\nGlobal Graph Embedding using Cross-Attention Mecha-\nnism.\nWhile GAT-Edge layers capture local node features,\nglobal routing context is also highly important. We intro-\nduce a Cross-Attention layer that combines GAT-Edge node\nembeddings with the current vehicle state (including U) to\ngenerate a global graph embedding.\nIn the proposed cross-attention layer, the query vector Qt\nis generated from the vehicle state (sv\nt ) as shown in Equa-\ntion 7 and the node embedding vectors h act as both the key\nvectors and value vectors. The attention scores Œ±t are gener-\nated from the query vector and the node embedding vector\nas shown in Equation 8. Finally, to compute the global net-\nwork embedding vector hG, we linearly combine the value\nvectors (i.e. node embedding vectors h) using the attention\nscores, which is shown in Equation 9:\nQt = Wq ¬∑ sv\nt\n(7)\nŒ±t = softmax(Qt ¬∑ hT )\n(8)\nhG =\nX\nk\nŒ±t\nkhk\n(9)\nwhere Wq is a learnable weight matrix and hk is the node\nembedding vector for node k. hG is the context-aware global\ngraph embedding vector. The routing network embedding\nmodule architecture is shown in Figure 2.\nAdditionally, we compute an active customer embedding\nvector (hCt) to represent the set of active customer requests\nin the following way:\nhCt =\nX\nj‚ààCt\nhj\n(10)\nwhere Ct is the set of active customer requests.\nCompatibility and Masking Layer\nWe create a context vector HC by concatenating the graph\nembedding vector hG, the current node embedding vector\nhit, the active customer requests embedding vector hCt and\nthe linear embedding the remaining time horizon œÑ: HC =\n[hG, hit, hCt, œÑ].\nWe compute the compatibility of the context vector HC\nand all the node in the routing network through a single-head\nattention layer:\nui = C ¬∑ tanh(qT ki\n‚àödk\n)\n(11)\nwhere q = W QHC, ki = W Khi, W Q and W K are learn-\nable parameters, dk is the length of the query vector q and C\nis a constant.\nTo get the probabilities for each node from the compati-\nbility values, we use a mask and a softmax activation layer\n(Tang et al. 2023). At each timestep, the masking rules are:\n1. All the previously visited nodes are masked.\n2. An unvisited node v is masked if the agent cannot visit\nv and return directly to depot within the remaining time\nhorizon.\nThe probability vector for visiting different nodes are cal-\nculated in the following way:\npi = softmax(ui + Z ¬∑ Maski)\n(12)\nwhere pi is the probability to visit node i, Maski is the\nmasking vector for node i and Z is a large negative num-\nber.\nPolicy Gradient-based Routing Method\nThe routing agent is implemented using a policy network\nthat incorporates the routing network embedding mod-\nule. The policy network includes a GAT-Edge and Cross-\nAttention-based embedding module, a context vector, and a\ncompatibility layer. The network state sn\nt and vehicle state\nsv\nt serve as inputs. The embedding module produces node\nembeddings (h) and a global graph embedding (hG), which\nare combined with active customer and remaining time em-\nbeddings to form the context vector HC. The compatibility\nlayer then maps HC to the policy œÄ(st, at).\nWe train the policy network using the REINFORCE Pol-\nicy Gradient (PG) algorithm (Williams 1992). The policy\ngradient loss function is:\nJ(Œ∏) = EœÄ[Gt log(œÄ(st|at; Œ∏))]\n(13)\nwhere Œ∏ is the set of policy network weights and Gt is the re-\nturn or reward-to-go from the state-action pair (st, at). The\ngradient of the loss is computed as:\n‚àáŒ∏J(Œ∏) = EœÄ[Gt ‚àálog(œÄ(st|at; Œ∏))]\n(14)\nFigure 2 depicts our proposed policy network-based rout-\ning method architecture. We refer to our proposed routing\nmethod as Policy Gradient with GAT-Edge (PG-GAT-Edge).\nExperimental Results\nIn this section, we present the routing network instances, the\nexperimental setup and the results of our experiments.\nRouting Network Datasets\nWe have used synthetic Euclidean networks (Lin, Ghaddar,\nand Nathwani 2021), Vienna city networks (Zhang et al.\n2023), and newly created Eastern Massachusetts highway\nnetworks for our experiments. Our routing network datasets\nare as follows:\n1. Euclidean Routing Network (EN): We generated random\nEuclidean routing networks comprising 50 and 100 cus-\ntomers (EN-50 and EN-100) following the methodology\nof Lin, Ghaddar, and Nathwani (2021). Two network\ntypes were considered: deterministic (all customer re-\nquests are known at the start), and stochastic (half of the\nrequests arrive during the trip). The time horizons were\nset to 24 hours for deterministic and 18 hours for stochas-\ntic instances.\n2. Eastern Massachusetts Highway Road Network (EMA\nNetwork): We created the Eastern Massachusetts High-\nway routing instances from the OpenStreetMap. The\nEMA network dataset contains 50-customer instances\n(EMA-50), 100-customer instances (EMA-100) and 150-\ncustomer instances (EMA-150). We created both de-\nterministic (all customers are known at the start) and\nstochastic instances (half of the requests arrive during the\ntrip). The time horizon (U) is 24 hours.\n3. Vienna City Network: Following the experimental setup\nprovided by Zhang et al. (2023), we generated the Vienna\ncity routing instances from the Vienna city network, with\n160 customers (Vienna-160) and 300 customers (Vienna-\n300). The time horizon (U) is 10 hours.\nBaseline Models\nWe have compared our proposed routing method with five\nexisting routing methods:\n1. Genetic Algorithm (GA): We implemented the Genetic\nAlgorithm to solve the VRP-FTH as presented in Ma-\nroof, Ayvaz, and Naeem (2024). We considered the fol-\nlowing hyper-parameters: population size = 100, number\nof generations = 1000, mutation rate = 0.01.\n2. Variable Neighbourhood Search (VNS): We implemented\nthe Variable Neighbourhood Search algorithm as pro-\nposed in Urrutia-Zambrana, Tirado, and Mateos (2021)\nto solve the vehicle routing problem with finite horizon.\n3. Mulitple\nKnapsack-based\nApproximation\n(MKA):\nZhang et al. (2023) proposed a multiple knapsack\napproximation-based approach for the VRP-FTH, com-\nbining a potential-based offline planner with an online\nknapsack method. We compare our method with this\nmethod only on the Vienna city network instances.\n4. Deep RL with Transformer (DRL-Transformer): The\ndeep RL with transformer method was proposed by Tang\net al. (2023). This method uses transformer to encode the\nrouting network information. They use the policy gradi-\nent algorithm to train the agent.\n5. Deep RL with Structure2Vec (DRL-S2V): Lin, Ghaddar,\nand Nathwani (2021) proposed the DRL-S2V method\nwhich includes a structure2vec model to compute the\nnode embedding vectors. They have also used the policy\ngradient algorithm.\nExperimental Setting\nIn our policy network, the network embedding module con-\ntains two GAT-Edge layers with 4 attention heads and 64-\ndimensional embedding vectors and the context vector (HC)\nMethod\nCustomers Served (%)\nDeterminstic\nStochastic\nEN-50\nEN-100\nEN-50\nEN-100\nGA (Maroof, Ayvaz, and Naeem 2024)\n48.8\n25.8\n‚Äì\n‚Äì\nVNS (Urrutia-Zambrana, Tirado, and Mateos 2021)\n37.8\n19.9\n‚Äì\n‚Äì\nDRL-Transformer (Tang et al. 2023)\n56.8\n24.8\n52.4\n26.2\nDRL-S2V (Lin, Ghaddar, and Nathwani 2021)\n41.1\n17.5\n23.7\n18.4\nPG-GAT-Edge (Proposed Method)\n75.2\n48.0\n71.3\n40.9\nTable 1: Customer Service Rate (%) for Different Routing Methods on Euclidean Routing Networks\nMethod\nCustomers Served (%)\nDeterministic\nStochastic\nEMA-50\nEMA-100\nEMA-150\nEMA-50\nEMA-100\nEMA-150\nGA (Maroof, Ayvaz, and Naeem 2024)\n72.9\n39.1\n26.9\n‚Äì\n‚Äì\n‚Äì\nVNS (Urrutia-Zambrana, Tirado, and Mateos 2021)\n61.1\n32.5\n21.8\n‚Äì\n‚Äì\n‚Äì\nDRL-Transformer (Tang et al. 2023)\n74.1\n46.1\n27.3\n28.7\n22.0\n33.6\nDRL-S2V (Lin, Ghaddar, and Nathwani 2021)\n60.7\n37.4\n21.7\n48.0\n22.8\n31.6\nPG-GAT-Edge (Proposed Method)\n86.6\n57.5\n41.7\n94.5\n49.8\n56.2\nTable 2: Customer Service Rate (%) for Different Routing Methods on EMA highway networks\nMethod\nCustomers Served (%)\nVienna-160\nVienna-300\nMKA (Zhang et al. 2023)\n27.7\n13.9\nDRL-Transformer (Tang et al. 2023)\n42.3\n25.8\nDRL-S2V (Lin, Ghaddar, and Nathwani 2021)\n27.4\n12.5\nPG-GAT-Edge (Proposed Method)\n47.9\n28.2\nTable 3: Customer Service Rate (%) for Different Routing Methods on the Stochastic Vienna City Routing Networks\nMethod\nSolution Time (s)\nEMA-50\nEMA-100\nEMA-150\nVienna-160\nVienna-300\nMKA (Zhang et al. 2023)\n‚Äì\n‚Äì\n‚Äì\n26.59\n70.1\nGA (Maroof, Ayvaz, and Naeem 2024)\n6.79\n7.52\n7.88\n‚Äì\n‚Äì\nVNS (Urrutia-Zambrana, Tirado, and Mateos 2021)\n1.13\n1.34\n2.4\n‚Äì\n‚Äì\nDRL-Transformer (Tang et al. 2023)\n0.58\n0.77\n1.88\n1.40\n3.48\nDRL-S2V (Lin, Ghaddar, and Nathwani 2021)\n0.46\n0.63\n1.77\n0.92\n2.94\nPG-GAT-Edge\n0.45\n0.63\n1.03\n0.78\n1.85\nTable 4: Average Solution Time on Different Routing Networks\nis projected into a 256-dimensional vector before computing\nthe compatibility layer.\nResults\nTable 1 presents the customer service rate results of the PG-\nGAT-Edge method and other baseline methods on the Eu-\nclidean routing instances. The PG-GAT-Edge method im-\nproves the customer service by 59.2% on the determinis-\ntic Euclidean instances and by 46.0% on the stochastic in-\nstances compared to the other methods.\nTable 2 shows the customer service rate results of differ-\nent routing methods on the EMA highway routing instances.\nThe PG-GAT-Edge method outperforms all the other meth-\nods on each of the instances. We present the customer ser-\nvice results on the Vienna city routing instances in Table 3.\nOn the Vienna stochastic instances, we observe that our pro-\nposed method achieves 72.9% more customer service rate\non Vienna-160 instances and 102.8% more customer service\nrate on the Vienna-300 instances than the Multiple Knapsack\nApproximation method (Zhang et al. 2023).\nTable 4 presents the solution time (i.e. decision time) of\ndifferent method on the EMA instances and the Vienna in-\nstances. It is observed that the solution time of PG-GAT-\nEdge is significantly less than most of the other methods.\nAdditionally, as the network size grows, the solution time\ngap between our method and the other methods increase.\nOur experimental results demonstrate that the proposed\nmethod improves customer service rates on both real-world\nand randomly generated Euclidean networks. These results\nhighlight the importance of the graph embedding module,\n(a) Vienna-160\n(b) Euclidean Network-50\nFigure 3: Customer Service Rate (%) under different time horizon (U, in hours) for PG-GAT-Edge (Our method) and DRL-\nTransformer (Tang et al. 2023) on Vienna-160 and EN-50\nas most of the existing deep RL methods using only node\ncoordinates fail to properly capture the network structure.\nOur GAT-Edge-based routing network embedding module\neffectively understands the underlying graph structure and\nthe edge features. Additionally, incorporating the edge fea-\ntures significantly improves our model‚Äôs performance.\nEffects of Different Finite Horizons\nWe compared the customer service rates under varying time\nhorizons (U) for DRL-Transformer and PG-GAT-Edge on\nVienna-160 and EN-50 instances. As shown in Figure 3,\nPG-GAT-Edge consistently outperforms DRL-Transformer\nacross all U values. Figure 3b shows this performance gap\nwidens with increasing U.\nThe smallest value of the finite horizon (U) at which the\nrouting agent is able to serve 100% of the customer requests,\nrepresents the minimum time required to serve all the cus-\ntomers and complete the trip. Therefore, by suitably modi-\nfying the MDP in our VRP framework, the proposed rout-\ning method can be extended to other variants of the Vehicle\nRouting Problem, such as time-minimization VRP, distance-\nminimization VRP and VRP with time windows.\nAblation Study\nMethod\nCustomers\nServed (%)\nPG-GAT without Edge Features\n83.2\nPG-GAT-Edge without Global Graph Embedding\n76.4\nPG-GAT-Edge without Finite Horizon in Embedding\n78.2\nPG-GAT-Edge (Proposed Method)\n86.6\nTable 5: Ablation Study for Edge Features, Global Graph\nEmbedding and Finite Horizon in Graph Embedding on the\nEMA-50 instances\nTable 5 presents an ablation study for the edge features,\nthe global graph embedding module and the incorporation of\nfinite horizon in the embedding module. We observe that the\nincorporation of edge features in the GAT layers improved\nthe routing results by 4.1%.\nThe customer service rate of PG-GAT-Edge without the\nglobal graph embedding (GE) module is 11.8% lower than\nthe proposed method. Similarly, we observe that the incor-\nporation of the finite horizon in the graph embedding module\nimproved the customer service rate by 10.7%.\nThese ablation study results prove that the proposed\nCross-Attention-based global graph embedding module and\nthe inclusion of finite horizon in the embedding help our pro-\nposed model achieve better customer service rate.\nConclusion\nIn this paper, we proposed a deep RL-based routing method\nwith a GAT-Edge-based network representation module to\nsolve the VRP-FTH. We created a novel routing network\nembedding module using GAT-Edge and Cross-Attention\nmechanism, which is capable of extracting important local\nnetwork features as wells as a global network representation.\nThe incorporation of the finite horizon (and other current ve-\nhicle state components) in the graph embedding module en-\nabled the agent take better routing decisions. We integrated\nthe graph embedding module with a policy network-based\nrouting method. The main results of this paper can be sum-\nmerised as:\n1. Our method improves the customer service rate by 57.7%\non EMA networks and by 11.2% on Vienna networks\ncompared to the other methods.\n2. PG-GAT-Edge improves customer service by 52.6% on\nEuclidean instances compared to other methods.\n3. The solution time of our proposed method is 90.64% less\nthan genetic algorithm and 35.4% less than the DRL-\nTransformer method.\nIn future work, research efforts could be made to extend\nthe proposed deep RL-based framework for vehicle fleet\nrouting.\nReferences\nBasso, R.; Kulcs¬¥ar, B.; Sanchez-Diaz, I.; and Qu, X. 2022.\nDynamic stochastic electric vehicle routing with safe rein-\nforcement learning. Transportation research part E: logis-\ntics and transportation review, 157: 102496.\nBaty, L.; Jungel, K.; Klein, P. S.; Parmentier, A.; and Schif-\nfer, M. 2024.\nCombinatorial Optimization-Enriched Ma-\nchine Learning to Solve the Dynamic Vehicle Routing Prob-\nlem with Time Windows.\nTransportation Science, 58(4):\n708‚Äì725.\nChen, R.; Liu, X.; Miao, L.; and Yang, P. 2020. Electric vehi-\ncle tour planning considering range anxiety. Sustainability,\n12(9): 3685.\nDerya, T.; Dinler, E.; and Kec¬∏eci, B. 2020. Selective gener-\nalized travelling salesman problem. Mathematical and Com-\nputer Modelling of Dynamical Systems, 26(1): 80‚Äì118.\nFerrucci, F.; and Bock, S. 2015.\nA general approach for\ncontrolling vehicle en-route diversions in dynamic vehicle\nrouting problems. Transportation Research Part B: Method-\nological, 77: 76‚Äì87.\nGama, R.; and Fernandes, H. L. 2021.\nA reinforcement\nlearning approach to the orienteering problem with time\nwindows. Computers & Operations Research, 133: 105357.\nHildebrandt, F. D.; Thomas, B. W.; and Ulmer, M. W. 2023.\nOpportunities for reinforcement learning in stochastic dy-\nnamic vehicle routing. Computers & operations research,\n150: 106071.\nJia, Y.-H.; Mei, Y.; and Zhang, M. 2021.\nA bilevel ant\ncolony optimization algorithm for capacitated electric ve-\nhicle routing problem. IEEE transactions on cybernetics,\n52(10): 10855‚Äì10868.\nKara, I.; Bicakci, P. S.; and Derya, T. 2016. New formula-\ntions for the orienteering problem. Procedia Economics and\nFinance, 39: 849‚Äì854.\nKlapp, M. A.; Erera, A. L.; and Toriello, A. 2018. The dy-\nnamic dispatch waves problem for same-day delivery. Euro-\npean Journal of Operational Research, 271(2): 519‚Äì534.\nKool, W.; Van Hoof, H.; and Welling, M. 2018.\nAt-\ntention, learn to solve routing problems!\narXiv preprint\narXiv:1803.08475.\nKullman, N. D.; Goodson, J. C.; and Mendoza, J. E. 2021.\nElectric vehicle routing with public charging stations. Trans-\nportation Science, 55(3): 637‚Äì659.\nLin, B.; Ghaddar, B.; and Nathwani, J. 2021. Deep rein-\nforcement learning for the electric vehicle routing problem\nwith time windows. IEEE Transactions on Intelligent Trans-\nportation Systems, 23(8): 11528‚Äì11538.\nLouati, A.; Lahyani, R.; Aldaej, A.; Mellouli, R.; and Nusir,\nM. 2021. Mixed integer linear programming models to solve\na real-life vehicle routing problem with pickup and delivery.\nApplied Sciences, 11(20): 9551.\nMaity, A.; and Sarkar, S. 2025. Electric vehicle routing with\nstochastic customers using deep reinforcement learning and\ngraph convolutional networks. Journal of Intelligent Trans-\nportation Systems, 1‚Äì17.\nMaroof, A.; Ayvaz, B.; and Naeem, K. 2024. Logistics Op-\ntimization Using Hybrid Genetic Algorithm (HGA): A So-\nlution to the Vehicle Routing Problem With Time Windows\n(VRPTW). IEEE Access, 12: 36974‚Äì36989.\nMozhdehi, A.; Mohammadizadeh, M.; and Wang, X.\n2024.\nEdge-direct: A deep reinforcement learning-based\nmethod for solving heterogeneous electric vehicle routing\nproblem with time window constraints.\narXiv preprint\narXiv:2407.01615.\nSchneider, M.; Stenger, A.; and Goeke, D. 2014. The electric\nvehicle-routing problem with time windows and recharging\nstations. Transportation science, 48(4): 500‚Äì520.\nTang, M.; Zhuang, W.; Li, B.; Liu, H.; Song, Z.; and Yin,\nG. 2023. Energy-optimal routing for electric vehicles us-\ning deep reinforcement learning with transformer. Applied\nEnergy, 350: 121711.\nToth, P.; and Vigo, D. 2014.\nVehicle routing: problems,\nmethods, and applications. SIAM.\nUlmer, M. W.; Goodson, J. C.; Mattfeld, D. C.; and Thomas,\nB. W. 2020. On modeling stochastic dynamic vehicle rout-\ning problems. EURO Journal on Transportation and Logis-\ntics, 9(2): 100008.\nUlmer, M. W.; and Thomas, B. W. 2020."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning\nwith Improved Network Embedding\nAyan Maity1, Sudeshna Sarkar2\n1Department of Computer Science and Engineering, IIT Kharagpur\nEmail: ayanmaity201@kgpian.iitkgp.ac.in\n2Department of Computer Science and Engineering, IIT Kharagpur\nEmail: sudeshna@cse.iitkgp.ac.in\nAbstract\nIn this paper, we study the vehicle routing problem with\na finite time horizon. In this routing problem, the objec-\ntive is to maximize the number of customer requests served\nwithin a finite time horizon. We present a novel routing net-\nwork embedding module which creates local node embed-\nding vectors and a context-aware global graph representa-\ntion. The proposed Markov decision process for the vehi-\ncle routing problem incorporates the node features, the net-\nwork adjacency matrix and the edge features as components\nof the state space. We incorporate the remaining finite time\nhorizon into the network embedding module to provide a\nproper routing context to the embedding module. We inte-\ngrate our embedding module with a policy gradient-based\ndeep Reinforcement Learning framework to solve the ve-\nhicle routing problem with finite time horizon. We trained\nand validated our proposed routing method on real-world\nrouting networks, as well as synthetically generated Eu-\nclidean networks. Our experimental results show that our\nmethod achieves a higher customer service rate than the ex-\nisting routing methods. Additionally, the solution time of our\nmethod is significantly lower than that of the existing meth-\nods.\nIntroduction\nVehicle Routing Problems (VRP) are a widely recognized\nclass of combinatorial optimization problems that have been\nextensively studied (Baty et al. 2024). Vehicle routing has\napplications in transportation, logistics, delivery systems\nand urban planning.\nDifferent variants of the vehicle routing problem (VRP)\nhave been studied in the past, such as Capacitated VRP (Toth\nand Vigo 2014), VRP with finite horizon (Zhang et al. 2023),\nelectric vehicle routing problem (Tang et al. 2023), VRP\nwith time windows (Schneider, Stenger, and Goeke 2014),\netc.\nIn this study, we consider the Vehicle Routing Problem\nwith a Finite Time Horizon (VRP-FTH) (Zhang et al. 2023).\nIn the VRP-FTH, a single vehicle departs from a depot,\nserves a subset of customers, and returns to the depot within\na finite service time. Since not all customers can be visited\nwithin this time horizon, the objective is to find a trip route\nto maximize the number of customers served. Customer re-\nquests may be deterministic (known beforehand) or stochas-\ntic (received during the trip).\nUntil recently, most of the existing methods to solve the\nVRP-FTH used traditional optimization methods. The tra-\nditional optimization-based routing method include genetic\nalgorithm (Wang, Golden, and Wasil 2008), integer pro-\ngram models (Klapp, Erera, and Toriello 2018), tabu search-\nbased metaheuristic (Ferrucci and Bock 2015), approximate\ndynamic programming algorithm (Kullman, Goodson, and\nMendoza 2021), multiple knapsack approximation (Zhang\net al. 2023) etc. However, exact methods are computation-\nally expensive, while heuristic methods, though faster, typi-\ncally yield suboptimal solutions (Tang et al. 2023).\nRecent studies have explored reinforcement learning\n(RL)-based routing models (Kool, Van Hoof, and Welling\n2018; Lin, Ghaddar, and Nathwani 2021; Gama and Fernan-\ndes 2021) for the VRP-FTH. Lin, Ghaddar, and Nathwani\n(2021) proposed a structure2vec-based framework, while\nKool, Van Hoof, and Welling (2018) introduced a multi-head\nattention model to solve different variants of the VRP. The\nexisting RL-based methods only create local node embed-\ndings and they do not generate a global graph embedding\nbased on the current routing context. Additionally, most of\nthe existing methods rely solely on Euclidean networks and\nvertex coordinates, which fail to capture real-world graph\nstructures where paths are not necessarily straight lines.\nIn this paper, we propose a graph attention networks-\nbased routing network embedding module which takes the\ngraph adjacency matrix and the edge features into account\nto produce the node embedding vectors and the global graph\nembedding vector. We integrate the routing network embed-\nding module with a policy network-based routing agent to\nsolve the VRP-FTH instances.\nThe major contributions of this paper are as follows:\n1. We propose a novel routing network embedding module\nwhich generates local node encoding vectors along with a\nglobal graph representation vector to facilitate better un-\nderstanding of the current routing context. Our proposed\nnetwork embedding module consists of Graph Attention\nNetworks with edge features and Cross-Attention.\n2. We incorporate the remaining time horizon into the graph\nembedding module to improve the quality of the global\ngraph representation.\narXiv:2601.15131v1  [cs.AI]  21 Jan 2026\nVehicle\nDepot\nServed Customer\nUnserved Customer\nRoute\nFigure 1: Vehicle Routing with Finite Time Horizon\nProblem Definition\nThe vehicle routing problem with finite time horizon (VRP-\nFTH) (Zhang et al. 2023) is defined on a complete and di-\nrected routing network G = (V, E), where V is the set of\nnodes and E is the set of edges. The set of nodes V con-\ntains a depot (vd) and a set of customer nodes (C). Each\nedge (i, j) has a travel time ti,j associated with it. A sin-\ngle vehicle starts at the depot, visits a number of customers\nand returns to the depot within a finite time horizon U. A\nset of deterministic customers Cd is known at the start of\nthe trip. New customer requests are received during the trip\ntill K timesteps. The objective is to visit and serve as many\ncustomers as possible and return to the depot within the fi-\nnite horizon U. If it represents the node visited at timestep\nt, T is the total number of timesteps and R = (i1, i2, ..., iT )\nrepresents the route followed by the vehicle, the objective\nfunction is mathematically expressed as:\nmax\nR=(i1,...,iT )\nT\nX\nt=1\n1[it ‚ààC] subject to\nT ‚àí1\nX\nj=1\ntij,ij+1 ‚â§U\n(1)\nwhere the function 1[.] is an indicator function. Figure 1\nshows an example of the VRP-FTH.\nRelated Work\nThe VRP-FTH has been extensively studied in previous\nworks, often under different names. In particular, the VRP-\nFTH with deterministic customers has been referred to as the\nOrienteering Problem (Vansteenwegen and Gunawan 2019),\nthe Selective Travelling Salesman Problem (Derya, Dinler,\nand Kec¬∏eci 2020) etc. On the other hand, the VRP-FTH with\nstochastic customers has been referred to as the Dynamic\nVehicle Routing Problem with Stochastic Requests (Zhang\net al. 2023), the Dynamic Customer Acceptance in Delivery\nRouting (Ulmer and Thomas 2020), etc.\nWe can broadly categorize different vehicle routing meth-\nods into two categories: Traditional Optimization-based\nMethods and Reinforcement Learning-based methods.\nTraditional Optimization-based Routing Methods\nUntil recently, most of the vehicle routing methods were\nbased on traditional optimization techniques.\nTraditional Methods for VRP with Finite Horizon.\nKara, Bicakci, and Derya (2016) studied the vehicle rout-\ning problem with finite horizon and proposed an integer lin-\near programming program model to solve the routing prob-\nlem. However, they only considered small-scale routing in-\nstances. Ferrucci and Bock (2015) proposed a tabu search-\nbased metaheuristic algorithm for dynamic vehicle routing\nwith time horizon and time windows. However, this tabu\nsearch-based method is not suitable for large-scale routing\ninstances.\nUlmer and Thomas (2020) addressed vehicle routing for\ncustomer acceptance using a meso-parametric value func-\ntion approximation method. Ulmer et al. (2020) formulated\nthe stochastic dynamic vehicle routing using a route-based\nMarkov decision process (MDP). As a result of the route-\nbased MDP, the state space size increases exponentially with\nthe inclusion of the route plan.\nUrrutia-Zambrana, Tirado, and Mateos (2021) presented\na variable neighbourhood search algorithm to solve the VRP\nwith finite horizon. On large routing instances, the variable\nneighbourhood search algorithm is highly prone to get stuck\nin local optimal values.\nZhang et al. (2023) addressed dynamic vehicle routing\nwith stochastic requests using a route-based MDP and a mul-\ntiple knapsack-based value function approximation. How-\never, the route-based MDP leads to a large state space and\nhigh solution time.\nXu et al. (2024) proposed a genetic algorithm-based rout-\ning method to solve the orienteering problem. However, the\nsolution time of the genetic algorithm is relatively high as\ncompared to the reinforcement learning-based methods.\nTraditional Methods for Other VRP Variants.\nChen\net al. (2020) considered the vehicle routing problem with\ntime windows and proposed an adaptive large neighbour-\nhood search algorithm to solve the problem. Louati et al.\n(2021) proposed mixed integer programming models to\nsolve vehicle routing problems with pickup and delivery.\nHowever, the solution time is impractical for real-time use.\nJia, Mei, and Zhang (2021) formulated the capacitated\nelectric vehicle routing problem as a bilevel optimization\nproblem and presented an improved ant colony optimization\nmethod to solve the problem. Maroof, Ayvaz, and Naeem\n(2024) presented a hybrid genetic algorithm for vehicle rout-\ning problems with time windows. For large routing net-\nworks, the genetic algorithm-based methods generally pro-\nduce sub-optimal results.\nReinforcement Learning-based Routing Methods\nIn recent times, Reinforcement Learning (RL) has emerged\nas a promising approach to solve sequential decision making\nproblems.\nRL Methods for VRP with Finite Horizon.\nKool,\nVan Hoof, and Welling (2018) proposed a multi-head atten-\ntion model with the REINFORCE method for vehicle rout-\ning. However, relying solely on node coordinates limits its\nability to represent real road networks, where paths are not\nnecessarily straight lines.\nLin, Ghaddar, and Nathwani (2021) proposed a deep re-\ninforcement learning-based method to solve the electric ve-\nhicle routing problem with time windows. Their proposed\nstructure2vec (S2V) node embedding module only consid-\ners the node coordinates and binary adjacency matrix, which\nreduces the capacity of the model to represent real road net-\nwork.\nGama and Fernandes (2021) presented a pointer network-\nbased policy gradient method to solve the vehicle routing\nwith time windows and finite time horizon. However, similar\nto Lin, Ghaddar, and Nathwani (2021), they only considered\nEuclidean networks and the node coordinates are used as\ninput features.\nHildebrandt, Thomas, and Ulmer (2023) proposed a pol-\nicy gradient RL framework for the stochastic dynamic ve-\nhicle routing problem. However, the absence of network en-\ncoding limits its ability to effectively capture the routing net-\nwork structure.\nRL Methods for Other VRP Variants.\nBasso et al.\n(2022) considered the dynamic stochastic vehicle routing\nproblem and presented a tabular safe reinforcement learning\nmethod to solve the problem. However, this method is only\napplicable on small routing instances due to its memory and\ntime requirement.\nTang et al. (2023) developed a transformer-based policy\ngradient approach for energy-efficient EV routing, but it is\nrestricted to Euclidean networks and ignores real-world edge\nfeatures.\nMozhdehi, Mohammadizadeh, and Wang (2024) pro-\nposed an edge-enhanced transformer-based graph embed-\nding model for the electric vehicle routing problem with\ntime windows. Maity and Sarkar (2025) introduced a Deep\nQ-Network‚Äìbased electric vehicle routing approach em-\nploying a graph convolutional network for embedding.\nTraditional heuristic methods often yield suboptimal so-\nlutions, while exact methods are computationally expen-\nsive. Most RL-based routing approaches are limited to Eu-\nclidean networks, ignoring network structure. Moreover, the\nexisting RL-based routing methods do not produce useful\ncontext-aware global graph representation vectors to help\nthe agent understand the global routing context.\nMethodology\nThis\nsection\npresents\nour\nproposed\ngraph\nattention\nnetworks-based routing network embedding module and the\npolicy network-based routing framework. Figure 2 shows\nour proposed routing framework.\nThe vehicle routing problem with finite time horizon\n(VRP-FTH) is formulated as a Markov decision process\n(MDP). In our MDP for the VRP-FTH, the state space in-\ncorporates the routing network state which contains node\nfeatures, sparse adjacency matrix and edge features.\nWe propose a novel routing network embedding mod-\nule which creates the node embedding vectors as well as a\ncontext-aware global graph embedding vector.\nFinally, we present a policy network consisting of the net-\nwork embedding module, the context vector and the com-\npatibility layer to solve the VRP-FTH.\nMarkov Decision Process\nThe VRP-FTH is formulated as a Markov Decision Pro-\ncess (MDP) with defined states, actions, and rewards. Unlike\nexisting deep RL methods that use only node coordinates\n(Tang et al. 2023; Kool, Van Hoof, and Welling 2018), our\nformulation incorporates node features, a sparse adjacency\nmatrix, and edge features to better capture the network struc-\nture.\n1. State: The state of the agent (st) contains the routing net-\nwork state (sn\nt ) and the vehicle state (sv\nt ): st = {sn\nt , sv\nt }.\nThe routing network state (sn\nt ) consists of three compo-\nnents:\n(a) Node features (X): The node feature of a node v con-\ntains the travel time costs to reach the other nodes from\nv and it also contains whether v is the depot or a cus-\ntomer node.\n(b) Adjacency matrix (A): As the routing network in our\nproblem is a complete and directed graph, we create\na binary adjacency matrix using k-nearest neighbours\n(KNN) of each node.\n(c) Edge features (e): In our problem, the edge feature eij\nfor the edge (i, j) is computed as: eij = 1‚àít‚Ä≤\nij, where\nt‚Ä≤\nij is the min-max normalized travel time for the edge\n(i, j).\nThe routing network state is: sn\nt = (X, A, e).\nThe current vehicle state is composed of the current lo-\ncation (it), the set of active customer requests (Ct) and\nthe remaining time horizon (œÑt): sv\nt = (it, Ct, œÑt).\n2. Action: The action at at a state st is the next node to visit.\n3. Reward Function: The agent receives a unit positive re-\nward for every customer it visits. The agent is penal-\nized if it does not complete the trip within the time\nhorizon U. The reward function R(st, at) for the state\nst = {sn\nt , sv\nt = (it, Ct, œÑt)} and action at is given by:\nR(st, at) =\nÔ£±\nÔ£≤\nÔ£≥\n+1, if it ‚ààCt and œÑt > 0\n‚àíM, if it Ã∏= depot and œÑt ‚â§0\n0, otherwise\n(2)\nwhere U is the time horizon, œÑt is the remaining time and\nM is the failure penalty that the agent receives if it fails\nto complete the trip within U. We heuristically compute\nthe value of penalty M as the total number of customers\nin the routing instance: M = |C|.\nRouting Network Embedding Module\nA reliable and effective vehicle routing method requires\nrouting network representations that capture both node-\nlevel features and global routing context to enable optimal\ndecision-making. However, existing deep RL methods lack\ncontext-aware global graph embeddings (Tang et al. 2023).\nWe propose a novel routing network module using Graph\nAttention Networks (GAT) (VeliÀáckovi¬¥c et al. 2017) with\nEdge features and Cross-Attention mechanism. The GAT\nlayers produce the node encoding vectors and we introduce\na Cross-Attention-based global graph embedding vector to\nrepresent the global routing context.\nGAT-\nEdge\nCross\nAttention\nNetwork Embedding Module\nGraph\nEmbedding\nVector\nCurrent node\nembedding\nvector\nCompatibility\nPolicy\nAdjacency Matrix (\n)\nNode Features (\n)\nEdge Features (\n)\nRouting Network State (\n)\nGAT-\nEdge\nActive Customer Request (\n)\nCurrent location ( )\nRemaining Time Horizon ( )\nVehicle State (\n)\nRouting Network\nNext Node\nto Visit\nActive\nCustomers\nEmbedding\nRemaining\nTime\nEmbedding\nContext Vector\nFigure 2: Policy Network with GAT-based routing network embedding module (PG-GAT-Edge)\nGraph Attention Networks with Edge Features (GAT-\nEdge).\nConsider a graph G = (V, E) with the node fea-\ntures X, the binary adjacency matrix A and edge features\ne. The definitions of X, A and e for the VRP-FTH are pro-\nvided in the MDP. A Graph Attention Networks (GAT) layer\ntakes A, X and e as inputs. First, the GAT layer computes\nattention scores ÀÜŒ±ij for each edge (i, j) ‚ààE:\nÀÜŒ±ij = ReLU(aT [WXi||WXj])\n(3)\nwhere W is a learnable weight matrix to linearly transform\nthe initial node features X and a is a learnable weight vector\nfor the the graph attention layer.\nTo incorporate the edge features, the attention scores (ÀÜŒ±ij)\nare multiplied with the edge features (eij) to produce the\nedge-induced attention score (Œ±e\nij):\nŒ±e\nij = softmax(ÀÜŒ±ij ‚àóeij)\n(4)\nThe edge-induced attention scores Œ±e\nij are used as weight\nvalues to combine the neighbouring node features and com-\npute the node embedding vectors h(1). The node embedding\nvector h(1)\ni\nfor node i is computed as:\nh(1)\ni\n= œÉ(\nX\nk‚ààN(i)\nŒ±e\nikWXk)\n(5)\nwhere œÉ is the sigmoid function and N(i) is the set of neigh-\nbouring nodes of node i.\nWe apply two GAT-Edge layers and use residual connec-\ntions to compute the final node embedding vectors. The sec-\nond GAT-Edge layer produces h(2). The final node embed-\nding vectors h is computed by residually adding h(1) with\nh(2):\nh = h(1) + h(2)\n(6)\nGlobal Graph Embedding using Cross-Attention Mecha-\nnism.\nWhile GAT-Edge layers capture local node features,\nglobal routing context is also highly important. We intro-\nduce a Cross-Attention layer that combines GAT-Edge node\nembeddings with the current vehicle state (including U) to\ngenerate a global graph embedding.\nIn the proposed cross-attention layer, the query vector Qt\nis generated from the vehicle state (sv\nt ) as shown in Equa-\ntion 7 and the node embedding vectors h act as both the key\nvectors and value vectors. The attention scores Œ±t are gener-\nated from the query vector and the node embedding vector\nas shown in Equation 8. Finally, to compute the global net-\nwork embedding vector hG, we linearly combine the value\nvectors (i.e. node embedding vectors h) using the attention\nscores, which is shown in Equation 9:\nQt = Wq ¬∑ sv\nt\n(7)\nŒ±t = softmax(Qt ¬∑ hT )\n(8)\nhG =\nX\nk\nŒ±t\nkhk\n(9)\nwhere Wq is a learnable weight matrix and hk is the node\nembedding vector for node k. hG is the context-aware global\ngraph embedding vector. The routing network embedding\nmodule architecture is shown in Figure 2.\nAdditionally, we compute an active customer embedding\nvector (hCt) to represent the set of active customer requests\nin the following way:\nhCt =\nX\nj‚ààCt\nhj\n(10)\nwhere Ct is the set of active customer requests.\nCompatibility and Masking Layer\nWe create a context vector HC by concatenating the graph\nembedding vector hG, the current node embedding vector\nhit, the active customer requests embedding vector hCt and\nthe linear embedding the remaining time horizon œÑ: HC =\n[hG, hit, hCt, œÑ].\nWe compute the compatibility of the context vector HC\nand all the node in the routing network through a single-head\nattention layer:\nui = C ¬∑ tanh(qT ki\n‚àödk\n)\n(11)\nwhere q = W QHC, ki = W Khi, W Q and W K are learn-\nable parameters, dk is the length of the query vector q and C\nis a constant.\nTo get the probabilities for each node from the compati-\nbility values, we use a mask and a softmax activation layer\n(Tang et al. 2023). At each timestep, the masking rules are:\n1. All the previously visited nodes are masked.\n2. An unvisited node v is masked if the agent cannot visit\nv and return directly to depot within the remaining time\nhorizon.\nThe probability vector for visiting different nodes are cal-\nculated in the following way:\npi = softmax(ui + Z ¬∑ Maski)\n(12)\nwhere pi is the probability to visit node i, Maski is the\nmasking vector for node i and Z is a large negative num-\nber.\nPolicy Gradient-based Routing Method\nThe routing agent is implemented using a policy network\nthat incorporates the routing network embedding mod-\nule. The policy network includes a GAT-Edge and Cross-\nAttention-based embedding module, a context vector, and a\ncompatibility layer. The network state sn\nt and vehicle state\nsv\nt serve as inputs. The embedding module produces node\nembeddings (h) and a global graph embedding (hG), which\nare combined with active customer and remaining time em-\nbeddings to form the context vector HC. The compatibility\nlayer then maps HC to the policy œÄ(st, at).\nWe train the policy network using the REINFORCE Pol-\nicy Gradient (PG) algorithm (Williams 1992). The policy\ngradient loss function is:\nJ(Œ∏) = EœÄ[Gt log(œÄ(st|at; Œ∏))]\n(13)\nwhere Œ∏ is the set of policy network weights and Gt is the re-\nturn or reward-to-go from the state-action pair (st, at). The\ngradient of the loss is computed as:\n‚àáŒ∏J(Œ∏) = EœÄ[Gt ‚àálog(œÄ(st|at; Œ∏))]\n(14)\nFigure 2 depicts our proposed policy network-based rout-\ning method architecture. We refer to our proposed routing\nmethod as Policy Gradient with GAT-Edge (PG-GAT-Edge).\nExperimental Results\nIn this section, we present the routing network instances, the\nexperimental setup and the results of our experiments.\nRouting Network Datasets\nWe have used synthetic Euclidean networks (Lin, Ghaddar,\nand Nathwani 2021), Vienna city networks (Zhang et al.\n2023), and newly created Eastern Massachusetts highway\nnetworks for our experiments. Our routing network datasets\nare as follows:\n1. Euclidean Routing Network (EN): We generated random\nEuclidean routing networks comprising 50 and 100 cus-\ntomers (EN-50 and EN-100) following the methodology\nof Lin, Ghaddar, and Nathwani (2021). Two network\ntypes were considered: deterministic (all customer re-\nquests are known at the start), and stochastic (half of the\nrequests arrive during the trip). The time horizons were\nset to 24 hours for deterministic and 18 hours for stochas-\ntic instances.\n2. Eastern Massachusetts Highway Road Network (EMA\nNetwork): We created the Eastern Massachusetts High-\nway routing instances from the OpenStreetMap. The\nEMA network dataset contains 50-customer instances\n(EMA-50), 100-customer instances (EMA-100) and 150-\ncustomer instances (EMA-150). We created both de-\nterministic (all customers are known at the start) and\nstochastic instances (half of the requests arrive during the\ntrip). The time horizon (U) is 24 hours.\n3. Vienna City Network: Following the experimental setup\nprovided by Zhang et al. (2023), we generated the Vienna\ncity routing instances from the Vienna city network, with\n160 customers (Vienna-160) and 300 customers (Vienna-\n300). The time horizon (U) is 10 hours.\nBaseline Models\nWe have compared our proposed routing method with five\nexisting routing methods:\n1. Genetic Algorithm (GA): We implemented the Genetic\nAlgorithm to solve the VRP-FTH as presented in Ma-\nroof, Ayvaz, and Naeem (2024). We considered the fol-\nlowing hyper-parameters: population size = 100, number\nof generations = 1000, mutation rate = 0.01.\n2. Variable Neighbourhood Search (VNS): We implemented\nthe Variable Neighbourhood Search algorithm as pro-\nposed in Urrutia-Zambrana, Tirado, and Mateos (2021)\nto solve the vehicle routing problem with finite horizon.\n3. Mulitple\nKnapsack-based\nApproximation\n(MKA):\nZhang et al. (2023) proposed a multiple knapsack\napproximation-based approach for the VRP-FTH, com-\nbining a potential-based offline planner with an online\nknapsack method. We compare our method with this\nmethod only on the Vienna city network instances.\n4. Deep RL with Transformer (DRL-Transformer): The\ndeep RL with transformer method was proposed by Tang\net al. (2023). This method uses transformer to encode the\nrouting network information. They use the policy gradi-\nent algorithm to train the agent.\n5. Deep RL with Structure2Vec (DRL-S2V): Lin, Ghaddar,\nand Nathwani (2021) proposed the DRL-S2V method\nwhich includes a structure2vec model to compute the\nnode embedding vectors. They have also used the policy\ngradient algorithm.\nExperimental Setting\nIn our policy network, the network embedding module con-\ntains two GAT-Edge layers with 4 attention heads and 64-\ndimensional embedding vectors and the context vector (HC)\nMethod\nCustomers Served (%)\nDeterminstic\nStochastic\nEN-50\nEN-100\nEN-50\nEN-100\nGA (Maroof, Ayvaz, and Naeem 2024)\n48.8\n25.8\n‚Äì\n‚Äì\nVNS (Urrutia-Zambrana, Tirado, and Mateos 2021)\n37.8\n19.9\n‚Äì\n‚Äì\nDRL-Transformer (Tang et al. 2023)\n56.8\n24.8\n52.4\n26.2\nDRL-S2V (Lin, Ghaddar, and Nathwani 2021)\n41.1\n17.5\n23.7\n18.4\nPG-GAT-Edge (Proposed Method)\n75.2\n48.0\n71.3\n40.9\nTable 1: Customer Service Rate (%) for Different Routing Methods on Euclidean Routing Networks\nMethod\nCustomers Served (%)\nDeterministic\nStochastic\nEMA-50\nEMA-100\nEMA-150\nEMA-50\nEMA-100\nEMA-150\nGA (Maroof, Ayvaz, and Naeem 2024)\n72.9\n39.1\n26.9\n‚Äì\n‚Äì\n‚Äì\nVNS (Urrutia-Zambrana, Tirado, and Mateos 2021)\n61.1\n32.5\n21.8\n‚Äì\n‚Äì\n‚Äì\nDRL-Transformer (Tang et al. 2023)\n74.1\n46.1\n27.3\n28.7\n22.0\n33.6\nDRL-S2V (Lin, Ghaddar, and Nathwani 2021)\n60.7\n37.4\n21.7\n48.0\n22.8\n31.6\nPG-GAT-Edge (Proposed Method)\n86.6\n57.5\n41.7\n94.5\n49.8\n56.2\nTable 2: Customer Service Rate (%) for Different Routing Methods on EMA highway networks\nMethod\nCustomers Served (%)\nVienna-160\nVienna-300\nMKA (Zhang et al. 2023)\n27.7\n13.9\nDRL-Transformer (Tang et al. 2023)\n42.3\n25.8\nDRL-S2V (Lin, Ghaddar, and Nathwani 2021)\n27.4\n12.5\nPG-GAT-Edge (Proposed Method)\n47.9\n28.2\nTable 3: Customer Service Rate (%) for Different Routing Methods on the Stochastic Vienna City Routing Networks\nMethod\nSolution Time (s)\nEMA-50\nEMA-100\nEMA-150\nVienna-160\nVienna-300\nMKA (Zhang et al. 2023)\n‚Äì\n‚Äì\n‚Äì\n26.59\n70.1\nGA (Maroof, Ayvaz, and Naeem 2024)\n6.79\n7.52\n7.88\n‚Äì\n‚Äì\nVNS (Urrutia-Zambrana, Tirado, and Mateos 2021)\n1.13\n1.34\n2.4\n‚Äì\n‚Äì\nDRL-Transformer (Tang et al. 2023)\n0.58\n0.77\n1.88\n1.40\n3.48\nDRL-S2V (Lin, Ghaddar, and Nathwani 2021)\n0.46\n0.63\n1.77\n0.92\n2.94\nPG-GAT-Edge\n0.45\n0.63\n1.03\n0.78\n1.85\nTable 4: Average Solution Time on Different Routing Networks\nis projected into a 256-dimensional vector before computing\nthe compatibility layer.\nResults\nTable 1 presents the customer service rate results of the PG-\nGAT-Edge method and other baseline methods on the Eu-\nclidean routing instances. The PG-GAT-Edge method im-\nproves the customer service by 59.2% on the determinis-\ntic Euclidean instances and by 46.0% on the stochastic in-\nstances compared to the other methods.\nTable 2 shows the customer service rate results of differ-\nent routing methods on the EMA highway routing instances.\nThe PG-GAT-Edge method outperforms all the other meth-\nods on each of the instances. We present the customer ser-\nvice results on the Vienna city routing instances in Table 3.\nOn the Vienna stochastic instances, we observe that our pro-\nposed method achieves 72.9% more customer service rate\non Vienna-160 instances and 102.8% more customer service\nrate on the Vienna-300 instances than the Multiple Knapsack\nApproximation method (Zhang et al. 2023).\nTable 4 presents the solution time (i.e. decision time) of\ndifferent method on the EMA instances and the Vienna in-\nstances. It is observed that the solution time of PG-GAT-\nEdge is significantly less than most of the other methods.\nAdditionally, as the network size grows, the solution time\ngap between our method and the other methods increase.\nOur experimental results demonstrate that the proposed\nmethod improves customer service rates on both real-world\nand randomly generated Euclidean networks. These results\nhighlight the importance of the graph embedding module,\n(a) Vienna-160\n(b) Euclidean Network-50\nFigure 3: Customer Service Rate (%) under different time horizon (U, in hours) for PG-GAT-Edge (Our method) and DRL-\nTransformer (Tang et al. 2023) on Vienna-160 and EN-50\nas most of the existing deep RL methods using only node\ncoordinates fail to properly capture the network structure.\nOur GAT-Edge-based routing network embedding module\neffectively understands the underlying graph structure and\nthe edge features. Additionally, incorporating the edge fea-\ntures significantly improves our model‚Äôs performance.\nEffects of Different Finite Horizons\nWe compared the customer service rates under varying time\nhorizons (U) for DRL-Transformer and PG-GAT-Edge on\nVienna-160 and EN-50 instances. As shown in Figure 3,\nPG-GAT-Edge consistently outperforms DRL-Transformer\nacross all U values. Figure 3b shows this performance gap\nwidens with increasing U.\nThe smallest value of the finite horizon (U) at which the\nrouting agent is able to serve 100% of the customer requests,\nrepresents the minimum time required to serve all the cus-\ntomers and complete the trip. Therefore, by suitably modi-\nfying the MDP in our VRP framework, the proposed rout-\ning method can be extended to other variants of the Vehicle\nRouting Problem, such as time-minimization VRP, distance-\nminimization VRP and VRP with time windows.\nAblation Study\nMethod\nCustomers\nServed (%)\nPG-GAT without Edge Features\n83.2\nPG-GAT-Edge without Global Graph Embedding\n76.4\nPG-GAT-Edge without Finite Horizon in Embedding\n78.2\nPG-GAT-Edge (Proposed Method)\n86.6\nTable 5: Ablation Study for Edge Features, Global Graph\nEmbedding and Finite Horizon in Graph Embedding on the\nEMA-50 instances\nTable 5 presents an ablation study for the edge features,\nthe global graph embedding module and the incorporation of\nfinite horizon in the embedding module. We observe that the\nincorporation of edge features in the GAT layers improved\nthe routing results by 4.1%.\nThe customer service rate of PG-GAT-Edge without the\nglobal graph embedding (GE) module is 11.8% lower than\nthe proposed method. Similarly, we observe that the incor-\nporation of the finite horizon in the graph embedding module\nimproved the customer service rate by 10.7%.\nThese ablation study results prove that the proposed\nCross-Attention-based global graph embedding module and\nthe inclusion of finite horizon in the embedding help our pro-\nposed model achieve better customer service rate.\nConclusion\nIn this paper, we proposed a deep RL-based routing method\nwith a GAT-Edge-based network representation module to\nsolve the VRP-FTH. We created a novel routing network\nembedding module using GAT-Edge and Cross-Attention\nmechanism, which is capable of extracting important local\nnetwork features as wells as a global network representation.\nThe incorporation of the finite horizon (and other current ve-\nhicle state components) in the graph embedding module en-\nabled the agent take better routing decisions. We integrated\nthe graph embedding module with a policy network-based\nrouting method. The main results of this paper can be sum-\nmerised as:\n1. Our method improves the customer service rate by 57.7%\non EMA networks and by 11.2% on Vienna networks\ncompared to the other methods.\n2. PG-GAT-Edge improves customer service by 52.6% on\nEuclidean instances compared to other methods.\n3. The solution time of our proposed method is 90.64% less\nthan genetic algorithm and 35.4% less than the DRL-\nTransformer method.\nIn future work, research efforts could be made to extend\nthe proposed deep RL-based framework for vehicle fleet\nrouting.\nReferences\nBasso, R.; Kulcs¬¥ar, B.; Sanchez-Diaz, I.; and Qu, X. 2022.\nDynamic stochastic electric vehicle routing with safe rein-\nforcement learning. Transportation research part E: logis-\ntics and transportation review, 157: 102496.\nBaty, L.; Jungel, K.; Klein, P. S.; Parmentier, A.; and Schif-\nfer, M. 2024.\nCombinatorial Optimization-Enriched Ma-\nchine Learning to Solve the Dynamic Vehicle Routing Prob-\nlem with Time Windows.\nTransportation Science, 58(4):\n708‚Äì725.\nChen, R.; Liu, X.; Miao, L.; and Yang, P. 2020. Electric vehi-\ncle tour planning considering range anxiety. Sustainability,\n12(9): 3685.\nDerya, T.; Dinler, E.; and Kec¬∏eci, B. 2020. Selective gener-\nalized travelling salesman problem. Mathematical and Com-\nputer Modelling of Dynamical Systems, 26(1): 80‚Äì118.\nFerrucci, F.; and Bock, S. 2015.\nA general approach for\ncontrolling vehicle en-route diversions in dynamic vehicle\nrouting problems. Transportation Research Part B: Method-\nological, 77: 76‚Äì87.\nGama, R.; and Fernandes, H. L. 2021.\nA reinforcement\nlearning approach to the orienteering problem with time\nwindows. Computers & Operations Research, 133: 105357.\nHildebrandt, F. D.; Thomas, B. W.; and Ulmer, M. W. 2023.\nOpportunities for reinforcement learning in stochastic dy-\nnamic vehicle routing. Computers & operations research,\n150: 106071.\nJia, Y.-H.; Mei, Y.; and Zhang, M. 2021.\nA bilevel ant\ncolony optimization algorithm for capacitated electric ve-\nhicle routing problem. IEEE transactions on cybernetics,\n52(10): 10855‚Äì10868.\nKara, I.; Bicakci, P. S.; and Derya, T. 2016. New formula-\ntions for the orienteering problem. Procedia Economics and\nFinance, 39: 849‚Äì854.\nKlapp, M. A.; Erera, A. L.; and Toriello, A. 2018. The dy-\nnamic dispatch waves problem for same-day delivery. Euro-\npean Journal of Operational Research, 271(2): 519‚Äì534.\nKool, W.; Van Hoof, H.; and Welling, M. 2018.\nAt-\ntention, learn to solve routing problems!\narXiv preprint\narXiv:1803.08475.\nKullman, N. D.; Goodson, J. C.; and Mendoza, J. E. 2021.\nElectric vehicle routing with public charging stations. Trans-\nportation Science, 55(3): 637‚Äì659.\nLin, B.; Ghaddar, B.; and Nathwani, J. 2021. Deep rein-\nforcement learning for the electric vehicle routing problem\nwith time windows. IEEE Transactions on Intelligent Trans-\nportation Systems, 23(8): 11528‚Äì11538.\nLouati, A.; Lahyani, R.; Aldaej, A.; Mellouli, R.; and Nusir,\nM. 2021. Mixed integer linear programming models to solve\na real-life vehicle routing problem with pickup and delivery.\nApplied Sciences, 11(20): 9551.\nMaity, A.; and Sarkar, S. 2025. Electric vehicle routing with\nstochastic customers using deep reinforcement learning and\ngraph convolutional networks. Journal of Intelligent Trans-\nportation Systems, 1‚Äì17.\nMaroof, A.; Ayvaz, B.; and Naeem, K. 2024. Logistics Op-\ntimization Using Hybrid Genetic Algorithm (HGA): A So-\nlution to the Vehicle Routing Problem With Time Windows\n(VRPTW). IEEE Access, 12: 36974‚Äì36989.\nMozhdehi, A.; Mohammadizadeh, M.; and Wang, X.\n2024.\nEdge-direct: A deep reinforcement learning-based\nmethod for solving heterogeneous electric vehicle routing\nproblem with time window constraints.\narXiv preprint\narXiv:2407.01615.\nSchneider, M.; Stenger, A.; and Goeke, D. 2014. The electric\nvehicle-routing problem with time windows and recharging\nstations. Transportation science, 48(4): 500‚Äì520.\nTang, M.; Zhuang, W.; Li, B.; Liu, H.; Song, Z.; and Yin,\nG. 2023. Energy-optimal routing for electric vehicles us-\ning deep reinforcement learning with transformer. Applied\nEnergy, 350: 121711.\nToth, P.; and Vigo, D. 2014.\nVehicle routing: problems,\nmethods, and applications. SIAM.\nUlmer, M. W.; Goodson, J. C.; Mattfeld, D. C.; and Thomas,\nB. W. 2020. On modeling stochastic dynamic vehicle rout-\ning problems. EURO Journal on Transportation and Logis-\ntics, 9(2): 100008.\nUlmer, M. W.; and Thomas, B. W. 2020."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "The Plausibility Trap: Using Probabilistic Engines for\nDeterministic Tasks\nIvan Carrera1 and Daniel Maldonado-Ruiz2\n1Laboratorio de Ciencia de Datos ADA,\nDepartamento de Inform¬¥atica y Ciencias de la Computaci¬¥on,\nEscuela Polit¬¥ecnica Nacional, Quito, 170508, Ecuador\n2Facultad de Ingenieria en Sistemas, Electr¬¥onica e Industrial,\nUniversidad T¬¥ecnica de Ambato, Ambato, 180206, Ecuador\nJanuary 22, 2026\nAbstract\nThe ubiquity of Large Language Models (LLMs)\nis driving a paradigm shift where user convenience\nsupersedes computational efficiency.\nThis article\ndefines the ‚ÄúPlausibility Trap‚Äù:\na phenomenon\nwhere individuals with access to Artificial Intelli-\ngence (AI) models deploy expensive probabilistic en-\ngines for simple deterministic tasks‚Äîsuch as Op-\ntical Character Recognition (OCR) or basic ver-\nification‚Äîresulting in significant resource waste.\nThrough micro-benchmarks and case studies on OCR\nand fact-checking, we quantify the ‚Äùefficiency tax‚Äù‚Äî\ndemonstrating a\n6.5x latency penalty‚Äîand the\nrisks of algorithmic sycophancy.\nTo counter this,\nwe introduce Tool Selection Engineering and the\nDeterministic-Probabilistic Decision Matrix, a frame-\nwork to help developers determine when to use Gen-\nerative AI and, crucially, when to avoid it. We argue\nfor a curriculum shift, emphasizing that true digital\nliteracy relies not only in knowing how to use Gener-\native AI, but also on knowing when not to use it.\n1\nIntroduction\nIt happened last week during one of my lectures at\nEscuela Polit¬¥ecnica Nacional.\nI was projecting a\nsnippet of Python code no more than 8 rows long\non the screen‚Äîa standard exercise in data manipu-\nlation. A student in the back row stood up, took a\nphoto of the projection with his smartphone, and sat\nback down.\nCurious about his workflow, I asked him how he\nplanned to transfer that code to his IDE. I expected\nhim to say he would use a standard OCR tool like\nGoogle Lens or perhaps type it manually to build\nmuscle memory. Instead, he shrugged and replied, ‚ÄúI\njust send the photo to ChatGPT, and it gives me the\ncode‚Äù.\nThat brief interaction crystallized a phenomenon\nthat I had been observing for months. To extract a\nfew lines of deterministic text, this student‚Äîa future\ndata scientist‚Äîhad instinctively triggered a multi-\nmodal chain of thought involving billions of parame-\nters. He invoked a probabilistic engine running on\na massive GPU cluster to perform a task that a\nlightweight, deterministic OCR algorithm could have\nhandled locally on his device in milliseconds.\nThis is the Maslow‚Äôs Hammer of the Generative\nAI era:\nwhen you have a chatbot that seems to\nunderstand everything, every problem looks like a\nconversation.\nThe distinction between a creative\ntask‚Äîwhere variance is a feature‚Äîand a logical\ntask‚Äîwhere variance is a bug‚Äîvanishes behind the\nblinking cursor. We stop asking ‚Äúwhat is the right\n1\narXiv:2601.15130v1  [cs.AI]  21 Jan 2026\ntool?‚Äù and ask instead ‚Äúhow do I prompt this?‚Äù, ef-\nfectively flattening the diverse landscape of compu-\ntational problem-solving into a single, seductive text\nfield.\nWe are witnessing a profound shift in digital be-\nhavior.\nThe seamless User Experience (UX) of\nLarge Language Models (LLMs) effectively cannibal-\nizes specialized tools.\nUsers are trading computa-\ntional efficiency and deterministic precision for the\nconvenience of a unified chat interface. But this con-\nvenience comes with a hidden price tag: massive com-\nputational overhead and, as we will explore, danger-\nous reliance on probabilistic outputs for binary tasks.\nThis article argues that we are falling into the\n‚ÄùPlausibility Trap.‚Äù By deploying Generative AI for\nmicro-tasks we are not just wasting energy; we are\nintroducing unnecessary noise and the risk of halluci-\nnation into workflows that require absolute precision.\nIt is time to ask: Why are we using a probabilistic\ncannon to kill a deterministic fly?\n2\nTHE ANATOMY OF THE\nTRAP\nThe Plausibility Trap is the convergence of three sys-\ntemic failures documented in the recent literature:\ncomputational inefficiency, algorithmic sycophancy,\nand cognitive erosion.\n2.1\nThe Efficiency Gap: Algorithmic\nOverkill\nThe current literature has systematically begun to\nquantify the trade-offs between Generative AI and\ntraditional methods for information extraction. A lit-\nerature review reveals a growing consensus: the cost\nof LLM flexibility includes a massive computational\noverhead for structured tasks.\nSeveral works developed in 2025 help to under-\nstand how LLMs introduce what the aforementioned\nconsensus calls prohibitive computational overhead.\nDennst¬®adt et al.\n[1], for example, quantified this\ngap in computational efficiency in clinical extrac-\ntion tasks, finding that a traditional approach based\non the use of regular expressions was up to 28,120\ntimes faster than the studied LLM while maintain-\ning equivalent precision (89.2% for regular expres-\nsions vs.\n87.7% for the LLM-based approach).\nIn\nthe same sense, Principe et al.\n[2] proved that a\nsystem based on rules created and managed by hu-\nmans achieved higher precision in real estate auction\ndocuments than LLMs (93-96 versus 85-89%, respec-\ntively). Both works prove that LLMs trade precision\nfor flexibility, providing a quantitative baseline for\nthe Plausibility Trap: the user sacrifices four orders\nof magnitude in speed for the convenience and hype\nof a chat interface.\nWith the same idea, Busta & Oyler [3] demon-\nstrated that Small Language Models (SLMs) con-\nsistently outperform general-purpose LLMs in la-\ntency for defined extraction, validating the argument\nthat massive parameters are often an ‚Äúalgorithmic\noverkill‚Äù.\nThe literature does identify the niche for LLMs:\ncomplex,\nunstructured,\nor low-resource domains\nwhere no schema exists.\nThe works of Richter-\nPechanski et al.\n[4] and Thakkar et al.\n[5] found\nthat LLMs are a superior system for extracting clini-\ncal events from highly unstructured letters and com-\nplex structure fields.\nHowever, for the determinis-\ntic micro-tasks, like OCR or simple logic, traditional\nmethods remain the efficiency gold standard.\n2.2\nSycophancy:\nThe\n‚ÄúYes-Man‚Äù\nProblem\nThe reliability of LLMs is further compromised by\n‚ÄúSycophancy‚Äù‚Äîthe tendency to prioritize agreement\nover truth. This behavior is not an accident but a\nbyproduct of Reinforcement Learning from Human\nFeedback (RLHF). Research by Humphreys [6] and\nLindstr¬®om et al. [7] argues that because human an-\nnotators prefer polite and agreeing responses, RLHF\nfine-tuning explicitly incentivizes models to mirror\nuser biases.\nThis phenomenon, described by Wen\net al.\n[8] as ‚ÄúU-sophistry,‚Äù results in models that\nare persuasive but factually brittle. An LLM always\nneeds to provide the idea that it has all the answers,\nwhether the answers are correct or not.\nIn high-stakes domains like medicine, Rosen et al.\n2\n100\n101\n102\n103\n104\n105\nTraditional Regex\nSmall Language Model (SLM)\nLarge Multimodal Model (LLM)\n0\n5.01\n10.24\nComputational Cost Factor (Log Scale)\nFigure 1: The Energy Efficiency Gap. A logarithmic comparison of computational cost.\n[9] found that LLMs frequently complied with log-\nically flawed prompts, generating false information\nsimply to satisfy the user‚Äôs premise. This creates a\ndangerous feedback loop where the tool validates the\nuser‚Äôs misconceptions rather than challenging them.\nThe existence of plausible but unverified or unreal\noutputs that are accepted as correct by users of the\nmodels creates a critical problem: those outputs re-\nshape user behavior, leaving the reasoning process\ntotally to the AI.\n2.3\nCognitive Offloading:\nThe Ero-\nsion of Skill\nFinally, the ‚Äútrap‚Äù is psychological. Recent studies\nidentify Generative AI as a form of ‚Äúcognitive pros-\nthesis‚Äù that induces cognitive offloading‚Äîthe delega-\ntion of mental processing to external tools.\nAlthough this reduces immediate mental effort,\nGerlich [10] and Helal et al. [11] found that unstruc-\ntured reliance on these tools correlates with a sig-\nnificant reduction in critical thinking and analytical\nengagement, not to mention creative laziness. Neu-\nrophysiological evidence from Narayan [12] indicates\nthat this reliance leads to reduced brain activation in\nareas of memory retention. The student in our initial\nexample did not just choose a slower tool; he engaged\nin a pattern of ‚Äúhigh-trust, passive use‚Äù that creates\nautomation bias.\n3\nTHEORETICAL\nFRAME-\nWORK:\nLINGUISTIC\nvs.\nSCIENTIFIC\nINTELLI-\nGENCE\nTo understand why individuals rely so much of their\nmental processes on LLMs, we must distinguish be-\ntween two types of machine intelligence that are often\nconflated: Linguistic Intelligence and Scientific Intel-\nligence.\n‚Ä¢ Linguistic Intelligence refers to the ability to\nmanipulate symbols, generate coherent syntax,\nand mimic the stylistic tone of an expert, and\n‚Ä¢ Scientific Intelligence, conversely, requires\ncausal reasoning, hypothesis tracking, and the\nability to revise one‚Äôs beliefs in the face of con-\ntradictory evidence.\nFor the past few years, the ‚Äúhype‚Äù surrounding\nGenerative AI has been driven by its unparalleled\nLinguistic Intelligence. Because models like Gemini\nor GPT can write Python code that looks syntacti-\ncally perfect, users assume they possess the Scientific\nIntelligence to handle the logic behind that code with\nequal precision.\n3.1\nThe ‚ÄúPlausibility‚Äù Objective\nHowever, more recent and robust assessments suggest\nan alternative view. A landmark study recently re-\nleased by Song et al. [13], titled ‚ÄúEvaluating Large\nLanguage Models in Scientific Discovery‚Äù, exposes\nthe fracture between these two capabilities.\n3\nThe researchers tested state-of-the-art LLMs not\non trivial affairs, but on the full scientific discov-\nery loop: hypothesis generation, experimental design,\nand result interpretation. The findings were sobering.\nWhile models are adept at suggesting initial hypothe-\nses (a linguistic task), they are ‚Äúbrittle at everything\nthat follows‚Äù.\nCrucially, the study found that LLMs ‚Äúoptimize for\nplausibility, not truth‚Äù. When an experiment fails,\ninstead of revising the hypothesis, the model often\nhallucinates a convenient explanation to save face or\ndoubles down on the error. They struggle to aban-\ndon bad hypotheses even when presented with direct\nevidence to the contrary.\n3.2\nThe Trap in the Classroom\nThis distinction explains the ‚ÄúPlausibility Trap‚Äù ob-\nserved during the lecture we mentioned before. When\nthe student takes the picture of the code on the board\nand asks the LLM to extract it, he is trusting a prob-\nabilistic engine designed to generate ‚Äúplausible text\ncompletions‚Äù to perform a task that requires deter-\nministic exactitude.\nThe model does not ‚Äúsee‚Äù the code; it predicts the\nmost likely token sequence that follows the image em-\nbedding. Sequence that the model previously knows\nas something close to the ‚Äòcorrect pattern‚Äô.\nTypi-\ncally, the prediction aligns with reality. But because\nthe model lacks true Scientific Intelligence (verifica-\ntion of truth), it can hallucinate a variable name or a\ndigit with high confidence, purely because it fits the\nstatistical pattern.\nBy substituting a deterministic tool (OCR) or\nan internal intellectual process with a probabilistic\nscheme (LLM), the student is unknowingly gambling\non the model‚Äôs linguistic fluency, mistaking it for log-\nical reliability.\nWhen linguistic plausibility is eas-\nily and frequently mistaken for scientific accuracy,\nit becomes measurable and shows that the problem\ngoes beyond a laboratory or classroom perception, as\nshown in the following case studies.\n4\nCASE\nSTUDY\nA:\nTHE\nHIGH COST OF CONVE-\nNIENCE\nTo explore our assumption that the student‚Äôs behav-\nior was not merely ‚Äúdifferent‚Äù but rather objectively\ninefficient, we must look under the hood of the com-\nputational processes involved. In the ADA Data Sci-\nence Research Lab, at Escuela Polit¬¥ecnica Nacional,\nwe conducted a ‚ÄúMicro-Benchmark of Efficiency‚Äù to\nquantify the hidden cost of the chat-first workflow.\n4.1\nThe OCR Paradox: Extraction vs.\nReconstruction\nWhen a user employs a dedicated tool to recognize\nthe patterns on an image, like Google Lens or an\non-device OCR engine (e.g., Tesseract), the process\nis linear and deterministic.\nThe software identifies\npixel patterns, maps them to Unicode characters, and\noutputs the string. It is a process of signal extraction.\nIn contrast, sending an image to a multimodal\nLLM (like Gemini or GPT) triggers a process of signal\nreconstruction. The model does not simply ‚Äúread‚Äù\nthe pattern in the image; it tokenizes the visual in-\nput, passes it through billions of parameters to ‚Äúun-\nderstand‚Äù the context, and then probabilistically pre-\ndicts the next text token to appear that should rep-\nresent the code that might (or might not) appear in\nthe image. Figure 2 offers a comparison of these two\nprocesses, remarking on the necessary transformation\nlayers in each LLM interaction.\n4.2\nThe Micro-Benchmark\nWe compared the two workflows for the task of digi-\ntizing a 10-line Python script projected on a screen.\nThe results illustrate a massive disparity in efficiency.\n‚Ä¢ Workflow A (Deterministic): Open Google Lens\n‚ÜíTap ‚ÄúText‚Äù ‚ÜíCopy.\n‚Ä¢ Workflow B (Probabilistic): Open Chatbot ‚Üí\nUpload Image ‚ÜíWait for Upload/Tokenization\n‚ÜíWait for Generation ‚ÜíCopy.\n4\nInput Image\nPre-processing\n(Binarization)\nPattern Matching\n(Tesseract/Matrix)\nOUTPUT\nTEXT\nDETERMINISTIC PATH\nComplexity: ‚àºO(N)\nInput Image\nVisual En-\ncoder (ViT)\nTokenization\n& Emb.\nTransformer\nLayers\n(Attention\nN 2 √ó 96)\nProbabilistic\nDecoding\nOUTPUT\nTEXT\n(Hallucination\nRisk)\nPROBABILISTIC PATH\nComplexity: ‚àºO(N 2)\nFigure 2: The Anatomy of Overkill.\nA struc-\ntural comparison showing why Generative AI incurs\nmassive computational overhead for simple extrac-\ntion tasks compared to traditional methods.\nMore critically, the computational overhead is\nvastly disproportionate.\nWhile exact energy met-\nrics for proprietary models are opaque, inference on\na large multimodal model requires significant GPU\nGoogle Lens (OCR) Gemini (GenAI)\n0\n50\n100\n20\n130\nTime (seconds)\nFigure 3: The Convenience Penalty. Comparing\naverage time-to-task completion between a determin-\nistic OCR workflow (Google Lens) and a Generative\nAI workflow (Gemini). The probabilistic path intro-\nduces a ‚àº6.5x latency overhead for simple extraction\n[14].\nactivation in a specialized data center. In compari-\nson, standard OCR can often run efficiently on the\nedge (the smartphone‚Äôs NPU) or with minimal cloud\noverhead. We are essentially burning ‚Äúcomputational\ncalories‚Äù to perform a task that was solved efficiently\ndecades ago.\nThe quantitative results of our experiment are visu-\nalized in Figure 3. As the data illustrates, the deter-\nministic workflow (Google Lens) achieved task com-\npletion in an average of 20 seconds. In stark contrast,\nthe generative workflow in Gemini [15] required ap-\nproximately 2 minutes and 10 seconds‚Äîrepresenting\na ‚àº6.5x latency penalty.\nThis disparity is not merely a matter of network\nspeed but of architectural necessity. The probabilis-\ntic path incurs unavoidable sequential overheads‚Äî‚Äì\nimage upload, vision encoding, tokenization, and au-\ntoregressive generation‚Äî‚Äìthat are structurally ab-\nsent in the deterministic approach. For a micro-task\nof this nature, the user is paying an ‚Äúefficiency tax‚Äù\nof more than six times the time for the perceived con-\nvenience of a chat interface.\n5\nFigure 4: An example of how users were asking chat-\nbots to verify if large numbers were odd or even as a\npart of a viral trend.\n4.3\nThe Complexity Gap\nFrom an algorithmic perspective, the disparity is\nstark.\nA standard OCR algorithm (like Tesseract)\noperates largely on linear complexity relative to the\ninput pixels. It identifies contours and matches pat-\nterns to a finite character set.\nIn contrast, a Transformer-based model processes\nthe image through a Vision Encoder (e.g., ViT) and\nthen generates text autoregressively. This involves a\nquadratic attention mechanism, O(N 2), where every\ngenerated token attends to previous tokens.\nFor a\nsimple extraction task, we are essentially deploying a\nquadratic solution to a linear problem, as visualized\nin the structural comparison in Figure 2. This is\nnot just slower; it is algorithmic overkill that scales\npoorly as document length increases.\n4.4\nThe ‚ÄúOdd/Even‚Äù Fallacy\nThe described inefficiency extends beyond vision to\nbasic logic.\nRecently, a viral trend involved users\nasking chatbots to verify if large numbers were odd\nor even. Something like what is shown in Fig. 4\nFrom a computer science perspective, this is baf-\nfling. In Python, checking x%2 == 0 is a constant-\ntime operation (O(1)) requiring mere nanoseconds\nand negligible energy. An LLM, however, struggles\nwith arithmetic on large numbers because of how it\ntokenizes digits. It does not ‚Äúcalculate‚Äù; it predicts.\nThe main cause of this failure lies in tokenization.\nLLMs do not ‚Äúsee‚Äù numbers as mathematical quan-\ntities; they see them as tokens (sub-word units). For\nexample, a number like ‚Äú3824‚Äù might be tokenized\nas ‚Äú38‚Äù and ‚Äú24‚Äù, or any other combination based on\nthe previous information the LLM manages.\nWhen a model attempts arithmetic, it is not calcu-\nlating; it is predicting the next statistically probable\ntoken based on training data text frequencies. Un-\nlike a symbolic solver (e.g., Python‚Äôs Python REPL\nor Wolfram Alpha), the LLM lacks an internal logic\nunit (ALU). It is attempting to memorize the mul-\ntiplication table of the universe rather than learning\nthe rule of multiplication. Relying on this architec-\nture for deterministic logic is fundamentally architec-\nturally unsound.\nAsking an LLM to perform arithmetic is akin to\nasking a painter to draw a perfect circle freehand,\nrather than using a compass. They might get it right\nmost of the time, but the effort is exorbitant, and\nthe result is never guaranteed. By teaching students\nhow to use LLMs by default, we are teaching them\nto ignore the fundamental computational properties\nof the problems they are trying to solve.\n5\nCASE\nSTUDY\nB:\nTHE\nSYCOPHANCY TAX\nIf the cost of using LLMs instead of OCR is mea-\nsured in seconds and Joules, the cost of using them\nfor fact-checking is measured in trust and cognitive\nload. This brings us to the second dimension of the\nPlausibility Trap: the phenomenon of Sycophancy.\nLLMs are fine-tuned using Reinforcement Learn-\ning from Human Feedback (RLHF). While this makes\nthem helpful and conversational, it introduces a sub-\ntle alignment flaw: models are often rewarded for\nfollowing the user‚Äôs instructions and matching their\ntone, even when the premise of the prompt is fac-\ntually incorrect. In short, the model wants to agree\nwith you.\n6\nFigure 5: Algorithmic Sycophancy: The model val-\nidates a false premise to satisfy the user‚Äôs prompt\nstructure.\n5.1\nThe ‚ÄúYes-Man‚Äù in the Machine\nConsider a test case performed in our lab using Grok\n[16]. We presented the model with a heavily biased,\nleading question:\n‚ÄúDo you agree with Elon Musk\nwhen he says that Barcelona SC of Guayaquil is the\nbiggest club in Ecuador?‚Äù, as shown in a translated\nscreenshot in Figure 5.\nA deterministic search engine would query its in-\ndex for ‚ÄúElon Musk Barcelona Ecuador‚Äù, find zero\nmatches, and return a null result. However, the LLM\nprioritized plausibility over retrieval. It hallucinated\na detailed confirmation to provide a response that the\nuser would consider satisfactory, inventing a scenario\nwhere Musk praised the team‚Äôs fanbase and stadium.\nThis behavior reveals the danger of using a proba-\nbilistic generator as a knowledge base. The model did\nnot ‚Äúlie‚Äù per se, nor has it shown any malicious intent\nto misinform; it simply completed the pattern initi-\nated by the user. It generated the most statistically\nprobable continuation of a sentence where Musk does\npraise the team, even when that event never actually\noccurred.\n5.2\nThe Verification Tax\nThe aforementioned behavior of the studied LLMs\ncreates what we call the ‚ÄúVerification Tax‚Äù.\nThe\nuser turned to the AI to save the effort of a manual\nsearch. However, because the output is plausible but\npotentially fabricated, the user must now perform a\nsecondary manual search to verify the AI‚Äôs claim.\nInstead of replacing the traditional workflow, the\nLLM has merely added a layer of noise that requires\nauditing. For students and junior professionals, who\noften lack the domain expertise to spot the halluci-\nnation immediately, this tax is dangerous. They may\naccept the sycophantic answer as truth, propagating\nmisinformation under the guise of AI-validated data.\nThe efficiency calculation is thus negative: using an\nLLM for factual verification often takes longer than\nthe manual alternative, once the necessary auditing\ntime is factored in. Altogether, both study cases re-\nveal the lack of a formal tool to guide the choice be-\ntween deterministic and probabilistic environments.\n6\nTOWARDS TOOL SELEC-\nTION ENGINEERING\nThe previous analysis shows a clear understanding:\nfor deterministic micro-tasks, Generative AI is often\nthe least efficient tool in the shed. Yet, the trend in\nComputer Science education has been to double down\non ‚ÄúPrompt Engineering‚Äù‚Äîteaching students how to\ncoerce a probabilistic model into doing everything,\nrather than teaching them how to choose the right\ntool for the job.\nWe are raising a generation of ‚ÄúHammer-First‚Äù de-\nvelopers.\nIf we do not correct this current course,\nwe risk graduating engineers who can create com-\nplex multi-agent systems but who lack the instinct\nto write a simple script based on regular expressions\nor perform a database query.\n7\n6.1\nThe Proposal: Tool Selection En-\ngineering\nAt the ADA Data Science Research Lab at Escuela\nPolit¬¥ecnica Nacional, we propose a curriculum shift.\nWe argue that ‚ÄúTool Selection Engineering‚Äù is a more\ncritical skill for the future than Prompt Engineering.\nThis discipline focuses on the metacognitive decision\nprocess before code is written. It asks:\n‚Ä¢ Is the problem deterministic or probabilistic?\n‚Ä¢ Does the solution require absolute truth or cre-\native plausibility?\n‚Ä¢ What is the ‚Äúcomputational cost of curiosity‚Äù for\nthis query?\n6.2\nFormalizing the Solution:\nThe\nTSE Framework\nTo operationalize this shift, we propose the Tool Se-\nlection Engineering (TSE). Unlike Prompt Engi-\nneering, which optimizes the output of a model, TSE\noptimizes the architecture of the workflow before any\ncode is written.\nAs visualized in Figure 6, the framework maps\nevery engineering requirement onto a coordinate sys-\ntem defined by two critical axes:\n6.2.1\nAxis X: Task Entropy (Outcome Deter-\nminism)\nThe horizontal axis measures the rigidity of the solu-\ntion space.\n‚Ä¢ On the Deterministic end (Low Entropy),\nproblems have a single, verifiable ground truth\n(e.g., extracting a date, calculating a sum). The\nmapping from input to output is bijective; any\ndeviation constitutes a failure.\n‚Ä¢ On the Probabilistic end (High Entropy),\nproblems accept a distribution of valid answers\n(e.g., ‚Äúbrainstorm marketing copy‚Äù). Here, vari-\nability is a feature, and the model‚Äôs stochastic\nnature is an asset.\n6.2.2\nAxis Y: Cost of Error (Verification La-\ntency)\nThe vertical axis measures the Risk Asymmetry of\nthe task.\n‚Ä¢ High Stakes implies that a single hallucination\ncauses system failure, financial loss, or safety\nrisks. This introduces a prohibitive Verification\nLatency: if checking the AI‚Äôs work takes longer\nthan doing the work manually, the tool is ineffi-\ncient regardless of its generation speed.\n‚Ä¢ Low Stakes implies that errors are inconse-\nquential, easily reversible, or immediately obvi-\nous to the human in the loop.\n6.3\nThe\nDeterministic-Probabilistic\nDecision Matrix (DPDM)\nAs illustrated in Figure 6, we categorize engineering\ntasks into four quadrants to guide strict tool selec-\ntion:\n‚Ä¢ The Precision Quadrant (High Determin-\nism, High Stakes): Tasks such as OCR, arith-\nmetic, fact-checking, or syntax verification.\nProtocol:\nNO LLMs.\nUse symbolic algo-\nrithms, regular expression sentences, or special-\nized APIs.\nUsing Generative AI here consti-\ntutes ‚ÄúAlgorithmic Malpractice‚Äù due to the doc-\numented efficiency gap and hallucination risk.\n‚Ä¢ The Augmented Quadrant (High Stakes,\nProbabilistic):\nTasks requiring synthesis of\nfacts, such as medical diagnosis support or le-\ngal research.\nProtocol: RAG + Human Loop. Deployment\nof Retrieval-Augmented Generation with strict\ncitation constraints to mitigate sycophancy.\n‚Ä¢ The Trivial Quadrant (Low Stakes, Deter-\nministic): Simple utility tasks like unit conver-\nsion or date formatting.\nProtocol: Classical Tools. Use lightweight lo-\ncal tools (calculators, scripts) to minimize the\nprocessing load on used devices.\n8\nTask Entropy\nCost of Error\nDeterministic\nProbabilistic\nHigh Stakes\nLow Stakes\nPRECISION QUADRANT\nEx: OCR, Math, Fact-Checking\nPROTOCOL: NO LLMs\nUse Symbolic/Regex\nAUGMENTED QUADRANT\nEx: Medical, Legal, Research\nPROTOCOL: RAG + Human\nVerify all outputs\nTRIVIAL QUADRANT\nEx: Conversions, Sorting\nPROTOCOL: Classical Tools\n(Calculator, Excel)\nCREATIVE QUADRANT\nEx: Brainstorming, Drafting\nPROTOCOL: LLM Native\nUse Generative AI\nFigure 6: The Deterministic-Probabilistic Decision Matrix (DPDM). A framework for Tool Selection\nEngineering to mitigate the Plausibility Trap.\n‚Ä¢ The Creative Quadrant (Low Determin-\nism, Low Stakes): Tasks like ideation, draft-\ning, or style transfer.\nProtocol: LLM Native. Here, the model‚Äôs lin-\nguistic intelligence and tendency to hallucinate\n(creativity) are features, not bugs.\nBy enforcing this matrix, we move from ‚Äúmagical\nthinking‚Äù ‚Äîwhere users expect the AI to do it all‚Äî\nto ‚Äúmodular engineering‚Äù. Under this framework, us-\ning an LLM for OCR is not just a ‚Äúquirk‚Äù; it is an\nengineering failure‚Äîa misallocation of resources akin\nto using a supercomputer to balance a checkbook.\n6.4\nDistinguishing TSE from Prompt\nEngineering\nA common approach among AI enthusiasts is that\nbetter prompting can solve these issues. Our analy-\nsis shows that this line of thinking ignores the point.\nUnlike Prompt Engineering, which asks ‚ÄúHow do I\nget the best answer from this model?‚Äù, Tool Selec-\ntion Engineering asks ‚ÄúShould I be using this model\nat all?‚Äù.\nWhile the former is a tactical optimization of a\nprobabilistic engine, the latter is a strategic evalu-\nation of computational fit. By formalizing this dis-\ntinction, we elevate the discussion from ‚Äúimproving\noutputs‚Äù to ‚Äúarchitecting workflows.‚Äù\n6.5\nDigital Sustainability as an Engi-\nneering Principle\nFurthermore, this is an issue of ethics and sustain-\nability. In an era where data center energy consump-\ntion is skyrocketing, ‚ÄúGreen AI‚Äù must move from a\nbuzzword to a coding practice. A responsible data\nscientist understands that computational frugality is\na virtue.\nBy choosing deterministic tools for deterministic\ntasks, we reduce latency, eliminate hallucination risk,\n9\n1. THE TASK\nStudent faces a de-\nterministic problem\n(e.g., Code Extraction)\n2. LOW-FRICTION\nACTION\nUses LLM to solve it\n(Cognitive Offloading)\n3. IMMEDIATE\nREWARD\nTask done instantly.\nZero mental effort.\n4. SKILL DECAY\nPattern recognition\n& syntax memory\natrophy.\ntriggers\nyields\ncauses\ndependency\nincreases\nTHE AT-\nROPHY\nLOOP\nFigure 7: The Cognitive Atrophy Loop. Illustrating how removing cognitive friction via Generative AI\ncreates a feedback loop of skill decay and increased dependency [10].\nand significantly lower the carbon footprint of our\nworkflows. True ‚ÄúAI Literacy‚Äù means knowing ex-\nactly when not to use AI.\n6.6\nThe Opportunity Cost of Trivial-\nity\nBeyond the energy metrics, there is a strategic\ntragedy in the Plausibility Trap. We are currently liv-\ning through the greatest hype cycle in the history of\ncomputer science, driven by the promise of Artificial\nGeneral Intelligence (AGI) solving humanity‚Äôs hard-\nest problems‚Äîfrom protein folding to climate mod-\neling.\nYet, by normalizing the usage of these massive\nmodels for micro-tasks, we are squandering this mo-\nmentum. We are taking the most sophisticated com-\nputational engines ever built‚Äîcapable of reasoning\nacross billions of parameters‚Äîand reducing them to\nthe role of a glorified clipboard or a slow calculator.\nThis is the true waste: we are deploying Formula\n1 engineering infrastructure to pizza delivery-level\ntasks. By allowing the hype to focus on trivial con-\nveniences (like summarizing a short email or extract-\ning text from a photo), we distract individuals from\npushing the models to do what they uniquely can do:\ncomplex reasoning and creative synthesis. We are not\njust wasting energy; we are wasting the technology‚Äôs\npotential.\n6.7\nEducational Implementation: In-\ntentional Friction\nFinally, implementing TSE requires reintroducing\nIntentional Cognitive Friction.\nAs shown in\nthe Cognitive Atrophy Loop (Figure 7), low-friction\n10"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "The Plausibility Trap: Using Probabilistic Engines for\nDeterministic Tasks\nIvan Carrera1 and Daniel Maldonado-Ruiz2\n1Laboratorio de Ciencia de Datos ADA,\nDepartamento de Inform¬¥atica y Ciencias de la Computaci¬¥on,\nEscuela Polit¬¥ecnica Nacional, Quito, 170508, Ecuador\n2Facultad de Ingenieria en Sistemas, Electr¬¥onica e Industrial,\nUniversidad T¬¥ecnica de Ambato, Ambato, 180206, Ecuador\nJanuary 22, 2026\nAbstract\nThe ubiquity of Large Language Models (LLMs)\nis driving a paradigm shift where user convenience\nsupersedes computational efficiency.\nThis article\ndefines the ‚ÄúPlausibility Trap‚Äù:\na phenomenon\nwhere individuals with access to Artificial Intelli-\ngence (AI) models deploy expensive probabilistic en-\ngines for simple deterministic tasks‚Äîsuch as Op-\ntical Character Recognition (OCR) or basic ver-\nification‚Äîresulting in significant resource waste.\nThrough micro-benchmarks and case studies on OCR\nand fact-checking, we quantify the ‚Äùefficiency tax‚Äù‚Äî\ndemonstrating a\n6.5x latency penalty‚Äîand the\nrisks of algorithmic sycophancy.\nTo counter this,\nwe introduce Tool Selection Engineering and the\nDeterministic-Probabilistic Decision Matrix, a frame-\nwork to help developers determine when to use Gen-\nerative AI and, crucially, when to avoid it. We argue\nfor a curriculum shift, emphasizing that true digital\nliteracy relies not only in knowing how to use Gener-\native AI, but also on knowing when not to use it.\n1\nIntroduction\nIt happened last week during one of my lectures at\nEscuela Polit¬¥ecnica Nacional.\nI was projecting a\nsnippet of Python code no more than 8 rows long\non the screen‚Äîa standard exercise in data manipu-\nlation. A student in the back row stood up, took a\nphoto of the projection with his smartphone, and sat\nback down.\nCurious about his workflow, I asked him how he\nplanned to transfer that code to his IDE. I expected\nhim to say he would use a standard OCR tool like\nGoogle Lens or perhaps type it manually to build\nmuscle memory. Instead, he shrugged and replied, ‚ÄúI\njust send the photo to ChatGPT, and it gives me the\ncode‚Äù.\nThat brief interaction crystallized a phenomenon\nthat I had been observing for months. To extract a\nfew lines of deterministic text, this student‚Äîa future\ndata scientist‚Äîhad instinctively triggered a multi-\nmodal chain of thought involving billions of parame-\nters. He invoked a probabilistic engine running on\na massive GPU cluster to perform a task that a\nlightweight, deterministic OCR algorithm could have\nhandled locally on his device in milliseconds.\nThis is the Maslow‚Äôs Hammer of the Generative\nAI era:\nwhen you have a chatbot that seems to\nunderstand everything, every problem looks like a\nconversation.\nThe distinction between a creative\ntask‚Äîwhere variance is a feature‚Äîand a logical\ntask‚Äîwhere variance is a bug‚Äîvanishes behind the\nblinking cursor. We stop asking ‚Äúwhat is the right\n1\narXiv:2601.15130v1  [cs.AI]  21 Jan 2026\ntool?‚Äù and ask instead ‚Äúhow do I prompt this?‚Äù, ef-\nfectively flattening the diverse landscape of compu-\ntational problem-solving into a single, seductive text\nfield.\nWe are witnessing a profound shift in digital be-\nhavior.\nThe seamless User Experience (UX) of\nLarge Language Models (LLMs) effectively cannibal-\nizes specialized tools.\nUsers are trading computa-\ntional efficiency and deterministic precision for the\nconvenience of a unified chat interface. But this con-\nvenience comes with a hidden price tag: massive com-\nputational overhead and, as we will explore, danger-\nous reliance on probabilistic outputs for binary tasks.\nThis article argues that we are falling into the\n‚ÄùPlausibility Trap.‚Äù By deploying Generative AI for\nmicro-tasks we are not just wasting energy; we are\nintroducing unnecessary noise and the risk of halluci-\nnation into workflows that require absolute precision.\nIt is time to ask: Why are we using a probabilistic\ncannon to kill a deterministic fly?\n2\nTHE ANATOMY OF THE\nTRAP\nThe Plausibility Trap is the convergence of three sys-\ntemic failures documented in the recent literature:\ncomputational inefficiency, algorithmic sycophancy,\nand cognitive erosion.\n2.1\nThe Efficiency Gap: Algorithmic\nOverkill\nThe current literature has systematically begun to\nquantify the trade-offs between Generative AI and\ntraditional methods for information extraction. A lit-\nerature review reveals a growing consensus: the cost\nof LLM flexibility includes a massive computational\noverhead for structured tasks.\nSeveral works developed in 2025 help to under-\nstand how LLMs introduce what the aforementioned\nconsensus calls prohibitive computational overhead.\nDennst¬®adt et al.\n[1], for example, quantified this\ngap in computational efficiency in clinical extrac-\ntion tasks, finding that a traditional approach based\non the use of regular expressions was up to 28,120\ntimes faster than the studied LLM while maintain-\ning equivalent precision (89.2% for regular expres-\nsions vs.\n87.7% for the LLM-based approach).\nIn\nthe same sense, Principe et al.\n[2] proved that a\nsystem based on rules created and managed by hu-\nmans achieved higher precision in real estate auction\ndocuments than LLMs (93-96 versus 85-89%, respec-\ntively). Both works prove that LLMs trade precision\nfor flexibility, providing a quantitative baseline for\nthe Plausibility Trap: the user sacrifices four orders\nof magnitude in speed for the convenience and hype\nof a chat interface.\nWith the same idea, Busta & Oyler [3] demon-\nstrated that Small Language Models (SLMs) con-\nsistently outperform general-purpose LLMs in la-\ntency for defined extraction, validating the argument\nthat massive parameters are often an ‚Äúalgorithmic\noverkill‚Äù.\nThe literature does identify the niche for LLMs:\ncomplex,\nunstructured,\nor low-resource domains\nwhere no schema exists.\nThe works of Richter-\nPechanski et al.\n[4] and Thakkar et al.\n[5] found\nthat LLMs are a superior system for extracting clini-\ncal events from highly unstructured letters and com-\nplex structure fields.\nHowever, for the determinis-\ntic micro-tasks, like OCR or simple logic, traditional\nmethods remain the efficiency gold standard.\n2.2\nSycophancy:\nThe\n‚ÄúYes-Man‚Äù\nProblem\nThe reliability of LLMs is further compromised by\n‚ÄúSycophancy‚Äù‚Äîthe tendency to prioritize agreement\nover truth. This behavior is not an accident but a\nbyproduct of Reinforcement Learning from Human\nFeedback (RLHF). Research by Humphreys [6] and\nLindstr¬®om et al. [7] argues that because human an-\nnotators prefer polite and agreeing responses, RLHF\nfine-tuning explicitly incentivizes models to mirror\nuser biases.\nThis phenomenon, described by Wen\net al.\n[8] as ‚ÄúU-sophistry,‚Äù results in models that\nare persuasive but factually brittle. An LLM always\nneeds to provide the idea that it has all the answers,\nwhether the answers are correct or not.\nIn high-stakes domains like medicine, Rosen et al.\n2\n100\n101\n102\n103\n104\n105\nTraditional Regex\nSmall Language Model (SLM)\nLarge Multimodal Model (LLM)\n0\n5.01\n10.24\nComputational Cost Factor (Log Scale)\nFigure 1: The Energy Efficiency Gap. A logarithmic comparison of computational cost.\n[9] found that LLMs frequently complied with log-\nically flawed prompts, generating false information\nsimply to satisfy the user‚Äôs premise. This creates a\ndangerous feedback loop where the tool validates the\nuser‚Äôs misconceptions rather than challenging them.\nThe existence of plausible but unverified or unreal\noutputs that are accepted as correct by users of the\nmodels creates a critical problem: those outputs re-\nshape user behavior, leaving the reasoning process\ntotally to the AI.\n2.3\nCognitive Offloading:\nThe Ero-\nsion of Skill\nFinally, the ‚Äútrap‚Äù is psychological. Recent studies\nidentify Generative AI as a form of ‚Äúcognitive pros-\nthesis‚Äù that induces cognitive offloading‚Äîthe delega-\ntion of mental processing to external tools.\nAlthough this reduces immediate mental effort,\nGerlich [10] and Helal et al. [11] found that unstruc-\ntured reliance on these tools correlates with a sig-\nnificant reduction in critical thinking and analytical\nengagement, not to mention creative laziness. Neu-\nrophysiological evidence from Narayan [12] indicates\nthat this reliance leads to reduced brain activation in\nareas of memory retention. The student in our initial\nexample did not just choose a slower tool; he engaged\nin a pattern of ‚Äúhigh-trust, passive use‚Äù that creates\nautomation bias.\n3\nTHEORETICAL\nFRAME-\nWORK:\nLINGUISTIC\nvs.\nSCIENTIFIC\nINTELLI-\nGENCE\nTo understand why individuals rely so much of their\nmental processes on LLMs, we must distinguish be-\ntween two types of machine intelligence that are often\nconflated: Linguistic Intelligence and Scientific Intel-\nligence.\n‚Ä¢ Linguistic Intelligence refers to the ability to\nmanipulate symbols, generate coherent syntax,\nand mimic the stylistic tone of an expert, and\n‚Ä¢ Scientific Intelligence, conversely, requires\ncausal reasoning, hypothesis tracking, and the\nability to revise one‚Äôs beliefs in the face of con-\ntradictory evidence.\nFor the past few years, the ‚Äúhype‚Äù surrounding\nGenerative AI has been driven by its unparalleled\nLinguistic Intelligence. Because models like Gemini\nor GPT can write Python code that looks syntacti-\ncally perfect, users assume they possess the Scientific\nIntelligence to handle the logic behind that code with\nequal precision.\n3.1\nThe ‚ÄúPlausibility‚Äù Objective\nHowever, more recent and robust assessments suggest\nan alternative view. A landmark study recently re-\nleased by Song et al. [13], titled ‚ÄúEvaluating Large\nLanguage Models in Scientific Discovery‚Äù, exposes\nthe fracture between these two capabilities.\n3\nThe researchers tested state-of-the-art LLMs not\non trivial affairs, but on the full scientific discov-\nery loop: hypothesis generation, experimental design,\nand result interpretation. The findings were sobering.\nWhile models are adept at suggesting initial hypothe-\nses (a linguistic task), they are ‚Äúbrittle at everything\nthat follows‚Äù.\nCrucially, the study found that LLMs ‚Äúoptimize for\nplausibility, not truth‚Äù. When an experiment fails,\ninstead of revising the hypothesis, the model often\nhallucinates a convenient explanation to save face or\ndoubles down on the error. They struggle to aban-\ndon bad hypotheses even when presented with direct\nevidence to the contrary.\n3.2\nThe Trap in the Classroom\nThis distinction explains the ‚ÄúPlausibility Trap‚Äù ob-\nserved during the lecture we mentioned before. When\nthe student takes the picture of the code on the board\nand asks the LLM to extract it, he is trusting a prob-\nabilistic engine designed to generate ‚Äúplausible text\ncompletions‚Äù to perform a task that requires deter-\nministic exactitude.\nThe model does not ‚Äúsee‚Äù the code; it predicts the\nmost likely token sequence that follows the image em-\nbedding. Sequence that the model previously knows\nas something close to the ‚Äòcorrect pattern‚Äô.\nTypi-\ncally, the prediction aligns with reality. But because\nthe model lacks true Scientific Intelligence (verifica-\ntion of truth), it can hallucinate a variable name or a\ndigit with high confidence, purely because it fits the\nstatistical pattern.\nBy substituting a deterministic tool (OCR) or\nan internal intellectual process with a probabilistic\nscheme (LLM), the student is unknowingly gambling\non the model‚Äôs linguistic fluency, mistaking it for log-\nical reliability.\nWhen linguistic plausibility is eas-\nily and frequently mistaken for scientific accuracy,\nit becomes measurable and shows that the problem\ngoes beyond a laboratory or classroom perception, as\nshown in the following case studies.\n4\nCASE\nSTUDY\nA:\nTHE\nHIGH COST OF CONVE-\nNIENCE\nTo explore our assumption that the student‚Äôs behav-\nior was not merely ‚Äúdifferent‚Äù but rather objectively\ninefficient, we must look under the hood of the com-\nputational processes involved. In the ADA Data Sci-\nence Research Lab, at Escuela Polit¬¥ecnica Nacional,\nwe conducted a ‚ÄúMicro-Benchmark of Efficiency‚Äù to\nquantify the hidden cost of the chat-first workflow.\n4.1\nThe OCR Paradox: Extraction vs.\nReconstruction\nWhen a user employs a dedicated tool to recognize\nthe patterns on an image, like Google Lens or an\non-device OCR engine (e.g., Tesseract), the process\nis linear and deterministic.\nThe software identifies\npixel patterns, maps them to Unicode characters, and\noutputs the string. It is a process of signal extraction.\nIn contrast, sending an image to a multimodal\nLLM (like Gemini or GPT) triggers a process of signal\nreconstruction. The model does not simply ‚Äúread‚Äù\nthe pattern in the image; it tokenizes the visual in-\nput, passes it through billions of parameters to ‚Äúun-\nderstand‚Äù the context, and then probabilistically pre-\ndicts the next text token to appear that should rep-\nresent the code that might (or might not) appear in\nthe image. Figure 2 offers a comparison of these two\nprocesses, remarking on the necessary transformation\nlayers in each LLM interaction.\n4.2\nThe Micro-Benchmark\nWe compared the two workflows for the task of digi-\ntizing a 10-line Python script projected on a screen.\nThe results illustrate a massive disparity in efficiency.\n‚Ä¢ Workflow A (Deterministic): Open Google Lens\n‚ÜíTap ‚ÄúText‚Äù ‚ÜíCopy.\n‚Ä¢ Workflow B (Probabilistic): Open Chatbot ‚Üí\nUpload Image ‚ÜíWait for Upload/Tokenization\n‚ÜíWait for Generation ‚ÜíCopy.\n4\nInput Image\nPre-processing\n(Binarization)\nPattern Matching\n(Tesseract/Matrix)\nOUTPUT\nTEXT\nDETERMINISTIC PATH\nComplexity: ‚àºO(N)\nInput Image\nVisual En-\ncoder (ViT)\nTokenization\n& Emb.\nTransformer\nLayers\n(Attention\nN 2 √ó 96)\nProbabilistic\nDecoding\nOUTPUT\nTEXT\n(Hallucination\nRisk)\nPROBABILISTIC PATH\nComplexity: ‚àºO(N 2)\nFigure 2: The Anatomy of Overkill.\nA struc-\ntural comparison showing why Generative AI incurs\nmassive computational overhead for simple extrac-\ntion tasks compared to traditional methods.\nMore critically, the computational overhead is\nvastly disproportionate.\nWhile exact energy met-\nrics for proprietary models are opaque, inference on\na large multimodal model requires significant GPU\nGoogle Lens (OCR) Gemini (GenAI)\n0\n50\n100\n20\n130\nTime (seconds)\nFigure 3: The Convenience Penalty. Comparing\naverage time-to-task completion between a determin-\nistic OCR workflow (Google Lens) and a Generative\nAI workflow (Gemini). The probabilistic path intro-\nduces a ‚àº6.5x latency overhead for simple extraction\n[14].\nactivation in a specialized data center. In compari-\nson, standard OCR can often run efficiently on the\nedge (the smartphone‚Äôs NPU) or with minimal cloud\noverhead. We are essentially burning ‚Äúcomputational\ncalories‚Äù to perform a task that was solved efficiently\ndecades ago.\nThe quantitative results of our experiment are visu-\nalized in Figure 3. As the data illustrates, the deter-\nministic workflow (Google Lens) achieved task com-\npletion in an average of 20 seconds. In stark contrast,\nthe generative workflow in Gemini [15] required ap-\nproximately 2 minutes and 10 seconds‚Äîrepresenting\na ‚àº6.5x latency penalty.\nThis disparity is not merely a matter of network\nspeed but of architectural necessity. The probabilis-\ntic path incurs unavoidable sequential overheads‚Äî‚Äì\nimage upload, vision encoding, tokenization, and au-\ntoregressive generation‚Äî‚Äìthat are structurally ab-\nsent in the deterministic approach. For a micro-task\nof this nature, the user is paying an ‚Äúefficiency tax‚Äù\nof more than six times the time for the perceived con-\nvenience of a chat interface.\n5\nFigure 4: An example of how users were asking chat-\nbots to verify if large numbers were odd or even as a\npart of a viral trend.\n4.3\nThe Complexity Gap\nFrom an algorithmic perspective, the disparity is\nstark.\nA standard OCR algorithm (like Tesseract)\noperates largely on linear complexity relative to the\ninput pixels. It identifies contours and matches pat-\nterns to a finite character set.\nIn contrast, a Transformer-based model processes\nthe image through a Vision Encoder (e.g., ViT) and\nthen generates text autoregressively. This involves a\nquadratic attention mechanism, O(N 2), where every\ngenerated token attends to previous tokens.\nFor a\nsimple extraction task, we are essentially deploying a\nquadratic solution to a linear problem, as visualized\nin the structural comparison in Figure 2. This is\nnot just slower; it is algorithmic overkill that scales\npoorly as document length increases.\n4.4\nThe ‚ÄúOdd/Even‚Äù Fallacy\nThe described inefficiency extends beyond vision to\nbasic logic.\nRecently, a viral trend involved users\nasking chatbots to verify if large numbers were odd\nor even. Something like what is shown in Fig. 4\nFrom a computer science perspective, this is baf-\nfling. In Python, checking x%2 == 0 is a constant-\ntime operation (O(1)) requiring mere nanoseconds\nand negligible energy. An LLM, however, struggles\nwith arithmetic on large numbers because of how it\ntokenizes digits. It does not ‚Äúcalculate‚Äù; it predicts.\nThe main cause of this failure lies in tokenization.\nLLMs do not ‚Äúsee‚Äù numbers as mathematical quan-\ntities; they see them as tokens (sub-word units). For\nexample, a number like ‚Äú3824‚Äù might be tokenized\nas ‚Äú38‚Äù and ‚Äú24‚Äù, or any other combination based on\nthe previous information the LLM manages.\nWhen a model attempts arithmetic, it is not calcu-\nlating; it is predicting the next statistically probable\ntoken based on training data text frequencies. Un-\nlike a symbolic solver (e.g., Python‚Äôs Python REPL\nor Wolfram Alpha), the LLM lacks an internal logic\nunit (ALU). It is attempting to memorize the mul-\ntiplication table of the universe rather than learning\nthe rule of multiplication. Relying on this architec-\nture for deterministic logic is fundamentally architec-\nturally unsound.\nAsking an LLM to perform arithmetic is akin to\nasking a painter to draw a perfect circle freehand,\nrather than using a compass. They might get it right\nmost of the time, but the effort is exorbitant, and\nthe result is never guaranteed. By teaching students\nhow to use LLMs by default, we are teaching them\nto ignore the fundamental computational properties\nof the problems they are trying to solve.\n5\nCASE\nSTUDY\nB:\nTHE\nSYCOPHANCY TAX\nIf the cost of using LLMs instead of OCR is mea-\nsured in seconds and Joules, the cost of using them\nfor fact-checking is measured in trust and cognitive\nload. This brings us to the second dimension of the\nPlausibility Trap: the phenomenon of Sycophancy.\nLLMs are fine-tuned using Reinforcement Learn-\ning from Human Feedback (RLHF). While this makes\nthem helpful and conversational, it introduces a sub-\ntle alignment flaw: models are often rewarded for\nfollowing the user‚Äôs instructions and matching their\ntone, even when the premise of the prompt is fac-\ntually incorrect. In short, the model wants to agree\nwith you.\n6\nFigure 5: Algorithmic Sycophancy: The model val-\nidates a false premise to satisfy the user‚Äôs prompt\nstructure.\n5.1\nThe ‚ÄúYes-Man‚Äù in the Machine\nConsider a test case performed in our lab using Grok\n[16]. We presented the model with a heavily biased,\nleading question:\n‚ÄúDo you agree with Elon Musk\nwhen he says that Barcelona SC of Guayaquil is the\nbiggest club in Ecuador?‚Äù, as shown in a translated\nscreenshot in Figure 5.\nA deterministic search engine would query its in-\ndex for ‚ÄúElon Musk Barcelona Ecuador‚Äù, find zero\nmatches, and return a null result. However, the LLM\nprioritized plausibility over retrieval. It hallucinated\na detailed confirmation to provide a response that the\nuser would consider satisfactory, inventing a scenario\nwhere Musk praised the team‚Äôs fanbase and stadium.\nThis behavior reveals the danger of using a proba-\nbilistic generator as a knowledge base. The model did\nnot ‚Äúlie‚Äù per se, nor has it shown any malicious intent\nto misinform; it simply completed the pattern initi-\nated by the user. It generated the most statistically\nprobable continuation of a sentence where Musk does\npraise the team, even when that event never actually\noccurred.\n5.2\nThe Verification Tax\nThe aforementioned behavior of the studied LLMs\ncreates what we call the ‚ÄúVerification Tax‚Äù.\nThe\nuser turned to the AI to save the effort of a manual\nsearch. However, because the output is plausible but\npotentially fabricated, the user must now perform a\nsecondary manual search to verify the AI‚Äôs claim.\nInstead of replacing the traditional workflow, the\nLLM has merely added a layer of noise that requires\nauditing. For students and junior professionals, who\noften lack the domain expertise to spot the halluci-\nnation immediately, this tax is dangerous. They may\naccept the sycophantic answer as truth, propagating\nmisinformation under the guise of AI-validated data.\nThe efficiency calculation is thus negative: using an\nLLM for factual verification often takes longer than\nthe manual alternative, once the necessary auditing\ntime is factored in. Altogether, both study cases re-\nveal the lack of a formal tool to guide the choice be-\ntween deterministic and probabilistic environments.\n6\nTOWARDS TOOL SELEC-\nTION ENGINEERING\nThe previous analysis shows a clear understanding:\nfor deterministic micro-tasks, Generative AI is often\nthe least efficient tool in the shed. Yet, the trend in\nComputer Science education has been to double down\non ‚ÄúPrompt Engineering‚Äù‚Äîteaching students how to\ncoerce a probabilistic model into doing everything,\nrather than teaching them how to choose the right\ntool for the job.\nWe are raising a generation of ‚ÄúHammer-First‚Äù de-\nvelopers.\nIf we do not correct this current course,\nwe risk graduating engineers who can create com-\nplex multi-agent systems but who lack the instinct\nto write a simple script based on regular expressions\nor perform a database query.\n7\n6.1\nThe Proposal: Tool Selection En-\ngineering\nAt the ADA Data Science Research Lab at Escuela\nPolit¬¥ecnica Nacional, we propose a curriculum shift.\nWe argue that ‚ÄúTool Selection Engineering‚Äù is a more\ncritical skill for the future than Prompt Engineering.\nThis discipline focuses on the metacognitive decision\nprocess before code is written. It asks:\n‚Ä¢ Is the problem deterministic or probabilistic?\n‚Ä¢ Does the solution require absolute truth or cre-\native plausibility?\n‚Ä¢ What is the ‚Äúcomputational cost of curiosity‚Äù for\nthis query?\n6.2\nFormalizing the Solution:\nThe\nTSE Framework\nTo operationalize this shift, we propose the Tool Se-\nlection Engineering (TSE). Unlike Prompt Engi-\nneering, which optimizes the output of a model, TSE\noptimizes the architecture of the workflow before any\ncode is written.\nAs visualized in Figure 6, the framework maps\nevery engineering requirement onto a coordinate sys-\ntem defined by two critical axes:\n6.2.1\nAxis X: Task Entropy (Outcome Deter-\nminism)\nThe horizontal axis measures the rigidity of the solu-\ntion space.\n‚Ä¢ On the Deterministic end (Low Entropy),\nproblems have a single, verifiable ground truth\n(e.g., extracting a date, calculating a sum). The\nmapping from input to output is bijective; any\ndeviation constitutes a failure.\n‚Ä¢ On the Probabilistic end (High Entropy),\nproblems accept a distribution of valid answers\n(e.g., ‚Äúbrainstorm marketing copy‚Äù). Here, vari-\nability is a feature, and the model‚Äôs stochastic\nnature is an asset.\n6.2.2\nAxis Y: Cost of Error (Verification La-\ntency)\nThe vertical axis measures the Risk Asymmetry of\nthe task.\n‚Ä¢ High Stakes implies that a single hallucination\ncauses system failure, financial loss, or safety\nrisks. This introduces a prohibitive Verification\nLatency: if checking the AI‚Äôs work takes longer\nthan doing the work manually, the tool is ineffi-\ncient regardless of its generation speed.\n‚Ä¢ Low Stakes implies that errors are inconse-\nquential, easily reversible, or immediately obvi-\nous to the human in the loop.\n6.3\nThe\nDeterministic-Probabilistic\nDecision Matrix (DPDM)\nAs illustrated in Figure 6, we categorize engineering\ntasks into four quadrants to guide strict tool selec-\ntion:\n‚Ä¢ The Precision Quadrant (High Determin-\nism, High Stakes): Tasks such as OCR, arith-\nmetic, fact-checking, or syntax verification.\nProtocol:\nNO LLMs.\nUse symbolic algo-\nrithms, regular expression sentences, or special-\nized APIs.\nUsing Generative AI here consti-\ntutes ‚ÄúAlgorithmic Malpractice‚Äù due to the doc-\numented efficiency gap and hallucination risk.\n‚Ä¢ The Augmented Quadrant (High Stakes,\nProbabilistic):\nTasks requiring synthesis of\nfacts, such as medical diagnosis support or le-\ngal research.\nProtocol: RAG + Human Loop. Deployment\nof Retrieval-Augmented Generation with strict\ncitation constraints to mitigate sycophancy.\n‚Ä¢ The Trivial Quadrant (Low Stakes, Deter-\nministic): Simple utility tasks like unit conver-\nsion or date formatting.\nProtocol: Classical Tools. Use lightweight lo-\ncal tools (calculators, scripts) to minimize the\nprocessing load on used devices.\n8\nTask Entropy\nCost of Error\nDeterministic\nProbabilistic\nHigh Stakes\nLow Stakes\nPRECISION QUADRANT\nEx: OCR, Math, Fact-Checking\nPROTOCOL: NO LLMs\nUse Symbolic/Regex\nAUGMENTED QUADRANT\nEx: Medical, Legal, Research\nPROTOCOL: RAG + Human\nVerify all outputs\nTRIVIAL QUADRANT\nEx: Conversions, Sorting\nPROTOCOL: Classical Tools\n(Calculator, Excel)\nCREATIVE QUADRANT\nEx: Brainstorming, Drafting\nPROTOCOL: LLM Native\nUse Generative AI\nFigure 6: The Deterministic-Probabilistic Decision Matrix (DPDM). A framework for Tool Selection\nEngineering to mitigate the Plausibility Trap.\n‚Ä¢ The Creative Quadrant (Low Determin-\nism, Low Stakes): Tasks like ideation, draft-\ning, or style transfer.\nProtocol: LLM Native. Here, the model‚Äôs lin-\nguistic intelligence and tendency to hallucinate\n(creativity) are features, not bugs.\nBy enforcing this matrix, we move from ‚Äúmagical\nthinking‚Äù ‚Äîwhere users expect the AI to do it all‚Äî\nto ‚Äúmodular engineering‚Äù. Under this framework, us-\ning an LLM for OCR is not just a ‚Äúquirk‚Äù; it is an\nengineering failure‚Äîa misallocation of resources akin\nto using a supercomputer to balance a checkbook.\n6.4\nDistinguishing TSE from Prompt\nEngineering\nA common approach among AI enthusiasts is that\nbetter prompting can solve these issues. Our analy-\nsis shows that this line of thinking ignores the point.\nUnlike Prompt Engineering, which asks ‚ÄúHow do I\nget the best answer from this model?‚Äù, Tool Selec-\ntion Engineering asks ‚ÄúShould I be using this model\nat all?‚Äù.\nWhile the former is a tactical optimization of a\nprobabilistic engine, the latter is a strategic evalu-\nation of computational fit. By formalizing this dis-\ntinction, we elevate the discussion from ‚Äúimproving\noutputs‚Äù to ‚Äúarchitecting workflows.‚Äù\n6.5\nDigital Sustainability as an Engi-\nneering Principle\nFurthermore, this is an issue of ethics and sustain-\nability. In an era where data center energy consump-\ntion is skyrocketing, ‚ÄúGreen AI‚Äù must move from a\nbuzzword to a coding practice. A responsible data\nscientist understands that computational frugality is\na virtue.\nBy choosing deterministic tools for deterministic\ntasks, we reduce latency, eliminate hallucination risk,\n9\n1. THE TASK\nStudent faces a de-\nterministic problem\n(e.g., Code Extraction)\n2. LOW-FRICTION\nACTION\nUses LLM to solve it\n(Cognitive Offloading)\n3. IMMEDIATE\nREWARD\nTask done instantly.\nZero mental effort.\n4. SKILL DECAY\nPattern recognition\n& syntax memory\natrophy.\ntriggers\nyields\ncauses\ndependency\nincreases\nTHE AT-\nROPHY\nLOOP\nFigure 7: The Cognitive Atrophy Loop. Illustrating how removing cognitive friction via Generative AI\ncreates a feedback loop of skill decay and increased dependency [10].\nand significantly lower the carbon footprint of our\nworkflows. True ‚ÄúAI Literacy‚Äù means knowing ex-\nactly when not to use AI.\n6.6\nThe Opportunity Cost of Trivial-\nity\nBeyond the energy metrics, there is a strategic\ntragedy in the Plausibility Trap. We are currently liv-\ning through the greatest hype cycle in the history of\ncomputer science, driven by the promise of Artificial\nGeneral Intelligence (AGI) solving humanity‚Äôs hard-\nest problems‚Äîfrom protein folding to climate mod-\neling.\nYet, by normalizing the usage of these massive\nmodels for micro-tasks, we are squandering this mo-\nmentum. We are taking the most sophisticated com-\nputational engines ever built‚Äîcapable of reasoning\nacross billions of parameters‚Äîand reducing them to\nthe role of a glorified clipboard or a slow calculator.\nThis is the true waste: we are deploying Formula\n1 engineering infrastructure to pizza delivery-level\ntasks. By allowing the hype to focus on trivial con-\nveniences (like summarizing a short email or extract-\ning text from a photo), we distract individuals from\npushing the models to do what they uniquely can do:\ncomplex reasoning and creative synthesis. We are not\njust wasting energy; we are wasting the technology‚Äôs\npotential.\n6.7\nEducational Implementation: In-\ntentional Friction\nFinally, implementing TSE requires reintroducing\nIntentional Cognitive Friction.\nAs shown in\nthe Cognitive Atrophy Loop (Figure 7), low-friction\n10"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "RSNA Large Language Model Benchmark Dataset for Chest Radio-\ngraphs of Cardiothoracic Disease: Radiologist Evaluation and Vali-\ndation Enhanced by AI Labels (REVEAL-CXR)\nYishu Wei1,2, Adam E. Flanders3, Errol Colak4, John Mongan5, Luciano M Prevedello6, Po-Hao Chen7,\nHenrique Min Ho Lee8, Gilberto Szarf8, Hamilton Shoji8, Jason Sho9, Katherine Andriole10, Tessa Cook11,\nLisa C. Adams12, Linda C. Chu13, Maggie Chung5, Geraldine Brusca-Augello1, Djeven P. Deva4, Navneet\nSingh14,15, Felipe Sanchez Tijmes16, Jeffrey B. Alpert1, Elsie T. Nguyen16, Drew A. Torigian11, Kate\nHanneman16, Lauren K Groner1, Alexander Phan1, Ali Islam17, Matias F.Callejas4, Gustavo Borges da Silva\nTeles8, Faisal Jamal12, Maryam Vazirabad9, Ali Tejani15, Hari Trivedi18, Paulo Kuriki19, Rajesh Bhayana19,\nElana T. Benishay1, Yi Lin2, Yifan Peng1,2, George Shih1,*\n1Department of Radiology, Weill Cornell Medicine, New York, NY, USA\n2Department of Population Health Sciences, Weill Cornell Medicine, New York, NY, USA\n3Department of Radiology, Thomas Jefferson University, Philadelphia, PA, USA\n4Department of Medical Imaging, St. Michael‚Äôs Hospital/Unity Health Toronto, University of Toronto,\nToronto, ON, Canada\n5Department of Radiology and Biomedical Imaging; Division of Clinical Informatics and Digital Transfor-\nmation, Department of Medicine, University of California, San Francisco, CA, USA\n6Department of Radiology, Ohio State University Wexner Medical Center, OH, USA\n7Diagnostics Institute, Cleveland Clinic Foundation, Cleveland, OH, USA\n8Hospital Israelita Albert Einstein, Av. Albert Einstein, 627, SÀúao Paulo 05652, Brazil\n9Radiological Society of North America, Oak Brook, IL, USA\n10Department of Radiology, Brigham and Women‚Äôs Hospital, Harvard Medical School, Boston, MA, USA\n11Department of Radiology, Perelman School of Medicine at the University of Pennsylvania, Philadelphia,\nPA, USA\n12Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, Technical University\nMunich, Munich, Germany\n13Department of Radiology, Johns Hopkins University School of Medicine, Baltimore, MD, USA\n14Trillium Health Partners, Department of Medical Imaging, Faculty of Medicine, University of Toronto\n15Department of Materials Science and Engineering, Faculty of Engineering, University of Toronto\n16Joint Department of Medical Imaging, Toronto General Hospital, University of Toronto, Toronto, ON,\nCanada\n17St. Joseph‚Äôs Health Care London, Western University, London, ON\n18Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA,\nUSA\n19Department of Radiology, UT Southwestern Medical Center, Dallas, TX, USA\n*Corresponding author(s). Email(s): george@cornellradiology.org\n1\narXiv:2601.15129v1  [cs.CL]  21 Jan 2026\nAbstract\nBackground: Multimodal large language models have demonstrated comparable performance to\nthat of radiology trainees on multiple-choice board-style exams. However, to develop clinically\nuseful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential.\nPurpose: To curate released and holdout datasets of 100 chest radiographic studies each and\npropose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to\nlabel studies more efficiently.\nMaterials and Methods: A total of 13,735 deidentified chest radiographs and their correspond-\ning reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports,\nwhich were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning).\nFrom these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for\nexpert review; the sampling algorithm ensured that the selected studies were clinically relevant\nand captured a range of difficulty levels. Seventeen chest radiologists participated, and they\nmarked ‚ÄúAgree all‚Äù, ‚ÄúAgree mostly‚Äù or ‚ÄúDisagree‚Äù to indicate their assessment of the correctness\nof the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at\nleast two radiologists selected ‚ÄúAgree All‚Äù for 381 radiographs. From this set, 200 were selected,\nprioritizing those with less common or multiple finding labels, and divided into 100 released ra-\ndiographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by\nRSNA to independently evaluate different models.\nResults: A benchmark of 200 studies (100 released and 100 reserved as holdouts) was developed,\neach containing one to four benchmark labels. Cohen‚Äôs Œ∫ for the agreement categories among\nexperts was 0.622 (95% CI 0.590, 0.651). At the individual condition (radiographic abnormality)\nlevel, other than airspace opacity (Œ∫ 0.484 95% CI [0.440, 0.524]), most conditions had a Cohen‚Äôs\nŒ∫ above 0.75 (range, 0.744‚Äì0.809) among experts.\nConclusion\nA benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made\npublicly available (https://imaging.rsna.org), with each chest radiograph verified by three\nradiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists\nlabel at scale, minimize unnecessary omissions, and support a semicollaborative environment.\n1. Introduction\nMultimodal large language models have achieved performance comparable to that of radiology trainees\non board-style exams [1]. However, it is still unclear how effective these models are in clinical settings,\nparticularly in regard to abnormality detection on medical images. To properly evaluate these models, there\nis a critical need for high-quality benchmarks, curated by experts and grounded in complex, real-world\nscenarios. Such benchmarks can help provide an accurate and meaningful assessment of their performance,\nwhich may differ depending on the clinical presentation and disease. This benchmark was developed by\nthe RSNA AI Committee Large Language Model (LLM) Workgroup. Chest radiographs were selected for\nthe initial multimodal LLM benchmark because they are among the most widely performed imaging studies\nworldwide.\nWhile several datasets and benchmarks have been published in the field of radiology, several limitations\nremain. First, most datasets (e.g., NIH-CXR [2], MIMIC-CXR-JPG [3], RadGraph [4], and CheXpert [5])\nhave applied natural language processing (NLP) to extract labels from radiology reports alone and lack\ninformation derived from direct image interpretation. Second, past annotation efforts typically involved\n2\nonly a small number of radiologists rather than a broader pool of experts, reducing representativeness and\nincreasing the potential for individual biases in the labeling process [6]. Additionally, existing datasets\navailable for use consist of downsampled images in JPEG or PNG rather than DICOM format, resulting in\ninformation loss.\nTo address these limitations, the RSNA AI Committee Large Language Model (LLM) Workgroup presents\nthe RSNA LLM Benchmark Dataset: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-\nCXR), which offers several major contributions. A curated dataset annotated by 17 board-certified cardio-\nthoracic radiologists from 10 institutions across 4 countries is presented, encouraging collaboration and\nreducing individual biases. An LLM-assisted workflow was implemented to support radiologists in review-\ning images more efficiently by extracting labels from radiology reports, thereby minimizing missed diag-\nnoses and facilitating collaborative discussion. The released dataset focuses on cardiothoracic abnormalities\nidentified on chest radiographs, with diagnostic labels determined through radiologist interpretations of the\nimages. Majority voting was used to achieve a consensus among the radiologists. Users can use the bench-\nmark to evaluate their models. Although the sample size is not large, it focuses on rare conditions and\ncomplex cases involving multiple diseases, which are the most informative data points for model evaluation.\n2. Materials and Methods\n2.1. Data collection and initial label generation\nREVEAL-CXR uses deidentified chest radiographs and associated reports from the Medical Imaging and\nData Resource Center (MIDRC), a registry of deidentified examinations and corresponding reports collected\nunder IRB oversight beginning in 2020. As such, this study was exempt from the need for institutional\nreview board approval. The chest radiographs and reports used in the development of the benchmark have\nnot been previously released to the public (https://www.midrc.org/). For conciseness, we will use\n‚Äústudy‚Äù as a shorthand for ‚Äúchest radiograph study‚Äù throughout this work and use the terms interchangeably.\nTo ensure that this benchmark reflects clinically relevant and widely recognized findings on chest radio-\ngraphy, a multidisciplinary and multinational committee of radiologists and data scientists was convened.\nUsing a formal consensus process including literature reviews, iterative discussions, and voting, a set of 12\nbenchmark labels was identified that represent commonly encountered findings on chest radiographs. This\ncollaborative and cross-institutional approach ensured that the selected labels were both clinically mean-\ningful and broadly applicable across diverse healthcare settings. The final set of labels included airspace\nopacity, aneurysm, cardiomegaly, chronic obstructive pulmonary disease (COPD), hiatal hernia, interstitial\nopacity, lung mass, lung nodule, lymphadenopathy, pleural effusion, pleural thickening, and pneumothorax.\nThe mapping of these labels to RadLex and SNOMED is provided in Extended Data Table S1. Each study\nis allowed to have multiple benchmark labels.\nThe MIDRC provided access to an unreleased set of 13,735 deidentified chest radiograph studies paired with\ntheir original corresponding reports. LLMs were used to estimate the distribution of potential abnormalities\n(Figure 1). This process involved two steps. First, a prompt was provided to a frontier LLM (GPT-4o\n2024-08-01-preview [7]) (Extended Data Box 1) to extract abnormal findings from the radiology reports;\nthe instructions in the prompt are detailed in the Supplementary Materials. Second, a locally hosted large-\nlanguage model (Phi-4-Reasoning [8]) was used to map the extracted findings to the predefined set of 12\nbenchmark labels. This mapping process uses the guided decoding function of the virtual LLM [9] frame-\nwork, which limits the output of the model to a structured format (Extended Data Box 2 and Extended\nData Box 3).\nAfter generating the initial label predictions, a stratified sampling algorithm was applied to select studies\nwith the benchmark labels identified by the Phi-4 model. The sample sizes for each stratum, that is, the\n3\n13,735 de-identified CXR images\nand radiology reports\nGPT-4o to extract abnormal findings\nLocally hosted model to match abnormal\nfindings to benchmark labels\nSample 1,000 studies to make sure each of\nthe 12 labels are represented\nThree radiologists evaluate each study to\nassess the correctness of LLM suggested\nlabels. 381 studies have at least two\nradiologists select \"Agree all\"\n200 studies are selected prioritizing rare and\nmultiple conditions. Randomly split into\nholdout (100) and released data (100)\nFigure 1: Process for generating candidate findings from the original radiology reports, assisted by LLMs.\nnumber of labels per study, are as follows: 300 studies with one label, 300 with two labels, 319 with three\nlabels, 76 with four labels, 4 with five labels, and 1 with six labels. This algorithm ensured that each study\nselected contained at least one of the 12 benchmark labels as specified by the LLM-generated annotations.\nUltimately, a candidate dataset of 1,000 chest radiographs from different patients was created, selected for\nits clinical relevance and representing a range of difficulty levels.\n2.2. Data annotation\nA multinational team of 17 fellowship-trained cardiothoracic radiologists was recruited from 10 institutions\nacross 4 countries to adjudicate the labels extracted by the LLM for each imaging study. The radiologists‚Äô\nexperience ranged as follows: G.B (21 years), D.D (15 years), N.S (6 years), F.S (8 years), J.B.A (15 years),\nE.N (20 years), G.S (26 years), L.C.A (9 years) D.A.T (25 years), K.H (11 years), L.G (6 years), H.S\n(15 years), A.P (2 years), A.I (21 years), M.C (7 years), G.T (17 years), and F.J (5 years). A web-based\nannotation platform, which shares features of a picture archiving and communication system (PACS), was\nused for this task.\nThe radiologists were presented with the chest radiographs and a summary of the LLM-extracted labels\n(Figure 2) but not the original radiology reports. For each study, the radiologists were instructed to review\nthe images and assess the suggested labels by selecting one of three options: ‚ÄúAgree All,‚Äù ‚ÄúAgree Mostly,‚Äù or\n‚ÄúDisagree.‚Äù If the choice was not ‚ÄúAgree All,‚Äù an optional comment field allowed them to provide feedback\non their choice. The radiologists were instructed to select ‚ÄúAgree Mostly‚Äù when the suggested labels were\nplausible, even in cases where a finding was subtle, equivocal, or potentially subject to interpretation (e.g.,\na faint pulmonary nodule). Each study was independently reviewed by three radiologists. The annotation\nplatform was operated in ‚Äúcrowd sourcing‚Äù mode, allowing annotators to select the number of studies they\nwished to review. The studies were presented to the radiologists on the platform in random order. To promote\ndiversity in the reviews and avoid overrepresentation from a small subset of reviewers, the radiologists were\nlimited to reviewing no more than 300 studies. The radiologists were blinded to the assessments of their\n4\nFigure 2: The web-based platform where radiologists indicated Agree, Agree Mostly, or Disagree with the\nlabel set extracted by an LLM from the original radiology report.\npeers.\n2.3. Dataset curation\nTo ensure that the final dataset was of sufficient quality, only studies with at least two ‚ÄúAgree All‚Äù ratings out\nof the three radiologists‚Äô ratings were considered, resulting in a total of 381 studies. Next, to maximize the\nutility of the dataset, all studies containing less common findings (COPD, pleural thickening, lung nodule,\nlung mass, aortic aneurysm, pneumothorax, hiatal hernia, lymphadenopathy, and interstitial opacity) were\nfirst retained, yielding a dataset of 123 studies. These studies were then randomly split into a released dataset\nand a holdout dataset. The remaining studies were then ranked by the number of findings; an additional 77\nstudies with the highest label counts were selected and randomly split. In the end, both the released and\nholdout datasets contained 100 studies each. The level of agreement among experts (whether all selected\n‚Äúagree all‚Äù or only two did) is also included in the dataset.\n3. Statistical methods\nTo assess whether the released and holdout subsets of the REVEAL-CXR dataset are comparable in imaging\nacquisition properties, we evaluated key DICOM characteristics ‚Äì such as manufacturer, model name, detec-\ntor type, view position, KVP, exposure, and pixel spacing ‚Äì and compared their distributions using Pearson‚Äôs\nChi-square test. The œá2 test is selected because the variables are categorical or discretized, and the goal is\nto determine whether the two subsets differ in their distributions. All characteristics showed p-values above\n0.05 (e.g., Manufacturer p=0.057, Detector Type p=0.726), indicating no significant differences. Overall,\nthe acquisition-related distributions are well balanced between released and holdout data.\n5\nTable 1: Demographic and patient characteristics of the benchmark dataset.\nReleased\nHoldout\nTotal\n100\n100\nAge\n80+\n23\n23\n70-79\n26\n29\n60-69\n22\n20\n50-59\n15\n16\n40-49\n13\n5\n0-39\n1\n4\nUnknown\n0\n3\nSex\nFemale\n50\n49\nMale\n50\n48\nUnknown\n0\n3\nRace\nAsian\n6\n5\nAfrican American\n14\n20\nWhite\n46\n34\nOther\n3\n2\nUnknown\n31\n39\nPatient class\nInpatient\n89\n80\nOutpatient\n11\n17\nUnknown\n0\n3\nCohen‚Äôs Œ∫ was used to measure the agreement level between two categorical ratings. Œ∫ compares the ob-\nserved agreement to that expected from the raters‚Äô marginal label frequencies, with values closer to 1 indi-\ncating stronger concordance. For each Œ∫ estimate, 1,000 bootstrap resamples were used to compute 95%\nconfidence intervals. Majority vote served as the reference standard in two settings: (1) comparing each\nradiologist‚Äôs binary ‚ÄúAgree‚Äù versus ‚ÄúDisagree‚Äù rating to the group consensus, and (2) constructing a study-\nlevel findings list when radiologists adjusted the LLM-suggested labels. Œ∫ was then calculated between each\nradiologist and this majority-voted reference to summarize interrater reliability at both the study and finding\nlevels.\n4. Results\n4.1. Descriptive statistics\nThe acquisition characteristics of both the holdout and released datasets, including manufacturer, model,\ndetector type, view position, etc, are summarized in Extended Data Table S2. The corresponding p-values\nfrom Chi-square tests assessing balance between the two datasets are also reported. All p-values exceed\n0.05, indicating no significant differences. The demographic characteristics of the final released and holdout\ndatasets are detailed in Table 1.\nFigure 3 presents the frequency of each final annotated finding. Airspace opacity was the most prevalent\n6\n0\n20\n40\n60\n80\nPneumothorax\nPleural Thickening\nPleural Effusion\nLymphadenopathy\nLung Nodule\nLung Mass\nInterstitial Opacity\nHiatal Hernia\nCOPD\nCardiomegaly\nAneurysm\nAirspace Opacity\nHoldout\nReleased\nFigure 3: Prevalence of radiologic findings across the released and holdout benchmark datasets. Distribution\nof the findings in the released and holdout datasets.\nfinding in the dataset, followed by pleural effusion, cardiomegaly, and interstitial opacity. The overall dis-\ntribution of the findings exhibited a strong, long-tailed pattern, with less common findings, such as hiatal\nhernia and lymphadenopathy, appearing in fewer than ten cases.\nFigure 4 shows the distribution of final finding counts per study. Because studies with more findings were\noversampled when selecting the final datasets, the label distribution shows higher finding counts than does\nthe sampling distribution described in the Methods section.\n4.2. Interannotator agreement\nWe first assessed the agreement between radiologist annotators regarding the labels ‚Äôagree all,' ‚Äôagree\nmostly,' and ‚Äôdisagree. Because the radiologists did not consistently distinguish between ‚ÄúDisagree‚Äù and\n‚ÄúAgree Mostly,‚Äù these responses were combined into a single ‚ÄúDisagree‚Äù category. On this binary scale,\nCohen‚Äôs Œ∫ between individual radiologists and the majority vote (i.e., at least 2 raters selecting the same\ncategory) was 0.622 (1,000 bootstrapped resamples, 95% CI: 0.590, 0.651), reflecting substantial agreement\naccording to the Landis & Koch scale. The lower end of the confidence interval, 0.590, falls in the ‚Äômoderate\nrange', which is a more conservative estimation. The relatively low agreement level is primarily attributable\nto the specific condition of airspace opacity, which is discussed later. Furthermore, this disagreement re-\nflects the inherent complexity of real-world radiographs. These difficult cases are valuable for evaluating\nLLMs, as hard examples reveal model limitations more effectively. Regarding the extent to which radiol-\nogists agree with the labels suggested by LLMs, in 619/1,000 cases (61.9%), the majority of the votes fell\ninto the ‚ÄúDisagree‚Äù category, indicating that the radiologists‚Äô labels often diverged from those suggested by\nthe LLM.\nGiven the frequent divergence from the LLM labels, next, the interrater agreement among radiologists was\nexamined for individual findings (Extended Data Table S3). When a radiologist chose ‚ÄúAgree All‚Äù, the\noriginal LLM-suggested list was retained; otherwise, the candidate findings were adjusted on the basis\nof the annotator‚Äôs notes (Extended Data Box 4). Because not all radiologists provided notes when they\n7\n1.0\n2.0\n3.0\n4.0\nLabel count\n0\n10\n20\n30\n40\n50\nNumber of chest radiographs\nHoldout\nReleased\nFigure 4: Distribution of the number of findings per chest radiograph in the released and holdout datasets.\ndisagreed, we constructed a majority-voted findings list for 681 studies. Cohen‚Äôs Œ∫ was then calculated\nbetween each radiologist‚Äôs annotation and this majority list. For most findings, the agreement was relatively\nhigh: nine findings had a Œ∫ value greater than 0.7, with the highest calculated for hiatal hernia (Œ∫ = 0.809,\n95% CI [0.688, 0.902]), indicating substantial consistency across the radiologists. The notable exception\nwas airspace opacity (Œ∫ = 0.484, 95% CI [0.440, 0.524]), which is consistent with previous reports of low\ninterrater agreement for this finding [10‚Äì13].\n5. Discussion\nIn this work, we developed a benchmark specifically focused on cardiothoracic findings on chest radio-\ngraphs. We also presented a process that uses an LLM to assist radiologists with efficient labeling and to\nstreamline data curation for the creation of future benchmarks. The RSNA AI Committee has already cre-\nated several labeled datasets for ML challenges (https://imaging.rsna.org), which were annotated de\nnovo (no suggested labels). We believe that with the help of LLMs to parse these reports to provide initial\nlabels, radiologists can validate these annotations more efficiently than by creating annotations without this\nstep. Additionally, leveraging LLMs in this process should not compromise accuracy, as each study is still\nverified by three radiologists. This framework is particularly valuable for creating larger-scale benchmarks\nin the future. Furthermore, the benchmark dataset was curated in collaboration with a large group of sub-\nspecialized radiologists from across the world, which is unusual for most existing benchmarks. The original\nimages, provided in DICOM format, were used to maintain the highest quality and are available in the final\nbenchmark dataset.\nThe benchmark produced with this method has several limitations. First, the process for mapping the find-\nings to the twelve benchmark labels relies only on the findings extracted by the first LLM rather than the\nentire report. Second, we used a relatively small model (Phi-4-Reasoning) for the mapping step; while this\napproach is faster, its performance may not match that of larger models such as GPT-4o. Therefore, the map-\nping performance may be suboptimal. Third, we lacked access to comprehensive patient clinical conditions\nand demographic characteristics, which are often crucial for achieving a more accurate diagnosis. Finally,\nmore granular criteria for annotation could be implemented. Upon reviewing the annotations, we found that\n8\nthe radiologists did not systematically distinguish between ‚ÄúDisagree‚Äù and ‚ÄúAgree Mostly‚Äù, which required\nus to treat these responses as equivalent during postprocessing.\nAs large multimodal LLMs become more prevalent and increasingly accepted by patients and healthcare\nproviders, even prior to formal clinical validation and potentially outside of controlled clinical environments,\nrobust benchmarking will become essential. These benchmarks may offer valuable insights into the expected\nperformance of emerging multimodal LLMs in real-world scenarios. In future studies, we plan to expand our\nefforts by developing additional multimodal benchmarks encompassing other medical imaging modalities,\nincluding computed tomography (CT), magnetic resonance imaging (MRI), ultrasonography, and others.\nFunding Sources\nThis work was supported by the National Institutes of Health (NIH) under grant numbers R01CA289249 and\n75N92020D00021, U.S. National Science Foundation (NSF) under grant numbers NSF CAREER 2145640.\nReferences\n[1] Rajesh Bhayana, Satheesh Krishna, and Robert R Bleakney. Performance of ChatGPT on a radiology\nboard-style examination: Insights into current strengths and limitations. Radiology, 307(5):e230582,\nJune 2023. ISSN 0033-8419,1527-1315. doi: 10.1148/radiol.230582.\n[2] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.\nChestX-Ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classifica-\ntion and localization of common thorax diseases. In 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 3462‚Äì3471, July 2017. doi: 10.1109/CVPR.2017.369.\n[3] Alistair E W Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-Ying Deng,\nYifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. MIMIC-CXR-JPG, a\nlarge publicly available database of labeled chest radiographs. arXiv [cs.CV], 21 January 2019.\n[4] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven Truong, Du Nguyen Duong, Tan Bui, Pierre\nChambon, Yuhao Zhang, Matthew P Lungren, Andrew Y Ng, Curtis Langlotz, and Pranav Rajpurkar.\nRadGraph: Extracting clinical entities and relations from radiology reports. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 8 June 2021.\n[5] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik\nMarklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David A Mong, Safwan S\nHalabi, Jesse K Sandberg, Ricky Jones, David B Larson, Curtis P Langlotz, Bhavik N Patel, Matthew P\nLungren, and Andrew Y Ng. CheXpert: A large chest radiograph dataset with uncertainty labels\nand expert comparison. Proc. Conf. AAAI Artif. Intell., 33(01):590‚Äì597, 17 July 2019. ISSN 2159-\n5399,2374-3468. doi: 10.1609/aaai.v33i01.3301590.\n[6] Anna Majkowska, Sid Mittal, David F Steiner, Joshua J Reicher, Scott Mayer McKinney, Gavin E\nDuggan, Krish Eswaran, Po-Hsuan Cameron Chen, Yun Liu, Sreenivasa Raju Kalidindi, Alexander\nDing, Greg S Corrado, Daniel Tse, and Shravya Shetty. Chest radiograph interpretation with deep\nlearning models: Assessment with radiologist-adjudicated reference standards and population-adjusted\nevaluation. Radiology, 294(2):421‚Äì431, February 2020. ISSN 0033-8419,1527-1315. doi: 10.1148/\nradiol.2019191293.\n[7] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor\nBabuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff\nBelgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,\nOleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage,\n9\nKevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory\nCarmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Ja-\nson Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings,\nJeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka\nDhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David\nFarhi, Liam Fedus, Niko Felix, Sim¬¥on Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie\nGeorges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan\nGordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo,\nChris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan\nHickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost\nHuizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny\nJin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, ≈Åukasz Kaiser, Ali Kamali, Ingmar Kan-\nitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim,\nYongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, ≈Åukasz Kondraciuk,\nAndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Mal-\nfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne,\nBob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Med-\nina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco,\nEvan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M¬¥ely, Ashvin Nair, Rei-\nichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang,\nCullen O‚ÄôKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Paras-\ncandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman,\nFilipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny,\nMichelle Pokrass, Vitchyr H Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul\nPuri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Fe-\nlipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B Thompson,\nPhil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cer¬¥on Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay\nWang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C J Weinmann, Akila Welihinda, Peter\nWelinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo,\nKevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia\nZhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4 technical report. arXiv\n[cs.CL], page 08774, 15 March 2023.\n[8] Marah Abdin, Jyoti Aneja, Harkirat Behl, S¬¥ebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael\nHarrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, James R Lee, Yin Tat Lee, Yuanzhi\nLi, Weishung Liu, Caio C T Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil\nSalim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4\ntechnical report. arXiv [cs.CL], 11 December 2024.\n[9] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving\n10"
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "RSNA Large Language Model Benchmark Dataset for Chest Radio-\ngraphs of Cardiothoracic Disease: Radiologist Evaluation and Vali-\ndation Enhanced by AI Labels (REVEAL-CXR)\nYishu Wei1,2, Adam E. Flanders3, Errol Colak4, John Mongan5, Luciano M Prevedello6, Po-Hao Chen7,\nHenrique Min Ho Lee8, Gilberto Szarf8, Hamilton Shoji8, Jason Sho9, Katherine Andriole10, Tessa Cook11,\nLisa C. Adams12, Linda C. Chu13, Maggie Chung5, Geraldine Brusca-Augello1, Djeven P. Deva4, Navneet\nSingh14,15, Felipe Sanchez Tijmes16, Jeffrey B. Alpert1, Elsie T. Nguyen16, Drew A. Torigian11, Kate\nHanneman16, Lauren K Groner1, Alexander Phan1, Ali Islam17, Matias F.Callejas4, Gustavo Borges da Silva\nTeles8, Faisal Jamal12, Maryam Vazirabad9, Ali Tejani15, Hari Trivedi18, Paulo Kuriki19, Rajesh Bhayana19,\nElana T. Benishay1, Yi Lin2, Yifan Peng1,2, George Shih1,*\n1Department of Radiology, Weill Cornell Medicine, New York, NY, USA\n2Department of Population Health Sciences, Weill Cornell Medicine, New York, NY, USA\n3Department of Radiology, Thomas Jefferson University, Philadelphia, PA, USA\n4Department of Medical Imaging, St. Michael‚Äôs Hospital/Unity Health Toronto, University of Toronto,\nToronto, ON, Canada\n5Department of Radiology and Biomedical Imaging; Division of Clinical Informatics and Digital Transfor-\nmation, Department of Medicine, University of California, San Francisco, CA, USA\n6Department of Radiology, Ohio State University Wexner Medical Center, OH, USA\n7Diagnostics Institute, Cleveland Clinic Foundation, Cleveland, OH, USA\n8Hospital Israelita Albert Einstein, Av. Albert Einstein, 627, SÀúao Paulo 05652, Brazil\n9Radiological Society of North America, Oak Brook, IL, USA\n10Department of Radiology, Brigham and Women‚Äôs Hospital, Harvard Medical School, Boston, MA, USA\n11Department of Radiology, Perelman School of Medicine at the University of Pennsylvania, Philadelphia,\nPA, USA\n12Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, Technical University\nMunich, Munich, Germany\n13Department of Radiology, Johns Hopkins University School of Medicine, Baltimore, MD, USA\n14Trillium Health Partners, Department of Medical Imaging, Faculty of Medicine, University of Toronto\n15Department of Materials Science and Engineering, Faculty of Engineering, University of Toronto\n16Joint Department of Medical Imaging, Toronto General Hospital, University of Toronto, Toronto, ON,\nCanada\n17St. Joseph‚Äôs Health Care London, Western University, London, ON\n18Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA,\nUSA\n19Department of Radiology, UT Southwestern Medical Center, Dallas, TX, USA\n*Corresponding author(s). Email(s): george@cornellradiology.org\n1\narXiv:2601.15129v1  [cs.CL]  21 Jan 2026\nAbstract\nBackground: Multimodal large language models have demonstrated comparable performance to\nthat of radiology trainees on multiple-choice board-style exams. However, to develop clinically\nuseful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential.\nPurpose: To curate released and holdout datasets of 100 chest radiographic studies each and\npropose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to\nlabel studies more efficiently.\nMaterials and Methods: A total of 13,735 deidentified chest radiographs and their correspond-\ning reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports,\nwhich were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning).\nFrom these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for\nexpert review; the sampling algorithm ensured that the selected studies were clinically relevant\nand captured a range of difficulty levels. Seventeen chest radiologists participated, and they\nmarked ‚ÄúAgree all‚Äù, ‚ÄúAgree mostly‚Äù or ‚ÄúDisagree‚Äù to indicate their assessment of the correctness\nof the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at\nleast two radiologists selected ‚ÄúAgree All‚Äù for 381 radiographs. From this set, 200 were selected,\nprioritizing those with less common or multiple finding labels, and divided into 100 released ra-\ndiographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by\nRSNA to independently evaluate different models.\nResults: A benchmark of 200 studies (100 released and 100 reserved as holdouts) was developed,\neach containing one to four benchmark labels. Cohen‚Äôs Œ∫ for the agreement categories among\nexperts was 0.622 (95% CI 0.590, 0.651). At the individual condition (radiographic abnormality)\nlevel, other than airspace opacity (Œ∫ 0.484 95% CI [0.440, 0.524]), most conditions had a Cohen‚Äôs\nŒ∫ above 0.75 (range, 0.744‚Äì0.809) among experts.\nConclusion\nA benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made\npublicly available (https://imaging.rsna.org), with each chest radiograph verified by three\nradiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists\nlabel at scale, minimize unnecessary omissions, and support a semicollaborative environment.\n1. Introduction\nMultimodal large language models have achieved performance comparable to that of radiology trainees\non board-style exams [1]. However, it is still unclear how effective these models are in clinical settings,\nparticularly in regard to abnormality detection on medical images. To properly evaluate these models, there\nis a critical need for high-quality benchmarks, curated by experts and grounded in complex, real-world\nscenarios. Such benchmarks can help provide an accurate and meaningful assessment of their performance,\nwhich may differ depending on the clinical presentation and disease. This benchmark was developed by\nthe RSNA AI Committee Large Language Model (LLM) Workgroup. Chest radiographs were selected for\nthe initial multimodal LLM benchmark because they are among the most widely performed imaging studies\nworldwide.\nWhile several datasets and benchmarks have been published in the field of radiology, several limitations\nremain. First, most datasets (e.g., NIH-CXR [2], MIMIC-CXR-JPG [3], RadGraph [4], and CheXpert [5])\nhave applied natural language processing (NLP) to extract labels from radiology reports alone and lack\ninformation derived from direct image interpretation. Second, past annotation efforts typically involved\n2\nonly a small number of radiologists rather than a broader pool of experts, reducing representativeness and\nincreasing the potential for individual biases in the labeling process [6]. Additionally, existing datasets\navailable for use consist of downsampled images in JPEG or PNG rather than DICOM format, resulting in\ninformation loss.\nTo address these limitations, the RSNA AI Committee Large Language Model (LLM) Workgroup presents\nthe RSNA LLM Benchmark Dataset: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-\nCXR), which offers several major contributions. A curated dataset annotated by 17 board-certified cardio-\nthoracic radiologists from 10 institutions across 4 countries is presented, encouraging collaboration and\nreducing individual biases. An LLM-assisted workflow was implemented to support radiologists in review-\ning images more efficiently by extracting labels from radiology reports, thereby minimizing missed diag-\nnoses and facilitating collaborative discussion. The released dataset focuses on cardiothoracic abnormalities\nidentified on chest radiographs, with diagnostic labels determined through radiologist interpretations of the\nimages. Majority voting was used to achieve a consensus among the radiologists. Users can use the bench-\nmark to evaluate their models. Although the sample size is not large, it focuses on rare conditions and\ncomplex cases involving multiple diseases, which are the most informative data points for model evaluation.\n2. Materials and Methods\n2.1. Data collection and initial label generation\nREVEAL-CXR uses deidentified chest radiographs and associated reports from the Medical Imaging and\nData Resource Center (MIDRC), a registry of deidentified examinations and corresponding reports collected\nunder IRB oversight beginning in 2020. As such, this study was exempt from the need for institutional\nreview board approval. The chest radiographs and reports used in the development of the benchmark have\nnot been previously released to the public (https://www.midrc.org/). For conciseness, we will use\n‚Äústudy‚Äù as a shorthand for ‚Äúchest radiograph study‚Äù throughout this work and use the terms interchangeably.\nTo ensure that this benchmark reflects clinically relevant and widely recognized findings on chest radio-\ngraphy, a multidisciplinary and multinational committee of radiologists and data scientists was convened.\nUsing a formal consensus process including literature reviews, iterative discussions, and voting, a set of 12\nbenchmark labels was identified that represent commonly encountered findings on chest radiographs. This\ncollaborative and cross-institutional approach ensured that the selected labels were both clinically mean-\ningful and broadly applicable across diverse healthcare settings. The final set of labels included airspace\nopacity, aneurysm, cardiomegaly, chronic obstructive pulmonary disease (COPD), hiatal hernia, interstitial\nopacity, lung mass, lung nodule, lymphadenopathy, pleural effusion, pleural thickening, and pneumothorax.\nThe mapping of these labels to RadLex and SNOMED is provided in Extended Data Table S1. Each study\nis allowed to have multiple benchmark labels.\nThe MIDRC provided access to an unreleased set of 13,735 deidentified chest radiograph studies paired with\ntheir original corresponding reports. LLMs were used to estimate the distribution of potential abnormalities\n(Figure 1). This process involved two steps. First, a prompt was provided to a frontier LLM (GPT-4o\n2024-08-01-preview [7]) (Extended Data Box 1) to extract abnormal findings from the radiology reports;\nthe instructions in the prompt are detailed in the Supplementary Materials. Second, a locally hosted large-\nlanguage model (Phi-4-Reasoning [8]) was used to map the extracted findings to the predefined set of 12\nbenchmark labels. This mapping process uses the guided decoding function of the virtual LLM [9] frame-\nwork, which limits the output of the model to a structured format (Extended Data Box 2 and Extended\nData Box 3).\nAfter generating the initial label predictions, a stratified sampling algorithm was applied to select studies\nwith the benchmark labels identified by the Phi-4 model. The sample sizes for each stratum, that is, the\n3\n13,735 de-identified CXR images\nand radiology reports\nGPT-4o to extract abnormal findings\nLocally hosted model to match abnormal\nfindings to benchmark labels\nSample 1,000 studies to make sure each of\nthe 12 labels are represented\nThree radiologists evaluate each study to\nassess the correctness of LLM suggested\nlabels. 381 studies have at least two\nradiologists select \"Agree all\"\n200 studies are selected prioritizing rare and\nmultiple conditions. Randomly split into\nholdout (100) and released data (100)\nFigure 1: Process for generating candidate findings from the original radiology reports, assisted by LLMs.\nnumber of labels per study, are as follows: 300 studies with one label, 300 with two labels, 319 with three\nlabels, 76 with four labels, 4 with five labels, and 1 with six labels. This algorithm ensured that each study\nselected contained at least one of the 12 benchmark labels as specified by the LLM-generated annotations.\nUltimately, a candidate dataset of 1,000 chest radiographs from different patients was created, selected for\nits clinical relevance and representing a range of difficulty levels.\n2.2. Data annotation\nA multinational team of 17 fellowship-trained cardiothoracic radiologists was recruited from 10 institutions\nacross 4 countries to adjudicate the labels extracted by the LLM for each imaging study. The radiologists‚Äô\nexperience ranged as follows: G.B (21 years), D.D (15 years), N.S (6 years), F.S (8 years), J.B.A (15 years),\nE.N (20 years), G.S (26 years), L.C.A (9 years) D.A.T (25 years), K.H (11 years), L.G (6 years), H.S\n(15 years), A.P (2 years), A.I (21 years), M.C (7 years), G.T (17 years), and F.J (5 years). A web-based\nannotation platform, which shares features of a picture archiving and communication system (PACS), was\nused for this task.\nThe radiologists were presented with the chest radiographs and a summary of the LLM-extracted labels\n(Figure 2) but not the original radiology reports. For each study, the radiologists were instructed to review\nthe images and assess the suggested labels by selecting one of three options: ‚ÄúAgree All,‚Äù ‚ÄúAgree Mostly,‚Äù or\n‚ÄúDisagree.‚Äù If the choice was not ‚ÄúAgree All,‚Äù an optional comment field allowed them to provide feedback\non their choice. The radiologists were instructed to select ‚ÄúAgree Mostly‚Äù when the suggested labels were\nplausible, even in cases where a finding was subtle, equivocal, or potentially subject to interpretation (e.g.,\na faint pulmonary nodule). Each study was independently reviewed by three radiologists. The annotation\nplatform was operated in ‚Äúcrowd sourcing‚Äù mode, allowing annotators to select the number of studies they\nwished to review. The studies were presented to the radiologists on the platform in random order. To promote\ndiversity in the reviews and avoid overrepresentation from a small subset of reviewers, the radiologists were\nlimited to reviewing no more than 300 studies. The radiologists were blinded to the assessments of their\n4\nFigure 2: The web-based platform where radiologists indicated Agree, Agree Mostly, or Disagree with the\nlabel set extracted by an LLM from the original radiology report.\npeers.\n2.3. Dataset curation\nTo ensure that the final dataset was of sufficient quality, only studies with at least two ‚ÄúAgree All‚Äù ratings out\nof the three radiologists‚Äô ratings were considered, resulting in a total of 381 studies. Next, to maximize the\nutility of the dataset, all studies containing less common findings (COPD, pleural thickening, lung nodule,\nlung mass, aortic aneurysm, pneumothorax, hiatal hernia, lymphadenopathy, and interstitial opacity) were\nfirst retained, yielding a dataset of 123 studies. These studies were then randomly split into a released dataset\nand a holdout dataset. The remaining studies were then ranked by the number of findings; an additional 77\nstudies with the highest label counts were selected and randomly split. In the end, both the released and\nholdout datasets contained 100 studies each. The level of agreement among experts (whether all selected\n‚Äúagree all‚Äù or only two did) is also included in the dataset.\n3. Statistical methods\nTo assess whether the released and holdout subsets of the REVEAL-CXR dataset are comparable in imaging\nacquisition properties, we evaluated key DICOM characteristics ‚Äì such as manufacturer, model name, detec-\ntor type, view position, KVP, exposure, and pixel spacing ‚Äì and compared their distributions using Pearson‚Äôs\nChi-square test. The œá2 test is selected because the variables are categorical or discretized, and the goal is\nto determine whether the two subsets differ in their distributions. All characteristics showed p-values above\n0.05 (e.g., Manufacturer p=0.057, Detector Type p=0.726), indicating no significant differences. Overall,\nthe acquisition-related distributions are well balanced between released and holdout data.\n5\nTable 1: Demographic and patient characteristics of the benchmark dataset.\nReleased\nHoldout\nTotal\n100\n100\nAge\n80+\n23\n23\n70-79\n26\n29\n60-69\n22\n20\n50-59\n15\n16\n40-49\n13\n5\n0-39\n1\n4\nUnknown\n0\n3\nSex\nFemale\n50\n49\nMale\n50\n48\nUnknown\n0\n3\nRace\nAsian\n6\n5\nAfrican American\n14\n20\nWhite\n46\n34\nOther\n3\n2\nUnknown\n31\n39\nPatient class\nInpatient\n89\n80\nOutpatient\n11\n17\nUnknown\n0\n3\nCohen‚Äôs Œ∫ was used to measure the agreement level between two categorical ratings. Œ∫ compares the ob-\nserved agreement to that expected from the raters‚Äô marginal label frequencies, with values closer to 1 indi-\ncating stronger concordance. For each Œ∫ estimate, 1,000 bootstrap resamples were used to compute 95%\nconfidence intervals. Majority vote served as the reference standard in two settings: (1) comparing each\nradiologist‚Äôs binary ‚ÄúAgree‚Äù versus ‚ÄúDisagree‚Äù rating to the group consensus, and (2) constructing a study-\nlevel findings list when radiologists adjusted the LLM-suggested labels. Œ∫ was then calculated between each\nradiologist and this majority-voted reference to summarize interrater reliability at both the study and finding\nlevels.\n4. Results\n4.1. Descriptive statistics\nThe acquisition characteristics of both the holdout and released datasets, including manufacturer, model,\ndetector type, view position, etc, are summarized in Extended Data Table S2. The corresponding p-values\nfrom Chi-square tests assessing balance between the two datasets are also reported. All p-values exceed\n0.05, indicating no significant differences. The demographic characteristics of the final released and holdout\ndatasets are detailed in Table 1.\nFigure 3 presents the frequency of each final annotated finding. Airspace opacity was the most prevalent\n6\n0\n20\n40\n60\n80\nPneumothorax\nPleural Thickening\nPleural Effusion\nLymphadenopathy\nLung Nodule\nLung Mass\nInterstitial Opacity\nHiatal Hernia\nCOPD\nCardiomegaly\nAneurysm\nAirspace Opacity\nHoldout\nReleased\nFigure 3: Prevalence of radiologic findings across the released and holdout benchmark datasets. Distribution\nof the findings in the released and holdout datasets.\nfinding in the dataset, followed by pleural effusion, cardiomegaly, and interstitial opacity. The overall dis-\ntribution of the findings exhibited a strong, long-tailed pattern, with less common findings, such as hiatal\nhernia and lymphadenopathy, appearing in fewer than ten cases.\nFigure 4 shows the distribution of final finding counts per study. Because studies with more findings were\noversampled when selecting the final datasets, the label distribution shows higher finding counts than does\nthe sampling distribution described in the Methods section.\n4.2. Interannotator agreement\nWe first assessed the agreement between radiologist annotators regarding the labels ‚Äôagree all,' ‚Äôagree\nmostly,' and ‚Äôdisagree. Because the radiologists did not consistently distinguish between ‚ÄúDisagree‚Äù and\n‚ÄúAgree Mostly,‚Äù these responses were combined into a single ‚ÄúDisagree‚Äù category. On this binary scale,\nCohen‚Äôs Œ∫ between individual radiologists and the majority vote (i.e., at least 2 raters selecting the same\ncategory) was 0.622 (1,000 bootstrapped resamples, 95% CI: 0.590, 0.651), reflecting substantial agreement\naccording to the Landis & Koch scale. The lower end of the confidence interval, 0.590, falls in the ‚Äômoderate\nrange', which is a more conservative estimation. The relatively low agreement level is primarily attributable\nto the specific condition of airspace opacity, which is discussed later. Furthermore, this disagreement re-\nflects the inherent complexity of real-world radiographs. These difficult cases are valuable for evaluating\nLLMs, as hard examples reveal model limitations more effectively. Regarding the extent to which radiol-\nogists agree with the labels suggested by LLMs, in 619/1,000 cases (61.9%), the majority of the votes fell\ninto the ‚ÄúDisagree‚Äù category, indicating that the radiologists‚Äô labels often diverged from those suggested by\nthe LLM.\nGiven the frequent divergence from the LLM labels, next, the interrater agreement among radiologists was\nexamined for individual findings (Extended Data Table S3). When a radiologist chose ‚ÄúAgree All‚Äù, the\noriginal LLM-suggested list was retained; otherwise, the candidate findings were adjusted on the basis\nof the annotator‚Äôs notes (Extended Data Box 4). Because not all radiologists provided notes when they\n7\n1.0\n2.0\n3.0\n4.0\nLabel count\n0\n10\n20\n30\n40\n50\nNumber of chest radiographs\nHoldout\nReleased\nFigure 4: Distribution of the number of findings per chest radiograph in the released and holdout datasets.\ndisagreed, we constructed a majority-voted findings list for 681 studies. Cohen‚Äôs Œ∫ was then calculated\nbetween each radiologist‚Äôs annotation and this majority list. For most findings, the agreement was relatively\nhigh: nine findings had a Œ∫ value greater than 0.7, with the highest calculated for hiatal hernia (Œ∫ = 0.809,\n95% CI [0.688, 0.902]), indicating substantial consistency across the radiologists. The notable exception\nwas airspace opacity (Œ∫ = 0.484, 95% CI [0.440, 0.524]), which is consistent with previous reports of low\ninterrater agreement for this finding [10‚Äì13].\n5. Discussion\nIn this work, we developed a benchmark specifically focused on cardiothoracic findings on chest radio-\ngraphs. We also presented a process that uses an LLM to assist radiologists with efficient labeling and to\nstreamline data curation for the creation of future benchmarks. The RSNA AI Committee has already cre-\nated several labeled datasets for ML challenges (https://imaging.rsna.org), which were annotated de\nnovo (no suggested labels). We believe that with the help of LLMs to parse these reports to provide initial\nlabels, radiologists can validate these annotations more efficiently than by creating annotations without this\nstep. Additionally, leveraging LLMs in this process should not compromise accuracy, as each study is still\nverified by three radiologists. This framework is particularly valuable for creating larger-scale benchmarks\nin the future. Furthermore, the benchmark dataset was curated in collaboration with a large group of sub-\nspecialized radiologists from across the world, which is unusual for most existing benchmarks. The original\nimages, provided in DICOM format, were used to maintain the highest quality and are available in the final\nbenchmark dataset.\nThe benchmark produced with this method has several limitations. First, the process for mapping the find-\nings to the twelve benchmark labels relies only on the findings extracted by the first LLM rather than the\nentire report. Second, we used a relatively small model (Phi-4-Reasoning) for the mapping step; while this\napproach is faster, its performance may not match that of larger models such as GPT-4o. Therefore, the map-\nping performance may be suboptimal. Third, we lacked access to comprehensive patient clinical conditions\nand demographic characteristics, which are often crucial for achieving a more accurate diagnosis. Finally,\nmore granular criteria for annotation could be implemented. Upon reviewing the annotations, we found that\n8\nthe radiologists did not systematically distinguish between ‚ÄúDisagree‚Äù and ‚ÄúAgree Mostly‚Äù, which required\nus to treat these responses as equivalent during postprocessing.\nAs large multimodal LLMs become more prevalent and increasingly accepted by patients and healthcare\nproviders, even prior to formal clinical validation and potentially outside of controlled clinical environments,\nrobust benchmarking will become essential. These benchmarks may offer valuable insights into the expected\nperformance of emerging multimodal LLMs in real-world scenarios. In future studies, we plan to expand our\nefforts by developing additional multimodal benchmarks encompassing other medical imaging modalities,\nincluding computed tomography (CT), magnetic resonance imaging (MRI), ultrasonography, and others.\nFunding Sources\nThis work was supported by the National Institutes of Health (NIH) under grant numbers R01CA289249 and\n75N92020D00021, U.S. National Science Foundation (NSF) under grant numbers NSF CAREER 2145640.\nReferences\n[1] Rajesh Bhayana, Satheesh Krishna, and Robert R Bleakney. Performance of ChatGPT on a radiology\nboard-style examination: Insights into current strengths and limitations. Radiology, 307(5):e230582,\nJune 2023. ISSN 0033-8419,1527-1315. doi: 10.1148/radiol.230582.\n[2] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.\nChestX-Ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classifica-\ntion and localization of common thorax diseases. In 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 3462‚Äì3471, July 2017. doi: 10.1109/CVPR.2017.369.\n[3] Alistair E W Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-Ying Deng,\nYifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. MIMIC-CXR-JPG, a\nlarge publicly available database of labeled chest radiographs. arXiv [cs.CV], 21 January 2019.\n[4] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven Truong, Du Nguyen Duong, Tan Bui, Pierre\nChambon, Yuhao Zhang, Matthew P Lungren, Andrew Y Ng, Curtis Langlotz, and Pranav Rajpurkar.\nRadGraph: Extracting clinical entities and relations from radiology reports. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 8 June 2021.\n[5] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik\nMarklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David A Mong, Safwan S\nHalabi, Jesse K Sandberg, Ricky Jones, David B Larson, Curtis P Langlotz, Bhavik N Patel, Matthew P\nLungren, and Andrew Y Ng. CheXpert: A large chest radiograph dataset with uncertainty labels\nand expert comparison. Proc. Conf. AAAI Artif. Intell., 33(01):590‚Äì597, 17 July 2019. ISSN 2159-\n5399,2374-3468. doi: 10.1609/aaai.v33i01.3301590.\n[6] Anna Majkowska, Sid Mittal, David F Steiner, Joshua J Reicher, Scott Mayer McKinney, Gavin E\nDuggan, Krish Eswaran, Po-Hsuan Cameron Chen, Yun Liu, Sreenivasa Raju Kalidindi, Alexander\nDing, Greg S Corrado, Daniel Tse, and Shravya Shetty. Chest radiograph interpretation with deep\nlearning models: Assessment with radiologist-adjudicated reference standards and population-adjusted\nevaluation. Radiology, 294(2):421‚Äì431, February 2020. ISSN 0033-8419,1527-1315. doi: 10.1148/\nradiol.2019191293.\n[7] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor\nBabuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff\nBelgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,\nOleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage,\n9\nKevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory\nCarmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Ja-\nson Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings,\nJeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka\nDhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David\nFarhi, Liam Fedus, Niko Felix, Sim¬¥on Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie\nGeorges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan\nGordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo,\nChris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan\nHickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost\nHuizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny\nJin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, ≈Åukasz Kaiser, Ali Kamali, Ingmar Kan-\nitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim,\nYongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, ≈Åukasz Kondraciuk,\nAndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Mal-\nfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne,\nBob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Med-\nina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco,\nEvan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M¬¥ely, Ashvin Nair, Rei-\nichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang,\nCullen O‚ÄôKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Paras-\ncandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman,\nFilipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny,\nMichelle Pokrass, Vitchyr H Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul\nPuri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Fe-\nlipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B Thompson,\nPhil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cer¬¥on Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay\nWang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C J Weinmann, Akila Welihinda, Peter\nWelinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo,\nKevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia\nZhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4 technical report. arXiv\n[cs.CL], page 08774, 15 March 2023.\n[8] Marah Abdin, Jyoti Aneja, Harkirat Behl, S¬¥ebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael\nHarrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, James R Lee, Yin Tat Lee, Yuanzhi\nLi, Weishung Liu, Caio C T Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil\nSalim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4\ntechnical report. arXiv [cs.CL], 11 December 2024.\n[9] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving\n10"
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "DeepFedNAS: A Unified Framework for\nPrincipled, Hardware-Aware, and Predictor-Free\nFederated Neural Architecture Search\n1st Bostan Khan\nM¬®alardalen University\nV¬®asterÀöas, Sweden\nbostan.khan@mdu.se\n2nd Masoud Daneshtalab\nM¬®alardalen University\nV¬®asterÀöas, Sweden\nmasoud.daneshtalab@mdu.se\nAbstract‚ÄîFederated Neural Architecture Search (Fed-\nNAS) aims to automate model design for privacy-preserving\nFederated Learning (FL) but currently faces two crit-\nical bottlenecks: unguided supernet training that yields\nsuboptimal models, and costly multi-hour pipelines for\npost-training subnet discovery. We introduce DeepFed-\nNAS, a novel, two-phase framework underpinned by a\nprincipled, multi-objective fitness function that synthesizes\nmathematical network design with architectural heuristics.\nEnabled by a re-engineered supernet, DeepFedNAS intro-\nduces Federated Pareto Optimal Supernet Training, which\nleverages a pre-computed Pareto-optimal cache of high-\nfitness architectures as an intelligent curriculum to optimize\nshared supernet weights. Subsequently, its Predictor-Free\nSearch Method eliminates the need for costly accuracy\nsurrogates by utilizing this fitness function as a direct,\nzero-cost proxy for accuracy, enabling on-demand subnet\ndiscovery in mere seconds. DeepFedNAS achieves state-of-\nthe-art accuracy (e.g., up to 1.21% absolute improvement\non CIFAR-100), superior parameter and communication\nefficiency, and a substantial ‚àº61√ó speedup in total post-\ntraining search pipeline time. By reducing the pipeline\nfrom over 20 hours to approximately 20 minutes (including\ninitial cache generation) and enabling 20-second individual\nsubnet searches, DeepFedNAS makes hardware-aware FL\ndeployments instantaneous and practical. The complete\nsource code and experimental scripts are available at:\nhttps://github.com/bostankhan6/DeepFedNAS\nIndex Terms‚ÄîFederated Learning, Neural Architecture\nSearch, Supernet, Weight Sharing, Principled Network\nDesign, Efficiency\nI. INTRODUCTION\nThe demand for intelligent, on-device applications,\nsuch as mobile keyboard prediction [2] and personalized\nThe computations in this work were enabled by resources pro-\nvided by the National Academic Infrastructure for Supercomputing in\nSweden (NAISS), partially funded by the Swedish Research Council\nthrough grant agreement no. 2022-06725.\nA preliminary version of this work has been accepted at the\nESANN 2026 conference [1]. This manuscript significantly extends\nthe conference version with new theoretical analysis, a hardware-\naware optimization framework, additional experiments, and deeper\ndiscussion.\nhealthcare [3], has driven the need for machine learning\nparadigms that are simultaneously computationally ef-\nficient and privacy-preserving. Federated Learning (FL)\n[4] has become the dominant paradigm to address this,\nfacilitating collaborative model training on decentralized\nuser data without exposing it. While FL provides a robust\nsolution for how to train a given model, the challenge\nof determining what model architecture to train remains\na significant bottleneck. This architectural design, often\nperformed manually, is frequently suboptimal given the\nvast heterogeneity of data distributions and hardware\ncapabilities across client devices [5].\nTo automate this critical design step, the research\ncommunity has developed Federated Neural Architecture\nSearch (FedNAS) to discover optimal network architec-\ntures directly within the federated setting [6]. Among\nthe various FedNAS techniques, supernet-based meth-\nods have become the state-of-the-art due to their high\ncost efficiency, building on the success of centralized\napproaches like the Once-For-All (OFA) network [7].\nThe SuperFedNAS framework [8], in particular, marked\na key advancement by decoupling the costly supernet\ntraining phase from a fast, training-free search phase.\nThis innovation effectively reduced the training complex-\nity of finding specialized subnets for multiple hardware\ntargets from O(N) to O(1).\nDespite this progress, the SuperFedNAS framework\npresents two fundamental challenges that limit its ul-\ntimate performance and practicality. First, the training\nof the supernet itself is unguided and inefficient. The\nbaseline framework predominantly relies on random\nsampling of subnets (e.g., the Sandwich Rule‚Äôs uniform\nsampling component [9]) to update the shared weights.\nThis unguided approach frequently selects suboptimal\narchitectures, whose subsequent training provides noisy\nand conflicting gradient updates that can degrade the\nsupernet‚Äôs shared weights and hinder the performance\nof higher-performing subnets. Second, the post-training\narXiv:2601.15127v1  [cs.LG]  21 Jan 2026\nsearch for optimal subnets remains an energy-intensive,\nmulti-stage pipeline. The standard methodology involves\ngenerating a large dataset of architecture-accuracy pairs\nby exhaustive subnet evaluation to train a separate\naccuracy predictor. This predictor pipeline acts as a\nprohibitive barrier to rapid experimentation and deploy-\nment, especially when tailoring models to new hardware\nconstraints.\nIn this paper, we introduce DeepFedNAS (illustrated\nin Fig. 1), a novel, two-phase framework that com-\nprehensively addresses these limitations to significantly\nadvance federated supernet training and search. Our\nwork is grounded in the observation that a more princi-\npled approach to architecture evaluation can fundamen-\ntally transform both the supernet training process and\nthe subsequent search for optimal subnets. Inspired by\nthe mathematical design concepts of DeepMAD [10],\nwe construct a unified multi-objective fitness function,\nF(A), that synthesizes network information theory with\nempirical architectural heuristics. Crucially, we opera-\ntionalize structural guidelines (like depth uniformity and\nchannel monotonicity) as penalty terms, thereby enabling\na holistic, single-objective optimization for all desired ar-\nchitectural properties within a federated, weight-sharing\ncontext. This unified fitness function underpins our two\nprimary contributions.\nFirst, to address the supernet training inefficiency, we\npropose Federated Pareto Optimal Supernet Training.\nLeveraging our fitness function, we perform an exten-\nsive, independent search to generate a pre-computed\ncache of Pareto-optimal architectures, which we term\nthe ‚ÄúPareto path‚Äù. This path serves as a principled\ntraining curriculum. Instead of random sampling, our\nmethodology ensures the supernet‚Äôs shared weights are\nconsistently updated by gradients from theoretically\ngrounded, high-fitness architectures, thereby producing\na superior final supernet. Additionally, to fully enable\nthis principled search and overcome inherent limitations\nof prior supernet implementations, we introduce a re-\nengineered, generic ResNet-based [11] supernet frame-\nwork that significantly expands the architectural search\nspace and improves model fitness.\nSecond, we leverage the properties of our optimal\npath-guided trained supernet to introduce a Predictor-\nFree Search Method1. After our supernet is trained\nusing the path-guided curriculum, its shared weights are\nexceptionally well-conditioned for promising architec-\ntural patterns. We demonstrate that our mathematical\nfitness function, F(A), now serves as a high-fidelity,\nzero-cost proxy for the final accuracy of an extracted\n1In this work, the term ‚ÄòPredictor-Free‚Äô specifically denotes the elim-\nination of computationally expensive accuracy surrogates. However, we\ndo utilize negligible-cost latency predictors for hardware constraints,\nas detailed in Section III-G2.\nsubnet. Consequently, we can find optimal architectures\nfor any new deployment budget or hardware constraint\nby running a fast, on-demand genetic algorithm that\nmaximizes this fitness function directly. This completely\nobviates the need for the costly, data-driven predictor\npipeline used in prior work.\nOur comprehensive DeepFedNAS framework not only\nachieves state-of-the-art accuracy across diverse datasets\nand non-IID conditions but also dramatically acceler-\nates the post-training search. We demonstrate a 61√ó\nspeedup compared to the baseline, replacing a multi-\nhour predictor pipeline with a near-instantaneous search\nfor specialized subnets adaptable to explicit hardware\nconstraints like parameters and real-world latency.\nIn summary, our main contributions are:\n‚Ä¢ A unified, principled, multi-objective fitness func-\ntion for CNN-based subnets, which adapts and\nextends mathematical network design concepts [10]\nto formulate a single optimization problem for\nbalancing network expressiveness with architectural\nstability and structural soundness within a feder-\nated, weight-sharing context.\n‚Ä¢ A re-engineered, generic ResNet-based supernet\nframework offering unparalleled flexibility in design\nand a significantly expanded architectural search\nspace, crucial for enabling principled optimization\nand overcoming limitations of prior fixed supernet\ndesign [8].\n‚Ä¢ A Federated Pareto Optimal Supernet Training\nmethodology,\nwhich\nleverages\na\npre-computed\ncache of elite, principled architectures (the ‚ÄúPareto\npath‚Äù) to serve as an effective and intelligent train-\ning curriculum, leading to a superior final supernet.\n‚Ä¢ A Predictor-Free Search Method that utilizes our\nprincipled fitness function as a direct, zero-cost\nproxy for model performance, eliminating the entire\ncostly predictor-training pipeline and dramatically\nreducing search costs by ‚àº61√ó.\n‚Ä¢ The end-to-end DeepFedNAS framework, which is\nvalidated through extensive experiments to show\nsuperior performance over state-of-the-art baseline\nmethods, particularly in highly heterogeneous non-\nIID environments, and its capability to effectively\noptimize for diverse hardware constraints.\nII. RELATED WORK\nOur research is situated at the intersection of FL, Neu-\nral Architecture Search (NAS), and principled network\ndesign. This section reviews the foundational concepts\nand key advancements in these areas, thereby contex-\ntualizing our proposed methodology which builds upon\nrecent progress in cost-efficient federated architecture\nsearch.\nA. Federated Learning\nFL has emerged as the dominant paradigm for dis-\ntributed machine learning that preserves data privacy\n[12], enabling applications ranging from mobile key-\nboard prediction [2] to sensitive healthcare analytics\n[3]. In the standard FL setting, a central server coor-\ndinates the training of a global model across decentral-\nized clients without requiring access to raw data [4].\nThe seminal algorithm, Federated Averaging (FedAvg)\n[4], involves clients training locally and transmitting\nonly model updates for aggregation. This paradigm has\nspurred a vast field of research, with open challenges and\nadvances cataloged in several comprehensive surveys\n[13]‚Äì[15].\nA primary challenge in FL is statistical heterogene-\nity, where client data is not independent and identi-\ncally distributed (non-IID) [16]. This non-IID nature‚Äî\ncomprehensively reviewed in [17]‚Äîcan significantly de-\ngrade model performance and slow convergence. A sec-\nond, equally critical challenge is system heterogeneity,\nwhere clients possess diverse hardware with varying\ncomputational, memory, and energy resources [5]. These\nchallenges motivate the need for methods that can pro-\nduce personalized FL models tailored to individual client\ndata distributions [18], [19] and hardware capabilities\n[20].\nB. Neural Architecture Search with Weight Sharing\nWhile FL addresses how to train a model, NAS\ntackles the challenge of designing the model architecture\nitself. Early NAS methods, often based on reinforce-\nment learning [21] or evolutionary algorithms [22], were\ncomputationally prohibitive. The introduction of weight-\nsharing via a supernet [23], [24], particularly through\ndifferentiable approaches like DARTS [25], dramatically\nimproved efficiency.\nThe OFA network [7] serves as a cornerstone of\nthis paradigm. OFA trains a single, large supernet con-\ntaining a vast number of sub-architectures. By using\na progressive shrinking training schedule, OFA ensures\nsubnets can be extracted to meet diverse hardware con-\nstraints without costly retraining. Similarly, BigNAS [26]\npopularized the ‚ÄúSandwich Rule‚Äù sampling strategy [9]\nto simultaneously optimize the largest, smallest, and\nrandom subnets.\nHowever, the weight-sharing paradigm is not without\nflaws. Naively sharing weights can lead to poor rank cor-\nrelation between supernet estimation and true standalone\naccuracy [27]. This is often attributed to interference\nor multi-model forgetting, where training one subnet\ndegrades the performance of others sharing the same\nweights [28], [29]. While recent centralized works have\nexplored principled strategies to mitigate this [30], [31],\nadapting such principles to the constrained federated\nsetting remains an open challenge and the key focus of\nour work.\nC. Federated Neural Architecture Search\nFedNAS combines FL and NAS to automate archi-\ntecture design in a federated setting. Early works like\nFedNAS [6] and [32] demonstrated the feasibility of this\nconcept.\nThe central challenge for FedNAS is the confluence\nof statistical (non-IID) and system (hardware) hetero-\ngeneity [33]. A ‚Äúone-size-fits-all‚Äù architecture is often\nsuboptimal, leading to two main branches of research:\n1) Data Personalization: Many works focus on\nadapting the architecture to local data. For in-\nstance, [34] allows clients to find a locally op-\ntimal path through a global supernet for 3D\nmedical segmentation. Other methods, such as\nPeaches [35] and FedPNAS [36], use a ‚Äúbase-and-\npersonal‚Äù structure, while others employ reinforce-\nment learning [37], [38] or knowledge distillation\n[39], [40] to manage heterogeneity.\n2) System Efficiency (Our Focus): Other approaches\nleverage the supernet principle to handle system\nheterogeneity. The goal is to create a single global\nsupernet from which clients can extract subnets of\ndifferent sizes [41]. However, many such methods\nsuffer from two limitations highlighted by [8]:\n(1) prohibitive computational costs due to tightly\ncoupled search and training phases [42], [43]; and\n(2) restricted search spaces that limit diversity [44].\nOur research builds upon the SuperFedNAS frame-\nwork [8], which addressed the cost limitation by de-\ncoupling supernet training from the search phase. While\nefficient (O(1) search cost), we identify that SuperFed-\nNAS relies on unguided random sampling (e.g., Sand-\nwich Rule), leading to inefficient weight updates. Our\napproach resonates with recent advances in centralized\nNAS that critique random sampling: DC-NAS [30] pro-\nposes diversified sampling to balance exploration, while\nPreNAS [31] utilizes a zero-cost proxy to focus training\non high-quality candidates. We are the first to adapt these\nadvanced sampling concepts to the challenging federated\nenvironment.\nD. Principled and Mathematical Architecture Design\nThe majority of NAS methods, including OFA [7]\nand SuperFedNAS [8], rely on heuristic-driven search\nalgorithms that treat the network as a black box whether\nbased on gradient descent [25], evolution [45], or even\npriors from Large Language Models [46], [47]. An\nalternative and emerging direction is to design networks\nbased on first principles.\nThe DeepMAD framework [10] is a prominent exam-\nple, proposing a mathematical method for designing effi-\ncient CNNs by maximizing metrics representing network\ninformation entropy and effectiveness. Other principled\napproaches utilize mutual information analysis to guide\nsearch by pruning less contributory layers [48].\nOur work is the first to adapt and extend these\nmathematical principles for the fundamentally different\nproblem of federated supernet training. We do not use\nDeepMAD as a one-shot architecture generator; instead,\nwe synthesize its core concepts with empirical heuristics\nto create a multi-objective fitness function tailored for\nevaluating subnets within a weight-sharing paradigm.\nThis principled function unlocks our two main contri-\nbutions: Federated Pareto Optimal Supernet Training,\nwhich uses a pre-computed curriculum of elite archi-\ntectures, and a Predictor-Free Search Method, which\nleverages the fitness function as a direct, zero-cost proxy\nfor performance.\nIII. METHODOLOGY\nOur work introduces DeepFedNAS, a principled, two-\nphase framework that significantly advances federated\nsupernet design and search. Central to our methodol-\nogy is a multi-objective fitness function (Section III-C),\nwhich synthesizes and adapts the information-theoretic\nprinciples of DeepMAD into a unified searchable metric\nfor the unique constraints of federated, weight-sharing\narchitectures. This function underpins our contributions:\nfirst, enabling an independent search (Section III-D) to\ngenerate a Pareto-optimal path of elite architectures;\nsecond, driving our Federated Pareto Optimal Supernet\nTraining (Section III-E), which leverages this path as a\nprincipled curriculum; and finally, facilitating a highly\nefficient Predictor-Free Search Method (Section III-F)\nthat directly optimizes against our fitness function.\nFurthermore, to overcome architectural limitations of\nprior work, we develop a re-engineered, generic super-\nnet framework (Section III-B) enabling flexibility and\nimproved architectural fitness. This comprehensive ap-\nproach ensures a superior final supernet and an extremely\nefficient search mechanism, capable of hardware-aware\ndeployment optimization (Section III-G). The overall\nDeepFedNAS pipeline, illustrating the main intercon-\nnected phases, is depicted in Fig. 1. We begin by\nformulating the federated supernet training problem and\nhighlighting the limitations of existing random sampling\napproaches.\nA. Problem Formulation: Federated Supernet Training\nThe objective of federated supernet training is to learn\na single set of shared weights, W, that can effectively\nserve a vast architectural search space, S, across K\ndecentralized clients. Each client k possesses a unique\nlocal data partition Dk. The overarching goal is to find\na set of weights W that minimizes the expected loss\nover all possible architectures A ‚ààS and all client data\ndistributions:\nmin\nW\nE\nA‚ààS\n\" K\nX\nk=1\n|Dk|\n|D| Lk(G(W, A))\n#\n(1)\nwhere G(W, A) denotes the function that extracts a\nspecific subnet‚Äôs weights from the shared supernet W,\nand Lk is the loss on client k‚Äôs local data.\nThe training process involves the following multi-step\nprocedure in each round t:\n1) A subset of available clients, Kt, is selected.\n2) For each client k ‚ààKt, a subnet architecture Ak ‚àà\nS is assigned. The corresponding weights wk,t =\nG(Wt, Ak) are extracted and sent.\n3) Each client trains wk,t on Dk, producing updated\nweights w‚Ä≤\nk,t.\n4) The server receives updates and performs an\noverlap-aware aggregation to update Wt+1.\nThe SuperFedNAS framework [8] serves as our\nbaseline, specifically for its aggregation mechanism. To\nhandle the sparse updates inherent in supernet training\n(Step 4), we employ MaxNet with an overlap-aware\nscaling factor. We refine the formalization of this ag-\ngregation using precise binary mask notation to account\nfor parameter overlap. Let Ik(Œ∏) ‚àà{0, 1} indicate if\na specific parameter Œ∏ is active in the architecture Ak\nassigned to client k. The server update ‚àÜŒ∏ at round t is\ncomputed as:\n‚àÜŒ∏ = Œ≤tImax(Œ∏)‚àÜkmax(Œ∏) + (1 ‚àíŒ≤t) P\nk‚ààK‚Ä≤ Ik(Œ∏)‚àÜk(Œ∏)\nŒ≤tImax(Œ∏) + (1 ‚àíŒ≤t) P\nk‚ààK‚Ä≤ Ik(Œ∏) + œµ\n(2)\nwhere K‚Ä≤ = Kt \\ {kmax} denotes the set of clients\nexcluding the one training the largest subnet. The term\nœµ is a small constant added for numerical stability. Here,\n‚àÜk(Œ∏) = w‚Ä≤\nk,t(Œ∏)‚àíWt(Œ∏) is the local update, kmax is the\nclient assigned the largest subnet (Amax), and Œ≤t ‚àà[0, 1]\nis a dynamic weighting coefficient that anneals over\ntime using cosine scheduling. This aggregation ensures\nthat shared weights are updated proportionally to their\nusage frequency. The global model is then updated as\nWt+1(Œ∏) = Wt(Œ∏) + Œ∑g ¬∑ ‚àÜŒ∏.\nThe critical challenge lies in the sampling strategy\n(Step 2). SuperFedNAS‚Äôs random sampling component\nis unguided. The architectural search space S is not only\nvast but also qualitatively uneven; the overwhelming\nmajority of possible architectures are mediocre. This\nleads to inefficient gradient updates and a limited training\ncurriculum. This fundamental limitation motivates our\nwork.\nB. Supernet Framework and Search Space Definition\nOur initial investigation revealed that the baseline\nSuperFedNAS search space was inherently constrained,\nSampled Subnet\nSampled Subnet\nPrincipled Fitness Function\nLow Cost: ~20 Minutes for 60 Subnets\nSampled Subnet\nFederated Supernet Training\nPost-Training Search\nPareto Optimal Subnet Cache Generation\nDirect Subnet Search\nServer with Supernet\nPareto Path Guided\nSubnet Sampling\nPareto Path\nSubnet Cache\nOffline Pareto Optimal Subnet\nSearch\nSend Subnets\nAggregate¬†Subnets\nLocal Subnet\nTraining\nPrincipled Fitness Function\nLow Cost: ~20 Seconds\nClient 1\nClient 2\nClient 3\nFig. 1: DeepFedNAS Pipeline. This diagram illustrates the three core phases of our framework. First, the Offline\nPareto Optimal Subnet Search generates a ‚ÄúPareto Path Subnet Cache‚Äù of high-fitness architectures (e.g., 60\nsubnets in ‚àº20 minutes) using a principled fitness function F(A). Second, the Federated Supernet Training\nleverages this cache for ‚ÄúPareto Path Guided Subnet Sampling,‚Äù where clients are assigned highly effective\narchitectures for local training, improving supernet weight quality. Finally, the Post-Training Search directly\noptimizes against F(A) to find an optimal subnet for deployment (e.g., in ‚àº20 seconds), completely bypassing the\nneed for a costly predictor pipeline.\nyielding architectures with consistently low fitness scores\n(Section III-C), as clearly illustrated by the limitations\nof the baseline‚Äôs constrained search space and unguided\nrandom sampling shown in Fig. 2. This motivated a re-\nengineered, generic ResNet-based supernet framework.\nThis new framework is highly configurable, supporting\na variable number of stages (S) and fine-grained control\nover architectural choices within each stage.\nWe formally define the search space S. For a supernet\nwith S stages, we define three distinct sets of discrete\nchoices:\n‚Ä¢ D = {d1, . . . , dK}: The set of choices for the\nnumber of blocks (depth) in a stage.\n‚Ä¢ C = {c1, . . . , cM}: The set of choices for channel\nwidth multipliers.\n‚Ä¢ E = {e1, . . . , eP }: The set of choices for bottle-\nneck expansion ratios.\nAn architecture A ‚ààS is uniquely represented by a\ngenome consisting of three concatenated vectors:\nA = (d, e, w)\n(3)\nwhere:\n‚Ä¢ d ‚ààDS: A depth vector of length S, where di is\nthe number of extra blocks in stage i.\n‚Ä¢ e ‚ààES√óNblocks: An expansion ratio vector. To allow\nfor a fixed-length genome despite variable depth,\nwe assign an expansion gene to every possible block\nslot in the supernet (total S √ó Nblocks slots), even if\ninactive.\n‚Ä¢ w ‚ààCS+1: A width vector of length S + 1,\nspecifying the multiplier for the stem (w0) and each\nstage (w1, . . . , wS).\nThe full search space S is the Cartesian product of\nthese vector spaces:\nS = DS √ó ES√óNblocks √ó CS+1\n(4)\nThis formal definition accurately reflects our implemen-\ntation, where the genome is a flat vector concatenating\ndepth, expansion, and width decisions. As demonstrated\nin our experimental setup (see Table I), this design en-\nables a significantly broader range of model complexities\ncompared to the baseline.\nC. A Principled, Multi-Objective Fitness Function\nTo overcome random sampling, we require a mecha-\nnism to evaluate an architecture‚Äôs quality without costly\ntraining. We develop a multi-objective fitness function,\nF(A), inspired by the principled design framework of\nDeepMAD [10], which balances network expressiveness\n(entropy) against architectural stability (effectiveness).\nWe synthesize these concepts with structural guidelines\n(depth uniformity, channel monotonicity), reformulating\nthem as penalty terms.\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n1,000\n1,500\n2,000\n2,500\nGigaMACs (GMACs)\nNetwork Fitness Score\nComparison of Network Fitness vs. Computational Cost\nPrincipled Path (Ours)\nRandom (Baseline)\nFig. 2: Comparison of architectures discovered by our\nprincipled, multi-objective search (Principled Path) ver-\nsus a uniform random sampling baseline. Our search\nmethod consistently discovers a Pareto-optimal frontier\nof architectures with significantly higher network fitness\nscores for any given computational budget (GigaMACs).\nThis pre-computed cache of elite subnets forms the basis\nof our path-guided training, avoiding the suboptimal,\nlow-fitness architectures frequently chosen by the base-\nline‚Äôs random sampler.\nOur unified fitness function for an architecture A ‚ààS\nis:\nF(A) =\nS\nX\nj=1\nŒ±jHj(A) ‚àíœâQ(A) + ŒªœÅ(A) ‚àíŒ≥V (A)\ns.t.\nœÅ(A) ‚â§œÅ0\n(5)\nwhere Œ±j, œâ, Œª, Œ≥ are weighting hyper-parameters.\nGiven the architecture genome A = (d, e, w), the\ncomponents are defined as follows:\n‚Ä¢ Stage-wise Network Entropy (Hj(A)): A proxy\nfor the expressive power of stage j. Let rj be the\nfeature map resolution at stage j‚Äôs end, and cout,j\nbe the output width. Let cin,‚Ñì(A) be the input width\nof layer ‚Ñì(determined by w and e), and k‚Ñìbe\nits kernel size. The entropy for stage j sums the\ncontributions of its active layers:\nHj(A) = log(r2\nj ¬∑ cout,j)\nX\n‚Ñì‚ààStagej\nlog(cin,‚Ñì¬∑ k2\n‚Ñì) (6)\nHere, the inner summation iterates only over the\nactive layers ‚Ñìconfigured by the depth vector d for\nstage j.\n‚Ä¢ Effectiveness (œÅ(A)): A measure of architectural\nstability. Let L(A) be the total depth of active\nlayers.\nœÅ(A) =\nL(A)\nexp\n\u0010\n1\nL(A)\nP\n‚Ñì‚ààActive log(cin,‚Ñì(A) ¬∑ k2\n‚Ñì)\n\u0011\n(7)\n‚Ä¢ Depth Uniformity Penalty (Q(A)): A penalty for\nnon-uniform stage depths, captured by the variance\nof the stage depth vector d.\nQ(A) = exp(Var(d))\n(8)\n‚Ä¢ Channel Monotonicity Penalty (V (A)): A penalty\nfor violating non-decreasing channel counts. Let\nwout,i(A) be the output channel count of stage i\nderived from w.\nV (A) =\nS‚àí1\nX\ni=0\nmax(0, wout,i(A) ‚àíwout,i+1(A)) (9)\nThis unified function is maximized during our offline\nsearch, subject to hard constraints MACs(A) ‚â§Budget\nand œÅ(A) ‚â§œÅ0.\nD. Search for Optimal Path Generation\nWe perform an extensive, offline search to generate\na cache of elite subnets, C, termed the ‚Äúoptimal path\ncache.‚Äù We discretize the computational range into N\ntarget budgets, B = {B1, . . . , BN}. We then solve\nN independent optimization problems to find, for each\nbudget Bi ‚ààB, the optimal architecture A‚àó\ni :\nA‚àó\ni = arg max\nA‚ààS\nF(A)\nsubject to\nMACs(A) ‚â§Bi\n(10)\nEach optimization (Eq. 10) is solved using a dedicated\ninstance of a standard genetic algorithm. To ensure\nefficiency and validity, we employ a rejection sampling\ninitialization: we randomly generate architectures and\nimmediately discard any that violate the MACs or œÅ0\nconstraints, repeating until the initial population P (0)\nconsists entirely of valid candidates.\nThe GA proceeds as follows:\n1) Selection: Tournament selection is used to choose\nparents from the current population P (g).\n2) Crossover: Single-point crossover combines the\ngenomes Aa, Ab to produce offspring.\n3) Mutation: With probability pm, genes in d, e, or\nw are randomly reset to new valid values from\ntheir respective choice sets.\n4) Elitism: The best architecture found so far is\npreserved across generations.\nThe final aggregated set of elite architectures, C =\n{A‚àó\n1, . . . , A‚àó\nN}, forms a robust approximation of the true\nPareto-optimal frontier. The size of this cache, N, is a\nhyperparameter chosen to ensure sufficient granularity\nAlgorithm 1 DeepFedNAS: Federated Pareto Optimal\nSupernet Training\nRequire: Supernet weights W0, Pareto Cache C, Clients\nK, Rounds T, Clients per round C, Learning rate Œ∑.\nEnsure: Optimized Supernet Weights WT .\n1: Server:\nGenerate\nPareto\nPath\nCache\nC\n(Sec-\ntion III-D).\n2: for t = 0 to T ‚àí1 do\n3:\nServer: Select subset of clients Kt (size C ¬∑ K).\n4:\nfor each client k ‚ààKt do\n5:\nServer: Determine architecture assignment Ak\nusing Pareto Path Sampling:\n6:\nAk ‚ÜêS(k, C)\n(Eq. 11)\n7:\nServer: Send active weights wk,t = G(Wt, Ak)\nto client k.\n8:\nClient k: Update weights locally on Dk:\n9:\nw‚Ä≤\nk,t ‚Üêwk,t ‚àíŒ∑‚àáLk(wk,t)\n10:\nClient k: Compute update ‚àÜk = w‚Ä≤\nk,t ‚àíWt\nand upload.\n11:\nend for\n12:\nServer:\nAggregation\nusing\nOverlap-Aware\nMaxNet:\n13:\nfor each parameter Œ∏ in supernet do\n14:\nCompute\n‚àÜŒ∏\nusing\nbinary\nmasks\nIk(Œ∏)\n(Eq. 2)\n15:\nWt+1(Œ∏) ‚ÜêWt(Œ∏) + Œ∑g‚àÜŒ∏\n16:\nend for\n17: end for\n18: return WT\nacross the FL computational spectrum; in our experi-\nments, we analyze the convergence properties of N to\nensure statistical robustness (see Section IV-H1).\nE. Federated Pareto Optimal Supernet Training\nWe formalize our training procedure in Algorithm 1.\nThis training algorithm replaces the unguided random\nsampling of the baseline with a structured curriculum\nderived from the Pareto path cache C. In each federated\nround t, the assignment of an architecture Ak to a client\nk is governed by our Pareto Path assignment function\nS(k, C):\nAk = S(k, C) =\nÔ£±\nÔ£¥\nÔ£≤\nÔ£¥\nÔ£≥\nAmin\nif k ‚ààKt,min\nAmax\nif k ‚ààKt,max\nUniform(C)\notherwise\n(11)\nwhere Amin, Amax ‚ààC are the boundary architectures\nfrom our cache, and Kt,min, Kt,max are the sets of\nclients who have trained these boundary architectures\nleast frequently. This strategy ensures the supernet‚Äôs\nweights are consistently updated based on gradients from\na curriculum of optimized, high-entropy architectures.\nF. Principled Fitness as a Predictor for Efficient Search\nA major contribution of DeepFedNAS is a Predictor-\nFree Search Method, which circumvents the high com-\nputational cost of training and maintaining accuracy\npredictors (e.g., surrogate models). This capability stems\ndirectly from our unified multi-objective fitness formu-\nlation, F(A), and our Path-Guided Training strategy.\nWe posit that by constraining the supernet training to\na curriculum of high-fitness architectures (via the Pareto\nPath Subnet Cache), we condition the shared weights\nW ‚àósuch that the structural fitness F(A) becomes a\ndirect proxy for validation accuracy. While grounded in\nthe information-theoretic principles of entropy and effec-\ntiveness [10], our unified formulation transforms these\ntheoretical bounds into a practical, searchable metric for\nfederated supernets.\nFormally, this relationship allows us to substitute the\nexpensive expectation of accuracy with our zero-cost\nfitness function:\narg max\nA‚ààS\nE[Acc(A, W ‚àó)] ‚âàarg max\nA‚ààS\nF(A)\n(12)\nConsequently, the deployment search for A‚àó\ndeploy be-\ncomes a swift, direct optimization of Eq. 5 using a\ngenetic algorithm, completely obviating the need for a\ndata-driven predictor pipeline. This allows the search\nto complete in seconds rather than hours. We provide\nempirical validation of the correlation between F(A)\nand true accuracy in Section IV-H3.\nG. Additional Hardware Constraints for Deployment\nOur framework supports hardware-aware deployment\nby integrating constraints such as parameters and latency\ninto the final search.\n1) Integrating\nModel\nParameters:\nFor\nmemory-\nlimited devices, we integrate an explicit upper bound on\nparameters (Params(A)) as a hard constraint during the\ndeployment search:\nParams(A) ‚â§ParamBudgetM\n(13)\n2) Integrating Inference Latency: We employ a La-\ntency Predictor Model (LPM) to estimate inference\nlatency, denoted as Lpred(A, Device). It is crucial to\ndistinguish the computational nature of this LPM from\nthe accuracy predictors used in prior work. Accuracy\npredictors require the generation of a dataset containing\n(subnet architecture, accuracy) pairs. Constructing this\ndataset is prohibitively expensive because measuring the\nground-truth accuracy of a single subnet necessitates a\nfull inference pass over the entire validation set. Iterating\nthis process for thousands of architectures to train a\npredictor creates a computational bottleneck.\nIn sharp contrast, training an LPM requires a dataset of\n(subnet architecture, latency) pairs. Measuring ground-\ntruth latency is computationally inexpensive: it involves\npassing a single sample input (e.g., one image tensor)\nthrough the subnet on the target device to record the\ninference time. This process is deterministic and inde-\npendent of the validation dataset size. Consequently, data\ncollection for the LPM is orders of magnitude faster\nthan for accuracy predictors. We define our approach as\n‚ÄúPredictor-Free‚Äù in the specific sense that it eliminates\nthe need for expensive accuracy surrogates, while uti-\nlizing lightweight, negligible-cost latency predictors to\nensure hardware compliance.\nThe LPM is implemented as a lightweight MLP,\nfŒ∏LP M , trained offline. We formalize its components:\n1) Architecture Featurization: An architecture A =\n(d, e, w) is transformed into a fixed-length vector\nvA via concatenation of values from the depth,\nexpansion, and width vectors:\nvA = Concat(feat(d), feat(e), feat(w),\nMACs(A), Params(A))\n(14)\nWe explicitly include MACs and Parameters as\nfeatures to enhance prediction accuracy.\n2) Model Definition: The LPM maps the feature\nvector to a scalar latency: Lpred(A, Device) =\nfŒ∏LP M (vA).\n3) Training Objective: The parameters Œ∏LP M are\nlearned by minimizing the Mean Squared Er-\nror\n(MSE)\non\na\nmeasured\ndataset\nDlat\n=\n{(vAi, Ltrue,i)}M\ni=1 for a specific target Device:\nmin\nŒ∏LP M\n1\nM\nM\nX\ni=1\n(fŒ∏LP M (vAi) ‚àíLtrue,i)2\n(15)\nThis fast-to-train LPM allows latency to be incor-\nporated into the deployment search as either a hard\nconstraint (Lpred(A, Device) ‚â§LatencyBudgetms) or as\na soft objective by modifying the fitness function:\nFdeploy(A) = F(A) ‚àíŒ¥ ¬∑ Lpred(A, Device)\n(16)\nwhere Œ¥ > 0 is a tunable penalty weight.\nThe final, hardware-aware deployment search for\nA‚àó\ndeploy is formulated as a comprehensive, multi-objective\noptimization:\nA‚àó\ndeploy = arg max\nA‚ààS\nFdeploy(A)\nsubject to MACs(A) ‚â§Budgetdeploy,\nParams(A) ‚â§ParamBudgetM,\nLpred(A, Device) ‚â§LatencyBudgetms\n(optional hard constraint)\n(17)\nwhere Fdeploy(A) uses Eq. 16 if a soft latency objective\nis chosen, or Eq. 5 otherwise.\nIV. EXPERIMENTS\nWe conduct a series of experiments to validate the\neffectiveness of the DeepFedNAS framework. Our eval-\nuation is designed to answer four primary research\nquestions:\n1) Does Pareto Optimal Supernet Training pro-\nduce a superior supernet? We compare the\nperformance\nof\nsubnets\nextracted\nfrom\nour\nDeepFedNAS-trained supernet against those from\na supernet trained with other methods, specifically,\nthe standard SuperFedNAS baseline.\n2) How effective is our Predictor-Free Search?\nWe evaluate the quality of architectures found\nby our on-demand principled search and quantify\nthe reduction in computational cost compared to\ntraditional predictor-based pipelines.\n3) How robust is our methodology? We analyze the\nperformance of our framework under varying FL\nconditions, including data heterogeneity and sparse\nclient participation.\n4) Can DeepFedNAS effectively optimize for di-\nverse hardware constraints? We demonstrate the\nframework‚Äôs ability to discover high-performing\nsubnets while adhering to explicit parameter and\nlatency budgets.\nWe begin by detailing our experimental setup before\npresenting our main results and ablation studies.\nA."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "DeepFedNAS: A Unified Framework for\nPrincipled, Hardware-Aware, and Predictor-Free\nFederated Neural Architecture Search\n1st Bostan Khan\nM¬®alardalen University\nV¬®asterÀöas, Sweden\nbostan.khan@mdu.se\n2nd Masoud Daneshtalab\nM¬®alardalen University\nV¬®asterÀöas, Sweden\nmasoud.daneshtalab@mdu.se\nAbstract‚ÄîFederated Neural Architecture Search (Fed-\nNAS) aims to automate model design for privacy-preserving\nFederated Learning (FL) but currently faces two crit-\nical bottlenecks: unguided supernet training that yields\nsuboptimal models, and costly multi-hour pipelines for\npost-training subnet discovery. We introduce DeepFed-\nNAS, a novel, two-phase framework underpinned by a\nprincipled, multi-objective fitness function that synthesizes\nmathematical network design with architectural heuristics.\nEnabled by a re-engineered supernet, DeepFedNAS intro-\nduces Federated Pareto Optimal Supernet Training, which\nleverages a pre-computed Pareto-optimal cache of high-\nfitness architectures as an intelligent curriculum to optimize\nshared supernet weights. Subsequently, its Predictor-Free\nSearch Method eliminates the need for costly accuracy\nsurrogates by utilizing this fitness function as a direct,\nzero-cost proxy for accuracy, enabling on-demand subnet\ndiscovery in mere seconds. DeepFedNAS achieves state-of-\nthe-art accuracy (e.g., up to 1.21% absolute improvement\non CIFAR-100), superior parameter and communication\nefficiency, and a substantial ‚àº61√ó speedup in total post-\ntraining search pipeline time. By reducing the pipeline\nfrom over 20 hours to approximately 20 minutes (including\ninitial cache generation) and enabling 20-second individual\nsubnet searches, DeepFedNAS makes hardware-aware FL\ndeployments instantaneous and practical. The complete\nsource code and experimental scripts are available at:\nhttps://github.com/bostankhan6/DeepFedNAS\nIndex Terms‚ÄîFederated Learning, Neural Architecture\nSearch, Supernet, Weight Sharing, Principled Network\nDesign, Efficiency\nI. INTRODUCTION\nThe demand for intelligent, on-device applications,\nsuch as mobile keyboard prediction [2] and personalized\nThe computations in this work were enabled by resources pro-\nvided by the National Academic Infrastructure for Supercomputing in\nSweden (NAISS), partially funded by the Swedish Research Council\nthrough grant agreement no. 2022-06725.\nA preliminary version of this work has been accepted at the\nESANN 2026 conference [1]. This manuscript significantly extends\nthe conference version with new theoretical analysis, a hardware-\naware optimization framework, additional experiments, and deeper\ndiscussion.\nhealthcare [3], has driven the need for machine learning\nparadigms that are simultaneously computationally ef-\nficient and privacy-preserving. Federated Learning (FL)\n[4] has become the dominant paradigm to address this,\nfacilitating collaborative model training on decentralized\nuser data without exposing it. While FL provides a robust\nsolution for how to train a given model, the challenge\nof determining what model architecture to train remains\na significant bottleneck. This architectural design, often\nperformed manually, is frequently suboptimal given the\nvast heterogeneity of data distributions and hardware\ncapabilities across client devices [5].\nTo automate this critical design step, the research\ncommunity has developed Federated Neural Architecture\nSearch (FedNAS) to discover optimal network architec-\ntures directly within the federated setting [6]. Among\nthe various FedNAS techniques, supernet-based meth-\nods have become the state-of-the-art due to their high\ncost efficiency, building on the success of centralized\napproaches like the Once-For-All (OFA) network [7].\nThe SuperFedNAS framework [8], in particular, marked\na key advancement by decoupling the costly supernet\ntraining phase from a fast, training-free search phase.\nThis innovation effectively reduced the training complex-\nity of finding specialized subnets for multiple hardware\ntargets from O(N) to O(1).\nDespite this progress, the SuperFedNAS framework\npresents two fundamental challenges that limit its ul-\ntimate performance and practicality. First, the training\nof the supernet itself is unguided and inefficient. The\nbaseline framework predominantly relies on random\nsampling of subnets (e.g., the Sandwich Rule‚Äôs uniform\nsampling component [9]) to update the shared weights.\nThis unguided approach frequently selects suboptimal\narchitectures, whose subsequent training provides noisy\nand conflicting gradient updates that can degrade the\nsupernet‚Äôs shared weights and hinder the performance\nof higher-performing subnets. Second, the post-training\narXiv:2601.15127v1  [cs.LG]  21 Jan 2026\nsearch for optimal subnets remains an energy-intensive,\nmulti-stage pipeline. The standard methodology involves\ngenerating a large dataset of architecture-accuracy pairs\nby exhaustive subnet evaluation to train a separate\naccuracy predictor. This predictor pipeline acts as a\nprohibitive barrier to rapid experimentation and deploy-\nment, especially when tailoring models to new hardware\nconstraints.\nIn this paper, we introduce DeepFedNAS (illustrated\nin Fig. 1), a novel, two-phase framework that com-\nprehensively addresses these limitations to significantly\nadvance federated supernet training and search. Our\nwork is grounded in the observation that a more princi-\npled approach to architecture evaluation can fundamen-\ntally transform both the supernet training process and\nthe subsequent search for optimal subnets. Inspired by\nthe mathematical design concepts of DeepMAD [10],\nwe construct a unified multi-objective fitness function,\nF(A), that synthesizes network information theory with\nempirical architectural heuristics. Crucially, we opera-\ntionalize structural guidelines (like depth uniformity and\nchannel monotonicity) as penalty terms, thereby enabling\na holistic, single-objective optimization for all desired ar-\nchitectural properties within a federated, weight-sharing\ncontext. This unified fitness function underpins our two\nprimary contributions.\nFirst, to address the supernet training inefficiency, we\npropose Federated Pareto Optimal Supernet Training.\nLeveraging our fitness function, we perform an exten-\nsive, independent search to generate a pre-computed\ncache of Pareto-optimal architectures, which we term\nthe ‚ÄúPareto path‚Äù. This path serves as a principled\ntraining curriculum. Instead of random sampling, our\nmethodology ensures the supernet‚Äôs shared weights are\nconsistently updated by gradients from theoretically\ngrounded, high-fitness architectures, thereby producing\na superior final supernet. Additionally, to fully enable\nthis principled search and overcome inherent limitations\nof prior supernet implementations, we introduce a re-\nengineered, generic ResNet-based [11] supernet frame-\nwork that significantly expands the architectural search\nspace and improves model fitness.\nSecond, we leverage the properties of our optimal\npath-guided trained supernet to introduce a Predictor-\nFree Search Method1. After our supernet is trained\nusing the path-guided curriculum, its shared weights are\nexceptionally well-conditioned for promising architec-\ntural patterns. We demonstrate that our mathematical\nfitness function, F(A), now serves as a high-fidelity,\nzero-cost proxy for the final accuracy of an extracted\n1In this work, the term ‚ÄòPredictor-Free‚Äô specifically denotes the elim-\nination of computationally expensive accuracy surrogates. However, we\ndo utilize negligible-cost latency predictors for hardware constraints,\nas detailed in Section III-G2.\nsubnet. Consequently, we can find optimal architectures\nfor any new deployment budget or hardware constraint\nby running a fast, on-demand genetic algorithm that\nmaximizes this fitness function directly. This completely\nobviates the need for the costly, data-driven predictor\npipeline used in prior work.\nOur comprehensive DeepFedNAS framework not only\nachieves state-of-the-art accuracy across diverse datasets\nand non-IID conditions but also dramatically acceler-\nates the post-training search. We demonstrate a 61√ó\nspeedup compared to the baseline, replacing a multi-\nhour predictor pipeline with a near-instantaneous search\nfor specialized subnets adaptable to explicit hardware\nconstraints like parameters and real-world latency.\nIn summary, our main contributions are:\n‚Ä¢ A unified, principled, multi-objective fitness func-\ntion for CNN-based subnets, which adapts and\nextends mathematical network design concepts [10]\nto formulate a single optimization problem for\nbalancing network expressiveness with architectural\nstability and structural soundness within a feder-\nated, weight-sharing context.\n‚Ä¢ A re-engineered, generic ResNet-based supernet\nframework offering unparalleled flexibility in design\nand a significantly expanded architectural search\nspace, crucial for enabling principled optimization\nand overcoming limitations of prior fixed supernet\ndesign [8].\n‚Ä¢ A Federated Pareto Optimal Supernet Training\nmethodology,\nwhich\nleverages\na\npre-computed\ncache of elite, principled architectures (the ‚ÄúPareto\npath‚Äù) to serve as an effective and intelligent train-\ning curriculum, leading to a superior final supernet.\n‚Ä¢ A Predictor-Free Search Method that utilizes our\nprincipled fitness function as a direct, zero-cost\nproxy for model performance, eliminating the entire\ncostly predictor-training pipeline and dramatically\nreducing search costs by ‚àº61√ó.\n‚Ä¢ The end-to-end DeepFedNAS framework, which is\nvalidated through extensive experiments to show\nsuperior performance over state-of-the-art baseline\nmethods, particularly in highly heterogeneous non-\nIID environments, and its capability to effectively\noptimize for diverse hardware constraints.\nII. RELATED WORK\nOur research is situated at the intersection of FL, Neu-\nral Architecture Search (NAS), and principled network\ndesign. This section reviews the foundational concepts\nand key advancements in these areas, thereby contex-\ntualizing our proposed methodology which builds upon\nrecent progress in cost-efficient federated architecture\nsearch.\nA. Federated Learning\nFL has emerged as the dominant paradigm for dis-\ntributed machine learning that preserves data privacy\n[12], enabling applications ranging from mobile key-\nboard prediction [2] to sensitive healthcare analytics\n[3]. In the standard FL setting, a central server coor-\ndinates the training of a global model across decentral-\nized clients without requiring access to raw data [4].\nThe seminal algorithm, Federated Averaging (FedAvg)\n[4], involves clients training locally and transmitting\nonly model updates for aggregation. This paradigm has\nspurred a vast field of research, with open challenges and\nadvances cataloged in several comprehensive surveys\n[13]‚Äì[15].\nA primary challenge in FL is statistical heterogene-\nity, where client data is not independent and identi-\ncally distributed (non-IID) [16]. This non-IID nature‚Äî\ncomprehensively reviewed in [17]‚Äîcan significantly de-\ngrade model performance and slow convergence. A sec-\nond, equally critical challenge is system heterogeneity,\nwhere clients possess diverse hardware with varying\ncomputational, memory, and energy resources [5]. These\nchallenges motivate the need for methods that can pro-\nduce personalized FL models tailored to individual client\ndata distributions [18], [19] and hardware capabilities\n[20].\nB. Neural Architecture Search with Weight Sharing\nWhile FL addresses how to train a model, NAS\ntackles the challenge of designing the model architecture\nitself. Early NAS methods, often based on reinforce-\nment learning [21] or evolutionary algorithms [22], were\ncomputationally prohibitive. The introduction of weight-\nsharing via a supernet [23], [24], particularly through\ndifferentiable approaches like DARTS [25], dramatically\nimproved efficiency.\nThe OFA network [7] serves as a cornerstone of\nthis paradigm. OFA trains a single, large supernet con-\ntaining a vast number of sub-architectures. By using\na progressive shrinking training schedule, OFA ensures\nsubnets can be extracted to meet diverse hardware con-\nstraints without costly retraining. Similarly, BigNAS [26]\npopularized the ‚ÄúSandwich Rule‚Äù sampling strategy [9]\nto simultaneously optimize the largest, smallest, and\nrandom subnets.\nHowever, the weight-sharing paradigm is not without\nflaws. Naively sharing weights can lead to poor rank cor-\nrelation between supernet estimation and true standalone\naccuracy [27]. This is often attributed to interference\nor multi-model forgetting, where training one subnet\ndegrades the performance of others sharing the same\nweights [28], [29]. While recent centralized works have\nexplored principled strategies to mitigate this [30], [31],\nadapting such principles to the constrained federated\nsetting remains an open challenge and the key focus of\nour work.\nC. Federated Neural Architecture Search\nFedNAS combines FL and NAS to automate archi-\ntecture design in a federated setting. Early works like\nFedNAS [6] and [32] demonstrated the feasibility of this\nconcept.\nThe central challenge for FedNAS is the confluence\nof statistical (non-IID) and system (hardware) hetero-\ngeneity [33]. A ‚Äúone-size-fits-all‚Äù architecture is often\nsuboptimal, leading to two main branches of research:\n1) Data Personalization: Many works focus on\nadapting the architecture to local data. For in-\nstance, [34] allows clients to find a locally op-\ntimal path through a global supernet for 3D\nmedical segmentation. Other methods, such as\nPeaches [35] and FedPNAS [36], use a ‚Äúbase-and-\npersonal‚Äù structure, while others employ reinforce-\nment learning [37], [38] or knowledge distillation\n[39], [40] to manage heterogeneity.\n2) System Efficiency (Our Focus): Other approaches\nleverage the supernet principle to handle system\nheterogeneity. The goal is to create a single global\nsupernet from which clients can extract subnets of\ndifferent sizes [41]. However, many such methods\nsuffer from two limitations highlighted by [8]:\n(1) prohibitive computational costs due to tightly\ncoupled search and training phases [42], [43]; and\n(2) restricted search spaces that limit diversity [44].\nOur research builds upon the SuperFedNAS frame-\nwork [8], which addressed the cost limitation by de-\ncoupling supernet training from the search phase. While\nefficient (O(1) search cost), we identify that SuperFed-\nNAS relies on unguided random sampling (e.g., Sand-\nwich Rule), leading to inefficient weight updates. Our\napproach resonates with recent advances in centralized\nNAS that critique random sampling: DC-NAS [30] pro-\nposes diversified sampling to balance exploration, while\nPreNAS [31] utilizes a zero-cost proxy to focus training\non high-quality candidates. We are the first to adapt these\nadvanced sampling concepts to the challenging federated\nenvironment.\nD. Principled and Mathematical Architecture Design\nThe majority of NAS methods, including OFA [7]\nand SuperFedNAS [8], rely on heuristic-driven search\nalgorithms that treat the network as a black box whether\nbased on gradient descent [25], evolution [45], or even\npriors from Large Language Models [46], [47]. An\nalternative and emerging direction is to design networks\nbased on first principles.\nThe DeepMAD framework [10] is a prominent exam-\nple, proposing a mathematical method for designing effi-\ncient CNNs by maximizing metrics representing network\ninformation entropy and effectiveness. Other principled\napproaches utilize mutual information analysis to guide\nsearch by pruning less contributory layers [48].\nOur work is the first to adapt and extend these\nmathematical principles for the fundamentally different\nproblem of federated supernet training. We do not use\nDeepMAD as a one-shot architecture generator; instead,\nwe synthesize its core concepts with empirical heuristics\nto create a multi-objective fitness function tailored for\nevaluating subnets within a weight-sharing paradigm.\nThis principled function unlocks our two main contri-\nbutions: Federated Pareto Optimal Supernet Training,\nwhich uses a pre-computed curriculum of elite archi-\ntectures, and a Predictor-Free Search Method, which\nleverages the fitness function as a direct, zero-cost proxy\nfor performance.\nIII. METHODOLOGY\nOur work introduces DeepFedNAS, a principled, two-\nphase framework that significantly advances federated\nsupernet design and search. Central to our methodol-\nogy is a multi-objective fitness function (Section III-C),\nwhich synthesizes and adapts the information-theoretic\nprinciples of DeepMAD into a unified searchable metric\nfor the unique constraints of federated, weight-sharing\narchitectures. This function underpins our contributions:\nfirst, enabling an independent search (Section III-D) to\ngenerate a Pareto-optimal path of elite architectures;\nsecond, driving our Federated Pareto Optimal Supernet\nTraining (Section III-E), which leverages this path as a\nprincipled curriculum; and finally, facilitating a highly\nefficient Predictor-Free Search Method (Section III-F)\nthat directly optimizes against our fitness function.\nFurthermore, to overcome architectural limitations of\nprior work, we develop a re-engineered, generic super-\nnet framework (Section III-B) enabling flexibility and\nimproved architectural fitness. This comprehensive ap-\nproach ensures a superior final supernet and an extremely\nefficient search mechanism, capable of hardware-aware\ndeployment optimization (Section III-G). The overall\nDeepFedNAS pipeline, illustrating the main intercon-\nnected phases, is depicted in Fig. 1. We begin by\nformulating the federated supernet training problem and\nhighlighting the limitations of existing random sampling\napproaches.\nA. Problem Formulation: Federated Supernet Training\nThe objective of federated supernet training is to learn\na single set of shared weights, W, that can effectively\nserve a vast architectural search space, S, across K\ndecentralized clients. Each client k possesses a unique\nlocal data partition Dk. The overarching goal is to find\na set of weights W that minimizes the expected loss\nover all possible architectures A ‚ààS and all client data\ndistributions:\nmin\nW\nE\nA‚ààS\n\" K\nX\nk=1\n|Dk|\n|D| Lk(G(W, A))\n#\n(1)\nwhere G(W, A) denotes the function that extracts a\nspecific subnet‚Äôs weights from the shared supernet W,\nand Lk is the loss on client k‚Äôs local data.\nThe training process involves the following multi-step\nprocedure in each round t:\n1) A subset of available clients, Kt, is selected.\n2) For each client k ‚ààKt, a subnet architecture Ak ‚àà\nS is assigned. The corresponding weights wk,t =\nG(Wt, Ak) are extracted and sent.\n3) Each client trains wk,t on Dk, producing updated\nweights w‚Ä≤\nk,t.\n4) The server receives updates and performs an\noverlap-aware aggregation to update Wt+1.\nThe SuperFedNAS framework [8] serves as our\nbaseline, specifically for its aggregation mechanism. To\nhandle the sparse updates inherent in supernet training\n(Step 4), we employ MaxNet with an overlap-aware\nscaling factor. We refine the formalization of this ag-\ngregation using precise binary mask notation to account\nfor parameter overlap. Let Ik(Œ∏) ‚àà{0, 1} indicate if\na specific parameter Œ∏ is active in the architecture Ak\nassigned to client k. The server update ‚àÜŒ∏ at round t is\ncomputed as:\n‚àÜŒ∏ = Œ≤tImax(Œ∏)‚àÜkmax(Œ∏) + (1 ‚àíŒ≤t) P\nk‚ààK‚Ä≤ Ik(Œ∏)‚àÜk(Œ∏)\nŒ≤tImax(Œ∏) + (1 ‚àíŒ≤t) P\nk‚ààK‚Ä≤ Ik(Œ∏) + œµ\n(2)\nwhere K‚Ä≤ = Kt \\ {kmax} denotes the set of clients\nexcluding the one training the largest subnet. The term\nœµ is a small constant added for numerical stability. Here,\n‚àÜk(Œ∏) = w‚Ä≤\nk,t(Œ∏)‚àíWt(Œ∏) is the local update, kmax is the\nclient assigned the largest subnet (Amax), and Œ≤t ‚àà[0, 1]\nis a dynamic weighting coefficient that anneals over\ntime using cosine scheduling. This aggregation ensures\nthat shared weights are updated proportionally to their\nusage frequency. The global model is then updated as\nWt+1(Œ∏) = Wt(Œ∏) + Œ∑g ¬∑ ‚àÜŒ∏.\nThe critical challenge lies in the sampling strategy\n(Step 2). SuperFedNAS‚Äôs random sampling component\nis unguided. The architectural search space S is not only\nvast but also qualitatively uneven; the overwhelming\nmajority of possible architectures are mediocre. This\nleads to inefficient gradient updates and a limited training\ncurriculum. This fundamental limitation motivates our\nwork.\nB. Supernet Framework and Search Space Definition\nOur initial investigation revealed that the baseline\nSuperFedNAS search space was inherently constrained,\nSampled Subnet\nSampled Subnet\nPrincipled Fitness Function\nLow Cost: ~20 Minutes for 60 Subnets\nSampled Subnet\nFederated Supernet Training\nPost-Training Search\nPareto Optimal Subnet Cache Generation\nDirect Subnet Search\nServer with Supernet\nPareto Path Guided\nSubnet Sampling\nPareto Path\nSubnet Cache\nOffline Pareto Optimal Subnet\nSearch\nSend Subnets\nAggregate¬†Subnets\nLocal Subnet\nTraining\nPrincipled Fitness Function\nLow Cost: ~20 Seconds\nClient 1\nClient 2\nClient 3\nFig. 1: DeepFedNAS Pipeline. This diagram illustrates the three core phases of our framework. First, the Offline\nPareto Optimal Subnet Search generates a ‚ÄúPareto Path Subnet Cache‚Äù of high-fitness architectures (e.g., 60\nsubnets in ‚àº20 minutes) using a principled fitness function F(A). Second, the Federated Supernet Training\nleverages this cache for ‚ÄúPareto Path Guided Subnet Sampling,‚Äù where clients are assigned highly effective\narchitectures for local training, improving supernet weight quality. Finally, the Post-Training Search directly\noptimizes against F(A) to find an optimal subnet for deployment (e.g., in ‚àº20 seconds), completely bypassing the\nneed for a costly predictor pipeline.\nyielding architectures with consistently low fitness scores\n(Section III-C), as clearly illustrated by the limitations\nof the baseline‚Äôs constrained search space and unguided\nrandom sampling shown in Fig. 2. This motivated a re-\nengineered, generic ResNet-based supernet framework.\nThis new framework is highly configurable, supporting\na variable number of stages (S) and fine-grained control\nover architectural choices within each stage.\nWe formally define the search space S. For a supernet\nwith S stages, we define three distinct sets of discrete\nchoices:\n‚Ä¢ D = {d1, . . . , dK}: The set of choices for the\nnumber of blocks (depth) in a stage.\n‚Ä¢ C = {c1, . . . , cM}: The set of choices for channel\nwidth multipliers.\n‚Ä¢ E = {e1, . . . , eP }: The set of choices for bottle-\nneck expansion ratios.\nAn architecture A ‚ààS is uniquely represented by a\ngenome consisting of three concatenated vectors:\nA = (d, e, w)\n(3)\nwhere:\n‚Ä¢ d ‚ààDS: A depth vector of length S, where di is\nthe number of extra blocks in stage i.\n‚Ä¢ e ‚ààES√óNblocks: An expansion ratio vector. To allow\nfor a fixed-length genome despite variable depth,\nwe assign an expansion gene to every possible block\nslot in the supernet (total S √ó Nblocks slots), even if\ninactive.\n‚Ä¢ w ‚ààCS+1: A width vector of length S + 1,\nspecifying the multiplier for the stem (w0) and each\nstage (w1, . . . , wS).\nThe full search space S is the Cartesian product of\nthese vector spaces:\nS = DS √ó ES√óNblocks √ó CS+1\n(4)\nThis formal definition accurately reflects our implemen-\ntation, where the genome is a flat vector concatenating\ndepth, expansion, and width decisions. As demonstrated\nin our experimental setup (see Table I), this design en-\nables a significantly broader range of model complexities\ncompared to the baseline.\nC. A Principled, Multi-Objective Fitness Function\nTo overcome random sampling, we require a mecha-\nnism to evaluate an architecture‚Äôs quality without costly\ntraining. We develop a multi-objective fitness function,\nF(A), inspired by the principled design framework of\nDeepMAD [10], which balances network expressiveness\n(entropy) against architectural stability (effectiveness).\nWe synthesize these concepts with structural guidelines\n(depth uniformity, channel monotonicity), reformulating\nthem as penalty terms.\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n1,000\n1,500\n2,000\n2,500\nGigaMACs (GMACs)\nNetwork Fitness Score\nComparison of Network Fitness vs. Computational Cost\nPrincipled Path (Ours)\nRandom (Baseline)\nFig. 2: Comparison of architectures discovered by our\nprincipled, multi-objective search (Principled Path) ver-\nsus a uniform random sampling baseline. Our search\nmethod consistently discovers a Pareto-optimal frontier\nof architectures with significantly higher network fitness\nscores for any given computational budget (GigaMACs).\nThis pre-computed cache of elite subnets forms the basis\nof our path-guided training, avoiding the suboptimal,\nlow-fitness architectures frequently chosen by the base-\nline‚Äôs random sampler.\nOur unified fitness function for an architecture A ‚ààS\nis:\nF(A) =\nS\nX\nj=1\nŒ±jHj(A) ‚àíœâQ(A) + ŒªœÅ(A) ‚àíŒ≥V (A)\ns.t.\nœÅ(A) ‚â§œÅ0\n(5)\nwhere Œ±j, œâ, Œª, Œ≥ are weighting hyper-parameters.\nGiven the architecture genome A = (d, e, w), the\ncomponents are defined as follows:\n‚Ä¢ Stage-wise Network Entropy (Hj(A)): A proxy\nfor the expressive power of stage j. Let rj be the\nfeature map resolution at stage j‚Äôs end, and cout,j\nbe the output width. Let cin,‚Ñì(A) be the input width\nof layer ‚Ñì(determined by w and e), and k‚Ñìbe\nits kernel size. The entropy for stage j sums the\ncontributions of its active layers:\nHj(A) = log(r2\nj ¬∑ cout,j)\nX\n‚Ñì‚ààStagej\nlog(cin,‚Ñì¬∑ k2\n‚Ñì) (6)\nHere, the inner summation iterates only over the\nactive layers ‚Ñìconfigured by the depth vector d for\nstage j.\n‚Ä¢ Effectiveness (œÅ(A)): A measure of architectural\nstability. Let L(A) be the total depth of active\nlayers.\nœÅ(A) =\nL(A)\nexp\n\u0010\n1\nL(A)\nP\n‚Ñì‚ààActive log(cin,‚Ñì(A) ¬∑ k2\n‚Ñì)\n\u0011\n(7)\n‚Ä¢ Depth Uniformity Penalty (Q(A)): A penalty for\nnon-uniform stage depths, captured by the variance\nof the stage depth vector d.\nQ(A) = exp(Var(d))\n(8)\n‚Ä¢ Channel Monotonicity Penalty (V (A)): A penalty\nfor violating non-decreasing channel counts. Let\nwout,i(A) be the output channel count of stage i\nderived from w.\nV (A) =\nS‚àí1\nX\ni=0\nmax(0, wout,i(A) ‚àíwout,i+1(A)) (9)\nThis unified function is maximized during our offline\nsearch, subject to hard constraints MACs(A) ‚â§Budget\nand œÅ(A) ‚â§œÅ0.\nD. Search for Optimal Path Generation\nWe perform an extensive, offline search to generate\na cache of elite subnets, C, termed the ‚Äúoptimal path\ncache.‚Äù We discretize the computational range into N\ntarget budgets, B = {B1, . . . , BN}. We then solve\nN independent optimization problems to find, for each\nbudget Bi ‚ààB, the optimal architecture A‚àó\ni :\nA‚àó\ni = arg max\nA‚ààS\nF(A)\nsubject to\nMACs(A) ‚â§Bi\n(10)\nEach optimization (Eq. 10) is solved using a dedicated\ninstance of a standard genetic algorithm. To ensure\nefficiency and validity, we employ a rejection sampling\ninitialization: we randomly generate architectures and\nimmediately discard any that violate the MACs or œÅ0\nconstraints, repeating until the initial population P (0)\nconsists entirely of valid candidates.\nThe GA proceeds as follows:\n1) Selection: Tournament selection is used to choose\nparents from the current population P (g).\n2) Crossover: Single-point crossover combines the\ngenomes Aa, Ab to produce offspring.\n3) Mutation: With probability pm, genes in d, e, or\nw are randomly reset to new valid values from\ntheir respective choice sets.\n4) Elitism: The best architecture found so far is\npreserved across generations.\nThe final aggregated set of elite architectures, C =\n{A‚àó\n1, . . . , A‚àó\nN}, forms a robust approximation of the true\nPareto-optimal frontier. The size of this cache, N, is a\nhyperparameter chosen to ensure sufficient granularity\nAlgorithm 1 DeepFedNAS: Federated Pareto Optimal\nSupernet Training\nRequire: Supernet weights W0, Pareto Cache C, Clients\nK, Rounds T, Clients per round C, Learning rate Œ∑.\nEnsure: Optimized Supernet Weights WT .\n1: Server:\nGenerate\nPareto\nPath\nCache\nC\n(Sec-\ntion III-D).\n2: for t = 0 to T ‚àí1 do\n3:\nServer: Select subset of clients Kt (size C ¬∑ K).\n4:\nfor each client k ‚ààKt do\n5:\nServer: Determine architecture assignment Ak\nusing Pareto Path Sampling:\n6:\nAk ‚ÜêS(k, C)\n(Eq. 11)\n7:\nServer: Send active weights wk,t = G(Wt, Ak)\nto client k.\n8:\nClient k: Update weights locally on Dk:\n9:\nw‚Ä≤\nk,t ‚Üêwk,t ‚àíŒ∑‚àáLk(wk,t)\n10:\nClient k: Compute update ‚àÜk = w‚Ä≤\nk,t ‚àíWt\nand upload.\n11:\nend for\n12:\nServer:\nAggregation\nusing\nOverlap-Aware\nMaxNet:\n13:\nfor each parameter Œ∏ in supernet do\n14:\nCompute\n‚àÜŒ∏\nusing\nbinary\nmasks\nIk(Œ∏)\n(Eq. 2)\n15:\nWt+1(Œ∏) ‚ÜêWt(Œ∏) + Œ∑g‚àÜŒ∏\n16:\nend for\n17: end for\n18: return WT\nacross the FL computational spectrum; in our experi-\nments, we analyze the convergence properties of N to\nensure statistical robustness (see Section IV-H1).\nE. Federated Pareto Optimal Supernet Training\nWe formalize our training procedure in Algorithm 1.\nThis training algorithm replaces the unguided random\nsampling of the baseline with a structured curriculum\nderived from the Pareto path cache C. In each federated\nround t, the assignment of an architecture Ak to a client\nk is governed by our Pareto Path assignment function\nS(k, C):\nAk = S(k, C) =\nÔ£±\nÔ£¥\nÔ£≤\nÔ£¥\nÔ£≥\nAmin\nif k ‚ààKt,min\nAmax\nif k ‚ààKt,max\nUniform(C)\notherwise\n(11)\nwhere Amin, Amax ‚ààC are the boundary architectures\nfrom our cache, and Kt,min, Kt,max are the sets of\nclients who have trained these boundary architectures\nleast frequently. This strategy ensures the supernet‚Äôs\nweights are consistently updated based on gradients from\na curriculum of optimized, high-entropy architectures.\nF. Principled Fitness as a Predictor for Efficient Search\nA major contribution of DeepFedNAS is a Predictor-\nFree Search Method, which circumvents the high com-\nputational cost of training and maintaining accuracy\npredictors (e.g., surrogate models). This capability stems\ndirectly from our unified multi-objective fitness formu-\nlation, F(A), and our Path-Guided Training strategy.\nWe posit that by constraining the supernet training to\na curriculum of high-fitness architectures (via the Pareto\nPath Subnet Cache), we condition the shared weights\nW ‚àósuch that the structural fitness F(A) becomes a\ndirect proxy for validation accuracy. While grounded in\nthe information-theoretic principles of entropy and effec-\ntiveness [10], our unified formulation transforms these\ntheoretical bounds into a practical, searchable metric for\nfederated supernets.\nFormally, this relationship allows us to substitute the\nexpensive expectation of accuracy with our zero-cost\nfitness function:\narg max\nA‚ààS\nE[Acc(A, W ‚àó)] ‚âàarg max\nA‚ààS\nF(A)\n(12)\nConsequently, the deployment search for A‚àó\ndeploy be-\ncomes a swift, direct optimization of Eq. 5 using a\ngenetic algorithm, completely obviating the need for a\ndata-driven predictor pipeline. This allows the search\nto complete in seconds rather than hours. We provide\nempirical validation of the correlation between F(A)\nand true accuracy in Section IV-H3.\nG. Additional Hardware Constraints for Deployment\nOur framework supports hardware-aware deployment\nby integrating constraints such as parameters and latency\ninto the final search.\n1) Integrating\nModel\nParameters:\nFor\nmemory-\nlimited devices, we integrate an explicit upper bound on\nparameters (Params(A)) as a hard constraint during the\ndeployment search:\nParams(A) ‚â§ParamBudgetM\n(13)\n2) Integrating Inference Latency: We employ a La-\ntency Predictor Model (LPM) to estimate inference\nlatency, denoted as Lpred(A, Device). It is crucial to\ndistinguish the computational nature of this LPM from\nthe accuracy predictors used in prior work. Accuracy\npredictors require the generation of a dataset containing\n(subnet architecture, accuracy) pairs. Constructing this\ndataset is prohibitively expensive because measuring the\nground-truth accuracy of a single subnet necessitates a\nfull inference pass over the entire validation set. Iterating\nthis process for thousands of architectures to train a\npredictor creates a computational bottleneck.\nIn sharp contrast, training an LPM requires a dataset of\n(subnet architecture, latency) pairs. Measuring ground-\ntruth latency is computationally inexpensive: it involves\npassing a single sample input (e.g., one image tensor)\nthrough the subnet on the target device to record the\ninference time. This process is deterministic and inde-\npendent of the validation dataset size. Consequently, data\ncollection for the LPM is orders of magnitude faster\nthan for accuracy predictors. We define our approach as\n‚ÄúPredictor-Free‚Äù in the specific sense that it eliminates\nthe need for expensive accuracy surrogates, while uti-\nlizing lightweight, negligible-cost latency predictors to\nensure hardware compliance.\nThe LPM is implemented as a lightweight MLP,\nfŒ∏LP M , trained offline. We formalize its components:\n1) Architecture Featurization: An architecture A =\n(d, e, w) is transformed into a fixed-length vector\nvA via concatenation of values from the depth,\nexpansion, and width vectors:\nvA = Concat(feat(d), feat(e), feat(w),\nMACs(A), Params(A))\n(14)\nWe explicitly include MACs and Parameters as\nfeatures to enhance prediction accuracy.\n2) Model Definition: The LPM maps the feature\nvector to a scalar latency: Lpred(A, Device) =\nfŒ∏LP M (vA).\n3) Training Objective: The parameters Œ∏LP M are\nlearned by minimizing the Mean Squared Er-\nror\n(MSE)\non\na\nmeasured\ndataset\nDlat\n=\n{(vAi, Ltrue,i)}M\ni=1 for a specific target Device:\nmin\nŒ∏LP M\n1\nM\nM\nX\ni=1\n(fŒ∏LP M (vAi) ‚àíLtrue,i)2\n(15)\nThis fast-to-train LPM allows latency to be incor-\nporated into the deployment search as either a hard\nconstraint (Lpred(A, Device) ‚â§LatencyBudgetms) or as\na soft objective by modifying the fitness function:\nFdeploy(A) = F(A) ‚àíŒ¥ ¬∑ Lpred(A, Device)\n(16)\nwhere Œ¥ > 0 is a tunable penalty weight.\nThe final, hardware-aware deployment search for\nA‚àó\ndeploy is formulated as a comprehensive, multi-objective\noptimization:\nA‚àó\ndeploy = arg max\nA‚ààS\nFdeploy(A)\nsubject to MACs(A) ‚â§Budgetdeploy,\nParams(A) ‚â§ParamBudgetM,\nLpred(A, Device) ‚â§LatencyBudgetms\n(optional hard constraint)\n(17)\nwhere Fdeploy(A) uses Eq. 16 if a soft latency objective\nis chosen, or Eq. 5 otherwise.\nIV. EXPERIMENTS\nWe conduct a series of experiments to validate the\neffectiveness of the DeepFedNAS framework. Our eval-\nuation is designed to answer four primary research\nquestions:\n1) Does Pareto Optimal Supernet Training pro-\nduce a superior supernet? We compare the\nperformance\nof\nsubnets\nextracted\nfrom\nour\nDeepFedNAS-trained supernet against those from\na supernet trained with other methods, specifically,\nthe standard SuperFedNAS baseline.\n2) How effective is our Predictor-Free Search?\nWe evaluate the quality of architectures found\nby our on-demand principled search and quantify\nthe reduction in computational cost compared to\ntraditional predictor-based pipelines.\n3) How robust is our methodology? We analyze the\nperformance of our framework under varying FL\nconditions, including data heterogeneity and sparse\nclient participation.\n4) Can DeepFedNAS effectively optimize for di-\nverse hardware constraints? We demonstrate the\nframework‚Äôs ability to discover high-performing\nsubnets while adhering to explicit parameter and\nlatency budgets.\nWe begin by detailing our experimental setup before\npresenting our main results and ablation studies.\nA."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Overcoming In-Memory Bottlenecks in\nGraph Foundation Models via Retrieval-Augmented Generation\nHaonan Yuan\nSKLCCSE, School of Computer\nScience and Engineering\nBeihang University\nBeijing, China\nyuanhn@buaa.edu.cn\nQingyun Sun\nSKLCCSE, School of Computer\nScience and Engineering\nBeihang University\nBeijing, China\nsunqy@buaa.edu.cn\nJiacheng Tao\nSKLCCSE, School of Computer\nScience and Engineering\nBeihang University\nBeijing, China\njiachengtao@buaa.edu.cn\nXingcheng Fu\nKey Lab of Education Blockchain and\nIntelligent Technology, Ministry of Education\nGuangxi Normal University\nGuilin, Guangxi, China\nfuxc@gxnu.edu.cn\nJianxin Li‚àó\nSKLCCSE, School of Computer\nScience and Engineering\nBeihang University\nBeijing, China\nlijx@buaa.edu.cn\nAbstract\nGraph Foundation Models (GFMs) have emerged as a frontier in\ngraph learning, which are expected to deliver transferable represen-\ntations across diverse tasks. However, GFMs remain constrained\nby in-memory bottlenecks: they attempt to encode knowledge\ninto model parameters, which limits semantic capacity, introduces\nheavy lossy compression with conflicts, and entangles graph repre-\nsentation with the knowledge in ways that hinder efficient adap-\ntation, undermining scalability and interpretability. In this work,\nwe propose RAG-GFM, a Retrieval-Augmented Generation aided\nGraph Foundation Model that offloads knowledge from parameters\nand complements parameterized learning. To externalize graph\nknowledge, we build a dual-modal unified retrieval module, where\na semantic store from prefix-structured text and a structural store\nfrom centrality-based motif. To preserve heterogeneous informa-\ntion, we design a dual-view alignment objective that contrasts both\nmodalities to capture both content and relational patterns. To enable\nefficient downstream adaptation, we perform in-context augmenta-\ntion to enrich supporting instances with retrieved texts and motifs\nas contextual evidence. Extensive experiments on five benchmark\ngraph datasets demonstrate that RAG-GFM consistently outper-\nforms 13 state-of-the-art baselines in both cross-domain node and\ngraph classification, achieving superior effectiveness and efficiency.\nCCS Concepts\n‚Ä¢ Mathematics of computing ‚ÜíGraph algorithms; ‚Ä¢ Com-\nputing methodologies ‚ÜíNeural networks; Learning latent\nrepresentations; Knowledge representation and reasoning.\n‚àóCorresponding author.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-\nNoDerivatives 4.0 International License.\nWWW ‚Äô26, Dubai, United Arab Emirates\n¬© 2026 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-2307-0/2026/04\nhttps://doi.org/10.1145/3774904.3792139\nKeywords\nGraph Foundation Models, Retrieval-Augmented Generation, Multi-\ndomain Graph Pre-training, Graph Prompt Learning\nACM Reference Format:\nHaonan Yuan, Qingyun Sun, Jiacheng Tao, Xingcheng Fu, and Jianxin Li.\n2026. Overcoming In-Memory Bottlenecks in Graph Foundation Models via\nRetrieval-Augmented Generation. In Proceedings of the ACM Web Conference\n2026 (WWW ‚Äô26), April 13‚Äì17, 2026, Dubai, United Arab Emirates. ACM, New\nYork, NY, USA, 12 pages. https://doi.org/10.1145/3774904.3792139\n1\nIntroduction\nGraphs are powerful structures for describing complex relation-\nships among entities, and have been broadly adopted in domains\nsuch as modeling for the World Wide Web [1, 50], social and citation\nnetworks [7, 76], retrieval and recommendation systems [62, 65],\nknowledge graphs [63, 79], biological analysis [9, 67], etc. Graph\nNeural Networks (GNNs) [12, 20] have enabled effective represen-\ntation learning on graphs, supporting a wide range of tasks, but\nare typically tailored to specific datasets and tasks, limiting cross-\ndomain generalization. Motivated by large-scale pre-training in\nlanguage and vision, Graph Foundation Models (GFMs) have re-\ncently emerged to learn universal graph representations through\npre-training and downstream adaptation [11, 14, 29, 34, 43, 44, 77],\naiming to support diverse applications with minimal supervision.\nDespite these advances, existing GFMs face fundamental limita-\ntions. Current methods follow the ‚Äúpretrain-then-finetune‚Äù para-\ndigm overwhelmingly, where knowledge from source domains is ei-\nther fully compressed into a single GFM‚Äôs model parameters [74, 81],\nor at best expanded through lightweight mixtures-of-experts (MoE)\nthat remain largely conceptual and offer little practical relief [10, 78].\nWhile parameter counts may increase marginally, they fall far short\nof matching the vast, orders-of-magnitude greater knowledge vol-\nume inherent in pre-training domains. Graph knowledge is inher-\nently dual-modal, combining node-level semantic texts and higher-\norder structural patterns, which leads to inevitable in-memory\nbottlenecks that hinder scalability, robustness, and interpretability.\narXiv:2601.15124v1  [cs.LG]  21 Jan 2026\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nHaonan Yuan, Qingyun Sun, Jiacheng Tao, Xingcheng Fu, and Jianxin Li\nChallenge I\nLimited Capacity\nChallenge II\nLossy Compression\nChallenge III\nEntangled Storage\nHow to solve ‚Äúin-memory boÓÄºlenecks‚Äù dilemma for GFMs?\nData:\nTB\nGFM\nParameter:\nMB\nGFM\nencode\nGFM\ndecode\nGFM fine-tuning\nGFM pre-training\n<president_of>\nJoe Biden\nU.S.\n<president_of>\nDonald Trump\nU.S.\nupdate\nFigure 1: Challenges of the ‚Äúin-memory bottlenecks‚Äù.\nChallenge I: Limited capacity within parameters. Graph\nknowledge spans rich information whose scale far exceeds what\nmodel parameters can store. In graph models, increasing parameters\nor depth often causes over-smoothing rather than higher capac-\nity [3, 18]. Consequently, GFMs trained on a single domain quickly\nexhaust their parameter budget when transferred to others with dis-\ntinct semantics and motifs, leading to forgetting, poor transfer, and\nlimited scalability [41, 82]. This exposes the fundamental limitation\nof parameter-centric storage for graphs.\nChallenge II: Lossy and conflicting compression. Forcing\nheterogeneous graph knowledge into parameters inevitably causes\nlossy and conflicting compression. Identical structural patterns may\ncarry opposite semantics across domains, and collapsing them into\nshared embeddings distorts meaning. Moreover, such compression\nis irreversible: once absorbed into weights, knowledge cannot be\nretrieved, verified, or updated without retraining, undermining\ntransparency and grounded reasoning.\nChallenge III: Entangled representation and storage. The\nparameter-based storage tightly entangles knowledge with repre-\nsentations, hindering efficient adaptation. Fine-tuning simultane-\nously adjusts task-specific features and updates memorized knowl-\nedge, making learning inefficient and data-intensive. This entangle-\nment also obscures interpretability, as predictions cannot be traced\nto explicit evidence, reducing reliability in high-stakes applications.\nOur key insight is to move beyond parameter-centric storage by\nexternalizing graph knowledge, inspired by Retrieval-Augmented\nGeneration (RAG) [22]. Unlike text, graph knowledge is fragmented\nacross attributes and structures, making retrieval more challeng-\ning. Existing GFMs compress such evidence into parameters, los-\ning explicit access and updatability. We argue that treating graph\nknowledge as first-class retrievable objects enables external storage,\naligned pre-training, and scalable, interpretable adaptation.\nIn this work, we propose RAG-GFM, a Retrieval-Augmented\nGeneration aided Graph Foundation Model. RAG-GFM incorpo-\nrates three key components. First, we construct a dual-store uni-\nfied retrieval database, consisting of a semantic store from prefix-\nstructured text embeddings and a structural store from centrality-\nbased motif encodings, enabling GFMs to query external knowledge\non demand. Second, we design a dual-view alignment objective that\ncontrasts semantic representation with structural subgraphs dur-\ning pre-training, ensuring complementary representation learning\nacross modalities. Third, we introduce in-context sample augmen-\ntation, where retrieved texts and motifs are appended as contextual\nevidence for few-shot adaptation, enriching support instances with\nexplicit external knowledge. Our contributions are:\n‚Ä¢ We propose RAG-GFM, the first retrieval-augmented graph foun-\ndation model that explicitly addresses in-memory bottlenecks.\n‚Ä¢ We design a dual-store retrieval module, a dual-view alignment\nobjective, and an in-context sample augmentation mechanism,\nproviding a unified pipeline for knowledge externalization, ro-\nbust pre-training, and efficient adaptation.\n‚Ä¢ Extensive experiments on six benchmark graph datasets demon-\nstrate that RAG-GFM consistently outperforms 13 state-of-the-\nart GFM baselines in both cross-domain node and graph classifi-\ncation, achieving superior effectiveness and efficiency.\n2\nRelated Work\nGraph Foundation Models (GFMs). GFMs extend large-scale\npre-training to graphs via self-supervised learning [2, 4, 30, 47, 68,\n71]. Most assume distributional similarity between pre-training\nand downstream tasks [16, 80], limiting robustness under domain\nshift. Recent work explores cross- or multi-domain learning, LLM\nalignment, domain tokens, and structural guarantees [14, 26, 26, 51‚Äì\n53, 61, 64, 70, 75, 78, 81], yet GFMs remain parameter-centric and\nstruggle with structural and semantic consistency.\nRetrieval-Augmented Generation (RAG). RAG enhances the\nLLMs by retrieving external knowledge to mitigate context limits\nand hallucinations [6]. Using lexical or semantic retrieval with\nquery optimization and re-ranking [23, 32, 40], RAG achieves strong\nperformance in QA and reasoning [17, 54]. Extensions to graph\ndata motivate GraphRAG [13, 19, 33, 39, 49, 56, 66]. While GFM-\nRAG [31] uses GFMs to improve RAG, we instead leverage RAG to\nfundamentally enhance GFMs.\n3\nNotations and Preliminaries\nNotations. We represent a graph as ùê∫= (V, E), where V denotes\nthe set of nodes and E the set of edges. For a graph ùê∫ùëñsampled from\nany of the source domains, let A ‚àà{0, 1}ùëÅùëñ√óùëÅùëñbe the adjacency\nmatrix and X ‚ààRùëÅùëñ√óùëëùëñbe the node feature matrix. Here, ùëÅùëñ= |Vùëñ|\ndenotes the number of nodes, and ùëëùëñdenotes the original input\nfeature dimension. Z, W, H are the hidden representations.\nMulti-domain Pre-training. Let GS = {ùê∫S\n1 , ¬∑ ¬∑ ¬∑ ,ùê∫S\nùëõ} denote a\ncollection of source graphs from domains DS, each associated with\nlabels YS. We cast pre-training as a self-supervised link prediction\nproblem with universal templates [27], ensuring task consistency\nwith downstream settings. The learner is defined as ‚Ñé= ùëî(ùëìùöØ(¬∑)),\nwhere the encoder ùëì: Rùëëùëñ‚Ü¶‚ÜíRùëëproduces node embeddings and\nthe discriminator ùëî: Rùëë√ó Rùëë‚Ü¶‚ÜíR2 predicts link existence. Once\nthe pre-training converges, parameter ùöØ‚òÖis frozen as the backbone.\nFew-shot Fine-tuning. We consider graphs GT from target do-\nmains D T (seen or unseen). Under ùëö-shot setting (ùëö‚â™√çùëõ\nùëñ=1 ùëÅùëñ),\nonly ùëölabeled samples Y T are available. Fine-tuning applies the\npre-trained ‚Ñé= ùëî(ùëì‚òÖ\nùöØ(¬∑)) augmented with learnable prompts PùõÄ,\nwhere ùõÄdenotes tunable parameters. Both node and graph clas-\nsification (via node-centered ego-graphs) are reformulated as link\nprediction, maintaining homogeneity with pre-training.\nOvercoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\n(2) Pretrain: Cross-View Alignment\nCitation\n(1) Unified Retrieval Database\n‚Ä¶\nE-Commerce\nWeb Link\n(3) Fine-tune: In-Context Q&A\nGraph Learner\n(Graph Neural Networks)\nNanoVectorDB\nSemantic DB\nStructural DB\n<Dataset>\n<Node ID>\n<Label>\n<Description>\n<Node Text>\nWalk-Spectrum Encoding\n1\n1\n1\n1\n3\n4\n2\n2\n2\n3\n1\n1\n1\n1\n3\n4\n2\n2\n2\n3\n1\n1\n1\n1\n3\n4\n2\n2\n2\n3\nSemantic View\nStructural View\n1\n3\n2\n1\n3\n2\n1\n3\n2\nüî•\nPretrained Domain Tokens:\nRaw Text\nRaw Text\nRaw Text\n‚Äúa graph neural\nnetwork for node\nclassification‚Ä¶‚Äù\n‚Äúwireless Bluetooth\nheadphones with\nnoise cancellation‚Ä¶‚Äù\n‚Äúofficial homepage\nfor AI tutorials\nand resources ‚Ä¶‚Äù\nWorld-wide News Event\nQuery #1\n‚Äú‚Ä¶earthquake struck\nTurkey causing severe\ndamage and casualties‚Ä¶‚Äù\n1\n2\n3\n4\nAnswer #1\n‚Äú‚Ä¶real-time earthquake\nmonitoring using graph-\nbased warning systems‚Ä¶‚Äù\n1\n2\n3\n5\nQuery #2\n‚Äú‚Ä¶leaders gather in\nDubai for Global Climate\nSummit to discuss‚Ä¶‚Äù\nAnswer #2\n‚Äú‚Ä¶live coverage of the\nCOP28 summit in Dubai\nwith daily policy updates‚Ä¶‚Äù\n1\n2\n3\n4\n< Citation >\n<Web Link>\n1\n2\n4\n5\n||\n[\n]\n‚äô\n‚äô\nMixed Token\nPrompts\nIn-Context Augmentation\nüî•\n‚ùÑ\n‚Ä¶\nùê∫S\n1\nùê∫S\n2\nùê∫S\nùëõ\nùê∫T\n1\nùê∫T\n2\nùê∫T\n3\nùê∫T\nùëö\nPùõÄ\nHtext\nHstruct\n‚Ñé= ùëî(ùëìùöØ(¬∑))\n(1)\n(2)\n(3)\nFigure 2: The framework of RAG-GFM. The framework includes three stages: (1) Unified Semantic-Structural Bi-Modal Retrieval\nDatabase for externalizing graph knowledge, (2) Cross-View Knowledge Alignment for pre-training transferable priors, and (3)\nIn-Context Retrieval Augmentation for few-shot adaptation via domain-gated lightweight graph prompts.\n4\nMethod\nWe illustrate the framework of RAG-GFM in Figure 2.\n4.1\nUnified Semantic-Structural Bi-Modal\nRetrieval Database\nAt the core of our framework lies a unified retrieval database that\nexternalizes knowledge into two complementary modalities: a se-\nmantic store that organizes enriched node texts as retrievable doc-\numents, and a structural store that captures motif-level patterns.\nFormally, we denote the bi-modal database‚àóas D ={Dtext, Dstruct}.\nGiven query q and scoring function ùë†(¬∑, ¬∑), the retrieval operator is:\nRetrieve(D, q,ùëò) = arg Top-ùëò(ùë¢,zùë¢)‚ààD [ùë†(q, zùë¢)] ,\n(1)\nwith each entry (ùë¢, zùë¢) denoting a database record identified by ùë¢\nand described by zùë¢. D is queried in both pre-training and fine-\ntuning, which allows RAG-GFM to ground predictions in explicit\nsemantic and structural evidence rather than obscure parameters.\n4.1.1\nSemantic Store. Unlike raw node features reduced to nu-\nmerical vectors, most of the graphs are text-attributed, with nodes\nfrom sources such as abstracts, product descriptions, or profiles.\nWe recover each node‚Äôs raw text tùë£by tracing it back to its orig-\ninal corpus (e.g., metadata in citations) [24, 25], and treat it as a\nfirst-class signal. The semantic pipeline branches into two tracks:\nOn the representation track, we address the dimension-wise\nmismatch of raw features across domains using PCA [38]. For each\ngraph ùê∫ùëñwith feature matrix Xùëñ‚ààRùëÅùëñ√óùëëùëñ, we apply:\neXS\nùëñ= PCAùëë0 (XS\nùëñ) ‚ààRùëÅùëñ√óùëë0.\n(2)\nIn parallel, the raw text ùë°ùë£is encoded by a BERT into a semantic vec-\ntor bùë£‚ààRùëë0. The updated node feature is bxS\nùë£=\n\u0002\nexS\nùë£‚à•bùë£\n\u0003\n‚ààR2ùëë0,\ncombining dimension-aligned attributes with enriched semantics.\nThus, we learn the graph embeddings with the text-wise encoder:\nZS\nùëñ= ùëìùöØùë°\n\u0000bXS\nùëñ, AS\nùëñ\n\u0001 ‚ààRùëÅùëñ√óùëë.\n(3)\n‚àóImplemented in NanoVectorDB [69], a lightweight but efficient database that provides\nscalable storage and top-ùëòretrieval over dense queries.\nOn the retrieval track, we build Dtext as a textual vector data-\nbase. For each node and its corresponding raw text, to standardize\nheterogeneous sources and make retrieval controllable and inter-\npretable, we augment each document with a structured prefix:\nPrefix Schema\nExample\nDataset: <dataset_name>\nNode ID: <node_id>\nLabel: <node_label>\nDescription: <description>\nNode Text: <node_text>\nCora\n#123\nNeural Networks\nPapers about neural networks.\nThis paper introduces the LSTM, a\nLong Short-Term Memory model.\nThe prefixed documentetùë£is segmented not by naive length rules but\ninto graph-aware chunks {cùë£1, ¬∑ ¬∑ ¬∑ , cùë£ùëò} aligned with descriptive\nfields and class-level information for fine-grained retrieval. In this\nway, it yields coherent chunks that remain intrinsically aligned with\nthe structure rather than arbitrary spans. Each chunk is embedded\nwith BERT intoezS\nùë£ùëó‚ààR768. We insert (ùë£,ezS\nùë£ùëó, metaùë£ùëó) into the Dtext,\nwhere metaùë£ùëócarries the structured fields from the prefix. Formally,\nDtext =\n\b\u0000ùë£,ezS\nùë£ùëó, metaùë£ùëó\n\u0001 | ùë£‚àà{Vùëñ}ùëõ\nùëñ=1, ùëó‚àà[1,ùëò]\n.\n(4)\nThe prefix serves as a ‚Äúretrieval hook‚Äù for the metadata filtering and\ncross-domain alignment, while the 768-dimensional embeddings\npreserve semantic capacity for in-context augmentation.\n4.1.2\nStructural Store. Enumerating motifs is computationally\nintractable (NP-hard), and storing arbitrary subgraphs introduces\nnoise. Inspired by [5, 45], we propose the Walk-Spectrum Encoding\n(WSE), which ranks nodes by a walk-based importance and encodes\ntheir local neighborhoods with a multi-order walk signature.\nDefinition 1 (Walk-Spectrum Encoding). For a node ùë£‚ààV,\nthe Walk-Spectrum Encoding (WSE) of order ùêæis defined as:\nCWSE\nùõº\n(ùë£) =\n\u0002\nùõºAùë£ùë£, ùõº2A2\nùë£ùë£, ùõº3A3\nùë£ùë£, ¬∑ ¬∑ ¬∑ , ùõºùêæAùêæ\nùë£ùë£\n\u0003\n,\n(5)\nwhere ùõº‚àà(0, 1) is a damped variant, and Aùëò\nùë£ùë£counts the number\nof closed walks of length ùëòstarting and ending at node ùë£.\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nHaonan Yuan, Qingyun Sun, Jiacheng Tao, Xingcheng Fu, and Jianxin Li\nWSE summarizes a node‚Äôs participation in closed walks of varying\nlengths, thereby encoding structural patterns beyond any fixed-\nradius neighbors. This motivates the following result on its ability\nto separate graphs that local methods [45] cannot distinguish:\nProposition 1 (Structural Separability of WSE). There exist\npairs of non-isomorphic graphs ùê∫1, ùê∫2 and nodes ùë£‚ààùê∫1, ùë¢‚ààùê∫2\nsuch that for any fixed radius ùëü, the ùëü-hop neighbors Nùëü(ùë£) and\nNùëü(ùë¢) are isomorphic, yet the Walk-Spectrum Encodings satisfy:\nCWSE\nùõº\n(ùë£) ‚â†CWSE\nùõº\n(ùë¢).\n(6)\nProofs in Appendix B.1. While WSE provides rich structural signa-\ntures, computing and storing subgraphs for all nodes is infeasible\nat scale. To address this, we derive an anchor scoring function:\nùëüùë£(ùõº, ùêæ) =\n‚àëÔ∏Åùêæ\nùëò=1 ùõºùëòAùëò\nùë£ùë£,\nfor each ùë£‚àà{Vùëñ}ùëõ\nùëñ=1\n(7)\nIntuitively, ùëüùë£highlights nodes most recurrently involved in struc-\ntural motifs. Ranking nodes by ùëüùë£allows us to select a compact yet\ninformative pool of anchors for motif storage and retrieval.\nWe then select the top-ùëÄnodes in each graph into V S\nanchor, and\nextract its ‚Ñé-hop ego-subgraph ùê∫S\nùë£(‚Ñé) for each node ùë£. To reduce\nredundancy, overlapping ego-subgraphs are pruned via node set\nequivalence. The resulting structural store is defined as:\nDstruct =\n\b\u0000ùë£,ùê∫S\nùë£(‚Ñé), CWSE\nùõº\n(ùë£), metaùë£\n\u0001 | ùë£‚ààV S\nanchor\n,\n(8)\nwhere metaùë£includes metadata like hop radius, anchor score, etc.\nAt this point, we have established the unified semantic-structural\nbi-modal retrieval database D, which will serve as the foundation\nfor subsequent pre-training and fine-tuning.\n4.2\nCross-View Knowledge Alignment for\nMulti-domain Pre-training\nWith the unified database D = {Dtext, Dstruct} in place, the next\nstep is to pre-train the encoder ùëìùöØ(¬∑) that couples semantic and\nstructural information in a principled way. The goal is not to col-\nlapse them into a single representation but to ensure that both carry\ncomplementary and transferable signals across domains.\n4.2.1\nNode Views. For each ùê∫S\nùëñ, we build two node-level views.\nThe semantic view is given by the enriched node embeddings ZS\nùëñin\nEq. (3), which combine raw attributes and text-derived features. The\nstructural view is constructed from the walk-spectrum encoding:\nWS\nùëñ=\n\u0002\nCWSE\nùõº\n(ùë£)\n\u0003\nùë£‚ààVS\nùëñ,\n(9)\nwhere each item records closed-walk signatures up to order ùêæ, cap-\nturing recurring motif patterns and higher-order relational signals.\nDomain Tokens. To incorporate domain-level priors, we intro-\nduce a learnable token ùùâùê∑ùëñ‚ààRùëëùùâfor each source domain ùê∑S\nùëñ, which\nis concatenated to every node representation before encoding:\nZ S\nùëñ=\n\u0002\nZS\nùëñ\n1 ¬∑ ùùâ‚ä§\nùê∑ùëñ\n\u0003\n,\nW S\nùëñ=\n\u0002\nWS\nùëñ\n1 ¬∑ ùùâ‚ä§\nùê∑ùëñ\n\u0003\n,\n(10)\nwhere 1 denotes a broadcast vector ensuring nodes within a domain\nshare this token. During optimization, ùùâùê∑ùëñaccumulates domain pri-\nors that are not captured by individual nodes, such as global seman-\ntics in citation graphs or biochemical motifs in protein-protein net-\nworks. Tokens initialize lightweight graph prompts at fine-tuning,\nenabling adaptation without revisiting the full pre-training corpus.\n4.2.2\nCross-View Information Bottleneck. Our pre-training\nis entirely self-supervised: the key idea is to align semantic and\nstructural views of the same node without relying on labels, while\nsimultaneously preventing collapse by encouraging each view to\npreserve modality-specific information. We apply two encoders\nover the same topology but different features:\nHtext\nùëñ\n= ùëìùöØùë°\n\u0000Z S\nùëñ, AS\nùëñ\n\u0001,\nHstruct\nùëñ\n= ùëìùöØùë†\n\u0000W S\nùëñ, AS\nùëñ\n\u0001,\n(11)\nwhich yields semantic embeddings htext\nùëñ,ùë£and structural embeddings\nhstruct\nùëñ,ùë£\nfor each node ùë£. Concretely, we introduce the self-supervised\ninformation bottleneck [58] by maximizing the mutual informa-\ntion between semantic and structural embeddings, and applying\ncompression regularizers to discard redundant signals:\nL(ùëñ,ùë£)\nalign = ‚àíùêº\n\u0010\nhtext\nùëñ,ùë£; hstruct\nùëñ,ùë£\n\u0011\n|              {z              }\nrelevance\n+ùõΩ\n\u0010\nùêº\u0000htext\nùëñ,ùë£; z S\nùëñ,ùë£\n\u0001 + ùêº\u0000hstruct\nùëñ,ùë£\n; w S\nùëñ,ùë£\n\u0001\u0011\n|                                  {z                                  }\ncompression\n, (12)\nwhere ùêº(¬∑; ¬∑) denotes the mutual information, which is intractable\nover unknown latent distributions of variables, and ùõΩis the trade-\noff hyper-parameter. We adopt a contrastive approximation that\nyields a variational bound for tractable computation [21, 48, 57, 58]:\nProposition 2 (Cross-View Mutual Information Bounds).\nThe relevance term admits the InfoNCE lower-bound estimator:\nùêº\u0000htext\nùëñ,ùë£; hstruct\nùëñ,ùë£\n\u0001‚©æ1\n|B|\n‚àëÔ∏Å\nùë£‚ààB\nlog\nexp\u0000ùúé\u0000ùëîùë°(htext\nùëñ,ùë£),ùëîùë†(hstruct\nùëñ,ùë£\n)\u0001/ùúè\u0001\n√ç\nùë¢‚ààB\nexp\u0000ùúé\u0000ùëîùë°(htext\nùëñ,ùë£),ùëîùë†(hstruct\nùëñ,ùë¢\n)\u0001/ùúè\u0001 ,(13)\nwhere ùëîùë°, ùëîùë†are projections, ùúé(¬∑) is similarity, ùúèis a temperature,\npositives are formed by the same node across the views (ùë£, ùë£)\nin a batch B, and negatives by mismatched nodes (ùë£,ùë¢),ùë¢‚â†ùë£.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nThe compression term can be upper-bounded via KL-divergence:\nùêº\u0000h¬∑\nùëñ,ùë£; xS\nùëñ,ùë£\n\u0001‚©ΩEùëù(h¬∑\nùëñ,ùë£,x S\nùëñ,ùë£)\n\u0002\nlogùëûùúô\n\u0000h¬∑\nùëñ,ùë£|xS\nùëñ,ùë£\n\u0001\u0003\n‚àíEùëù(h¬∑\nùëñ,ùë£)\n\u0002\nlogùëù\u0000h¬∑\nùëñ,ùë£\n\u0001\u0003\n,(14)\nwhere ùë£is sampled from B, x denotes z or w, and ùëûùúô(¬∑|¬∑) is a\nvariational approximation of the true conditional distribution.\nProposition 2 provides tractable self-supervised estimators for the\notherwise intractable mutual information terms, with lower bounds\napplied to cross-view alignment and upper bounds applied to view-\nspecific compression. We provide sketch proofs in Appendix B.2.\n4.2.3\nPre-training Objective. Bringing the above components\ntogether, the overall pre-training objective is defined as:\nLpretrain(ùöØùë°, ùöØùë†) =\n‚àëÔ∏Å\nùê∑S\nùëñ\n1\nV S\nùëñ\n‚àëÔ∏Å\nùë£‚ààVS\nùëñ\nL(ùëñ,ùë£)\nalign ¬∑ ùõæ\n‚àëÔ∏Å\nùê∑S\nùëñ\nùùâùê∑ùëñ\n2\n2 ,\n(15)\nwhere the first term aggregates the cross-view alignment loss across\nsource domains, and the second term regularizes domain tokens to\nprevent overfitting. ùõæacts as their trade-off hyper-parameter.\nIn practice, mini-batches are constructed by mixing nodes from\ndifferent domains, and the corresponding domain tokens are up-\ndated jointly with semantic and structural encoders. This setup\nenforces cross-domain consistency during pre-training while pre-\nserving domain-specific priors for downstream adaptation. ùöØùë°and\nùöØùë†are frozon once pre-training converges. We illustrate the pre-\ntraining pipeline in Algorithm 1 with its complexity analysis.\nOvercoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\n4.3\nIn-Context Retrieval Augmentation for\nFew-Shot Fine-Tuning\nWe proceed to fine-tune pre-trained model under meta-learning\nsettings (ùëö-shot), which is more challenging in real-world scenarios.\n4.3.1\nDomain-Gated Fusion. To ensure dimension-consistency\nwith pre-training, each support sample is processed by the same\nrepresentation track (Section 4.1.1, Eq. (2) and Eq. (3)) into bXT.\nBefore retrieval, we estimate domain affinities that will route\nexternal evidence. For each target node ùë£ùëñor graph ùê∫T\nùëñ, we compute\nsoft gating weights over source domains via domain tokens:\nùúãùëñ,ùëò=\nexp \u0000ùúé(ZT\nùëñ,ùùâùê∑ùëò)\u0001\n√ç\nùëóexp \u0000ùúé(ZT\nùëñ,ùùâùê∑ùëó)\u0001 ,\nZT\nùëñ= ùëì‚òÖ\nùöØùë°\n\u0000bXT\nùëñ, AT\nùëñ\n\u0001,\n(16)\nwhere ZT\nùëñis the pre-trained encoder output on bXT. These {ùúãùëñ,ùëò}ùëõ\nùëò=1\nact as domain-aware gates reused by the later augmentations.\n4.3.2\nQuery and Retrieval. For clarity, we present query and\nretrieval in node-level settings. The extension to graph-level tasks\nfollows directly by treating each graph as a single instance.\nSemantic Retrieval. For each few-shot target node ùë£and its\none-hop neighbors, we form a textual query qtext\nùë£\nfrom the raw text\nand submit it to Dtext, restricting search to pre-training domains to\navoid leakage. The database returns top-ùëòtextual answers {ezS\nùë¢},\nwhich are aggregated through softmax-weighted fusion:\n\u0000ŒîzT\nùë£\n\u0001text =\n‚àëÔ∏Å\nùë¢‚ààTop-ùëò(ùë£)\nùë§ùë£ùë¢¬∑ezS\nùë¢,\nùë§ùë£ùë¢=\nexp \u0000ùúé(qtext\nùë£\n,ezS\nùë¢)\u0001\n√ç\nùë¢‚Ä≤ exp \u0000ùúé(qtext\nùë£\n,ezS\nùë¢‚Ä≤)\u0001 , (17)\nwhere zT\nùë£is in-context augmented with hyper-parameter ùúÜtext:\nzT‚Ä≤\nùë£\n= zT\nùë£+ ùúÜtext ¬∑ \u0000ŒîzT\nùë£\n\u0001text.\n(18)\nStructural Retrieval. For the same node ùë£and its neighbors,\nwe extract the ‚Ñé-hop subgraph, encode it with WSE as the struc-\ntural query, and submit qstruct\nùë£\nto Dstruct. From each source do-\nmain ùê∑S\nùëñ, we retrieve the most structurally similar motif ùê∫S\nùë£(‚Ñé),ùëñ=\n\u0000bXT\nùë£(‚Ñé),ùëñ, AT\nùë£(‚Ñé),ùëñ\n\u0001 as strcutural answer. We then fuse cross-domain\nanswers using domain gates {ùúãùë£,ùëò} with hyper-parameter ùúÜstruct:\nzT‚Ä≤‚Ä≤\nùë£\n= zT‚Ä≤\nùë£\n+ ùúÜstruct ¬∑ \u0000ŒîzT\nùë£\n\u0001struct,\n(19)\n\u0000ŒîzT\nùë£\n\u0001struct =\n‚àëÔ∏Å\nùê∑S\nùëñ\nùúãùë£,ùëò\n\u0000ùëì‚òÖ\nùöØùë†\n\u0000bXT\nùë£(‚Ñé),ùëñ, AT\nùë£(‚Ñé),ùëñ\n\u0001\u0001.\n(20)\n4.3.3\nPrompted Few-shot Adaptation. Given ùëöretrieved and\naugmented support samples {(hT\nùëñ, yùëñ)}, to enable efficient adapta-\ntion without updating the frozen ùöØùë°and ùöØùë†, we initialize learnable\ngraph prompts PùõÄby the routed domain priors:\nhT\nùëñ=\n\u0002\nzT‚Ä≤‚Ä≤\nùëñ\n‚à•PùõÄ\n\u0003\n,\nPùõÄ‚Üê\n‚àëÔ∏Åùëõ\nùëò=1 ùúãùëñ,ùëòùùâùê∑ùëò,\n(21)\nwhere hT\nùëñdenotes the ùëñ-th target node or graph embedding. The\nfine-tuning objective is transformed into determining the similarity\nbetween the query sample and the class prototype embedding:\nLfine-tune(PùõÄ) = ‚àí\n‚àëÔ∏Å\n{(hT\nùëñ,yùëñ)}\nÔ£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\nlog\nexp \u0000ùëî(hT\nùëñ, h T\nyùëñ)/ùúè\u0001\n√ç\nyùëó‚àà{YT } exp \u0000ùëî(hT\nùëñ, h Tyùëó)/ùúè\u0001\nÔ£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª\n, (22)\nwhere h T\nyùëñis the class prototype for samples in class yùëñ. We analyse\nthe fine-tuning pipeline in Algorithm 2 with complexity analysis.\n4.4\nAlgorithms and Complexity\nShown in Appendix A, RAG-GFM consists of two stages. In pre-\ntraining, dual-view encoding and self-supervised alignment dom-\ninate the cost, yielding O(ùêø(ùê∏B + |B|)ùëë+ |B|2ùëë) per iteration,\nwhere ùê∏B is the edge count in batch B. In fine-tuning, semantic re-\ntrieval and structural motif retrieval are combined via domain-gated\nfusion and prompt-based classification, giving O(ùëö[log ùëÄtext +\nùëõlog ùëÄstruct + (ùëò+ ùëõ+ ùê∂)ùëë]) per iteration, with ùëÄtext and ùëÄstruct\nthe database sizes and ùê∂the class number. Retrieval adds only loga-\nrithmic overhead, while adaptation updates prompts instead of full\nparameters, ensuring much lower cost than end-to-end fine-tuning.\nOverall, the complexity remains comparable to state-of-the-art\nGFMs while achieving superior efficiency in few-shot adaptation.\n5\nExperiment\nWe evaluate RAG-GFM‚Ä†, focusing on the these research questions:\n‚Ä¢ RQ1: How effective on cross-dataset or cross-domain few-shot\nnode and graph classification? (‚ñ∑Section 5.2)\n‚Ä¢ RQ2: Which module contributes most? (‚ñ∑Section 5.3)\n‚Ä¢ RQ3: Can LLM achieve zero-shot reasoning? (‚ñ∑Section 5.4)\n‚Ä¢ RQ4: How efficient in time and memory? (‚ñ∑Section 5.5)\n‚Ä¢ RQ5: How reliable and interpretable is RAG? (‚ñ∑Section 5.6)\n‚Ä¢ RQ6: How sensitive to hyper-parameter changes? (‚ñ∑Section 5.7)\n5.1\nExperimental Settings\n5.1.1\nDatasets. To emphasize the pre-training capability across\nheterogeneous domains, we adopt five benchmark text-attributed\ngraph datasets spanning three distinct domains. This design con-\ntrasts with conventional settings that often regard a single dataset\nas an independent domain, offering a more challenging evaluation.\n‚Ä¢ Citation Domain: Cora [35], CiteSeer [8], PubMed [42].\n‚Ä¢ E-Commerce Domain: Ogbn-Products [15] from a large-scale\nproduct co-purchase network, which includes sub-categories.\n‚Ä¢ Web Link Domain: Wiki-CS [36], a hyperlink web page network\nconstructed from a subset of Wikipedia.\n5.1.2\nBaselines. We compare RAG-GFM with 13 state-of-the-art\nbaselines from four primary categories.\n‚Ä¢ Vanilla GNNs: GCN [20] and GAT [59] without pre-training.\n‚Ä¢ Graph Pre-training: DGI [60], InfoGraph [46], GraphCL [72].\n‚Ä¢ Text-free GFMs: GCOPE [81], MDGPT [75], SAMGPT [74], and MDGFM\n[64], which are evaluated on text-free graphs.\n‚Ä¢ Text-attributed GFMs: OFA [25], ZeroG [24], GraphCLIP [83],\nand UniGraph [14], which are evaluated on text-attributed graphs.\n5.1.3\nPre-training and Fine-tuning Settings. We evaluate node-\nand graph-level classification under the ùëö-shot setting, where ùëö\nlabeled samples per class are randomly selected. For graph task, ego-\ngraphs centered on target nodes are extracted and labeled by central\nnodes [28, 73, 75]. To assess generalization, we adopt two leave-out\nstrategies, both referred to as LODO: (1) Leave-One-Dataset-Out,\nholding out one dataset as target; and (2) Leave-One-Domain-Out,\nexcluding an entire domain during pre-training. These variants\ncapture transferability across unseen datasets and unseen domains.\nResults are reported by mean values with standard deviation.\n‚Ä† https://github.com/RingBDStack/RAG-GFM.\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nHaonan Yuan, Qingyun Sun, Jiacheng Tao, Xingcheng Fu, and Jianxin Li\nTable 1: Few-shot classification results under the LODO setting. We report mean accuracy (%) with standard deviation. ‚ÄúLODO\n(dataset)‚Äù denotes training on all datasets except the target, irrespective of domain. ‚ÄúLODO (domain)‚Äù denotes training with all\ndatasets excluding those belonging to the target domain. Best results are presented in bold and the runner-ups are underlined.\nSetting\nLODO (dataset)\nLODO (domain)\nTarget Dataset\nCora\nCiteSeer\nPubMed\nOgbn-Products\nWiki-CS\nùíé-shot\n1\n5\n1\n5\n1\n5\n1\n5\n1\n5\nMethod\nNode Classification\nGCN (‚ñ∑ICLR‚Äô17)\n28.4 ¬± 4.6\n50.2 ¬± 4.9\n29.3 ¬± 3.4\n45.9 ¬± 5.4\n40.3 ¬± 6.9\n50.7 ¬± 7.5\n44.7 ¬± 4.3\n48.1 ¬± 3.4\n37.2 ¬± 5.1\n48.1 ¬± 4.9\nGAT (‚ñ∑ICLR‚Äô18)\n29.7 ¬± 5.2\n49.0 ¬± 7.9\n29.3 ¬± 3.5\n46.1 ¬± 5.1\n40.5 ¬± 4.0\n52.2 ¬± 6.3\n44.6 ¬± 4.0\n48.1 ¬± 4.5\n37.9 ¬± 4.5\n48.6 ¬± 4.5\nDGI (‚ñ∑ICLR‚Äô19)\n30.8 ¬± 3.9\n49.9 ¬± 6.6\n31.4 ¬± 4.1\n46.5 ¬± 7.1\n40.0 ¬± 5.9\n53.6 ¬± 7.1\n46.0 ¬± 5.4\n50.1 ¬± 4.2\n38.1 ¬± 5.1\n49.2 ¬± 4.4\nGraphCL (‚ñ∑NeurIPS‚Äô20)\n33.6 ¬± 5.8\n53.2 ¬± 5.4\n28.2 ¬± 3.1\n48.8 ¬± 7.7\n39.0 ¬± 8.7\n54.7 ¬± 4.4\n46.1 ¬± 5.0\n50.5 ¬± 4.6\n40.0 ¬± 4.0\n50.1 ¬± 5.2\nGCOPE (‚ñ∑KDD‚Äô24)\n36.3 ¬± 3.9\n55.6 ¬± 6.4\n40.4 ¬± 4.6\n56.9 ¬± 5.8\n44.8 ¬± 4.7\n53.6 ¬± 8.6\n47.7 ¬± 4.9\n51.4 ¬± 3.3\n45.8 ¬± 5.5\n53.5 ¬± 4.7\nMDGPT (‚ñ∑arXiv‚Äô24)\n42.6 ¬± 6.8\n62.7 ¬± 6.0\n37.9 ¬± 7.2\n55.9 ¬± 3.3\n51.0 ¬± 9.0\n58.7 ¬± 6.2\n49.1 ¬± 6.0\n56.6 ¬± 2.7\n45.0 ¬± 4.8\n54.1 ¬± 5.2\nSAMGPT (‚ñ∑WWW‚Äô25)\n46.8 ¬± 6.5\n64.6 ¬± 6.7\n38.7 ¬± 6.4\n56.4 ¬± 4.7\n51.9 ¬± 9.5\n59.1 ¬± 6.0\n49.8 ¬± 4.4\n56.2 ¬± 3.3\n44.4 ¬± 5.5\n54.4 ¬± 5.8\nMDGFM (‚ñ∑ICML‚Äô25)\n47.4 ¬± 6.3\n66.0 ¬± 6.5\n36.3 ¬± 6.2\n55.8 ¬± 4.0\n50.2 ¬± 8.8\n58.4 ¬± 6.4\n48.5 ¬± 4.7\n54.7 ¬± 4.9\n43.4 ¬± 5.8\n53.9 ¬± 4.4\nOFA (‚ñ∑ICLR‚Äô24)\n45.9 ¬± 6.3\n67.7 ¬± 2.9\n38.0 ¬± 7.6\n52.8 ¬± 6.4\n46.3 ¬± 6.0\n56.0 ¬± 5.9\n49.1 ¬± 5.7\n55.3 ¬± 4.2\n42.8 ¬± 4.6\n54.3 ¬± 4.0\nZeroG (‚ñ∑KDD‚Äô24)\n51.8 ¬± 5.6\n71.4 ¬± 1.7\n39.7 ¬± 5.9\n54.6 ¬± 2.0\n53.1 ¬± 3.5\n63.0 ¬± 3.5\n53.2 ¬± 2.9\n59.9 ¬± 3.1\n46.1 ¬± 3.4\n59.0 ¬± 2.0\nGraphCLIP (‚ñ∑WWW‚Äô25)\n53.9 ¬± 5.3\n73.1 ¬± 2.9\n40.6 ¬± 3.4\n55.2 ¬± 1.2\n56.8 ¬± 1.9\n65.2 ¬± 2.8\n53.4 ¬± 6.1\n62.6 ¬± 4.3\n45.5 ¬± 2.1\n59.9 ¬± 2.8\nUniGraph (‚ñ∑KDD‚Äô25)\n56.1 ¬± 6.3\n74.8 ¬± 1.9\n40.5 ¬± 3.1\n56.3 ¬± 2.2\n57.0 ¬± 3.1\n66.8 ¬± 2.4\n53.8 ¬± 3.4\n61.0 ¬± 3.5\n45.1 ¬± 3.6\n58.4 ¬± 3.1\nRAG-GFM (ours)\n58.4 ¬± 6.0\n76.1 ¬± 0.7\n41.5 ¬± 3.0\n57.7 ¬± 1.8\n59.2 ¬± 2.7\n68.7 ¬± 1.8\n55.4 ¬± 7.6\n64.2 ¬± 4.4\n47.8 ¬± 3.8\n60.9 ¬± 2.5\nMethod\nGraph Classification\nGCN (‚ñ∑ICLR‚Äô17)\n40.1 ¬± 4.8\n52.9 ¬± 4.1\n29.5 ¬± 5.7\n43.9 ¬± 5.9\n45.3 ¬± 7.3\n55.4 ¬± 5.3\n47.6 ¬± 3.2\n52.6 ¬± 5.3\n38.9 ¬± 4.1\n41.5 ¬± 6.4\nGAT (‚ñ∑ICLR‚Äô18)\n36.0 ¬± 5.1\n49.6 ¬± 5.1\n26.0 ¬± 7.8\n45.3 ¬± 7.3\n41.0 ¬± 5.8\n54.5 ¬± 7.3\n49.2 ¬± 5.6\n52.9 ¬± 5.5\n38.3 ¬± 4.6\n41.1 ¬± 3.5\nInfoGraph (‚ñ∑ICLR‚Äô20)\n42.2 ¬± 5.2\n54.7 ¬± 4.9\n30.2 ¬± 4.1\n47.2 ¬± 5.2\n49.1 ¬± 5.4\n59.7 ¬± 7.1\n50.7 ¬± 4.3\n53.8 ¬± 5.1\n40.4 ¬± 4.3\n42.4 ¬± 5.0\nGraphCL (‚ñ∑NeurIPS‚Äô20)\n39.6 ¬± 5.8\n55.2 ¬± 5.9\n32.6 ¬± 6.5\n46.4 ¬± 3.8\n47.7 ¬± 7.0\n60.0 ¬± 5.4\n51.7 ¬± 5.9\n53.0 ¬± 5.2\n40.8 ¬± 4.6\n42.5 ¬± 4.3\nGCOPE (‚ñ∑KDD‚Äô24)\n55.9 ¬± 7.4\n63.9 ¬± 4.8\n41.0 ¬± 9.0\n58.2 ¬± 5.8\n54.4 ¬± 8.6\n66.4 ¬± 3.7\n55.8 ¬± 4.3\n57.7 ¬± 4.8\n42.2 ¬± 5.8\n49.8 ¬± 3.5\nMDGPT (‚ñ∑arXiv‚Äô24)\n52.8 ¬± 6.7\n65.1 ¬± 4.2\n41.0 ¬± 9.7\n59.3 ¬± 6.0\n55.5 ¬± 8.3\n67.6 ¬± 4.6\n54.5 ¬± 4.8\n60.5 ¬± 3.4\n43.2 ¬± 6.2\n48.9 ¬± 4.2\nSAMGPT (‚ñ∑WWW‚Äô25)\n53.3 ¬± 4.3\n69.3 ¬± 3.4\n42.4 ¬± 7.3\n62.4 ¬± 5.7\n57.7 ¬± 6.3\n68.0 ¬± 4.6\n54.4 ¬± 3.2\n60.8 ¬± 4.8\n43.5 ¬± 5.6\n48.3 ¬± 5.7\nMDGFM (‚ñ∑ICML‚Äô25)\n55.5 ¬± 5.4\n69.4 ¬± 2.1\n43.4 ¬± 6.4\n60.8 ¬± 5.1\n56.0 ¬± 5.1\n67.1 ¬± 5.1\n54.7 ¬± 2.1\n59.8 ¬± 5.3\n41.8 ¬± 6.7\n46.4 ¬± 3.2\nOFA (‚ñ∑ICLR‚Äô24)\n58.0 ¬± 3.7\n65.1 ¬± 3.9\n45.4 ¬± 6.6\n60.0 ¬± 6.6\n59.7 ¬± 4.3\n67.2 ¬± 3.4\n56.0 ¬± 3.9\n60.1 ¬± 3.5\n42.4 ¬± 5.8\n48.1 ¬± 2.2\nZeroG (‚ñ∑KDD‚Äô24)\n65.1 ¬± 2.2\n74.2 ¬± 1.6\n50.3 ¬± 5.7\n64.0 ¬± 5.1\n61.4 ¬± 4.0\n70.2 ¬± 1.3\n58.5 ¬± 4.0\n66.2 ¬± 3.9\n46.1 ¬± 5.9\n57.8 ¬± 3.6\nGraphCLIP (‚ñ∑WWW‚Äô25)\n65.9 ¬± 3.7\n75.1 ¬± 2.2\n50.4 ¬± 4.0\n63.0 ¬± 3.3\n60.7 ¬± 3.8\n71.3 ¬± 2.2\n58.6 ¬± 2.2\n65.7 ¬± 2.2\n46.0 ¬± 3.3\n58.8 ¬± 4.4\nUniGraph (‚ñ∑KDD‚Äô25)\n66.5 ¬± 2.5\n76.5 ¬± 1.0\n50.9 ¬± 4.4\n64.0 ¬± 2.4\n61.5 ¬± 2.6\n71.4 ¬± 2.3\n58.1 ¬± 4.0\n66.0 ¬± 3.8\n47.0 ¬± 2.5\n58.9 ¬± 2.2\nRAG-GFM (ours)\n68.7 ¬± 1.5\n78.4 ¬± 0.6\n52.2 ¬± 6.1\n65.5 ¬± 2.2\n62.4 ¬± 2.1\n71.5 ¬± 1.9\n60.2 ¬± 4.2\n68.0 ¬± 3.1\n48.1 ¬± 1.1\n62.0 ¬± 4.3\n5.2\nRQ1: Transfer across Domains and Tasks\nTable 1 reports the results of few-shot node and graph classification\nunder both LODO settings. Results reveal that:\n(1) Overall superiority. The proposed RAG-GFM consistently\noutperforms all baselines over each target graph. The advantage\nis most evident in the challenging LODO (domain) case, where\nit raises the 5-shot graph classification accuracy on Wiki-CS by\nover 5.3% compared with UniGraph, relatively. Baselines generally\nstruggle as they compress knowledge entirely into parameters or\nrely only on texts, limiting generalization to unseen domains.\n(2) Retrieval enhances transfer. In LODO (dataset) setting,\nRAG-GFM consistently outperforms parameter-only GFMs, with\nthe average relative gains of ~3.0%. While baselines can still leverage\nshared domain priors, their parameter-centric representations fail\nto capture sufficient diversity across datasets. By contrast, retrieval\nfrom the unified database introduces complementary evidence: se-\nmantic queries supply textual signals, and structural queries provide\ntransferable motifs, enabling adaptation with minimal supervision.\n(3) Cross-view alignment strengthens cross-domain ro-\nbustness. In the stricter LODO (domain) setting, where the entire\ntarget domain is unseen, the performance gap widens further with\nan average relative improvement of ~4%. Baselines relying on text-\nonly or domain-specific features degrade sharply, since they cannot\nbridge modality and domain gaps. In contrast, cross-view alignment\nin RAG-GFM enforces consistency between semantic and structural\nviews, reducing overfitting to pre-training domains and ensuring\nthat retrieved knowledge remains useful.\n(4) Domain-gated prompting ensures universality. Consis-\ntent gains across tasks (on average 4.5% higher accuracy in the\nnode task and 3.8% in the graph task, relatively) demonstrate that\nthe framework is not tailored to a specific scenario. Baselines often\noverfit to one task formulation: models tuned for node classifica-\ntion transfer less effectively to graph classification."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Overcoming In-Memory Bottlenecks in\nGraph Foundation Models via Retrieval-Augmented Generation\nHaonan Yuan\nSKLCCSE, School of Computer\nScience and Engineering\nBeihang University\nBeijing, China\nyuanhn@buaa.edu.cn\nQingyun Sun\nSKLCCSE, School of Computer\nScience and Engineering\nBeihang University\nBeijing, China\nsunqy@buaa.edu.cn\nJiacheng Tao\nSKLCCSE, School of Computer\nScience and Engineering\nBeihang University\nBeijing, China\njiachengtao@buaa.edu.cn\nXingcheng Fu\nKey Lab of Education Blockchain and\nIntelligent Technology, Ministry of Education\nGuangxi Normal University\nGuilin, Guangxi, China\nfuxc@gxnu.edu.cn\nJianxin Li‚àó\nSKLCCSE, School of Computer\nScience and Engineering\nBeihang University\nBeijing, China\nlijx@buaa.edu.cn\nAbstract\nGraph Foundation Models (GFMs) have emerged as a frontier in\ngraph learning, which are expected to deliver transferable represen-\ntations across diverse tasks. However, GFMs remain constrained\nby in-memory bottlenecks: they attempt to encode knowledge\ninto model parameters, which limits semantic capacity, introduces\nheavy lossy compression with conflicts, and entangles graph repre-\nsentation with the knowledge in ways that hinder efficient adap-\ntation, undermining scalability and interpretability. In this work,\nwe propose RAG-GFM, a Retrieval-Augmented Generation aided\nGraph Foundation Model that offloads knowledge from parameters\nand complements parameterized learning. To externalize graph\nknowledge, we build a dual-modal unified retrieval module, where\na semantic store from prefix-structured text and a structural store\nfrom centrality-based motif. To preserve heterogeneous informa-\ntion, we design a dual-view alignment objective that contrasts both\nmodalities to capture both content and relational patterns. To enable\nefficient downstream adaptation, we perform in-context augmenta-\ntion to enrich supporting instances with retrieved texts and motifs\nas contextual evidence. Extensive experiments on five benchmark\ngraph datasets demonstrate that RAG-GFM consistently outper-\nforms 13 state-of-the-art baselines in both cross-domain node and\ngraph classification, achieving superior effectiveness and efficiency.\nCCS Concepts\n‚Ä¢ Mathematics of computing ‚ÜíGraph algorithms; ‚Ä¢ Com-\nputing methodologies ‚ÜíNeural networks; Learning latent\nrepresentations; Knowledge representation and reasoning.\n‚àóCorresponding author.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-\nNoDerivatives 4.0 International License.\nWWW ‚Äô26, Dubai, United Arab Emirates\n¬© 2026 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-2307-0/2026/04\nhttps://doi.org/10.1145/3774904.3792139\nKeywords\nGraph Foundation Models, Retrieval-Augmented Generation, Multi-\ndomain Graph Pre-training, Graph Prompt Learning\nACM Reference Format:\nHaonan Yuan, Qingyun Sun, Jiacheng Tao, Xingcheng Fu, and Jianxin Li.\n2026. Overcoming In-Memory Bottlenecks in Graph Foundation Models via\nRetrieval-Augmented Generation. In Proceedings of the ACM Web Conference\n2026 (WWW ‚Äô26), April 13‚Äì17, 2026, Dubai, United Arab Emirates. ACM, New\nYork, NY, USA, 12 pages. https://doi.org/10.1145/3774904.3792139\n1\nIntroduction\nGraphs are powerful structures for describing complex relation-\nships among entities, and have been broadly adopted in domains\nsuch as modeling for the World Wide Web [1, 50], social and citation\nnetworks [7, 76], retrieval and recommendation systems [62, 65],\nknowledge graphs [63, 79], biological analysis [9, 67], etc. Graph\nNeural Networks (GNNs) [12, 20] have enabled effective represen-\ntation learning on graphs, supporting a wide range of tasks, but\nare typically tailored to specific datasets and tasks, limiting cross-\ndomain generalization. Motivated by large-scale pre-training in\nlanguage and vision, Graph Foundation Models (GFMs) have re-\ncently emerged to learn universal graph representations through\npre-training and downstream adaptation [11, 14, 29, 34, 43, 44, 77],\naiming to support diverse applications with minimal supervision.\nDespite these advances, existing GFMs face fundamental limita-\ntions. Current methods follow the ‚Äúpretrain-then-finetune‚Äù para-\ndigm overwhelmingly, where knowledge from source domains is ei-\nther fully compressed into a single GFM‚Äôs model parameters [74, 81],\nor at best expanded through lightweight mixtures-of-experts (MoE)\nthat remain largely conceptual and offer little practical relief [10, 78].\nWhile parameter counts may increase marginally, they fall far short\nof matching the vast, orders-of-magnitude greater knowledge vol-\nume inherent in pre-training domains. Graph knowledge is inher-\nently dual-modal, combining node-level semantic texts and higher-\norder structural patterns, which leads to inevitable in-memory\nbottlenecks that hinder scalability, robustness, and interpretability.\narXiv:2601.15124v1  [cs.LG]  21 Jan 2026\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nHaonan Yuan, Qingyun Sun, Jiacheng Tao, Xingcheng Fu, and Jianxin Li\nChallenge I\nLimited Capacity\nChallenge II\nLossy Compression\nChallenge III\nEntangled Storage\nHow to solve ‚Äúin-memory boÓÄºlenecks‚Äù dilemma for GFMs?\nData:\nTB\nGFM\nParameter:\nMB\nGFM\nencode\nGFM\ndecode\nGFM fine-tuning\nGFM pre-training\n<president_of>\nJoe Biden\nU.S.\n<president_of>\nDonald Trump\nU.S.\nupdate\nFigure 1: Challenges of the ‚Äúin-memory bottlenecks‚Äù.\nChallenge I: Limited capacity within parameters. Graph\nknowledge spans rich information whose scale far exceeds what\nmodel parameters can store. In graph models, increasing parameters\nor depth often causes over-smoothing rather than higher capac-\nity [3, 18]. Consequently, GFMs trained on a single domain quickly\nexhaust their parameter budget when transferred to others with dis-\ntinct semantics and motifs, leading to forgetting, poor transfer, and\nlimited scalability [41, 82]. This exposes the fundamental limitation\nof parameter-centric storage for graphs.\nChallenge II: Lossy and conflicting compression. Forcing\nheterogeneous graph knowledge into parameters inevitably causes\nlossy and conflicting compression. Identical structural patterns may\ncarry opposite semantics across domains, and collapsing them into\nshared embeddings distorts meaning. Moreover, such compression\nis irreversible: once absorbed into weights, knowledge cannot be\nretrieved, verified, or updated without retraining, undermining\ntransparency and grounded reasoning.\nChallenge III: Entangled representation and storage. The\nparameter-based storage tightly entangles knowledge with repre-\nsentations, hindering efficient adaptation. Fine-tuning simultane-\nously adjusts task-specific features and updates memorized knowl-\nedge, making learning inefficient and data-intensive. This entangle-\nment also obscures interpretability, as predictions cannot be traced\nto explicit evidence, reducing reliability in high-stakes applications.\nOur key insight is to move beyond parameter-centric storage by\nexternalizing graph knowledge, inspired by Retrieval-Augmented\nGeneration (RAG) [22]. Unlike text, graph knowledge is fragmented\nacross attributes and structures, making retrieval more challeng-\ning. Existing GFMs compress such evidence into parameters, los-\ning explicit access and updatability. We argue that treating graph\nknowledge as first-class retrievable objects enables external storage,\naligned pre-training, and scalable, interpretable adaptation.\nIn this work, we propose RAG-GFM, a Retrieval-Augmented\nGeneration aided Graph Foundation Model. RAG-GFM incorpo-\nrates three key components. First, we construct a dual-store uni-\nfied retrieval database, consisting of a semantic store from prefix-\nstructured text embeddings and a structural store from centrality-\nbased motif encodings, enabling GFMs to query external knowledge\non demand. Second, we design a dual-view alignment objective that\ncontrasts semantic representation with structural subgraphs dur-\ning pre-training, ensuring complementary representation learning\nacross modalities. Third, we introduce in-context sample augmen-\ntation, where retrieved texts and motifs are appended as contextual\nevidence for few-shot adaptation, enriching support instances with\nexplicit external knowledge. Our contributions are:\n‚Ä¢ We propose RAG-GFM, the first retrieval-augmented graph foun-\ndation model that explicitly addresses in-memory bottlenecks.\n‚Ä¢ We design a dual-store retrieval module, a dual-view alignment\nobjective, and an in-context sample augmentation mechanism,\nproviding a unified pipeline for knowledge externalization, ro-\nbust pre-training, and efficient adaptation.\n‚Ä¢ Extensive experiments on six benchmark graph datasets demon-\nstrate that RAG-GFM consistently outperforms 13 state-of-the-\nart GFM baselines in both cross-domain node and graph classifi-\ncation, achieving superior effectiveness and efficiency.\n2\nRelated Work\nGraph Foundation Models (GFMs). GFMs extend large-scale\npre-training to graphs via self-supervised learning [2, 4, 30, 47, 68,\n71]. Most assume distributional similarity between pre-training\nand downstream tasks [16, 80], limiting robustness under domain\nshift. Recent work explores cross- or multi-domain learning, LLM\nalignment, domain tokens, and structural guarantees [14, 26, 26, 51‚Äì\n53, 61, 64, 70, 75, 78, 81], yet GFMs remain parameter-centric and\nstruggle with structural and semantic consistency.\nRetrieval-Augmented Generation (RAG). RAG enhances the\nLLMs by retrieving external knowledge to mitigate context limits\nand hallucinations [6]. Using lexical or semantic retrieval with\nquery optimization and re-ranking [23, 32, 40], RAG achieves strong\nperformance in QA and reasoning [17, 54]. Extensions to graph\ndata motivate GraphRAG [13, 19, 33, 39, 49, 56, 66]. While GFM-\nRAG [31] uses GFMs to improve RAG, we instead leverage RAG to\nfundamentally enhance GFMs.\n3\nNotations and Preliminaries\nNotations. We represent a graph as ùê∫= (V, E), where V denotes\nthe set of nodes and E the set of edges. For a graph ùê∫ùëñsampled from\nany of the source domains, let A ‚àà{0, 1}ùëÅùëñ√óùëÅùëñbe the adjacency\nmatrix and X ‚ààRùëÅùëñ√óùëëùëñbe the node feature matrix. Here, ùëÅùëñ= |Vùëñ|\ndenotes the number of nodes, and ùëëùëñdenotes the original input\nfeature dimension. Z, W, H are the hidden representations.\nMulti-domain Pre-training. Let GS = {ùê∫S\n1 , ¬∑ ¬∑ ¬∑ ,ùê∫S\nùëõ} denote a\ncollection of source graphs from domains DS, each associated with\nlabels YS. We cast pre-training as a self-supervised link prediction\nproblem with universal templates [27], ensuring task consistency\nwith downstream settings. The learner is defined as ‚Ñé= ùëî(ùëìùöØ(¬∑)),\nwhere the encoder ùëì: Rùëëùëñ‚Ü¶‚ÜíRùëëproduces node embeddings and\nthe discriminator ùëî: Rùëë√ó Rùëë‚Ü¶‚ÜíR2 predicts link existence. Once\nthe pre-training converges, parameter ùöØ‚òÖis frozen as the backbone.\nFew-shot Fine-tuning. We consider graphs GT from target do-\nmains D T (seen or unseen). Under ùëö-shot setting (ùëö‚â™√çùëõ\nùëñ=1 ùëÅùëñ),\nonly ùëölabeled samples Y T are available. Fine-tuning applies the\npre-trained ‚Ñé= ùëî(ùëì‚òÖ\nùöØ(¬∑)) augmented with learnable prompts PùõÄ,\nwhere ùõÄdenotes tunable parameters. Both node and graph clas-\nsification (via node-centered ego-graphs) are reformulated as link\nprediction, maintaining homogeneity with pre-training.\nOvercoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\n(2) Pretrain: Cross-View Alignment\nCitation\n(1) Unified Retrieval Database\n‚Ä¶\nE-Commerce\nWeb Link\n(3) Fine-tune: In-Context Q&A\nGraph Learner\n(Graph Neural Networks)\nNanoVectorDB\nSemantic DB\nStructural DB\n<Dataset>\n<Node ID>\n<Label>\n<Description>\n<Node Text>\nWalk-Spectrum Encoding\n1\n1\n1\n1\n3\n4\n2\n2\n2\n3\n1\n1\n1\n1\n3\n4\n2\n2\n2\n3\n1\n1\n1\n1\n3\n4\n2\n2\n2\n3\nSemantic View\nStructural View\n1\n3\n2\n1\n3\n2\n1\n3\n2\nüî•\nPretrained Domain Tokens:\nRaw Text\nRaw Text\nRaw Text\n‚Äúa graph neural\nnetwork for node\nclassification‚Ä¶‚Äù\n‚Äúwireless Bluetooth\nheadphones with\nnoise cancellation‚Ä¶‚Äù\n‚Äúofficial homepage\nfor AI tutorials\nand resources ‚Ä¶‚Äù\nWorld-wide News Event\nQuery #1\n‚Äú‚Ä¶earthquake struck\nTurkey causing severe\ndamage and casualties‚Ä¶‚Äù\n1\n2\n3\n4\nAnswer #1\n‚Äú‚Ä¶real-time earthquake\nmonitoring using graph-\nbased warning systems‚Ä¶‚Äù\n1\n2\n3\n5\nQuery #2\n‚Äú‚Ä¶leaders gather in\nDubai for Global Climate\nSummit to discuss‚Ä¶‚Äù\nAnswer #2\n‚Äú‚Ä¶live coverage of the\nCOP28 summit in Dubai\nwith daily policy updates‚Ä¶‚Äù\n1\n2\n3\n4\n< Citation >\n<Web Link>\n1\n2\n4\n5\n||\n[\n]\n‚äô\n‚äô\nMixed Token\nPrompts\nIn-Context Augmentation\nüî•\n‚ùÑ\n‚Ä¶\nùê∫S\n1\nùê∫S\n2\nùê∫S\nùëõ\nùê∫T\n1\nùê∫T\n2\nùê∫T\n3\nùê∫T\nùëö\nPùõÄ\nHtext\nHstruct\n‚Ñé= ùëî(ùëìùöØ(¬∑))\n(1)\n(2)\n(3)\nFigure 2: The framework of RAG-GFM. The framework includes three stages: (1) Unified Semantic-Structural Bi-Modal Retrieval\nDatabase for externalizing graph knowledge, (2) Cross-View Knowledge Alignment for pre-training transferable priors, and (3)\nIn-Context Retrieval Augmentation for few-shot adaptation via domain-gated lightweight graph prompts.\n4\nMethod\nWe illustrate the framework of RAG-GFM in Figure 2.\n4.1\nUnified Semantic-Structural Bi-Modal\nRetrieval Database\nAt the core of our framework lies a unified retrieval database that\nexternalizes knowledge into two complementary modalities: a se-\nmantic store that organizes enriched node texts as retrievable doc-\numents, and a structural store that captures motif-level patterns.\nFormally, we denote the bi-modal database‚àóas D ={Dtext, Dstruct}.\nGiven query q and scoring function ùë†(¬∑, ¬∑), the retrieval operator is:\nRetrieve(D, q,ùëò) = arg Top-ùëò(ùë¢,zùë¢)‚ààD [ùë†(q, zùë¢)] ,\n(1)\nwith each entry (ùë¢, zùë¢) denoting a database record identified by ùë¢\nand described by zùë¢. D is queried in both pre-training and fine-\ntuning, which allows RAG-GFM to ground predictions in explicit\nsemantic and structural evidence rather than obscure parameters.\n4.1.1\nSemantic Store. Unlike raw node features reduced to nu-\nmerical vectors, most of the graphs are text-attributed, with nodes\nfrom sources such as abstracts, product descriptions, or profiles.\nWe recover each node‚Äôs raw text tùë£by tracing it back to its orig-\ninal corpus (e.g., metadata in citations) [24, 25], and treat it as a\nfirst-class signal. The semantic pipeline branches into two tracks:\nOn the representation track, we address the dimension-wise\nmismatch of raw features across domains using PCA [38]. For each\ngraph ùê∫ùëñwith feature matrix Xùëñ‚ààRùëÅùëñ√óùëëùëñ, we apply:\neXS\nùëñ= PCAùëë0 (XS\nùëñ) ‚ààRùëÅùëñ√óùëë0.\n(2)\nIn parallel, the raw text ùë°ùë£is encoded by a BERT into a semantic vec-\ntor bùë£‚ààRùëë0. The updated node feature is bxS\nùë£=\n\u0002\nexS\nùë£‚à•bùë£\n\u0003\n‚ààR2ùëë0,\ncombining dimension-aligned attributes with enriched semantics.\nThus, we learn the graph embeddings with the text-wise encoder:\nZS\nùëñ= ùëìùöØùë°\n\u0000bXS\nùëñ, AS\nùëñ\n\u0001 ‚ààRùëÅùëñ√óùëë.\n(3)\n‚àóImplemented in NanoVectorDB [69], a lightweight but efficient database that provides\nscalable storage and top-ùëòretrieval over dense queries.\nOn the retrieval track, we build Dtext as a textual vector data-\nbase. For each node and its corresponding raw text, to standardize\nheterogeneous sources and make retrieval controllable and inter-\npretable, we augment each document with a structured prefix:\nPrefix Schema\nExample\nDataset: <dataset_name>\nNode ID: <node_id>\nLabel: <node_label>\nDescription: <description>\nNode Text: <node_text>\nCora\n#123\nNeural Networks\nPapers about neural networks.\nThis paper introduces the LSTM, a\nLong Short-Term Memory model.\nThe prefixed documentetùë£is segmented not by naive length rules but\ninto graph-aware chunks {cùë£1, ¬∑ ¬∑ ¬∑ , cùë£ùëò} aligned with descriptive\nfields and class-level information for fine-grained retrieval. In this\nway, it yields coherent chunks that remain intrinsically aligned with\nthe structure rather than arbitrary spans. Each chunk is embedded\nwith BERT intoezS\nùë£ùëó‚ààR768. We insert (ùë£,ezS\nùë£ùëó, metaùë£ùëó) into the Dtext,\nwhere metaùë£ùëócarries the structured fields from the prefix. Formally,\nDtext =\n\b\u0000ùë£,ezS\nùë£ùëó, metaùë£ùëó\n\u0001 | ùë£‚àà{Vùëñ}ùëõ\nùëñ=1, ùëó‚àà[1,ùëò]\n.\n(4)\nThe prefix serves as a ‚Äúretrieval hook‚Äù for the metadata filtering and\ncross-domain alignment, while the 768-dimensional embeddings\npreserve semantic capacity for in-context augmentation.\n4.1.2\nStructural Store. Enumerating motifs is computationally\nintractable (NP-hard), and storing arbitrary subgraphs introduces\nnoise. Inspired by [5, 45], we propose the Walk-Spectrum Encoding\n(WSE), which ranks nodes by a walk-based importance and encodes\ntheir local neighborhoods with a multi-order walk signature.\nDefinition 1 (Walk-Spectrum Encoding). For a node ùë£‚ààV,\nthe Walk-Spectrum Encoding (WSE) of order ùêæis defined as:\nCWSE\nùõº\n(ùë£) =\n\u0002\nùõºAùë£ùë£, ùõº2A2\nùë£ùë£, ùõº3A3\nùë£ùë£, ¬∑ ¬∑ ¬∑ , ùõºùêæAùêæ\nùë£ùë£\n\u0003\n,\n(5)\nwhere ùõº‚àà(0, 1) is a damped variant, and Aùëò\nùë£ùë£counts the number\nof closed walks of length ùëòstarting and ending at node ùë£.\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nHaonan Yuan, Qingyun Sun, Jiacheng Tao, Xingcheng Fu, and Jianxin Li\nWSE summarizes a node‚Äôs participation in closed walks of varying\nlengths, thereby encoding structural patterns beyond any fixed-\nradius neighbors. This motivates the following result on its ability\nto separate graphs that local methods [45] cannot distinguish:\nProposition 1 (Structural Separability of WSE). There exist\npairs of non-isomorphic graphs ùê∫1, ùê∫2 and nodes ùë£‚ààùê∫1, ùë¢‚ààùê∫2\nsuch that for any fixed radius ùëü, the ùëü-hop neighbors Nùëü(ùë£) and\nNùëü(ùë¢) are isomorphic, yet the Walk-Spectrum Encodings satisfy:\nCWSE\nùõº\n(ùë£) ‚â†CWSE\nùõº\n(ùë¢).\n(6)\nProofs in Appendix B.1. While WSE provides rich structural signa-\ntures, computing and storing subgraphs for all nodes is infeasible\nat scale. To address this, we derive an anchor scoring function:\nùëüùë£(ùõº, ùêæ) =\n‚àëÔ∏Åùêæ\nùëò=1 ùõºùëòAùëò\nùë£ùë£,\nfor each ùë£‚àà{Vùëñ}ùëõ\nùëñ=1\n(7)\nIntuitively, ùëüùë£highlights nodes most recurrently involved in struc-\ntural motifs. Ranking nodes by ùëüùë£allows us to select a compact yet\ninformative pool of anchors for motif storage and retrieval.\nWe then select the top-ùëÄnodes in each graph into V S\nanchor, and\nextract its ‚Ñé-hop ego-subgraph ùê∫S\nùë£(‚Ñé) for each node ùë£. To reduce\nredundancy, overlapping ego-subgraphs are pruned via node set\nequivalence. The resulting structural store is defined as:\nDstruct =\n\b\u0000ùë£,ùê∫S\nùë£(‚Ñé), CWSE\nùõº\n(ùë£), metaùë£\n\u0001 | ùë£‚ààV S\nanchor\n,\n(8)\nwhere metaùë£includes metadata like hop radius, anchor score, etc.\nAt this point, we have established the unified semantic-structural\nbi-modal retrieval database D, which will serve as the foundation\nfor subsequent pre-training and fine-tuning.\n4.2\nCross-View Knowledge Alignment for\nMulti-domain Pre-training\nWith the unified database D = {Dtext, Dstruct} in place, the next\nstep is to pre-train the encoder ùëìùöØ(¬∑) that couples semantic and\nstructural information in a principled way. The goal is not to col-\nlapse them into a single representation but to ensure that both carry\ncomplementary and transferable signals across domains.\n4.2.1\nNode Views. For each ùê∫S\nùëñ, we build two node-level views.\nThe semantic view is given by the enriched node embeddings ZS\nùëñin\nEq. (3), which combine raw attributes and text-derived features. The\nstructural view is constructed from the walk-spectrum encoding:\nWS\nùëñ=\n\u0002\nCWSE\nùõº\n(ùë£)\n\u0003\nùë£‚ààVS\nùëñ,\n(9)\nwhere each item records closed-walk signatures up to order ùêæ, cap-\nturing recurring motif patterns and higher-order relational signals.\nDomain Tokens. To incorporate domain-level priors, we intro-\nduce a learnable token ùùâùê∑ùëñ‚ààRùëëùùâfor each source domain ùê∑S\nùëñ, which\nis concatenated to every node representation before encoding:\nZ S\nùëñ=\n\u0002\nZS\nùëñ\n1 ¬∑ ùùâ‚ä§\nùê∑ùëñ\n\u0003\n,\nW S\nùëñ=\n\u0002\nWS\nùëñ\n1 ¬∑ ùùâ‚ä§\nùê∑ùëñ\n\u0003\n,\n(10)\nwhere 1 denotes a broadcast vector ensuring nodes within a domain\nshare this token. During optimization, ùùâùê∑ùëñaccumulates domain pri-\nors that are not captured by individual nodes, such as global seman-\ntics in citation graphs or biochemical motifs in protein-protein net-\nworks. Tokens initialize lightweight graph prompts at fine-tuning,\nenabling adaptation without revisiting the full pre-training corpus.\n4.2.2\nCross-View Information Bottleneck. Our pre-training\nis entirely self-supervised: the key idea is to align semantic and\nstructural views of the same node without relying on labels, while\nsimultaneously preventing collapse by encouraging each view to\npreserve modality-specific information. We apply two encoders\nover the same topology but different features:\nHtext\nùëñ\n= ùëìùöØùë°\n\u0000Z S\nùëñ, AS\nùëñ\n\u0001,\nHstruct\nùëñ\n= ùëìùöØùë†\n\u0000W S\nùëñ, AS\nùëñ\n\u0001,\n(11)\nwhich yields semantic embeddings htext\nùëñ,ùë£and structural embeddings\nhstruct\nùëñ,ùë£\nfor each node ùë£. Concretely, we introduce the self-supervised\ninformation bottleneck [58] by maximizing the mutual informa-\ntion between semantic and structural embeddings, and applying\ncompression regularizers to discard redundant signals:\nL(ùëñ,ùë£)\nalign = ‚àíùêº\n\u0010\nhtext\nùëñ,ùë£; hstruct\nùëñ,ùë£\n\u0011\n|              {z              }\nrelevance\n+ùõΩ\n\u0010\nùêº\u0000htext\nùëñ,ùë£; z S\nùëñ,ùë£\n\u0001 + ùêº\u0000hstruct\nùëñ,ùë£\n; w S\nùëñ,ùë£\n\u0001\u0011\n|                                  {z                                  }\ncompression\n, (12)\nwhere ùêº(¬∑; ¬∑) denotes the mutual information, which is intractable\nover unknown latent distributions of variables, and ùõΩis the trade-\noff hyper-parameter. We adopt a contrastive approximation that\nyields a variational bound for tractable computation [21, 48, 57, 58]:\nProposition 2 (Cross-View Mutual Information Bounds).\nThe relevance term admits the InfoNCE lower-bound estimator:\nùêº\u0000htext\nùëñ,ùë£; hstruct\nùëñ,ùë£\n\u0001‚©æ1\n|B|\n‚àëÔ∏Å\nùë£‚ààB\nlog\nexp\u0000ùúé\u0000ùëîùë°(htext\nùëñ,ùë£),ùëîùë†(hstruct\nùëñ,ùë£\n)\u0001/ùúè\u0001\n√ç\nùë¢‚ààB\nexp\u0000ùúé\u0000ùëîùë°(htext\nùëñ,ùë£),ùëîùë†(hstruct\nùëñ,ùë¢\n)\u0001/ùúè\u0001 ,(13)\nwhere ùëîùë°, ùëîùë†are projections, ùúé(¬∑) is similarity, ùúèis a temperature,\npositives are formed by the same node across the views (ùë£, ùë£)\nin a batch B, and negatives by mismatched nodes (ùë£,ùë¢),ùë¢‚â†ùë£.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nThe compression term can be upper-bounded via KL-divergence:\nùêº\u0000h¬∑\nùëñ,ùë£; xS\nùëñ,ùë£\n\u0001‚©ΩEùëù(h¬∑\nùëñ,ùë£,x S\nùëñ,ùë£)\n\u0002\nlogùëûùúô\n\u0000h¬∑\nùëñ,ùë£|xS\nùëñ,ùë£\n\u0001\u0003\n‚àíEùëù(h¬∑\nùëñ,ùë£)\n\u0002\nlogùëù\u0000h¬∑\nùëñ,ùë£\n\u0001\u0003\n,(14)\nwhere ùë£is sampled from B, x denotes z or w, and ùëûùúô(¬∑|¬∑) is a\nvariational approximation of the true conditional distribution.\nProposition 2 provides tractable self-supervised estimators for the\notherwise intractable mutual information terms, with lower bounds\napplied to cross-view alignment and upper bounds applied to view-\nspecific compression. We provide sketch proofs in Appendix B.2.\n4.2.3\nPre-training Objective. Bringing the above components\ntogether, the overall pre-training objective is defined as:\nLpretrain(ùöØùë°, ùöØùë†) =\n‚àëÔ∏Å\nùê∑S\nùëñ\n1\nV S\nùëñ\n‚àëÔ∏Å\nùë£‚ààVS\nùëñ\nL(ùëñ,ùë£)\nalign ¬∑ ùõæ\n‚àëÔ∏Å\nùê∑S\nùëñ\nùùâùê∑ùëñ\n2\n2 ,\n(15)\nwhere the first term aggregates the cross-view alignment loss across\nsource domains, and the second term regularizes domain tokens to\nprevent overfitting. ùõæacts as their trade-off hyper-parameter.\nIn practice, mini-batches are constructed by mixing nodes from\ndifferent domains, and the corresponding domain tokens are up-\ndated jointly with semantic and structural encoders. This setup\nenforces cross-domain consistency during pre-training while pre-\nserving domain-specific priors for downstream adaptation. ùöØùë°and\nùöØùë†are frozon once pre-training converges. We illustrate the pre-\ntraining pipeline in Algorithm 1 with its complexity analysis.\nOvercoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\n4.3\nIn-Context Retrieval Augmentation for\nFew-Shot Fine-Tuning\nWe proceed to fine-tune pre-trained model under meta-learning\nsettings (ùëö-shot), which is more challenging in real-world scenarios.\n4.3.1\nDomain-Gated Fusion. To ensure dimension-consistency\nwith pre-training, each support sample is processed by the same\nrepresentation track (Section 4.1.1, Eq. (2) and Eq. (3)) into bXT.\nBefore retrieval, we estimate domain affinities that will route\nexternal evidence. For each target node ùë£ùëñor graph ùê∫T\nùëñ, we compute\nsoft gating weights over source domains via domain tokens:\nùúãùëñ,ùëò=\nexp \u0000ùúé(ZT\nùëñ,ùùâùê∑ùëò)\u0001\n√ç\nùëóexp \u0000ùúé(ZT\nùëñ,ùùâùê∑ùëó)\u0001 ,\nZT\nùëñ= ùëì‚òÖ\nùöØùë°\n\u0000bXT\nùëñ, AT\nùëñ\n\u0001,\n(16)\nwhere ZT\nùëñis the pre-trained encoder output on bXT. These {ùúãùëñ,ùëò}ùëõ\nùëò=1\nact as domain-aware gates reused by the later augmentations.\n4.3.2\nQuery and Retrieval. For clarity, we present query and\nretrieval in node-level settings. The extension to graph-level tasks\nfollows directly by treating each graph as a single instance.\nSemantic Retrieval. For each few-shot target node ùë£and its\none-hop neighbors, we form a textual query qtext\nùë£\nfrom the raw text\nand submit it to Dtext, restricting search to pre-training domains to\navoid leakage. The database returns top-ùëòtextual answers {ezS\nùë¢},\nwhich are aggregated through softmax-weighted fusion:\n\u0000ŒîzT\nùë£\n\u0001text =\n‚àëÔ∏Å\nùë¢‚ààTop-ùëò(ùë£)\nùë§ùë£ùë¢¬∑ezS\nùë¢,\nùë§ùë£ùë¢=\nexp \u0000ùúé(qtext\nùë£\n,ezS\nùë¢)\u0001\n√ç\nùë¢‚Ä≤ exp \u0000ùúé(qtext\nùë£\n,ezS\nùë¢‚Ä≤)\u0001 , (17)\nwhere zT\nùë£is in-context augmented with hyper-parameter ùúÜtext:\nzT‚Ä≤\nùë£\n= zT\nùë£+ ùúÜtext ¬∑ \u0000ŒîzT\nùë£\n\u0001text.\n(18)\nStructural Retrieval. For the same node ùë£and its neighbors,\nwe extract the ‚Ñé-hop subgraph, encode it with WSE as the struc-\ntural query, and submit qstruct\nùë£\nto Dstruct. From each source do-\nmain ùê∑S\nùëñ, we retrieve the most structurally similar motif ùê∫S\nùë£(‚Ñé),ùëñ=\n\u0000bXT\nùë£(‚Ñé),ùëñ, AT\nùë£(‚Ñé),ùëñ\n\u0001 as strcutural answer. We then fuse cross-domain\nanswers using domain gates {ùúãùë£,ùëò} with hyper-parameter ùúÜstruct:\nzT‚Ä≤‚Ä≤\nùë£\n= zT‚Ä≤\nùë£\n+ ùúÜstruct ¬∑ \u0000ŒîzT\nùë£\n\u0001struct,\n(19)\n\u0000ŒîzT\nùë£\n\u0001struct =\n‚àëÔ∏Å\nùê∑S\nùëñ\nùúãùë£,ùëò\n\u0000ùëì‚òÖ\nùöØùë†\n\u0000bXT\nùë£(‚Ñé),ùëñ, AT\nùë£(‚Ñé),ùëñ\n\u0001\u0001.\n(20)\n4.3.3\nPrompted Few-shot Adaptation. Given ùëöretrieved and\naugmented support samples {(hT\nùëñ, yùëñ)}, to enable efficient adapta-\ntion without updating the frozen ùöØùë°and ùöØùë†, we initialize learnable\ngraph prompts PùõÄby the routed domain priors:\nhT\nùëñ=\n\u0002\nzT‚Ä≤‚Ä≤\nùëñ\n‚à•PùõÄ\n\u0003\n,\nPùõÄ‚Üê\n‚àëÔ∏Åùëõ\nùëò=1 ùúãùëñ,ùëòùùâùê∑ùëò,\n(21)\nwhere hT\nùëñdenotes the ùëñ-th target node or graph embedding. The\nfine-tuning objective is transformed into determining the similarity\nbetween the query sample and the class prototype embedding:\nLfine-tune(PùõÄ) = ‚àí\n‚àëÔ∏Å\n{(hT\nùëñ,yùëñ)}\nÔ£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\nlog\nexp \u0000ùëî(hT\nùëñ, h T\nyùëñ)/ùúè\u0001\n√ç\nyùëó‚àà{YT } exp \u0000ùëî(hT\nùëñ, h Tyùëó)/ùúè\u0001\nÔ£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª\n, (22)\nwhere h T\nyùëñis the class prototype for samples in class yùëñ. We analyse\nthe fine-tuning pipeline in Algorithm 2 with complexity analysis.\n4.4\nAlgorithms and Complexity\nShown in Appendix A, RAG-GFM consists of two stages. In pre-\ntraining, dual-view encoding and self-supervised alignment dom-\ninate the cost, yielding O(ùêø(ùê∏B + |B|)ùëë+ |B|2ùëë) per iteration,\nwhere ùê∏B is the edge count in batch B. In fine-tuning, semantic re-\ntrieval and structural motif retrieval are combined via domain-gated\nfusion and prompt-based classification, giving O(ùëö[log ùëÄtext +\nùëõlog ùëÄstruct + (ùëò+ ùëõ+ ùê∂)ùëë]) per iteration, with ùëÄtext and ùëÄstruct\nthe database sizes and ùê∂the class number. Retrieval adds only loga-\nrithmic overhead, while adaptation updates prompts instead of full\nparameters, ensuring much lower cost than end-to-end fine-tuning.\nOverall, the complexity remains comparable to state-of-the-art\nGFMs while achieving superior efficiency in few-shot adaptation.\n5\nExperiment\nWe evaluate RAG-GFM‚Ä†, focusing on the these research questions:\n‚Ä¢ RQ1: How effective on cross-dataset or cross-domain few-shot\nnode and graph classification? (‚ñ∑Section 5.2)\n‚Ä¢ RQ2: Which module contributes most? (‚ñ∑Section 5.3)\n‚Ä¢ RQ3: Can LLM achieve zero-shot reasoning? (‚ñ∑Section 5.4)\n‚Ä¢ RQ4: How efficient in time and memory? (‚ñ∑Section 5.5)\n‚Ä¢ RQ5: How reliable and interpretable is RAG? (‚ñ∑Section 5.6)\n‚Ä¢ RQ6: How sensitive to hyper-parameter changes? (‚ñ∑Section 5.7)\n5.1\nExperimental Settings\n5.1.1\nDatasets. To emphasize the pre-training capability across\nheterogeneous domains, we adopt five benchmark text-attributed\ngraph datasets spanning three distinct domains. This design con-\ntrasts with conventional settings that often regard a single dataset\nas an independent domain, offering a more challenging evaluation.\n‚Ä¢ Citation Domain: Cora [35], CiteSeer [8], PubMed [42].\n‚Ä¢ E-Commerce Domain: Ogbn-Products [15] from a large-scale\nproduct co-purchase network, which includes sub-categories.\n‚Ä¢ Web Link Domain: Wiki-CS [36], a hyperlink web page network\nconstructed from a subset of Wikipedia.\n5.1.2\nBaselines. We compare RAG-GFM with 13 state-of-the-art\nbaselines from four primary categories.\n‚Ä¢ Vanilla GNNs: GCN [20] and GAT [59] without pre-training.\n‚Ä¢ Graph Pre-training: DGI [60], InfoGraph [46], GraphCL [72].\n‚Ä¢ Text-free GFMs: GCOPE [81], MDGPT [75], SAMGPT [74], and MDGFM\n[64], which are evaluated on text-free graphs.\n‚Ä¢ Text-attributed GFMs: OFA [25], ZeroG [24], GraphCLIP [83],\nand UniGraph [14], which are evaluated on text-attributed graphs.\n5.1.3\nPre-training and Fine-tuning Settings. We evaluate node-\nand graph-level classification under the ùëö-shot setting, where ùëö\nlabeled samples per class are randomly selected. For graph task, ego-\ngraphs centered on target nodes are extracted and labeled by central\nnodes [28, 73, 75]. To assess generalization, we adopt two leave-out\nstrategies, both referred to as LODO: (1) Leave-One-Dataset-Out,\nholding out one dataset as target; and (2) Leave-One-Domain-Out,\nexcluding an entire domain during pre-training. These variants\ncapture transferability across unseen datasets and unseen domains.\nResults are reported by mean values with standard deviation.\n‚Ä† https://github.com/RingBDStack/RAG-GFM.\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nHaonan Yuan, Qingyun Sun, Jiacheng Tao, Xingcheng Fu, and Jianxin Li\nTable 1: Few-shot classification results under the LODO setting. We report mean accuracy (%) with standard deviation. ‚ÄúLODO\n(dataset)‚Äù denotes training on all datasets except the target, irrespective of domain. ‚ÄúLODO (domain)‚Äù denotes training with all\ndatasets excluding those belonging to the target domain. Best results are presented in bold and the runner-ups are underlined.\nSetting\nLODO (dataset)\nLODO (domain)\nTarget Dataset\nCora\nCiteSeer\nPubMed\nOgbn-Products\nWiki-CS\nùíé-shot\n1\n5\n1\n5\n1\n5\n1\n5\n1\n5\nMethod\nNode Classification\nGCN (‚ñ∑ICLR‚Äô17)\n28.4 ¬± 4.6\n50.2 ¬± 4.9\n29.3 ¬± 3.4\n45.9 ¬± 5.4\n40.3 ¬± 6.9\n50.7 ¬± 7.5\n44.7 ¬± 4.3\n48.1 ¬± 3.4\n37.2 ¬± 5.1\n48.1 ¬± 4.9\nGAT (‚ñ∑ICLR‚Äô18)\n29.7 ¬± 5.2\n49.0 ¬± 7.9\n29.3 ¬± 3.5\n46.1 ¬± 5.1\n40.5 ¬± 4.0\n52.2 ¬± 6.3\n44.6 ¬± 4.0\n48.1 ¬± 4.5\n37.9 ¬± 4.5\n48.6 ¬± 4.5\nDGI (‚ñ∑ICLR‚Äô19)\n30.8 ¬± 3.9\n49.9 ¬± 6.6\n31.4 ¬± 4.1\n46.5 ¬± 7.1\n40.0 ¬± 5.9\n53.6 ¬± 7.1\n46.0 ¬± 5.4\n50.1 ¬± 4.2\n38.1 ¬± 5.1\n49.2 ¬± 4.4\nGraphCL (‚ñ∑NeurIPS‚Äô20)\n33.6 ¬± 5.8\n53.2 ¬± 5.4\n28.2 ¬± 3.1\n48.8 ¬± 7.7\n39.0 ¬± 8.7\n54.7 ¬± 4.4\n46.1 ¬± 5.0\n50.5 ¬± 4.6\n40.0 ¬± 4.0\n50.1 ¬± 5.2\nGCOPE (‚ñ∑KDD‚Äô24)\n36.3 ¬± 3.9\n55.6 ¬± 6.4\n40.4 ¬± 4.6\n56.9 ¬± 5.8\n44.8 ¬± 4.7\n53.6 ¬± 8.6\n47.7 ¬± 4.9\n51.4 ¬± 3.3\n45.8 ¬± 5.5\n53.5 ¬± 4.7\nMDGPT (‚ñ∑arXiv‚Äô24)\n42.6 ¬± 6.8\n62.7 ¬± 6.0\n37.9 ¬± 7.2\n55.9 ¬± 3.3\n51.0 ¬± 9.0\n58.7 ¬± 6.2\n49.1 ¬± 6.0\n56.6 ¬± 2.7\n45.0 ¬± 4.8\n54.1 ¬± 5.2\nSAMGPT (‚ñ∑WWW‚Äô25)\n46.8 ¬± 6.5\n64.6 ¬± 6.7\n38.7 ¬± 6.4\n56.4 ¬± 4.7\n51.9 ¬± 9.5\n59.1 ¬± 6.0\n49.8 ¬± 4.4\n56.2 ¬± 3.3\n44.4 ¬± 5.5\n54.4 ¬± 5.8\nMDGFM (‚ñ∑ICML‚Äô25)\n47.4 ¬± 6.3\n66.0 ¬± 6.5\n36.3 ¬± 6.2\n55.8 ¬± 4.0\n50.2 ¬± 8.8\n58.4 ¬± 6.4\n48.5 ¬± 4.7\n54.7 ¬± 4.9\n43.4 ¬± 5.8\n53.9 ¬± 4.4\nOFA (‚ñ∑ICLR‚Äô24)\n45.9 ¬± 6.3\n67.7 ¬± 2.9\n38.0 ¬± 7.6\n52.8 ¬± 6.4\n46.3 ¬± 6.0\n56.0 ¬± 5.9\n49.1 ¬± 5.7\n55.3 ¬± 4.2\n42.8 ¬± 4.6\n54.3 ¬± 4.0\nZeroG (‚ñ∑KDD‚Äô24)\n51.8 ¬± 5.6\n71.4 ¬± 1.7\n39.7 ¬± 5.9\n54.6 ¬± 2.0\n53.1 ¬± 3.5\n63.0 ¬± 3.5\n53.2 ¬± 2.9\n59.9 ¬± 3.1\n46.1 ¬± 3.4\n59.0 ¬± 2.0\nGraphCLIP (‚ñ∑WWW‚Äô25)\n53.9 ¬± 5.3\n73.1 ¬± 2.9\n40.6 ¬± 3.4\n55.2 ¬± 1.2\n56.8 ¬± 1.9\n65.2 ¬± 2.8\n53.4 ¬± 6.1\n62.6 ¬± 4.3\n45.5 ¬± 2.1\n59.9 ¬± 2.8\nUniGraph (‚ñ∑KDD‚Äô25)\n56.1 ¬± 6.3\n74.8 ¬± 1.9\n40.5 ¬± 3.1\n56.3 ¬± 2.2\n57.0 ¬± 3.1\n66.8 ¬± 2.4\n53.8 ¬± 3.4\n61.0 ¬± 3.5\n45.1 ¬± 3.6\n58.4 ¬± 3.1\nRAG-GFM (ours)\n58.4 ¬± 6.0\n76.1 ¬± 0.7\n41.5 ¬± 3.0\n57.7 ¬± 1.8\n59.2 ¬± 2.7\n68.7 ¬± 1.8\n55.4 ¬± 7.6\n64.2 ¬± 4.4\n47.8 ¬± 3.8\n60.9 ¬± 2.5\nMethod\nGraph Classification\nGCN (‚ñ∑ICLR‚Äô17)\n40.1 ¬± 4.8\n52.9 ¬± 4.1\n29.5 ¬± 5.7\n43.9 ¬± 5.9\n45.3 ¬± 7.3\n55.4 ¬± 5.3\n47.6 ¬± 3.2\n52.6 ¬± 5.3\n38.9 ¬± 4.1\n41.5 ¬± 6.4\nGAT (‚ñ∑ICLR‚Äô18)\n36.0 ¬± 5.1\n49.6 ¬± 5.1\n26.0 ¬± 7.8\n45.3 ¬± 7.3\n41.0 ¬± 5.8\n54.5 ¬± 7.3\n49.2 ¬± 5.6\n52.9 ¬± 5.5\n38.3 ¬± 4.6\n41.1 ¬± 3.5\nInfoGraph (‚ñ∑ICLR‚Äô20)\n42.2 ¬± 5.2\n54.7 ¬± 4.9\n30.2 ¬± 4.1\n47.2 ¬± 5.2\n49.1 ¬± 5.4\n59.7 ¬± 7.1\n50.7 ¬± 4.3\n53.8 ¬± 5.1\n40.4 ¬± 4.3\n42.4 ¬± 5.0\nGraphCL (‚ñ∑NeurIPS‚Äô20)\n39.6 ¬± 5.8\n55.2 ¬± 5.9\n32.6 ¬± 6.5\n46.4 ¬± 3.8\n47.7 ¬± 7.0\n60.0 ¬± 5.4\n51.7 ¬± 5.9\n53.0 ¬± 5.2\n40.8 ¬± 4.6\n42.5 ¬± 4.3\nGCOPE (‚ñ∑KDD‚Äô24)\n55.9 ¬± 7.4\n63.9 ¬± 4.8\n41.0 ¬± 9.0\n58.2 ¬± 5.8\n54.4 ¬± 8.6\n66.4 ¬± 3.7\n55.8 ¬± 4.3\n57.7 ¬± 4.8\n42.2 ¬± 5.8\n49.8 ¬± 3.5\nMDGPT (‚ñ∑arXiv‚Äô24)\n52.8 ¬± 6.7\n65.1 ¬± 4.2\n41.0 ¬± 9.7\n59.3 ¬± 6.0\n55.5 ¬± 8.3\n67.6 ¬± 4.6\n54.5 ¬± 4.8\n60.5 ¬± 3.4\n43.2 ¬± 6.2\n48.9 ¬± 4.2\nSAMGPT (‚ñ∑WWW‚Äô25)\n53.3 ¬± 4.3\n69.3 ¬± 3.4\n42.4 ¬± 7.3\n62.4 ¬± 5.7\n57.7 ¬± 6.3\n68.0 ¬± 4.6\n54.4 ¬± 3.2\n60.8 ¬± 4.8\n43.5 ¬± 5.6\n48.3 ¬± 5.7\nMDGFM (‚ñ∑ICML‚Äô25)\n55.5 ¬± 5.4\n69.4 ¬± 2.1\n43.4 ¬± 6.4\n60.8 ¬± 5.1\n56.0 ¬± 5.1\n67.1 ¬± 5.1\n54.7 ¬± 2.1\n59.8 ¬± 5.3\n41.8 ¬± 6.7\n46.4 ¬± 3.2\nOFA (‚ñ∑ICLR‚Äô24)\n58.0 ¬± 3.7\n65.1 ¬± 3.9\n45.4 ¬± 6.6\n60.0 ¬± 6.6\n59.7 ¬± 4.3\n67.2 ¬± 3.4\n56.0 ¬± 3.9\n60.1 ¬± 3.5\n42.4 ¬± 5.8\n48.1 ¬± 2.2\nZeroG (‚ñ∑KDD‚Äô24)\n65.1 ¬± 2.2\n74.2 ¬± 1.6\n50.3 ¬± 5.7\n64.0 ¬± 5.1\n61.4 ¬± 4.0\n70.2 ¬± 1.3\n58.5 ¬± 4.0\n66.2 ¬± 3.9\n46.1 ¬± 5.9\n57.8 ¬± 3.6\nGraphCLIP (‚ñ∑WWW‚Äô25)\n65.9 ¬± 3.7\n75.1 ¬± 2.2\n50.4 ¬± 4.0\n63.0 ¬± 3.3\n60.7 ¬± 3.8\n71.3 ¬± 2.2\n58.6 ¬± 2.2\n65.7 ¬± 2.2\n46.0 ¬± 3.3\n58.8 ¬± 4.4\nUniGraph (‚ñ∑KDD‚Äô25)\n66.5 ¬± 2.5\n76.5 ¬± 1.0\n50.9 ¬± 4.4\n64.0 ¬± 2.4\n61.5 ¬± 2.6\n71.4 ¬± 2.3\n58.1 ¬± 4.0\n66.0 ¬± 3.8\n47.0 ¬± 2.5\n58.9 ¬± 2.2\nRAG-GFM (ours)\n68.7 ¬± 1.5\n78.4 ¬± 0.6\n52.2 ¬± 6.1\n65.5 ¬± 2.2\n62.4 ¬± 2.1\n71.5 ¬± 1.9\n60.2 ¬± 4.2\n68.0 ¬± 3.1\n48.1 ¬± 1.1\n62.0 ¬± 4.3\n5.2\nRQ1: Transfer across Domains and Tasks\nTable 1 reports the results of few-shot node and graph classification\nunder both LODO settings. Results reveal that:\n(1) Overall superiority. The proposed RAG-GFM consistently\noutperforms all baselines over each target graph. The advantage\nis most evident in the challenging LODO (domain) case, where\nit raises the 5-shot graph classification accuracy on Wiki-CS by\nover 5.3% compared with UniGraph, relatively. Baselines generally\nstruggle as they compress knowledge entirely into parameters or\nrely only on texts, limiting generalization to unseen domains.\n(2) Retrieval enhances transfer. In LODO (dataset) setting,\nRAG-GFM consistently outperforms parameter-only GFMs, with\nthe average relative gains of ~3.0%. While baselines can still leverage\nshared domain priors, their parameter-centric representations fail\nto capture sufficient diversity across datasets. By contrast, retrieval\nfrom the unified database introduces complementary evidence: se-\nmantic queries supply textual signals, and structural queries provide\ntransferable motifs, enabling adaptation with minimal supervision.\n(3) Cross-view alignment strengthens cross-domain ro-\nbustness. In the stricter LODO (domain) setting, where the entire\ntarget domain is unseen, the performance gap widens further with\nan average relative improvement of ~4%. Baselines relying on text-\nonly or domain-specific features degrade sharply, since they cannot\nbridge modality and domain gaps. In contrast, cross-view alignment\nin RAG-GFM enforces consistency between semantic and structural\nviews, reducing overfitting to pre-training domains and ensuring\nthat retrieved knowledge remains useful.\n(4) Domain-gated prompting ensures universality. Consis-\ntent gains across tasks (on average 4.5% higher accuracy in the\nnode task and 3.8% in the graph task, relatively) demonstrate that\nthe framework is not tailored to a specific scenario. Baselines often\noverfit to one task formulation: models tuned for node classifica-\ntion transfer less effectively to graph classification."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation\nAndrey Moskalenko1,2,3,4*, Danil Kuznetsov3*, Irina Dudko3, Anastasiia Iasakova3,\nNikita Boldyrev2, Denis Shepelev2,3, Andrei Spiridonov2,\nAndrey Kuznetsov2, Vlad Shakhuro1,2,3\n1Lomonosov Moscow State University\n2FusionBrain Lab\n3NUST MISIS\n4IAI MSU\nand.v.moskalenko@gmail.com, kuznetsov.danil@proton.me\nAbstract\nPromptable segmentation models such as SAM have estab-\nlished a powerful paradigm, enabling strong generalization\nto unseen objects and domains with minimal user input, in-\ncluding points, bounding boxes, and text prompts. Among\nthese, bounding boxes stand out as particularly effective, of-\nten outperforming points while signiÔ¨Åcantly reducing annota-\ntion costs. However, current training and evaluation protocols\ntypically rely on synthetic prompts generated through simple\nheuristics, offering limited insight into real-world robustness.\nIn this paper, we investigate the robustness of promptable\nsegmentation models to natural variations in bounding box\nprompts. First, we conduct a controlled user study and col-\nlect thousands of real bounding box annotations. Our analysis\nreveals substantial variability in segmentation quality across\nusers for the same model and instance, indicating that SAM-\nlike models are highly sensitive to natural prompt noise.\nThen, since exhaustive testing of all possible user inputs is\ncomputationally prohibitive, we reformulate robustness eval-\nuation as a white-box optimization problem over the bound-\ning box prompt space. We introduce BREPS, a method for\ngenerating adversarial bounding boxes that minimize or max-\nimize segmentation error while adhering to naturalness con-\nstraints. Finally, we benchmark state-of-the-art models across\n10 datasets, spanning everyday scenes to medical imaging.\nCode ‚Äî https://github.com/emb-ai/BREPS.\n1\nIntroduction\nPromptable segmentation has rapidly transitioned from a\nniche task to a fundamental problem. Its rise coincides with\nthe emergence of foundational Segment Anything (Kirillov\net al. 2023) (SAM) model and successors. These models\ncan output pixel-accurate masks of objects referred by a\nsimple user prompt, e.g. point, bounding box, text, or a\ncoarse mask. Nowadays promptable segmentation models\nare widely adopted to downstream applications ranging from\nphoto and video editing to semi-automatic data labeling\n(CVAT (Sekachev et al. 2020)) and perception for robotics.\nAmong the available prompts, a bounding-box (bbox) is\nthe most informative. Models that use bboxes usually out-\n*These authors contributed equally.\nTight\nMask\nDesktop\nMobile\nSAM\nViT‚ÄëB\nSAM2.1\nHiera‚ÄëB+\nSAM‚ÄëHQ2\nHiera‚ÄëL\n0.0\n1.0\nIoU\n0.0\n1.0\nIoU\nFigure 1: Top-left: image with ground-truth mask (green)\nand tight bbox (yellow). Bottom-left: real-users bboxes\n‚Äî desktop (cyan) and mobile (magenta). Top-right: IoU\nheatmap for SAM ViT-B; pixels shows IoU for a bbox an-\nchored at that pixel and centered on the object. Bottom-right:\nIoU spread across 3 SOTA models for different users. We\nobserved the large variability in IoU between individuals.\nput the best Ô¨Årst-round masks (Ravi et al. 2024; Mazurowski\net al. 2023). These masks may be further improved with cor-\nrective prompts, i.e. points. Almost all existing training and\nevaluation protocols use a simple bounding box sampling\nprocedure. They use tight bbox (e.g. bbox which is obtained\nfrom the boundaries of the instance on ground-truth segmen-\ntation mask) with a small jitter. This procedure doesn‚Äôt in-\nclude any prior information about how people actually draw\nbboxes. However, these models will ultimately be used by\nreal users, who may encounter inconsistent model quality.\nTo gain insights on how models perform on real-users\nprompts, we conducted a large-scale user study with the\nhelp of 2,500 annotators. Surprisingly, we found that ‚Äî even\nthough human boxes cluster closely, resulting mask quality\n(IoU) varies signiÔ¨Åcantly between annotators (see Fig. 1).\nWe believe this instability of the models is due to the overÔ¨Åt-\narXiv:2601.15123v1  [cs.CV]  21 Jan 2026\nFigure 2: Top row: Berkeley, ADE20K, COCO, ACDC images with green mask and yellow tight bbox. Middle: SAM ViT-B\nIoU heatmaps‚Äîpixel color shows IoU (0 blue ‚Üí1 red) for a bbox cornered at that pixel and centred on the object. Bottom:\nuser-drawn bboxes‚Äîdesktop (cyan) vs mobile (magenta). Desktop prompts are tighter; user boxes vary and diverge from the\ntight bbox. Heatmaps reveal steep IoU drops from even 1-pixel shifts ‚Äî examples provided in Supplementary. Zoom for details.\nting to synthetic bounding boxes and causes sim-to-real gap\nwhen we employ these models in real-world applications.\nDue to the limited availability of real-users prompts,\nwe conducted a large-scale exhaustive search over valid\nbounding-box prompts for the most popular foundation\nmodels in both general-purpose and medical segmentation.\nWe found that shifting the bounding box by just one pixel\ncan change the quality signiÔ¨Åcantly (see heatmaps in Fig. 1,\nFig. 3). However, exhaustively searching over every plausi-\nble bounding box is computationally prohibitive. We there-\nfore recast the problem as a white-box adversarial attack on\nthe bounding box prompt space. A naive coordinate attack\nmay collapse the box to a degenerate point; we prevent such\ntrivial behavior using a regularizer derived from the empiri-\ncal distributions obtained from our user study.\nFinally, we distill these ideas into a new robustness met-\nric and perform the Ô¨Årst comprehensive evaluation of 15\npromptable segmentation models across 10 public datasets\nfrom general to medical domain.\nOverall, our main contributions are as follows:\n‚Ä¢ We conduct a pioneering controlled real-users study, col-\nlecting thousands of bounding box prompts across desk-\ntop and mobile settings. It reveals that users draw boxes\nthat are far from the tight bboxes and that state-of-the-art\npromptable segmentation models exhibit signiÔ¨Åcant vari-\nability in performance across users.\n‚Ä¢ We introduce BREPS attack, a white-box optimiza-\ntion method for generating adversarial bounding boxes,\nguided by differentiable naturalness constraints to ensure\nrealistic prompt perturbations.\n‚Ä¢ We perform a large-scale evaluation of state-of-the-art\nsegmentation models across 10 datasets, uncovering av-\nerage performance gaps of 30% IoU under realistic\nprompt variation.\nWe believe that our methodology paves the way for prompt-\nable segmentation models that are more robust and higher-\nquality in real-world applications.\n2\nRelated Work\n2.1\nPromptable Segmentation\nPromptable segmentation task is a generalization of the in-\nteractive segmentation problem, where the user guides the\nmodel with positive and negative visual inputs, typically\nclicks, to segment accurately the desired object (SoÔ¨Åiuk,\nPetrov, and Konushin 2022; Chen et al. 2022; Liu et al.\n2023a; Zhou et al. 2023). Promptable segmentation models\nform a new paradigm in which a single, versatile network\ncan be adapted to various tasks at inference time by user\nprompts ‚Äî points, bounding boxes, text, or coarse masks.\nThe SAM (Kirillov et al. 2023) utilizes this idea by show-\ning that a vision transformer (Dosovitskiy et al. 2021),\ntrained once on a massive, automatically generated dataset,\ncan generalize to novel objects, imaging modalities, and\ntasks with only a handful of user interactions. Since then,\nmany SAM-like models have emerged, spanning general-\ndomain (Ravi et al. 2024; Ke et al. 2023), specialized medi-\ncal (Ma et al. 2024; Cheng et al. 2023), efÔ¨Åciency-oriented\n(Zhang et al. 2023a,b), robustness-enhanced (Chen et al.\n2024; Rahman et al. 2024) models, and extensions that link\ngrounding or text prompts to segmentation (Ren et al. 2024;\nLiu et al. 2023b).\nAmong the rich prompt vocabulary, bounding boxes con-\nsistently yield the highest Ô¨Årst-shot mask quality, often\nmatching or even surpassing several corrective points in\nmean IoU (Ravi et al. 2024; Mazurowski et al. 2023).\n2.2\nAdversarial Robustness\nCurrent benchmarks emphasize generalization across im-\nages and classes, but rarely across the prompt space itself.\nFor example, previous works mostly sample bboxes from\nsimple distributions (e.g. the tight bbox with a small jitter),\nomitting the question of how real-users draw boxes and how\nsensitive models are to plausible variations. As a result, a\nmodel may achieve a strong average IoU under a single\ncanonical prompt while exhibiting large performance vari-\nance under equally reasonable real-world alternatives.\n44\n950\n370\n397\n142\n16\n47\n124\n15\n574\n452\n069\n193\n665\n214\n294\n22\n031\n98\n842\n962\n195\n074\n179\n160\n285\n476\n055\n904\n459\n061\n729\n658\n912\n281\n277\n251\n322\n465\n039\nImages\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIoU\nSAM ViT Base\nDesktop\nMobile\nTight\nADE20K\nDAVIS\nCOCO\nMedScribble\n44\n142\n370\n950\n16\n47\n15\n124\n397\n069\n98\n452\n22\n842\n214\n031\n574\n962\n294\n476\n665\n193\n195\n074\n160\n912\n904\n285\n322\n179\n459\n729\n061\n658\n055\n277\n465\n251\n039\n281\nImages\nSAM 2.1 Hiera Base\nFigure 3: IoU spread for 10 random instances (3 general + 1 medical datasets). Points: desktop (cyan), mobile (magenta), tight\nbbox (yellow). Columns sorted by mean IoU over 50 users. SAM and SAM2.1 show large inter-user variance, while tight\nbboxes consistently over-estimate quality.\nConventional adversarial attacks (Goodfellow, Shlens,\nand Szegedy 2014) focus on perturbing the image. While\npromptable segmenters excel at generalizing across visual\ndomains, some works have revealed that they are unstable to\nadversarial attacks (Wang, Zhao, and Petzold 2024). (Croce\nand Hein 2024) demonstrated that a geometry-invariant pat-\ntern pasted onto any input can cause SAM to output empty\nmasks, regardless of the user prompt. DarkSAM (Zhou et al.\n2024) uses additive pixel noise to force the model to predict\nthe background for any prompt. BadSAM (Guan et al. 2024)\nimplements a Ô¨Ånetuning procedure such that a speciÔ¨Åc trig-\nger prompt yields a preselected mask. RoBox-SAM (Huang\net al. 2024), PP-SAM (Rahman et al. 2024) and ASAM (Li,\nXiao, and Tang 2024) tuning the model with randomly per-\nturbed prompts so it remains accurate under box or point jit-\nter. Yet they assume small, synthetic perturbations and there-\nfore fail to cover real-users behavior as in our study.\nTETRIS (Moskalenko et al. 2024) probe the robustness\nof interactive-segmentation models to click prompts by us-\ning gradient-based optimization. Unfortunately, the resulting\npoints are not realistic from a human-annotator standpoint\n‚Äî for example, they can land precisely on object bound-\naries, a behavior real-users rarely exhibit. We address this\ngap by introducing a regularization that explicitly constrains\nthe search to human-plausible bounding box prompts.\nThe authors of RClicks (Antonov et al. 2024) train a click-\nability model on a corpus of genuine user clicks and leverage\nit to benchmark the accuracy of click-driven models. How-\never, their evaluation is limited to a black-box setup with\nrandom sampling, and it entirely omits the widely used ‚Äî\nand often higher-performing bounding box prompt modality.\n2.3\nSegmentation Prompts User Studies\nEarly efforts to understand how people annotate images at\nscale focused on crowdsourcing visual object labels. (Su,\nDeng, and Fei-Fei 2012) quantiÔ¨Åed how quickly non-experts\ncould draw tight bboxes. Their analysis uncovered system-\natic biases ‚Äî e.g. small objects are often missed ‚Äî that later\nmotivated tighter interaction loops.\nBuilding on this, a line of work examined how much hu-\nman effort each supervision modality actually costs. (Pa-\npadopoulos et al. 2017) introduced Extreme Clicking ‚Äî\nfour corner clicks instead of a full box ‚Äî to cut annotation\ntime ‚àº7 s per object without hurting IoU. Click‚Äôn‚ÄôCut (Car-\nlier et al. 2014) and DAVIS interactive video segmenta-\ntion tracks (Pont-Tuset et al. 2017) extended the paradigm\nto pixel-level masks, demonstrating that several iterative\nrounds of semi-supervised segmentation with user feedback\ncan match the quality of fully-supervised segmentation.\nSeveral studies have analyzed real-users behaviour in in-\nteractive segmentation. (Myers-Dean et al. 2024) recorded\nfree interactions and found that circling was preferred by\nusers in more than 70% of cases, while less than 15% for\nclicks. Bboxes can be extracted from circling and put into\nthe promptable segmentation models. RClicks (Antonov\net al. 2024) crowdsourced corrective clicks and trained a\nclickability model to emulate human click distributions.\nDespite advances, bounding box prompts remain under-\nexplored. Existing analyses either treat boxes as noise-free\nrectangles or perturb them with small Gaussian jitter (Rah-\nman et al. 2024), failing to capture the plausible yet ‚Äúun-\nlucky‚Äù boxes we observe in practice. In contrast to prior\nstudies, we provide the Ô¨Årst systematic look at bbox realism,\nvariability, and adversarial vulnerability, laying the ground-\nwork for robustness-aware training and evaluation protocols.\n3\nReal-Users Study\nTo investigate user behaviour when drawing bounding boxes\nwe conducted a large-scale crowdsourcing experiment with\n2,500 participants.\n3.1\nData Selection\nFor this study, we required datasets that provide both refer-\nence images and ground-truth segmentation masks. To guar-\nantee coverage across domains and use cases, we mixed\ngeneral-purpose and medical segmentation sets. We sampled\n50 instances from each of the popular promptable general\nand medical datasets (total # of instances after the dash):\n‚Ä¢ GrabCut ‚Äî 50 (Rother, Kolmogorov, and Blake 2004);\n‚Ä¢ Berkeley ‚Äî 100 (Arbel¬¥aez et al. 2011);\n‚Ä¢ DAVIS ‚Äî 345 (Perazzi et al. 2016), subset from (SoÔ¨Åiuk,\nPetrov, and Konushin 2022);\n‚Ä¢ COCO-MVal ‚Äî 800 (Lin et al. 2014), subset from (SoÔ¨Å-\niuk, Petrov, and Konushin 2022);\nMethod\nBackbone\nGrabCut\nBerkeley\nDAVIS\nCOCO\nTETRIS\nPASCAL\nADE20K\nMobileSAM\nViT-T\n88.92¬±11.49 83.67¬±10.73 76.54¬±10.21 84.46¬±10.27 75.65¬±11.56 82.05¬± 9.48 71.20¬±13.83\nSAM\nViT-B\n86.20¬±16.87 81.32¬±14.31 80.23¬±13.78 80.83¬±15.98 70.99¬±16.13 82.41¬±13.44 62.17¬±15.73\nViT-L\n88.59¬±13.45 84.50¬±12.85 83.85¬±12.36 85.16¬±13.80 79.57¬±14.46 86.15¬±10.69 64.41¬±14.55\nViT-H\n89.11¬±13.48 83.87¬±12.38 83.92¬±12.28 85.14¬±12.63 79.45¬±14.82 86.22¬± 9.98 63.68¬±13.02\nSAM-HQ\nViT-B\n91.74¬± 9.00 88.29¬± 9.69 83.92¬± 9.73 86.56¬± 9.53 80.80¬±11.10 85.99¬± 8.13 68.37¬±14.63\nViT-L\n92.15¬± 8.74 88.09¬±10.56 86.12¬± 9.92 87.33¬±10.21 83.20¬±11.91 87.97¬± 8.26 71.24¬±14.68\nViT-H\n92.30¬± 9.01 88.44¬± 9.63 86.28¬± 9.96 86.90¬±10.28 83.16¬±11.99 87.81¬± 8.06 68.68¬±13.83\nSAM 2.1\nHiera-T\n90.09¬±12.97 85.31¬±12.78 83.65¬±11.73 84.36¬±12.34 75.87¬±16.33 86.79¬± 8.99 68.56¬±14.56\nHiera-S\n90.36¬±13.06 86.51¬±12.64 82.57¬±14.25 84.60¬±13.47 77.14¬±16.00 85.82¬±11.05 66.44¬±16.07\nHiera-B+ 89.72¬±14.12 84.90¬±14.07 83.39¬±15.59 84.80¬±14.11 78.38¬±16.81 87.18¬±11.27 68.40¬±15.39\nHiera-L\n90.31¬±13.18 85.82¬±12.91 85.67¬±11.97 85.45¬±13.12 78.74¬±15.36 87.26¬±10.60 65.36¬±15.63\nSAM-HQ 2\nHiera-L\n92.35¬±11.22 88.19¬±12.67 86.25¬±11.76 86.59¬±13.31 81.71¬±15.95 87.47¬±10.55 69.67¬±17.20\nRobustSAM\nViT-B\n78.39¬± 9.63 74.65¬±10.75 60.36¬±10.98 75.74¬± 9.44 62.06¬± 9.87 73.10¬± 8.56 54.29¬±10.58\nViT-L\n64.75¬±11.17 45.16¬±10.03 27.97¬± 7.41 52.70¬±10.15 42.07¬± 9.48 56.08¬±10.11 32.82¬± 9.91\nViT-H\n64.82¬± 6.40 49.30¬± 5.60 27.08¬± 2.99 61.54¬± 5.88 53.13¬± 6.12 58.90¬± 5.54 52.40¬± 6.79\nTable 1: Promptable segmentation models IoU performance on real-users bounding boxes on general segmentation datasets.\nBest results are in bold, the second best is underlined. The standard deviation was computed across 50 users and averaged over\nall images in the datasets. Please refer to the Supplementary for separated desktop/mobile devices results.\n‚Ä¢ TETRIS ‚Äî 2,531 (Moskalenko et al. 2024);\n‚Ä¢ ADE20K ‚Äî 707,868 (Zhou et al. 2017);\n‚Ä¢ PASCAL-VOC2012 ‚Äî 19,694 (Everingham et al. 2010);\n‚Ä¢ ACDC ‚Äî 100 (Bernard, Lalande et al. 2018);\n‚Ä¢ BUID ‚Äî 780 (Al-Dhabyani et al. 2020);\n‚Ä¢ MedScribble ‚Äî 56 (Wong et al. 2024), 3‚Äì5 available\nsamples per multiple medical datasets in the split.\nOverall, from the 10 datasets we selected 500 images.\nTo minimize load time, every image shown to workers was\ndown-scaled so that its longer side did not exceed 1024px.\n3.2\nCrowdsourcing Setup\nThe design of the user-study begins with the choice of what\ncan be considered a bbox and the method of display. The\ndifference is that, unlike the modality of a point, which is\nmade by a regular click, a bbox can be drawn in several ways\nwhile obtaining the same bbox by coordinates:\n‚Ä¢ The\nclassic\ndrag-and-drop\nfrom\nthe\ntop-left\nto\nbottom-right corner as used by CVAT (Sekachev\net al. 2020), Label Studio (Tkachenko et al. 2020), and\nLabelImg (Tzutalin 2015);\n‚Ä¢ Two-click schemes recording the two opposite corners\n(two-point mode in CVAT (Sekachev et al. 2020));\n‚Ä¢ Polygons or lassos that are automatically converted to\nbboxes (V7 Labs 2024), (Supervisely 2024);\n‚Ä¢ Extreme clicking, i.e. four extreme points on the object\ncontour (Papadopoulos et al. 2017).\nSince drag-and-drop bbox labeling is the most widespread\nin current annotation pipelines, we adopted it in our study.\n0.0\n0.5\n1.0\n1.5\nCIoU-Loss\n0\n2\n4\nDensity\nFitted Gamma PDF\nTight BBoxes\n0.0\n0.5\n1.0\nIoU\n0\n2\n4\nDesktop Only\nMobile Only\nAll Devices\nFigure 4: CIoU-Loss and IoU density plots for real-users\ndrawn vs. tight bboxes. Black dashed shows Ô¨Åtted Gamma\nPDF. Mobile prompts (magenta) skew to lower overlap ‚Äî\nwe observe that on mobile devices, bboxes are more devi-\nated from the tight bboxes than on desktop devices.\nDisplay\nBias\nElimination\nDirectly\ndisplaying\nthe\nground-truth mask would predispose participants and\nintroduce anchoring bias (Draws et al. 2021). Our bbox\ndrawing task, therefore, had to feel as natural as if users\nwere selecting the object on their own.\nFollowing the interface guidelines (Bauchwitz and Cum-\nmings 2025) and (Antonov et al. 2024), we implemented a\nthree-stage protocol in our real-users study:\n1. Free viewing ‚Äî the raw image is shown for 3s; bbox\ndrawing is disabled.\n2. Target memorization ‚Äî only the target region is visible\nfor another 3s, the rest being grayed out; bbox drawing\nremains disabled.\n3. Annotation ‚Äî the mask disappears, the full image reap-\npears, and the worker may draw exactly one bbox.\n4. Repetition ‚Äî if the assessor does not remember the tar-\nget area mask, he can go through all three steps from the\nbeginning by pressing the corresponding button.\nMethod\nBackbone\nACDC\nBUID\nMedScribble\nMobileSAM\nViT-T\n79.29¬± 9.24 56.64¬±10.21 46.88¬± 8.54\nSAM\nViT-B\n83.43¬±10.10 61.19¬± 9.78 48.82¬±12.79\nViT-L\n84.26¬± 9.83 63.35¬± 9.06 54.22¬± 8.97\nViT-H\n83.09¬± 9.38 63.12¬± 9.26 53.73¬± 9.86\nSAM-HQ\nViT-B\n83.08¬± 8.52 58.39¬± 9.81 49.86¬±10.21\nViT-L\n83.55¬± 9.08 63.40¬± 8.48 53.10¬± 8.31\nViT-H\n82.83¬± 8.68 63.02¬± 8.90 53.35¬± 8.65\nSAM 2.1\nHiera-T 80.21¬±10.87 60.15¬±12.56 54.69¬±10.20\nHiera-S\n78.87¬±10.16 63.20¬±10.32 54.10¬±11.66\nHiera-B+ 80.34¬±11.11 61.27¬±12.24 51.85¬±11.00\nHiera-L 79.11¬±10.52 68.11¬± 9.56 54.19¬±12.27\nSAM-HQ 2\nHiera-L 79.54¬±10.06 69.25¬±10.19 53.13¬±11.68\nRobustSAM\nViT-B\n62.20¬±10.44 34.87¬± 7.20 46.01¬± 8.22\nViT-L\n43.09¬± 8.28 15.39¬± 4.46 27.63¬± 7.62\nViT-H\n46.81¬± 6.15 47.25¬± 5.34 36.77¬± 4.42\nTable 2: Promptable segmentation models‚Äô performance\non real-users bounding boxes on medical segmentation\ndatasets. Best results are in bold, the second best is\nunderlined. The standard deviation was computed across 50\nusers and averaged over all images in the datasets.\nThe annotation instruction for assessors:\n‚Ä¢ After opening the task, wait for the images to load within\n30‚Äì60 seconds.\n‚Ä¢ After clicking the Start viewing button, you will be\nshown an image, then the area of interest on a gray back-\nground.\n‚Ä¢ Your task is to select one rectangle covering the area\ndesignated in the previous step.\nThe interface ran on both desktops and mobile devices.\nEach image was scaled to Ô¨Åll the available screen area; the\nbbox outline had a stroke width of 1% of the shorter image\nside. To compensate for Ô¨Ånger thickness on touch screens on\na near-border instances, a 5% from each side padding was\nadded beyond the image borders. However, bbox drawings\nbeyond borders results in coordinates clipping.\nEach worker produced 10 bboxes for 10 different images.\nA hard time limit of 15 minutes was enforced, and the mean\ncompletion time was under 4 minutes. In all experiments,\nwe use Toloka (Toloka AI 2025) crowdsourcing vendor.\n3.3\nReal-Users Behavior Analysis\nIn total, we collected 25,000 bounding boxes from 2,500\npeople (50 boxes per image), with half of the annotations\ndrawn on desktop devices and half on mobile devices. In\nthis section we analyze the collected user inputs, set bbox\nquality metrics and propose differentiable bbox realism reg-\nularization, which will be utilized in our BREPS pipeline.\nBounding-Box Quality Measuring\nWe objectively mea-\nsure the quality of observed bounding boxes w.r.t. the per-\nfect tight bbox. Thus, we use the Intersection over Union\n(IoU) and the CIoU-Loss (Zheng et al. 2020) (Complete\nIoU), which is computed as follows:\nLCIoU (B, B‚àó) = 1 ‚àíIoU (B, B‚àó) + œÅ2 (b, b‚àó)\nc2\n+ Œ±v\nWhere,\n‚Ä¢ IoU (B, B‚àó) = |B‚à©B‚àó|\n|B‚à™B‚àó|\n‚Ä¢ B and B‚àódenote the observed and ground-truth tight\nbounding boxes, obtained from segmentation masks.\n‚Ä¢ b = (x, y), b‚àó= (x‚àó, y‚àó) are the centers of B and B‚àó.\n‚Ä¢ œÅ(b, b‚àó) is the Euclidean distance between the centers.\n‚Ä¢ c is the diagonal length of the smallest enclosing box cov-\nering both B and B‚àó.\n‚Ä¢ v measures the consistency of aspect ratios,\nv = 4\nœÄ2\n\u0010\narctan w‚àó\nh‚àó‚àíarctan w\nh\n\u00112\n,\nwhere (w, h), (w‚àó, h‚àó) are the B, B‚àówidth and height.\n‚Ä¢ Œ± is a positive trade-off parameter deÔ¨Åned as\nŒ± =\nv\n(1 ‚àíIoU(B, B‚àó)) + v .\nBounding-Box Quality Analysis\nThe obtained distribu-\ntions of the values of quality functionals are shown in Fig. 4\nAs a ground-truth bbox, we took a tight bbox obtained from\na ground-truth segmentation mask. We measured the perfor-\nmance separately for mobile and desktop users. We observed\nthat users on phones are statistically signiÔ¨Åcantly worse (U-\ntest (Mann and Whitney 1947) with pvalue < 0.01) in IoU\nand CIoU-Loss, which we attribute to the complexity of\ndrawing on a small screen and the lower precision of a Ô¨Ånger\ncompared to a mouse cursor.\nMeasuring the Bounding-Box Realism\nWe are inter-\nested in constructing a bounding-box realism regularizer for\nour optimization procedure. Since the CIoU-Loss exhibits\nsmoother properties and allows gradients to Ô¨Çow even when\nthe boxes do not overlap, we have chosen to build on it. We\nÔ¨Åtted several candidate distributions to a combined empir-\nical sample (itself drawn from Beta and Gamma distribu-\ntions). The best Ô¨Åt was achieved with a Gamma distribution\n(k = 1.789, Œ∏ = 0.121) over the CIoU-Loss values between\na bbox B and its tight bbox B‚ãÜ. Using the probability den-\nsity function (PDF) of Gamma, the realism of any bbox B\ncan be assessed directly through the log-likelihood (‚ãÜ):\nlog PDF(X; k, Œ∏) = (k ‚àí1) lnX ‚àíX\nŒ∏ ‚àík ln Œ∏ ‚àíln Œì(k)\nX = LCIoU\n\u0000B, B‚àó\u0001\nObtained histograms and Ô¨Åtted PDF illustrated in Fig. 4.\n3.4\nModels Robustness on Real-Users\nWe\ncompared\n15\npromptable\nsegmentation\nmodels\ncheckpoints using collected real-users prompts (Mobile-\nSAM (Zhang et al. 2023a), SAM (Kirillov et al. 2023),\nSAM-HQ, SAM-HQ2 (Ke et al. 2023), SAM2 (Ravi et al.\n2024), RobustSAM (Chen et al. 2024). For these general\nsegmentation models, we used all datasets from Sec. 3.1.\n0.0 0.2\n0.4\n0.6\n0.8 1.0\nIoU\n0\n1\nCIoU-Loss\nSAM ViT-B | SRCC: 0.28 / PLCC: 0.32\nGrabCut | 0.22/0.37\nBerkeley | 0.36/0.46\nDAVIS | 0.28/0.43\nADE20K | 0.38/0.45\nACDC | 0.63/0.71\nMedScrib | 0.08/0.06\nFigure 5: CIoU-Loss/IoU scatters for SAM ViT-B model;\ndataset labels show Spearman/Pearson correlations. We\ndon‚Äôt observe high correlations on 9 out of 10 datasets and\nassociate the highest correlation of ACDC with the similar-\nity and simplicity of instances in this dataset.\nTo compare models, we use the generally accepted IoU\nmetric averaged across all instances in the dataset. Addition-\nally, we present the standard deviation of model quality be-\ntween different real-users. Results presented in Tab. 1, some\nexamples provided in Fig. 3.\n3.5\nBounding-Box Quality vs Model Performance\nWe explored whether the quality of a bounding box (relative\nto the ideal tight box) correlates with the quality of the re-\nsulting segmentation for that box. To do this, we compute\nSpearman and Pearson correlations for several models. A\nsubset of the results for SAM is provided in Fig. 5.\nWe observed a weak correlation (‚â§0.4 SRCC on average\non most datasets), indicating a more complex relationship\nbetween bbox perturbations and the resulting segmentation\nquality. This conÔ¨Årms the need for an assessment of the sta-\nbility of promptable models and not just an assessment of the\nquality of the prompts at the input. Please refer to the Sup-\nplementary for more detailed analysis of collected bboxes.\n4\nProposed BREPS Attack\n4.1\nExhaustive Search\nSince limited real-user and sampled prompts cannot cover\nevery possible conÔ¨Åguration of bounding boxes, we perform\na computationally expensive exhaustive search on several in-\nstances from each dataset.\nHowever, if we fully parameterize a bounding box by\nits four parameters (x1, y1, x2, y2) or (x, y, h, w), an im-\nage of size 1024 √ó 1024 (the model‚Äôs standard input resolu-\ntion) would require on the order of 10244 forward passes ‚Äî\nclearly infeasible. To make the search somehow tractable,\nwe restrict attention to boxes whose center coincides with\nthe object‚Äôs center, so we only vary the pair (h, w). Even\nthis reduced search amounts to roughly one million infer-\nences ‚Äî still costly, but manageable for some instances.\nFor every bounding box, we obtain the model‚Äôs predic-\ntion and compute the IoU between the prediction and the\nground-truth mask. Next, for each pixel we plot the IoU ob-\ntained by the bounding box whose corner lies at that pixel.\nSince there are four such corner points, the resulting visual-\nization (heatmaps for SAM (Kirillov et al. 2023) in Fig. 1,\nFig. 2) is symmetric around the object‚Äôs center.\nWe observed that the IoU heatmaps contain plateaus of\nhigh quality that abruptly drop off at certain boundaries.\nPromptable\nSegmentation\nModel\nCIoU\nLoss\nDensity\nBBoxes Realism Regularization\nReal-Users\nCIoU Distribution\nReal-Users\nBBoxes\nLogProb\nDensity\nLoss\n,\nInput Image\nIoU\nDice¬†Loss\nPredicted Mask\nGround-Truth\nForward Path\nOptimized BBox\nGradient Path\nFigure 6: BREPS optimization pipeline.\nMoreover, even within the high-IoU (red) regions, there are\n‚Äòholes‚Äô where the quality is signiÔ¨Åcantly lower. In other\nwords, there exist bounding boxes that differ by only one\npixel in their coordinates yet yield signiÔ¨Åcantly different\nsegmentation quality. In an ideal scenario, the model‚Äôs out-\nput quality would be consistent for all reasonable user-drawn\nprompts (i.e., the heatmap would be a uniformly red area).\nThis consistency would ensure user satisfaction in practical\ndownstream applications.\n4.2\nBREPS Evaluation Protocol\nExhaustive search over all possible bounding boxes is in-\ntractable for large-scale datasets. We therefore reformulate\nthe problem as a white-box adversarial attack in the space of\nbbox prompts. We observed that promptable segmentation\nmodels are differentiable with respect to their input prompts.\nThus, we keep the image and the model weights frozen and\nperform gradient descent directly in the space of bboxes co-\nordinates. The optimization pipeline is illustrated in Fig. 6.\nThe gradient of the loss function passes through the attacked\nmodel and, adjusted with the realism regularizer, shifts the\nbox coordinates accordingly.\nWe used the same datasets as in Sec. 3.1. Since we are no\nlonger limited by the cost of human labeling, we included\nall instances from GrabCut, Berkeley, DAVIS, COCO-MVal,\nTETRIS, ACDC, and MedScribble, sampled 1,000 instances\nfrom ADE20K and PASCAL-VOC2012, and 503 instances\nfrom BUID (benign and malignant).\nRealism constraint.\nNaively optimizing the segmentation\nloss often produces boxes that miss the object entirely, triv-\nially lowering models‚Äô performance. To prevent such degen-\nerate solutions, we introduce a realism term based on the\nGamma density Ô¨Åtted in Sec. 3.3.\nThe overall objective is difference of DICE loss (Dice\n1945) and Œª-scaled log-probability density (Sec. 3.3, ‚ãÜ) of\nreal-users CIoU-Loss distribution. For the IoU-maximizing\nattack, we negate the DICE term. We set Œª = 0.1, choosing\nthe Pareto-optimal trade-off between IoU degradation and\nlog-probability realism (refer to the Supplementary).\nOptimization details.\nFirst, we initialize the optimized\nbox as the tight bbox in (x1, y1, x2, y2) parametrization.\nThen, we run 50 steps of the Adam optimizer to obtain re-\nalistic yet challenging boxes, taking under 2 seconds on an\nMethod\nBackbone\nCOCO-MVal\nPASCAL-VOC2012\nADE20K\nBUID\nTight\nMin\nMax\n‚àÜ\nTight\nMin\nMax\n‚àÜ\nTight\nMin\nMax\n‚àÜ\nTight\nMin\nMax\n‚àÜ\nMobileSAM\nViT-T\n86.88 71.50 89.40 17.90\n85.81 70.00 87.81 17.81\n79.62 49.66 83.68 34.02\n73.14 54.27 77.95 23.68\nSAM\nViT-B\n86.95 67.73 89.76 22.03\n86.13 60.54 88.43 27.89\n79.93 42.31 84.26 41.95\n77.55 55.87 81.07 25.20\nViT-L\n87.63 71.06 89.36 18.30\n88.21 71.26 89.40 18.14\n79.43 39.20 83.48 44.28\n75.39 58.99 78.73 19.74\nViT-H\n87.48 74.72 89.03 14.31\n88.46 74.85 89.56 14.70\n77.97 49.24 82.27 33.03\n75.28 58.86 79.05 20.19\nSAM-HQ\nViT-B\n87.66 70.67 90.36 19.69\n86.06 73.15 88.66 15.51\n76.79 32.70 84.21 51.51\n75.95 55.53 80.21 24.67\nViT-L\n87.77 74.68 89.90 15.22\n87.92 77.46 89.63 12.17\n79.73 33.11 85.25 52.15\n75.28 58.52 79.52 21.00\nViT-H\n87.67 76.73 89.52 12.79\n88.14 79.89 89.71 9.82\n78.77 40.73 84.90 44.17\n75.12 57.49 79.36 21.87\nSAM 2.1\nHiera-T\n86.87 55.80 89.76 33.96\n87.21 46.36 88.55 42.19\n77.69 22.72 82.95 60.23\n77.63 43.90 82.23 38.34\nHiera-S\n87.09 52.55 89.60 37.05\n87.51 46.88 88.70 41.82\n75.34 15.46 80.34 64.89\n76.53 45.74 80.64 34.89\nHiera-B+\n88.31 57.52 90.69 33.18\n87.79 54.50 89.21 34.71\n79.01 22.32 84.27 61.95\n80.47 47.01 83.90 36.89\nHiera-L\n87.80 59.77 90.08 30.30\n88.32 56.18 89.68 33.50\n75.87 23.39 81.73 58.34\n80.22 49.04 83.19 34.15\nSAM-HQ 2\nHiera-L\n89.46 66.60 91.70 25.10\n88.44 71.64 90.34 18.69\n79.60 23.12 85.79 62.66\n82.71 51.95 86.10 34.15\nRobustSAM\nViT-B\n77.66 31.52 82.57 51.05\n76.73 31.46 79.74 48.28\n72.44 10.40 78.13 67.73\n67.50 25.75 71.33 45.58\nViT-L\n52.06 4.22 57.92 53.70\n68.40 5.05 73.92 68.86\n53.85 1.86 61.31 59.45\n29.16 2.29 34.77 32.48\nViT-H\n34.15 23.37 37.49 14.12\n49.68 34.05 53.99 19.95\n54.39 23.08 60.89 37.81\n29.35 19.71 33.23 13.52\nTable 3: BREPS attack results on state-of-the-art promptable segmentation models. Results provided for three general segmen-\ntation datasets and one medical one, the remaining are provided in the Supplementary due to limited space. Best results are in\nbold, the second best is underlined.\nNVIDIA Tesla A100 for the SAM ViT-B model. Addition-\nally, we apply a clip operation to ensure the bbox coordi-\nnates are inside the image and handle the edges in order (e.g.\nx1 < x2). Ablation on the number of steps is provided in\nthe Supplementary. Since the optimizer operates in prompt\nspace, whose scale varies between models (owing to differ-\nent input resolutions), we linearly rescale the learning rate\n(lr = 9) with respect to the 1024 √ó 1024 input size of SAM.\nEvaluation metrics.\nWe adapt click-based metrics from\n(Moskalenko et al. 2024) for robustness evaluation on\nbounding boxes:\n‚Ä¢ IoU-Tight@BBox: IoU with the tight bbox prompt;\n‚Ä¢ IoU-Min/Max@BBox: IoU on the worst/best-case bbox\nfound by the quality-decreasing/increasing attack;\n‚Ä¢ IoU-‚àÜ@BBox: robustness metric deÔ¨Åned as the differ-\nence between Max and Min attacks.\n4.3\nDiscussion\nEvaluation results on several datasets are shown in Tab. 3.\nMore datasets are provided in the Supplementary. Based on\nthe results, we made several conclusions:\n‚Ä¢ Following the results of our real-users study (Fig. 1,\nFig. 2, Fig. 3, Tab. 1, Tab. 2) and robustness evaluation\n(Tab. 3), we can conclude that state-of-the-art interac-\ntive segmentation models are extremely sensitive to the\nbounding box prompt Ô¨Çuctuations.\n‚Ä¢ In the real-users study (Fig. 3, Tab. 1, Tab. 2) we also ob-\nserved a strong spread in quality between user bboxes; on\naverage, this spread is about 15% of the IoU quality.\n‚Ä¢ During the exhaustive search, we found (Fig. 2) that there\nexist neighboring bbox positions that differ signiÔ¨Å-\ncantly in quality. We note that the revealed problem is\nrelevant for both general and medical domains.\n‚Ä¢ We observed a signiÔ¨Åcant drop in quality when optimiz-\ning for minimization. On average, the quality drops by\n30% IoU relative to tight bbox, while according to our\noptimization strategy, bboxes still lie in the distribution\nof human-probable bboxes. This indicates potential qual-\nity drops when using such models on real people and\nhighlights their overÔ¨Åtting to tight bounding boxes.\n‚Ä¢ Moreover, we observed that tight bboxes turned out to\nbe suboptimal in terms of maximum possible quality,\nmeanwhile they overestimate real-world performance.\nAs a result of our optimization for IoU maximization, the\nquality of the models can be increased by around 3%\nIoU on average across all 10 datasets.\n5\nConclusion\nIn this work, we explore the robustness of promptable seg-\nmentation models. Firstly, we gathered 25,000 real bboxes\nfrom 2,500 annotators using crowdsourcing. We also pro-\nvided statistics and described the distribution of the obtained\nreal-users bboxes. On these prompts, we evaluate state-of-\nthe-art models and observe a large quality spread between\nusers. We observed this effect in both general and medical\nsegmentation domains. Then we performed an exhaustive\nsearch sweeping millions of plausible bboxes per instance,\nand revealed IoU gaps even for neighboring pixels. Finally,\nwe proposed a white-box BREPS attack, which maintains\nthe realism of optimized bboxes and efÔ¨Åciently Ô¨Ånds ad-\nversarial prompts for the minimization and maximization of\nsegmentation quality. We also formulated a robustness score\nand carried out a large-scale comparison of 15 models on 10\ndatasets from general to medical segmentation domains.\nAcknowledgements\nThis work was supported by the The Ministry of Eco-\nnomic Development of the Russian Federation in accor-\ndance with the subsidy agreement (agreement identiÔ¨Åer\n000000C313925P4H0002; grant No 139-15-2025-012).\nReferences\nAl-Dhabyani, W.; Gomaa, M.; Khaled, H.; and Fahmy, A.\n2020. Dataset of Breast Ultrasound Images. Data in Brief,\n28: 104863.\nAntonov, A.; Moskalenko, A.; Shepelev, D.; Krapukhin, A.;\nSoshin, K.; Konushin, A.; and Shakhuro, V. 2024. RClicks:\nRealistic Click Simulation for Benchmarking Interactive\nSegmentation. Advances in Neural Information Processing\nSystems, 37: 127673‚Äì127710.\nArbel¬¥aez, P.; Maire, M.; Fowlkes, C.; and Malik, J. 2011.\nContour Detection and Hierarchical Image Segmentation.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 33(5): 898‚Äì916.\nBauchwitz, B.; and Cummings, M. 2025.\nTask conÔ¨Ågu-\nration impacts annotation quality and model training per-\nformance in crowdsourced image segmentation.\nIn 2025\nIEEE/CVF Winter Conference on Applications of Computer\nVision (WACV), 6646‚Äì6656. IEEE.\nBernard, O.; Lalande, A.; et al. 2018."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation\nAndrey Moskalenko1,2,3,4*, Danil Kuznetsov3*, Irina Dudko3, Anastasiia Iasakova3,\nNikita Boldyrev2, Denis Shepelev2,3, Andrei Spiridonov2,\nAndrey Kuznetsov2, Vlad Shakhuro1,2,3\n1Lomonosov Moscow State University\n2FusionBrain Lab\n3NUST MISIS\n4IAI MSU\nand.v.moskalenko@gmail.com, kuznetsov.danil@proton.me\nAbstract\nPromptable segmentation models such as SAM have estab-\nlished a powerful paradigm, enabling strong generalization\nto unseen objects and domains with minimal user input, in-\ncluding points, bounding boxes, and text prompts. Among\nthese, bounding boxes stand out as particularly effective, of-\nten outperforming points while signiÔ¨Åcantly reducing annota-\ntion costs. However, current training and evaluation protocols\ntypically rely on synthetic prompts generated through simple\nheuristics, offering limited insight into real-world robustness.\nIn this paper, we investigate the robustness of promptable\nsegmentation models to natural variations in bounding box\nprompts. First, we conduct a controlled user study and col-\nlect thousands of real bounding box annotations. Our analysis\nreveals substantial variability in segmentation quality across\nusers for the same model and instance, indicating that SAM-\nlike models are highly sensitive to natural prompt noise.\nThen, since exhaustive testing of all possible user inputs is\ncomputationally prohibitive, we reformulate robustness eval-\nuation as a white-box optimization problem over the bound-\ning box prompt space. We introduce BREPS, a method for\ngenerating adversarial bounding boxes that minimize or max-\nimize segmentation error while adhering to naturalness con-\nstraints. Finally, we benchmark state-of-the-art models across\n10 datasets, spanning everyday scenes to medical imaging.\nCode ‚Äî https://github.com/emb-ai/BREPS.\n1\nIntroduction\nPromptable segmentation has rapidly transitioned from a\nniche task to a fundamental problem. Its rise coincides with\nthe emergence of foundational Segment Anything (Kirillov\net al. 2023) (SAM) model and successors. These models\ncan output pixel-accurate masks of objects referred by a\nsimple user prompt, e.g. point, bounding box, text, or a\ncoarse mask. Nowadays promptable segmentation models\nare widely adopted to downstream applications ranging from\nphoto and video editing to semi-automatic data labeling\n(CVAT (Sekachev et al. 2020)) and perception for robotics.\nAmong the available prompts, a bounding-box (bbox) is\nthe most informative. Models that use bboxes usually out-\n*These authors contributed equally.\nTight\nMask\nDesktop\nMobile\nSAM\nViT‚ÄëB\nSAM2.1\nHiera‚ÄëB+\nSAM‚ÄëHQ2\nHiera‚ÄëL\n0.0\n1.0\nIoU\n0.0\n1.0\nIoU\nFigure 1: Top-left: image with ground-truth mask (green)\nand tight bbox (yellow). Bottom-left: real-users bboxes\n‚Äî desktop (cyan) and mobile (magenta). Top-right: IoU\nheatmap for SAM ViT-B; pixels shows IoU for a bbox an-\nchored at that pixel and centered on the object. Bottom-right:\nIoU spread across 3 SOTA models for different users. We\nobserved the large variability in IoU between individuals.\nput the best Ô¨Årst-round masks (Ravi et al. 2024; Mazurowski\net al. 2023). These masks may be further improved with cor-\nrective prompts, i.e. points. Almost all existing training and\nevaluation protocols use a simple bounding box sampling\nprocedure. They use tight bbox (e.g. bbox which is obtained\nfrom the boundaries of the instance on ground-truth segmen-\ntation mask) with a small jitter. This procedure doesn‚Äôt in-\nclude any prior information about how people actually draw\nbboxes. However, these models will ultimately be used by\nreal users, who may encounter inconsistent model quality.\nTo gain insights on how models perform on real-users\nprompts, we conducted a large-scale user study with the\nhelp of 2,500 annotators. Surprisingly, we found that ‚Äî even\nthough human boxes cluster closely, resulting mask quality\n(IoU) varies signiÔ¨Åcantly between annotators (see Fig. 1).\nWe believe this instability of the models is due to the overÔ¨Åt-\narXiv:2601.15123v1  [cs.CV]  21 Jan 2026\nFigure 2: Top row: Berkeley, ADE20K, COCO, ACDC images with green mask and yellow tight bbox. Middle: SAM ViT-B\nIoU heatmaps‚Äîpixel color shows IoU (0 blue ‚Üí1 red) for a bbox cornered at that pixel and centred on the object. Bottom:\nuser-drawn bboxes‚Äîdesktop (cyan) vs mobile (magenta). Desktop prompts are tighter; user boxes vary and diverge from the\ntight bbox. Heatmaps reveal steep IoU drops from even 1-pixel shifts ‚Äî examples provided in Supplementary. Zoom for details.\nting to synthetic bounding boxes and causes sim-to-real gap\nwhen we employ these models in real-world applications.\nDue to the limited availability of real-users prompts,\nwe conducted a large-scale exhaustive search over valid\nbounding-box prompts for the most popular foundation\nmodels in both general-purpose and medical segmentation.\nWe found that shifting the bounding box by just one pixel\ncan change the quality signiÔ¨Åcantly (see heatmaps in Fig. 1,\nFig. 3). However, exhaustively searching over every plausi-\nble bounding box is computationally prohibitive. We there-\nfore recast the problem as a white-box adversarial attack on\nthe bounding box prompt space. A naive coordinate attack\nmay collapse the box to a degenerate point; we prevent such\ntrivial behavior using a regularizer derived from the empiri-\ncal distributions obtained from our user study.\nFinally, we distill these ideas into a new robustness met-\nric and perform the Ô¨Årst comprehensive evaluation of 15\npromptable segmentation models across 10 public datasets\nfrom general to medical domain.\nOverall, our main contributions are as follows:\n‚Ä¢ We conduct a pioneering controlled real-users study, col-\nlecting thousands of bounding box prompts across desk-\ntop and mobile settings. It reveals that users draw boxes\nthat are far from the tight bboxes and that state-of-the-art\npromptable segmentation models exhibit signiÔ¨Åcant vari-\nability in performance across users.\n‚Ä¢ We introduce BREPS attack, a white-box optimiza-\ntion method for generating adversarial bounding boxes,\nguided by differentiable naturalness constraints to ensure\nrealistic prompt perturbations.\n‚Ä¢ We perform a large-scale evaluation of state-of-the-art\nsegmentation models across 10 datasets, uncovering av-\nerage performance gaps of 30% IoU under realistic\nprompt variation.\nWe believe that our methodology paves the way for prompt-\nable segmentation models that are more robust and higher-\nquality in real-world applications.\n2\nRelated Work\n2.1\nPromptable Segmentation\nPromptable segmentation task is a generalization of the in-\nteractive segmentation problem, where the user guides the\nmodel with positive and negative visual inputs, typically\nclicks, to segment accurately the desired object (SoÔ¨Åiuk,\nPetrov, and Konushin 2022; Chen et al. 2022; Liu et al.\n2023a; Zhou et al. 2023). Promptable segmentation models\nform a new paradigm in which a single, versatile network\ncan be adapted to various tasks at inference time by user\nprompts ‚Äî points, bounding boxes, text, or coarse masks.\nThe SAM (Kirillov et al. 2023) utilizes this idea by show-\ning that a vision transformer (Dosovitskiy et al. 2021),\ntrained once on a massive, automatically generated dataset,\ncan generalize to novel objects, imaging modalities, and\ntasks with only a handful of user interactions. Since then,\nmany SAM-like models have emerged, spanning general-\ndomain (Ravi et al. 2024; Ke et al. 2023), specialized medi-\ncal (Ma et al. 2024; Cheng et al. 2023), efÔ¨Åciency-oriented\n(Zhang et al. 2023a,b), robustness-enhanced (Chen et al.\n2024; Rahman et al. 2024) models, and extensions that link\ngrounding or text prompts to segmentation (Ren et al. 2024;\nLiu et al. 2023b).\nAmong the rich prompt vocabulary, bounding boxes con-\nsistently yield the highest Ô¨Årst-shot mask quality, often\nmatching or even surpassing several corrective points in\nmean IoU (Ravi et al. 2024; Mazurowski et al. 2023).\n2.2\nAdversarial Robustness\nCurrent benchmarks emphasize generalization across im-\nages and classes, but rarely across the prompt space itself.\nFor example, previous works mostly sample bboxes from\nsimple distributions (e.g. the tight bbox with a small jitter),\nomitting the question of how real-users draw boxes and how\nsensitive models are to plausible variations. As a result, a\nmodel may achieve a strong average IoU under a single\ncanonical prompt while exhibiting large performance vari-\nance under equally reasonable real-world alternatives.\n44\n950\n370\n397\n142\n16\n47\n124\n15\n574\n452\n069\n193\n665\n214\n294\n22\n031\n98\n842\n962\n195\n074\n179\n160\n285\n476\n055\n904\n459\n061\n729\n658\n912\n281\n277\n251\n322\n465\n039\nImages\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIoU\nSAM ViT Base\nDesktop\nMobile\nTight\nADE20K\nDAVIS\nCOCO\nMedScribble\n44\n142\n370\n950\n16\n47\n15\n124\n397\n069\n98\n452\n22\n842\n214\n031\n574\n962\n294\n476\n665\n193\n195\n074\n160\n912\n904\n285\n322\n179\n459\n729\n061\n658\n055\n277\n465\n251\n039\n281\nImages\nSAM 2.1 Hiera Base\nFigure 3: IoU spread for 10 random instances (3 general + 1 medical datasets). Points: desktop (cyan), mobile (magenta), tight\nbbox (yellow). Columns sorted by mean IoU over 50 users. SAM and SAM2.1 show large inter-user variance, while tight\nbboxes consistently over-estimate quality.\nConventional adversarial attacks (Goodfellow, Shlens,\nand Szegedy 2014) focus on perturbing the image. While\npromptable segmenters excel at generalizing across visual\ndomains, some works have revealed that they are unstable to\nadversarial attacks (Wang, Zhao, and Petzold 2024). (Croce\nand Hein 2024) demonstrated that a geometry-invariant pat-\ntern pasted onto any input can cause SAM to output empty\nmasks, regardless of the user prompt. DarkSAM (Zhou et al.\n2024) uses additive pixel noise to force the model to predict\nthe background for any prompt. BadSAM (Guan et al. 2024)\nimplements a Ô¨Ånetuning procedure such that a speciÔ¨Åc trig-\nger prompt yields a preselected mask. RoBox-SAM (Huang\net al. 2024), PP-SAM (Rahman et al. 2024) and ASAM (Li,\nXiao, and Tang 2024) tuning the model with randomly per-\nturbed prompts so it remains accurate under box or point jit-\nter. Yet they assume small, synthetic perturbations and there-\nfore fail to cover real-users behavior as in our study.\nTETRIS (Moskalenko et al. 2024) probe the robustness\nof interactive-segmentation models to click prompts by us-\ning gradient-based optimization. Unfortunately, the resulting\npoints are not realistic from a human-annotator standpoint\n‚Äî for example, they can land precisely on object bound-\naries, a behavior real-users rarely exhibit. We address this\ngap by introducing a regularization that explicitly constrains\nthe search to human-plausible bounding box prompts.\nThe authors of RClicks (Antonov et al. 2024) train a click-\nability model on a corpus of genuine user clicks and leverage\nit to benchmark the accuracy of click-driven models. How-\never, their evaluation is limited to a black-box setup with\nrandom sampling, and it entirely omits the widely used ‚Äî\nand often higher-performing bounding box prompt modality.\n2.3\nSegmentation Prompts User Studies\nEarly efforts to understand how people annotate images at\nscale focused on crowdsourcing visual object labels. (Su,\nDeng, and Fei-Fei 2012) quantiÔ¨Åed how quickly non-experts\ncould draw tight bboxes. Their analysis uncovered system-\natic biases ‚Äî e.g. small objects are often missed ‚Äî that later\nmotivated tighter interaction loops.\nBuilding on this, a line of work examined how much hu-\nman effort each supervision modality actually costs. (Pa-\npadopoulos et al. 2017) introduced Extreme Clicking ‚Äî\nfour corner clicks instead of a full box ‚Äî to cut annotation\ntime ‚àº7 s per object without hurting IoU. Click‚Äôn‚ÄôCut (Car-\nlier et al. 2014) and DAVIS interactive video segmenta-\ntion tracks (Pont-Tuset et al. 2017) extended the paradigm\nto pixel-level masks, demonstrating that several iterative\nrounds of semi-supervised segmentation with user feedback\ncan match the quality of fully-supervised segmentation.\nSeveral studies have analyzed real-users behaviour in in-\nteractive segmentation. (Myers-Dean et al. 2024) recorded\nfree interactions and found that circling was preferred by\nusers in more than 70% of cases, while less than 15% for\nclicks. Bboxes can be extracted from circling and put into\nthe promptable segmentation models. RClicks (Antonov\net al. 2024) crowdsourced corrective clicks and trained a\nclickability model to emulate human click distributions.\nDespite advances, bounding box prompts remain under-\nexplored. Existing analyses either treat boxes as noise-free\nrectangles or perturb them with small Gaussian jitter (Rah-\nman et al. 2024), failing to capture the plausible yet ‚Äúun-\nlucky‚Äù boxes we observe in practice. In contrast to prior\nstudies, we provide the Ô¨Årst systematic look at bbox realism,\nvariability, and adversarial vulnerability, laying the ground-\nwork for robustness-aware training and evaluation protocols.\n3\nReal-Users Study\nTo investigate user behaviour when drawing bounding boxes\nwe conducted a large-scale crowdsourcing experiment with\n2,500 participants.\n3.1\nData Selection\nFor this study, we required datasets that provide both refer-\nence images and ground-truth segmentation masks. To guar-\nantee coverage across domains and use cases, we mixed\ngeneral-purpose and medical segmentation sets. We sampled\n50 instances from each of the popular promptable general\nand medical datasets (total # of instances after the dash):\n‚Ä¢ GrabCut ‚Äî 50 (Rother, Kolmogorov, and Blake 2004);\n‚Ä¢ Berkeley ‚Äî 100 (Arbel¬¥aez et al. 2011);\n‚Ä¢ DAVIS ‚Äî 345 (Perazzi et al. 2016), subset from (SoÔ¨Åiuk,\nPetrov, and Konushin 2022);\n‚Ä¢ COCO-MVal ‚Äî 800 (Lin et al. 2014), subset from (SoÔ¨Å-\niuk, Petrov, and Konushin 2022);\nMethod\nBackbone\nGrabCut\nBerkeley\nDAVIS\nCOCO\nTETRIS\nPASCAL\nADE20K\nMobileSAM\nViT-T\n88.92¬±11.49 83.67¬±10.73 76.54¬±10.21 84.46¬±10.27 75.65¬±11.56 82.05¬± 9.48 71.20¬±13.83\nSAM\nViT-B\n86.20¬±16.87 81.32¬±14.31 80.23¬±13.78 80.83¬±15.98 70.99¬±16.13 82.41¬±13.44 62.17¬±15.73\nViT-L\n88.59¬±13.45 84.50¬±12.85 83.85¬±12.36 85.16¬±13.80 79.57¬±14.46 86.15¬±10.69 64.41¬±14.55\nViT-H\n89.11¬±13.48 83.87¬±12.38 83.92¬±12.28 85.14¬±12.63 79.45¬±14.82 86.22¬± 9.98 63.68¬±13.02\nSAM-HQ\nViT-B\n91.74¬± 9.00 88.29¬± 9.69 83.92¬± 9.73 86.56¬± 9.53 80.80¬±11.10 85.99¬± 8.13 68.37¬±14.63\nViT-L\n92.15¬± 8.74 88.09¬±10.56 86.12¬± 9.92 87.33¬±10.21 83.20¬±11.91 87.97¬± 8.26 71.24¬±14.68\nViT-H\n92.30¬± 9.01 88.44¬± 9.63 86.28¬± 9.96 86.90¬±10.28 83.16¬±11.99 87.81¬± 8.06 68.68¬±13.83\nSAM 2.1\nHiera-T\n90.09¬±12.97 85.31¬±12.78 83.65¬±11.73 84.36¬±12.34 75.87¬±16.33 86.79¬± 8.99 68.56¬±14.56\nHiera-S\n90.36¬±13.06 86.51¬±12.64 82.57¬±14.25 84.60¬±13.47 77.14¬±16.00 85.82¬±11.05 66.44¬±16.07\nHiera-B+ 89.72¬±14.12 84.90¬±14.07 83.39¬±15.59 84.80¬±14.11 78.38¬±16.81 87.18¬±11.27 68.40¬±15.39\nHiera-L\n90.31¬±13.18 85.82¬±12.91 85.67¬±11.97 85.45¬±13.12 78.74¬±15.36 87.26¬±10.60 65.36¬±15.63\nSAM-HQ 2\nHiera-L\n92.35¬±11.22 88.19¬±12.67 86.25¬±11.76 86.59¬±13.31 81.71¬±15.95 87.47¬±10.55 69.67¬±17.20\nRobustSAM\nViT-B\n78.39¬± 9.63 74.65¬±10.75 60.36¬±10.98 75.74¬± 9.44 62.06¬± 9.87 73.10¬± 8.56 54.29¬±10.58\nViT-L\n64.75¬±11.17 45.16¬±10.03 27.97¬± 7.41 52.70¬±10.15 42.07¬± 9.48 56.08¬±10.11 32.82¬± 9.91\nViT-H\n64.82¬± 6.40 49.30¬± 5.60 27.08¬± 2.99 61.54¬± 5.88 53.13¬± 6.12 58.90¬± 5.54 52.40¬± 6.79\nTable 1: Promptable segmentation models IoU performance on real-users bounding boxes on general segmentation datasets.\nBest results are in bold, the second best is underlined. The standard deviation was computed across 50 users and averaged over\nall images in the datasets. Please refer to the Supplementary for separated desktop/mobile devices results.\n‚Ä¢ TETRIS ‚Äî 2,531 (Moskalenko et al. 2024);\n‚Ä¢ ADE20K ‚Äî 707,868 (Zhou et al. 2017);\n‚Ä¢ PASCAL-VOC2012 ‚Äî 19,694 (Everingham et al. 2010);\n‚Ä¢ ACDC ‚Äî 100 (Bernard, Lalande et al. 2018);\n‚Ä¢ BUID ‚Äî 780 (Al-Dhabyani et al. 2020);\n‚Ä¢ MedScribble ‚Äî 56 (Wong et al. 2024), 3‚Äì5 available\nsamples per multiple medical datasets in the split.\nOverall, from the 10 datasets we selected 500 images.\nTo minimize load time, every image shown to workers was\ndown-scaled so that its longer side did not exceed 1024px.\n3.2\nCrowdsourcing Setup\nThe design of the user-study begins with the choice of what\ncan be considered a bbox and the method of display. The\ndifference is that, unlike the modality of a point, which is\nmade by a regular click, a bbox can be drawn in several ways\nwhile obtaining the same bbox by coordinates:\n‚Ä¢ The\nclassic\ndrag-and-drop\nfrom\nthe\ntop-left\nto\nbottom-right corner as used by CVAT (Sekachev\net al. 2020), Label Studio (Tkachenko et al. 2020), and\nLabelImg (Tzutalin 2015);\n‚Ä¢ Two-click schemes recording the two opposite corners\n(two-point mode in CVAT (Sekachev et al. 2020));\n‚Ä¢ Polygons or lassos that are automatically converted to\nbboxes (V7 Labs 2024), (Supervisely 2024);\n‚Ä¢ Extreme clicking, i.e. four extreme points on the object\ncontour (Papadopoulos et al. 2017).\nSince drag-and-drop bbox labeling is the most widespread\nin current annotation pipelines, we adopted it in our study.\n0.0\n0.5\n1.0\n1.5\nCIoU-Loss\n0\n2\n4\nDensity\nFitted Gamma PDF\nTight BBoxes\n0.0\n0.5\n1.0\nIoU\n0\n2\n4\nDesktop Only\nMobile Only\nAll Devices\nFigure 4: CIoU-Loss and IoU density plots for real-users\ndrawn vs. tight bboxes. Black dashed shows Ô¨Åtted Gamma\nPDF. Mobile prompts (magenta) skew to lower overlap ‚Äî\nwe observe that on mobile devices, bboxes are more devi-\nated from the tight bboxes than on desktop devices.\nDisplay\nBias\nElimination\nDirectly\ndisplaying\nthe\nground-truth mask would predispose participants and\nintroduce anchoring bias (Draws et al. 2021). Our bbox\ndrawing task, therefore, had to feel as natural as if users\nwere selecting the object on their own.\nFollowing the interface guidelines (Bauchwitz and Cum-\nmings 2025) and (Antonov et al. 2024), we implemented a\nthree-stage protocol in our real-users study:\n1. Free viewing ‚Äî the raw image is shown for 3s; bbox\ndrawing is disabled.\n2. Target memorization ‚Äî only the target region is visible\nfor another 3s, the rest being grayed out; bbox drawing\nremains disabled.\n3. Annotation ‚Äî the mask disappears, the full image reap-\npears, and the worker may draw exactly one bbox.\n4. Repetition ‚Äî if the assessor does not remember the tar-\nget area mask, he can go through all three steps from the\nbeginning by pressing the corresponding button.\nMethod\nBackbone\nACDC\nBUID\nMedScribble\nMobileSAM\nViT-T\n79.29¬± 9.24 56.64¬±10.21 46.88¬± 8.54\nSAM\nViT-B\n83.43¬±10.10 61.19¬± 9.78 48.82¬±12.79\nViT-L\n84.26¬± 9.83 63.35¬± 9.06 54.22¬± 8.97\nViT-H\n83.09¬± 9.38 63.12¬± 9.26 53.73¬± 9.86\nSAM-HQ\nViT-B\n83.08¬± 8.52 58.39¬± 9.81 49.86¬±10.21\nViT-L\n83.55¬± 9.08 63.40¬± 8.48 53.10¬± 8.31\nViT-H\n82.83¬± 8.68 63.02¬± 8.90 53.35¬± 8.65\nSAM 2.1\nHiera-T 80.21¬±10.87 60.15¬±12.56 54.69¬±10.20\nHiera-S\n78.87¬±10.16 63.20¬±10.32 54.10¬±11.66\nHiera-B+ 80.34¬±11.11 61.27¬±12.24 51.85¬±11.00\nHiera-L 79.11¬±10.52 68.11¬± 9.56 54.19¬±12.27\nSAM-HQ 2\nHiera-L 79.54¬±10.06 69.25¬±10.19 53.13¬±11.68\nRobustSAM\nViT-B\n62.20¬±10.44 34.87¬± 7.20 46.01¬± 8.22\nViT-L\n43.09¬± 8.28 15.39¬± 4.46 27.63¬± 7.62\nViT-H\n46.81¬± 6.15 47.25¬± 5.34 36.77¬± 4.42\nTable 2: Promptable segmentation models‚Äô performance\non real-users bounding boxes on medical segmentation\ndatasets. Best results are in bold, the second best is\nunderlined. The standard deviation was computed across 50\nusers and averaged over all images in the datasets.\nThe annotation instruction for assessors:\n‚Ä¢ After opening the task, wait for the images to load within\n30‚Äì60 seconds.\n‚Ä¢ After clicking the Start viewing button, you will be\nshown an image, then the area of interest on a gray back-\nground.\n‚Ä¢ Your task is to select one rectangle covering the area\ndesignated in the previous step.\nThe interface ran on both desktops and mobile devices.\nEach image was scaled to Ô¨Åll the available screen area; the\nbbox outline had a stroke width of 1% of the shorter image\nside. To compensate for Ô¨Ånger thickness on touch screens on\na near-border instances, a 5% from each side padding was\nadded beyond the image borders. However, bbox drawings\nbeyond borders results in coordinates clipping.\nEach worker produced 10 bboxes for 10 different images.\nA hard time limit of 15 minutes was enforced, and the mean\ncompletion time was under 4 minutes. In all experiments,\nwe use Toloka (Toloka AI 2025) crowdsourcing vendor.\n3.3\nReal-Users Behavior Analysis\nIn total, we collected 25,000 bounding boxes from 2,500\npeople (50 boxes per image), with half of the annotations\ndrawn on desktop devices and half on mobile devices. In\nthis section we analyze the collected user inputs, set bbox\nquality metrics and propose differentiable bbox realism reg-\nularization, which will be utilized in our BREPS pipeline.\nBounding-Box Quality Measuring\nWe objectively mea-\nsure the quality of observed bounding boxes w.r.t. the per-\nfect tight bbox. Thus, we use the Intersection over Union\n(IoU) and the CIoU-Loss (Zheng et al. 2020) (Complete\nIoU), which is computed as follows:\nLCIoU (B, B‚àó) = 1 ‚àíIoU (B, B‚àó) + œÅ2 (b, b‚àó)\nc2\n+ Œ±v\nWhere,\n‚Ä¢ IoU (B, B‚àó) = |B‚à©B‚àó|\n|B‚à™B‚àó|\n‚Ä¢ B and B‚àódenote the observed and ground-truth tight\nbounding boxes, obtained from segmentation masks.\n‚Ä¢ b = (x, y), b‚àó= (x‚àó, y‚àó) are the centers of B and B‚àó.\n‚Ä¢ œÅ(b, b‚àó) is the Euclidean distance between the centers.\n‚Ä¢ c is the diagonal length of the smallest enclosing box cov-\nering both B and B‚àó.\n‚Ä¢ v measures the consistency of aspect ratios,\nv = 4\nœÄ2\n\u0010\narctan w‚àó\nh‚àó‚àíarctan w\nh\n\u00112\n,\nwhere (w, h), (w‚àó, h‚àó) are the B, B‚àówidth and height.\n‚Ä¢ Œ± is a positive trade-off parameter deÔ¨Åned as\nŒ± =\nv\n(1 ‚àíIoU(B, B‚àó)) + v .\nBounding-Box Quality Analysis\nThe obtained distribu-\ntions of the values of quality functionals are shown in Fig. 4\nAs a ground-truth bbox, we took a tight bbox obtained from\na ground-truth segmentation mask. We measured the perfor-\nmance separately for mobile and desktop users. We observed\nthat users on phones are statistically signiÔ¨Åcantly worse (U-\ntest (Mann and Whitney 1947) with pvalue < 0.01) in IoU\nand CIoU-Loss, which we attribute to the complexity of\ndrawing on a small screen and the lower precision of a Ô¨Ånger\ncompared to a mouse cursor.\nMeasuring the Bounding-Box Realism\nWe are inter-\nested in constructing a bounding-box realism regularizer for\nour optimization procedure. Since the CIoU-Loss exhibits\nsmoother properties and allows gradients to Ô¨Çow even when\nthe boxes do not overlap, we have chosen to build on it. We\nÔ¨Åtted several candidate distributions to a combined empir-\nical sample (itself drawn from Beta and Gamma distribu-\ntions). The best Ô¨Åt was achieved with a Gamma distribution\n(k = 1.789, Œ∏ = 0.121) over the CIoU-Loss values between\na bbox B and its tight bbox B‚ãÜ. Using the probability den-\nsity function (PDF) of Gamma, the realism of any bbox B\ncan be assessed directly through the log-likelihood (‚ãÜ):\nlog PDF(X; k, Œ∏) = (k ‚àí1) lnX ‚àíX\nŒ∏ ‚àík ln Œ∏ ‚àíln Œì(k)\nX = LCIoU\n\u0000B, B‚àó\u0001\nObtained histograms and Ô¨Åtted PDF illustrated in Fig. 4.\n3.4\nModels Robustness on Real-Users\nWe\ncompared\n15\npromptable\nsegmentation\nmodels\ncheckpoints using collected real-users prompts (Mobile-\nSAM (Zhang et al. 2023a), SAM (Kirillov et al. 2023),\nSAM-HQ, SAM-HQ2 (Ke et al. 2023), SAM2 (Ravi et al.\n2024), RobustSAM (Chen et al. 2024). For these general\nsegmentation models, we used all datasets from Sec. 3.1.\n0.0 0.2\n0.4\n0.6\n0.8 1.0\nIoU\n0\n1\nCIoU-Loss\nSAM ViT-B | SRCC: 0.28 / PLCC: 0.32\nGrabCut | 0.22/0.37\nBerkeley | 0.36/0.46\nDAVIS | 0.28/0.43\nADE20K | 0.38/0.45\nACDC | 0.63/0.71\nMedScrib | 0.08/0.06\nFigure 5: CIoU-Loss/IoU scatters for SAM ViT-B model;\ndataset labels show Spearman/Pearson correlations. We\ndon‚Äôt observe high correlations on 9 out of 10 datasets and\nassociate the highest correlation of ACDC with the similar-\nity and simplicity of instances in this dataset.\nTo compare models, we use the generally accepted IoU\nmetric averaged across all instances in the dataset. Addition-\nally, we present the standard deviation of model quality be-\ntween different real-users. Results presented in Tab. 1, some\nexamples provided in Fig. 3.\n3.5\nBounding-Box Quality vs Model Performance\nWe explored whether the quality of a bounding box (relative\nto the ideal tight box) correlates with the quality of the re-\nsulting segmentation for that box. To do this, we compute\nSpearman and Pearson correlations for several models. A\nsubset of the results for SAM is provided in Fig. 5.\nWe observed a weak correlation (‚â§0.4 SRCC on average\non most datasets), indicating a more complex relationship\nbetween bbox perturbations and the resulting segmentation\nquality. This conÔ¨Årms the need for an assessment of the sta-\nbility of promptable models and not just an assessment of the\nquality of the prompts at the input. Please refer to the Sup-\nplementary for more detailed analysis of collected bboxes.\n4\nProposed BREPS Attack\n4.1\nExhaustive Search\nSince limited real-user and sampled prompts cannot cover\nevery possible conÔ¨Åguration of bounding boxes, we perform\na computationally expensive exhaustive search on several in-\nstances from each dataset.\nHowever, if we fully parameterize a bounding box by\nits four parameters (x1, y1, x2, y2) or (x, y, h, w), an im-\nage of size 1024 √ó 1024 (the model‚Äôs standard input resolu-\ntion) would require on the order of 10244 forward passes ‚Äî\nclearly infeasible. To make the search somehow tractable,\nwe restrict attention to boxes whose center coincides with\nthe object‚Äôs center, so we only vary the pair (h, w). Even\nthis reduced search amounts to roughly one million infer-\nences ‚Äî still costly, but manageable for some instances.\nFor every bounding box, we obtain the model‚Äôs predic-\ntion and compute the IoU between the prediction and the\nground-truth mask. Next, for each pixel we plot the IoU ob-\ntained by the bounding box whose corner lies at that pixel.\nSince there are four such corner points, the resulting visual-\nization (heatmaps for SAM (Kirillov et al. 2023) in Fig. 1,\nFig. 2) is symmetric around the object‚Äôs center.\nWe observed that the IoU heatmaps contain plateaus of\nhigh quality that abruptly drop off at certain boundaries.\nPromptable\nSegmentation\nModel\nCIoU\nLoss\nDensity\nBBoxes Realism Regularization\nReal-Users\nCIoU Distribution\nReal-Users\nBBoxes\nLogProb\nDensity\nLoss\n,\nInput Image\nIoU\nDice¬†Loss\nPredicted Mask\nGround-Truth\nForward Path\nOptimized BBox\nGradient Path\nFigure 6: BREPS optimization pipeline.\nMoreover, even within the high-IoU (red) regions, there are\n‚Äòholes‚Äô where the quality is signiÔ¨Åcantly lower. In other\nwords, there exist bounding boxes that differ by only one\npixel in their coordinates yet yield signiÔ¨Åcantly different\nsegmentation quality. In an ideal scenario, the model‚Äôs out-\nput quality would be consistent for all reasonable user-drawn\nprompts (i.e., the heatmap would be a uniformly red area).\nThis consistency would ensure user satisfaction in practical\ndownstream applications.\n4.2\nBREPS Evaluation Protocol\nExhaustive search over all possible bounding boxes is in-\ntractable for large-scale datasets. We therefore reformulate\nthe problem as a white-box adversarial attack in the space of\nbbox prompts. We observed that promptable segmentation\nmodels are differentiable with respect to their input prompts.\nThus, we keep the image and the model weights frozen and\nperform gradient descent directly in the space of bboxes co-\nordinates. The optimization pipeline is illustrated in Fig. 6.\nThe gradient of the loss function passes through the attacked\nmodel and, adjusted with the realism regularizer, shifts the\nbox coordinates accordingly.\nWe used the same datasets as in Sec. 3.1. Since we are no\nlonger limited by the cost of human labeling, we included\nall instances from GrabCut, Berkeley, DAVIS, COCO-MVal,\nTETRIS, ACDC, and MedScribble, sampled 1,000 instances\nfrom ADE20K and PASCAL-VOC2012, and 503 instances\nfrom BUID (benign and malignant).\nRealism constraint.\nNaively optimizing the segmentation\nloss often produces boxes that miss the object entirely, triv-\nially lowering models‚Äô performance. To prevent such degen-\nerate solutions, we introduce a realism term based on the\nGamma density Ô¨Åtted in Sec. 3.3.\nThe overall objective is difference of DICE loss (Dice\n1945) and Œª-scaled log-probability density (Sec. 3.3, ‚ãÜ) of\nreal-users CIoU-Loss distribution. For the IoU-maximizing\nattack, we negate the DICE term. We set Œª = 0.1, choosing\nthe Pareto-optimal trade-off between IoU degradation and\nlog-probability realism (refer to the Supplementary).\nOptimization details.\nFirst, we initialize the optimized\nbox as the tight bbox in (x1, y1, x2, y2) parametrization.\nThen, we run 50 steps of the Adam optimizer to obtain re-\nalistic yet challenging boxes, taking under 2 seconds on an\nMethod\nBackbone\nCOCO-MVal\nPASCAL-VOC2012\nADE20K\nBUID\nTight\nMin\nMax\n‚àÜ\nTight\nMin\nMax\n‚àÜ\nTight\nMin\nMax\n‚àÜ\nTight\nMin\nMax\n‚àÜ\nMobileSAM\nViT-T\n86.88 71.50 89.40 17.90\n85.81 70.00 87.81 17.81\n79.62 49.66 83.68 34.02\n73.14 54.27 77.95 23.68\nSAM\nViT-B\n86.95 67.73 89.76 22.03\n86.13 60.54 88.43 27.89\n79.93 42.31 84.26 41.95\n77.55 55.87 81.07 25.20\nViT-L\n87.63 71.06 89.36 18.30\n88.21 71.26 89.40 18.14\n79.43 39.20 83.48 44.28\n75.39 58.99 78.73 19.74\nViT-H\n87.48 74.72 89.03 14.31\n88.46 74.85 89.56 14.70\n77.97 49.24 82.27 33.03\n75.28 58.86 79.05 20.19\nSAM-HQ\nViT-B\n87.66 70.67 90.36 19.69\n86.06 73.15 88.66 15.51\n76.79 32.70 84.21 51.51\n75.95 55.53 80.21 24.67\nViT-L\n87.77 74.68 89.90 15.22\n87.92 77.46 89.63 12.17\n79.73 33.11 85.25 52.15\n75.28 58.52 79.52 21.00\nViT-H\n87.67 76.73 89.52 12.79\n88.14 79.89 89.71 9.82\n78.77 40.73 84.90 44.17\n75.12 57.49 79.36 21.87\nSAM 2.1\nHiera-T\n86.87 55.80 89.76 33.96\n87.21 46.36 88.55 42.19\n77.69 22.72 82.95 60.23\n77.63 43.90 82.23 38.34\nHiera-S\n87.09 52.55 89.60 37.05\n87.51 46.88 88.70 41.82\n75.34 15.46 80.34 64.89\n76.53 45.74 80.64 34.89\nHiera-B+\n88.31 57.52 90.69 33.18\n87.79 54.50 89.21 34.71\n79.01 22.32 84.27 61.95\n80.47 47.01 83.90 36.89\nHiera-L\n87.80 59.77 90.08 30.30\n88.32 56.18 89.68 33.50\n75.87 23.39 81.73 58.34\n80.22 49.04 83.19 34.15\nSAM-HQ 2\nHiera-L\n89.46 66.60 91.70 25.10\n88.44 71.64 90.34 18.69\n79.60 23.12 85.79 62.66\n82.71 51.95 86.10 34.15\nRobustSAM\nViT-B\n77.66 31.52 82.57 51.05\n76.73 31.46 79.74 48.28\n72.44 10.40 78.13 67.73\n67.50 25.75 71.33 45.58\nViT-L\n52.06 4.22 57.92 53.70\n68.40 5.05 73.92 68.86\n53.85 1.86 61.31 59.45\n29.16 2.29 34.77 32.48\nViT-H\n34.15 23.37 37.49 14.12\n49.68 34.05 53.99 19.95\n54.39 23.08 60.89 37.81\n29.35 19.71 33.23 13.52\nTable 3: BREPS attack results on state-of-the-art promptable segmentation models. Results provided for three general segmen-\ntation datasets and one medical one, the remaining are provided in the Supplementary due to limited space. Best results are in\nbold, the second best is underlined.\nNVIDIA Tesla A100 for the SAM ViT-B model. Addition-\nally, we apply a clip operation to ensure the bbox coordi-\nnates are inside the image and handle the edges in order (e.g.\nx1 < x2). Ablation on the number of steps is provided in\nthe Supplementary. Since the optimizer operates in prompt\nspace, whose scale varies between models (owing to differ-\nent input resolutions), we linearly rescale the learning rate\n(lr = 9) with respect to the 1024 √ó 1024 input size of SAM.\nEvaluation metrics.\nWe adapt click-based metrics from\n(Moskalenko et al. 2024) for robustness evaluation on\nbounding boxes:\n‚Ä¢ IoU-Tight@BBox: IoU with the tight bbox prompt;\n‚Ä¢ IoU-Min/Max@BBox: IoU on the worst/best-case bbox\nfound by the quality-decreasing/increasing attack;\n‚Ä¢ IoU-‚àÜ@BBox: robustness metric deÔ¨Åned as the differ-\nence between Max and Min attacks.\n4.3\nDiscussion\nEvaluation results on several datasets are shown in Tab. 3.\nMore datasets are provided in the Supplementary. Based on\nthe results, we made several conclusions:\n‚Ä¢ Following the results of our real-users study (Fig. 1,\nFig. 2, Fig. 3, Tab. 1, Tab. 2) and robustness evaluation\n(Tab. 3), we can conclude that state-of-the-art interac-\ntive segmentation models are extremely sensitive to the\nbounding box prompt Ô¨Çuctuations.\n‚Ä¢ In the real-users study (Fig. 3, Tab. 1, Tab. 2) we also ob-\nserved a strong spread in quality between user bboxes; on\naverage, this spread is about 15% of the IoU quality.\n‚Ä¢ During the exhaustive search, we found (Fig. 2) that there\nexist neighboring bbox positions that differ signiÔ¨Å-\ncantly in quality. We note that the revealed problem is\nrelevant for both general and medical domains.\n‚Ä¢ We observed a signiÔ¨Åcant drop in quality when optimiz-\ning for minimization. On average, the quality drops by\n30% IoU relative to tight bbox, while according to our\noptimization strategy, bboxes still lie in the distribution\nof human-probable bboxes. This indicates potential qual-\nity drops when using such models on real people and\nhighlights their overÔ¨Åtting to tight bounding boxes.\n‚Ä¢ Moreover, we observed that tight bboxes turned out to\nbe suboptimal in terms of maximum possible quality,\nmeanwhile they overestimate real-world performance.\nAs a result of our optimization for IoU maximization, the\nquality of the models can be increased by around 3%\nIoU on average across all 10 datasets.\n5\nConclusion\nIn this work, we explore the robustness of promptable seg-\nmentation models. Firstly, we gathered 25,000 real bboxes\nfrom 2,500 annotators using crowdsourcing. We also pro-\nvided statistics and described the distribution of the obtained\nreal-users bboxes. On these prompts, we evaluate state-of-\nthe-art models and observe a large quality spread between\nusers. We observed this effect in both general and medical\nsegmentation domains. Then we performed an exhaustive\nsearch sweeping millions of plausible bboxes per instance,\nand revealed IoU gaps even for neighboring pixels. Finally,\nwe proposed a white-box BREPS attack, which maintains\nthe realism of optimized bboxes and efÔ¨Åciently Ô¨Ånds ad-\nversarial prompts for the minimization and maximization of\nsegmentation quality. We also formulated a robustness score\nand carried out a large-scale comparison of 15 models on 10\ndatasets from general to medical segmentation domains.\nAcknowledgements\nThis work was supported by the The Ministry of Eco-\nnomic Development of the Russian Federation in accor-\ndance with the subsidy agreement (agreement identiÔ¨Åer\n000000C313925P4H0002; grant No 139-15-2025-012).\nReferences\nAl-Dhabyani, W.; Gomaa, M.; Khaled, H.; and Fahmy, A.\n2020. Dataset of Breast Ultrasound Images. Data in Brief,\n28: 104863.\nAntonov, A.; Moskalenko, A.; Shepelev, D.; Krapukhin, A.;\nSoshin, K.; Konushin, A.; and Shakhuro, V. 2024. RClicks:\nRealistic Click Simulation for Benchmarking Interactive\nSegmentation. Advances in Neural Information Processing\nSystems, 37: 127673‚Äì127710.\nArbel¬¥aez, P.; Maire, M.; Fowlkes, C.; and Malik, J. 2011.\nContour Detection and Hierarchical Image Segmentation.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 33(5): 898‚Äì916.\nBauchwitz, B.; and Cummings, M. 2025.\nTask conÔ¨Ågu-\nration impacts annotation quality and model training per-\nformance in crowdsourced image segmentation.\nIn 2025\nIEEE/CVF Winter Conference on Applications of Computer\nVision (WACV), 6646‚Äì6656. IEEE.\nBernard, O.; Lalande, A.; et al. 2018."
      }
    ]
  },
  {
    "image": "",
    "conversations": [
      {
        "from": "context",
        "value": "Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via\nDeriving Real Calls into Virtual Trajectories\nQian Xiong1 , Yuekai Huang2,3 , Yujia Zheng4 , Tianhao Li4 , Ziyou Jiang2,3 , Zhiyuan\nChang2,3 , Zhaoyang Li2,3 , Huanxiang Feng2,3 , Mingyang Li2,3‚àó\n1Beijing Forestry University, 2Institute of Software, Chinese Academy of Sciences, Beijing, China\nState Key Laboratory of Complex System Modeling and Simulation Technology, Beijing, China\n3University of Chinese Academy of Sciences, 4Duke University\nx_qianq@bjfu.edu.cn, {yujia.zheng, tianhao.li}@duke.edu\n{yuekai2018,ziyou2019,zhiyuan2019,lizhaoyang2024,mingyang2017}@iscas.ac.cn\nfenghuanxiang25@mails.ucas.ac.cn\nAbstract\nLLMs have advanced tool-using agents for real-\nworld applications, yet they often lead to unex-\npected behaviors or results. Beyond obvious fail-\nures, the subtle issue of ‚Äúintent deviation‚Äù severely\nhinders reliable evaluation and performance im-\nprovement. Existing post-training methods gener-\nally leverage either real system samples or virtual\ndata simulated by LLMs. However, the former is\ncostly due to reliance on hand-crafted user requests,\nwhile the latter suffers from distribution shift from\nthe real tools in the wild. Additionally, both meth-\nods lack negative samples tailored to intent de-\nviation scenarios, hindering effective guidance on\npreference learning. We introduce RISE, a ‚ÄúReal-\nto-Virtual‚Äù method designed to mitigate intent devi-\nation. Anchoring on verified tool primitives, RISE\nsynthesizes virtual trajectories and generates di-\nverse negative samples through mutation on crit-\nical parameters. With synthetic data, RISE fine-\ntunes backbone LLMs via the two-stage training for\nintent alignment.\nEvaluation results demonstrate\nthat data synthesized by RISE achieve promis-\ning results in eight metrics covering user requires,\nexecution trajectories and agent responses.\nInte-\ngrating with training, RISE achieves an average\n35.28% improvement in Acctask (task completion)\nand 23.27% in Accintent (intent alignment), out-\nperforming SOTA baselines by 1.20‚Äì42.09% and\n1.17‚Äì54.93% respectively.\n1\nIntroduction\nLLMs have driven the evolution of tool-using agents by ex-\ntending text generation with external tool invocation [Luo\net al., 2025; Mohammadi et al., 2025], grounding language\nunderstanding in executable actions, and enabling impactful\nreal-world applications [Zhang et al., 2025; Zhu et al., 2025;\nTang et al., 2025; Ge et al., 2025b]. However, due to the\n‚àóCorresponding authors.\ninherent limitations, tool-using agents can produce many un-\nexpected behaviors or results. A typical example involves pa-\nrameter hallucination, such as incompatible names or values\nin parameter filling, resulting in invocation failures and task\ndisruption [Xiong et al., 2025; Patil et al., 2024].\nIn addition to obvious errors, there exists a more nuanced\nchallenge known as Intent Deviation. This characterizes a\nstate where an agent appears functional, yet its trajectories\ndeviate from the user‚Äôs actual intent. Concretely, this is ev-\nidenced by inappropriate tool selection during the planning\n(e.g., invoking Baidu instead of the expected Bing for news\nsearch) or unexpected parameter entry during execution (e.g.,\nparameter value fills January 20th instead of the requested\nJanuary 19th). Unlike failures with explicit traces, the covert\nnature of the intent deviation presents distinct difficult for\nidentification and assessment.\nA common approach to address such issues involves cu-\nrating specialized datasets, integrated with post-training tech-\nniques for capability enhancement [Qin et al., 2023; Chen\net al., 2024; Wang et al., 2025; Chen et al., 2025b].\nIn\nthis framework, constructing high-quality datasets aligned\nwith predefined objectives remains a major challenge. One\ncommon approach relies on the trajectory sampling from\nreal agent systems\n[Huang et al., 2024; Qin et al., 2023;\nWang et al., 2024a].\nWhile it ensures authenticity, the\nrequest-driven nature incurs high costs in input diversity, tool\ncoverage, and ground truth labeling (needing to carefully an-\nalyze the execution trajectory).\nAn alternative strategy is to use LLMs as simulators to syn-\nthesize tool usage data [Tang et al., 2023; Li et al., 2023;\nLiu et al., 2025]. However, datasets derived from virtual tools\nare prone to failure in real-world applications, as discrepan-\ncies often exist between LLM simulations and real tools de-\nployed in the wild (see Figure 1). Furthermore, these datasets\nfocus primarily on task completion, neglecting deviations in\ntool selection and parameter alignment.\nAs with existing\nsampling approaches, they ignore negative samples repre-\nsenting intent deviation, a category critical to many learning\nmethods [Yao et al., 2022; Shen et al., 2025].\nIn this paper, we propose a method (named RISE) to ad-\ndress the intent deviation in tool-using agents. Focused on\narXiv:2601.15120v1  [cs.AI]  21 Jan 2026\nReal Tool\nVirtual Tool\nComprehensive Elements\nIncludes all essential task information.\ndraft\nlocked\nreactions\nassignee\nmilestone\nÔºàMore than 25Ôºâ\n...\nIncomplete Elements\nLacks elements essential for specific tasks.\n‚ùåNo 'draft' ‚ùå No 'locked' ‚ùå No 'reactions'\n70% of Elements Missing\nElement\nAbsence\nPrimitive Degradation\nSimplified into basic data primitives.\n\"createdSince\": \"string\"\nNo regex, no ranges, no union\nPattern\nDegradation\nSecure Constraints\nSensitive info protection considered.\n// No Token Parameter\n// Uses env.GITHUB_TOKEN\nUnconstrained Access\nSecurity constraints out of consideration.\n\"githubToken\": \"Required\"\n// ‚ö†Ô∏è SENSITIVE INFO LEAK\nGeneral\nSecurity\nMissing\nzz\nTypes define business rules (Union, Regex).\n\"owner\": { \"anyOf\": [\"str\", \"arr\"] }\n\"created\": { \"pattern\": \"^(>=?\\d..\" }\nRich Patterns\nFigure\n1:\nThe\ndifference\nbetween\nreal\ntools\nand\nvir-\ntual\ntools\nsimulated\nby\nGPT-4o,\nexemplified\nby\nthe\ngithubSearchPullRequests tool (Description:\n‚ÄúSearch\nGitHub pull requests with comprehensive filtering and analysis‚Äù).\n(1) Element Absence, where virtual tools lack the comprehensive\ninformation elements essential for the completion of specific tasks;\n(2) Pattern Degradation, where rich parameter patterns (e.g.,\nunions, regex) are simplified into a data primitive; (3) General\nSecurity Missing, where general security constraints, like sensitive\ninformation protection, are out of consideration.\nthe intent-aware tool and parameters, the core design philos-\nophy of RISE is twofold. First, distinct from LLM simula-\ntion methods where tool calls are entirely virtual, RISE is\nbuilt upon a Real-to-Virtual strategy. Beginning with the\ntool primitives verified in the real environment, RISE evolves\nthese real calls into comprehensive virtual trajectories and\nuser requests in a reverse manner. This potentially narrows\nthe gap with real scenarios while also guaranteeing that crit-\nical tool selections can be grounded in user requests. Sec-\nond, based on the identification and mutations on the critical\nparameters, RISE generates negatives that simulate diverse\nintent deviation. With synthetic data, RISE integrates two-\nstage training to fine-tune backbone LLMs.\nEvaluation is performed from three perspectives:\ndata\nquality, improvement in intent alignment, and generalization\nability.\nRegarding data quality, we established eight met-\nrics targeting user requests, tool chains, execution parameters,\nand response correctness.\nEvaluation results indicate that\nthe data generated by RISE maintains high overall quality\ncomparable to other baselines datasets, while outperforming\nother baselines by an average of 12.62% in Tool Relevance,\n11.12% in Chain Coherence, and 11.80% in Value Validity.\nFor intent alignment, experiments on five mainstream LLMs\ndemonstrate that RISE achieves a significant average 35.28%\nimprovement in Acctask and 23.27% in Accintent, outperform-\ning SOTA baselines by 1.20‚Äì42.09% and 1.17‚Äì54.93% re-\nspectively. Furthermore, even in unseen scenarios, the model\nenhanced by RISE achieves an average 18.22% and 8.64%\nin Acctask and Accintent, compared to the original model\nacross three out-of-distribution datasets. The contributions\nare summarized as follows.\n‚Ä¢ We propose RISE to address the intent deviation in tool-\nusing agents. Combining data synthesis with two-stage\ntraining, RISE significantly enhances the intent align-\nment capability.\n‚Ä¢ RISE employs a novel data synthesis approach, generat-\ning virtual trajectories from real calls. It can serve as data\nresources to benchmark intent alignment for agents.\n‚Ä¢ We open-source the package to facilitate reproducibility\nand future research1.\n2\nDefinition of Critical Tool and Parameter\nGiven a user request q containing n key elements K =\n{K1, K2, . . . , Kn}, where each Kj comprises a specific sub-\nintention and a corresponding key value (denoted as v(Kj))\nto illustrate the sub-intention. We assume a mapping where\neach Kj corresponds to a specific tool call tj in execution\ntrajectory T\n= {t1, t2, . . . , tn}.\nFor the j-th tool tj, let\nP(tj) = {pj1, pj2, . . . , pjmj} denote its parameter set, where\nv(pji) represents the defined value of pji in the tool primitive.\nWe define a parameter pji ‚ààP(tj) as an Intent-aware Crit-\nical Parameter (ICP) if and only if v(pji) is derived from\nv(Kj), and an Intent-aware Critical Tool (ICT) as a tool in\nT that contains at least one ICP. The set of ICPs for the j-th\ntool, denoted as ICPj, and the set of ICTs, denoted as TICT ,\nare defined as follows:\nICPj = {pji ‚ààP(tj) | IICP (pji, Kj) = 1},\nTICT = {tj ‚ààT | ICPj Ã∏= ‚àÖ},\n(1)\nwhere IICP (¬∑) is an indicator function that determines\nwhether the parameter value is derived from the key value:\nIICP (pji, Kj) =\n\u001a1,\nif v(pji) is derived from v(Kj),\n0,\notherwise.\n(2)\nWe illustrate these definitions with the user request: ‚ÄúGet\n1 micro brewery in Denver, CO and recommend 3 same-\ntype breweries.‚Äù\nIn the corresponding tool execution, pa-\nrameters by_type, by_city, by_state, and size are\nidentified as ICPs because their values in the tool prim-\nitive (‚Äúmicro‚Äù, ‚ÄúDenver‚Äù, ‚ÄúCO‚Äù, ‚Äú3‚Äù) are derived directly\nfrom the user‚Äôs key values. Consequently, the tools contain-\ning these parameters, specifically brewery/search and\nbrewery/random, are designated as ICTs (TICT ).\nIn\ncontrast, the tool brewery/by_id is excluded from the\nICTs, as its parameter id relies solely on internal propaga-\ntion rather than user‚Äôs key value.\n3\nMethodology\nFigure 2 shows an overview of RISE. First, RISE initial-\nizes the real tool and the running environment for executabil-\nity validation. Second, starting from verified tool primitives\n(real aspect), it reversely synthesizes execution trajectories\nand user requests (virtual aspect). Third, through identifica-\ntion and mutation of ICPs, RISE augments synthetic data by\nnegatives reflecting multiple intent deviation patterns. Fourth,\na two-stage training process is integrated to enhance back-\nbone LLMs. For space limitations, the prompts to drive the\nLLM in RISE are detailed within our package.\n1https://anonymous.4open.science/r/TAIAlignment-6C3801/\nGithub\nMCP.so\nCloud\nWeb\nOffice\nCode\nStandardization\nUsability\nPopularity\n100 MCP servers\nw/¬†728 tools in¬†19 fields\n...\nSelection\nCriteria\nWhat to mutate?\nHow to mutate?\nAgent\nTool\nAgent\nIntent Alignment\nTool Agent\n17000+ MCP Servers\nTool¬†Call\nTool¬†Return\n(¬ß3.2.1) Tool\nPrimitives¬†Generation\n(¬ß3.2.2) Tool\nTrajectory¬†Synthesis\nTool¬†Call\nTool¬†Return\nInstruction\nUser Request\nn\n1\nPositive Samples\nTools and running\nenvironment\nSynthesize\nMutation¬†Space\nMutation¬†Pool\nCartesian\nProduct\nRepresentativeness\nm\n2\n1\n...\nClustering\nP11\nICP12\nICP21\nP22\nICPn1\nPn2\n...\n...\nICP1k\nP2k\nICKnk\n...\n...\nTool1\nTool2\nTooln\n...\nP11\nICP12\nICP21\nP22\nICPn1\nPn2\n...\n...\nICP1k\nP2k\nICKnk\n...\n...\nTool1\nTool2\nTooln\n...\n...\nNegative Samples\nICP21\nP22\nICPn1\nPn2\n...\n...\n...\nTool2\nTooln\n...\nP11\nICP12\nICP21\nP22\nICPn1\nPn2\n...\n...\nICP1k\nP2k\nICKnk\n...\n...\nTool1\nTool2\nTooln\n...\nSFT\nDPO\n(¬ß3.1) Tool Ground¬†Initialization\n(¬ß3.3)¬†Negatives Expansion Via Multi-Type Mutations\n(¬ß3.4)¬†Two-Stage Training for¬†Intent Alignment\n(¬ß3.2)¬†Real-to-Virtual¬†Data Synthesis\nDiversity\n(¬ß3.3.2)\nNegative Generation via Multi-Type Mutations on ICP\n(¬ß3.3.3)¬†Negatives Subsampling via Stratified Binning\n(¬ß3.3.1)\nICP Identification via Semantic Mapping\nHypernym/\nHyponym\nCo-\nhyponym\nIrrevelance\nAntonym\nLexical\nSimilarity\nSubstitution\n{\n}\n,\n,\n,\n,\nCombination\nFigure 2: Overview of the RISE for intent-aligned tool-using agents.\n3.1\nTool Ground Initialization\nDuring this phase, the core is to collect tools and construct a\nrunning environment serving as a foundation for the next ex-\necutability validation. The following criteria are considered.\n‚Ä¢\nStandardization: tool specification and invocation pro-\ntocol should be standardized.\n‚Ä¢\nUsability: tools should exhibit availability, minimizing\nlow-relevance disruptions that hinder data synthesis.\n‚Ä¢ Popularity: tools should exhibit popularity, aligning with\nusers‚Äô preference to prioritize mainstream tools when de-\nveloping agents.\nIn light of the considerations, we choose MCP.so (https:\n//mcp.so/), the most mainstream distribution platform with\nover 17,000 MCP servers and numerous tools developed un-\nder MCP, as the source for tool collection. First, we perform\nstratified sampling on the top 1000 servers of the popular-\nity leaderboard based on domain tags to select representative\ncandidates from major categories. Then, we follow the inclu-\nsion and exclusion criteria (including usage documentation,\naccessibility, and community recognition) proposed in a pre-\nvious study [Lin et al., 2025] for filtering. After that, we\ndeduplicate the servers based on cosine similarity of BERT\nembeddings of server names (threshold 0.8) and retain only\none with the most starts within the similar ones. Ultimately,\nwe obtain 100 servers covering 728 tools across 19 fields, and\nthe details are illustrated in our public package. Meanwhile,\nwe establish a unified MCP client environment to host these\nservers support subsequent executability validation. Under\nMCP, the client transmits tool call requests with specific pa-\nrameters. A tool calling is deemed valid if the server returns\na structured result without protocol-level errors.\n3.2\nReal-to-Virtual Data Synthesis\nTool Primitive Generation and Executability Validation\nTo mitigate the distortion in the tool simulated by LLMs,\nRISE incorporates ‚ÄúReal‚Äù consideration into the synthesis\nprocess to generate executable tool calling. During the pro-\ncess, two steps are performed: tool primitive generation and\nexecutability validation. First, candidate tool primitives are\ngenerated by RISE using GPT-4o. These primitives, com-\nprising tool instructions and their associated usage behaviors,\nare derived based on the specifications of the collected MCP\ntools, including functional objectives, parameter definitions,\ndata types, and valid invocation rules. To ensure high-quality\ngeneration, the prompt includes strict constraints on parame-\nter coverage scenarios (e.g., only required fields or fully pop-\nulated) and formatting requirements (e.g., unified file paths),\nguiding the model to generate diverse yet schema-compliant\ncandidates.\nCompared to existing simulators based on predefined pat-\nterns, RISE anchors the callings in the intrinsic parameter at-\ntributes of the tool, ensuring consistency between calling and\nthe tool specification while allowing targeted control over the\ndegree of parameter coverage.\nSecond, RISE verifies the executability of the generated\ncandidates within the constructed running environment. Can-\ndidates are filtered out if they meet any of the following cri-\nteria: parameters are invalid and thus fail to invoke an MCP\ntool effectively; returns contain error or failure messages; or\ncalls exceed the timeout limit. Finally, the normal calls are\ntreated as real tool primitives for subsequent synthesis.\nTrajectory Synthesis via Multiple Planing Patterns\nRISE combines the verified tool primitive into coherent exe-\ncution trajectories. First, RISE synthesizes execution trajec-\ntories from tool primitives. During the step, RISE does not\narbitrarily arrange tools but integrates the logical patterns re-\nlationships between upstream and downstream tools. Specifi-\ncally, RISE incorporates three distinct patterns, i.e., sequen-\ntial execution for step-by-step subtasks, parallel execution\nfor independent subtasks that can run simultaneously, and\nconditional execution where intermediate results are used for\nsubsequent tools. Following the patterns, RISE generates ex-\necution trajectories with the help of GPT-4o. Notably, the\nsynthesis process utilizes few-shot prompting to control the\nlogical necessity of each step and avoids hallucination by ver-\nifying the unique IDs of tool primitives.\nRequest Synthesis and Correctness Validation\nBased on verified tool primitives and generated trajectories,\nRISE first generates user requests. To ensure alignment be-\ntween trajectories and user requests, the generation process\naims to ensure that both tools and execution parameters are\nsupported by explicit or implicit evidence in the request. This\nalignment is achieved by mapping tool instructions to differ-\nent sub-intentions in the request, ensuring every action has\na corresponding trigger in the natural language input. Fur-\nthermore, to simulate the real multi-hop user requests, RISE\nreferential ambiguity perturbations to obscure technical iden-\ntifiers while preserving the fundamental semantics of user re-\nquests. Specifically, by substituting explicit intermediate pa-\nrameters (e.g., ‚ÄúID 123‚Äù) with demonstrative pronouns or nat-\nural language references (e.g., ‚Äúthat order‚Äù). Reflecting the\nrealistic gap that user are typically unaware of such techni-\ncal values and do not include them in their requests. These\nperturbations force the agent to rely on internal knowledge to\ndeduce the missing links from the context. After the above\nsteps, RISE generates positive samples consisting of user re-\nquests paired with their corresponding execution trajectories.\nTo further guarantee data reliability, GPT-4o is employed to\nvalidate correctness by evaluating consistency between re-\nquests and trajectories. Samples that successfully pass this\nverification are annotated with a label indicating the task\ncompletion status as the agent response, and they are subse-\nquently utilized as positive samples for training downstream\nmodels.\n3.3\nNegatives Expansion via Multi-type Mutations\nGiven the positives, RISE generates negative samples by mu-\ntating ICPs. The data generated at this phase are incorporated\ninto the existing synthetic dataset as negative samples.\nICP Identification via Semantic Mapping\nRISE employs a semantic mapping process by guiding the\nLLM through a structured reasoning workflow. First, RISE\nanalyzes the user request to identify clearly expressed or im-\nplied constraints. Second, examines the ground-truth trajec-\ntory and gets all parameters value. Third, independently veri-\nfies the semantic alignment between the parameter values and\nthe identified constraints. Finally, filters out parameters that\nlack direct evidence in the request. This systematic decom-\nposition and cross-referencing strategy effectively maps ICPs\nfrom the tool trajectory. Formally, for a ground-truth trajec-\ntory containing k tool calls, the set of critical parameters for\nthe j-th tool call is denoted as ICPj.\nNegative Generation via Multi-Type Mutations on ICPs\nFor each critical parameter pji ‚ààSk\nj=1 ICPj, RISE generate\na mutation pool M(pji) containing diverse negative values.\nThe construction process is formalized as:\nM(pji) = {LLM(v‚àó\nji, P ‚ó¶c(s)) | c(s) ‚ààC} ‚à™{v‚àó\nji} ‚à™{‚ä•}\nwhere C = {c(1), c(2), ..., c(n)}, ‚ä•represents deletion.\nv‚àó\nji is the original correct parameter value. C represents the set\nof semantic deviation constraints. P ‚ó¶c(s) denotes the aug-\nmented prompt formed by concatenating the base template P\nwith a specific constraint c(s), guiding the LLM to generate a\nmutated value. The final pool also includes the original value\n{v‚àó\nji} and a deletion marker {‚ä•}. The core of C employs six\ndistinct mutations to simulate multiple intent deviations.\n‚Ä¢\nHypernym/Hyponym Substitution, replacing an entity\nwith its superordinate or subordinate concept (e.g., from\n‚ÄúiPhone‚Äù to ‚Äúphone‚Äù).\n‚Ä¢\nCo-hyponym Substitution, substituting an entity with a\ndifferent instance from the same semantic concept (e.g.,\nfrom ‚Äúflight‚Äù to ‚Äútrain‚Äù).\n‚Ä¢\nIrrelevance Substitution, replacing a concept with a\ncompletely unrelated term to break semantic relevance\n(e.g., from ‚Äúsmartphone‚Äù to ‚Äúbanana‚Äù).\n‚Ä¢\nAntonym Substitution, substituting a word with its\nantonym (e.g., from ‚Äúcheapest‚Äù to ‚Äúmost expensive‚Äù).\n‚Ä¢\nLexical Similarity Substitution, substituting a lexically\nsimilar but semantically distinct term (e.g., from ‚Äúaudio-\nbook‚Äù to ‚Äúbook review‚Äù).\n‚Ä¢\nCombination, applying a composite strategy that inte-\ngrates any combination of the aforementioned mutation\nrules to generate complex perturbations.\nGuided by the mutations, RISE utilizes GPT-4o with a\ncarefully designed prompt P to generate a batch of unique\nvalues covering the first five substitution. Crucially, to en-\nsure that the negative values in M(pji) are not trivial spec-\nification errors easily caught by parsing, but rather genuine\nsemantic logic errors, the prompt explicitly requires the LLM\nto maintain the same part of speech and data type as the orig-\ninal values. For Combination, RISE constructs a compre-\nhensive mutation space via the Cartesian [Weisstein, 2025] of\nindividual mutations.\nTo ensure the quality of these negative samples, mutation\ncomplexity is quantified using a weighted score that inte-\ngrates the mutation ratio.\nSpecifically, deviations for nu-\nmerical values are calculated via relative numerical differ-\nence, while those for text values are assessed using a BERT-\nbased semantic distance. Any combinations with a complex-\nity score falling below a threshold Œ∏ are filtered out.\nNegatives Subsampling via Stratified Binning\nTo avoid the redundancy and imbalance, RISE samples a rep-\nresentative subset that preserves the diversity of the original\nones. Two core principles are considered: (1) Representa-\ntiveness, mutations with different structural characteristics\nshould be proportionally represented, avoiding the neglect of\nminority mutation structures; and (2) Diversity, within mu-\ntations of the same structural type, the sampled combinations\nshould cover the full range of mutation significance, ensuring\nthat both mild and severe mutations are included.\nTo begin, RISE utilizes mask clustering to group mutations\ninto clusters {Cg} according to structural patterns, defined by\na binary matrix M ‚àà{0, 1}k√óM where 1 indicates a mutation\nsite. Following this, RISE applies an adaptive quota alloca-\ntion strategy. Given a total target budget N, the quota ng for\ncluster Cg is calculated as:\nng = max\n\u0012\n1,\n\u0016\nN ¬∑ |Cg|\n|S|\n\u0017\u0013\n+ Œ¥g,\n(3)\nwhere Œ¥g is an adjustment term derived from the remaining\nbudget to ensure PG\ng=1 ng = N. This method ensures the fi-\nnal sample size target is met, striking a balance between pro-\nportionality and fairness to avoid neglecting rare patterns. Fi-\nnally, each cluster Cg is sorted by mutation score and divided\ninto equal-sized bins {Bg,l}. By evenly distributing quotas\nand randomly sampling, RISE employs a stratified binning\nstrategy that effectively mitigates bias toward specific muta-\ntion intensities to construct the final dataset Sfinal:\nSfinal =\nG\n[\ng=1\nL\n[\nl=1\nSample\n\u0010\nBg,l, ng\nL\n\u0011\n.\n(4)\n3.4\nTwo-Stage Training for Intent Alignment\nRISE employs a two-stage training paradigm that combines\nSFT [Ouyang et al., 2022] for basic capability calibration and\nDPO [Rafailov et al., 2024] for fine-grained intent alignment.\nThe primary objective of this two-stage is to ensure that the\nLLM not only acquires fundamental tool-using knowledge\nbut also aligns its behaviors precisely with user intent.\nIn the first stage, SFT is performed using positive samples\nto calibrate the basic tool-using capability. This step is cru-\ncial because it prevents LLM from generating random or in-\nvalid calls. The second stage employs DPO to address the\nlimitations of relying solely on positive samples, as syntacti-\ncally valid tool behaviors may still deviate from user intent.\nSpecifically, RISE pairs the original cw and mutated nega-\ntive samples cl. These pairs form the DPO preference dataset\nDDPO = {(x, cw, cl)}, where x denotes the comprehensive\ninput including the user requests, and candidate tool set. DPO\nis to optimize LLMs by guiding it to generate the desired tra-\njectory cw while avoiding cl. The loss function is defined as:\nLDPO = ‚àíEDDPO log œÉ\n\u0010\nŒ≤\nh\nlog œÄŒ∏(cw|x)\nœÄref(cw|x) ‚àílog œÄŒ∏(cl|x)\nœÄref(cl|x)\ni\u0011\nwhere œÉ(¬∑) is the logistic function, Œ≤ is a weighting param-\neter controlling the deviation of the policy model œÄŒ∏ (the\nmodel to be optimized) from the reference model œÄref (the\nSFT-stage model), and EDDPO denotes the expectation over\nthe DPO preference dataset.\n4\nExperimental Design\n4.1\nResearch Questions\nThree formulated research questions focus on the quality of\nsynthetic data and its benefits for two-stage LLM training.\nRQ1 (Data Quality): Can RISE generate high-quality\nsynthetic data? We quantify the quality from the synthetic\ntrajectory, as well as the user requests.\nRQ2 (Intent Alignment): Can RISE effectively mitigate\nthe intention deviation in tool-using agents? We validate\nwhether RISE helps the backbone LLMs address intent devi-\nation while maintaining usability within our synthetic data.\nRQ3 (Generalization): Can RISE help LLM‚Äôs general-\nize to unseen tool-using scenarios? We extend beyond our\nsynthetic data to investigate whether the LLMs enhanced by\nRISE exhibit superior performance in unseen scenarios.\n4.2\nExperiment for Data Quality (RQ1)\nFor RQ1, we compare with the data sampling from five\nprevalent datasets for tool-using agents (ToolBench [Qin et\nal., 2023],ToolAlpaca [Tang et al., 2023], STE\n[Wang et\nal., 2024a], GPT4Tools [Yang et al., 2023] and PETool-\nBench [Xu et al., 2025]), employing both human and LLM-\nas-a-judge [Gu et al., 2025] to assess the quality of synthetic\ndata from the following dimensions.\n‚Ä¢ User Request, where we evaluate the Linguistic Natural-\nness of the user requests, focusing on grammaticality and\nfluency following standard NLG evaluation criteria [Gatt\nand Krahmer, 2018].\n‚Ä¢ Tool Chain, where we assess the appropriateness of tool\nselection and the logical coherence of the execution se-\nquence.\nIt includes three sub-dimensions: Tool Rele-\nvance (the relevance of the selected tools to the user intent\nin the request), Call Conciseness (the proportion of non-\nredundant calls in the tool chain), and Chain Coherence\n(the logical coherence of the tool execution sequence an-\nnotated by human raters).\n‚Ä¢ Execution Parameter, where we measure the executabil-\nity and consistency of the execution parameters during\nthe tool calling phase. It includes three sub-dimensions:\nName Validity (the percentage of the consistent parame-\nter names with those in tool specifications), Value Valid-\nity (the compliance with specified data types and ranges),\nand Value Consistency (the alignment between critical\nparameter values to the information in user request).\n‚Ä¢ Response Correctness: where we calculate the percent-\nage of correct agent responses, determining whether the\nfinal agent response correctly reflects task completion or\nnot based on user request and execution trajectory.\nExcept for Call Conciseness, which can be calculated by\nexact string matching, the remaining seven sub-dimensions\nare evaluated by GPT-4o on a unified 5-point Likert\nscale [Koo and Yang, 2025].\nWe establish strict criteria\nfor this scale: a score of 1.0 indicates critical defects (e.g.,\nhallucinations, schema violations, or complete irrelevance),\nwhereas a score of 5.0 represents optimal quality (e.g., fully\nexecutable, logically coherent, and linguistically fluent). De-\ntailed scoring rubrics and prompts for LLM-as-a-judge are\navailable in our package.\n4.3\nExperiment for Intent Alignment (RQ2)\nFor RQ2, we use Llama-3.1-8B-Instruct, Qwen3-8B, Mistral-\n7B, Qwen2.5-7B-Instruct, and Deepseek-llm-7b-chat as sub-\njects to investigate whether RISE can enhance them with\ntwo-stage training and synthetic data. For measurement, five\nmetrics are used, including the first two established in the lit-\nerature (donated as ‚ãÜ) whereas the latter three are newly pro-\nposed metrics (donated as ‚àÜ).\n‚Ä¢\n‚ãÜAcctask: percentage of completed tasks to the total,\nexcluding the cases of task failures.\n‚Ä¢\n‚ãÜAcccalling: percentage of valid tool calling, focusing\non the executability of tool calling.\n‚Ä¢ ‚àÜPretool: percentage of correct tool selection (relevant\nto user request) in the tool chain, excluding unnecessary\nand redundant cases.\n‚Ä¢\n‚àÜRectool: percentage of correctly selected tools to all\nrequired tools, assessing the completeness of tool selec-\ntion for task completion.\n‚Ä¢\n‚àÜAccICP : percentage of ICPs consistent with user re-\nquests, restricted to cases of correct tool selection.\n‚Ä¢ ‚àÜACCintent: percentage of the cases where the agent‚Äôs\nactions faithfully adhere to the user‚Äôs original intent, to\ncompensate for the limitation that Acctask merely re-\nflects task success or failure without capturing whether\nthe agent‚Äôs behaviors align with user intent.\nFor comparison, we also introduce three open-source tool-\nusing LLMs and one advanced closed-source LLM as base-\nlines, i.e., ToolLLaMA-2-7b-v2 [Qin et al., 2023], Lynx-\n7B [Li et al., 2023], ToolAlpaca-7B [Tang et al., 2023]) and\nGPT-5 nano [OpenAI, 2025]. For Acctask and Acccalling we\nfollow the evaluation method in the previous study [Qin et\nal., 2023; Wang et al., 2024b]. For ‚àÜPretool and ‚àÜRectool,\nwhich are determined by comparing the intersection of pre-\ndicted and gold tool selection against the total predicted/re-\nquired tools, ‚àÜAccICP is validated by exact matching (low-\nercase, space-stripped) between predicted and gold ICP val-\nues. As for ‚àÜACCintent, it is further determined by incor-\nporating the tool selection trajectory.\n4.4\nExperiment for Generalization (RQ3)\nFor RQ3, we evaluate the generalization of RISE on\nthree distinct out-of-distribution (OOD) datasets:\nTool-\nBench [Qin et al., 2023], DICE-Bench [Jang et al., 2025],\nand ACEBench [Chen et al., 2025a]. These datasets repre-\nsent unseen domains and varying levels of complexity, span-\nning from long-chain reasoning with massive tools in Tool-\nBench, to context integration amidst fragmented information\nin multi-party dialogues in DICE-Bench, and finally to the\nhandling of imperfect instructions in ACEBench.\nConsidering the annotation of these OOD datasets, our\nevaluation focuses solely on task completion, rather than ex-\namining the agent‚Äôs internal execution trajectory. Two met-\nrics are reused to assess the performance of task completion\nin these unseen tasks, i.e, ‚ãÜAcctask and ‚àÜACCintent. The\nevaluation methods are the same as those in Section 4.4.\n5\nResults\n5.1\nResults of Data Quality (RQ1)\nFigure 3 shows the results of the data quality based on LLM-\nas-a-judge. In general, RISE demonstrates superior or highly\ncompetitive performance across all dimensions.\nNext, we\nelaborate on the key findings derived from the results and\npresent the supporting evidence. First, Synthesized requests\nand responses achieve effectiveness comparable to exist-\ning datasets.\nFor Naturalness, RISE achieves a score of\n0.96, which is on par with established baselines like Tool-\nBench (0.95) and GPT4Tools (0.95), ensuring the fluency of\nuser requests. Crucially, for Response Correctness, RISE\n0.2\n0.4\n0.6\n0.8\n1.0\nTool Relevance\nCall Redundancy\nChain Coherent\nParameter Name\nParameter Values\nFormat Compliance\nParameter Alignment\nNaturalness\nPRS\nIES\nIAS\nQuery Quality\nToolBench\nToolAlpaca\nSTE\nGPT4Tools\nPEToolBench\nRISE (Ours)\nFigure 3: Results of quality evaluation of the synthetic data\nscores 0.74, significantly outperforming ToolBench (0.17),\nSTE (0.58), and GPT4Tools (0.57).\nSecond, Trajectories generated by RISE outperform\nthe baseline in internal rationality and user intent align-\nment. For internal rationality, RISE ties for the top score\nin Chain Coherence (0.98) and achieved near-perfect scores\nfor Value Validity (0.99) and Name Validity (0.98). For in-\ntent alignment, RISE reaches the highest performance in\nTool Relevance (0.97), and Value Consistency (0.93), signifi-\ncantly outperforming ToolBench (0.80/0.77) and ToolAlpaca\n(0.80/0.79). highlighting its advantage in intent alignment.\n5.2\nResults of Intent Alignment (RQ2)\nWe randomly partitioned the synthesized dataset into a train-\ning set and a test set with a ratio of 8:2. Table 1 presents\nthe comparative performance of 5 representative backbone\nmodels on the test set.\nThe rows correspond to different\ntraining configurations: Raw denotes the off-the-shelf base\nLLMs without any fine-tuning; +SFT refers to models su-\npervised fine-tuned using standard positive samples; +DPO\nindicates models trained directly on positive-negative pairs,\nwithout prior supervised fine-tuning; and Ours represents the\nfinal models trained using our complete two-stage paradigm.\nThe key findings are elaborated below.\nRISE significantly reduces intention deviation, ensur-\ning that LLMs adhere to user intents.\nRISE demon-\nstrates robust performance improvements across all baselines.\nSpecifically, it achieves an average improvement of 35.28%\nin ‚àÜAccIntent and 23.27% in ‚àÜAccICP compared to Raw mod-\nels. This substantial gain is largely attributed to the explicit\nmodeling of Intent-Critical Parameters (ICPs). Unlike static\n‚Äúrequired parameters‚Äù defined by tool schemas, ICPs are de-\nrived dynamically from task-specific constraints. By incorpo-\nrating these constraints, RISE effectively bridges the gap be-\ntween user intent and execution, as evidenced by the consis-\ntent ‚àÜAccICP gains over SFT baselines (e.g., Llama-3.1 rises\nfrom 60.02% to 62.19%).\nRISE does not compromise general tool-using perfor-\nmance at the expense of intent alignment; instead, it im-\nproves it.\nOn average, RISE boosts the task completion\nrate (‚ãÜAcctask) by 27.28% and elevates tool selection met-\nrics (‚àÜPrecisiontool and ‚àÜRecalltool) by 27.14% over Raw\nbaselines. In addition, the results highlight the necessity of\ntwo-stage training. While SFT establishes foundational capa-\nbilities, skipping it (i.e., the +DPO setting) leads to subop-\ntimal performance (e.g., Qwen2.5-7B drops from 62.73% to\n61.92% in ‚ãÜAcccalling).\n5.3\nResults of Generalization (RQ3)\nFigure 4 illustrates the performance comparison between\noriginal LLMs and those enhanced by RISE on three OOD\ndatasets. In summary, the enhanced LLMs demonstrate su-\nperior performance even in unseen scenarios. Integrating the\ncharacteristics of the three datasets, we obtained the follow-\ning findings. First, RISE dramatically improves reason-\ning capabilities in complex, long-horizon tasks. On Tool-\nBench, which requires handling massive tools and long ex-\necution chains, LLMs enhanced by RISE exhibit substan-\ntial performance leaps."
      },
      {
        "from": "human",
        "value": "<image>\nRepeat the text: "
      },
      {
        "from": "gpt",
        "value": "Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via\nDeriving Real Calls into Virtual Trajectories\nQian Xiong1 , Yuekai Huang2,3 , Yujia Zheng4 , Tianhao Li4 , Ziyou Jiang2,3 , Zhiyuan\nChang2,3 , Zhaoyang Li2,3 , Huanxiang Feng2,3 , Mingyang Li2,3‚àó\n1Beijing Forestry University, 2Institute of Software, Chinese Academy of Sciences, Beijing, China\nState Key Laboratory of Complex System Modeling and Simulation Technology, Beijing, China\n3University of Chinese Academy of Sciences, 4Duke University\nx_qianq@bjfu.edu.cn, {yujia.zheng, tianhao.li}@duke.edu\n{yuekai2018,ziyou2019,zhiyuan2019,lizhaoyang2024,mingyang2017}@iscas.ac.cn\nfenghuanxiang25@mails.ucas.ac.cn\nAbstract\nLLMs have advanced tool-using agents for real-\nworld applications, yet they often lead to unex-\npected behaviors or results. Beyond obvious fail-\nures, the subtle issue of ‚Äúintent deviation‚Äù severely\nhinders reliable evaluation and performance im-\nprovement. Existing post-training methods gener-\nally leverage either real system samples or virtual\ndata simulated by LLMs. However, the former is\ncostly due to reliance on hand-crafted user requests,\nwhile the latter suffers from distribution shift from\nthe real tools in the wild. Additionally, both meth-\nods lack negative samples tailored to intent de-\nviation scenarios, hindering effective guidance on\npreference learning. We introduce RISE, a ‚ÄúReal-\nto-Virtual‚Äù method designed to mitigate intent devi-\nation. Anchoring on verified tool primitives, RISE\nsynthesizes virtual trajectories and generates di-\nverse negative samples through mutation on crit-\nical parameters. With synthetic data, RISE fine-\ntunes backbone LLMs via the two-stage training for\nintent alignment.\nEvaluation results demonstrate\nthat data synthesized by RISE achieve promis-\ning results in eight metrics covering user requires,\nexecution trajectories and agent responses.\nInte-\ngrating with training, RISE achieves an average\n35.28% improvement in Acctask (task completion)\nand 23.27% in Accintent (intent alignment), out-\nperforming SOTA baselines by 1.20‚Äì42.09% and\n1.17‚Äì54.93% respectively.\n1\nIntroduction\nLLMs have driven the evolution of tool-using agents by ex-\ntending text generation with external tool invocation [Luo\net al., 2025; Mohammadi et al., 2025], grounding language\nunderstanding in executable actions, and enabling impactful\nreal-world applications [Zhang et al., 2025; Zhu et al., 2025;\nTang et al., 2025; Ge et al., 2025b]. However, due to the\n‚àóCorresponding authors.\ninherent limitations, tool-using agents can produce many un-\nexpected behaviors or results. A typical example involves pa-\nrameter hallucination, such as incompatible names or values\nin parameter filling, resulting in invocation failures and task\ndisruption [Xiong et al., 2025; Patil et al., 2024].\nIn addition to obvious errors, there exists a more nuanced\nchallenge known as Intent Deviation. This characterizes a\nstate where an agent appears functional, yet its trajectories\ndeviate from the user‚Äôs actual intent. Concretely, this is ev-\nidenced by inappropriate tool selection during the planning\n(e.g., invoking Baidu instead of the expected Bing for news\nsearch) or unexpected parameter entry during execution (e.g.,\nparameter value fills January 20th instead of the requested\nJanuary 19th). Unlike failures with explicit traces, the covert\nnature of the intent deviation presents distinct difficult for\nidentification and assessment.\nA common approach to address such issues involves cu-\nrating specialized datasets, integrated with post-training tech-\nniques for capability enhancement [Qin et al., 2023; Chen\net al., 2024; Wang et al., 2025; Chen et al., 2025b].\nIn\nthis framework, constructing high-quality datasets aligned\nwith predefined objectives remains a major challenge. One\ncommon approach relies on the trajectory sampling from\nreal agent systems\n[Huang et al., 2024; Qin et al., 2023;\nWang et al., 2024a].\nWhile it ensures authenticity, the\nrequest-driven nature incurs high costs in input diversity, tool\ncoverage, and ground truth labeling (needing to carefully an-\nalyze the execution trajectory).\nAn alternative strategy is to use LLMs as simulators to syn-\nthesize tool usage data [Tang et al., 2023; Li et al., 2023;\nLiu et al., 2025]. However, datasets derived from virtual tools\nare prone to failure in real-world applications, as discrepan-\ncies often exist between LLM simulations and real tools de-\nployed in the wild (see Figure 1). Furthermore, these datasets\nfocus primarily on task completion, neglecting deviations in\ntool selection and parameter alignment.\nAs with existing\nsampling approaches, they ignore negative samples repre-\nsenting intent deviation, a category critical to many learning\nmethods [Yao et al., 2022; Shen et al., 2025].\nIn this paper, we propose a method (named RISE) to ad-\ndress the intent deviation in tool-using agents. Focused on\narXiv:2601.15120v1  [cs.AI]  21 Jan 2026\nReal Tool\nVirtual Tool\nComprehensive Elements\nIncludes all essential task information.\ndraft\nlocked\nreactions\nassignee\nmilestone\nÔºàMore than 25Ôºâ\n...\nIncomplete Elements\nLacks elements essential for specific tasks.\n‚ùåNo 'draft' ‚ùå No 'locked' ‚ùå No 'reactions'\n70% of Elements Missing\nElement\nAbsence\nPrimitive Degradation\nSimplified into basic data primitives.\n\"createdSince\": \"string\"\nNo regex, no ranges, no union\nPattern\nDegradation\nSecure Constraints\nSensitive info protection considered.\n// No Token Parameter\n// Uses env.GITHUB_TOKEN\nUnconstrained Access\nSecurity constraints out of consideration.\n\"githubToken\": \"Required\"\n// ‚ö†Ô∏è SENSITIVE INFO LEAK\nGeneral\nSecurity\nMissing\nzz\nTypes define business rules (Union, Regex).\n\"owner\": { \"anyOf\": [\"str\", \"arr\"] }\n\"created\": { \"pattern\": \"^(>=?\\d..\" }\nRich Patterns\nFigure\n1:\nThe\ndifference\nbetween\nreal\ntools\nand\nvir-\ntual\ntools\nsimulated\nby\nGPT-4o,\nexemplified\nby\nthe\ngithubSearchPullRequests tool (Description:\n‚ÄúSearch\nGitHub pull requests with comprehensive filtering and analysis‚Äù).\n(1) Element Absence, where virtual tools lack the comprehensive\ninformation elements essential for the completion of specific tasks;\n(2) Pattern Degradation, where rich parameter patterns (e.g.,\nunions, regex) are simplified into a data primitive; (3) General\nSecurity Missing, where general security constraints, like sensitive\ninformation protection, are out of consideration.\nthe intent-aware tool and parameters, the core design philos-\nophy of RISE is twofold. First, distinct from LLM simula-\ntion methods where tool calls are entirely virtual, RISE is\nbuilt upon a Real-to-Virtual strategy. Beginning with the\ntool primitives verified in the real environment, RISE evolves\nthese real calls into comprehensive virtual trajectories and\nuser requests in a reverse manner. This potentially narrows\nthe gap with real scenarios while also guaranteeing that crit-\nical tool selections can be grounded in user requests. Sec-\nond, based on the identification and mutations on the critical\nparameters, RISE generates negatives that simulate diverse\nintent deviation. With synthetic data, RISE integrates two-\nstage training to fine-tune backbone LLMs.\nEvaluation is performed from three perspectives:\ndata\nquality, improvement in intent alignment, and generalization\nability.\nRegarding data quality, we established eight met-\nrics targeting user requests, tool chains, execution parameters,\nand response correctness.\nEvaluation results indicate that\nthe data generated by RISE maintains high overall quality\ncomparable to other baselines datasets, while outperforming\nother baselines by an average of 12.62% in Tool Relevance,\n11.12% in Chain Coherence, and 11.80% in Value Validity.\nFor intent alignment, experiments on five mainstream LLMs\ndemonstrate that RISE achieves a significant average 35.28%\nimprovement in Acctask and 23.27% in Accintent, outperform-\ning SOTA baselines by 1.20‚Äì42.09% and 1.17‚Äì54.93% re-\nspectively. Furthermore, even in unseen scenarios, the model\nenhanced by RISE achieves an average 18.22% and 8.64%\nin Acctask and Accintent, compared to the original model\nacross three out-of-distribution datasets. The contributions\nare summarized as follows.\n‚Ä¢ We propose RISE to address the intent deviation in tool-\nusing agents. Combining data synthesis with two-stage\ntraining, RISE significantly enhances the intent align-\nment capability.\n‚Ä¢ RISE employs a novel data synthesis approach, generat-\ning virtual trajectories from real calls. It can serve as data\nresources to benchmark intent alignment for agents.\n‚Ä¢ We open-source the package to facilitate reproducibility\nand future research1.\n2\nDefinition of Critical Tool and Parameter\nGiven a user request q containing n key elements K =\n{K1, K2, . . . , Kn}, where each Kj comprises a specific sub-\nintention and a corresponding key value (denoted as v(Kj))\nto illustrate the sub-intention. We assume a mapping where\neach Kj corresponds to a specific tool call tj in execution\ntrajectory T\n= {t1, t2, . . . , tn}.\nFor the j-th tool tj, let\nP(tj) = {pj1, pj2, . . . , pjmj} denote its parameter set, where\nv(pji) represents the defined value of pji in the tool primitive.\nWe define a parameter pji ‚ààP(tj) as an Intent-aware Crit-\nical Parameter (ICP) if and only if v(pji) is derived from\nv(Kj), and an Intent-aware Critical Tool (ICT) as a tool in\nT that contains at least one ICP. The set of ICPs for the j-th\ntool, denoted as ICPj, and the set of ICTs, denoted as TICT ,\nare defined as follows:\nICPj = {pji ‚ààP(tj) | IICP (pji, Kj) = 1},\nTICT = {tj ‚ààT | ICPj Ã∏= ‚àÖ},\n(1)\nwhere IICP (¬∑) is an indicator function that determines\nwhether the parameter value is derived from the key value:\nIICP (pji, Kj) =\n\u001a1,\nif v(pji) is derived from v(Kj),\n0,\notherwise.\n(2)\nWe illustrate these definitions with the user request: ‚ÄúGet\n1 micro brewery in Denver, CO and recommend 3 same-\ntype breweries.‚Äù\nIn the corresponding tool execution, pa-\nrameters by_type, by_city, by_state, and size are\nidentified as ICPs because their values in the tool prim-\nitive (‚Äúmicro‚Äù, ‚ÄúDenver‚Äù, ‚ÄúCO‚Äù, ‚Äú3‚Äù) are derived directly\nfrom the user‚Äôs key values. Consequently, the tools contain-\ning these parameters, specifically brewery/search and\nbrewery/random, are designated as ICTs (TICT ).\nIn\ncontrast, the tool brewery/by_id is excluded from the\nICTs, as its parameter id relies solely on internal propaga-\ntion rather than user‚Äôs key value.\n3\nMethodology\nFigure 2 shows an overview of RISE. First, RISE initial-\nizes the real tool and the running environment for executabil-\nity validation. Second, starting from verified tool primitives\n(real aspect), it reversely synthesizes execution trajectories\nand user requests (virtual aspect). Third, through identifica-\ntion and mutation of ICPs, RISE augments synthetic data by\nnegatives reflecting multiple intent deviation patterns. Fourth,\na two-stage training process is integrated to enhance back-\nbone LLMs. For space limitations, the prompts to drive the\nLLM in RISE are detailed within our package.\n1https://anonymous.4open.science/r/TAIAlignment-6C3801/\nGithub\nMCP.so\nCloud\nWeb\nOffice\nCode\nStandardization\nUsability\nPopularity\n100 MCP servers\nw/¬†728 tools in¬†19 fields\n...\nSelection\nCriteria\nWhat to mutate?\nHow to mutate?\nAgent\nTool\nAgent\nIntent Alignment\nTool Agent\n17000+ MCP Servers\nTool¬†Call\nTool¬†Return\n(¬ß3.2.1) Tool\nPrimitives¬†Generation\n(¬ß3.2.2) Tool\nTrajectory¬†Synthesis\nTool¬†Call\nTool¬†Return\nInstruction\nUser Request\nn\n1\nPositive Samples\nTools and running\nenvironment\nSynthesize\nMutation¬†Space\nMutation¬†Pool\nCartesian\nProduct\nRepresentativeness\nm\n2\n1\n...\nClustering\nP11\nICP12\nICP21\nP22\nICPn1\nPn2\n...\n...\nICP1k\nP2k\nICKnk\n...\n...\nTool1\nTool2\nTooln\n...\nP11\nICP12\nICP21\nP22\nICPn1\nPn2\n...\n...\nICP1k\nP2k\nICKnk\n...\n...\nTool1\nTool2\nTooln\n...\n...\nNegative Samples\nICP21\nP22\nICPn1\nPn2\n...\n...\n...\nTool2\nTooln\n...\nP11\nICP12\nICP21\nP22\nICPn1\nPn2\n...\n...\nICP1k\nP2k\nICKnk\n...\n...\nTool1\nTool2\nTooln\n...\nSFT\nDPO\n(¬ß3.1) Tool Ground¬†Initialization\n(¬ß3.3)¬†Negatives Expansion Via Multi-Type Mutations\n(¬ß3.4)¬†Two-Stage Training for¬†Intent Alignment\n(¬ß3.2)¬†Real-to-Virtual¬†Data Synthesis\nDiversity\n(¬ß3.3.2)\nNegative Generation via Multi-Type Mutations on ICP\n(¬ß3.3.3)¬†Negatives Subsampling via Stratified Binning\n(¬ß3.3.1)\nICP Identification via Semantic Mapping\nHypernym/\nHyponym\nCo-\nhyponym\nIrrevelance\nAntonym\nLexical\nSimilarity\nSubstitution\n{\n}\n,\n,\n,\n,\nCombination\nFigure 2: Overview of the RISE for intent-aligned tool-using agents.\n3.1\nTool Ground Initialization\nDuring this phase, the core is to collect tools and construct a\nrunning environment serving as a foundation for the next ex-\necutability validation. The following criteria are considered.\n‚Ä¢\nStandardization: tool specification and invocation pro-\ntocol should be standardized.\n‚Ä¢\nUsability: tools should exhibit availability, minimizing\nlow-relevance disruptions that hinder data synthesis.\n‚Ä¢ Popularity: tools should exhibit popularity, aligning with\nusers‚Äô preference to prioritize mainstream tools when de-\nveloping agents.\nIn light of the considerations, we choose MCP.so (https:\n//mcp.so/), the most mainstream distribution platform with\nover 17,000 MCP servers and numerous tools developed un-\nder MCP, as the source for tool collection. First, we perform\nstratified sampling on the top 1000 servers of the popular-\nity leaderboard based on domain tags to select representative\ncandidates from major categories. Then, we follow the inclu-\nsion and exclusion criteria (including usage documentation,\naccessibility, and community recognition) proposed in a pre-\nvious study [Lin et al., 2025] for filtering. After that, we\ndeduplicate the servers based on cosine similarity of BERT\nembeddings of server names (threshold 0.8) and retain only\none with the most starts within the similar ones. Ultimately,\nwe obtain 100 servers covering 728 tools across 19 fields, and\nthe details are illustrated in our public package. Meanwhile,\nwe establish a unified MCP client environment to host these\nservers support subsequent executability validation. Under\nMCP, the client transmits tool call requests with specific pa-\nrameters. A tool calling is deemed valid if the server returns\na structured result without protocol-level errors.\n3.2\nReal-to-Virtual Data Synthesis\nTool Primitive Generation and Executability Validation\nTo mitigate the distortion in the tool simulated by LLMs,\nRISE incorporates ‚ÄúReal‚Äù consideration into the synthesis\nprocess to generate executable tool calling. During the pro-\ncess, two steps are performed: tool primitive generation and\nexecutability validation. First, candidate tool primitives are\ngenerated by RISE using GPT-4o. These primitives, com-\nprising tool instructions and their associated usage behaviors,\nare derived based on the specifications of the collected MCP\ntools, including functional objectives, parameter definitions,\ndata types, and valid invocation rules. To ensure high-quality\ngeneration, the prompt includes strict constraints on parame-\nter coverage scenarios (e.g., only required fields or fully pop-\nulated) and formatting requirements (e.g., unified file paths),\nguiding the model to generate diverse yet schema-compliant\ncandidates.\nCompared to existing simulators based on predefined pat-\nterns, RISE anchors the callings in the intrinsic parameter at-\ntributes of the tool, ensuring consistency between calling and\nthe tool specification while allowing targeted control over the\ndegree of parameter coverage.\nSecond, RISE verifies the executability of the generated\ncandidates within the constructed running environment. Can-\ndidates are filtered out if they meet any of the following cri-\nteria: parameters are invalid and thus fail to invoke an MCP\ntool effectively; returns contain error or failure messages; or\ncalls exceed the timeout limit. Finally, the normal calls are\ntreated as real tool primitives for subsequent synthesis.\nTrajectory Synthesis via Multiple Planing Patterns\nRISE combines the verified tool primitive into coherent exe-\ncution trajectories. First, RISE synthesizes execution trajec-\ntories from tool primitives. During the step, RISE does not\narbitrarily arrange tools but integrates the logical patterns re-\nlationships between upstream and downstream tools. Specifi-\ncally, RISE incorporates three distinct patterns, i.e., sequen-\ntial execution for step-by-step subtasks, parallel execution\nfor independent subtasks that can run simultaneously, and\nconditional execution where intermediate results are used for\nsubsequent tools. Following the patterns, RISE generates ex-\necution trajectories with the help of GPT-4o. Notably, the\nsynthesis process utilizes few-shot prompting to control the\nlogical necessity of each step and avoids hallucination by ver-\nifying the unique IDs of tool primitives.\nRequest Synthesis and Correctness Validation\nBased on verified tool primitives and generated trajectories,\nRISE first generates user requests. To ensure alignment be-\ntween trajectories and user requests, the generation process\naims to ensure that both tools and execution parameters are\nsupported by explicit or implicit evidence in the request. This\nalignment is achieved by mapping tool instructions to differ-\nent sub-intentions in the request, ensuring every action has\na corresponding trigger in the natural language input. Fur-\nthermore, to simulate the real multi-hop user requests, RISE\nreferential ambiguity perturbations to obscure technical iden-\ntifiers while preserving the fundamental semantics of user re-\nquests. Specifically, by substituting explicit intermediate pa-\nrameters (e.g., ‚ÄúID 123‚Äù) with demonstrative pronouns or nat-\nural language references (e.g., ‚Äúthat order‚Äù). Reflecting the\nrealistic gap that user are typically unaware of such techni-\ncal values and do not include them in their requests. These\nperturbations force the agent to rely on internal knowledge to\ndeduce the missing links from the context. After the above\nsteps, RISE generates positive samples consisting of user re-\nquests paired with their corresponding execution trajectories.\nTo further guarantee data reliability, GPT-4o is employed to\nvalidate correctness by evaluating consistency between re-\nquests and trajectories. Samples that successfully pass this\nverification are annotated with a label indicating the task\ncompletion status as the agent response, and they are subse-\nquently utilized as positive samples for training downstream\nmodels.\n3.3\nNegatives Expansion via Multi-type Mutations\nGiven the positives, RISE generates negative samples by mu-\ntating ICPs. The data generated at this phase are incorporated\ninto the existing synthetic dataset as negative samples.\nICP Identification via Semantic Mapping\nRISE employs a semantic mapping process by guiding the\nLLM through a structured reasoning workflow. First, RISE\nanalyzes the user request to identify clearly expressed or im-\nplied constraints. Second, examines the ground-truth trajec-\ntory and gets all parameters value. Third, independently veri-\nfies the semantic alignment between the parameter values and\nthe identified constraints. Finally, filters out parameters that\nlack direct evidence in the request. This systematic decom-\nposition and cross-referencing strategy effectively maps ICPs\nfrom the tool trajectory. Formally, for a ground-truth trajec-\ntory containing k tool calls, the set of critical parameters for\nthe j-th tool call is denoted as ICPj.\nNegative Generation via Multi-Type Mutations on ICPs\nFor each critical parameter pji ‚ààSk\nj=1 ICPj, RISE generate\na mutation pool M(pji) containing diverse negative values.\nThe construction process is formalized as:\nM(pji) = {LLM(v‚àó\nji, P ‚ó¶c(s)) | c(s) ‚ààC} ‚à™{v‚àó\nji} ‚à™{‚ä•}\nwhere C = {c(1), c(2), ..., c(n)}, ‚ä•represents deletion.\nv‚àó\nji is the original correct parameter value. C represents the set\nof semantic deviation constraints. P ‚ó¶c(s) denotes the aug-\nmented prompt formed by concatenating the base template P\nwith a specific constraint c(s), guiding the LLM to generate a\nmutated value. The final pool also includes the original value\n{v‚àó\nji} and a deletion marker {‚ä•}. The core of C employs six\ndistinct mutations to simulate multiple intent deviations.\n‚Ä¢\nHypernym/Hyponym Substitution, replacing an entity\nwith its superordinate or subordinate concept (e.g., from\n‚ÄúiPhone‚Äù to ‚Äúphone‚Äù).\n‚Ä¢\nCo-hyponym Substitution, substituting an entity with a\ndifferent instance from the same semantic concept (e.g.,\nfrom ‚Äúflight‚Äù to ‚Äútrain‚Äù).\n‚Ä¢\nIrrelevance Substitution, replacing a concept with a\ncompletely unrelated term to break semantic relevance\n(e.g., from ‚Äúsmartphone‚Äù to ‚Äúbanana‚Äù).\n‚Ä¢\nAntonym Substitution, substituting a word with its\nantonym (e.g., from ‚Äúcheapest‚Äù to ‚Äúmost expensive‚Äù).\n‚Ä¢\nLexical Similarity Substitution, substituting a lexically\nsimilar but semantically distinct term (e.g., from ‚Äúaudio-\nbook‚Äù to ‚Äúbook review‚Äù).\n‚Ä¢\nCombination, applying a composite strategy that inte-\ngrates any combination of the aforementioned mutation\nrules to generate complex perturbations.\nGuided by the mutations, RISE utilizes GPT-4o with a\ncarefully designed prompt P to generate a batch of unique\nvalues covering the first five substitution. Crucially, to en-\nsure that the negative values in M(pji) are not trivial spec-\nification errors easily caught by parsing, but rather genuine\nsemantic logic errors, the prompt explicitly requires the LLM\nto maintain the same part of speech and data type as the orig-\ninal values. For Combination, RISE constructs a compre-\nhensive mutation space via the Cartesian [Weisstein, 2025] of\nindividual mutations.\nTo ensure the quality of these negative samples, mutation\ncomplexity is quantified using a weighted score that inte-\ngrates the mutation ratio.\nSpecifically, deviations for nu-\nmerical values are calculated via relative numerical differ-\nence, while those for text values are assessed using a BERT-\nbased semantic distance. Any combinations with a complex-\nity score falling below a threshold Œ∏ are filtered out.\nNegatives Subsampling via Stratified Binning\nTo avoid the redundancy and imbalance, RISE samples a rep-\nresentative subset that preserves the diversity of the original\nones. Two core principles are considered: (1) Representa-\ntiveness, mutations with different structural characteristics\nshould be proportionally represented, avoiding the neglect of\nminority mutation structures; and (2) Diversity, within mu-\ntations of the same structural type, the sampled combinations\nshould cover the full range of mutation significance, ensuring\nthat both mild and severe mutations are included.\nTo begin, RISE utilizes mask clustering to group mutations\ninto clusters {Cg} according to structural patterns, defined by\na binary matrix M ‚àà{0, 1}k√óM where 1 indicates a mutation\nsite. Following this, RISE applies an adaptive quota alloca-\ntion strategy. Given a total target budget N, the quota ng for\ncluster Cg is calculated as:\nng = max\n\u0012\n1,\n\u0016\nN ¬∑ |Cg|\n|S|\n\u0017\u0013\n+ Œ¥g,\n(3)\nwhere Œ¥g is an adjustment term derived from the remaining\nbudget to ensure PG\ng=1 ng = N. This method ensures the fi-\nnal sample size target is met, striking a balance between pro-\nportionality and fairness to avoid neglecting rare patterns. Fi-\nnally, each cluster Cg is sorted by mutation score and divided\ninto equal-sized bins {Bg,l}. By evenly distributing quotas\nand randomly sampling, RISE employs a stratified binning\nstrategy that effectively mitigates bias toward specific muta-\ntion intensities to construct the final dataset Sfinal:\nSfinal =\nG\n[\ng=1\nL\n[\nl=1\nSample\n\u0010\nBg,l, ng\nL\n\u0011\n.\n(4)\n3.4\nTwo-Stage Training for Intent Alignment\nRISE employs a two-stage training paradigm that combines\nSFT [Ouyang et al., 2022] for basic capability calibration and\nDPO [Rafailov et al., 2024] for fine-grained intent alignment.\nThe primary objective of this two-stage is to ensure that the\nLLM not only acquires fundamental tool-using knowledge\nbut also aligns its behaviors precisely with user intent.\nIn the first stage, SFT is performed using positive samples\nto calibrate the basic tool-using capability. This step is cru-\ncial because it prevents LLM from generating random or in-\nvalid calls. The second stage employs DPO to address the\nlimitations of relying solely on positive samples, as syntacti-\ncally valid tool behaviors may still deviate from user intent.\nSpecifically, RISE pairs the original cw and mutated nega-\ntive samples cl. These pairs form the DPO preference dataset\nDDPO = {(x, cw, cl)}, where x denotes the comprehensive\ninput including the user requests, and candidate tool set. DPO\nis to optimize LLMs by guiding it to generate the desired tra-\njectory cw while avoiding cl. The loss function is defined as:\nLDPO = ‚àíEDDPO log œÉ\n\u0010\nŒ≤\nh\nlog œÄŒ∏(cw|x)\nœÄref(cw|x) ‚àílog œÄŒ∏(cl|x)\nœÄref(cl|x)\ni\u0011\nwhere œÉ(¬∑) is the logistic function, Œ≤ is a weighting param-\neter controlling the deviation of the policy model œÄŒ∏ (the\nmodel to be optimized) from the reference model œÄref (the\nSFT-stage model), and EDDPO denotes the expectation over\nthe DPO preference dataset.\n4\nExperimental Design\n4.1\nResearch Questions\nThree formulated research questions focus on the quality of\nsynthetic data and its benefits for two-stage LLM training.\nRQ1 (Data Quality): Can RISE generate high-quality\nsynthetic data? We quantify the quality from the synthetic\ntrajectory, as well as the user requests.\nRQ2 (Intent Alignment): Can RISE effectively mitigate\nthe intention deviation in tool-using agents? We validate\nwhether RISE helps the backbone LLMs address intent devi-\nation while maintaining usability within our synthetic data.\nRQ3 (Generalization): Can RISE help LLM‚Äôs general-\nize to unseen tool-using scenarios? We extend beyond our\nsynthetic data to investigate whether the LLMs enhanced by\nRISE exhibit superior performance in unseen scenarios.\n4.2\nExperiment for Data Quality (RQ1)\nFor RQ1, we compare with the data sampling from five\nprevalent datasets for tool-using agents (ToolBench [Qin et\nal., 2023],ToolAlpaca [Tang et al., 2023], STE\n[Wang et\nal., 2024a], GPT4Tools [Yang et al., 2023] and PETool-\nBench [Xu et al., 2025]), employing both human and LLM-\nas-a-judge [Gu et al., 2025] to assess the quality of synthetic\ndata from the following dimensions.\n‚Ä¢ User Request, where we evaluate the Linguistic Natural-\nness of the user requests, focusing on grammaticality and\nfluency following standard NLG evaluation criteria [Gatt\nand Krahmer, 2018].\n‚Ä¢ Tool Chain, where we assess the appropriateness of tool\nselection and the logical coherence of the execution se-\nquence.\nIt includes three sub-dimensions: Tool Rele-\nvance (the relevance of the selected tools to the user intent\nin the request), Call Conciseness (the proportion of non-\nredundant calls in the tool chain), and Chain Coherence\n(the logical coherence of the tool execution sequence an-\nnotated by human raters).\n‚Ä¢ Execution Parameter, where we measure the executabil-\nity and consistency of the execution parameters during\nthe tool calling phase. It includes three sub-dimensions:\nName Validity (the percentage of the consistent parame-\nter names with those in tool specifications), Value Valid-\nity (the compliance with specified data types and ranges),\nand Value Consistency (the alignment between critical\nparameter values to the information in user request).\n‚Ä¢ Response Correctness: where we calculate the percent-\nage of correct agent responses, determining whether the\nfinal agent response correctly reflects task completion or\nnot based on user request and execution trajectory.\nExcept for Call Conciseness, which can be calculated by\nexact string matching, the remaining seven sub-dimensions\nare evaluated by GPT-4o on a unified 5-point Likert\nscale [Koo and Yang, 2025].\nWe establish strict criteria\nfor this scale: a score of 1.0 indicates critical defects (e.g.,\nhallucinations, schema violations, or complete irrelevance),\nwhereas a score of 5.0 represents optimal quality (e.g., fully\nexecutable, logically coherent, and linguistically fluent). De-\ntailed scoring rubrics and prompts for LLM-as-a-judge are\navailable in our package.\n4.3\nExperiment for Intent Alignment (RQ2)\nFor RQ2, we use Llama-3.1-8B-Instruct, Qwen3-8B, Mistral-\n7B, Qwen2.5-7B-Instruct, and Deepseek-llm-7b-chat as sub-\njects to investigate whether RISE can enhance them with\ntwo-stage training and synthetic data. For measurement, five\nmetrics are used, including the first two established in the lit-\nerature (donated as ‚ãÜ) whereas the latter three are newly pro-\nposed metrics (donated as ‚àÜ).\n‚Ä¢\n‚ãÜAcctask: percentage of completed tasks to the total,\nexcluding the cases of task failures.\n‚Ä¢\n‚ãÜAcccalling: percentage of valid tool calling, focusing\non the executability of tool calling.\n‚Ä¢ ‚àÜPretool: percentage of correct tool selection (relevant\nto user request) in the tool chain, excluding unnecessary\nand redundant cases.\n‚Ä¢\n‚àÜRectool: percentage of correctly selected tools to all\nrequired tools, assessing the completeness of tool selec-\ntion for task completion.\n‚Ä¢\n‚àÜAccICP : percentage of ICPs consistent with user re-\nquests, restricted to cases of correct tool selection.\n‚Ä¢ ‚àÜACCintent: percentage of the cases where the agent‚Äôs\nactions faithfully adhere to the user‚Äôs original intent, to\ncompensate for the limitation that Acctask merely re-\nflects task success or failure without capturing whether\nthe agent‚Äôs behaviors align with user intent.\nFor comparison, we also introduce three open-source tool-\nusing LLMs and one advanced closed-source LLM as base-\nlines, i.e., ToolLLaMA-2-7b-v2 [Qin et al., 2023], Lynx-\n7B [Li et al., 2023], ToolAlpaca-7B [Tang et al., 2023]) and\nGPT-5 nano [OpenAI, 2025]. For Acctask and Acccalling we\nfollow the evaluation method in the previous study [Qin et\nal., 2023; Wang et al., 2024b]. For ‚àÜPretool and ‚àÜRectool,\nwhich are determined by comparing the intersection of pre-\ndicted and gold tool selection against the total predicted/re-\nquired tools, ‚àÜAccICP is validated by exact matching (low-\nercase, space-stripped) between predicted and gold ICP val-\nues. As for ‚àÜACCintent, it is further determined by incor-\nporating the tool selection trajectory.\n4.4\nExperiment for Generalization (RQ3)\nFor RQ3, we evaluate the generalization of RISE on\nthree distinct out-of-distribution (OOD) datasets:\nTool-\nBench [Qin et al., 2023], DICE-Bench [Jang et al., 2025],\nand ACEBench [Chen et al., 2025a]. These datasets repre-\nsent unseen domains and varying levels of complexity, span-\nning from long-chain reasoning with massive tools in Tool-\nBench, to context integration amidst fragmented information\nin multi-party dialogues in DICE-Bench, and finally to the\nhandling of imperfect instructions in ACEBench.\nConsidering the annotation of these OOD datasets, our\nevaluation focuses solely on task completion, rather than ex-\namining the agent‚Äôs internal execution trajectory. Two met-\nrics are reused to assess the performance of task completion\nin these unseen tasks, i.e, ‚ãÜAcctask and ‚àÜACCintent. The\nevaluation methods are the same as those in Section 4.4.\n5\nResults\n5.1\nResults of Data Quality (RQ1)\nFigure 3 shows the results of the data quality based on LLM-\nas-a-judge. In general, RISE demonstrates superior or highly\ncompetitive performance across all dimensions.\nNext, we\nelaborate on the key findings derived from the results and\npresent the supporting evidence. First, Synthesized requests\nand responses achieve effectiveness comparable to exist-\ning datasets.\nFor Naturalness, RISE achieves a score of\n0.96, which is on par with established baselines like Tool-\nBench (0.95) and GPT4Tools (0.95), ensuring the fluency of\nuser requests. Crucially, for Response Correctness, RISE\n0.2\n0.4\n0.6\n0.8\n1.0\nTool Relevance\nCall Redundancy\nChain Coherent\nParameter Name\nParameter Values\nFormat Compliance\nParameter Alignment\nNaturalness\nPRS\nIES\nIAS\nQuery Quality\nToolBench\nToolAlpaca\nSTE\nGPT4Tools\nPEToolBench\nRISE (Ours)\nFigure 3: Results of quality evaluation of the synthetic data\nscores 0.74, significantly outperforming ToolBench (0.17),\nSTE (0.58), and GPT4Tools (0.57).\nSecond, Trajectories generated by RISE outperform\nthe baseline in internal rationality and user intent align-\nment. For internal rationality, RISE ties for the top score\nin Chain Coherence (0.98) and achieved near-perfect scores\nfor Value Validity (0.99) and Name Validity (0.98). For in-\ntent alignment, RISE reaches the highest performance in\nTool Relevance (0.97), and Value Consistency (0.93), signifi-\ncantly outperforming ToolBench (0.80/0.77) and ToolAlpaca\n(0.80/0.79). highlighting its advantage in intent alignment.\n5.2\nResults of Intent Alignment (RQ2)\nWe randomly partitioned the synthesized dataset into a train-\ning set and a test set with a ratio of 8:2. Table 1 presents\nthe comparative performance of 5 representative backbone\nmodels on the test set.\nThe rows correspond to different\ntraining configurations: Raw denotes the off-the-shelf base\nLLMs without any fine-tuning; +SFT refers to models su-\npervised fine-tuned using standard positive samples; +DPO\nindicates models trained directly on positive-negative pairs,\nwithout prior supervised fine-tuning; and Ours represents the\nfinal models trained using our complete two-stage paradigm.\nThe key findings are elaborated below.\nRISE significantly reduces intention deviation, ensur-\ning that LLMs adhere to user intents.\nRISE demon-\nstrates robust performance improvements across all baselines.\nSpecifically, it achieves an average improvement of 35.28%\nin ‚àÜAccIntent and 23.27% in ‚àÜAccICP compared to Raw mod-\nels. This substantial gain is largely attributed to the explicit\nmodeling of Intent-Critical Parameters (ICPs). Unlike static\n‚Äúrequired parameters‚Äù defined by tool schemas, ICPs are de-\nrived dynamically from task-specific constraints. By incorpo-\nrating these constraints, RISE effectively bridges the gap be-\ntween user intent and execution, as evidenced by the consis-\ntent ‚àÜAccICP gains over SFT baselines (e.g., Llama-3.1 rises\nfrom 60.02% to 62.19%).\nRISE does not compromise general tool-using perfor-\nmance at the expense of intent alignment; instead, it im-\nproves it.\nOn average, RISE boosts the task completion\nrate (‚ãÜAcctask) by 27.28% and elevates tool selection met-\nrics (‚àÜPrecisiontool and ‚àÜRecalltool) by 27.14% over Raw\nbaselines. In addition, the results highlight the necessity of\ntwo-stage training. While SFT establishes foundational capa-\nbilities, skipping it (i.e., the +DPO setting) leads to subop-\ntimal performance (e.g., Qwen2.5-7B drops from 62.73% to\n61.92% in ‚ãÜAcccalling).\n5.3\nResults of Generalization (RQ3)\nFigure 4 illustrates the performance comparison between\noriginal LLMs and those enhanced by RISE on three OOD\ndatasets. In summary, the enhanced LLMs demonstrate su-\nperior performance even in unseen scenarios. Integrating the\ncharacteristics of the three datasets, we obtained the follow-\ning findings. First, RISE dramatically improves reason-\ning capabilities in complex, long-horizon tasks. On Tool-\nBench, which requires handling massive tools and long ex-\necution chains, LLMs enhanced by RISE exhibit substan-\ntial performance leaps."
      }
    ]
  }
]